{
    "id": "ujjq4zlxw4syfpzklhe5w244mibrgcjd",
    "title": "Geometric Methods and Manifold Learning",
    "info": {
        "author": [
            "Partha Niyogi, Department of Computer Science, University of Chicago",
            "Mikhail Belkin, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09us_niyogi_belkin_gmml/",
    "segmentation": [
        [
            "We're almost ready to begin, so welcome to the third tutorial review session of this.",
            "Workshop in summer school.",
            "As you know, we have these sort of tutorial reviews running all through the school for about 10 days or so, so.",
            "What we're going to talk about today is.",
            "Is about geometric methods and manifold learning.",
            "So what Misha, Belkin and I are going to do is split presentation between the two of us and hopefully what we would be able to communicate to you is.",
            "Some idea about class of methods that.",
            "Have received some attention over the last decade or so, and a perspective on machine learning that seems to have emerged over the last decade.",
            "2 An It's a very sort of geometrically oriented point of view with which one can approach questions in pattern recognition, machine learning, and sometimes even a numerical computation.",
            "And hopefully you will get a sense of the way in which ideas from probability and statistics ideas from computer science, algorithms, and especially graph theory an ideas from.",
            "Geometry and topology will come together in some sort of natural way in this setting, so the logistics of this is like this.",
            "I'll speak for about an hour, no more than an hour and 15 minutes, after which we will take a break for about 15 minutes.",
            "Misha will then speak for about an hour after that.",
            "Maybe an hour and 15 minutes will take a second break, and then we'll have a closing session for about half an hour to wrap up odds and ends and things of that sort.",
            "So."
        ],
        [
            "Context.",
            "In which all of this really developed is really the context of data analysis and machine learning problems in very, very high dimensions.",
            "So increasingly we live in a world in which we are faced all the time by learning problems in very, very high dimensional spaces and genetics and neuroscience in image analysis, speech analysis or other problems that we might have worked on and very.",
            "A basic and fundamental sort of question that faces all of us is to try and understand under what circumstances it might be possible to make effective inferences in high dimensional spaces without running into the so called curse of dimensionality.",
            "OK, so as far as I see it.",
            "People have of course known about the curse of dimensionality for a long time, and there are at least three different points of view from which one can approach this central issue.",
            "The 1st and the most classical of these is the point of view of smoothness.",
            "So let me clarify.",
            "So.",
            "It has been known for a long time that suppose you're trying to learn a function an it's a function of several variables.",
            "And it has say.",
            "S derivatives OK, so it belongs to what is called a Sobolev space HSA.",
            "So it's a function.",
            "Maybe I should use the board a little bit.",
            "Actually, if I just shout, can you hear me or should I use this as I have to handle?",
            "OK, maybe I'll use this Mike.",
            "Maybe this is better, OK?",
            "So I have a function.",
            "From Rd 2R, so it's a function of the variables and it has S derivatives in L2, so F. Belongs to HS say something like this, then the rates of convergence look like this.",
            "This is the number of examples you need to learn a function OK?",
            "And I think.",
            "It's actually something like this to S + D or something like that.",
            "OK, not S / D, but approximately S / D. So.",
            "If you look at that rate.",
            "OK, you see that if S, the smoothness is of order D. Then the rate does not depend upon the dimension.",
            "And therefore there is no curse of dimensionality.",
            "OK, so this is of course known for a long time and so people realize that if the function you're trying to learn is a function of several variables, it's a function in a high dimensional spaces, but it's very smooth and the smoothness.",
            "Is, in a particular sense, then there is no curse of dimensionality and this insight or this observation has an algorithmic reflection.",
            "So based on this insight, people have realized for a long time that perhaps it's a good idea to try and learn smooth functions.",
            "Perhaps it's dry.",
            "It's a good idea to try and fit the smoothest function that you can to the data, and so if you take this point of view, the kind of algorithmic consequence of that are things like splines, kernel methods, reproducing kernel Hilbert spaces, where the kernel K has certain smoothness properties.",
            "L2 regularization in such spaces, and so on.",
            "And these are the kinds of algorithms that arise out of this basic insight.",
            "Now there's a second point of view.",
            "And the second point of view is the point of view, not of smoothness in the classical sense, but the point of view of sparsity.",
            "So smoothness is a has been known for awhile.",
            "I think at least since the 60s or so and sparsity the point of view is this that people realize that maybe the function you're trying to learn is not smooth in any classical sense.",
            "But maybe it can be represented.",
            "In a sparse way, in terms of some basis functions.",
            "As a sparse combination of some basis functions an in this setting two, it might be possible to learn effectively.",
            "OK, in other words, this is the insight that the function depends on.",
            "A few relevant features perhaps.",
            "And this has an algorithm inconsequence too.",
            "If you take this point of view an you think of things like wavelets like L1 regularization, lasso, compressed sensing and these are algorithmic developments that somehow emerge out of this realization, that sparsity is a good thing and can be exploited.",
            "So that's the second point of view.",
            "And the Third Point of view, which has really the most recent of these, is the point of view of geometry.",
            "OK, so that's what I'll try and explain what that means.",
            "And this point of view also has an algorithmic consequence.",
            "So just like smoothness makes you think of splines and kernel methods and L2 regularization and sparsity makes you think of L1 and compressed sensing and lasso and so on.",
            "When you take the geometric point of view, the algorithms that emerge out of trying to exploit this POV make use of the notion of graph simplicial complexes.",
            "Laplacians, diffusions, and so on, and we will see these objects over the next hour or so."
        ],
        [
            "OK, So what is?",
            "The geometric POV, so this is somehow the geometric thesis of data analysis in very high dimensional spaces.",
            "OK, the central dogma is this.",
            "It's an argument in two parts, so the first part of the argument says that look in very very high dimensional spaces.",
            "The data is not going to be distributed uniformly.",
            "That's absurd.",
            "So in fact the data will not be distributed uniform.",
            "The distribution of the data will have some shape.",
            "If there is some shape, perhaps there is some geometry to be done if you want to understand this shape, maybe it concentrates around certain kinds of structures.",
            "Maybe these structures are low dimensional and this is going to be general fact of natural datasets that natural datasets are going to have particular shapes in the high dimensional spaces.",
            "And that's based on the intuition that natural data tends to be generated by systems, maybe physical systems or non physical systems that have few underlying degrees of freedom.",
            "So that's the first part of the argument.",
            "So the data has some shape and the second part of the argument says that if the data has some shape, perhaps you can exploit the shape of the data or the geometry of the data in some sense to define suitable classes of functions with which to operate that are geometrically motivated and adapted to the shape of the data and develop representations and develop algorithms that are adapted to the geometry of the data.",
            "And if this is done wisely, this might allow us to exploit the the geometry of the data to enable efficient learning.",
            "So."
        ],
        [
            "One setting in which one can.",
            "Explore this geometric thesis with some clarity.",
            "Anne understand what the consequences of this thesis are.",
            "Is the setting of the manifold is a setting of manifold learning?",
            "So manifold learning is not a single problem.",
            "But rather it's a collection of problems unified by some common assumption, and that common assumption is this that the data lives.",
            "On or near some low dimensional manifold embedded in this high dimensional space and we'll talk a little bit about how one can relax this an generalize this further.",
            "But this is our first attempt or the first attempt that community has made to try and make sense of the notion that the data might have some geometry that is far from uniform.",
            "And then try and understand.",
            "What the consequences of that fact might be in terms of leading us towards certain geometrically motivated classes of functions or leading us towards certain geometrically oriented learning algorithms so?",
            "This is manifold learning.",
            "You have to learn in a high dimensional space space all the data lives on or near some low demand.",
            "Typically low dimensional manifold embedded in this high dimensional space and the Canonical learning problem is always this.",
            "You want to learn a function.",
            "OK, but in this case the natural domain of the function is actually the manifold on which all the data lives.",
            "So you have to learn a function whose domain is this manifold an whose range might be a finite set.",
            "If you're trying to do clustering or classification, or it may be the reals if you're trying to do, say, regression or dimensionality reduction.",
            "So what's somewhat peculiar about this setting actually, which leads to some non trivial questions is this.",
            "That although all the data lives near some manifold, for the most part, we don't know what this manifold is.",
            "So we will somehow have to discover this manifold or discover a geometrically motivated class of functions that is adapted to this unknown manifold.",
            "Without knowing what this manifold is.",
            "But knowing that all the data lives on or near some manifold.",
            "And that leads to the following kind of somewhat basic algorithmic questions.",
            "Suppose.",
            "There is.",
            "Compact Riemannian submanifold of Euclidean space, say.",
            "And suppose I sample it and give you a collection of points that sit on this manifold.",
            "So this is a collection of points living in some very high dimensional space.",
            "Sitting on this manifold.",
            "What geometry got topological properties of this manifold?",
            "Can you in fact learn?",
            "From these randomly drawn points.",
            "OK, it's not a priori obvious that you can learn any geometry, or at least any nontrivial geometric or topological properties of this fixed underlying manifold from randomly drawn points presented to you.",
            "So all the data is living on this manifold.",
            "You're seeing this cloud of points in a very high dimensional space.",
            "All you can compute are distances in this very high dimensional space.",
            "Because these are points in this high dimensional space in this high dimensional space is Euclidean space.",
            "And you can't see geodesics.",
            "You can't see, you can't see anything.",
            "All you see is a cloud of points.",
            "OK.",
            "So.",
            "If you think for awhile about the simplest kind of learning problem I have.",
            "Coin with bias Pi.",
            "Toss it a few times and I tell you what the outcomes are.",
            "Can you infer what this underlying fixed unknown biases?",
            "And the answer is of course yes.",
            "You can infer the bias of a coin by tossing it a few times and counting basically how many times it came up heads and how many times it came up.",
            "Tails you can estimate lots of things we know how to do that from statistics, and so now we're asking a kind of statistical question about geometry.",
            "There's an underlying manifold.",
            "There's a bunch of points.",
            "What can you learn about this manifold?",
            "If I gave you the points and nothing else.",
            "What can you infer?",
            "And so we'll see answers to this kind of a question.",
            "And the reason this kind of question arises is if you take the view that in high dimensional spaces the data is living near some manifold and the structure of this manifold can be exploited.",
            "Then you need to at least understand what can you learn about this manifold in the 1st place before you begin to exploit its structure.",
            "OK.",
            "So the reason this is peculiar if you're trying to do regression, say, is that we are in the setting where we are trying to learn both the function and its domain simultaneously.",
            "The natural domain of this function is this manifold.",
            "And you might be given XY pairs.",
            "Where the X is all live on this manifold and the wise are what they are and you're trying to learn a function F from M to R. And you're trying to learn both M and this function simultaneously."
        ],
        [
            "OK, so now.",
            "One can give a formal justification for this in many different ways, and maybe we'll go through some of these examples in in a little while, but you can.",
            "Look at for example the given argument for why the set of speech sounds might live on or near some manifold, or why it might be a reasonable model for certain classes of images, and there has been some work along this, but it is not very easy all the time to give a formal or physical justification from first principles as to why the data would live on a manifold OK.",
            "So.",
            "The way to view this really is to say that.",
            "Maybe I can't give you a formal justification for why the data is lives on a manifold, but maybe I will use this as a modeling device because I know that the data is not going to be distributed uniformly, but will have some shape.",
            "And so it will allow me to define maybe a class of geometrically oriented probability distributions.",
            "These are what I will call the manifold plus noise probability distributions.",
            "That will allow us to reason about.",
            "Nonuniform data in very high dimensional space is now the simplest such model.",
            "The manifold plus noise model.",
            "The simplest such model is actually the mixture of Gaussians, although we don't think of it as such.",
            "So imagine you had a mixture of Gaussians.",
            "OK, so this is a mixture of Gaussians.",
            "And if you go and talk to a statistician, of course they've studied this for years.",
            "They'll tell you this is exactly what it is.",
            "A mixture of Gaussians.",
            "But now if you take a. Topological view are a geometric view of this.",
            "You can view this probability distribution.",
            "OK, which we've studied for many years as essentially data sampled from this distribution is data sampled from a manifold plus noise and the manifold here is kind of a degenerate manifold.",
            "Which is a 0 dimensional manifold.",
            "Would say K connected components.",
            "So you have K mixtures.",
            "Each of these is a point.",
            "OK. And there is noise in the normal direction.",
            "Gaussian noise in the normal direction.",
            "So imagine this setting of manifold plus noise OK.",
            "There's an underlying manifold which is 0 dimensional, made up of K connected components, which are these points OK, these points may be identified with the centers of these Gaussians and then there is noise in the normal direction.",
            "And if I sample this manifold plus noise probability distribution, I will essentially get the same distribution of data as I would if I sampled a mixture of Gaussians.",
            "So if I sample this manifold plus noise, this is what I would see a bunch of points sitting like this.",
            "OK. And from this.",
            "I might like to uncover.",
            "A very simple topological invariant I might want to find out how many connected components does my underlying manifold have.",
            "That's the same question as I give you points on mixture of K Gaussians.",
            "Can you tell me what K is?",
            "OK.",
            "But a significant generalization of this would be to say the data doesn't have to be from a mixture of Gaussians, the data code.",
            "For example, look like this.",
            "And all this data lives close to a manifold, but this manifold is not a single point.",
            "But rather, this manifold seems to be a circle.",
            "So there's a whole class of such geometrically oriented probability distributions which are used then to model data in very high dimensional spaces.",
            "And.",
            "Mom.",
            "That's sort of the manifold plus noise point of view is just a modeling technique for data and very high dimensional spaces, and you might just throw away any formal justification to say why data would actually live near a manifold.",
            "Rather use the manifold in the same way as you use a mixture of Gaussians as a generic nonlinear nonparametric method to approximate the probability distribution in a very high dimensional space."
        ],
        [
            "OK, so this is really the take home message of all of this.",
            "What we will see today is really.",
            "Nonlinear nonparametric method to learn in very high dimensional spaces where geometry plays a very big role in guiding our intuitions.",
            "And the starting role of all of this is really the heroes of the story.",
            "Are these two characters that are plus Beltrami operator on the manifold and the heat kernel on this manifold?",
            "And.",
            "I'll tell you a little bit about.",
            "These two objects and what their consequences are for the design of learning algorithms and other other things of that nature."
        ],
        [
            "So.",
            "A point I wanted to make an it's a good point to actually begin.",
            "This whole story is to go back to a very classical method that people have known for a long time.",
            "Maybe 100 years?",
            "Which is principle components analysis.",
            "So if I give you a lot of high dimensional data, the instinct of most people is to take this data, reduce it to a lower dimensional space by doing something like principle components analysis, which is a widely used technique OK, and to try and recover some low dimensional structure in the data.",
            "So what is principle components analysis look like?",
            "Most of you have seen this in some form.",
            "So given a bunch of data X1 through XN sitting in a high dimensional space, you want to project it.",
            "You want to say projected into a 1 dimensional space, just R and find numbers Y one through YN.",
            "Each Yi is just a projection of XI.",
            "OK.",
            "So you want to find.",
            "Y1 through YN to represent your high dimensional data X1 through XN.",
            "And you want to find the projection.",
            "That maximizes the variance of the data.",
            "Here I've assumed that all the excise have mean zero and so automatically the wise have mean zero, so that the variance has this particular formula which is the sum of the squares.",
            "I mean you actually have to mean subtract 2 for the correct definition of variance, but I've just assumed the mean is 0 here.",
            "So you want to find that W that maximizes the variance, and it's fairly easy to see that the sum of the squares of the wise ends up being this particular quadratic form in W, so you want to find the W that maximizes this particular quadratic form, and that gives rise to a rally rich like question and then leads to finding the leading eigenvector.",
            "Of the data covariance matrix.",
            "So I'm sure all of you have seen this.",
            "There is also something else which is a way to interpret principle components analysis.",
            "So what principle components analysis is doing?",
            "Also in some sense is fitting.",
            "The best.",
            "Linear subspace to the data of a certain rank of a certain desired.",
            "Dimension, so let me just write that.",
            "So you have all this data X1 through XN.",
            "Consider some subspace H. Consider the projection.",
            "Onto age that toggle projection onto age of XI.",
            "And consider this least squares.",
            "Problem.",
            "And I want to minimize this.",
            "Overall, H. This is also essentially the same problem as principle components.",
            "OK, So what principle components is doing is trying to essentially fit in this least squares sense.",
            "A linear manifold to the data.",
            "So this is very classical, widely used.",
            "Anne.",
            "You could think of it also just like the mixture of Gaussians is very classical and widely used and is the simplest instantiation of a.",
            "Manifold.",
            "Structured probability distribution.",
            "PCA, which is another classical example and widely used, can be thought of.",
            "As the simplest instantiation of fitting a manifold today to where this manifold is just.",
            "Linear space."
        ],
        [
            "So what was what is the more general manifold model say?",
            "Suppose the data does not lie on or near some linear space at all.",
            "Suppose actually the distribution of the data looks like this.",
            "OK, where you can clearly see that the data seems to lie on some set, which seems to have only one degree of freedom.",
            "And yet, it's not obvious that it lies in some linear space.",
            "It's not doesn't lie in some 1 dimensional space.",
            "So what happens if the data looks like this?",
            "Can you actually develop some scheme where you could unravel the structure of the data?",
            "Realize that there is 1 degree of freedom in the data.",
            "Represent the data appropriately without trying to either fit some linear subspace to something that looks like this fit a line through this curve.",
            "Or anything else you might think of."
        ],
        [
            "So let me give you an acoustic example which is remarkable she.",
            "Ends up with a distribution of data that looks more or less like what I just told you.",
            "OK, because the picture that I put up might seem artificial.",
            "It might seem that I just made it up to make some abstract point, so let me give you an actual example of data that's generated by a very simple physical system.",
            "OK, and here's the physical system.",
            "It's an acoustic tube of length L. And I blow puffs of air at one end of the tube, U of T is the excitation of this tube.",
            "So I blow puffs of air at one end of this tube and a sound comes out from the other side of the tube and that sound is SFT.",
            "OK.",
            "So you have to use the excitation.",
            "The puffs of air I blow into the tube, and SFT is the sound.",
            "The sound that this still makes both UFT an SFT are functions.",
            "They live in some function spaces, so let's assume for simplicity that these are actually periodic functions, so they have Fourier series expansions.",
            "They live in little L2.",
            "And let's play the following game.",
            "Every day you come to me and you collect the sound that I make for you on that particular day.",
            "OK, you don't know what I'm doing.",
            "I'm just generating sounds for you every day you come to me come to me and you collect the sound.",
            "The sound as we just discussed lives in little L2 so the sound lives in an infinite dimensional space.",
            "So every day you go back with an infinite dimensional data point.",
            "But what I'm doing from day to day?",
            "OK, unknown to you is I'm actually changing 1 degree of frayed freedom in the physical system that is generating the data for you.",
            "What I'm doing is I'm just changing the length L of this tube."
        ],
        [
            "So how would you reason about this?",
            "You would actually try to understand what is the set of all the sounds this physical system can generate, where there is only 1 degree of freedom in this physical system.",
            "And to understand this you would actually have to do something like this.",
            "Solve the equations of airflow in this tube.",
            "And these are the equations is a couple equations leading to what is called the Webster Horn equation.",
            "Ultimately VP of X, T is the pressure as a function of time.",
            "T is time.",
            "X is location in in this in the horizontal axis.",
            "So in other words S of T the sound this tube generates this tube makes is really the pressure wave.",
            "At when X is equal to L. OK. UFT is the pressure wave when X is equal to 0 at the other end of the tube."
        ],
        [
            "So you solve this and you get a nice closed form answer for how U of T is related to SFT.",
            "An if you have T looks like this, it's a periodic function.",
            "It has a Fourier series expansion.",
            "It lives in little L2.",
            "The Fourier coefficients are the Alpha ends, so every sound you represent by computing its Fourier coefficients, and then it's a point in this Fourier space OK?",
            "And betas are the Fourier coefficients of the S is and what I've plotted is the set of all sounds in this space spanned by beta 1 beta three and beta 7 for a particular simulation.",
            "OK, so basically the set of all sounds generated by this tube with just one degree of freedom would lie on a.",
            "1 dimensional curve embedded in little L2 and this picture looks more or less exactly like the picture that I made up.",
            "A few slides ago."
        ],
        [
            "Now.",
            "You might think even this is artificial.",
            "When does you know when are we in this situation where we are analyzing just single tubes, but actually there is a long tradition and some of you who?",
            "Come from PZ and acoustics and its application to speech analysis would be aware of this.",
            "There's a long tradition in modeling the Volk, the set of speech sounds as those generated by a non uniform tube.",
            "The vocal tract as this nonuniform tube is excited by puffs of air that you blow into this tube by moving your lungs.",
            "And that's how you speak.",
            "And this is essentially the acoustic model of speech production, which is the classical model developed in the 50s and 60s and 70s, and sort of elaborated over the last 30 or 40 years.",
            "And so if you take this view.",
            "You would say that look all the speech sounds that are being generated a generated by a physical system and the degrees of freedom in this physical system are not that many.",
            "You cannot move your vocal tract in arbitrary ways.",
            "You cannot generate any sound any desired sound by moving a vocal tract, you can only generate the set of speech sounds that are naturally generated by a vocal tract, so it's a physical system with few degrees of freedom.",
            "And the set of all speech sounds would therefore live in some low dimensional subset of the ambient space of all signals in which this is embedded."
        ],
        [
            "So that's an example from acoustic, so let me give you an example from images.",
            "So what's an image?",
            "An image more or less is something like this.",
            "A function from R2 to 01, where at any XY location F of XY is just the intensity of that image at that location.",
            "OK.",
            "So every image is therefore lives trivially in some function space, again, just like acoustic signals live trivially in some function space.",
            "And now I want you to consider the following class of images very simple.",
            "Class of image is almost a trivial class of images.",
            "These are images of a vertical bar under translation.",
            "These are pictures of vertical bars.",
            "OK.",
            "So as a set, this is the set script death.",
            "So every element of this set script death is a particular function that corresponds to a particular image of a vertical bar, and the way this set is generated is by taking a particular vertical bar, which we call V. That's the image of a particular vertical bar, and keep translating this vertical bar, so all F's in this set script F can be represented in this way by the translation of a vertical bar by T. In law in the X axis and Y.",
            "Buy an R in the Y axis, it's very.",
            "A simple example and now ask the question if script F is the set of all images of vertical bars.",
            "This set script DEF is embedded in some ambient space, which is the set of all functions.",
            "Because all images are functions OK and immediately notice that this set script F is not a linear space.",
            "If I take an element of script defan take another element of script F and add the two together, I no longer remain in this space.",
            "This is just a simple observation that if I take a picture of a vertical bar, take another another picture of a vertical bar, and add those two images together, I don't get a picture of a vertical bar.",
            "So this set of all images.",
            "Is not a linear space, it's some.",
            "Other kind of set?",
            "And also notice that there are actually only two degrees of freedom.",
            "In the set of all pictures of vertical bars and they are parameterized by TNR.",
            "And so I think to be technically right, if you want to be smooth manifold, this picture has to be perhaps not exactly a vertical bar, but something which is a slightly smoothed out version of a vertical bar, so it doesn't have sharp edges or discontinuity's like that, but more or less this general argument can be converted into an argument to say that the set of all pictures of vertical bars would be a 2 dimensional manifold embedded.",
            "In some space higher dimensional space."
        ],
        [
            "Here's an example from robotics.",
            "So say you want to move a robotic arm.",
            "OK, and you have a target location.",
            "And you have the joints.",
            "Which I think you see maybe 123 joints here.",
            "OK. And every joint location is indicated by the angles of the joints, which is Tita I anfi I so theater 131 is the first joint theater 2 feet two is the 2nd joint, an XY and Z is actually the location of the tip of this robotic arm.",
            "OK.",
            "So if you want to move this robotic arm around to a desired location.",
            "OK, you are trying to find a function G. Where the configuration space, the natural structure of the configuration space is the set of all valid joint angles.",
            "Which live essentially.",
            "In that space.",
            "And the tip of the arm lives in essentially in a Euclidean space.",
            "OK, so here's a function.",
            "Whose natural domain is some manifold and whose range here is some Euclidean space?",
            "And you might want to actually learn this function.",
            "And that's exactly the manifold learning one example of the manifold learning problem.",
            "Where I would like to learn a function whose domain is some manifold and whose range is the reals."
        ],
        [
            "So these are some examples OK, and these kinds of examples people have thought about a little bit over the last several years to try and give a more precise and accurate formal justification for why you might be interested in learning a function whose domain is the manifold and why that's a meaningful question.",
            "But as I said."
        ],
        [
            "More generally, you could also view.",
            "Manifold learning as really taking the view that all the data lives are manifold as the first simplest entry into a class of algorithms that are motivated by the intuition that the distribution of the data is not uniform.",
            "OK.",
            "So now that we have some sense of why we want to do something like this, and why people have wanted to do something like this.",
            "Let's just review a few basic objects."
        ],
        [
            "And just.",
            "A few very basic objects so that we know what we're talking about as we proceed."
        ],
        [
            "So.",
            "So we're going to mostly just for simplicity, discuss embedded manifolds.",
            "And those of some of you come from mathematics.",
            "They already know everything about this and don't need any of this.",
            "Some of you may come from other areas, and so just to guide your intuition.",
            "You may think of this fear as the simplest example, or a model manifold that you might want to think about.",
            "And.",
            "I'm not going to define a manifold through a charts and atlases and things like that."
        ],
        [
            "But just give you the intuition.",
            "And so, given a point P on the manifold, there is something the tangent space on the manifold which for a K dimensional manifold is essentially AK dimensional.",
            "Space.",
            "And you could think of it since we are discussing.",
            "Mostly embedded manifold you can think of it as a K dimensional affine subspace of RN, exactly like this picture of a sphere.",
            "The two sphere embedded in R3 where the tangent space is a 2 dimensional affine subspace of R3."
        ],
        [
            "So since this tangent space is this linear space.",
            "You can think naturally of tangent vectors.",
            "And one point I want to make."
        ],
        [
            "Is that somehow there's a?",
            "Natural.",
            "Connection between tangent vectors and curves.",
            "So if you think of a curve, so what's a curve?",
            "Fee of tea is a curve is a map from R to MK.",
            "OK, so that traces out a curve on the manifold.",
            "And it should take.",
            "The derivative of this with respect.",
            "To T. You get a vector, and that's essentially the tangent vector.",
            "So every tangent vector can be identified with a curve.",
            "The tangent vector sits in TP where P is a point on the manifold.",
            "So every tangent vector can be identified with a curve that passes through P whose derivative.",
            "Matches the vector."
        ],
        [
            "Tangent vectors can also be thought of as derivatives in the following way.",
            "If you take a function F from MK to R. And you pick a vector in the tangent space."
        ],
        [
            "Since you already discussed how vectors are identified with curves, so the vector V can be identified with the curphey of tea.",
            "From R2 MK.",
            "And then this object makes sense by composing the two F of C of T. And that's just a regular function from R2R.",
            "Right, so you have a function from M to R. You pick a point P on your manifold.",
            "You consider the tangent space at P, which is TP.",
            "You pick a vector V in this tangent space.",
            "That vector V can be identified with a curve on the manifold that passes through P. And if you take.",
            "F of 50 is just get a regular function.",
            "And if you differentiate it with respect to T, which we know how to do from general calculus.",
            "That's the same thing as basically taking the directional derivative of F in the direction V. And.",
            "You can think therefore of tangent vectors as operators that act on functions and take.",
            "Directional derivatives in certain directions.",
            "So tangent vectors I had five with curves on the one hand and with directional derivatives on the other hand, in this way."
        ],
        [
            "And the thing that gives it its Riemannian structure.",
            "Essentially.",
            "Will be.",
            "Norms and angles in the tangent space so that you can talk about inner products between any two vectors.",
            "V&W in the tangent space.",
            "Since everything here is embedded, we don't have to consider it abstractly right now, so you can do this abstractly, but you see the tangent space is just an embedded linear subspace, or just naturally inherits the inner product structure from the space in which it is embedded, and so this V&W is exactly what you would think it to be from this picture.",
            "OK.",
            "But that's what gives the Romanian structure."
        ],
        [
            "And so now that you have this, you can talk about lengths of curves.",
            "And you can talk about geodesics.",
            "So what is the length of a curve?",
            "So you begin with what is a curve.",
            "A curve is a map from 012 MC.",
            "And if you take its derivative, you actually get a vector, right?",
            "Defeat by DT is actually a vector.",
            "And if you take the norm of that vector, that makes sense because all the vectors are living in some space where you have norms or inner products defined.",
            "And if you integrate that along the length of the curve.",
            "You get the length of that curve.",
            "So that's how you would calculate the length of any curve and now between two points you can set up a variational problem to ask for the shortest curve that goes from one point P to another point Q.",
            "And that more or less will give you the notion of a geodesic."
        ],
        [
            "So one more thing.",
            "The gradient, so what's the gradient?",
            "Of a function.",
            "So if you take a function F. From M to R. OK.",
            "In the gradient of that function.",
            "OK will be.",
            "Will act as an operator on vectors in the tangent space.",
            "So that given any V on the tangent space.",
            "If I did.",
            "That inner product of the gradient with V. Is differentiating the function in the direction V?",
            "So I fixed the function FI, pick any vector V in the tangent space.",
            "I can differentiate DF in the direction V, and I get a number.",
            "So I get a map from functions to numbers.",
            "And this makes use of this connection that we just saw between tangent vectors and directional derivatives.",
            "And.",
            "Using this sort of.",
            "Notion you see that the gradient is going to point in the direction of maximum change.",
            "Because you would just maximize this over all possible directions."
        ],
        [
            "There's one more notion, basic notion, which is the exponential map.",
            "So the exponential map.",
            "Will take you from the tangent space back to the manifold, so we now have the manifold.",
            "We have a point P on the manifold.",
            "We have the tangent space at the point P. We have functions defined in the manifold.",
            "We have vectors living in the tangent space on the manifold and now consider TP of M which is the tangent space of M at the point P. OK, so if I pick any element of TP of M, any element of this tangent space is a vector.",
            "And the exponential map will take me.",
            "From TP back to M and the way you do this is that you essentially start.",
            "Going in a geodesic.",
            "Along V. For such that the length of the curve has Norm V. OK, so the vector V that you've picked has a certain norm as a number.",
            "And so now you will go essentially along this manifold.",
            "For a distance which is equal to the length of this vector V."
        ],
        [
            "The reason?",
            "To introduce it all here is that perhaps it's an easiest.",
            "It's an easy way.",
            "I don't know if it's the easiest way, but it's seems to be an easy way to think about and define the Laplace Beltrami operator on this manifold.",
            "Because this is an object that's going to come up, so we need to understand a little bit what this is.",
            "So.",
            "Because we have the exponential map that takes me from any vector.",
            "In the tangent space at P back to the manifold, this gives me a coordinate system, a natural coordinate system on the manifold.",
            "OK so I have this K dimensional space which is the tangent space.",
            "And if I pick any vector in this K dimensional space by applying the exponential map to this vector, I get a point on the manifold.",
            "OK, so every point on the manifold has these.",
            "Geodesic coordinates, so to speak, or.",
            "It gives you coordinate system for points in the manifold.",
            "So now if you're given a function F from M to R. OK. You can express it.",
            "As a function of just K variables.",
            "By composing it with the exponential map.",
            "OK, so F composed with the exponential map will take you from the tangent space.",
            "OK, the exponential map takes you from the tangent space to the manifold and the function F takes you from the manifold to R so that composition will take you from the tangent space to R. And the tangent space is just a regular K dimensional space identified with RK.",
            "So this composition is just a regular function, a standard function from RK2R.",
            "OK, and for a standard function from RK2R.",
            "The Laplacian is something which maybe many of you have encountered for the standard function, so this is just how to define the Laplacian for manifold.",
            "So the Laplacian for a standard function from RK2R looks exactly like this.",
            "So let me just write that on the board.",
            "So here's what it looks like.",
            "You're just looking at a standard, just the usual function from RK2R.",
            "Then Laplace of F. And sometimes there's a - it's just this.",
            "So it's a twice differentiable function from RK to R. From one through Ki, going from one through K. And you differentiate twice.",
            "In each direction and sum it up.",
            "So this gives you a map from twice differentiable functions from RK.",
            "And you get.",
            "Another function.",
            "Right?",
            "So it gives you this operator.",
            "So the standard Laplacian everyone has encountered probably.",
            "In Euclidian settings.",
            "This is how the Laplacian is defined for functions from RK2R an.",
            "What we're trying to do is define the natural Laplacian for functions defined on a manifold.",
            "So what you would do?",
            "Is basically consider the function from M to R. And you would like to differentiate it twice.",
            "You pick a point P. You would like to differentiate twice in each direction in each natural direction along P. OK, and then add it up.",
            "And formally, what you would do is this.",
            "You would use the exponential map to go from the tangent space to the manifold and composing those two just gives you a standard function from TP of M2R.",
            "DP FM is a K dimensional Euclidean space and the Laplacian for that is defined.",
            "And so this.",
            "Tells you what the Laplace Beltrami operator is for functions on a manifold.",
            "OK, so now if you think of the circle, the manifold is a circle or the manifold is the sphere or the manifold.",
            "You can actually workout exact formulas for what the Laplacian on the sphere or the circle or any such very well structured manifold and we will look in particular at the following eigenvalue problem.",
            "Laplace F is equal to Lambda F. OK, so if I take any twice differentiable function on the manifold an I apply the Laplace operator to it, I get another function and I will look at the eigenvalue problem Laplace F is equal to Lambda F. And those are the eigenvalues.",
            "An eigenfunctions of the Laplace Beltrami operator on this manifold.",
            "So we will workout.",
            "I think the case for the circle pretty explicitly where this will all make sense."
        ],
        [
            "I don't want to say.",
            "Too much about curvature because it'll get us into.",
            "Somewhat more technical things today.",
            "Want to get into?",
            "There's an intrinsic notion, and then there is an extrinsic notion, and so on.",
            "But intuitively, I just want to make the high level point that if something is intrinsically curved you cannot flatten it, just like you cannot flatten the.",
            "Sphere, although it is a 2 dimensional.",
            "Surface, think of it that way."
        ],
        [
            "OK, so those are just.",
            "Oh yeah, those are only the things I wanted to say, so we now have a rough idea for the manifold is.",
            "We have a rough idea of what it means to be a function defined in this manifold.",
            "What it means to differentiate this function in any direction on the manifold?",
            "We have a rough idea of what the tangent spaces are or the tangent vectors are that the tangent vectors are identified with directional derivatives and we have this operator.",
            "Call the Laplace Beltrami operator that acts on twice differentiable functions on the manifold.",
            "Which we will come to later.",
            "So now let's go to the following kind of question, which a lot of people in this subject began with an.",
            "It's still actually somewhat unresolved.",
            "And this is the question that people actually began with, which is the question of dimensionality reduction.",
            "And I'll talk about some of these algorithms that people proposed over the last decades.",
            "We're beginning with, I guess something in 2000.",
            "So in the year 2002, papers appeared in Science magazine.",
            "One proposing this algorithm called I's OMAP another, proposing an algorithm called Eli, and they both made the following observation OK.",
            "They appeared in Science magazine in the year 2000 and they had a huge impact over the last decade and a lot of the work in some sense was trying to make sense of.",
            "The idea is.",
            "Maybe surrounding this algorithm or right or developing new algorithmic alternatives that maybe were a little better.",
            "We're a little more principled, perhaps an to make sense really of the geometric thesis and here.",
            "Is the question the question of dimensionality reduction?",
            "Suppose I give you a collection of points X one through XN, which lives on this manifold.",
            "Embedded in RN?",
            "In this manifold is a low dimensional manifold.",
            "OK, so.",
            "Intuitively, what that means is that there are very few degrees of freedom on this manifold.",
            "It's essentially low dimensional object.",
            "So can I actually re represent my data?",
            "The X one through XN which is sitting in a very high dimensional space?",
            "In a D dimensional space.",
            "So that it correctly captures the natural geometric relationships that the data will have with each other.",
            "OK, now there's a fact which I won't actually discuss in any detail, but the fact which I wanted to actually point out to you, which actually leads to something which I think can be formulated as a as a clear question.",
            "And that is this that you can take this manifold M. And it's a D dimensional manifold and you can buy what is known as Nash's embedding theorem.",
            "Embedding embedded isometrically in more or less.",
            "I forget whether it's 2D, maybe it's R2D or 2D plus one or something like that is more or less AD dimensional space.",
            "OK, and it depends a little bit on whether you want this map to be continuous or how many derivatives you want from this for this embedding, but.",
            "That observation says essentially that if you take if you have this manifold M, abstractly, you can embed it.",
            "Isometrically in a more or less dimensional space.",
            "OK, so now what does that mean?",
            "If all your data lives on this manifold an if I apply this map?",
            "To the data then I should get an embedding of the data in more or less a dimensional space.",
            "OK.",
            "But of course I don't know this map.",
            "OK, we know that this map exists.",
            "But we don't know this map.",
            "We don't even know the manifold, so forget about knowing this map.",
            "So the natural question then is if I give you a bunch of points sample from this manifold.",
            "Can you somehow discover this map that would embed this manifold isometrically in a D dimensional space?",
            "And then apply this map to the data and thereby end up embedding the data.",
            "Now in AD dimensional space, where is all this data actually lived?",
            "Originally, in an end dimensional space.",
            "So what was dimensionality reduction?",
            "It was basically the notion that all my data lives in a very high dimensional space, but close to a low dimensional manifold.",
            "I would like to embed it into a lower dimensional space that captures the natural structure of the data.",
            "The simplest example of this, and the classical example.",
            "Again, we go back 100 years to the principle components analysis scheme.",
            "What does it do?",
            "All my data lives in a very high dimensional space.",
            "I can.",
            "Embed it into a K dimensional space and I can find this embedding by looking at the covariance structure of my data and looking at the eigenvectors of the covariance matrix.",
            "So a number of algorithms have been developed over the last decade for trying to actually solve this problem of dimensionality reduction, and most of these algorithms do not come with proofs of guarantees or sometimes even not even with a precise statement of.",
            "What the question is.",
            "OK, but the intuition has always been this that because all the data lives in some low dimensional manifold, somehow we feel that we can embed it into a lower dimensional space and capture all the natural structure of the data.",
            "Because there are only very few degrees of freedom in this data.",
            "OK, so given what I just told you about exponential Maps and that every point on the manifold can now be represented in this exponential coordinate system, an since every data point lives on this manifold in every data, .2 can be expressed in this exponential coordinate system.",
            "Then a natural thing you may want to do is recover this coordinate system somehow.",
            "OK, so the manifold is low dimensional, so although all the data is living all over the place in the manifold, they are fundamentally representable.",
            "As K dimensional vectors.",
            "By somehow taking the exponential coordinate system."
        ],
        [
            "Now the algorithmic framework for all of this has tended to have the following high level picture.",
            "OK, so there is some manifold OK sitting in a high dimensional space.",
            "I don't know this manifold.",
            "I get a bunch of points from this manifold.",
            "So what am I to do?",
            "OK, I have to recover something after recover either a map from the manifold to some low dimensional space or learn some function whose domain is the manifold and so on.",
            "What people do is well, I don't know the manifold.",
            "I have a bunch of points.",
            "Let me actually make some kind of a mesh like structure or a graph like structure by comparing combined by.",
            "Connecting nearby points to each other."
        ],
        [
            "And I make a graph like this and this is my approximation to the manifold.",
            "And who the hell knows whether this is good or bad?",
            "But this is what people do.",
            "OK, later I'll actually prove some theorems about this kind of a construction.",
            "But this is what people do.",
            "The basic algorithm framework is.",
            "I connect nearby points to each other and nearby means nearby in the Euclidean space in which the points are embedded, because that's the only distance that I can measure.",
            "All my points are living in a Euclidean space.",
            "Given any two points, I can measure the Euclidean distance, and that's in fact the only thing I can do.",
            "OK, so I'll make this nearest neighbor graph."
        ],
        [
            "So it's common to many of these methods, and that's going to be my approximation to the manifold, and so if I wanted to do something on the manifold I will do something on the graph instead."
        ],
        [
            "So let's discuss the first of these schemes, OK, which is Isomap.",
            "Here's an algorithm, so I'm going to now go through several algorithms.",
            "Most of these algorithms do not come with.",
            "Proofs of correctness of any sort, but I'm trying to give you a sense of what motivated people to think of algorithms of this sort and what would be more precise versions of the questions these algorithms were trying to.",
            "Solve.",
            "So this is an algorithm ISOMAP, and this is what it does.",
            "You construct a nearest neighbor graph from all this data data.",
            "You find the shortest path distances between all these points along the graph.",
            "OK, you can use Dijkstra's algorithm or something to find the shortest path distance between all points on this graph.",
            "And so now you've got this set of distance this dij.",
            "OK, now this is already become a somewhat intrinsic object, because notice that dij is not simply the Euclidean distance between the points XI and XJ.",
            "OK, so you made a graph where every vertex is identified with the data point and between the Hyatt data Points XI and the GF data points.",
            "The XJ there is the Euclidean distance between them, but you made this nearest neighbor graph and there is the shortest path distance between them.",
            "And now you've got this this new distance metric and this distance metric is trying to simulate the geodesic distance between those two points.",
            "If you were trying to go along the manifold.",
            "So the natural question is, is this DJ close?",
            "Do the geodesic distance between XI and XJ, and that I think under certain circumstances is true.",
            "So now I've got this.",
            "Set of distance.",
            "Distances between points and I would like to embed it into a low dimensional space.",
            "OK, an embed it using this scheme called multidimensional scaling.",
            "So what is multidimensional scaling?"
        ],
        [
            "Multidimensional scaling is the following idea.",
            "I give you a matrix of distances.",
            "You go from that to a matrix of inner products somehow, and then from a matrix of inner products you go to an embedding.",
            "Into vectors whose distances will correspond to the distances that you have been given by this distance matrix.",
            "So how do you go from distances to inner products?",
            "You just make use of this identity.",
            "That if you take two points X&Y, two vectors X&Y, which are identified with points.",
            "Then this is just.",
            "The distance between X&Y, the Euclidean distance, can be expressed in terms of the inner product.",
            "That gives you a relationship between inner products and distances, and this relationship of course is true only if the distances are consistent with inner products.",
            "This is actually not the case if the distances were actually distances.",
            "Geodesic distances on the manifold.",
            "OK.",
            "So multidimensional scaling says you give me a distance metric distance.",
            "A matrix of distances.",
            "If there is a set of points.",
            "In a Euclidean space.",
            "From which this distance matrix could have arisen.",
            "Then I can find those points for you.",
            "OK, so if indeed there were a set of points from which these distance made this distance matrix arose.",
            "Then the inner products between those points would satisfy this relationship.",
            "And you can actually solve this and see that to go from D to a, which is the matrix of inner products, is given by this formula.",
            "OK, this is a little bit of linear algebra.",
            "OK.",
            "So you give me a matrix of distances.",
            "I can recover the matrix of inner products if this distance matrix arose from a set of vectors in Euclidean space, and once I've got a metric matrix of inner products, how do I find the vectors?",
            "Simply by looking at the eigenvectors of this matrix."
        ],
        [
            "So you do the next step of embedding.",
            "By essentially noticing that the matrix of inner product is going to be, well, technically it's going to be positive semidefinite, so to be a little careful you have to actually do the spectral factorization carefully where you keep the orthogonal spaces and the null spaces correctly.",
            "So this is done over all non zero eigenvalues I suppose Lambda I and so this is a map that will take you from for any X from one through N. You get a map CX.",
            "Like this?",
            "As it is written there and you can check that this map will actually satisfy the inner products and will satisfy the distance matrix.",
            "So the way the multidimensional scaling works is that you start with a set of distance with a distance matrix.",
            "You go from there to a candidate set of inner products and you go from that candidate sort of inner products to a set of vectors which are consistent with that inner product matrix."
        ],
        [
            "So what people did is they applied this algorithm.",
            "Do a lot of data OK so here are many pictures and this is a picture of.",
            "I guess a hand.",
            "Where the wrist is rotating.",
            "And.",
            "The finger is also extending, so I don't know if you can see this very well, but each of these pictures is a picture of a hand and the pictures differ from each other along 2 dimensions.",
            "In some of those pictures, the fingers are extended an in some of those pictures the wrist is rotated so there are two degrees of freedom in the natural degrees of freedom in the data, pictures corresponding to rotated wrist in pictures corresponding to extended finger and basically you collect a lot of these pictures.",
            "Lot of images.",
            "You apply this ISOMAP technique an you can now embed this data every picture into a 2 dimensional space by asking for 2 dimensional embedding.",
            "Once you embedded every such picture into a 2 dimensional space, you can look at every point in this 2 dimensional space and every point in this 2 dimensional space corresponds to a picture.",
            "Anne, this is what those pictures look like.",
            "You see that you actually recover the wrist rotation and the finger extension."
        ],
        [
            "So suppose all the data lived actually on some manifold and this manifold was flat.",
            "Here is a statement that is actually true intrinsically flat, so it unfolds a flat manifold which is isometric to a convex domain in RN.",
            "There's another algorithm called Hessian Eigen Maps that I didn't talk about.",
            "And another algorithm called local Tangent space alignment didn't talk about and all of them are trying to do the same circle of things.",
            "OK, you have all this data living on a manifold.",
            "Can you actually unroll this manifold?",
            "Can you actually re embed the data into a lower dimensional space?",
            "And then you can ask whether this re embedding is correct or isometric.",
            "In some sense, and some of these statements may be true for some of these algorithms."
        ],
        [
            "Here is another algorithm which people use called local locally linear embedding.",
            "So how does this algorithm work so?",
            "The way this algorithm works is you again begin with the nearest neighbor graph.",
            "And now what you do is.",
            "Let X one through XN be the neighbors of X. OK. You Project X to the span of X1 through XN.",
            "And that projection we're calling X bar OK.",
            "So I have all my data.",
            "I've made my nearest neighbor graph for every data point I can look at its nearest neighbors, an project that data point into the span of the nearest neighbors.",
            "And then once I've got the span of the nearest neighbors and I've got X bar, I simply find the barycentric coordinates of X bar.",
            "Which is basically finding a set of numbers that sum to one such that expires the center of mass.",
            "OK. And I represent X bar in terms of this barycentric coordinates."
        ],
        [
            "So what I do is I construct a sparse matrix W. With the iatros of this.",
            "Matrix is the barycentric coordinates of XI in the basis of its nearest neighbors.",
            "OK, and then I use the lowest eigenvectors of this matrix to embed.",
            "So.",
            "Some of you have already seen this probably, but many of you may not have and I'm just giving these to give you a sense of what it is that people were trying to do.",
            "At the turn of the century.",
            "OK, this is algorithm from 2000.",
            "And this is the intuition they had, and they came up with these procedures and actually.",
            "For the most part, you can't.",
            "Prove things correct about these particular algorithms, but you could ask, are there other algorithms that would do meaningful things for the same general kind of a question?"
        ],
        [
            "So there is an argument for why the LLE is somehow computing in some sense the Laplacian.",
            "I don't want to go through this.",
            "It's a little technical.",
            "But there is an argument in a paper Misha Belkin I wrote actually shortly after this algorithm was published in Science, which is giving some kind of heuristic argument for how Eli was somehow calculating.",
            "The trace of the Hessian, which actually is the Laplacian or maybe discovering the square of the Laplacian."
        ],
        [
            "So then let me discuss the next algorithm, which is the algorithm which is called Laplacian eigen Maps OK, and about Laplacian Eigen Maps.",
            "You can actually prove a few things.",
            "And it's related to a few other algorithms that have also been discussed later.",
            "So let me tell you the Laplacian Eigen Maps algorithm.",
            "And the Laplacian algorithm Maps algorithm works like this.",
            "You again build a nearest neighbor graph.",
            "OK, so this is a graph where every data point is identified with a vertex and you put an edge between two data points.",
            "If XI is close to XJ in some sense, so you measure the Euclidean distance between XI and XJ, which is the only distance you can measure.",
            "And if it is less than epsilon, you connect them and thus you build your nearest neighbor graph.",
            "OK."
        ],
        [
            "Anne.",
            "You make this graph awaited graph.",
            "So now you have a graph with N vertices.",
            "I have N data points.",
            "I have a graph with N vertices.",
            "Each vertex is identified with the data point and WIJ.",
            "The weight between the Iatan.",
            "The Jade vertex is given by that exponential function.",
            "Now the general function is actually quite important, because the correctness of this algorithm will depend on actually that.",
            "Functional form not precisely in that function, but on the decay of that function.",
            "OK.",
            "So there is one parameter T here.",
            "That you see.",
            "If T is very small.",
            "Then it penalizes heavily if the point Seccion next year far apart an if T is very large.",
            "It doesn't do that.",
            "So it actually is enforcing locality.",
            "So now what I've got is I've got N data points.",
            "I've built a graph with N vertices.",
            "And it's a weighted graph where between the IAT vertex and the jet vertex I put the weight.",
            "WIJ, it's symmetric wiw is a symmetric matrix, and notice that actually in the setting that we're in, it's a random matrix.",
            "Because the way this matrix is generated is the following.",
            "I get a bunch of points randomly sampled from this manifold.",
            "X1X2X3X4X5 and so on.",
            "So I get a random number of points.",
            "Each point is on the manifold.",
            "OK, I've got 10 points and now I've generated an N by N matrix which is a matrix which looks like this.",
            "It's a random matrix.",
            "So this particular matrix will have eigenvectors and eigenvalues.",
            "OK, and then there is the manifold and the central result which we will talk about later after we just.",
            "Intuitively understand this algorithm a little better.",
            "Is that by looking at the spectrum of this matrix and the eigenvalues and eigenvectors.",
            "Of this matrix essentially are something very closely related to this matrix.",
            "We can recover the eigenvalues and eigenfunctions of the Laplace Beltrami operator of the manifold from which the data was sampled.",
            "OK, so.",
            "Will make sense of that statement shortly."
        ],
        [
            "The algorithm looks like this.",
            "You look at a matrix called the Laplacian Matrix L, and that Laplacian matrix is equal to D -- W. W is as I just defined it.",
            "It's a symmetric matrix where WI J is what it is with those exponential weighted decay and D is the diagonal matrix.",
            "Where DI is simply the row sums of WIJ overall J. OK.",
            "So now you have this matrix L. It's actually immediate that it is symmetric because D is a diagonal matrix and W is a symmetric matrix.",
            "It also turns out to be positive semidefinite.",
            "Which is not immediately obvious, but it turns out to be true.",
            "And eigenvalues and eigenvectors of this matrix gives us.",
            "Is used for embedding OK that's in Laplacian Eigen Maps."
        ],
        [
            "There's another distance metric.",
            "That was also proposed.",
            "This is due to Steven Lafon and Raffi Koifman.",
            "Call the diffusion distance and that's very closely related to this, essentially.",
            "Very, very similar.",
            "So what you do is you think of the heat diffusion operator on this manifold HT, and we'll look at that a little bit, because the two main objects that really we will need to get a sense of are the diffusion of heat on the manifold.",
            "The so called heat, kernel, the Laplacian and eigenvectors eigenfunctions of the Laplacian.",
            "Now the heat kernel gives rise to what is called the heat operator, the diffusion operator.",
            "And the diffusion distance.",
            "Between two points is defined like this, OK?",
            "If you imagine an initial distribution of heat on the manifold which is.",
            "Pulse at X so it's Delta function at X.",
            "And if you imagine the initial distribution of heat, which is a Delta function at Y, then you can ask if heat flow happened on the manifold if there was diffusion along the manifold.",
            "And I started with an initial distribution concentrated on XI will get a distribution of heat and that distribution of heat is given by HT, the heat operator, the diffusion operator applied to Delta X.",
            "And if instead I took Delta Y and applied the heat operator to it, I would get HT applied to Delta Y, which is the distribution of heat.",
            "From an initial condition, which was a point mass at Y.",
            "So basically on this manifold I would like to define a distance.",
            "I take a point X. I start with an initial pulse of heat.",
            "At that point, X Ann.",
            "I allow heat to dissipate over the manifold, so the heat flows along the manifold.",
            "In the following the geometry of the manifold.",
            "If there are kinks and bottlenecks in the manifold, it'll slow down there in some directions which are very smooth and flat.",
            "It will speed up there, so the heat is flowing along the manifold and you end up with the distribution of heat after time T. From initial location X you end up with the distribution of heat after time T from an initial location Y, and you take the Euclidean distance between those two.",
            "OK, that's what is called the diffusion distance.",
            "What we will see is that this is essentially."
        ],
        [
            "Very closely related to the Laplacian.",
            "Becausw the.",
            "Diffusion of heat on the manifold is governed by the heat equation on the manifold in the heat equation on the manifold.",
            "Is actually given by Laplace.",
            "F is the partial derivative with respect to time.",
            "OK, so the way the diffusion Maps as opposed to Laplacian Eigen Maps works is that it looks at the eigenfunctions of the Laplacian an it embeds every point into a lower dimensional space using this particular map.",
            "OK, so it looks at the Laplace matrix.",
            "Looks at the eigen values and eigen vectors of the Laplace matrix.",
            "The eigenvalues are the Lambda one Lambda, two Lambda three and so on and the eigenvectors are F1F2F3 and so on.",
            "And given a point X, you map it like this and that gives you an embedding for every point X.",
            "It can be related to taking a random walk on this graph.",
            "Another two notions of randomness going on.",
            "So we have to keep our heads clear about that.",
            "You're taking a random walk on a graph.",
            "But the graph itself is a random graph, so you have a manifold.",
            "You randomly sample this manifold and get this random graph by connecting nearby points to each other, and now you do a random walk on this random graph.",
            "OK. And the essential question.",
            "Which now is resolved, is whether this walk on this.",
            "Random graph is close to the actual flow of heat on the underlying manifold.",
            "And the answer is more or less yes."
        ],
        [
            "So this let me give you a let me give you a justification.",
            "For the Laplacian Eigen Maps algorithm first.",
            "Remember the high level intuition from where this comes, and this is a very simple justification.",
            "The high level intuition from where this comes is I have a bunch of points sitting on a manifold.",
            "OK, and I would like to embed it into a lower dimensional space so I have a collection of points X one through XN an.",
            "I would map it to a lower dimensional space and so end up with a bunch of points Y one through YN, and in this case we are taking an extreme form of this.",
            "We are embedding our entire set of points onto the line.",
            "Into the lowest dimensional space that we can think of.",
            "OK, the one dimensional space so we map them into Y one through YN an.",
            "What Laplacian Eigen Maps is trying to do is is trying to preserve locality in the following sense that if XI and XJ were close to each other in the high dimensional space?",
            "Their image is why I and YJ should be close to each other in the lower dimensional space.",
            "OK.",
            "In other words, we are looking for a smooth map.",
            "Because that will actually guarantee that locality preservation.",
            "Not preservation, but this particular notion of it, an.",
            "Smooth Maps are going to be given by the eigenfunctions of the Laplacian.",
            "So what you want to do is you want to minimize this particular quadratic form.",
            "So notice that WIJ.",
            "Is large.",
            "If XI and XJ are close to each other.",
            "Right, so when WIJ is large XI and XJ's clothes are close to each other and this forces Y&YJ to be close to each other in that case.",
            "If WIG is small.",
            "That means Ixion next year.",
            "We're not necessarily close to either each other.",
            "Then we don't care what Huaiyin Ijr.",
            "We only want to ensure that if points were close to each other in the high dimensional space, they continue to remain close to each other in the low dimensional space.",
            "So this is.",
            "The argument."
        ],
        [
            "And here's a fundamental identity.",
            "This is just a few lines of linear algebra which.",
            "You're encouraged to skip or try to prove for yourself, but basically that quadratic form which I just wrote down.",
            "Why I minus YJ squared?",
            "WIJ ends up being just the same quadratic form is Y transpose LY, so minimizing that is equivalent to taking the eigenvectors.",
            "Of El.",
            "And that's how the eigenvectors of the Laplacian show up."
        ],
        [
            "So let me actually give you an A picture first before I will tell you a little bit about what this might correspond to on the underlying manifold.",
            "OK, the picture is this.",
            "Remember the picture of vertical bars and horizontal bars.",
            "OK, so we have the set of all pictures of vertical bars and we have the set of all pictures of horizontal bars and every picture of a vertical bar is a point in a high dimensional space and every picture of a horizontal bar is a point in high dimensional space, and so suppose I give you all these points.",
            "OK, I give you a whole bunch of points.",
            "Some of those correspond to pictures of vertical bars, and some of them correspond to pictures of horizontal bars and you don't know which is which.",
            "You just have a collection of points in a high dimensional space, and the only thing you know how to do is PCA.",
            "And you apply PCA.",
            "The picture on the right is the picture that you will see.",
            "OK. Another reason you see that picture is as we just discussed, the the set of all images of vertical bars are not a linear space.",
            "The set of all images of horizontal bars are not a linear space.",
            "In fact they are.",
            "2 dimensional manifolds in the higher dimensional space and so trying to linearly fit something through it leads to disaster.",
            "OK. On the other hand, if you applied Laplacian Eigen Maps to it.",
            "The picture in the middle is the picture that you will see.",
            "You see two connected components in your data that you will pick out and one connected component consists of all the blue curves and the other connected component blue dots and the other connected component consists of all the red dots.",
            "Red is not working on this particular projector, so I think you're going to see this as black dots.",
            "And.",
            "You'll see from Laplacian Eigen Maps that there are two sets of points.",
            "And these corresponds to the two sets, the vertical bars and the horizontal bars."
        ],
        [
            "OK, so I've given you an algorithm and now let me tell you a little bit about what roughly this corresponds to on the manifold.",
            "So what we're trying to achieve is the following.",
            "I have a collection of points on a manifold.",
            "OK, these points.",
            "This is an embedded manifold OK. And so locally geode esic, distances and Euclidean distances are close to each other, up to a certain order.",
            "And what I want is a smooth map from M2R.",
            "If I have a smooth map from M2R.",
            "This ensures that if two points were close to each other on the manifold, their images under F would also be close to each other.",
            "So.",
            "I think.",
            "It's.",
            "Close to the time when we should be taking a break.",
            "And."
        ],
        [
            "Maybe we take a break and I'll continue.",
            "Army let me say something for 10 minutes and then we'll take a break.",
            "OK so?"
        ],
        [
            "So suppose you wanted to actually find this a smooth map from M2R.",
            "You smooth map from M2R is equivalent, more or less.",
            "So can be formulated as this.",
            "Trying to find a function F that minimizes that quadratic form.",
            "And that quadratic form is roughly approximated by this quadratic form."
        ],
        [
            "And here is a standard fact.",
            "If you look at the Laplacian on.",
            "Him that is actually equal to F times Laplace F on M and that is essentially what is called Stokes theorem.",
            "Anne.",
            "On the graph of the function is more than the graph.",
            "The quadratic form on the left summation, WI, GFI, minus FJ squared is equal to F transpose LF, so finding smooth functions on the manifold require you to essentially look at the eigen functions of the Laplace Beltrami operator on the manifold.",
            "Finding smooth functions on the graph that means a function is not changing much from one vertex to another.",
            "That's what the summation, WI, GFI, minus FJ squared is doing.",
            "Leads you to trying to find eigenvectors of the Laplacian on the graph.",
            "And the main central question is going to be are those two formally connected in the sense that does one converge to the other and we will see that that is true.",
            "That's all I wanted to see.",
            "And then maybe we take.",
            "Oh 15 minute break.",
            "OK, so we'll begin very soon, and what we've decided.",
            "Given that things are taking a little longer than we expected is, I will speak for 10 minutes.",
            "I just want to communicate one point to you.",
            "And I'll speak.",
            "Hopefully, 10 minutes will be enough to communicate this point.",
            "Which is what is the relationship between the graph Laplacian and the manifold Laplacian?",
            "Does one converge to the other?",
            "And the answer to that is yes.",
            "So I just want to make this one point.",
            "And after that Misha will take over an.",
            "It is now.",
            "1033 so.",
            "Hopefully I'll stop speaking bye.",
            "And in 10 minutes.",
            "And so somewhere around 10:40, ten 45 Misha begins and goes to the end of our allotted time.",
            "That's the plan.",
            "OK, and he'll tell you about some other aspects of this framework, especially as it applies to clustering and semi supervised learning and so on.",
            "And.",
            "So now in the next 10 minutes I just want to communicate to you one basic theoretical result.",
            "OK, and that theoretical result now can be, I think simply stated."
        ],
        [
            "So we understand now, roughly speaking, what the manifold Laplacian is.",
            "We see why it arises if we want dimensionality reduction if we require smooth Maps, we see that the eigenfunctions of the Laplacian will give us functions that are geometrically adapted to the geometer, adapted to the geometry of the manifold.",
            "And I just want to workout an exact formula for the case of the circle.",
            "So that you see very clearly that classical Fourier series is the eigen functions of the Laplacian where the manifold in question is just the circle."
        ],
        [
            "OK.",
            "So the first thing is we know what the given an arbitrary Romanian manifold.",
            "We know what the Laplace Beltrami operator is on this manifold, and you can look at the following Eigen system Laplace F is oh that's incorrect.",
            "So Laplace F is equal to Lambda.",
            "F and the eigen functions are fees and the eigenvalues are Lambda eyes, so that should be Laplace fee.",
            "I is equal to Lambda IVI.",
            "OK, and these fees form an orthonormal basis for L2 of M. And the eigenvalues characterize the smoothness of these fees, so the fees are ordered, roughly speaking, by smoothness to the smoothest ones come before and and when I say smoothness, I don't mean number of derivatives, they're all see Infinity.",
            "Typically what I mean is actually this statement, which is at the bottom the norm of.",
            "The gradient."
        ],
        [
            "So let's just work one through 1 example just so that we see this clearly.",
            "What this means.",
            "So let's work on the case of circle.",
            "So the manifold is the circle and now consider functions.",
            "Whose domain is the circle?",
            "So these are functions.",
            "On the circle now the set of functions on the circle are in fact the periodic functions.",
            "And we already know that the periodic functions admit a Fourier series expansion and can be written in terms of sines and cosines.",
            "So now we are asking the question given the circle.",
            "What is the Laplace operator?",
            "On the circle.",
            "So think of the circle.",
            "What is the tangent?",
            "The circle is a 1 dimensional manifold.",
            "What is the tangent space to any point on the circle?",
            "The tangent space to any point on the circle is a 1 dimensional space.",
            "It's essentially a line.",
            "OK. And so, given a circle given a function defined on the circle, I can write this function as a function of key to.",
            "In this case I've written it as a function of T. OK, and I what I want to do is differentiate the function twice.",
            "On the tangent space, and so differentiating the function U twice along the tangent spaces, just differentiating you twice with respect to T and the eigenvalue problem.",
            "Is this du Laplace U is exactly that formula.",
            "And Laplace U is equal to Lambda.",
            "U is the eigensystem that we are interested in.",
            "And this many of you have seen, presumably.",
            "In calculus, at some point I don't know.",
            "The eigenvalues are N squared and the eigenfunctions are sign in of T and cosine NFT so the eigenspaces have dimensionality to the multiplicity of the eigenvalues is 2.",
            "And you see that the Laplace F is equal to Lambda F. The eigenfunctions of the Laplacian on the circle are sines and cosines, and sines and cosines are exactly the right sort of functions that are adapted to the geometry of the circle.",
            "So in a high level.",
            "In a picture that I want you to think in your head is this.",
            "If all your data points lived on a circle embedded in a million dimensional space.",
            "OK, all your data lived on a circle embedded in a million dimensional space.",
            "What kinds of functions would you like to use to build classifiers?",
            "And it seems natural to think that the right kind of functions would be made up of sines and cosines, because all the data lives in this circle and the wrong kind of functions would be.",
            "Linear projections.",
            "Because the ambient coordinate system has no natural relationship to the geometry of the data which is changing along the circle.",
            "So if I knew the manifold on which the data lived.",
            "The eigenfunctions of the Laplace operator would give me a set of basis functions that are adapted in some way to the geometry of the manifold in the same way in which sines and cosines are adapted to the geometry of the circle.",
            "And.",
            "Those are the functions I will use to build classifiers.",
            "Now I don't have the manifold.",
            "I don't So what I have is a collection of points on the manifold.",
            "And I can build a graph out of these points and I can look at the Laplacian on this graph, which is an operator on functions defined on the vertex set of the graph.",
            "OK. And now I can look at the eigenvalues and eigenvectors of this.",
            "And from this reconstruct the eigenvalues and eigenfunctions of the Laplacian."
        ],
        [
            "So."
        ],
        [
            "Add a number of theorems.",
            "And let me just write this on the board.",
            "OK, so this is the basic theoretical result that justifies why this procedure might be well founded in more than a heuristic.",
            "OK, so I have here a manifold MI have Laplace F is equal to Lambda F. This is the eigensystem that I'm interested in that gives me eigenvalues Lambda eyes, which actually are in increasing order on, and it gives me eigenfunctions afei.",
            "It may have eigenspaces if their multiplicity and so on, but say it gives me eigen values and eigen spaces.",
            "This is my manifold.",
            "I get a bunch of points sampled on this manifold.",
            "OK, so I get X1 through XN.",
            "OK, where each XI is a randomly drawn point sitting on this manifold.",
            "OK, and this manifold itself is sitting.",
            "In Rd, so each of these XX one through XN is in M, which is in Rd.",
            "Right?",
            "Now I've got a graph G is equal to VE.",
            "The vertex set the size of the vertex set is equal to N. Because there is one vertex per datapoint.",
            "OK, I can look at functions from V2R.",
            "Functions defined on the vertex set of the graph.",
            "I can also look at functions from M to R, which are these functions.",
            "I have an operator on such functions which is given by the Laplace Beltrami operator.",
            "I look at these kinds of functions functions from the vertex set to R and I have an operator defined on these functions which is given by the graph Laplacian to any such function.",
            "I can apply this.",
            "Such a function is just a vector.",
            "OK, it's a collection of.",
            "One number per vertex, so this is I.",
            "Identified with R2 the V. OK, I can look at LF.",
            "OK. And here is.",
            "The following random matrix result.",
            "Which is.",
            "And this.",
            "Says the following.",
            "Take X one through XN in M. In Rd.",
            "Consider the matrix L, which is equal to D -- W. Where there is a parameter T. OK, so WIJ is equal to E to the power of minus XI minus XJ squared over 40.",
            "And this is I think, 4 by T to the D by 2 * T. Something like this, OK?",
            "So there's one parameter T, and this is a random matrix Lt, right?",
            "I sample X one through XN uniformly safe from him to keep things simple.",
            "And I get this random matrix and I look at the eigenvalues and eigenvectors of this matrix and the theorem says that let Lambda K be the Kate eigenvalue of this matrix.",
            "There's this random matrix LK.",
            "This is going to be a function of T, an A function of N. Because N is there and number of randomly drawn points that I've got anti is this parameter that Lambda KTN is going to converge to.",
            "Lambda K. As T goes to zero, an N goes to Infinity, so as the number of data points goes to Infinity and I let T this parameter, which is actually the localization parameter, go to zero at suitably chosen rate.",
            "The spectrum of this random matrix is converging to the spectrum of the Laplacian.",
            "And in a different sense, the eigenvectors of this matrix are converging to the eigenfunctions of the Laplacian and the rate at which all of this happens, and this is important.",
            "Is 1 / N. Do the 1 / D. Anet does not depend.",
            "On capital D. So the rates.",
            "Do not depend upon the ambient dimension at all, but depends only on the intrinsic low dimension.",
            "So in other words, if all the data lives in a high dimensional space but sits on a low dimensional manifold, you might be fooled into thinking Oh my God, I'm confronted with all this high dimensional data.",
            "I have the curse of dimensionality.",
            "But in fact becausw there is only.",
            "A small number of degrees of freedom that have generated the data.",
            "Roughly little Di degrees of freedom.",
            "All the rates will be governed by little D. And that's the sense in which you can make precise the notion of beating the curse of dimensionality if nature is.",
            "Helpful.",
            "OK, and this is also the sense in which you can now get.",
            "Geometric.",
            "Lee structured functions, with which you can do harmonic analysis on the manifold with which you can build classifiers with which you can do dimensionality reduction and all sorts of things.",
            "Because this is an empirical object, the graph which you have in your hands and everything you want to recover about the manifold you recover from this graph.",
            "So at this point I'll just stop, and that's the only really remark.",
            "I want to make about.",
            "About the use of the graph Laplacian and its relationship to the manifold Laplacian.",
            "At this point, Michelle will just carry on to elaborate more on the theme of the Laplacian and so on."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're almost ready to begin, so welcome to the third tutorial review session of this.",
                    "label": 0
                },
                {
                    "sent": "Workshop in summer school.",
                    "label": 0
                },
                {
                    "sent": "As you know, we have these sort of tutorial reviews running all through the school for about 10 days or so, so.",
                    "label": 0
                },
                {
                    "sent": "What we're going to talk about today is.",
                    "label": 0
                },
                {
                    "sent": "Is about geometric methods and manifold learning.",
                    "label": 1
                },
                {
                    "sent": "So what Misha, Belkin and I are going to do is split presentation between the two of us and hopefully what we would be able to communicate to you is.",
                    "label": 0
                },
                {
                    "sent": "Some idea about class of methods that.",
                    "label": 0
                },
                {
                    "sent": "Have received some attention over the last decade or so, and a perspective on machine learning that seems to have emerged over the last decade.",
                    "label": 0
                },
                {
                    "sent": "2 An It's a very sort of geometrically oriented point of view with which one can approach questions in pattern recognition, machine learning, and sometimes even a numerical computation.",
                    "label": 0
                },
                {
                    "sent": "And hopefully you will get a sense of the way in which ideas from probability and statistics ideas from computer science, algorithms, and especially graph theory an ideas from.",
                    "label": 0
                },
                {
                    "sent": "Geometry and topology will come together in some sort of natural way in this setting, so the logistics of this is like this.",
                    "label": 0
                },
                {
                    "sent": "I'll speak for about an hour, no more than an hour and 15 minutes, after which we will take a break for about 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "Misha will then speak for about an hour after that.",
                    "label": 0
                },
                {
                    "sent": "Maybe an hour and 15 minutes will take a second break, and then we'll have a closing session for about half an hour to wrap up odds and ends and things of that sort.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Context.",
                    "label": 0
                },
                {
                    "sent": "In which all of this really developed is really the context of data analysis and machine learning problems in very, very high dimensions.",
                    "label": 0
                },
                {
                    "sent": "So increasingly we live in a world in which we are faced all the time by learning problems in very, very high dimensional spaces and genetics and neuroscience in image analysis, speech analysis or other problems that we might have worked on and very.",
                    "label": 0
                },
                {
                    "sent": "A basic and fundamental sort of question that faces all of us is to try and understand under what circumstances it might be possible to make effective inferences in high dimensional spaces without running into the so called curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "OK, so as far as I see it.",
                    "label": 0
                },
                {
                    "sent": "People have of course known about the curse of dimensionality for a long time, and there are at least three different points of view from which one can approach this central issue.",
                    "label": 0
                },
                {
                    "sent": "The 1st and the most classical of these is the point of view of smoothness.",
                    "label": 0
                },
                {
                    "sent": "So let me clarify.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It has been known for a long time that suppose you're trying to learn a function an it's a function of several variables.",
                    "label": 0
                },
                {
                    "sent": "And it has say.",
                    "label": 0
                },
                {
                    "sent": "S derivatives OK, so it belongs to what is called a Sobolev space HSA.",
                    "label": 0
                },
                {
                    "sent": "So it's a function.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should use the board a little bit.",
                    "label": 0
                },
                {
                    "sent": "Actually, if I just shout, can you hear me or should I use this as I have to handle?",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I'll use this Mike.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is better, OK?",
                    "label": 0
                },
                {
                    "sent": "So I have a function.",
                    "label": 0
                },
                {
                    "sent": "From Rd 2R, so it's a function of the variables and it has S derivatives in L2, so F. Belongs to HS say something like this, then the rates of convergence look like this.",
                    "label": 0
                },
                {
                    "sent": "This is the number of examples you need to learn a function OK?",
                    "label": 0
                },
                {
                    "sent": "And I think.",
                    "label": 0
                },
                {
                    "sent": "It's actually something like this to S + D or something like that.",
                    "label": 0
                },
                {
                    "sent": "OK, not S / D, but approximately S / D. So.",
                    "label": 0
                },
                {
                    "sent": "If you look at that rate.",
                    "label": 0
                },
                {
                    "sent": "OK, you see that if S, the smoothness is of order D. Then the rate does not depend upon the dimension.",
                    "label": 0
                },
                {
                    "sent": "And therefore there is no curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is of course known for a long time and so people realize that if the function you're trying to learn is a function of several variables, it's a function in a high dimensional spaces, but it's very smooth and the smoothness.",
                    "label": 0
                },
                {
                    "sent": "Is, in a particular sense, then there is no curse of dimensionality and this insight or this observation has an algorithmic reflection.",
                    "label": 0
                },
                {
                    "sent": "So based on this insight, people have realized for a long time that perhaps it's a good idea to try and learn smooth functions.",
                    "label": 0
                },
                {
                    "sent": "Perhaps it's dry.",
                    "label": 0
                },
                {
                    "sent": "It's a good idea to try and fit the smoothest function that you can to the data, and so if you take this point of view, the kind of algorithmic consequence of that are things like splines, kernel methods, reproducing kernel Hilbert spaces, where the kernel K has certain smoothness properties.",
                    "label": 1
                },
                {
                    "sent": "L2 regularization in such spaces, and so on.",
                    "label": 0
                },
                {
                    "sent": "And these are the kinds of algorithms that arise out of this basic insight.",
                    "label": 0
                },
                {
                    "sent": "Now there's a second point of view.",
                    "label": 0
                },
                {
                    "sent": "And the second point of view is the point of view, not of smoothness in the classical sense, but the point of view of sparsity.",
                    "label": 0
                },
                {
                    "sent": "So smoothness is a has been known for awhile.",
                    "label": 0
                },
                {
                    "sent": "I think at least since the 60s or so and sparsity the point of view is this that people realize that maybe the function you're trying to learn is not smooth in any classical sense.",
                    "label": 0
                },
                {
                    "sent": "But maybe it can be represented.",
                    "label": 0
                },
                {
                    "sent": "In a sparse way, in terms of some basis functions.",
                    "label": 0
                },
                {
                    "sent": "As a sparse combination of some basis functions an in this setting two, it might be possible to learn effectively.",
                    "label": 0
                },
                {
                    "sent": "OK, in other words, this is the insight that the function depends on.",
                    "label": 0
                },
                {
                    "sent": "A few relevant features perhaps.",
                    "label": 0
                },
                {
                    "sent": "And this has an algorithm inconsequence too.",
                    "label": 0
                },
                {
                    "sent": "If you take this point of view an you think of things like wavelets like L1 regularization, lasso, compressed sensing and these are algorithmic developments that somehow emerge out of this realization, that sparsity is a good thing and can be exploited.",
                    "label": 1
                },
                {
                    "sent": "So that's the second point of view.",
                    "label": 0
                },
                {
                    "sent": "And the Third Point of view, which has really the most recent of these, is the point of view of geometry.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what I'll try and explain what that means.",
                    "label": 0
                },
                {
                    "sent": "And this point of view also has an algorithmic consequence.",
                    "label": 0
                },
                {
                    "sent": "So just like smoothness makes you think of splines and kernel methods and L2 regularization and sparsity makes you think of L1 and compressed sensing and lasso and so on.",
                    "label": 0
                },
                {
                    "sent": "When you take the geometric point of view, the algorithms that emerge out of trying to exploit this POV make use of the notion of graph simplicial complexes.",
                    "label": 0
                },
                {
                    "sent": "Laplacians, diffusions, and so on, and we will see these objects over the next hour or so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is?",
                    "label": 0
                },
                {
                    "sent": "The geometric POV, so this is somehow the geometric thesis of data analysis in very high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "OK, the central dogma is this.",
                    "label": 1
                },
                {
                    "sent": "It's an argument in two parts, so the first part of the argument says that look in very very high dimensional spaces.",
                    "label": 1
                },
                {
                    "sent": "The data is not going to be distributed uniformly.",
                    "label": 0
                },
                {
                    "sent": "That's absurd.",
                    "label": 0
                },
                {
                    "sent": "So in fact the data will not be distributed uniform.",
                    "label": 0
                },
                {
                    "sent": "The distribution of the data will have some shape.",
                    "label": 1
                },
                {
                    "sent": "If there is some shape, perhaps there is some geometry to be done if you want to understand this shape, maybe it concentrates around certain kinds of structures.",
                    "label": 0
                },
                {
                    "sent": "Maybe these structures are low dimensional and this is going to be general fact of natural datasets that natural datasets are going to have particular shapes in the high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "And that's based on the intuition that natural data tends to be generated by systems, maybe physical systems or non physical systems that have few underlying degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "So that's the first part of the argument.",
                    "label": 0
                },
                {
                    "sent": "So the data has some shape and the second part of the argument says that if the data has some shape, perhaps you can exploit the shape of the data or the geometry of the data in some sense to define suitable classes of functions with which to operate that are geometrically motivated and adapted to the shape of the data and develop representations and develop algorithms that are adapted to the geometry of the data.",
                    "label": 0
                },
                {
                    "sent": "And if this is done wisely, this might allow us to exploit the the geometry of the data to enable efficient learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One setting in which one can.",
                    "label": 0
                },
                {
                    "sent": "Explore this geometric thesis with some clarity.",
                    "label": 0
                },
                {
                    "sent": "Anne understand what the consequences of this thesis are.",
                    "label": 0
                },
                {
                    "sent": "Is the setting of the manifold is a setting of manifold learning?",
                    "label": 0
                },
                {
                    "sent": "So manifold learning is not a single problem.",
                    "label": 0
                },
                {
                    "sent": "But rather it's a collection of problems unified by some common assumption, and that common assumption is this that the data lives.",
                    "label": 0
                },
                {
                    "sent": "On or near some low dimensional manifold embedded in this high dimensional space and we'll talk a little bit about how one can relax this an generalize this further.",
                    "label": 0
                },
                {
                    "sent": "But this is our first attempt or the first attempt that community has made to try and make sense of the notion that the data might have some geometry that is far from uniform.",
                    "label": 0
                },
                {
                    "sent": "And then try and understand.",
                    "label": 0
                },
                {
                    "sent": "What the consequences of that fact might be in terms of leading us towards certain geometrically motivated classes of functions or leading us towards certain geometrically oriented learning algorithms so?",
                    "label": 0
                },
                {
                    "sent": "This is manifold learning.",
                    "label": 0
                },
                {
                    "sent": "You have to learn in a high dimensional space space all the data lives on or near some low demand.",
                    "label": 0
                },
                {
                    "sent": "Typically low dimensional manifold embedded in this high dimensional space and the Canonical learning problem is always this.",
                    "label": 0
                },
                {
                    "sent": "You want to learn a function.",
                    "label": 0
                },
                {
                    "sent": "OK, but in this case the natural domain of the function is actually the manifold on which all the data lives.",
                    "label": 0
                },
                {
                    "sent": "So you have to learn a function whose domain is this manifold an whose range might be a finite set.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to do clustering or classification, or it may be the reals if you're trying to do, say, regression or dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So what's somewhat peculiar about this setting actually, which leads to some non trivial questions is this.",
                    "label": 0
                },
                {
                    "sent": "That although all the data lives near some manifold, for the most part, we don't know what this manifold is.",
                    "label": 0
                },
                {
                    "sent": "So we will somehow have to discover this manifold or discover a geometrically motivated class of functions that is adapted to this unknown manifold.",
                    "label": 0
                },
                {
                    "sent": "Without knowing what this manifold is.",
                    "label": 0
                },
                {
                    "sent": "But knowing that all the data lives on or near some manifold.",
                    "label": 0
                },
                {
                    "sent": "And that leads to the following kind of somewhat basic algorithmic questions.",
                    "label": 0
                },
                {
                    "sent": "Suppose.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "Compact Riemannian submanifold of Euclidean space, say.",
                    "label": 0
                },
                {
                    "sent": "And suppose I sample it and give you a collection of points that sit on this manifold.",
                    "label": 0
                },
                {
                    "sent": "So this is a collection of points living in some very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Sitting on this manifold.",
                    "label": 0
                },
                {
                    "sent": "What geometry got topological properties of this manifold?",
                    "label": 0
                },
                {
                    "sent": "Can you in fact learn?",
                    "label": 0
                },
                {
                    "sent": "From these randomly drawn points.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not a priori obvious that you can learn any geometry, or at least any nontrivial geometric or topological properties of this fixed underlying manifold from randomly drawn points presented to you.",
                    "label": 0
                },
                {
                    "sent": "So all the data is living on this manifold.",
                    "label": 0
                },
                {
                    "sent": "You're seeing this cloud of points in a very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "All you can compute are distances in this very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Because these are points in this high dimensional space in this high dimensional space is Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "And you can't see geodesics.",
                    "label": 0
                },
                {
                    "sent": "You can't see, you can't see anything.",
                    "label": 0
                },
                {
                    "sent": "All you see is a cloud of points.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you think for awhile about the simplest kind of learning problem I have.",
                    "label": 0
                },
                {
                    "sent": "Coin with bias Pi.",
                    "label": 0
                },
                {
                    "sent": "Toss it a few times and I tell you what the outcomes are.",
                    "label": 0
                },
                {
                    "sent": "Can you infer what this underlying fixed unknown biases?",
                    "label": 0
                },
                {
                    "sent": "And the answer is of course yes.",
                    "label": 0
                },
                {
                    "sent": "You can infer the bias of a coin by tossing it a few times and counting basically how many times it came up heads and how many times it came up.",
                    "label": 0
                },
                {
                    "sent": "Tails you can estimate lots of things we know how to do that from statistics, and so now we're asking a kind of statistical question about geometry.",
                    "label": 0
                },
                {
                    "sent": "There's an underlying manifold.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of points.",
                    "label": 0
                },
                {
                    "sent": "What can you learn about this manifold?",
                    "label": 1
                },
                {
                    "sent": "If I gave you the points and nothing else.",
                    "label": 0
                },
                {
                    "sent": "What can you infer?",
                    "label": 0
                },
                {
                    "sent": "And so we'll see answers to this kind of a question.",
                    "label": 0
                },
                {
                    "sent": "And the reason this kind of question arises is if you take the view that in high dimensional spaces the data is living near some manifold and the structure of this manifold can be exploited.",
                    "label": 0
                },
                {
                    "sent": "Then you need to at least understand what can you learn about this manifold in the 1st place before you begin to exploit its structure.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the reason this is peculiar if you're trying to do regression, say, is that we are in the setting where we are trying to learn both the function and its domain simultaneously.",
                    "label": 0
                },
                {
                    "sent": "The natural domain of this function is this manifold.",
                    "label": 0
                },
                {
                    "sent": "And you might be given XY pairs.",
                    "label": 0
                },
                {
                    "sent": "Where the X is all live on this manifold and the wise are what they are and you're trying to learn a function F from M to R. And you're trying to learn both M and this function simultaneously.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now.",
                    "label": 0
                },
                {
                    "sent": "One can give a formal justification for this in many different ways, and maybe we'll go through some of these examples in in a little while, but you can.",
                    "label": 0
                },
                {
                    "sent": "Look at for example the given argument for why the set of speech sounds might live on or near some manifold, or why it might be a reasonable model for certain classes of images, and there has been some work along this, but it is not very easy all the time to give a formal or physical justification from first principles as to why the data would live on a manifold OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The way to view this really is to say that.",
                    "label": 0
                },
                {
                    "sent": "Maybe I can't give you a formal justification for why the data is lives on a manifold, but maybe I will use this as a modeling device because I know that the data is not going to be distributed uniformly, but will have some shape.",
                    "label": 0
                },
                {
                    "sent": "And so it will allow me to define maybe a class of geometrically oriented probability distributions.",
                    "label": 0
                },
                {
                    "sent": "These are what I will call the manifold plus noise probability distributions.",
                    "label": 0
                },
                {
                    "sent": "That will allow us to reason about.",
                    "label": 0
                },
                {
                    "sent": "Nonuniform data in very high dimensional space is now the simplest such model.",
                    "label": 0
                },
                {
                    "sent": "The manifold plus noise model.",
                    "label": 0
                },
                {
                    "sent": "The simplest such model is actually the mixture of Gaussians, although we don't think of it as such.",
                    "label": 0
                },
                {
                    "sent": "So imagine you had a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And if you go and talk to a statistician, of course they've studied this for years.",
                    "label": 0
                },
                {
                    "sent": "They'll tell you this is exactly what it is.",
                    "label": 0
                },
                {
                    "sent": "A mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "But now if you take a. Topological view are a geometric view of this.",
                    "label": 0
                },
                {
                    "sent": "You can view this probability distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, which we've studied for many years as essentially data sampled from this distribution is data sampled from a manifold plus noise and the manifold here is kind of a degenerate manifold.",
                    "label": 0
                },
                {
                    "sent": "Which is a 0 dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Would say K connected components.",
                    "label": 0
                },
                {
                    "sent": "So you have K mixtures.",
                    "label": 0
                },
                {
                    "sent": "Each of these is a point.",
                    "label": 0
                },
                {
                    "sent": "OK. And there is noise in the normal direction.",
                    "label": 0
                },
                {
                    "sent": "Gaussian noise in the normal direction.",
                    "label": 0
                },
                {
                    "sent": "So imagine this setting of manifold plus noise OK.",
                    "label": 0
                },
                {
                    "sent": "There's an underlying manifold which is 0 dimensional, made up of K connected components, which are these points OK, these points may be identified with the centers of these Gaussians and then there is noise in the normal direction.",
                    "label": 0
                },
                {
                    "sent": "And if I sample this manifold plus noise probability distribution, I will essentially get the same distribution of data as I would if I sampled a mixture of Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So if I sample this manifold plus noise, this is what I would see a bunch of points sitting like this.",
                    "label": 0
                },
                {
                    "sent": "OK. And from this.",
                    "label": 0
                },
                {
                    "sent": "I might like to uncover.",
                    "label": 0
                },
                {
                    "sent": "A very simple topological invariant I might want to find out how many connected components does my underlying manifold have.",
                    "label": 0
                },
                {
                    "sent": "That's the same question as I give you points on mixture of K Gaussians.",
                    "label": 0
                },
                {
                    "sent": "Can you tell me what K is?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But a significant generalization of this would be to say the data doesn't have to be from a mixture of Gaussians, the data code.",
                    "label": 0
                },
                {
                    "sent": "For example, look like this.",
                    "label": 0
                },
                {
                    "sent": "And all this data lives close to a manifold, but this manifold is not a single point.",
                    "label": 0
                },
                {
                    "sent": "But rather, this manifold seems to be a circle.",
                    "label": 0
                },
                {
                    "sent": "So there's a whole class of such geometrically oriented probability distributions which are used then to model data in very high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Mom.",
                    "label": 0
                },
                {
                    "sent": "That's sort of the manifold plus noise point of view is just a modeling technique for data and very high dimensional spaces, and you might just throw away any formal justification to say why data would actually live near a manifold.",
                    "label": 0
                },
                {
                    "sent": "Rather use the manifold in the same way as you use a mixture of Gaussians as a generic nonlinear nonparametric method to approximate the probability distribution in a very high dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is really the take home message of all of this.",
                    "label": 1
                },
                {
                    "sent": "What we will see today is really.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear nonparametric method to learn in very high dimensional spaces where geometry plays a very big role in guiding our intuitions.",
                    "label": 1
                },
                {
                    "sent": "And the starting role of all of this is really the heroes of the story.",
                    "label": 0
                },
                {
                    "sent": "Are these two characters that are plus Beltrami operator on the manifold and the heat kernel on this manifold?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you a little bit about.",
                    "label": 0
                },
                {
                    "sent": "These two objects and what their consequences are for the design of learning algorithms and other other things of that nature.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A point I wanted to make an it's a good point to actually begin.",
                    "label": 0
                },
                {
                    "sent": "This whole story is to go back to a very classical method that people have known for a long time.",
                    "label": 0
                },
                {
                    "sent": "Maybe 100 years?",
                    "label": 0
                },
                {
                    "sent": "Which is principle components analysis.",
                    "label": 1
                },
                {
                    "sent": "So if I give you a lot of high dimensional data, the instinct of most people is to take this data, reduce it to a lower dimensional space by doing something like principle components analysis, which is a widely used technique OK, and to try and recover some low dimensional structure in the data.",
                    "label": 0
                },
                {
                    "sent": "So what is principle components analysis look like?",
                    "label": 0
                },
                {
                    "sent": "Most of you have seen this in some form.",
                    "label": 0
                },
                {
                    "sent": "So given a bunch of data X1 through XN sitting in a high dimensional space, you want to project it.",
                    "label": 0
                },
                {
                    "sent": "You want to say projected into a 1 dimensional space, just R and find numbers Y one through YN.",
                    "label": 0
                },
                {
                    "sent": "Each Yi is just a projection of XI.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you want to find.",
                    "label": 0
                },
                {
                    "sent": "Y1 through YN to represent your high dimensional data X1 through XN.",
                    "label": 0
                },
                {
                    "sent": "And you want to find the projection.",
                    "label": 0
                },
                {
                    "sent": "That maximizes the variance of the data.",
                    "label": 0
                },
                {
                    "sent": "Here I've assumed that all the excise have mean zero and so automatically the wise have mean zero, so that the variance has this particular formula which is the sum of the squares.",
                    "label": 0
                },
                {
                    "sent": "I mean you actually have to mean subtract 2 for the correct definition of variance, but I've just assumed the mean is 0 here.",
                    "label": 0
                },
                {
                    "sent": "So you want to find that W that maximizes the variance, and it's fairly easy to see that the sum of the squares of the wise ends up being this particular quadratic form in W, so you want to find the W that maximizes this particular quadratic form, and that gives rise to a rally rich like question and then leads to finding the leading eigenvector.",
                    "label": 0
                },
                {
                    "sent": "Of the data covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So I'm sure all of you have seen this.",
                    "label": 0
                },
                {
                    "sent": "There is also something else which is a way to interpret principle components analysis.",
                    "label": 0
                },
                {
                    "sent": "So what principle components analysis is doing?",
                    "label": 0
                },
                {
                    "sent": "Also in some sense is fitting.",
                    "label": 0
                },
                {
                    "sent": "The best.",
                    "label": 0
                },
                {
                    "sent": "Linear subspace to the data of a certain rank of a certain desired.",
                    "label": 0
                },
                {
                    "sent": "Dimension, so let me just write that.",
                    "label": 0
                },
                {
                    "sent": "So you have all this data X1 through XN.",
                    "label": 0
                },
                {
                    "sent": "Consider some subspace H. Consider the projection.",
                    "label": 0
                },
                {
                    "sent": "Onto age that toggle projection onto age of XI.",
                    "label": 0
                },
                {
                    "sent": "And consider this least squares.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "And I want to minimize this.",
                    "label": 0
                },
                {
                    "sent": "Overall, H. This is also essentially the same problem as principle components.",
                    "label": 0
                },
                {
                    "sent": "OK, So what principle components is doing is trying to essentially fit in this least squares sense.",
                    "label": 0
                },
                {
                    "sent": "A linear manifold to the data.",
                    "label": 0
                },
                {
                    "sent": "So this is very classical, widely used.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You could think of it also just like the mixture of Gaussians is very classical and widely used and is the simplest instantiation of a.",
                    "label": 0
                },
                {
                    "sent": "Manifold.",
                    "label": 0
                },
                {
                    "sent": "Structured probability distribution.",
                    "label": 0
                },
                {
                    "sent": "PCA, which is another classical example and widely used, can be thought of.",
                    "label": 0
                },
                {
                    "sent": "As the simplest instantiation of fitting a manifold today to where this manifold is just.",
                    "label": 0
                },
                {
                    "sent": "Linear space.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what was what is the more general manifold model say?",
                    "label": 0
                },
                {
                    "sent": "Suppose the data does not lie on or near some linear space at all.",
                    "label": 1
                },
                {
                    "sent": "Suppose actually the distribution of the data looks like this.",
                    "label": 0
                },
                {
                    "sent": "OK, where you can clearly see that the data seems to lie on some set, which seems to have only one degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "And yet, it's not obvious that it lies in some linear space.",
                    "label": 0
                },
                {
                    "sent": "It's not doesn't lie in some 1 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So what happens if the data looks like this?",
                    "label": 0
                },
                {
                    "sent": "Can you actually develop some scheme where you could unravel the structure of the data?",
                    "label": 1
                },
                {
                    "sent": "Realize that there is 1 degree of freedom in the data.",
                    "label": 0
                },
                {
                    "sent": "Represent the data appropriately without trying to either fit some linear subspace to something that looks like this fit a line through this curve.",
                    "label": 0
                },
                {
                    "sent": "Or anything else you might think of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me give you an acoustic example which is remarkable she.",
                    "label": 1
                },
                {
                    "sent": "Ends up with a distribution of data that looks more or less like what I just told you.",
                    "label": 0
                },
                {
                    "sent": "OK, because the picture that I put up might seem artificial.",
                    "label": 0
                },
                {
                    "sent": "It might seem that I just made it up to make some abstract point, so let me give you an actual example of data that's generated by a very simple physical system.",
                    "label": 0
                },
                {
                    "sent": "OK, and here's the physical system.",
                    "label": 0
                },
                {
                    "sent": "It's an acoustic tube of length L. And I blow puffs of air at one end of the tube, U of T is the excitation of this tube.",
                    "label": 0
                },
                {
                    "sent": "So I blow puffs of air at one end of this tube and a sound comes out from the other side of the tube and that sound is SFT.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you have to use the excitation.",
                    "label": 0
                },
                {
                    "sent": "The puffs of air I blow into the tube, and SFT is the sound.",
                    "label": 0
                },
                {
                    "sent": "The sound that this still makes both UFT an SFT are functions.",
                    "label": 0
                },
                {
                    "sent": "They live in some function spaces, so let's assume for simplicity that these are actually periodic functions, so they have Fourier series expansions.",
                    "label": 0
                },
                {
                    "sent": "They live in little L2.",
                    "label": 0
                },
                {
                    "sent": "And let's play the following game.",
                    "label": 0
                },
                {
                    "sent": "Every day you come to me and you collect the sound that I make for you on that particular day.",
                    "label": 0
                },
                {
                    "sent": "OK, you don't know what I'm doing.",
                    "label": 0
                },
                {
                    "sent": "I'm just generating sounds for you every day you come to me come to me and you collect the sound.",
                    "label": 0
                },
                {
                    "sent": "The sound as we just discussed lives in little L2 so the sound lives in an infinite dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So every day you go back with an infinite dimensional data point.",
                    "label": 0
                },
                {
                    "sent": "But what I'm doing from day to day?",
                    "label": 0
                },
                {
                    "sent": "OK, unknown to you is I'm actually changing 1 degree of frayed freedom in the physical system that is generating the data for you.",
                    "label": 0
                },
                {
                    "sent": "What I'm doing is I'm just changing the length L of this tube.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how would you reason about this?",
                    "label": 0
                },
                {
                    "sent": "You would actually try to understand what is the set of all the sounds this physical system can generate, where there is only 1 degree of freedom in this physical system.",
                    "label": 0
                },
                {
                    "sent": "And to understand this you would actually have to do something like this.",
                    "label": 0
                },
                {
                    "sent": "Solve the equations of airflow in this tube.",
                    "label": 0
                },
                {
                    "sent": "And these are the equations is a couple equations leading to what is called the Webster Horn equation.",
                    "label": 0
                },
                {
                    "sent": "Ultimately VP of X, T is the pressure as a function of time.",
                    "label": 0
                },
                {
                    "sent": "T is time.",
                    "label": 0
                },
                {
                    "sent": "X is location in in this in the horizontal axis.",
                    "label": 0
                },
                {
                    "sent": "So in other words S of T the sound this tube generates this tube makes is really the pressure wave.",
                    "label": 0
                },
                {
                    "sent": "At when X is equal to L. OK. UFT is the pressure wave when X is equal to 0 at the other end of the tube.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you solve this and you get a nice closed form answer for how U of T is related to SFT.",
                    "label": 0
                },
                {
                    "sent": "An if you have T looks like this, it's a periodic function.",
                    "label": 0
                },
                {
                    "sent": "It has a Fourier series expansion.",
                    "label": 0
                },
                {
                    "sent": "It lives in little L2.",
                    "label": 0
                },
                {
                    "sent": "The Fourier coefficients are the Alpha ends, so every sound you represent by computing its Fourier coefficients, and then it's a point in this Fourier space OK?",
                    "label": 0
                },
                {
                    "sent": "And betas are the Fourier coefficients of the S is and what I've plotted is the set of all sounds in this space spanned by beta 1 beta three and beta 7 for a particular simulation.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically the set of all sounds generated by this tube with just one degree of freedom would lie on a.",
                    "label": 0
                },
                {
                    "sent": "1 dimensional curve embedded in little L2 and this picture looks more or less exactly like the picture that I made up.",
                    "label": 0
                },
                {
                    "sent": "A few slides ago.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "You might think even this is artificial.",
                    "label": 0
                },
                {
                    "sent": "When does you know when are we in this situation where we are analyzing just single tubes, but actually there is a long tradition and some of you who?",
                    "label": 0
                },
                {
                    "sent": "Come from PZ and acoustics and its application to speech analysis would be aware of this.",
                    "label": 0
                },
                {
                    "sent": "There's a long tradition in modeling the Volk, the set of speech sounds as those generated by a non uniform tube.",
                    "label": 0
                },
                {
                    "sent": "The vocal tract as this nonuniform tube is excited by puffs of air that you blow into this tube by moving your lungs.",
                    "label": 0
                },
                {
                    "sent": "And that's how you speak.",
                    "label": 0
                },
                {
                    "sent": "And this is essentially the acoustic model of speech production, which is the classical model developed in the 50s and 60s and 70s, and sort of elaborated over the last 30 or 40 years.",
                    "label": 0
                },
                {
                    "sent": "And so if you take this view.",
                    "label": 0
                },
                {
                    "sent": "You would say that look all the speech sounds that are being generated a generated by a physical system and the degrees of freedom in this physical system are not that many.",
                    "label": 0
                },
                {
                    "sent": "You cannot move your vocal tract in arbitrary ways.",
                    "label": 1
                },
                {
                    "sent": "You cannot generate any sound any desired sound by moving a vocal tract, you can only generate the set of speech sounds that are naturally generated by a vocal tract, so it's a physical system with few degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "And the set of all speech sounds would therefore live in some low dimensional subset of the ambient space of all signals in which this is embedded.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's an example from acoustic, so let me give you an example from images.",
                    "label": 0
                },
                {
                    "sent": "So what's an image?",
                    "label": 0
                },
                {
                    "sent": "An image more or less is something like this.",
                    "label": 0
                },
                {
                    "sent": "A function from R2 to 01, where at any XY location F of XY is just the intensity of that image at that location.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So every image is therefore lives trivially in some function space, again, just like acoustic signals live trivially in some function space.",
                    "label": 0
                },
                {
                    "sent": "And now I want you to consider the following class of images very simple.",
                    "label": 0
                },
                {
                    "sent": "Class of image is almost a trivial class of images.",
                    "label": 0
                },
                {
                    "sent": "These are images of a vertical bar under translation.",
                    "label": 0
                },
                {
                    "sent": "These are pictures of vertical bars.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So as a set, this is the set script death.",
                    "label": 0
                },
                {
                    "sent": "So every element of this set script death is a particular function that corresponds to a particular image of a vertical bar, and the way this set is generated is by taking a particular vertical bar, which we call V. That's the image of a particular vertical bar, and keep translating this vertical bar, so all F's in this set script F can be represented in this way by the translation of a vertical bar by T. In law in the X axis and Y.",
                    "label": 0
                },
                {
                    "sent": "Buy an R in the Y axis, it's very.",
                    "label": 0
                },
                {
                    "sent": "A simple example and now ask the question if script F is the set of all images of vertical bars.",
                    "label": 0
                },
                {
                    "sent": "This set script DEF is embedded in some ambient space, which is the set of all functions.",
                    "label": 0
                },
                {
                    "sent": "Because all images are functions OK and immediately notice that this set script F is not a linear space.",
                    "label": 0
                },
                {
                    "sent": "If I take an element of script defan take another element of script F and add the two together, I no longer remain in this space.",
                    "label": 0
                },
                {
                    "sent": "This is just a simple observation that if I take a picture of a vertical bar, take another another picture of a vertical bar, and add those two images together, I don't get a picture of a vertical bar.",
                    "label": 0
                },
                {
                    "sent": "So this set of all images.",
                    "label": 0
                },
                {
                    "sent": "Is not a linear space, it's some.",
                    "label": 0
                },
                {
                    "sent": "Other kind of set?",
                    "label": 0
                },
                {
                    "sent": "And also notice that there are actually only two degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "In the set of all pictures of vertical bars and they are parameterized by TNR.",
                    "label": 0
                },
                {
                    "sent": "And so I think to be technically right, if you want to be smooth manifold, this picture has to be perhaps not exactly a vertical bar, but something which is a slightly smoothed out version of a vertical bar, so it doesn't have sharp edges or discontinuity's like that, but more or less this general argument can be converted into an argument to say that the set of all pictures of vertical bars would be a 2 dimensional manifold embedded.",
                    "label": 0
                },
                {
                    "sent": "In some space higher dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example from robotics.",
                    "label": 0
                },
                {
                    "sent": "So say you want to move a robotic arm.",
                    "label": 0
                },
                {
                    "sent": "OK, and you have a target location.",
                    "label": 0
                },
                {
                    "sent": "And you have the joints.",
                    "label": 0
                },
                {
                    "sent": "Which I think you see maybe 123 joints here.",
                    "label": 0
                },
                {
                    "sent": "OK. And every joint location is indicated by the angles of the joints, which is Tita I anfi I so theater 131 is the first joint theater 2 feet two is the 2nd joint, an XY and Z is actually the location of the tip of this robotic arm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So if you want to move this robotic arm around to a desired location.",
                    "label": 0
                },
                {
                    "sent": "OK, you are trying to find a function G. Where the configuration space, the natural structure of the configuration space is the set of all valid joint angles.",
                    "label": 0
                },
                {
                    "sent": "Which live essentially.",
                    "label": 0
                },
                {
                    "sent": "In that space.",
                    "label": 0
                },
                {
                    "sent": "And the tip of the arm lives in essentially in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a function.",
                    "label": 0
                },
                {
                    "sent": "Whose natural domain is some manifold and whose range here is some Euclidean space?",
                    "label": 0
                },
                {
                    "sent": "And you might want to actually learn this function.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the manifold learning one example of the manifold learning problem.",
                    "label": 0
                },
                {
                    "sent": "Where I would like to learn a function whose domain is some manifold and whose range is the reals.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some examples OK, and these kinds of examples people have thought about a little bit over the last several years to try and give a more precise and accurate formal justification for why you might be interested in learning a function whose domain is the manifold and why that's a meaningful question.",
                    "label": 0
                },
                {
                    "sent": "But as I said.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More generally, you could also view.",
                    "label": 0
                },
                {
                    "sent": "Manifold learning as really taking the view that all the data lives are manifold as the first simplest entry into a class of algorithms that are motivated by the intuition that the distribution of the data is not uniform.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now that we have some sense of why we want to do something like this, and why people have wanted to do something like this.",
                    "label": 0
                },
                {
                    "sent": "Let's just review a few basic objects.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just.",
                    "label": 0
                },
                {
                    "sent": "A few very basic objects so that we know what we're talking about as we proceed.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we're going to mostly just for simplicity, discuss embedded manifolds.",
                    "label": 1
                },
                {
                    "sent": "And those of some of you come from mathematics.",
                    "label": 0
                },
                {
                    "sent": "They already know everything about this and don't need any of this.",
                    "label": 0
                },
                {
                    "sent": "Some of you may come from other areas, and so just to guide your intuition.",
                    "label": 0
                },
                {
                    "sent": "You may think of this fear as the simplest example, or a model manifold that you might want to think about.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to define a manifold through a charts and atlases and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But just give you the intuition.",
                    "label": 0
                },
                {
                    "sent": "And so, given a point P on the manifold, there is something the tangent space on the manifold which for a K dimensional manifold is essentially AK dimensional.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "And you could think of it since we are discussing.",
                    "label": 0
                },
                {
                    "sent": "Mostly embedded manifold you can think of it as a K dimensional affine subspace of RN, exactly like this picture of a sphere.",
                    "label": 0
                },
                {
                    "sent": "The two sphere embedded in R3 where the tangent space is a 2 dimensional affine subspace of R3.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So since this tangent space is this linear space.",
                    "label": 0
                },
                {
                    "sent": "You can think naturally of tangent vectors.",
                    "label": 0
                },
                {
                    "sent": "And one point I want to make.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is that somehow there's a?",
                    "label": 0
                },
                {
                    "sent": "Natural.",
                    "label": 0
                },
                {
                    "sent": "Connection between tangent vectors and curves.",
                    "label": 1
                },
                {
                    "sent": "So if you think of a curve, so what's a curve?",
                    "label": 0
                },
                {
                    "sent": "Fee of tea is a curve is a map from R to MK.",
                    "label": 0
                },
                {
                    "sent": "OK, so that traces out a curve on the manifold.",
                    "label": 0
                },
                {
                    "sent": "And it should take.",
                    "label": 0
                },
                {
                    "sent": "The derivative of this with respect.",
                    "label": 0
                },
                {
                    "sent": "To T. You get a vector, and that's essentially the tangent vector.",
                    "label": 0
                },
                {
                    "sent": "So every tangent vector can be identified with a curve.",
                    "label": 0
                },
                {
                    "sent": "The tangent vector sits in TP where P is a point on the manifold.",
                    "label": 0
                },
                {
                    "sent": "So every tangent vector can be identified with a curve that passes through P whose derivative.",
                    "label": 0
                },
                {
                    "sent": "Matches the vector.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tangent vectors can also be thought of as derivatives in the following way.",
                    "label": 0
                },
                {
                    "sent": "If you take a function F from MK to R. And you pick a vector in the tangent space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since you already discussed how vectors are identified with curves, so the vector V can be identified with the curphey of tea.",
                    "label": 0
                },
                {
                    "sent": "From R2 MK.",
                    "label": 0
                },
                {
                    "sent": "And then this object makes sense by composing the two F of C of T. And that's just a regular function from R2R.",
                    "label": 0
                },
                {
                    "sent": "Right, so you have a function from M to R. You pick a point P on your manifold.",
                    "label": 0
                },
                {
                    "sent": "You consider the tangent space at P, which is TP.",
                    "label": 0
                },
                {
                    "sent": "You pick a vector V in this tangent space.",
                    "label": 0
                },
                {
                    "sent": "That vector V can be identified with a curve on the manifold that passes through P. And if you take.",
                    "label": 0
                },
                {
                    "sent": "F of 50 is just get a regular function.",
                    "label": 0
                },
                {
                    "sent": "And if you differentiate it with respect to T, which we know how to do from general calculus.",
                    "label": 0
                },
                {
                    "sent": "That's the same thing as basically taking the directional derivative of F in the direction V. And.",
                    "label": 0
                },
                {
                    "sent": "You can think therefore of tangent vectors as operators that act on functions and take.",
                    "label": 1
                },
                {
                    "sent": "Directional derivatives in certain directions.",
                    "label": 0
                },
                {
                    "sent": "So tangent vectors I had five with curves on the one hand and with directional derivatives on the other hand, in this way.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the thing that gives it its Riemannian structure.",
                    "label": 0
                },
                {
                    "sent": "Essentially.",
                    "label": 0
                },
                {
                    "sent": "Will be.",
                    "label": 0
                },
                {
                    "sent": "Norms and angles in the tangent space so that you can talk about inner products between any two vectors.",
                    "label": 1
                },
                {
                    "sent": "V&W in the tangent space.",
                    "label": 0
                },
                {
                    "sent": "Since everything here is embedded, we don't have to consider it abstractly right now, so you can do this abstractly, but you see the tangent space is just an embedded linear subspace, or just naturally inherits the inner product structure from the space in which it is embedded, and so this V&W is exactly what you would think it to be from this picture.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But that's what gives the Romanian structure.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now that you have this, you can talk about lengths of curves.",
                    "label": 1
                },
                {
                    "sent": "And you can talk about geodesics.",
                    "label": 0
                },
                {
                    "sent": "So what is the length of a curve?",
                    "label": 0
                },
                {
                    "sent": "So you begin with what is a curve.",
                    "label": 0
                },
                {
                    "sent": "A curve is a map from 012 MC.",
                    "label": 0
                },
                {
                    "sent": "And if you take its derivative, you actually get a vector, right?",
                    "label": 0
                },
                {
                    "sent": "Defeat by DT is actually a vector.",
                    "label": 0
                },
                {
                    "sent": "And if you take the norm of that vector, that makes sense because all the vectors are living in some space where you have norms or inner products defined.",
                    "label": 0
                },
                {
                    "sent": "And if you integrate that along the length of the curve.",
                    "label": 0
                },
                {
                    "sent": "You get the length of that curve.",
                    "label": 0
                },
                {
                    "sent": "So that's how you would calculate the length of any curve and now between two points you can set up a variational problem to ask for the shortest curve that goes from one point P to another point Q.",
                    "label": 1
                },
                {
                    "sent": "And that more or less will give you the notion of a geodesic.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one more thing.",
                    "label": 0
                },
                {
                    "sent": "The gradient, so what's the gradient?",
                    "label": 0
                },
                {
                    "sent": "Of a function.",
                    "label": 0
                },
                {
                    "sent": "So if you take a function F. From M to R. OK.",
                    "label": 0
                },
                {
                    "sent": "In the gradient of that function.",
                    "label": 0
                },
                {
                    "sent": "OK will be.",
                    "label": 0
                },
                {
                    "sent": "Will act as an operator on vectors in the tangent space.",
                    "label": 0
                },
                {
                    "sent": "So that given any V on the tangent space.",
                    "label": 0
                },
                {
                    "sent": "If I did.",
                    "label": 0
                },
                {
                    "sent": "That inner product of the gradient with V. Is differentiating the function in the direction V?",
                    "label": 0
                },
                {
                    "sent": "So I fixed the function FI, pick any vector V in the tangent space.",
                    "label": 0
                },
                {
                    "sent": "I can differentiate DF in the direction V, and I get a number.",
                    "label": 0
                },
                {
                    "sent": "So I get a map from functions to numbers.",
                    "label": 0
                },
                {
                    "sent": "And this makes use of this connection that we just saw between tangent vectors and directional derivatives.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Using this sort of.",
                    "label": 0
                },
                {
                    "sent": "Notion you see that the gradient is going to point in the direction of maximum change.",
                    "label": 1
                },
                {
                    "sent": "Because you would just maximize this over all possible directions.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's one more notion, basic notion, which is the exponential map.",
                    "label": 0
                },
                {
                    "sent": "So the exponential map.",
                    "label": 0
                },
                {
                    "sent": "Will take you from the tangent space back to the manifold, so we now have the manifold.",
                    "label": 0
                },
                {
                    "sent": "We have a point P on the manifold.",
                    "label": 0
                },
                {
                    "sent": "We have the tangent space at the point P. We have functions defined in the manifold.",
                    "label": 0
                },
                {
                    "sent": "We have vectors living in the tangent space on the manifold and now consider TP of M which is the tangent space of M at the point P. OK, so if I pick any element of TP of M, any element of this tangent space is a vector.",
                    "label": 0
                },
                {
                    "sent": "And the exponential map will take me.",
                    "label": 1
                },
                {
                    "sent": "From TP back to M and the way you do this is that you essentially start.",
                    "label": 0
                },
                {
                    "sent": "Going in a geodesic.",
                    "label": 0
                },
                {
                    "sent": "Along V. For such that the length of the curve has Norm V. OK, so the vector V that you've picked has a certain norm as a number.",
                    "label": 0
                },
                {
                    "sent": "And so now you will go essentially along this manifold.",
                    "label": 0
                },
                {
                    "sent": "For a distance which is equal to the length of this vector V.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The reason?",
                    "label": 0
                },
                {
                    "sent": "To introduce it all here is that perhaps it's an easiest.",
                    "label": 0
                },
                {
                    "sent": "It's an easy way.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's the easiest way, but it's seems to be an easy way to think about and define the Laplace Beltrami operator on this manifold.",
                    "label": 0
                },
                {
                    "sent": "Because this is an object that's going to come up, so we need to understand a little bit what this is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Because we have the exponential map that takes me from any vector.",
                    "label": 0
                },
                {
                    "sent": "In the tangent space at P back to the manifold, this gives me a coordinate system, a natural coordinate system on the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK so I have this K dimensional space which is the tangent space.",
                    "label": 0
                },
                {
                    "sent": "And if I pick any vector in this K dimensional space by applying the exponential map to this vector, I get a point on the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so every point on the manifold has these.",
                    "label": 0
                },
                {
                    "sent": "Geodesic coordinates, so to speak, or.",
                    "label": 0
                },
                {
                    "sent": "It gives you coordinate system for points in the manifold.",
                    "label": 1
                },
                {
                    "sent": "So now if you're given a function F from M to R. OK. You can express it.",
                    "label": 0
                },
                {
                    "sent": "As a function of just K variables.",
                    "label": 0
                },
                {
                    "sent": "By composing it with the exponential map.",
                    "label": 0
                },
                {
                    "sent": "OK, so F composed with the exponential map will take you from the tangent space.",
                    "label": 0
                },
                {
                    "sent": "OK, the exponential map takes you from the tangent space to the manifold and the function F takes you from the manifold to R so that composition will take you from the tangent space to R. And the tangent space is just a regular K dimensional space identified with RK.",
                    "label": 0
                },
                {
                    "sent": "So this composition is just a regular function, a standard function from RK2R.",
                    "label": 0
                },
                {
                    "sent": "OK, and for a standard function from RK2R.",
                    "label": 0
                },
                {
                    "sent": "The Laplacian is something which maybe many of you have encountered for the standard function, so this is just how to define the Laplacian for manifold.",
                    "label": 0
                },
                {
                    "sent": "So the Laplacian for a standard function from RK2R looks exactly like this.",
                    "label": 0
                },
                {
                    "sent": "So let me just write that on the board.",
                    "label": 0
                },
                {
                    "sent": "So here's what it looks like.",
                    "label": 0
                },
                {
                    "sent": "You're just looking at a standard, just the usual function from RK2R.",
                    "label": 0
                },
                {
                    "sent": "Then Laplace of F. And sometimes there's a - it's just this.",
                    "label": 0
                },
                {
                    "sent": "So it's a twice differentiable function from RK to R. From one through Ki, going from one through K. And you differentiate twice.",
                    "label": 0
                },
                {
                    "sent": "In each direction and sum it up.",
                    "label": 0
                },
                {
                    "sent": "So this gives you a map from twice differentiable functions from RK.",
                    "label": 0
                },
                {
                    "sent": "And you get.",
                    "label": 0
                },
                {
                    "sent": "Another function.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So it gives you this operator.",
                    "label": 0
                },
                {
                    "sent": "So the standard Laplacian everyone has encountered probably.",
                    "label": 0
                },
                {
                    "sent": "In Euclidian settings.",
                    "label": 0
                },
                {
                    "sent": "This is how the Laplacian is defined for functions from RK2R an.",
                    "label": 0
                },
                {
                    "sent": "What we're trying to do is define the natural Laplacian for functions defined on a manifold.",
                    "label": 0
                },
                {
                    "sent": "So what you would do?",
                    "label": 0
                },
                {
                    "sent": "Is basically consider the function from M to R. And you would like to differentiate it twice.",
                    "label": 0
                },
                {
                    "sent": "You pick a point P. You would like to differentiate twice in each direction in each natural direction along P. OK, and then add it up.",
                    "label": 0
                },
                {
                    "sent": "And formally, what you would do is this.",
                    "label": 0
                },
                {
                    "sent": "You would use the exponential map to go from the tangent space to the manifold and composing those two just gives you a standard function from TP of M2R.",
                    "label": 0
                },
                {
                    "sent": "DP FM is a K dimensional Euclidean space and the Laplacian for that is defined.",
                    "label": 0
                },
                {
                    "sent": "And so this.",
                    "label": 0
                },
                {
                    "sent": "Tells you what the Laplace Beltrami operator is for functions on a manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so now if you think of the circle, the manifold is a circle or the manifold is the sphere or the manifold.",
                    "label": 0
                },
                {
                    "sent": "You can actually workout exact formulas for what the Laplacian on the sphere or the circle or any such very well structured manifold and we will look in particular at the following eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "Laplace F is equal to Lambda F. OK, so if I take any twice differentiable function on the manifold an I apply the Laplace operator to it, I get another function and I will look at the eigenvalue problem Laplace F is equal to Lambda F. And those are the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "An eigenfunctions of the Laplace Beltrami operator on this manifold.",
                    "label": 0
                },
                {
                    "sent": "So we will workout.",
                    "label": 0
                },
                {
                    "sent": "I think the case for the circle pretty explicitly where this will all make sense.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't want to say.",
                    "label": 0
                },
                {
                    "sent": "Too much about curvature because it'll get us into.",
                    "label": 0
                },
                {
                    "sent": "Somewhat more technical things today.",
                    "label": 0
                },
                {
                    "sent": "Want to get into?",
                    "label": 0
                },
                {
                    "sent": "There's an intrinsic notion, and then there is an extrinsic notion, and so on.",
                    "label": 0
                },
                {
                    "sent": "But intuitively, I just want to make the high level point that if something is intrinsically curved you cannot flatten it, just like you cannot flatten the.",
                    "label": 0
                },
                {
                    "sent": "Sphere, although it is a 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Surface, think of it that way.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so those are just.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, those are only the things I wanted to say, so we now have a rough idea for the manifold is.",
                    "label": 0
                },
                {
                    "sent": "We have a rough idea of what it means to be a function defined in this manifold.",
                    "label": 0
                },
                {
                    "sent": "What it means to differentiate this function in any direction on the manifold?",
                    "label": 0
                },
                {
                    "sent": "We have a rough idea of what the tangent spaces are or the tangent vectors are that the tangent vectors are identified with directional derivatives and we have this operator.",
                    "label": 0
                },
                {
                    "sent": "Call the Laplace Beltrami operator that acts on twice differentiable functions on the manifold.",
                    "label": 0
                },
                {
                    "sent": "Which we will come to later.",
                    "label": 0
                },
                {
                    "sent": "So now let's go to the following kind of question, which a lot of people in this subject began with an.",
                    "label": 0
                },
                {
                    "sent": "It's still actually somewhat unresolved.",
                    "label": 0
                },
                {
                    "sent": "And this is the question that people actually began with, which is the question of dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about some of these algorithms that people proposed over the last decades.",
                    "label": 0
                },
                {
                    "sent": "We're beginning with, I guess something in 2000.",
                    "label": 0
                },
                {
                    "sent": "So in the year 2002, papers appeared in Science magazine.",
                    "label": 0
                },
                {
                    "sent": "One proposing this algorithm called I's OMAP another, proposing an algorithm called Eli, and they both made the following observation OK.",
                    "label": 0
                },
                {
                    "sent": "They appeared in Science magazine in the year 2000 and they had a huge impact over the last decade and a lot of the work in some sense was trying to make sense of.",
                    "label": 0
                },
                {
                    "sent": "The idea is.",
                    "label": 0
                },
                {
                    "sent": "Maybe surrounding this algorithm or right or developing new algorithmic alternatives that maybe were a little better.",
                    "label": 0
                },
                {
                    "sent": "We're a little more principled, perhaps an to make sense really of the geometric thesis and here.",
                    "label": 0
                },
                {
                    "sent": "Is the question the question of dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "Suppose I give you a collection of points X one through XN, which lives on this manifold.",
                    "label": 0
                },
                {
                    "sent": "Embedded in RN?",
                    "label": 0
                },
                {
                    "sent": "In this manifold is a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, what that means is that there are very few degrees of freedom on this manifold.",
                    "label": 0
                },
                {
                    "sent": "It's essentially low dimensional object.",
                    "label": 0
                },
                {
                    "sent": "So can I actually re represent my data?",
                    "label": 0
                },
                {
                    "sent": "The X one through XN which is sitting in a very high dimensional space?",
                    "label": 0
                },
                {
                    "sent": "In a D dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So that it correctly captures the natural geometric relationships that the data will have with each other.",
                    "label": 0
                },
                {
                    "sent": "OK, now there's a fact which I won't actually discuss in any detail, but the fact which I wanted to actually point out to you, which actually leads to something which I think can be formulated as a as a clear question.",
                    "label": 0
                },
                {
                    "sent": "And that is this that you can take this manifold M. And it's a D dimensional manifold and you can buy what is known as Nash's embedding theorem.",
                    "label": 0
                },
                {
                    "sent": "Embedding embedded isometrically in more or less.",
                    "label": 0
                },
                {
                    "sent": "I forget whether it's 2D, maybe it's R2D or 2D plus one or something like that is more or less AD dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, and it depends a little bit on whether you want this map to be continuous or how many derivatives you want from this for this embedding, but.",
                    "label": 0
                },
                {
                    "sent": "That observation says essentially that if you take if you have this manifold M, abstractly, you can embed it.",
                    "label": 0
                },
                {
                    "sent": "Isometrically in a more or less dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what does that mean?",
                    "label": 0
                },
                {
                    "sent": "If all your data lives on this manifold an if I apply this map?",
                    "label": 0
                },
                {
                    "sent": "To the data then I should get an embedding of the data in more or less a dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But of course I don't know this map.",
                    "label": 0
                },
                {
                    "sent": "OK, we know that this map exists.",
                    "label": 0
                },
                {
                    "sent": "But we don't know this map.",
                    "label": 0
                },
                {
                    "sent": "We don't even know the manifold, so forget about knowing this map.",
                    "label": 0
                },
                {
                    "sent": "So the natural question then is if I give you a bunch of points sample from this manifold.",
                    "label": 0
                },
                {
                    "sent": "Can you somehow discover this map that would embed this manifold isometrically in a D dimensional space?",
                    "label": 0
                },
                {
                    "sent": "And then apply this map to the data and thereby end up embedding the data.",
                    "label": 0
                },
                {
                    "sent": "Now in AD dimensional space, where is all this data actually lived?",
                    "label": 0
                },
                {
                    "sent": "Originally, in an end dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So what was dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "It was basically the notion that all my data lives in a very high dimensional space, but close to a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "I would like to embed it into a lower dimensional space that captures the natural structure of the data.",
                    "label": 0
                },
                {
                    "sent": "The simplest example of this, and the classical example.",
                    "label": 0
                },
                {
                    "sent": "Again, we go back 100 years to the principle components analysis scheme.",
                    "label": 0
                },
                {
                    "sent": "What does it do?",
                    "label": 0
                },
                {
                    "sent": "All my data lives in a very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "Embed it into a K dimensional space and I can find this embedding by looking at the covariance structure of my data and looking at the eigenvectors of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So a number of algorithms have been developed over the last decade for trying to actually solve this problem of dimensionality reduction, and most of these algorithms do not come with proofs of guarantees or sometimes even not even with a precise statement of.",
                    "label": 0
                },
                {
                    "sent": "What the question is.",
                    "label": 0
                },
                {
                    "sent": "OK, but the intuition has always been this that because all the data lives in some low dimensional manifold, somehow we feel that we can embed it into a lower dimensional space and capture all the natural structure of the data.",
                    "label": 0
                },
                {
                    "sent": "Because there are only very few degrees of freedom in this data.",
                    "label": 0
                },
                {
                    "sent": "OK, so given what I just told you about exponential Maps and that every point on the manifold can now be represented in this exponential coordinate system, an since every data point lives on this manifold in every data, .2 can be expressed in this exponential coordinate system.",
                    "label": 0
                },
                {
                    "sent": "Then a natural thing you may want to do is recover this coordinate system somehow.",
                    "label": 0
                },
                {
                    "sent": "OK, so the manifold is low dimensional, so although all the data is living all over the place in the manifold, they are fundamentally representable.",
                    "label": 0
                },
                {
                    "sent": "As K dimensional vectors.",
                    "label": 0
                },
                {
                    "sent": "By somehow taking the exponential coordinate system.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the algorithmic framework for all of this has tended to have the following high level picture.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is some manifold OK sitting in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "I don't know this manifold.",
                    "label": 0
                },
                {
                    "sent": "I get a bunch of points from this manifold.",
                    "label": 0
                },
                {
                    "sent": "So what am I to do?",
                    "label": 0
                },
                {
                    "sent": "OK, I have to recover something after recover either a map from the manifold to some low dimensional space or learn some function whose domain is the manifold and so on.",
                    "label": 0
                },
                {
                    "sent": "What people do is well, I don't know the manifold.",
                    "label": 0
                },
                {
                    "sent": "I have a bunch of points.",
                    "label": 0
                },
                {
                    "sent": "Let me actually make some kind of a mesh like structure or a graph like structure by comparing combined by.",
                    "label": 0
                },
                {
                    "sent": "Connecting nearby points to each other.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I make a graph like this and this is my approximation to the manifold.",
                    "label": 0
                },
                {
                    "sent": "And who the hell knows whether this is good or bad?",
                    "label": 0
                },
                {
                    "sent": "But this is what people do.",
                    "label": 0
                },
                {
                    "sent": "OK, later I'll actually prove some theorems about this kind of a construction.",
                    "label": 0
                },
                {
                    "sent": "But this is what people do.",
                    "label": 0
                },
                {
                    "sent": "The basic algorithm framework is.",
                    "label": 0
                },
                {
                    "sent": "I connect nearby points to each other and nearby means nearby in the Euclidean space in which the points are embedded, because that's the only distance that I can measure.",
                    "label": 0
                },
                {
                    "sent": "All my points are living in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "Given any two points, I can measure the Euclidean distance, and that's in fact the only thing I can do.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll make this nearest neighbor graph.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's common to many of these methods, and that's going to be my approximation to the manifold, and so if I wanted to do something on the manifold I will do something on the graph instead.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's discuss the first of these schemes, OK, which is Isomap.",
                    "label": 0
                },
                {
                    "sent": "Here's an algorithm, so I'm going to now go through several algorithms.",
                    "label": 0
                },
                {
                    "sent": "Most of these algorithms do not come with.",
                    "label": 0
                },
                {
                    "sent": "Proofs of correctness of any sort, but I'm trying to give you a sense of what motivated people to think of algorithms of this sort and what would be more precise versions of the questions these algorithms were trying to.",
                    "label": 0
                },
                {
                    "sent": "Solve.",
                    "label": 0
                },
                {
                    "sent": "So this is an algorithm ISOMAP, and this is what it does.",
                    "label": 0
                },
                {
                    "sent": "You construct a nearest neighbor graph from all this data data.",
                    "label": 0
                },
                {
                    "sent": "You find the shortest path distances between all these points along the graph.",
                    "label": 0
                },
                {
                    "sent": "OK, you can use Dijkstra's algorithm or something to find the shortest path distance between all points on this graph.",
                    "label": 0
                },
                {
                    "sent": "And so now you've got this set of distance this dij.",
                    "label": 0
                },
                {
                    "sent": "OK, now this is already become a somewhat intrinsic object, because notice that dij is not simply the Euclidean distance between the points XI and XJ.",
                    "label": 0
                },
                {
                    "sent": "OK, so you made a graph where every vertex is identified with the data point and between the Hyatt data Points XI and the GF data points.",
                    "label": 0
                },
                {
                    "sent": "The XJ there is the Euclidean distance between them, but you made this nearest neighbor graph and there is the shortest path distance between them.",
                    "label": 0
                },
                {
                    "sent": "And now you've got this this new distance metric and this distance metric is trying to simulate the geodesic distance between those two points.",
                    "label": 0
                },
                {
                    "sent": "If you were trying to go along the manifold.",
                    "label": 0
                },
                {
                    "sent": "So the natural question is, is this DJ close?",
                    "label": 0
                },
                {
                    "sent": "Do the geodesic distance between XI and XJ, and that I think under certain circumstances is true.",
                    "label": 0
                },
                {
                    "sent": "So now I've got this.",
                    "label": 0
                },
                {
                    "sent": "Set of distance.",
                    "label": 0
                },
                {
                    "sent": "Distances between points and I would like to embed it into a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, an embed it using this scheme called multidimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "So what is multidimensional scaling?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multidimensional scaling is the following idea.",
                    "label": 1
                },
                {
                    "sent": "I give you a matrix of distances.",
                    "label": 0
                },
                {
                    "sent": "You go from that to a matrix of inner products somehow, and then from a matrix of inner products you go to an embedding.",
                    "label": 0
                },
                {
                    "sent": "Into vectors whose distances will correspond to the distances that you have been given by this distance matrix.",
                    "label": 0
                },
                {
                    "sent": "So how do you go from distances to inner products?",
                    "label": 1
                },
                {
                    "sent": "You just make use of this identity.",
                    "label": 0
                },
                {
                    "sent": "That if you take two points X&Y, two vectors X&Y, which are identified with points.",
                    "label": 0
                },
                {
                    "sent": "Then this is just.",
                    "label": 0
                },
                {
                    "sent": "The distance between X&Y, the Euclidean distance, can be expressed in terms of the inner product.",
                    "label": 0
                },
                {
                    "sent": "That gives you a relationship between inner products and distances, and this relationship of course is true only if the distances are consistent with inner products.",
                    "label": 0
                },
                {
                    "sent": "This is actually not the case if the distances were actually distances.",
                    "label": 0
                },
                {
                    "sent": "Geodesic distances on the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So multidimensional scaling says you give me a distance metric distance.",
                    "label": 0
                },
                {
                    "sent": "A matrix of distances.",
                    "label": 0
                },
                {
                    "sent": "If there is a set of points.",
                    "label": 0
                },
                {
                    "sent": "In a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "From which this distance matrix could have arisen.",
                    "label": 0
                },
                {
                    "sent": "Then I can find those points for you.",
                    "label": 0
                },
                {
                    "sent": "OK, so if indeed there were a set of points from which these distance made this distance matrix arose.",
                    "label": 0
                },
                {
                    "sent": "Then the inner products between those points would satisfy this relationship.",
                    "label": 0
                },
                {
                    "sent": "And you can actually solve this and see that to go from D to a, which is the matrix of inner products, is given by this formula.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a little bit of linear algebra.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you give me a matrix of distances.",
                    "label": 0
                },
                {
                    "sent": "I can recover the matrix of inner products if this distance matrix arose from a set of vectors in Euclidean space, and once I've got a metric matrix of inner products, how do I find the vectors?",
                    "label": 0
                },
                {
                    "sent": "Simply by looking at the eigenvectors of this matrix.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you do the next step of embedding.",
                    "label": 0
                },
                {
                    "sent": "By essentially noticing that the matrix of inner product is going to be, well, technically it's going to be positive semidefinite, so to be a little careful you have to actually do the spectral factorization carefully where you keep the orthogonal spaces and the null spaces correctly.",
                    "label": 0
                },
                {
                    "sent": "So this is done over all non zero eigenvalues I suppose Lambda I and so this is a map that will take you from for any X from one through N. You get a map CX.",
                    "label": 1
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "As it is written there and you can check that this map will actually satisfy the inner products and will satisfy the distance matrix.",
                    "label": 0
                },
                {
                    "sent": "So the way the multidimensional scaling works is that you start with a set of distance with a distance matrix.",
                    "label": 0
                },
                {
                    "sent": "You go from there to a candidate set of inner products and you go from that candidate sort of inner products to a set of vectors which are consistent with that inner product matrix.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what people did is they applied this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Do a lot of data OK so here are many pictures and this is a picture of.",
                    "label": 0
                },
                {
                    "sent": "I guess a hand.",
                    "label": 0
                },
                {
                    "sent": "Where the wrist is rotating.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The finger is also extending, so I don't know if you can see this very well, but each of these pictures is a picture of a hand and the pictures differ from each other along 2 dimensions.",
                    "label": 0
                },
                {
                    "sent": "In some of those pictures, the fingers are extended an in some of those pictures the wrist is rotated so there are two degrees of freedom in the natural degrees of freedom in the data, pictures corresponding to rotated wrist in pictures corresponding to extended finger and basically you collect a lot of these pictures.",
                    "label": 0
                },
                {
                    "sent": "Lot of images.",
                    "label": 0
                },
                {
                    "sent": "You apply this ISOMAP technique an you can now embed this data every picture into a 2 dimensional space by asking for 2 dimensional embedding.",
                    "label": 0
                },
                {
                    "sent": "Once you embedded every such picture into a 2 dimensional space, you can look at every point in this 2 dimensional space and every point in this 2 dimensional space corresponds to a picture.",
                    "label": 0
                },
                {
                    "sent": "Anne, this is what those pictures look like.",
                    "label": 0
                },
                {
                    "sent": "You see that you actually recover the wrist rotation and the finger extension.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose all the data lived actually on some manifold and this manifold was flat.",
                    "label": 0
                },
                {
                    "sent": "Here is a statement that is actually true intrinsically flat, so it unfolds a flat manifold which is isometric to a convex domain in RN.",
                    "label": 1
                },
                {
                    "sent": "There's another algorithm called Hessian Eigen Maps that I didn't talk about.",
                    "label": 0
                },
                {
                    "sent": "And another algorithm called local Tangent space alignment didn't talk about and all of them are trying to do the same circle of things.",
                    "label": 0
                },
                {
                    "sent": "OK, you have all this data living on a manifold.",
                    "label": 0
                },
                {
                    "sent": "Can you actually unroll this manifold?",
                    "label": 0
                },
                {
                    "sent": "Can you actually re embed the data into a lower dimensional space?",
                    "label": 0
                },
                {
                    "sent": "And then you can ask whether this re embedding is correct or isometric.",
                    "label": 0
                },
                {
                    "sent": "In some sense, and some of these statements may be true for some of these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is another algorithm which people use called local locally linear embedding.",
                    "label": 0
                },
                {
                    "sent": "So how does this algorithm work so?",
                    "label": 0
                },
                {
                    "sent": "The way this algorithm works is you again begin with the nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "And now what you do is.",
                    "label": 0
                },
                {
                    "sent": "Let X one through XN be the neighbors of X. OK. You Project X to the span of X1 through XN.",
                    "label": 1
                },
                {
                    "sent": "And that projection we're calling X bar OK.",
                    "label": 0
                },
                {
                    "sent": "So I have all my data.",
                    "label": 0
                },
                {
                    "sent": "I've made my nearest neighbor graph for every data point I can look at its nearest neighbors, an project that data point into the span of the nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And then once I've got the span of the nearest neighbors and I've got X bar, I simply find the barycentric coordinates of X bar.",
                    "label": 0
                },
                {
                    "sent": "Which is basically finding a set of numbers that sum to one such that expires the center of mass.",
                    "label": 0
                },
                {
                    "sent": "OK. And I represent X bar in terms of this barycentric coordinates.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I do is I construct a sparse matrix W. With the iatros of this.",
                    "label": 0
                },
                {
                    "sent": "Matrix is the barycentric coordinates of XI in the basis of its nearest neighbors.",
                    "label": 1
                },
                {
                    "sent": "OK, and then I use the lowest eigenvectors of this matrix to embed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some of you have already seen this probably, but many of you may not have and I'm just giving these to give you a sense of what it is that people were trying to do.",
                    "label": 0
                },
                {
                    "sent": "At the turn of the century.",
                    "label": 0
                },
                {
                    "sent": "OK, this is algorithm from 2000.",
                    "label": 0
                },
                {
                    "sent": "And this is the intuition they had, and they came up with these procedures and actually.",
                    "label": 0
                },
                {
                    "sent": "For the most part, you can't.",
                    "label": 0
                },
                {
                    "sent": "Prove things correct about these particular algorithms, but you could ask, are there other algorithms that would do meaningful things for the same general kind of a question?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is an argument for why the LLE is somehow computing in some sense the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go through this.",
                    "label": 0
                },
                {
                    "sent": "It's a little technical.",
                    "label": 0
                },
                {
                    "sent": "But there is an argument in a paper Misha Belkin I wrote actually shortly after this algorithm was published in Science, which is giving some kind of heuristic argument for how Eli was somehow calculating.",
                    "label": 0
                },
                {
                    "sent": "The trace of the Hessian, which actually is the Laplacian or maybe discovering the square of the Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then let me discuss the next algorithm, which is the algorithm which is called Laplacian eigen Maps OK, and about Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "You can actually prove a few things.",
                    "label": 0
                },
                {
                    "sent": "And it's related to a few other algorithms that have also been discussed later.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you the Laplacian Eigen Maps algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the Laplacian algorithm Maps algorithm works like this.",
                    "label": 0
                },
                {
                    "sent": "You again build a nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a graph where every data point is identified with a vertex and you put an edge between two data points.",
                    "label": 0
                },
                {
                    "sent": "If XI is close to XJ in some sense, so you measure the Euclidean distance between XI and XJ, which is the only distance you can measure.",
                    "label": 1
                },
                {
                    "sent": "And if it is less than epsilon, you connect them and thus you build your nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You make this graph awaited graph.",
                    "label": 0
                },
                {
                    "sent": "So now you have a graph with N vertices.",
                    "label": 0
                },
                {
                    "sent": "I have N data points.",
                    "label": 0
                },
                {
                    "sent": "I have a graph with N vertices.",
                    "label": 0
                },
                {
                    "sent": "Each vertex is identified with the data point and WIJ.",
                    "label": 0
                },
                {
                    "sent": "The weight between the Iatan.",
                    "label": 0
                },
                {
                    "sent": "The Jade vertex is given by that exponential function.",
                    "label": 0
                },
                {
                    "sent": "Now the general function is actually quite important, because the correctness of this algorithm will depend on actually that.",
                    "label": 0
                },
                {
                    "sent": "Functional form not precisely in that function, but on the decay of that function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there is one parameter T here.",
                    "label": 0
                },
                {
                    "sent": "That you see.",
                    "label": 0
                },
                {
                    "sent": "If T is very small.",
                    "label": 0
                },
                {
                    "sent": "Then it penalizes heavily if the point Seccion next year far apart an if T is very large.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do that.",
                    "label": 0
                },
                {
                    "sent": "So it actually is enforcing locality.",
                    "label": 0
                },
                {
                    "sent": "So now what I've got is I've got N data points.",
                    "label": 0
                },
                {
                    "sent": "I've built a graph with N vertices.",
                    "label": 0
                },
                {
                    "sent": "And it's a weighted graph where between the IAT vertex and the jet vertex I put the weight.",
                    "label": 0
                },
                {
                    "sent": "WIJ, it's symmetric wiw is a symmetric matrix, and notice that actually in the setting that we're in, it's a random matrix.",
                    "label": 0
                },
                {
                    "sent": "Because the way this matrix is generated is the following.",
                    "label": 0
                },
                {
                    "sent": "I get a bunch of points randomly sampled from this manifold.",
                    "label": 0
                },
                {
                    "sent": "X1X2X3X4X5 and so on.",
                    "label": 0
                },
                {
                    "sent": "So I get a random number of points.",
                    "label": 0
                },
                {
                    "sent": "Each point is on the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, I've got 10 points and now I've generated an N by N matrix which is a matrix which looks like this.",
                    "label": 0
                },
                {
                    "sent": "It's a random matrix.",
                    "label": 0
                },
                {
                    "sent": "So this particular matrix will have eigenvectors and eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "OK, and then there is the manifold and the central result which we will talk about later after we just.",
                    "label": 0
                },
                {
                    "sent": "Intuitively understand this algorithm a little better.",
                    "label": 0
                },
                {
                    "sent": "Is that by looking at the spectrum of this matrix and the eigenvalues and eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Of this matrix essentially are something very closely related to this matrix.",
                    "label": 0
                },
                {
                    "sent": "We can recover the eigenvalues and eigenfunctions of the Laplace Beltrami operator of the manifold from which the data was sampled.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Will make sense of that statement shortly.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The algorithm looks like this.",
                    "label": 0
                },
                {
                    "sent": "You look at a matrix called the Laplacian Matrix L, and that Laplacian matrix is equal to D -- W. W is as I just defined it.",
                    "label": 0
                },
                {
                    "sent": "It's a symmetric matrix where WI J is what it is with those exponential weighted decay and D is the diagonal matrix.",
                    "label": 1
                },
                {
                    "sent": "Where DI is simply the row sums of WIJ overall J. OK.",
                    "label": 0
                },
                {
                    "sent": "So now you have this matrix L. It's actually immediate that it is symmetric because D is a diagonal matrix and W is a symmetric matrix.",
                    "label": 0
                },
                {
                    "sent": "It also turns out to be positive semidefinite.",
                    "label": 0
                },
                {
                    "sent": "Which is not immediately obvious, but it turns out to be true.",
                    "label": 1
                },
                {
                    "sent": "And eigenvalues and eigenvectors of this matrix gives us.",
                    "label": 0
                },
                {
                    "sent": "Is used for embedding OK that's in Laplacian Eigen Maps.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's another distance metric.",
                    "label": 0
                },
                {
                    "sent": "That was also proposed.",
                    "label": 0
                },
                {
                    "sent": "This is due to Steven Lafon and Raffi Koifman.",
                    "label": 0
                },
                {
                    "sent": "Call the diffusion distance and that's very closely related to this, essentially.",
                    "label": 0
                },
                {
                    "sent": "Very, very similar.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you think of the heat diffusion operator on this manifold HT, and we'll look at that a little bit, because the two main objects that really we will need to get a sense of are the diffusion of heat on the manifold.",
                    "label": 0
                },
                {
                    "sent": "The so called heat, kernel, the Laplacian and eigenvectors eigenfunctions of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "Now the heat kernel gives rise to what is called the heat operator, the diffusion operator.",
                    "label": 0
                },
                {
                    "sent": "And the diffusion distance.",
                    "label": 0
                },
                {
                    "sent": "Between two points is defined like this, OK?",
                    "label": 0
                },
                {
                    "sent": "If you imagine an initial distribution of heat on the manifold which is.",
                    "label": 0
                },
                {
                    "sent": "Pulse at X so it's Delta function at X.",
                    "label": 0
                },
                {
                    "sent": "And if you imagine the initial distribution of heat, which is a Delta function at Y, then you can ask if heat flow happened on the manifold if there was diffusion along the manifold.",
                    "label": 0
                },
                {
                    "sent": "And I started with an initial distribution concentrated on XI will get a distribution of heat and that distribution of heat is given by HT, the heat operator, the diffusion operator applied to Delta X.",
                    "label": 0
                },
                {
                    "sent": "And if instead I took Delta Y and applied the heat operator to it, I would get HT applied to Delta Y, which is the distribution of heat.",
                    "label": 0
                },
                {
                    "sent": "From an initial condition, which was a point mass at Y.",
                    "label": 0
                },
                {
                    "sent": "So basically on this manifold I would like to define a distance.",
                    "label": 0
                },
                {
                    "sent": "I take a point X. I start with an initial pulse of heat.",
                    "label": 0
                },
                {
                    "sent": "At that point, X Ann.",
                    "label": 0
                },
                {
                    "sent": "I allow heat to dissipate over the manifold, so the heat flows along the manifold.",
                    "label": 0
                },
                {
                    "sent": "In the following the geometry of the manifold.",
                    "label": 0
                },
                {
                    "sent": "If there are kinks and bottlenecks in the manifold, it'll slow down there in some directions which are very smooth and flat.",
                    "label": 0
                },
                {
                    "sent": "It will speed up there, so the heat is flowing along the manifold and you end up with the distribution of heat after time T. From initial location X you end up with the distribution of heat after time T from an initial location Y, and you take the Euclidean distance between those two.",
                    "label": 1
                },
                {
                    "sent": "OK, that's what is called the diffusion distance.",
                    "label": 0
                },
                {
                    "sent": "What we will see is that this is essentially.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very closely related to the Laplacian.",
                    "label": 1
                },
                {
                    "sent": "Becausw the.",
                    "label": 0
                },
                {
                    "sent": "Diffusion of heat on the manifold is governed by the heat equation on the manifold in the heat equation on the manifold.",
                    "label": 0
                },
                {
                    "sent": "Is actually given by Laplace.",
                    "label": 0
                },
                {
                    "sent": "F is the partial derivative with respect to time.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way the diffusion Maps as opposed to Laplacian Eigen Maps works is that it looks at the eigenfunctions of the Laplacian an it embeds every point into a lower dimensional space using this particular map.",
                    "label": 0
                },
                {
                    "sent": "OK, so it looks at the Laplace matrix.",
                    "label": 0
                },
                {
                    "sent": "Looks at the eigen values and eigen vectors of the Laplace matrix.",
                    "label": 0
                },
                {
                    "sent": "The eigenvalues are the Lambda one Lambda, two Lambda three and so on and the eigenvectors are F1F2F3 and so on.",
                    "label": 0
                },
                {
                    "sent": "And given a point X, you map it like this and that gives you an embedding for every point X.",
                    "label": 0
                },
                {
                    "sent": "It can be related to taking a random walk on this graph.",
                    "label": 0
                },
                {
                    "sent": "Another two notions of randomness going on.",
                    "label": 0
                },
                {
                    "sent": "So we have to keep our heads clear about that.",
                    "label": 0
                },
                {
                    "sent": "You're taking a random walk on a graph.",
                    "label": 0
                },
                {
                    "sent": "But the graph itself is a random graph, so you have a manifold.",
                    "label": 0
                },
                {
                    "sent": "You randomly sample this manifold and get this random graph by connecting nearby points to each other, and now you do a random walk on this random graph.",
                    "label": 0
                },
                {
                    "sent": "OK. And the essential question.",
                    "label": 0
                },
                {
                    "sent": "Which now is resolved, is whether this walk on this.",
                    "label": 0
                },
                {
                    "sent": "Random graph is close to the actual flow of heat on the underlying manifold.",
                    "label": 0
                },
                {
                    "sent": "And the answer is more or less yes.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this let me give you a let me give you a justification.",
                    "label": 0
                },
                {
                    "sent": "For the Laplacian Eigen Maps algorithm first.",
                    "label": 0
                },
                {
                    "sent": "Remember the high level intuition from where this comes, and this is a very simple justification.",
                    "label": 0
                },
                {
                    "sent": "The high level intuition from where this comes is I have a bunch of points sitting on a manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, and I would like to embed it into a lower dimensional space so I have a collection of points X one through XN an.",
                    "label": 0
                },
                {
                    "sent": "I would map it to a lower dimensional space and so end up with a bunch of points Y one through YN, and in this case we are taking an extreme form of this.",
                    "label": 0
                },
                {
                    "sent": "We are embedding our entire set of points onto the line.",
                    "label": 0
                },
                {
                    "sent": "Into the lowest dimensional space that we can think of.",
                    "label": 0
                },
                {
                    "sent": "OK, the one dimensional space so we map them into Y one through YN an.",
                    "label": 0
                },
                {
                    "sent": "What Laplacian Eigen Maps is trying to do is is trying to preserve locality in the following sense that if XI and XJ were close to each other in the high dimensional space?",
                    "label": 1
                },
                {
                    "sent": "Their image is why I and YJ should be close to each other in the lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, we are looking for a smooth map.",
                    "label": 0
                },
                {
                    "sent": "Because that will actually guarantee that locality preservation.",
                    "label": 0
                },
                {
                    "sent": "Not preservation, but this particular notion of it, an.",
                    "label": 0
                },
                {
                    "sent": "Smooth Maps are going to be given by the eigenfunctions of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is you want to minimize this particular quadratic form.",
                    "label": 0
                },
                {
                    "sent": "So notice that WIJ.",
                    "label": 0
                },
                {
                    "sent": "Is large.",
                    "label": 0
                },
                {
                    "sent": "If XI and XJ are close to each other.",
                    "label": 0
                },
                {
                    "sent": "Right, so when WIJ is large XI and XJ's clothes are close to each other and this forces Y&YJ to be close to each other in that case.",
                    "label": 0
                },
                {
                    "sent": "If WIG is small.",
                    "label": 0
                },
                {
                    "sent": "That means Ixion next year.",
                    "label": 0
                },
                {
                    "sent": "We're not necessarily close to either each other.",
                    "label": 0
                },
                {
                    "sent": "Then we don't care what Huaiyin Ijr.",
                    "label": 0
                },
                {
                    "sent": "We only want to ensure that if points were close to each other in the high dimensional space, they continue to remain close to each other in the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "The argument.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's a fundamental identity.",
                    "label": 1
                },
                {
                    "sent": "This is just a few lines of linear algebra which.",
                    "label": 0
                },
                {
                    "sent": "You're encouraged to skip or try to prove for yourself, but basically that quadratic form which I just wrote down.",
                    "label": 0
                },
                {
                    "sent": "Why I minus YJ squared?",
                    "label": 0
                },
                {
                    "sent": "WIJ ends up being just the same quadratic form is Y transpose LY, so minimizing that is equivalent to taking the eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "Of El.",
                    "label": 0
                },
                {
                    "sent": "And that's how the eigenvectors of the Laplacian show up.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me actually give you an A picture first before I will tell you a little bit about what this might correspond to on the underlying manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, the picture is this.",
                    "label": 0
                },
                {
                    "sent": "Remember the picture of vertical bars and horizontal bars.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have the set of all pictures of vertical bars and we have the set of all pictures of horizontal bars and every picture of a vertical bar is a point in a high dimensional space and every picture of a horizontal bar is a point in high dimensional space, and so suppose I give you all these points.",
                    "label": 0
                },
                {
                    "sent": "OK, I give you a whole bunch of points.",
                    "label": 0
                },
                {
                    "sent": "Some of those correspond to pictures of vertical bars, and some of them correspond to pictures of horizontal bars and you don't know which is which.",
                    "label": 0
                },
                {
                    "sent": "You just have a collection of points in a high dimensional space, and the only thing you know how to do is PCA.",
                    "label": 0
                },
                {
                    "sent": "And you apply PCA.",
                    "label": 0
                },
                {
                    "sent": "The picture on the right is the picture that you will see.",
                    "label": 0
                },
                {
                    "sent": "OK. Another reason you see that picture is as we just discussed, the the set of all images of vertical bars are not a linear space.",
                    "label": 0
                },
                {
                    "sent": "The set of all images of horizontal bars are not a linear space.",
                    "label": 0
                },
                {
                    "sent": "In fact they are.",
                    "label": 0
                },
                {
                    "sent": "2 dimensional manifolds in the higher dimensional space and so trying to linearly fit something through it leads to disaster.",
                    "label": 0
                },
                {
                    "sent": "OK. On the other hand, if you applied Laplacian Eigen Maps to it.",
                    "label": 0
                },
                {
                    "sent": "The picture in the middle is the picture that you will see.",
                    "label": 0
                },
                {
                    "sent": "You see two connected components in your data that you will pick out and one connected component consists of all the blue curves and the other connected component blue dots and the other connected component consists of all the red dots.",
                    "label": 0
                },
                {
                    "sent": "Red is not working on this particular projector, so I think you're going to see this as black dots.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You'll see from Laplacian Eigen Maps that there are two sets of points.",
                    "label": 0
                },
                {
                    "sent": "And these corresponds to the two sets, the vertical bars and the horizontal bars.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I've given you an algorithm and now let me tell you a little bit about what roughly this corresponds to on the manifold.",
                    "label": 1
                },
                {
                    "sent": "So what we're trying to achieve is the following.",
                    "label": 0
                },
                {
                    "sent": "I have a collection of points on a manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, these points.",
                    "label": 0
                },
                {
                    "sent": "This is an embedded manifold OK. And so locally geode esic, distances and Euclidean distances are close to each other, up to a certain order.",
                    "label": 0
                },
                {
                    "sent": "And what I want is a smooth map from M2R.",
                    "label": 1
                },
                {
                    "sent": "If I have a smooth map from M2R.",
                    "label": 0
                },
                {
                    "sent": "This ensures that if two points were close to each other on the manifold, their images under F would also be close to each other.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Close to the time when we should be taking a break.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we take a break and I'll continue.",
                    "label": 0
                },
                {
                    "sent": "Army let me say something for 10 minutes and then we'll take a break.",
                    "label": 0
                },
                {
                    "sent": "OK so?",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose you wanted to actually find this a smooth map from M2R.",
                    "label": 0
                },
                {
                    "sent": "You smooth map from M2R is equivalent, more or less.",
                    "label": 1
                },
                {
                    "sent": "So can be formulated as this.",
                    "label": 0
                },
                {
                    "sent": "Trying to find a function F that minimizes that quadratic form.",
                    "label": 0
                },
                {
                    "sent": "And that quadratic form is roughly approximated by this quadratic form.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is a standard fact.",
                    "label": 0
                },
                {
                    "sent": "If you look at the Laplacian on.",
                    "label": 0
                },
                {
                    "sent": "Him that is actually equal to F times Laplace F on M and that is essentially what is called Stokes theorem.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "On the graph of the function is more than the graph.",
                    "label": 0
                },
                {
                    "sent": "The quadratic form on the left summation, WI, GFI, minus FJ squared is equal to F transpose LF, so finding smooth functions on the manifold require you to essentially look at the eigen functions of the Laplace Beltrami operator on the manifold.",
                    "label": 0
                },
                {
                    "sent": "Finding smooth functions on the graph that means a function is not changing much from one vertex to another.",
                    "label": 0
                },
                {
                    "sent": "That's what the summation, WI, GFI, minus FJ squared is doing.",
                    "label": 0
                },
                {
                    "sent": "Leads you to trying to find eigenvectors of the Laplacian on the graph.",
                    "label": 0
                },
                {
                    "sent": "And the main central question is going to be are those two formally connected in the sense that does one converge to the other and we will see that that is true.",
                    "label": 0
                },
                {
                    "sent": "That's all I wanted to see.",
                    "label": 0
                },
                {
                    "sent": "And then maybe we take.",
                    "label": 0
                },
                {
                    "sent": "Oh 15 minute break.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'll begin very soon, and what we've decided.",
                    "label": 0
                },
                {
                    "sent": "Given that things are taking a little longer than we expected is, I will speak for 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "I just want to communicate one point to you.",
                    "label": 0
                },
                {
                    "sent": "And I'll speak.",
                    "label": 0
                },
                {
                    "sent": "Hopefully, 10 minutes will be enough to communicate this point.",
                    "label": 0
                },
                {
                    "sent": "Which is what is the relationship between the graph Laplacian and the manifold Laplacian?",
                    "label": 1
                },
                {
                    "sent": "Does one converge to the other?",
                    "label": 0
                },
                {
                    "sent": "And the answer to that is yes.",
                    "label": 0
                },
                {
                    "sent": "So I just want to make this one point.",
                    "label": 0
                },
                {
                    "sent": "And after that Misha will take over an.",
                    "label": 0
                },
                {
                    "sent": "It is now.",
                    "label": 0
                },
                {
                    "sent": "1033 so.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I'll stop speaking bye.",
                    "label": 0
                },
                {
                    "sent": "And in 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "And so somewhere around 10:40, ten 45 Misha begins and goes to the end of our allotted time.",
                    "label": 0
                },
                {
                    "sent": "That's the plan.",
                    "label": 0
                },
                {
                    "sent": "OK, and he'll tell you about some other aspects of this framework, especially as it applies to clustering and semi supervised learning and so on.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So now in the next 10 minutes I just want to communicate to you one basic theoretical result.",
                    "label": 0
                },
                {
                    "sent": "OK, and that theoretical result now can be, I think simply stated.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we understand now, roughly speaking, what the manifold Laplacian is.",
                    "label": 1
                },
                {
                    "sent": "We see why it arises if we want dimensionality reduction if we require smooth Maps, we see that the eigenfunctions of the Laplacian will give us functions that are geometrically adapted to the geometer, adapted to the geometry of the manifold.",
                    "label": 0
                },
                {
                    "sent": "And I just want to workout an exact formula for the case of the circle.",
                    "label": 0
                },
                {
                    "sent": "So that you see very clearly that classical Fourier series is the eigen functions of the Laplacian where the manifold in question is just the circle.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is we know what the given an arbitrary Romanian manifold.",
                    "label": 0
                },
                {
                    "sent": "We know what the Laplace Beltrami operator is on this manifold, and you can look at the following Eigen system Laplace F is oh that's incorrect.",
                    "label": 0
                },
                {
                    "sent": "So Laplace F is equal to Lambda.",
                    "label": 0
                },
                {
                    "sent": "F and the eigen functions are fees and the eigenvalues are Lambda eyes, so that should be Laplace fee.",
                    "label": 0
                },
                {
                    "sent": "I is equal to Lambda IVI.",
                    "label": 0
                },
                {
                    "sent": "OK, and these fees form an orthonormal basis for L2 of M. And the eigenvalues characterize the smoothness of these fees, so the fees are ordered, roughly speaking, by smoothness to the smoothest ones come before and and when I say smoothness, I don't mean number of derivatives, they're all see Infinity.",
                    "label": 1
                },
                {
                    "sent": "Typically what I mean is actually this statement, which is at the bottom the norm of.",
                    "label": 0
                },
                {
                    "sent": "The gradient.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's just work one through 1 example just so that we see this clearly.",
                    "label": 0
                },
                {
                    "sent": "What this means.",
                    "label": 0
                },
                {
                    "sent": "So let's work on the case of circle.",
                    "label": 0
                },
                {
                    "sent": "So the manifold is the circle and now consider functions.",
                    "label": 0
                },
                {
                    "sent": "Whose domain is the circle?",
                    "label": 1
                },
                {
                    "sent": "So these are functions.",
                    "label": 0
                },
                {
                    "sent": "On the circle now the set of functions on the circle are in fact the periodic functions.",
                    "label": 0
                },
                {
                    "sent": "And we already know that the periodic functions admit a Fourier series expansion and can be written in terms of sines and cosines.",
                    "label": 0
                },
                {
                    "sent": "So now we are asking the question given the circle.",
                    "label": 0
                },
                {
                    "sent": "What is the Laplace operator?",
                    "label": 0
                },
                {
                    "sent": "On the circle.",
                    "label": 0
                },
                {
                    "sent": "So think of the circle.",
                    "label": 0
                },
                {
                    "sent": "What is the tangent?",
                    "label": 0
                },
                {
                    "sent": "The circle is a 1 dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "What is the tangent space to any point on the circle?",
                    "label": 0
                },
                {
                    "sent": "The tangent space to any point on the circle is a 1 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It's essentially a line.",
                    "label": 0
                },
                {
                    "sent": "OK. And so, given a circle given a function defined on the circle, I can write this function as a function of key to.",
                    "label": 0
                },
                {
                    "sent": "In this case I've written it as a function of T. OK, and I what I want to do is differentiate the function twice.",
                    "label": 0
                },
                {
                    "sent": "On the tangent space, and so differentiating the function U twice along the tangent spaces, just differentiating you twice with respect to T and the eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "Is this du Laplace U is exactly that formula.",
                    "label": 0
                },
                {
                    "sent": "And Laplace U is equal to Lambda.",
                    "label": 0
                },
                {
                    "sent": "U is the eigensystem that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "And this many of you have seen, presumably.",
                    "label": 0
                },
                {
                    "sent": "In calculus, at some point I don't know.",
                    "label": 1
                },
                {
                    "sent": "The eigenvalues are N squared and the eigenfunctions are sign in of T and cosine NFT so the eigenspaces have dimensionality to the multiplicity of the eigenvalues is 2.",
                    "label": 0
                },
                {
                    "sent": "And you see that the Laplace F is equal to Lambda F. The eigenfunctions of the Laplacian on the circle are sines and cosines, and sines and cosines are exactly the right sort of functions that are adapted to the geometry of the circle.",
                    "label": 0
                },
                {
                    "sent": "So in a high level.",
                    "label": 0
                },
                {
                    "sent": "In a picture that I want you to think in your head is this.",
                    "label": 0
                },
                {
                    "sent": "If all your data points lived on a circle embedded in a million dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, all your data lived on a circle embedded in a million dimensional space.",
                    "label": 0
                },
                {
                    "sent": "What kinds of functions would you like to use to build classifiers?",
                    "label": 0
                },
                {
                    "sent": "And it seems natural to think that the right kind of functions would be made up of sines and cosines, because all the data lives in this circle and the wrong kind of functions would be.",
                    "label": 0
                },
                {
                    "sent": "Linear projections.",
                    "label": 0
                },
                {
                    "sent": "Because the ambient coordinate system has no natural relationship to the geometry of the data which is changing along the circle.",
                    "label": 0
                },
                {
                    "sent": "So if I knew the manifold on which the data lived.",
                    "label": 0
                },
                {
                    "sent": "The eigenfunctions of the Laplace operator would give me a set of basis functions that are adapted in some way to the geometry of the manifold in the same way in which sines and cosines are adapted to the geometry of the circle.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Those are the functions I will use to build classifiers.",
                    "label": 0
                },
                {
                    "sent": "Now I don't have the manifold.",
                    "label": 0
                },
                {
                    "sent": "I don't So what I have is a collection of points on the manifold.",
                    "label": 0
                },
                {
                    "sent": "And I can build a graph out of these points and I can look at the Laplacian on this graph, which is an operator on functions defined on the vertex set of the graph.",
                    "label": 0
                },
                {
                    "sent": "OK. And now I can look at the eigenvalues and eigenvectors of this.",
                    "label": 0
                },
                {
                    "sent": "And from this reconstruct the eigenvalues and eigenfunctions of the Laplacian.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add a number of theorems.",
                    "label": 0
                },
                {
                    "sent": "And let me just write this on the board.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the basic theoretical result that justifies why this procedure might be well founded in more than a heuristic.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have here a manifold MI have Laplace F is equal to Lambda F. This is the eigensystem that I'm interested in that gives me eigenvalues Lambda eyes, which actually are in increasing order on, and it gives me eigenfunctions afei.",
                    "label": 0
                },
                {
                    "sent": "It may have eigenspaces if their multiplicity and so on, but say it gives me eigen values and eigen spaces.",
                    "label": 0
                },
                {
                    "sent": "This is my manifold.",
                    "label": 0
                },
                {
                    "sent": "I get a bunch of points sampled on this manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so I get X1 through XN.",
                    "label": 0
                },
                {
                    "sent": "OK, where each XI is a randomly drawn point sitting on this manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, and this manifold itself is sitting.",
                    "label": 0
                },
                {
                    "sent": "In Rd, so each of these XX one through XN is in M, which is in Rd.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now I've got a graph G is equal to VE.",
                    "label": 0
                },
                {
                    "sent": "The vertex set the size of the vertex set is equal to N. Because there is one vertex per datapoint.",
                    "label": 0
                },
                {
                    "sent": "OK, I can look at functions from V2R.",
                    "label": 0
                },
                {
                    "sent": "Functions defined on the vertex set of the graph.",
                    "label": 0
                },
                {
                    "sent": "I can also look at functions from M to R, which are these functions.",
                    "label": 0
                },
                {
                    "sent": "I have an operator on such functions which is given by the Laplace Beltrami operator.",
                    "label": 0
                },
                {
                    "sent": "I look at these kinds of functions functions from the vertex set to R and I have an operator defined on these functions which is given by the graph Laplacian to any such function.",
                    "label": 0
                },
                {
                    "sent": "I can apply this.",
                    "label": 0
                },
                {
                    "sent": "Such a function is just a vector.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a collection of.",
                    "label": 0
                },
                {
                    "sent": "One number per vertex, so this is I.",
                    "label": 0
                },
                {
                    "sent": "Identified with R2 the V. OK, I can look at LF.",
                    "label": 0
                },
                {
                    "sent": "OK. And here is.",
                    "label": 0
                },
                {
                    "sent": "The following random matrix result.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                },
                {
                    "sent": "Says the following.",
                    "label": 0
                },
                {
                    "sent": "Take X one through XN in M. In Rd.",
                    "label": 0
                },
                {
                    "sent": "Consider the matrix L, which is equal to D -- W. Where there is a parameter T. OK, so WIJ is equal to E to the power of minus XI minus XJ squared over 40.",
                    "label": 0
                },
                {
                    "sent": "And this is I think, 4 by T to the D by 2 * T. Something like this, OK?",
                    "label": 0
                },
                {
                    "sent": "So there's one parameter T, and this is a random matrix Lt, right?",
                    "label": 0
                },
                {
                    "sent": "I sample X one through XN uniformly safe from him to keep things simple.",
                    "label": 0
                },
                {
                    "sent": "And I get this random matrix and I look at the eigenvalues and eigenvectors of this matrix and the theorem says that let Lambda K be the Kate eigenvalue of this matrix.",
                    "label": 0
                },
                {
                    "sent": "There's this random matrix LK.",
                    "label": 0
                },
                {
                    "sent": "This is going to be a function of T, an A function of N. Because N is there and number of randomly drawn points that I've got anti is this parameter that Lambda KTN is going to converge to.",
                    "label": 0
                },
                {
                    "sent": "Lambda K. As T goes to zero, an N goes to Infinity, so as the number of data points goes to Infinity and I let T this parameter, which is actually the localization parameter, go to zero at suitably chosen rate.",
                    "label": 0
                },
                {
                    "sent": "The spectrum of this random matrix is converging to the spectrum of the Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And in a different sense, the eigenvectors of this matrix are converging to the eigenfunctions of the Laplacian and the rate at which all of this happens, and this is important.",
                    "label": 0
                },
                {
                    "sent": "Is 1 / N. Do the 1 / D. Anet does not depend.",
                    "label": 0
                },
                {
                    "sent": "On capital D. So the rates.",
                    "label": 0
                },
                {
                    "sent": "Do not depend upon the ambient dimension at all, but depends only on the intrinsic low dimension.",
                    "label": 0
                },
                {
                    "sent": "So in other words, if all the data lives in a high dimensional space but sits on a low dimensional manifold, you might be fooled into thinking Oh my God, I'm confronted with all this high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "I have the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "But in fact becausw there is only.",
                    "label": 0
                },
                {
                    "sent": "A small number of degrees of freedom that have generated the data.",
                    "label": 0
                },
                {
                    "sent": "Roughly little Di degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "All the rates will be governed by little D. And that's the sense in which you can make precise the notion of beating the curse of dimensionality if nature is.",
                    "label": 0
                },
                {
                    "sent": "Helpful.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is also the sense in which you can now get.",
                    "label": 0
                },
                {
                    "sent": "Geometric.",
                    "label": 0
                },
                {
                    "sent": "Lee structured functions, with which you can do harmonic analysis on the manifold with which you can build classifiers with which you can do dimensionality reduction and all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "Because this is an empirical object, the graph which you have in your hands and everything you want to recover about the manifold you recover from this graph.",
                    "label": 0
                },
                {
                    "sent": "So at this point I'll just stop, and that's the only really remark.",
                    "label": 0
                },
                {
                    "sent": "I want to make about.",
                    "label": 0
                },
                {
                    "sent": "About the use of the graph Laplacian and its relationship to the manifold Laplacian.",
                    "label": 0
                },
                {
                    "sent": "At this point, Michelle will just carry on to elaborate more on the theme of the Laplacian and so on.",
                    "label": 0
                }
            ]
        }
    }
}