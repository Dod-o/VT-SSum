{
    "id": "w53tumrndztt5aqvnpgcbelief4h2emu",
    "title": "Learning and Prediction - A Survey",
    "info": {
        "author": [
            "Kristiaan Pelckmans, Department of Information Technology, Uppsala University"
        ],
        "published": "July 20, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mla09_pelckmans_lap/",
    "segmentation": [
        [
            "Thanks David, thanks let you go to invite me over here today.",
            "I will speak a bit about learning and prediction.",
            "Hope to give you a flavor of what we machine learning people are doing and I hope to trigger some interest that you can say OK this looks interesting.",
            "Maybe we can look bits in that.",
            "So it's a survey really, but just want to you know.",
            "Be brief and some points which I think are really crucial, so I don't want to give a perfect survey of the field.",
            "My name is Christian book Mills I my PhD in Belgium and is it?",
            "In the engineering Department and their studied basically machine learning.",
            "But applications to system magnification and engineering topics at nowadays I'm assistant professor in Sweden in University and this is also like an engineering Department.",
            "And while I'm doing the more more like the filtering stuff later on in the talk I will give some examples of this and how you can apply machine learning techniques."
        ],
        [
            "This area.",
            "OK, an overview of this talk.",
            "This talk of consists of two parts, an an in the first part like today I will just give a Birds Eye view on the machine learning about machine learning has to offer to you, so I will try to convey the view that machine learning has different views.",
            "Basically on the same topic.",
            "So fuse of stochastic based on stochastic error.",
            "So classic POV on function approximation POV.",
            "I will say something about.",
            "Online learning, which I think it's very relevant for for you guys, and maybe a briefly mention something in optimization.",
            "If time permits, I will go into some algorithms today, but in recognition our mention the support vector machine the previous speaker also used and will say something about recursion and tomorrow I will do some some deeper into new areas which we are exploring, which is the first is reliability analysis like you want to analyze how reliable a certain mechanic part is and what how machine learning can help you there and the last one is system identification.",
            "So we have a dynamic system like is involved OK. How can machine learning help is there but this feature?"
        ],
        [
            "Tomorrow."
        ],
        [
            "So First Birds Eye view.",
            "So the first thing is people.",
            "Well, they tend to ask.",
            "OK, what are you doing in your research?",
            "And it's always difficult to explain what we're really doing because you can't go with mathematics directly, so I used to use this example.",
            "So we like to make mathematical models of observations.",
            "I mean, that's the short thing, so the observation could be like a flock of birds, and you observe birds in real life.",
            "And then you try to come up with a model tried to capture the essentials of the of the bird.",
            "So not necessarily like to mimic all aspects of birds like defenders and everything, but you just want to do the abstract meaning like OK, you won't have some lifting process and you try to come up with a model like this mounting.",
            "So if you have a model, you can implement it of course, but we people are just interested in OK in the model per say, like in the mathematical description.",
            "And why are we interested in a mathematical description?",
            "Because you want to have insight in the essentials of the flying, for example of the birds.",
            "So we don't talk about feathers and all that, but you want to talk about, for example, lifting these aspects.",
            "So we want to have insights we want to have a grip on power meters.",
            "What are the elephant?",
            "Images for.",
            "About to fly, it can't be too heavy, for example.",
            "And Thirdly, and that's really important aspect, we want to make predictions about the future outcome of our model.",
            "The prediction of this thing is hopefully it will fly somewhere, but well later on we will see more refined notions of predictions and at the end of the day we not only interesting predictions predictions that machine learning people are interested in, but people in engineering context.",
            "They're more interested in like design control strategy.",
            "They want, for example, they have a plant or something in their designs, or David Blaine and I want to control, for example, the special effects they want to minimize.",
            "Or to maximize efficiency, so they want to apply.",
            "Optimal inputs such that the yield of the planet of the model is perfect and this is an important distinction prediction.",
            "That's a relatively easy task.",
            "I mean, you can frame it in stochastic context and all that, but control it's really about causal behavior, so that's the next step beyond step of collection is just academic step, I would say, but the step towards control into about causal modeling is the big challenge."
        ],
        [
            "Engineering context.",
            "OK, so this is my.",
            "Well just a picture, say machine learning as we used to do it.",
            "In short, it's like on the Internet intersection on the grocers between application studies.",
            "For example, in my in computational biology, statistics and optimization, we engineering people like to optimize everything.",
            "So optimization is really crucial parts here."
        ],
        [
            "Hello.",
            "OK so I will talk about machine learning from majorly stochastic context, but machine learning you can explain the topic on many different points of views and one point of view which is very.",
            "It's a very popular those days.",
            "Is few machine learning from point of view of information theory, but of course you have also the data money branch and I like to see what machine learning people do in context of algorithms they want.",
            "For example, the clustering is typically orgasm POV.",
            "They want to come up with an algorithm which tends to give some insights in the data but OK, but also the senior processing area like.",
            "More developed filtering area they talk about machine learning, which is completely different language, but in essence it's all about the same thing.",
            "Also in optimization theory and optimization modeling, they tend to talk about machine learning algorithms and how to do it efficiently in practice.",
            "There's also a very mathematical branch of operators to basically inverse problems, which tend to look at the same problems with a different language, and you have also game theory.",
            "So these are regulations related topics, but I won't go into detail in those, but I will take the stochastic."
        ],
        [
            "I wrote many.",
            "So what I will do in this talk I will give you a flavor of machine learning can offer you and I'm route of more or less in the theoretical point of view, so immediate questions.",
            "Why theorems are?",
            "Why are theorems of interest to us?",
            "I mean, and that's not only a question one ask at in a bar or something, but that's a very relevant question for us people.",
            "Anne.",
            "So the first thing is teams are first like a sound check.",
            "If you can give a good team for an August in which you came up with it's worth more than like you Ristic, because you can guarantee that.",
            "It will mostly do the right thing.",
            "I mean if it's puristic then you can just give it a try in the machine and see what MATLAB gives you.",
            "But the theorem is basically a sanity check that's important motivation and the second thing is that making terms and thinking about how to derive theorems and proofs.",
            "It really gives a controls are in the environment, meaning that.",
            "The language of mathematics is really very controlled language.",
            "You can't do many things wrong.",
            "I mean, there are clear rules and you can't just neglect the rules.",
            "You have to follow the rules so it's controls are in the environment first.",
            "Um?",
            "So my three favorite theorems, which I will talk about a little in this presentation, are the mistake band for the perception, and this was the very first formal results in machine learning.",
            "It's now about 50 years old.",
            "Well, the perception was presented in 57.",
            "If I correct the bound came years later, but it's really the very first formal result, so I think it's important.",
            "It's very simple.",
            "You know it's just an application.",
            "Of course this was basically, but it's really captures what it's all about.",
            "The second one is he presented to you.",
            "This is a theorem which well originated from a more functional analysis context, but it is not very relevant, especially in the context of support vector machines.",
            "And this also says something about the nature of learning of machine learning, and I will say something about concentration bounds.",
            "Very briefly.",
            "So intuitively, but I think it's important because it says what stochastic modeling can do for you."
        ],
        [
            "OK, so that's a bit about how to introduce this topic, and if you keep on abstracting the topic.",
            "I mean this is really the problem we are all looking at.",
            "It's a problem.",
            "Suppose you have an unordered set of observations.",
            "So what can you say about next one?",
            "It's meaningless problem, but if you assume certain structure of your data, if you wish to make some assumptions of how the data originators how, which are the rules to play with, then you can say something about about a new observation.",
            "But if you're not willing to make any assumption at all this pointless question.",
            "I mean, that's really important and important here this session."
        ],
        [
            "So there was this guy George Box in the 60s made limit this quote which I found quite you know, let's say so.",
            "Essentially all models are wrong.",
            "But some are useful, and that's I mean, I think that's really saying the whole thing.",
            "So we said that already 40 years ago.",
            "But it's more children ever, so by the way, this is not George Box.",
            "I didn't find the picture of him, but his relative Cox, which is the closest in Hamming distance, well anyway.",
            "An so we set in the context of closed from parametric statistical models basically, but it's also a absolutely true for the context of our distribution.",
            "Free learning and learning theory.",
            "So we like to make models and we know they are not true.",
            "I mean models don't exist, they are just abstractions of reality.",
            "But some can be useful and they'll azatian grew more and more in last years that you have to specify your model, you have to talk with your model based on.",
            "What useful means for you?",
            "So you have to define useful for me is OK. Making good predictions in this norm, for example.",
            "That's useful.",
            "And then if you have this notion of useful then you can come up with some models.",
            "It's not the other way around.",
            "This not like OK, we have this nice methods lying around or we found on the Internet and we can maybe apply it.",
            "I mean, but you really should think just away what is used for your application and if you know what useful is then you can start thinking about a good model."
        ],
        [
            "OK, this is kind of heavy slides.",
            "I put it in the beginning because now you're awake, hopefully, but I think it also captures the more formal thing of.",
            "Of stochastic inference basically.",
            "So one of the main teams I want to explore here is the difference between stochastic and deterministic inference and in stochastic inference.",
            "Well, everybody of new of you know probability.",
            "But basically you make the assumption that you can associate to every set of probability.",
            "So you map a set to a number between zero and one, and now there's different.",
            "Well, there's huge debate of what these numbers are between zero and one mean.",
            "Like there's the frequency approach, so one is just the number of times.",
            "An event will occur in the set or not, but you also have to upload the Bashan approach that well.",
            "The zero one thing is just it's a belief, so if you don't believe in some events in some sets then you say, well, just close to 0 if you believe it's highly likely that event or fall on the they give you the number to one.",
            "So these are just let's say, more formal discussions, and there's a huge debate.",
            "But more practically, practically relevant, I would say is that even if you make the assumption of such a probability, then you still have to assume more.",
            "You have to assume IID exchangeable, and this is really very relevant, because we engineers tend to work in time series analysis like in dynamical system, and the context of where the assumption of ID data for exchangeability is not really very natural one.",
            "So what is it about it saying that OK, if you have any observations, the probability that all this info in the set you can factor as in different.",
            "You know in the probability that the first one falls in the set times probability of them finds instead, so it can do this factorization.",
            "So meaning that if you have a bunch of observation, each observation is worth as full information, basically.",
            "So it's very relevant if you have.",
            "A set of like 10 million data points, but they're all the same.",
            "There's no information in them, so that's the case where ID is, well, it's very tricky.",
            "So the assumption of ID is very relevant.",
            "One very important one, I would say.",
            "And you can replace this assumption by exchangeability, which is slightly weaker.",
            "It's saying that, well, the probability of a set is the same as any permutation of probability, but it's basically the same as IID.",
            "And this well, if you have this notion of probability, you can also come up with the notion of expectation, and this formula is probably one of the most well.",
            "Well, this is.",
            "One of the most important in probability.",
            "So it says that the probability that some observation will fall on the set is equal to the expectation of the indicator.",
            "An indicator says just one if it's true.",
            "So if it's not true, so the expectation of the indicator is equal to the probability, and that sounds like very mathematical or formal thing which you can write down just like that.",
            "But it turns out that you can start your complete framework of probability like.",
            ", cough and all those guys in their 30s and 40s.",
            "It's with defining probability.",
            "Or you can do it with defining expectation.",
            "So and that's well, that's quite crucial.",
            "There's a famous book by Peter Whittle from Cambridge who started.",
            "We started doing this and you started OK. Let's define a notion of expectation, and then everything follows.",
            "Like the complete context of probability of stochastic models of statiscal inference, everything follows, so this is like more than a formal statement.",
            "OK, so this is stochastic framework, but.",
            "If we're not willing to assume the stochastic framework if we don't believe in some probability which we can, you know which captures the data, then we say just a deterministic framework and we don't make any assumptions at all.",
            "Later on, I will say which how you can still overcome just prediction problems.",
            "So remember, if you don't make any assumptions on your data or how this data behaves, then you cannot make any predictions."
        ],
        [
            "At.",
            "OK, So what I will do now is go a bit in detail in different flavors of machine learning which are out there and they all basically capture the same of machine learning.",
            "So I will say something about this inference about optimization about function approximation, online learning, and stochastic inference.",
            "So."
        ],
        [
            "First, I will say something about pragmatics artistic influence, which is the oldest flavor out there.",
            "So this was basically the POV Gauss and all those people handle apostate.",
            "They opted, it says that.",
            "Well, if you have a bunch of observations, let's make.",
            "Let's find probabilistic model like P. Of such detail, certain parameters which do explain the data the best and the important thing is that they didn't have any notion of goodness of fit or least square or whatever, but they are talking about the maximum likelihood, which of course off you know it's like supposed to have.",
            "The different dots are samples.",
            "If three models like Direct Daniels like the Blue retired the green rectangle.",
            "And the hit rectangle.",
            "Then if you maximize.",
            "Over 123 divine.",
            "We just want to three.",
            "If you maximize the probability.",
            "The data under this model.",
            "Then it's clear that this one is only one which only model which explains his data based.",
            "Otherwise, suppose it's mixture of them all, then, well, you have this constraint or the probabilities sum to one, and does he have like rectangles which which goes to 1/3 of them?",
            "So the maximum likelihood will be small, so maximum likelihood.",
            "Basically select this guy, but it does not do so by least square criterion or by closing at noon or so, but it does so by just maximum like loot.",
            "And this is also very well let's say small example like academic example.",
            "But if we start talking about more complex models like more complex expression for P like for example the caution then we can do the same and then it turns out that you can.",
            "It's not about selection of the model, but it's about inference of the parimeter basically.",
            "Later on we will talk something some slides about classification and there we have basically two different probability distributions which you want to fit at the same time, and the classical methods of classification.",
            "If you go to statistical literature or the MATLAB toolbox for statistics, they have linear discriminant analysis or cutey cutey a logistic regression.",
            "They basically implement the same, but doing it with two different distributions 1/3.",
            "Blue guys, one for the red guys.",
            "The thing to remember here is that.",
            "In the statistical context, we like the model which explains the data best, so it's not about the best prediction, it's about explaining which it is good for you."
        ],
        [
            "So this is also maybe.",
            "A bit heavy slide, but I want to go into it because it's relevant for many, many problems.",
            "We are looking in machine learning.",
            "I mean it captures account amid the essence of what machine learning is about.",
            "So what do we see here?",
            "We see three plots.",
            "The first note is only like a 2 dimensional sample.",
            "The blue dots are sampled well.",
            "It's of course Gaussian here.",
            "But what we do there is we don't want to fit a Gaussian distribution explaining this data, but we say we're interested in this rectangles and what is the probability that a new data point falls in the certain rectangle?",
            "For example, probability that it's over.",
            "Mobility that it will fall in the left in this rectangle is fairly small, because why is intuitively because it contains a low portion of the data points, but the probability that falls in this big time is large.",
            "So that's the difference between statistical inference where we want to go and put some other an like distribution free inference so, and we can do it for rectangles, but it's it's well.",
            "It's basically a very.",
            "Let's say it's up to you which shape you like.",
            "So if you like to go for more complex shapes, then you can do the same inference.",
            "But the point is that the more complex the shapes are you're interested in how they model your probability well, the more you have to pay, basically and less certain you will be about the exact conclusion.",
            "So you make.",
            "So if you have only like 12345 rectangles, then the conclusions you have to pay a little bit because you have like.",
            "Different.",
            "Conclusions you want to make, but if you have like all the possible apology, but it comes, then you have to pay a lot.",
            "So and that's basically the.",
            "One of the main insights in well in machine learning theory.",
            "Basically, if we look at more complex models, these represents counter models.",
            "Then we have to pay in certainty, so we are less certain about more complex models that we are certainly for simpler models like the rectangles are good, but the polygons not so.",
            "So also a difference in just the statistical inference thing and as distribution free.",
            "Interesting is that in statistics there interested in what happens if goes to Infinity.",
            "So if your data points if your data set becomes very very large, I mean we're interested in limit distribution and all that while in machine learning in the theory of learning.",
            "We are more interested in what happens for finite samples of data, so if any is like fixed, like in this 100, how does you know the probabilistic guarantee look like?",
            "So that's the difference between sadistic answer to stick."
        ],
        [
            "In theory.",
            "So this looks like very abstract example, but found, as I mentioned already, it will find its use in many many different areas of statistics of where of handling data and one area which is particularly intuitive I would say, but not very well explored to date is the area of multiple testing.",
            "So what's this?"
        ],
        [
            "Not protesting, let's go back to this example an we want to test whether you know, for example, this rectangle contains more than half of the probability mass of the underlying data.",
            "So for this one is contains clearly very low probability mass because it will contain low number of samples.",
            "But this big rectangle it will capture.",
            "Well, lots of probability mass, so that's that's a simple example of multiple testing and the point I want to make here is that if we want to go for multiple this thing we have to pay for the multiplicity of, well, how many tests we want to make basically.",
            "But important is that this rectangles if they are nested, they are not independent, so we have to count the number of independent non overlapping rectangles.",
            "Basically because if they are independent you can figure out the probability is just by looking at how they are nested and and all that, so we don't have to count really the number of rectangles, but we have to count the number of rectangles which are non overlapping.",
            "And the same here.",
            "But I will make this issue bit more formal in a moment."
        ],
        [
            "And again, this is an academic example, but the example which is of interest is whether two probabilities are the same, for example.",
            "So for example, if you look at the devices behavior still the same is the rules underlying this device still the same?",
            "So we make measurements we sample like this device from time to time, and we give a warning when the probability distributions are different and testing for difference of probability distribution can be done by testing.",
            "By using this multiple testing approach.",
            "Yeah.",
            "So.",
            "I mentioned it's not well researched.",
            "It's fairly open topic, but there are some very deep mathematical results coming up like how you have to combine multiple testing procedures and how you have to adapt the testing outcomes.",
            "Well, as I say, the P values for this if you do multiple testing and the crucial name there is Benjamin quite mathematically oriented.",
            "Search OK so."
        ],
        [
            "Well, I will say here and this isn't even simpler.",
            "Example, it's like it's looking like a 1 dimensional example, so the X axis is.",
            "The index is just indexes, but does it is is a random variable in R. And we have.",
            "We want to look at the probability of of the innovative random sample.",
            "So what we do?",
            "Obviously we do it in MATLAB all the time.",
            "Is we look at the histogram of the sample, but the histogram is somewhat.",
            "I'll talk.",
            "You have to play with some para meters you have to play with in which you have to say OK, how many bins do we want to have?",
            "So so it's you know it's bit weekly so, but there's very precise mathematical way to deal with with distributions and how to.",
            "Estimated from data is called the empirical distribution function is this guy.",
            "It's not looking at rectangles as in history here, but it's looking at the open half planes.",
            "It's well, let's say.",
            "Instead of looking at this rectangle, it looks at what is probability.",
            "Most of the data?",
            "Well, what's the probability of a data point being on the right hand side and then you can.",
            "There's only one perimetre there, like the threshold and you can just visualize the thresholds on the Xbox and hear the the probability you measure on the data.",
            "So the point I'm trying to make here is the empirical distribution function.",
            "It's really the core of statistics, is really the core of what we're doing in machine learning.",
            "Later on we will look at more complex functions instead of rectangles, but it's really about the same, and people notice empirical distribution function already for a long time, like 70 years.",
            "There are famous terms like having Coke on telly supporting this, this plot and what they mean, an interpretation.",
            "And still to do the the the big chunk of statistical learning theory can be phrased as you know modification, slight modification of what this is about about empirical distribution function."
        ],
        [
            "OK. Yeah, but what I will do here is OK, so it's this.",
            "Model inference problems as a stochastic inference problems with empirical distribution function, I will say something here now about empirical risk minimum."
        ],
        [
            "Addition, and this is a slight I used when I use in all my presentation as one of the first slides.",
            "I will think it captures the notion of empirical risk minimization.",
            "So what we try to do is try to come up with some models with some functions F which captures somehow the relation between X&Y.",
            "So the first thing is we have to assume that X&Y, that come from an underlying distribution which is fixed but maybe unknown and we have to make the assumption of ID and again later in time series analysis.",
            "In dynamical systems this assumption is, well, we need some modification for that, but for the core of machine learning it's really very important assumption I would say.",
            "What we want to do is we want to make a mapping F from X to Y.",
            "So we call X like inputs while like output and for example in pattern recognition.",
            "Why is minus one one?",
            "For example, it says whether it's good or not, good weather, some parameters were feasible for implementation or not.",
            "I mean you can give various interpretation, but abstractly it's just like why is minus one one in the rehashing context.",
            "We say that why can be anything.",
            "Count can be continuous variable by the way.",
            "I used the statistical notation here.",
            "Like if we use capital letters X and why it says that X&Y are random variables.",
            "Basically, they're not just quantities, they're not just factors, but they have a probability attached to it.",
            "OK.",
            "So you have to think about OK. What is a good?",
            "How would the good model for me look like?",
            "It's like a linear model.",
            "A good idea like just some linear parameters.",
            "Or do we have a complex PDE with some unknown by meters?",
            "This might be the model class of interest.",
            "So how does F look like?",
            "What is the?",
            "Called the hypothesis space.",
            "What is the set of all possible outcomes which we can expect without seeing this data?",
            "Basically, so we have to think about F. We have also think about loss.",
            "Scoot mean first and now.",
            "That is important.",
            "That is mainly the difference of wealth.",
            "Artistical modeling as I showed a few slides back and empirical risk minimization here we explicitely quantified the notion of good.",
            "We say good for us means like we have a low error.",
            "I mean that's what we engineers understand fairly well.",
            "Good first means like we have a small uncertainty for example.",
            "There were these papers of Markowitz in stock market in the portfolio selection and say good for me means like if you have a lower risk of losing a lot of money.",
            "So any made this whole theory based on the notion of goodness?",
            "So the question, now that's a question to you guys.",
            "You want to implement like for example a machine learning technique.",
            "What does good mean for you?",
            "What is more interested in the core business of your estimates are more interested in good prediction accuracy.",
            "What is good for you?",
            "So what's the hypothesis?",
            "Was this the last function?",
            "OK, So what empirical his position is about.",
            "Just tries to come up with a model with the parameters optimal tries to play with the parameters, so it's just expected loss is minimal and expected loss.",
            "It basically says if you have a new random variable, what will those be without seeing this random variable, what do we expect from future behavior and now risk minimization is just minimizing this expected loss.",
            "We will say this expected loss.",
            "That's the risk of the model, so we have this risk minimisation.",
            "But of course we don't have access to the underlying probability distributions and all that, so we replaced expectation by empirical average basically.",
            "So we can place this expectation by an average and we have the empirical estimates of health.",
            "So what learning theory is really about is it tries to characterize how closely this F head is through this App Store.",
            "Very simple questions, but the main point is that it's really up to you, but it means good and risk is about in your model.",
            "What model structure is about.",
            "At."
        ],
        [
            "So for example, in the classification task we have like plus 1 -- 1 as I said as an artist like OK Now for example apart devices, good device is not good, for example.",
            "Well this certain gene causes this phenomena like cancer or this gene doesn't, so it's plus 1 -- 1 in abstract testing and we have some inputs X's moment of our D. So again, we assume IID very important.",
            "And then.",
            "If you make the assumption that we can separate basically the two classes, what does that mean?",
            "Suppose we have two samples.",
            "And if we find like a linear model which you know sets 2 examples are on either side of the after decision, then we say that it's realizable.",
            "So and that will be the crucial formal notion in classification, and there's a lot of machine learning theory involved with whether it's realizable case and then you can have very fast algorithms if we cannot make this assumption that we can separate this out, like in expectation like from assumption, then our algorithms for work slower and they have to take into account lots of issues.",
            "So if you assume it's realisable OK, so that's the classical assumption, then we will define the notion of his query, make mistakes, so whether W transpose X will be the same sign as Y, and empirical risk is just had this probability replaced by this expectation.",
            "Remember again here we have used this quality, that probability is equal to the expectation of the indicator variable, so so that's the step we make from history.",
            "Bigger risk.",
            "In Bucharest, minimization strategies like for example, the sportfisher machine, as I will talk later about it, just minimizes empirical risk like.",
            "And they would also like boosting.",
            "Methods and, well, different methods can be argued from this context.",
            "So here again, the aggregate is 0 if it's not true, and this one if it's true.",
            "So if we make them bigger, risk just counts the amount of mistakes.",
            "Anne.",
            "And the task is and if it gets a new point and we make a prediction, what's the probability of making a mistake?",
            "OK, makes it."
        ],
        [
            "So my first team like result is basically like on concentration and what it does say is it just bounced the probability of making a mistake on future data points.",
            "So suppose you have data set measures on your machine you have come up with a model like this artistical like like predictive model and they make predictions on new data which comes along and then your interest.",
            "What is the probability that you will make a mistake with this model?",
            "OK, simple task.",
            "And now.",
            "Also, the theory is very simple behind this one.",
            "It's just like a big number of about making mistakes or not making mistake and you have observed like guys which are not a mistake, so you have overwhelming evidence that, well, the next one won't be a mistake and you can also characterized using very simple.",
            "I'm your balance, and it turns out that the probability of making a mistake on the next example is like exponential decaying and.",
            "So.",
            "Anne.",
            "So epsilon is the probability of a mistake, so this.",
            "But it basically says OK, so you said at the accuracy to epsilon like OK, we allow for some 0.005% of probability of mistake and then the probability that you will overshoot this.",
            "This threshold is like exponential decay North.",
            "The larger data set is the lower this guy becomes if goes to Infinity.",
            "This one clearly goes to 0.",
            "But remember we are interested in finite sample regimes.",
            "We are not interested in limit behavior which is 0, but we are interested in what happens if it is fixed and is for example, hundreds.",
            "They can just calculate here like the probability of mistake and you can give your outcome certain number."
        ],
        [
            "Anne.",
            "Now there's one catch.",
            "This was basically for one model, but you don't know.",
            "Apparently, before you see the data before you did your modeling technique, which model you have to concern.",
            "So what people do they say is you have to make this guarantee of all possible outcomes.",
            "So this immediately implies that if you have like a very large set of hypothesis that we have to guarantee this exponential decay for each of them, then the probabilistic guarantees won't be very strong.",
            "But if you have only like 3 different models, then probabilistic guarantees will be extremely strong, so.",
            "With problems without data, we do not know which equivalent class which outcome to study basically, but.",
            "And the solution is guaranteed.",
            "This probabilistic inequality for each other."
        ],
        [
            "So I do not know whether I have to go into detail in this one.",
            "But just I want to drop the words of VC dimension we see standing for javelin skills.",
            "Dimension is basically capturing how many different hypothesis sets you have to make your union bound or so.",
            "Remember this example with rectangles.",
            "If you have a large set of rectangles, or if you consider the set of polygons or if you consider the set of all possible areas.",
            "Then it was clear that you have to pay for this complexity and good characterization of how much you have to pay is in the Nick Kevin Link dimension.",
            "So fat Nick Devil Inc. Is is in the 72 papers.",
            "They were the first one to basically note that this world is the quantity of interest.",
            "That is to say, recently came upon some papers of you.",
            "I don't know whether you know is famous more in engineering context, but yes, 67 papers where he discussed exactly the same without giving it the name.",
            "Of course, obvious late of PC dimension, but it meant their ideas were around already in the late 60s anyway.",
            "Japanese game also up with the support vector machine, which I will talk later and it gave a famous bound for the PC dimension of the support vector machine and it's basically saying that the VC dimension is bounded by hedges divided by margin.",
            "What is like if we consider classifications schemes which separates data with a very large margin?",
            "Basically then we have to consider less.",
            "You know models the complexity of the model space is slower and R is just a bound on the input on the input variables.",
            "So this is the quantity of interest for the VC dimension.",
            "And the important here Alization and also the underlying reason why support vector machines are.",
            "So well in any data mining techniques are in machine learning techniques is that this bound does not involve the dimension of data points.",
            "So even if we have data points which are represented in like very huge dimension, like 1000, like 10,000 dimension classical statistical techniques count to anything.",
            "They just fail because I mean it's the first assumption of classical statistics.",
            "Is that where we have only 5 number of bandwidth like loan number of damages?",
            "But if the bandwidth space is like very large that Nick says you don't have to look at the dimension of the parameter space, you only have to look at this quantity.",
            "So this was really an important realization.",
            "And the reason for this bound, it's also very intuitive in a certain sense.",
            "It's it's very geometric, about if you think about it.",
            "If you look at the papers of ethnic, so there's a whole first about covering numbers, entropy, numbers, I mean arcane subjects, very alien expressions, all that.",
            "But in the end you just use a very geometrical insight that the largest square.",
            "I mean, suppose you have like 8 points and you want to separate each of them with.",
            "This margin, which you set off.",
            "Then you have to make let's say.",
            "To be possible to classify each assignment of labels in arbitrary Way, then you have to distance between two points has to be this quantity.",
            "This margin, and that's the geometrical thing.",
            "So basically says that you have to look at the largest cube in the bowl, and that's basically the VC bound.",
            "I."
        ],
        [
            "What is important in this is also very intuitive, but I think it captures.",
            "Maybe that's about lost lights in this part of the presentation.",
            "It basically says that.",
            "If you go beyond the VC dimension, if you go because beyond this number, then the number of different hypothesis you have to consider and you have to make your bound over goes only polynomial, while in the previous lecture slides we said that at the concentration the probabilistic guarantees they go exponential.",
            "So family says if you go beyond this VC dimension beyond this point then the concentration goes faster than the number of hypothesis and that means exactly.",
            "Theoretically, that you can.",
            "You can do learning effectively, so you have enough data enough information to guarantee the coolness of your solution."
        ],
        [
            "OK, this was empirical risk minimization thing, but I want to say also before and here is the."
        ],
        [
            "The second think very deep result but very simple results in the end if you.",
            "You look at it, it's.",
            "Result of online learning.",
            "So in online learning we are not interested in just minimizing the empirical risk as previously, but we want to update our hypothesis like our vector with diameters.",
            "If new data point comes around.",
            "So what is the game of the perception of the very first learning algorithm?",
            "So if.",
            "In iteration, overtime instances for each time instance you have nature like, well, the world presents like and you input your Organism, your perception.",
            "It predicts the sign of the W transpose XT, where W is the previous hypothesis it predicts, and then nature, it turns out come.",
            "And if you make an error, they update your hypothesis.",
            "If you don't make an error.",
            "You do nothing.",
            "OK, so it's a very simple game, it's a.",
            "But the nice thing about this is that."
        ],
        [
            "You can give guarantees and again this guarantee will be in terms of the schedules margin, exactly the same quantity of fat Nick used in SVC bounds.",
            "But here in a slightly different context.",
            "So it again says that the perception on this very simple algorithm is not dependent straightforwardly under dimension of your instances.",
            "It's dependent on the schedule margin quantity.",
            "OK, so and you can also give lower bounds in terms of this 1 / T, so it's minimax optimal in the sense it's the best you can do, but in practice you can think of model."
        ],
        [
            "Location of."
        ],
        [
            "Yeah, I think I will stop here and I will go on with the next slides in the next presentation.",
            "So if you have any questions.",
            "My questions so far.",
            "Well thanks sweetheart."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks David, thanks let you go to invite me over here today.",
                    "label": 0
                },
                {
                    "sent": "I will speak a bit about learning and prediction.",
                    "label": 1
                },
                {
                    "sent": "Hope to give you a flavor of what we machine learning people are doing and I hope to trigger some interest that you can say OK this looks interesting.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can look bits in that.",
                    "label": 1
                },
                {
                    "sent": "So it's a survey really, but just want to you know.",
                    "label": 0
                },
                {
                    "sent": "Be brief and some points which I think are really crucial, so I don't want to give a perfect survey of the field.",
                    "label": 0
                },
                {
                    "sent": "My name is Christian book Mills I my PhD in Belgium and is it?",
                    "label": 0
                },
                {
                    "sent": "In the engineering Department and their studied basically machine learning.",
                    "label": 0
                },
                {
                    "sent": "But applications to system magnification and engineering topics at nowadays I'm assistant professor in Sweden in University and this is also like an engineering Department.",
                    "label": 0
                },
                {
                    "sent": "And while I'm doing the more more like the filtering stuff later on in the talk I will give some examples of this and how you can apply machine learning techniques.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This area.",
                    "label": 0
                },
                {
                    "sent": "OK, an overview of this talk.",
                    "label": 0
                },
                {
                    "sent": "This talk of consists of two parts, an an in the first part like today I will just give a Birds Eye view on the machine learning about machine learning has to offer to you, so I will try to convey the view that machine learning has different views.",
                    "label": 1
                },
                {
                    "sent": "Basically on the same topic.",
                    "label": 1
                },
                {
                    "sent": "So fuse of stochastic based on stochastic error.",
                    "label": 1
                },
                {
                    "sent": "So classic POV on function approximation POV.",
                    "label": 0
                },
                {
                    "sent": "I will say something about.",
                    "label": 0
                },
                {
                    "sent": "Online learning, which I think it's very relevant for for you guys, and maybe a briefly mention something in optimization.",
                    "label": 0
                },
                {
                    "sent": "If time permits, I will go into some algorithms today, but in recognition our mention the support vector machine the previous speaker also used and will say something about recursion and tomorrow I will do some some deeper into new areas which we are exploring, which is the first is reliability analysis like you want to analyze how reliable a certain mechanic part is and what how machine learning can help you there and the last one is system identification.",
                    "label": 0
                },
                {
                    "sent": "So we have a dynamic system like is involved OK. How can machine learning help is there but this feature?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So First Birds Eye view.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is people.",
                    "label": 0
                },
                {
                    "sent": "Well, they tend to ask.",
                    "label": 0
                },
                {
                    "sent": "OK, what are you doing in your research?",
                    "label": 0
                },
                {
                    "sent": "And it's always difficult to explain what we're really doing because you can't go with mathematics directly, so I used to use this example.",
                    "label": 0
                },
                {
                    "sent": "So we like to make mathematical models of observations.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the short thing, so the observation could be like a flock of birds, and you observe birds in real life.",
                    "label": 0
                },
                {
                    "sent": "And then you try to come up with a model tried to capture the essentials of the of the bird.",
                    "label": 0
                },
                {
                    "sent": "So not necessarily like to mimic all aspects of birds like defenders and everything, but you just want to do the abstract meaning like OK, you won't have some lifting process and you try to come up with a model like this mounting.",
                    "label": 0
                },
                {
                    "sent": "So if you have a model, you can implement it of course, but we people are just interested in OK in the model per say, like in the mathematical description.",
                    "label": 0
                },
                {
                    "sent": "And why are we interested in a mathematical description?",
                    "label": 0
                },
                {
                    "sent": "Because you want to have insight in the essentials of the flying, for example of the birds.",
                    "label": 0
                },
                {
                    "sent": "So we don't talk about feathers and all that, but you want to talk about, for example, lifting these aspects.",
                    "label": 0
                },
                {
                    "sent": "So we want to have insights we want to have a grip on power meters.",
                    "label": 0
                },
                {
                    "sent": "What are the elephant?",
                    "label": 0
                },
                {
                    "sent": "Images for.",
                    "label": 0
                },
                {
                    "sent": "About to fly, it can't be too heavy, for example.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, and that's really important aspect, we want to make predictions about the future outcome of our model.",
                    "label": 0
                },
                {
                    "sent": "The prediction of this thing is hopefully it will fly somewhere, but well later on we will see more refined notions of predictions and at the end of the day we not only interesting predictions predictions that machine learning people are interested in, but people in engineering context.",
                    "label": 0
                },
                {
                    "sent": "They're more interested in like design control strategy.",
                    "label": 0
                },
                {
                    "sent": "They want, for example, they have a plant or something in their designs, or David Blaine and I want to control, for example, the special effects they want to minimize.",
                    "label": 0
                },
                {
                    "sent": "Or to maximize efficiency, so they want to apply.",
                    "label": 0
                },
                {
                    "sent": "Optimal inputs such that the yield of the planet of the model is perfect and this is an important distinction prediction.",
                    "label": 0
                },
                {
                    "sent": "That's a relatively easy task.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can frame it in stochastic context and all that, but control it's really about causal behavior, so that's the next step beyond step of collection is just academic step, I would say, but the step towards control into about causal modeling is the big challenge.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Engineering context.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my.",
                    "label": 0
                },
                {
                    "sent": "Well just a picture, say machine learning as we used to do it.",
                    "label": 0
                },
                {
                    "sent": "In short, it's like on the Internet intersection on the grocers between application studies.",
                    "label": 0
                },
                {
                    "sent": "For example, in my in computational biology, statistics and optimization, we engineering people like to optimize everything.",
                    "label": 0
                },
                {
                    "sent": "So optimization is really crucial parts here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "OK so I will talk about machine learning from majorly stochastic context, but machine learning you can explain the topic on many different points of views and one point of view which is very.",
                    "label": 0
                },
                {
                    "sent": "It's a very popular those days.",
                    "label": 0
                },
                {
                    "sent": "Is few machine learning from point of view of information theory, but of course you have also the data money branch and I like to see what machine learning people do in context of algorithms they want.",
                    "label": 0
                },
                {
                    "sent": "For example, the clustering is typically orgasm POV.",
                    "label": 0
                },
                {
                    "sent": "They want to come up with an algorithm which tends to give some insights in the data but OK, but also the senior processing area like.",
                    "label": 0
                },
                {
                    "sent": "More developed filtering area they talk about machine learning, which is completely different language, but in essence it's all about the same thing.",
                    "label": 0
                },
                {
                    "sent": "Also in optimization theory and optimization modeling, they tend to talk about machine learning algorithms and how to do it efficiently in practice.",
                    "label": 0
                },
                {
                    "sent": "There's also a very mathematical branch of operators to basically inverse problems, which tend to look at the same problems with a different language, and you have also game theory.",
                    "label": 1
                },
                {
                    "sent": "So these are regulations related topics, but I won't go into detail in those, but I will take the stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I wrote many.",
                    "label": 0
                },
                {
                    "sent": "So what I will do in this talk I will give you a flavor of machine learning can offer you and I'm route of more or less in the theoretical point of view, so immediate questions.",
                    "label": 0
                },
                {
                    "sent": "Why theorems are?",
                    "label": 0
                },
                {
                    "sent": "Why are theorems of interest to us?",
                    "label": 0
                },
                {
                    "sent": "I mean, and that's not only a question one ask at in a bar or something, but that's a very relevant question for us people.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is teams are first like a sound check.",
                    "label": 0
                },
                {
                    "sent": "If you can give a good team for an August in which you came up with it's worth more than like you Ristic, because you can guarantee that.",
                    "label": 0
                },
                {
                    "sent": "It will mostly do the right thing.",
                    "label": 0
                },
                {
                    "sent": "I mean if it's puristic then you can just give it a try in the machine and see what MATLAB gives you.",
                    "label": 0
                },
                {
                    "sent": "But the theorem is basically a sanity check that's important motivation and the second thing is that making terms and thinking about how to derive theorems and proofs.",
                    "label": 1
                },
                {
                    "sent": "It really gives a controls are in the environment, meaning that.",
                    "label": 0
                },
                {
                    "sent": "The language of mathematics is really very controlled language.",
                    "label": 0
                },
                {
                    "sent": "You can't do many things wrong.",
                    "label": 0
                },
                {
                    "sent": "I mean, there are clear rules and you can't just neglect the rules.",
                    "label": 0
                },
                {
                    "sent": "You have to follow the rules so it's controls are in the environment first.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So my three favorite theorems, which I will talk about a little in this presentation, are the mistake band for the perception, and this was the very first formal results in machine learning.",
                    "label": 0
                },
                {
                    "sent": "It's now about 50 years old.",
                    "label": 0
                },
                {
                    "sent": "Well, the perception was presented in 57.",
                    "label": 0
                },
                {
                    "sent": "If I correct the bound came years later, but it's really the very first formal result, so I think it's important.",
                    "label": 0
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "You know it's just an application.",
                    "label": 0
                },
                {
                    "sent": "Of course this was basically, but it's really captures what it's all about.",
                    "label": 0
                },
                {
                    "sent": "The second one is he presented to you.",
                    "label": 0
                },
                {
                    "sent": "This is a theorem which well originated from a more functional analysis context, but it is not very relevant, especially in the context of support vector machines.",
                    "label": 1
                },
                {
                    "sent": "And this also says something about the nature of learning of machine learning, and I will say something about concentration bounds.",
                    "label": 0
                },
                {
                    "sent": "Very briefly.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, but I think it's important because it says what stochastic modeling can do for you.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's a bit about how to introduce this topic, and if you keep on abstracting the topic.",
                    "label": 0
                },
                {
                    "sent": "I mean this is really the problem we are all looking at.",
                    "label": 0
                },
                {
                    "sent": "It's a problem.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have an unordered set of observations.",
                    "label": 1
                },
                {
                    "sent": "So what can you say about next one?",
                    "label": 0
                },
                {
                    "sent": "It's meaningless problem, but if you assume certain structure of your data, if you wish to make some assumptions of how the data originators how, which are the rules to play with, then you can say something about about a new observation.",
                    "label": 0
                },
                {
                    "sent": "But if you're not willing to make any assumption at all this pointless question.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's really important and important here this session.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there was this guy George Box in the 60s made limit this quote which I found quite you know, let's say so.",
                    "label": 0
                },
                {
                    "sent": "Essentially all models are wrong.",
                    "label": 1
                },
                {
                    "sent": "But some are useful, and that's I mean, I think that's really saying the whole thing.",
                    "label": 0
                },
                {
                    "sent": "So we said that already 40 years ago.",
                    "label": 0
                },
                {
                    "sent": "But it's more children ever, so by the way, this is not George Box.",
                    "label": 0
                },
                {
                    "sent": "I didn't find the picture of him, but his relative Cox, which is the closest in Hamming distance, well anyway.",
                    "label": 0
                },
                {
                    "sent": "An so we set in the context of closed from parametric statistical models basically, but it's also a absolutely true for the context of our distribution.",
                    "label": 0
                },
                {
                    "sent": "Free learning and learning theory.",
                    "label": 0
                },
                {
                    "sent": "So we like to make models and we know they are not true.",
                    "label": 0
                },
                {
                    "sent": "I mean models don't exist, they are just abstractions of reality.",
                    "label": 0
                },
                {
                    "sent": "But some can be useful and they'll azatian grew more and more in last years that you have to specify your model, you have to talk with your model based on.",
                    "label": 0
                },
                {
                    "sent": "What useful means for you?",
                    "label": 0
                },
                {
                    "sent": "So you have to define useful for me is OK. Making good predictions in this norm, for example.",
                    "label": 0
                },
                {
                    "sent": "That's useful.",
                    "label": 0
                },
                {
                    "sent": "And then if you have this notion of useful then you can come up with some models.",
                    "label": 0
                },
                {
                    "sent": "It's not the other way around.",
                    "label": 0
                },
                {
                    "sent": "This not like OK, we have this nice methods lying around or we found on the Internet and we can maybe apply it.",
                    "label": 0
                },
                {
                    "sent": "I mean, but you really should think just away what is used for your application and if you know what useful is then you can start thinking about a good model.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this is kind of heavy slides.",
                    "label": 0
                },
                {
                    "sent": "I put it in the beginning because now you're awake, hopefully, but I think it also captures the more formal thing of.",
                    "label": 0
                },
                {
                    "sent": "Of stochastic inference basically.",
                    "label": 0
                },
                {
                    "sent": "So one of the main teams I want to explore here is the difference between stochastic and deterministic inference and in stochastic inference.",
                    "label": 0
                },
                {
                    "sent": "Well, everybody of new of you know probability.",
                    "label": 0
                },
                {
                    "sent": "But basically you make the assumption that you can associate to every set of probability.",
                    "label": 0
                },
                {
                    "sent": "So you map a set to a number between zero and one, and now there's different.",
                    "label": 0
                },
                {
                    "sent": "Well, there's huge debate of what these numbers are between zero and one mean.",
                    "label": 0
                },
                {
                    "sent": "Like there's the frequency approach, so one is just the number of times.",
                    "label": 0
                },
                {
                    "sent": "An event will occur in the set or not, but you also have to upload the Bashan approach that well.",
                    "label": 0
                },
                {
                    "sent": "The zero one thing is just it's a belief, so if you don't believe in some events in some sets then you say, well, just close to 0 if you believe it's highly likely that event or fall on the they give you the number to one.",
                    "label": 0
                },
                {
                    "sent": "So these are just let's say, more formal discussions, and there's a huge debate.",
                    "label": 0
                },
                {
                    "sent": "But more practically, practically relevant, I would say is that even if you make the assumption of such a probability, then you still have to assume more.",
                    "label": 0
                },
                {
                    "sent": "You have to assume IID exchangeable, and this is really very relevant, because we engineers tend to work in time series analysis like in dynamical system, and the context of where the assumption of ID data for exchangeability is not really very natural one.",
                    "label": 0
                },
                {
                    "sent": "So what is it about it saying that OK, if you have any observations, the probability that all this info in the set you can factor as in different.",
                    "label": 0
                },
                {
                    "sent": "You know in the probability that the first one falls in the set times probability of them finds instead, so it can do this factorization.",
                    "label": 0
                },
                {
                    "sent": "So meaning that if you have a bunch of observation, each observation is worth as full information, basically.",
                    "label": 0
                },
                {
                    "sent": "So it's very relevant if you have.",
                    "label": 0
                },
                {
                    "sent": "A set of like 10 million data points, but they're all the same.",
                    "label": 0
                },
                {
                    "sent": "There's no information in them, so that's the case where ID is, well, it's very tricky.",
                    "label": 0
                },
                {
                    "sent": "So the assumption of ID is very relevant.",
                    "label": 0
                },
                {
                    "sent": "One very important one, I would say.",
                    "label": 0
                },
                {
                    "sent": "And you can replace this assumption by exchangeability, which is slightly weaker.",
                    "label": 0
                },
                {
                    "sent": "It's saying that, well, the probability of a set is the same as any permutation of probability, but it's basically the same as IID.",
                    "label": 0
                },
                {
                    "sent": "And this well, if you have this notion of probability, you can also come up with the notion of expectation, and this formula is probably one of the most well.",
                    "label": 0
                },
                {
                    "sent": "Well, this is.",
                    "label": 0
                },
                {
                    "sent": "One of the most important in probability.",
                    "label": 0
                },
                {
                    "sent": "So it says that the probability that some observation will fall on the set is equal to the expectation of the indicator.",
                    "label": 0
                },
                {
                    "sent": "An indicator says just one if it's true.",
                    "label": 0
                },
                {
                    "sent": "So if it's not true, so the expectation of the indicator is equal to the probability, and that sounds like very mathematical or formal thing which you can write down just like that.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that you can start your complete framework of probability like.",
                    "label": 0
                },
                {
                    "sent": ", cough and all those guys in their 30s and 40s.",
                    "label": 0
                },
                {
                    "sent": "It's with defining probability.",
                    "label": 0
                },
                {
                    "sent": "Or you can do it with defining expectation.",
                    "label": 0
                },
                {
                    "sent": "So and that's well, that's quite crucial.",
                    "label": 0
                },
                {
                    "sent": "There's a famous book by Peter Whittle from Cambridge who started.",
                    "label": 0
                },
                {
                    "sent": "We started doing this and you started OK. Let's define a notion of expectation, and then everything follows.",
                    "label": 0
                },
                {
                    "sent": "Like the complete context of probability of stochastic models of statiscal inference, everything follows, so this is like more than a formal statement.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is stochastic framework, but.",
                    "label": 0
                },
                {
                    "sent": "If we're not willing to assume the stochastic framework if we don't believe in some probability which we can, you know which captures the data, then we say just a deterministic framework and we don't make any assumptions at all.",
                    "label": 0
                },
                {
                    "sent": "Later on, I will say which how you can still overcome just prediction problems.",
                    "label": 0
                },
                {
                    "sent": "So remember, if you don't make any assumptions on your data or how this data behaves, then you cannot make any predictions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I will do now is go a bit in detail in different flavors of machine learning which are out there and they all basically capture the same of machine learning.",
                    "label": 0
                },
                {
                    "sent": "So I will say something about this inference about optimization about function approximation, online learning, and stochastic inference.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, I will say something about pragmatics artistic influence, which is the oldest flavor out there.",
                    "label": 0
                },
                {
                    "sent": "So this was basically the POV Gauss and all those people handle apostate.",
                    "label": 0
                },
                {
                    "sent": "They opted, it says that.",
                    "label": 0
                },
                {
                    "sent": "Well, if you have a bunch of observations, let's make.",
                    "label": 0
                },
                {
                    "sent": "Let's find probabilistic model like P. Of such detail, certain parameters which do explain the data the best and the important thing is that they didn't have any notion of goodness of fit or least square or whatever, but they are talking about the maximum likelihood, which of course off you know it's like supposed to have.",
                    "label": 0
                },
                {
                    "sent": "The different dots are samples.",
                    "label": 0
                },
                {
                    "sent": "If three models like Direct Daniels like the Blue retired the green rectangle.",
                    "label": 0
                },
                {
                    "sent": "And the hit rectangle.",
                    "label": 0
                },
                {
                    "sent": "Then if you maximize.",
                    "label": 0
                },
                {
                    "sent": "Over 123 divine.",
                    "label": 0
                },
                {
                    "sent": "We just want to three.",
                    "label": 0
                },
                {
                    "sent": "If you maximize the probability.",
                    "label": 0
                },
                {
                    "sent": "The data under this model.",
                    "label": 0
                },
                {
                    "sent": "Then it's clear that this one is only one which only model which explains his data based.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, suppose it's mixture of them all, then, well, you have this constraint or the probabilities sum to one, and does he have like rectangles which which goes to 1/3 of them?",
                    "label": 0
                },
                {
                    "sent": "So the maximum likelihood will be small, so maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "Basically select this guy, but it does not do so by least square criterion or by closing at noon or so, but it does so by just maximum like loot.",
                    "label": 0
                },
                {
                    "sent": "And this is also very well let's say small example like academic example.",
                    "label": 0
                },
                {
                    "sent": "But if we start talking about more complex models like more complex expression for P like for example the caution then we can do the same and then it turns out that you can.",
                    "label": 0
                },
                {
                    "sent": "It's not about selection of the model, but it's about inference of the parimeter basically.",
                    "label": 0
                },
                {
                    "sent": "Later on we will talk something some slides about classification and there we have basically two different probability distributions which you want to fit at the same time, and the classical methods of classification.",
                    "label": 0
                },
                {
                    "sent": "If you go to statistical literature or the MATLAB toolbox for statistics, they have linear discriminant analysis or cutey cutey a logistic regression.",
                    "label": 0
                },
                {
                    "sent": "They basically implement the same, but doing it with two different distributions 1/3.",
                    "label": 0
                },
                {
                    "sent": "Blue guys, one for the red guys.",
                    "label": 0
                },
                {
                    "sent": "The thing to remember here is that.",
                    "label": 0
                },
                {
                    "sent": "In the statistical context, we like the model which explains the data best, so it's not about the best prediction, it's about explaining which it is good for you.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is also maybe.",
                    "label": 0
                },
                {
                    "sent": "A bit heavy slide, but I want to go into it because it's relevant for many, many problems.",
                    "label": 0
                },
                {
                    "sent": "We are looking in machine learning.",
                    "label": 0
                },
                {
                    "sent": "I mean it captures account amid the essence of what machine learning is about.",
                    "label": 0
                },
                {
                    "sent": "So what do we see here?",
                    "label": 0
                },
                {
                    "sent": "We see three plots.",
                    "label": 0
                },
                {
                    "sent": "The first note is only like a 2 dimensional sample.",
                    "label": 0
                },
                {
                    "sent": "The blue dots are sampled well.",
                    "label": 0
                },
                {
                    "sent": "It's of course Gaussian here.",
                    "label": 0
                },
                {
                    "sent": "But what we do there is we don't want to fit a Gaussian distribution explaining this data, but we say we're interested in this rectangles and what is the probability that a new data point falls in the certain rectangle?",
                    "label": 0
                },
                {
                    "sent": "For example, probability that it's over.",
                    "label": 0
                },
                {
                    "sent": "Mobility that it will fall in the left in this rectangle is fairly small, because why is intuitively because it contains a low portion of the data points, but the probability that falls in this big time is large.",
                    "label": 0
                },
                {
                    "sent": "So that's the difference between statistical inference where we want to go and put some other an like distribution free inference so, and we can do it for rectangles, but it's it's well.",
                    "label": 0
                },
                {
                    "sent": "It's basically a very.",
                    "label": 0
                },
                {
                    "sent": "Let's say it's up to you which shape you like.",
                    "label": 0
                },
                {
                    "sent": "So if you like to go for more complex shapes, then you can do the same inference.",
                    "label": 0
                },
                {
                    "sent": "But the point is that the more complex the shapes are you're interested in how they model your probability well, the more you have to pay, basically and less certain you will be about the exact conclusion.",
                    "label": 0
                },
                {
                    "sent": "So you make.",
                    "label": 0
                },
                {
                    "sent": "So if you have only like 12345 rectangles, then the conclusions you have to pay a little bit because you have like.",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "Conclusions you want to make, but if you have like all the possible apology, but it comes, then you have to pay a lot.",
                    "label": 0
                },
                {
                    "sent": "So and that's basically the.",
                    "label": 0
                },
                {
                    "sent": "One of the main insights in well in machine learning theory.",
                    "label": 0
                },
                {
                    "sent": "Basically, if we look at more complex models, these represents counter models.",
                    "label": 0
                },
                {
                    "sent": "Then we have to pay in certainty, so we are less certain about more complex models that we are certainly for simpler models like the rectangles are good, but the polygons not so.",
                    "label": 0
                },
                {
                    "sent": "So also a difference in just the statistical inference thing and as distribution free.",
                    "label": 0
                },
                {
                    "sent": "Interesting is that in statistics there interested in what happens if goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So if your data points if your data set becomes very very large, I mean we're interested in limit distribution and all that while in machine learning in the theory of learning.",
                    "label": 0
                },
                {
                    "sent": "We are more interested in what happens for finite samples of data, so if any is like fixed, like in this 100, how does you know the probabilistic guarantee look like?",
                    "label": 0
                },
                {
                    "sent": "So that's the difference between sadistic answer to stick.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In theory.",
                    "label": 0
                },
                {
                    "sent": "So this looks like very abstract example, but found, as I mentioned already, it will find its use in many many different areas of statistics of where of handling data and one area which is particularly intuitive I would say, but not very well explored to date is the area of multiple testing.",
                    "label": 0
                },
                {
                    "sent": "So what's this?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not protesting, let's go back to this example an we want to test whether you know, for example, this rectangle contains more than half of the probability mass of the underlying data.",
                    "label": 0
                },
                {
                    "sent": "So for this one is contains clearly very low probability mass because it will contain low number of samples.",
                    "label": 0
                },
                {
                    "sent": "But this big rectangle it will capture.",
                    "label": 0
                },
                {
                    "sent": "Well, lots of probability mass, so that's that's a simple example of multiple testing and the point I want to make here is that if we want to go for multiple this thing we have to pay for the multiplicity of, well, how many tests we want to make basically.",
                    "label": 0
                },
                {
                    "sent": "But important is that this rectangles if they are nested, they are not independent, so we have to count the number of independent non overlapping rectangles.",
                    "label": 0
                },
                {
                    "sent": "Basically because if they are independent you can figure out the probability is just by looking at how they are nested and and all that, so we don't have to count really the number of rectangles, but we have to count the number of rectangles which are non overlapping.",
                    "label": 0
                },
                {
                    "sent": "And the same here.",
                    "label": 0
                },
                {
                    "sent": "But I will make this issue bit more formal in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, this is an academic example, but the example which is of interest is whether two probabilities are the same, for example.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at the devices behavior still the same is the rules underlying this device still the same?",
                    "label": 0
                },
                {
                    "sent": "So we make measurements we sample like this device from time to time, and we give a warning when the probability distributions are different and testing for difference of probability distribution can be done by testing.",
                    "label": 0
                },
                {
                    "sent": "By using this multiple testing approach.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mentioned it's not well researched.",
                    "label": 0
                },
                {
                    "sent": "It's fairly open topic, but there are some very deep mathematical results coming up like how you have to combine multiple testing procedures and how you have to adapt the testing outcomes.",
                    "label": 0
                },
                {
                    "sent": "Well, as I say, the P values for this if you do multiple testing and the crucial name there is Benjamin quite mathematically oriented.",
                    "label": 0
                },
                {
                    "sent": "Search OK so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I will say here and this isn't even simpler.",
                    "label": 0
                },
                {
                    "sent": "Example, it's like it's looking like a 1 dimensional example, so the X axis is.",
                    "label": 0
                },
                {
                    "sent": "The index is just indexes, but does it is is a random variable in R. And we have.",
                    "label": 0
                },
                {
                    "sent": "We want to look at the probability of of the innovative random sample.",
                    "label": 0
                },
                {
                    "sent": "So what we do?",
                    "label": 0
                },
                {
                    "sent": "Obviously we do it in MATLAB all the time.",
                    "label": 0
                },
                {
                    "sent": "Is we look at the histogram of the sample, but the histogram is somewhat.",
                    "label": 1
                },
                {
                    "sent": "I'll talk.",
                    "label": 0
                },
                {
                    "sent": "You have to play with some para meters you have to play with in which you have to say OK, how many bins do we want to have?",
                    "label": 0
                },
                {
                    "sent": "So so it's you know it's bit weekly so, but there's very precise mathematical way to deal with with distributions and how to.",
                    "label": 0
                },
                {
                    "sent": "Estimated from data is called the empirical distribution function is this guy.",
                    "label": 1
                },
                {
                    "sent": "It's not looking at rectangles as in history here, but it's looking at the open half planes.",
                    "label": 0
                },
                {
                    "sent": "It's well, let's say.",
                    "label": 0
                },
                {
                    "sent": "Instead of looking at this rectangle, it looks at what is probability.",
                    "label": 0
                },
                {
                    "sent": "Most of the data?",
                    "label": 0
                },
                {
                    "sent": "Well, what's the probability of a data point being on the right hand side and then you can.",
                    "label": 0
                },
                {
                    "sent": "There's only one perimetre there, like the threshold and you can just visualize the thresholds on the Xbox and hear the the probability you measure on the data.",
                    "label": 0
                },
                {
                    "sent": "So the point I'm trying to make here is the empirical distribution function.",
                    "label": 0
                },
                {
                    "sent": "It's really the core of statistics, is really the core of what we're doing in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Later on we will look at more complex functions instead of rectangles, but it's really about the same, and people notice empirical distribution function already for a long time, like 70 years.",
                    "label": 0
                },
                {
                    "sent": "There are famous terms like having Coke on telly supporting this, this plot and what they mean, an interpretation.",
                    "label": 0
                },
                {
                    "sent": "And still to do the the the big chunk of statistical learning theory can be phrased as you know modification, slight modification of what this is about about empirical distribution function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Yeah, but what I will do here is OK, so it's this.",
                    "label": 0
                },
                {
                    "sent": "Model inference problems as a stochastic inference problems with empirical distribution function, I will say something here now about empirical risk minimum.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addition, and this is a slight I used when I use in all my presentation as one of the first slides.",
                    "label": 0
                },
                {
                    "sent": "I will think it captures the notion of empirical risk minimization.",
                    "label": 1
                },
                {
                    "sent": "So what we try to do is try to come up with some models with some functions F which captures somehow the relation between X&Y.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is we have to assume that X&Y, that come from an underlying distribution which is fixed but maybe unknown and we have to make the assumption of ID and again later in time series analysis.",
                    "label": 0
                },
                {
                    "sent": "In dynamical systems this assumption is, well, we need some modification for that, but for the core of machine learning it's really very important assumption I would say.",
                    "label": 1
                },
                {
                    "sent": "What we want to do is we want to make a mapping F from X to Y.",
                    "label": 0
                },
                {
                    "sent": "So we call X like inputs while like output and for example in pattern recognition.",
                    "label": 0
                },
                {
                    "sent": "Why is minus one one?",
                    "label": 0
                },
                {
                    "sent": "For example, it says whether it's good or not, good weather, some parameters were feasible for implementation or not.",
                    "label": 0
                },
                {
                    "sent": "I mean you can give various interpretation, but abstractly it's just like why is minus one one in the rehashing context.",
                    "label": 0
                },
                {
                    "sent": "We say that why can be anything.",
                    "label": 0
                },
                {
                    "sent": "Count can be continuous variable by the way.",
                    "label": 0
                },
                {
                    "sent": "I used the statistical notation here.",
                    "label": 0
                },
                {
                    "sent": "Like if we use capital letters X and why it says that X&Y are random variables.",
                    "label": 0
                },
                {
                    "sent": "Basically, they're not just quantities, they're not just factors, but they have a probability attached to it.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you have to think about OK. What is a good?",
                    "label": 0
                },
                {
                    "sent": "How would the good model for me look like?",
                    "label": 0
                },
                {
                    "sent": "It's like a linear model.",
                    "label": 0
                },
                {
                    "sent": "A good idea like just some linear parameters.",
                    "label": 0
                },
                {
                    "sent": "Or do we have a complex PDE with some unknown by meters?",
                    "label": 0
                },
                {
                    "sent": "This might be the model class of interest.",
                    "label": 0
                },
                {
                    "sent": "So how does F look like?",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "Called the hypothesis space.",
                    "label": 0
                },
                {
                    "sent": "What is the set of all possible outcomes which we can expect without seeing this data?",
                    "label": 0
                },
                {
                    "sent": "Basically, so we have to think about F. We have also think about loss.",
                    "label": 0
                },
                {
                    "sent": "Scoot mean first and now.",
                    "label": 0
                },
                {
                    "sent": "That is important.",
                    "label": 0
                },
                {
                    "sent": "That is mainly the difference of wealth.",
                    "label": 1
                },
                {
                    "sent": "Artistical modeling as I showed a few slides back and empirical risk minimization here we explicitely quantified the notion of good.",
                    "label": 0
                },
                {
                    "sent": "We say good for us means like we have a low error.",
                    "label": 0
                },
                {
                    "sent": "I mean that's what we engineers understand fairly well.",
                    "label": 0
                },
                {
                    "sent": "Good first means like we have a small uncertainty for example.",
                    "label": 0
                },
                {
                    "sent": "There were these papers of Markowitz in stock market in the portfolio selection and say good for me means like if you have a lower risk of losing a lot of money.",
                    "label": 0
                },
                {
                    "sent": "So any made this whole theory based on the notion of goodness?",
                    "label": 0
                },
                {
                    "sent": "So the question, now that's a question to you guys.",
                    "label": 0
                },
                {
                    "sent": "You want to implement like for example a machine learning technique.",
                    "label": 0
                },
                {
                    "sent": "What does good mean for you?",
                    "label": 0
                },
                {
                    "sent": "What is more interested in the core business of your estimates are more interested in good prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "What is good for you?",
                    "label": 0
                },
                {
                    "sent": "So what's the hypothesis?",
                    "label": 0
                },
                {
                    "sent": "Was this the last function?",
                    "label": 0
                },
                {
                    "sent": "OK, So what empirical his position is about.",
                    "label": 0
                },
                {
                    "sent": "Just tries to come up with a model with the parameters optimal tries to play with the parameters, so it's just expected loss is minimal and expected loss.",
                    "label": 0
                },
                {
                    "sent": "It basically says if you have a new random variable, what will those be without seeing this random variable, what do we expect from future behavior and now risk minimization is just minimizing this expected loss.",
                    "label": 0
                },
                {
                    "sent": "We will say this expected loss.",
                    "label": 0
                },
                {
                    "sent": "That's the risk of the model, so we have this risk minimisation.",
                    "label": 0
                },
                {
                    "sent": "But of course we don't have access to the underlying probability distributions and all that, so we replaced expectation by empirical average basically.",
                    "label": 0
                },
                {
                    "sent": "So we can place this expectation by an average and we have the empirical estimates of health.",
                    "label": 0
                },
                {
                    "sent": "So what learning theory is really about is it tries to characterize how closely this F head is through this App Store.",
                    "label": 0
                },
                {
                    "sent": "Very simple questions, but the main point is that it's really up to you, but it means good and risk is about in your model.",
                    "label": 0
                },
                {
                    "sent": "What model structure is about.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, in the classification task we have like plus 1 -- 1 as I said as an artist like OK Now for example apart devices, good device is not good, for example.",
                    "label": 0
                },
                {
                    "sent": "Well this certain gene causes this phenomena like cancer or this gene doesn't, so it's plus 1 -- 1 in abstract testing and we have some inputs X's moment of our D. So again, we assume IID very important.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "If you make the assumption that we can separate basically the two classes, what does that mean?",
                    "label": 0
                },
                {
                    "sent": "Suppose we have two samples.",
                    "label": 0
                },
                {
                    "sent": "And if we find like a linear model which you know sets 2 examples are on either side of the after decision, then we say that it's realizable.",
                    "label": 0
                },
                {
                    "sent": "So and that will be the crucial formal notion in classification, and there's a lot of machine learning theory involved with whether it's realizable case and then you can have very fast algorithms if we cannot make this assumption that we can separate this out, like in expectation like from assumption, then our algorithms for work slower and they have to take into account lots of issues.",
                    "label": 0
                },
                {
                    "sent": "So if you assume it's realisable OK, so that's the classical assumption, then we will define the notion of his query, make mistakes, so whether W transpose X will be the same sign as Y, and empirical risk is just had this probability replaced by this expectation.",
                    "label": 0
                },
                {
                    "sent": "Remember again here we have used this quality, that probability is equal to the expectation of the indicator variable, so so that's the step we make from history.",
                    "label": 0
                },
                {
                    "sent": "Bigger risk.",
                    "label": 0
                },
                {
                    "sent": "In Bucharest, minimization strategies like for example, the sportfisher machine, as I will talk later about it, just minimizes empirical risk like.",
                    "label": 0
                },
                {
                    "sent": "And they would also like boosting.",
                    "label": 0
                },
                {
                    "sent": "Methods and, well, different methods can be argued from this context.",
                    "label": 0
                },
                {
                    "sent": "So here again, the aggregate is 0 if it's not true, and this one if it's true.",
                    "label": 0
                },
                {
                    "sent": "So if we make them bigger, risk just counts the amount of mistakes.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And the task is and if it gets a new point and we make a prediction, what's the probability of making a mistake?",
                    "label": 0
                },
                {
                    "sent": "OK, makes it.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my first team like result is basically like on concentration and what it does say is it just bounced the probability of making a mistake on future data points.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have data set measures on your machine you have come up with a model like this artistical like like predictive model and they make predictions on new data which comes along and then your interest.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that you will make a mistake with this model?",
                    "label": 1
                },
                {
                    "sent": "OK, simple task.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "Also, the theory is very simple behind this one.",
                    "label": 0
                },
                {
                    "sent": "It's just like a big number of about making mistakes or not making mistake and you have observed like guys which are not a mistake, so you have overwhelming evidence that, well, the next one won't be a mistake and you can also characterized using very simple.",
                    "label": 0
                },
                {
                    "sent": "I'm your balance, and it turns out that the probability of making a mistake on the next example is like exponential decaying and.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So epsilon is the probability of a mistake, so this.",
                    "label": 1
                },
                {
                    "sent": "But it basically says OK, so you said at the accuracy to epsilon like OK, we allow for some 0.005% of probability of mistake and then the probability that you will overshoot this.",
                    "label": 0
                },
                {
                    "sent": "This threshold is like exponential decay North.",
                    "label": 0
                },
                {
                    "sent": "The larger data set is the lower this guy becomes if goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "This one clearly goes to 0.",
                    "label": 0
                },
                {
                    "sent": "But remember we are interested in finite sample regimes.",
                    "label": 0
                },
                {
                    "sent": "We are not interested in limit behavior which is 0, but we are interested in what happens if it is fixed and is for example, hundreds.",
                    "label": 0
                },
                {
                    "sent": "They can just calculate here like the probability of mistake and you can give your outcome certain number.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Now there's one catch.",
                    "label": 0
                },
                {
                    "sent": "This was basically for one model, but you don't know.",
                    "label": 0
                },
                {
                    "sent": "Apparently, before you see the data before you did your modeling technique, which model you have to concern.",
                    "label": 0
                },
                {
                    "sent": "So what people do they say is you have to make this guarantee of all possible outcomes.",
                    "label": 0
                },
                {
                    "sent": "So this immediately implies that if you have like a very large set of hypothesis that we have to guarantee this exponential decay for each of them, then the probabilistic guarantees won't be very strong.",
                    "label": 0
                },
                {
                    "sent": "But if you have only like 3 different models, then probabilistic guarantees will be extremely strong, so.",
                    "label": 0
                },
                {
                    "sent": "With problems without data, we do not know which equivalent class which outcome to study basically, but.",
                    "label": 1
                },
                {
                    "sent": "And the solution is guaranteed.",
                    "label": 0
                },
                {
                    "sent": "This probabilistic inequality for each other.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I do not know whether I have to go into detail in this one.",
                    "label": 0
                },
                {
                    "sent": "But just I want to drop the words of VC dimension we see standing for javelin skills.",
                    "label": 1
                },
                {
                    "sent": "Dimension is basically capturing how many different hypothesis sets you have to make your union bound or so.",
                    "label": 0
                },
                {
                    "sent": "Remember this example with rectangles.",
                    "label": 0
                },
                {
                    "sent": "If you have a large set of rectangles, or if you consider the set of polygons or if you consider the set of all possible areas.",
                    "label": 0
                },
                {
                    "sent": "Then it was clear that you have to pay for this complexity and good characterization of how much you have to pay is in the Nick Kevin Link dimension.",
                    "label": 0
                },
                {
                    "sent": "So fat Nick Devil Inc. Is is in the 72 papers.",
                    "label": 0
                },
                {
                    "sent": "They were the first one to basically note that this world is the quantity of interest.",
                    "label": 0
                },
                {
                    "sent": "That is to say, recently came upon some papers of you.",
                    "label": 0
                },
                {
                    "sent": "I don't know whether you know is famous more in engineering context, but yes, 67 papers where he discussed exactly the same without giving it the name.",
                    "label": 0
                },
                {
                    "sent": "Of course, obvious late of PC dimension, but it meant their ideas were around already in the late 60s anyway.",
                    "label": 0
                },
                {
                    "sent": "Japanese game also up with the support vector machine, which I will talk later and it gave a famous bound for the PC dimension of the support vector machine and it's basically saying that the VC dimension is bounded by hedges divided by margin.",
                    "label": 0
                },
                {
                    "sent": "What is like if we consider classifications schemes which separates data with a very large margin?",
                    "label": 0
                },
                {
                    "sent": "Basically then we have to consider less.",
                    "label": 0
                },
                {
                    "sent": "You know models the complexity of the model space is slower and R is just a bound on the input on the input variables.",
                    "label": 0
                },
                {
                    "sent": "So this is the quantity of interest for the VC dimension.",
                    "label": 0
                },
                {
                    "sent": "And the important here Alization and also the underlying reason why support vector machines are.",
                    "label": 0
                },
                {
                    "sent": "So well in any data mining techniques are in machine learning techniques is that this bound does not involve the dimension of data points.",
                    "label": 0
                },
                {
                    "sent": "So even if we have data points which are represented in like very huge dimension, like 1000, like 10,000 dimension classical statistical techniques count to anything.",
                    "label": 0
                },
                {
                    "sent": "They just fail because I mean it's the first assumption of classical statistics.",
                    "label": 0
                },
                {
                    "sent": "Is that where we have only 5 number of bandwidth like loan number of damages?",
                    "label": 0
                },
                {
                    "sent": "But if the bandwidth space is like very large that Nick says you don't have to look at the dimension of the parameter space, you only have to look at this quantity.",
                    "label": 0
                },
                {
                    "sent": "So this was really an important realization.",
                    "label": 0
                },
                {
                    "sent": "And the reason for this bound, it's also very intuitive in a certain sense.",
                    "label": 0
                },
                {
                    "sent": "It's it's very geometric, about if you think about it.",
                    "label": 0
                },
                {
                    "sent": "If you look at the papers of ethnic, so there's a whole first about covering numbers, entropy, numbers, I mean arcane subjects, very alien expressions, all that.",
                    "label": 0
                },
                {
                    "sent": "But in the end you just use a very geometrical insight that the largest square.",
                    "label": 0
                },
                {
                    "sent": "I mean, suppose you have like 8 points and you want to separate each of them with.",
                    "label": 0
                },
                {
                    "sent": "This margin, which you set off.",
                    "label": 0
                },
                {
                    "sent": "Then you have to make let's say.",
                    "label": 0
                },
                {
                    "sent": "To be possible to classify each assignment of labels in arbitrary Way, then you have to distance between two points has to be this quantity.",
                    "label": 0
                },
                {
                    "sent": "This margin, and that's the geometrical thing.",
                    "label": 0
                },
                {
                    "sent": "So basically says that you have to look at the largest cube in the bowl, and that's basically the VC bound.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is important in this is also very intuitive, but I think it captures.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's about lost lights in this part of the presentation.",
                    "label": 0
                },
                {
                    "sent": "It basically says that.",
                    "label": 0
                },
                {
                    "sent": "If you go beyond the VC dimension, if you go because beyond this number, then the number of different hypothesis you have to consider and you have to make your bound over goes only polynomial, while in the previous lecture slides we said that at the concentration the probabilistic guarantees they go exponential.",
                    "label": 0
                },
                {
                    "sent": "So family says if you go beyond this VC dimension beyond this point then the concentration goes faster than the number of hypothesis and that means exactly.",
                    "label": 0
                },
                {
                    "sent": "Theoretically, that you can.",
                    "label": 0
                },
                {
                    "sent": "You can do learning effectively, so you have enough data enough information to guarantee the coolness of your solution.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, this was empirical risk minimization thing, but I want to say also before and here is the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second think very deep result but very simple results in the end if you.",
                    "label": 0
                },
                {
                    "sent": "You look at it, it's.",
                    "label": 0
                },
                {
                    "sent": "Result of online learning.",
                    "label": 0
                },
                {
                    "sent": "So in online learning we are not interested in just minimizing the empirical risk as previously, but we want to update our hypothesis like our vector with diameters.",
                    "label": 0
                },
                {
                    "sent": "If new data point comes around.",
                    "label": 0
                },
                {
                    "sent": "So what is the game of the perception of the very first learning algorithm?",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "In iteration, overtime instances for each time instance you have nature like, well, the world presents like and you input your Organism, your perception.",
                    "label": 0
                },
                {
                    "sent": "It predicts the sign of the W transpose XT, where W is the previous hypothesis it predicts, and then nature, it turns out come.",
                    "label": 0
                },
                {
                    "sent": "And if you make an error, they update your hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If you don't make an error.",
                    "label": 0
                },
                {
                    "sent": "You do nothing.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very simple game, it's a.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing about this is that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can give guarantees and again this guarantee will be in terms of the schedules margin, exactly the same quantity of fat Nick used in SVC bounds.",
                    "label": 0
                },
                {
                    "sent": "But here in a slightly different context.",
                    "label": 0
                },
                {
                    "sent": "So it again says that the perception on this very simple algorithm is not dependent straightforwardly under dimension of your instances.",
                    "label": 0
                },
                {
                    "sent": "It's dependent on the schedule margin quantity.",
                    "label": 0
                },
                {
                    "sent": "OK, so and you can also give lower bounds in terms of this 1 / T, so it's minimax optimal in the sense it's the best you can do, but in practice you can think of model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I think I will stop here and I will go on with the next slides in the next presentation.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "My questions so far.",
                    "label": 0
                },
                {
                    "sent": "Well thanks sweetheart.",
                    "label": 0
                }
            ]
        }
    }
}