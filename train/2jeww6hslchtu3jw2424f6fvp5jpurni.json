{
    "id": "2jeww6hslchtu3jw2424f6fvp5jpurni",
    "title": "Analyzing Text and Social Network Data with Probabilistic Models",
    "info": {
        "author": [
            "Padhraic Smyth, Center for Machine Learning and Intelligent Systems, University of California, Irvine"
        ],
        "published": "Oct. 29, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Network Analysis->Social Networks"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2012_smyth_probabilistic_models/",
    "segmentation": [
        [
            "This evening I will be talking about text and social network data."
        ],
        [
            "I first should acknowledge a lot of the work that I'll be describing.",
            "His work with collaborators, indeed, other peoples work as well, so I've listed here some of my students, collaborators, and various funding agencies that have helped support all of this work.",
            "Few minutes ago, the 10 year award was mentioned from Helsinki."
        ],
        [
            "And in fact, I spoke at that meeting.",
            "It was a lot of fun and basically spoke for an hour about this equation, which is basically equation for a finite mixture model where we have a density function on X and we decompose that into components and weights for each of the K different components and today."
        ],
        [
            "After 10 years, I'm coming back with a new equation very similar, except you'll notice that there's a dependence on an extra variable here, D, so hopefully the have a little bit more to say than this, but as we get older, we realized there are a lot of commonality's in what we do, so this is basically the topic model equation, where now the mixture coefficients depend on the individual document rather than being global."
        ],
        [
            "So let me start by motivating the talk with a few examples and I just pick Facebook.",
            "Just because there had to be some nice slides flying around.",
            "But there are many other datasets like this and I think computer science really is changing rapidly and machine learning is a very big part of that, and data mining as these datasets are kind of forcing function on us in terms of what are we going to do with all of this data.",
            "So Facebook has, you know, by their own accounts, order of 500 million users give or take a few 100 million and the amount of data that they collect.",
            "For example the graph that they have on average each.",
            "User has about 130 other users that are connected to so you get a graph with 60 billion edges and you can imagine trying to analyze that kind of data in addition to the network.",
            "They also get, you know, by their own slides, 30 billion pieces of content every month and 3 billion photos.",
            "So this kind of interlinked data is provides all kinds of interesting opportunities that even ten years ago when many of us were in Helsinki, it was unimaginable that we could get access to data like this and.",
            "Course we may not have Facebook data, but there are many other datasets similar to this that we can think of working with."
        ],
        [
            "Even at the individual level, if we're not a Facebook, we just have our own data.",
            "The amount of data that we're collecting is still staggering.",
            "Some of you may be aware of Stephen Wolfram on his blog he posted recently.",
            "All of his email data.",
            "This is from 1990 up to 2012.",
            "Every dot.",
            "Here is an email sent.",
            "Email received, I don't remember exactly as a function of time of day an it's interesting to look at sort of his behavior over this time.",
            "For example, this is when he slept.",
            "There was relatively little email and it was getting later and later and then I believe this is when one of his books came out, so he kind of did a re reboot and got back into more normal schedule.",
            "You can also see things like evening time and so forth so."
        ],
        [
            "There were going to increasing to be collecting this kind of data as most of us are aware not just for things like email but also help type of data.",
            "Exercise with centers all around us and so forth.",
            "So this kind of individual data analysis is going to become more and more important."
        ],
        [
            "It's not just computer scientists and that are collecting this kind of data.",
            "Again, if you start walking around your campus, you'll realize that all your colleagues in other departments are becoming involved in data collection, so this is an example from history.",
            "One of the interesting things that's happening in history is there digitising many texts, turning them into ASCII.",
            "Previously historian would have to go to the library.",
            "Look at microfiche would be a slow process.",
            "Now they have access to all of their data at their fingertips.",
            "So the Pennsylvania Gazette is probably the most famous newspaper in the United States in its time was founded by Benjamin Franklin and a lot of very interesting information there.",
            "For historians studying US history recently became available in the last few years online, and you can imagine trying to study this just using keyword search.",
            "Not so easy.",
            "So historians are interested in humanities in general, looking for tools that can help them analyze these very large amounts of text.",
            "And rather than having to read every single document.",
            "OK."
        ],
        [
            "It's a common themes that you're going to see, and again, many of you are very familiar with these ideas.",
            "In fact, there's many people in the audience who worked on many of the techniques that will talk about will be, you know, taking this rich data and what we do in machine learning and data mining is flatten it and make it as simple as possible.",
            "Put it into vector and matrix form so we can do use our usual nice analytical tools, so we'll be talking about matrices within documents, W words, bag of words, type representation, or for social network data, square matrices where we're looking at perhaps just binary entries, or perhaps.",
            "Accounts and."
        ],
        [
            "What where I think it gets interesting is when we have additional side information which you pretty much always do in practice, so we don't usually just have the documents in the words or the social network.",
            "We often also have metadata in the sense of time when they coined the information was recorded, or we may have multiple copies of the information at different times.",
            "We may have covariates, attributes, alters of the documents, what Journal did appear in, or what source and sometimes will have supervised information as well.",
            "I'd say a little bit more about that later."
        ],
        [
            "So why use latent variables?",
            "Part of the issue is with many of these datasets.",
            "There really isn't.",
            "You can't just turn it into a standard classification or regression problem.",
            "You're dealing with data that there just isn't an obvious target variable to work with and.",
            "So latent variables when you're trying to model high dimensional data can be can be very handy.",
            "Very useful, at least the way that there tend to be used and one of the things from a probabilistic point of view is that we use some kind of conditional independence assumption too.",
            "It simplified the model and essentially make the say the the nodes in a network or the words in a document conditionally independent of each other given the latent variable.",
            "And you'll see this pattern over and over again with all of these latent variable models.",
            "One of the things I've tried to get across in my talk is even though these models look very different in a very different terminology, there's really a few basic principles guiding them, and one of them is find a latent variable that if you know the value of the latent variable, everything becomes conditionally independent, which which makes your likelihood and or objective function.",
            "Much more simple to work with.",
            "So the representation of form of these models is often very very simple.",
            "Learning the models can be more complicated, and there's been a fair bit of progress on that in the last few years.",
            "It's also interesting things when we use latent variable or hidden variable models.",
            "What is the purpose?",
            "What is our interpretation of the latent variables?",
            "And there are situations computer vision would be a good example for the latent variables represent something we could actually measure if we had access to the scene that the camera where the camera was positioned, how far away is the object?",
            "How many objects were there?",
            "There is an actual true answer to the question if we just go back in.",
            "Had enough data to measure it, but in many cases things are not measurable, and in that situation it may be that we're using our latent variable.",
            "Modeling is a black box, for example principal component analysis or hidden Markov models might be examples there and then.",
            "In other situations, the latent variables are not measurable, but we don't.",
            "We do want to interpret what they are.",
            "We're looking for some insight into the data, and scientists like this.",
            "If you work with scientists, biologists or even historians.",
            "For example, they love having tools that can suggest theories that that might be in the data.",
            "In fact, those of you that have worked with scientists will find that they're very willing to adopt latent variable models like principal components and give them physical interpretations that might not be really justified.",
            "But that's something you have to be aware aware of."
        ],
        [
            "And doing this in a statistical framework, at least I find, is very useful.",
            "There are many of the problems I'll talk about there are.",
            "You can approach them in a statistical probabilistic way, or you can use a more optimization.",
            "You know, maybe a clustering algorithm that doesn't really have any probabilistic semantics.",
            "I find that the probabilistic approach very useful.",
            "There's a rich literature you can build on many ideas.",
            "Many of these ideas going back a long way before people had the computational tools to use them.",
            "And you have you have a foundation, particularly when you start putting in things like time into the into these models you have all of the techniques from time series from Markov modeling, stochastic processes that you can leverage and take advantage of, so it gives you a great starting point.",
            "And of course, if you're trying to estimate parameters, then you have that machinery as well.",
            "But it is important, and I think we sometimes get carried away that all of this comes with a cost.",
            "So if person A comes along with K means and they can take a data set and analyze it and be done and go home and eat their dinner and.",
            "It's all night.",
            "The business person or the scientist is nice and happy.",
            "Versus perhaps somebody comes along with a very complicated during the process thing and requires 6 PhD students to go with the algorithm to make it work well.",
            "You know, we need to keep that in mind so so there is a kind of a cautionary note that the complexity of some of these models can get quite complicated and similar models sometimes are better.",
            "OK so."
        ],
        [
            "All of that was by way of introduction what I'm going to talk about is essentially in two parts.",
            "The first part will be about text, and particularly about topic models.",
            "I'll review basic concepts and I I'm thinking most people in the audience familiar with this, but I want to make sure everybody is on the same page, so were some of the basic concepts, and then I'll talk about some newer results that you might not be familiar with in semi supervised learning with topic models that I think are quite interesting.",
            "I then switch over to looking at network data, but hopefully in a way where you'll see the connections that the models for network data or not so different from.",
            "The types of things I talked about for text Ann will talk, time permitting about both static networks an an area where I think there's a lot more interesting things to be done, which is dynamic.",
            "Networks where you observe a network overtime and then we finish up."
        ],
        [
            "OK, so let's start to talk about text and just some very very simple concepts here.",
            "Will talk a lot about multinomial distributions and just what is a multinomial?",
            "Well, let's say this is a particular example here.",
            "This was in fact learn from a data set where we have the most likely words.",
            "Now keep in mind here this is just the top 10 or so words for this multinomial.",
            "They each have a probability the probabilities sum to one and then the number of words might be in the 10s of thousands.",
            "So we're just looking at a few words here that have most of the probability mass.",
            "The words at the bottom of the list.",
            "We went way down in the ground would have 10 to the minus 5 or 10.",
            "To the minus you know.",
            "We have very little probability, so this is a distribution that's very heavily skewed, sort or certain words, and the way we use it in our model is.",
            "Think of it as a die.",
            "So a die has six sides with each is equally likely.",
            "This particular die will have maybe 50,000 sides.",
            "It will have aside for each word, and it's not in each of the science is not equally likely they're going to be skewed in this manner.",
            "So the word president is much more likely than some of the other words.",
            "OK, so when we talk about multinomial, that's what we'll be talking about."
        ],
        [
            "This is the way this is used.",
            "It has it's.",
            "There's a conditional independence assumption that like just like in a dire a coin.",
            "When we use the model, the next word doesn't depend on the last word.",
            "It only depends on the parameters and.",
            "So that's what this picture is telling us.",
            "This graphical model saying we have the word probabilities which sum to one vector of them and each different word.",
            "The first word, second word third word only depends on the model parameters, not on the last worked.",
            "Now they're obviously this is the bag of words assumption, and so we're not looking at the sequence of words in the text where looks just looking at the counts essentially.",
            "And for some."
        ],
        [
            "Felicity will use plate diagram so we don't have to keep rewriting these things an if you haven't seen play targets just means that this is replicated end times given the parent here, so it's the same as the previous picture."
        ],
        [
            "We"
        ],
        [
            "Then replications."
        ],
        [
            "Now to do some anything interesting.",
            "We don't want to have a single multinomial we want.",
            "We'd like to have multiple multinomial's.",
            "Just having one multinomial for language would not be that interesting, and so here we have two an these these are going to be our topics, so when you hear about topic models, the topics themselves are multinomial distributions over words, and this is the one we had before and over here we have a completely different set of high probability words, and these are all about colors.",
            "And this in fact was a topic that was learned from some.",
            "Educational text, and so a very different focus in different very different set of words.",
            "But these other words would appear somewhere on the list with very low probability, but they're very different probability distributions.",
            "So."
        ],
        [
            "So to represent that in our graphical model, this is the same picture we had before, but we've added a little bit.",
            "We have an indicator variable now that can take say K values.",
            "K is our number of topics that we're going to have here.",
            "It might be hundred 200 something like that.",
            "And well, OK, so it's called sort of K. It's T and over here now we need T different distributions and so we have a plate representing the T distributions and so basically this is just going to switch if Z is equal to 7 then we look up the 7th.",
            "Multinomial over here.",
            "So this is a nice notation for these kinds of models.",
            "We can."
        ],
        [
            "Is this to do clustering?",
            "We just sort of keep building up the picture here and we've added another plate here.",
            "Now for D documents so words are conditionally independent inside of a document, their Z variable associated with each document.",
            "So that's the class label or cluster label for that document.",
            "Each has its own associated with one of these distributions.",
            "Over here and then, because we have a place here, these documents are assumed to be independent of each other and they depend on disease.",
            "Depend on some.",
            "Overall, mixture weights, so this would have been the story, probably about 10 or 15 years ago.",
            "In terms of probabilistic clustering of documents, and pretty much anybody used this model realized that yeah, it's OK, but documents often have multiple teams in the multiple topics."
        ],
        [
            "Very simple change representation Lee.",
            "Which we've done here is to move the Z variable into the word level, so previously it was out here at the document level.",
            "All we've done here is we've moved it inside, and we've also moved the mixture weights inside this plate.",
            "So now each document has a distribution over topics, and now each Z variable, each word is can be coming from a different topic if we want.",
            "So we might have a document that's focused on topics one and seven, with probabilities .5, and then we sample words from those two topics.",
            "Given given disease so very, very simple, but actually quite powerful and it's sort of interesting that people knew about this model.",
            "Prior to 2000, but we didn't really have good ways to estimate it.",
            "It wasn't that easy to."
        ],
        [
            "The model, so just to clarify, you know an example of how this works.",
            "Here is a paper from your body and colleagues that was at nips many years ago, maybe 94 or so, and it's interesting because it really is bridging two different fields.",
            "It's taking hidden Markov models and using them for sequence analysis and bioinformatics.",
            "Now today that's very standard, but at the time this was really combining two things that were very different to each other."
        ],
        [
            "And if you use clustering, we just ran this 2K means we get a cluster of.",
            "These are the sort of important words in the cluster that are, well.",
            "They're kind of a mixture of different words about neural networks and hidden Markov, and so forward.",
            "It's not particularly good representation if we."
        ],
        [
            "These topics it's assigned to these two different topics, so we get a much nicer representation of the document.",
            "And again, many of you have used topic models.",
            "You're aware of this, but I just want to bring everybody sort of up to speed on this."
        ],
        [
            "Alright, so this is the equation that's representing things, so the probability of word on the left here is represented by probability of the different under different topics.",
            "We sum out over the topic variable Z here and we have the probability of the different topics occurring in that document.",
            "Fairly simple, but we still haven't said how we're going to learn this from data."
        ],
        [
            "We can also think of this as a matrix decomposition or matrix factorization, which can have its advantages, and it's similar to PCA or SVD, but a little bit different in that the matrix on the left is.",
            "The probabilities of words given the document, so each row would be a probability of award.",
            "Given that document rather than the actual counts themselves and on the right we have two sort of skinny matrices, where are if you like, dimension reduction is happening with the topics here, so the."
        ],
        [
            "First one is the coefficients.",
            "If you like for each document and the second one each.",
            "Role here is a given topic and then it's the multinomial that has the different probabilities of the different words, and so if we sort of look at how this works with the equation probability, particular word in a particular document is basically the dot product of the coefficients for that document across topics, and then the probabilities of words given particular word given those different topics.",
            "So column from the from the second matrix, so they're very nice analogy here with things that we.",
            "Do in other contexts, such as principle components and so forth."
        ],
        [
            "And indeed, if you look at the history of this model, people had used the idea of matrix decomposition for text as LSA.",
            "And the problem with that was that we're modeling counts, and LSA is based on principle components, SPD, which is really suited for real valued data.",
            "So Thomas Huffman was really the person who brought topic modeling to forward his papers back in the late 1990s, the idea had been circulating around in image analysis, say at NIPS and other places, and also was present in some statistics papers.",
            "But people really haven't figured out how to fit this model, so image analysis, for example, the idea that.",
            "An object is composed of multiple parts, is very appealing, and so people had been thinking a lot about that kind of a model.",
            "These kind of factor models.",
            "Thomas applied it to text and I think immediately a lot of people said this is interesting.",
            "This is this is taking us beyond clustering.",
            "It's also interesting, sort of.",
            "Historically, the paper to get most of the attention is the David Blank paper, which is a great paper that came along later, but it was really building very strongly on.",
            "Thomas is Thomas's work and then you had the Gibbs sampling extension, which allowed people to easily estimate these models.",
            "And then many, many, many extensions and applications since the."
        ],
        [
            "So we haven't yet said anything about how we learn this model, and again, there are many people in the audience who know a lot about this, so, but some may not.",
            "So what do we actually need to learn?",
            "So this is back to our graphical model and we have we have these distributions of words given the topics we have.",
            "What else do we have?",
            "We have the mixtures for each document of the topics for that particular document, and we have these latent variables.",
            "These every single word hassey attached to it.",
            "So in theory, if you didn't know anything about learning, unsupervised learning would say this.",
            "Maybe is a little bit.",
            "Unconstrained, but the structure of texts actually one of the things as you know, if you work with these models is that you it works quite remarkably well, so."
        ],
        [
            "One of the ways to think about how did the learning algorithm they especially give sampling learning algorithms.",
            "Pretend if we knew disease if we didn't know disease, what would you be able to do?",
            "Well, the problem simplifies straight away, so I've colored in green as if disease were known.",
            "Now I know he Z the topic of every word in every document, so estimating the topic distributions the topic mixtures for any document distributor just count up.",
            "And similarly I know across the corpus for every word what it's assigned to, so I can easily figure out just again by counting.",
            "For a given topic, what is distribution over words is so getting a disease gives us an easy way to get Peter and five the other parameters and."
        ],
        [
            "That's that's how the Gibbs sampling learning algorithm essentially.",
            "So we have our bag of words input.",
            "We have the number of topics we have, no labels, the."
        ],
        [
            "The collapse keep sampling basically technical trick in sampling techniques to integrate out Peter Anfi and just figure out if I just wanted to know disease.",
            "What would I do if from abrasion POV and you get a very simple update equation that update equation has to be executed on every word in every document, maybe hundreds or thousands of times you do this kind of collapsed Gibbs sampling, but it's linear in the number of word tokens and then T and then once you have samples of these you can get.",
            "Point estimates of Theta and Phi.",
            "So that's basically the Gibbs sampling in Matlab.",
            "You can write it in a few lines which has been part of the appeal of topic models that it's very easy to use."
        ],
        [
            "Outputs then or the topic word.",
            "Probably situations.",
            "The document topic, probability distributions and if you want to look at the individual disease you have them.",
            "They tend to be quite noisy, but you can average over them.",
            "Get sentence level word level, section level, etc.",
            "I should also say there are other learning algorithms variationally in other techniques, but the collapsed Gibbs sampling is so easy that people have.",
            "Used it."
        ],
        [
            "So I think topic modeling is at a point where really it needs to find applications.",
            "There were so many papers being published on topic modeling, including my own group, but it's at a point now where we really start saying how can we use this to help people solve problems.",
            "One of the projects in my group right now is funded by the US government.",
            "Is we're looking at large amounts of historical scientific literature to try to track the emergence of ideas.",
            "We're working with people who study the history of science and technology is quite interesting, so as an example.",
            "We have 49,000 abstracts that are related to the area of DNA microarrays and we're interested in trying to figure out when did than you wanted new ideas.",
            "How did this area of DNA microarrays in biology?",
            "How did it emerge?",
            "What was the process that happened?",
            "And so if you run the topic model, you get, you know the usual set of different types of topics.",
            "Ranging and I've ordered them here.",
            "Just picked 5 and these are the top five words in each topic.",
            "Ties probability.",
            "I'm sort of very basic technology to things that are more application oriented.",
            "So at the beginning you have topics like just the basic technology of the microarray chips.",
            "Then you have the emergence of topics that are more.",
            "How do you use the data coming off the chips for a computer scientist started, get involved and then.",
            "Over here you have biologists turning to use actually use the data and clinicians to solve real problems.",
            "So what's nice is if you."
        ],
        [
            "Look at this through the lens of the topic model.",
            "Let me explain how this graph works.",
            "So this is time going this direction and this is the fraction of words per year assigned to a particular topic.",
            "There were 100 topics in the model, so if everything was equally likely and we took all the documents from, say, the year 2000, then everything would be here where we're showing.",
            "Here are four topics that are over represented in that year.",
            "In fact, the account these four account for nearly 50% of all the words and these topics.",
            "I don't know if you can read it.",
            "But they're very technology device physics oriented.",
            "There the papers that we published at that time were very focused on just how do you build these gadgets and get them to work, and then what's interesting is overtime.",
            "The prevalence of these went down and since these."
        ],
        [
            "Refraction something else must be going up and you see patients and treatment classification pathways and networks themselves.",
            "You know much more biological clinical application types of topics emerging, and there's quite a quite a pronounced change here, and so people were collaborating with are very interested in.",
            "Can you quantify, you know the papers that they tend to write in the history of science and technology are somewhat hand WAVY.",
            "They do some keyword searches, and this potentially gives them much more powerful tools to try to quantify what was going on.",
            "And so the funding agencies, for example, are very interested in this type of analysis.",
            "There are limits, of course, to what you can do, but it gives them something more quantitative where they can look at funding programs and see what was happening.",
            "So we're also looking at patterns here and so forth."
        ],
        [
            "Coming back to the historical example, the Pennsylvania Gazette data.",
            "This is from a colleague of mine, Dave Newman, where again the types that this particular topic that they found in the data is about constitutional law.",
            "And of course you know it's nice that in 1776 this starts to to increase and.",
            "The you can see how historians would be interested in using this as a new tool and are using it as a new tool in analyzing this kind of data.",
            "I should also say that the examples I'm showing here, the model knows nothing about time.",
            "This is very naive, very simple, where we're just fitting a model to all of the data and then retrospectively binning the data by year and counting a more sophisticated approach here and there are many such algorithms would be to put time into the model, but they tend to be a little bit trickier to get to work.",
            "There's a variety of such techniques out there."
        ],
        [
            "Let me show you another example.",
            "This is the Enron email data set, and for any young people interested in going out and making a lot of money, let me suggest a good start up would be used.",
            "Some kind of text analysis techniques and sell them to lawyers to legal companies.",
            "So if you go to work for any law firm, say as a consultant, you'll find that there's still buried in paper and PDF files, and they use keyword search and really could use a lot of help, and so the reason I'm mentioning this is because the Enron email data set became available.",
            "Through the legal action of the US government who prosecuted them several years ago and 250,000 emails were subpoenaed and arrived at the US Justice Department's desk.",
            "And you can imagine being a lawyer working, say, junior lawyer in the office.",
            "You come in on Monday morning.",
            "Your boss is great.",
            "We just got all the emails from Enron, or at least some of the emails from Enron.",
            "And you're thrilled.",
            "That's great.",
            "And then that your boss says an were in court on Wednesday.",
            "And I need you to find the you know the 100 emails that are most relevant and use them in court.",
            "And so if you had to use keyword search searching through all these emails it will be very difficult.",
            "So again, techniques clustering, topic modeling and so forth.",
            "I think have a lot of potential there, but we can't just sort of throw them over the fence to people in lower in history that we need to work with them, collaborate with them to build useful tools.",
            "So the types of topics that you will get if you run an Enron email are things like.",
            "Sorry, come back here.",
            "Standard sort of things about constructing, you know, construction.",
            "The environmental issues about air quality.",
            "These are the types of topics you might expect.",
            "Enron was a large energy company, and so these are typical of what we might."
        ],
        [
            "Expect maybe we wouldn't expect, but in hindsight is obvious would be there are these personal topics, so holiday parties and Ron is in Texas and they like their football American football, so they bet a lot on these.",
            "There's a whole topic about that.",
            "There was a Christian topic and was warned about online shopping.",
            "So again, if you are the lawyer, you might say, OK, I'm not interested in any of this stuff.",
            "This is just not relevant.",
            "And then."
        ],
        [
            "You do find sort of political topics.",
            "There was a big issue about power in California, and in particular bankruptcy aspect of that.",
            "Also, there's an interesting topic about lobbying in Washington, some topic about lawsuits and what's nice is you can go in and say where the emails that go with these particular topics, and there's actually some very interesting emails in that data set.",
            "If you have time to go and look."
        ],
        [
            "OK, so many of you have seen much of this before, and so from here on.",
            "Hopefully some of this would be newer.",
            "There are many, many extensions that people have come up with.",
            "Some people probably in this room and I can't mention everybody's name, but some of the highlights would be, you know, putting time into the model so you can properly take time into account putting correlation into it.",
            "Nonparametric techniques so we don't fix the number of topics ahead of time, author, topic models and an idea that my group has picked up on and I think is very useful and maybe underappreciated.",
            "Is this directly?",
            "Normal regression idea from David Mill and Andrew McCallum, where you can pretty much add in arbitrary metadata into the model using sort of a general regression framework, and that seems quite nice.",
            "It's sort of students meant much of the earlier work like alter topics and so forth."
        ],
        [
            "OK, so let me let me move on to some work that maybe is less familiar to you and let me motivate this by saying.",
            "You know one of the nice things about topic models clustering techniques is you just throw data added.",
            "You don't need any prior knowledge, it's just purely data driven and that's great.",
            "But when you start to work with people in applications you realize there is in fact often a lot of prior knowledge available that you might want to use.",
            "In particular, dealing with text.",
            "In some sense, it seems almost silly.",
            "You know your 4 year old would say you know Daddy where you're trying to learn all this just from data.",
            "When there's a lot of information out there already that structured.",
            "So let me expand a little bit on what I mean."
        ],
        [
            "The first example is going to talk about is labeled data, which you know for text documents is often small, but still very, very useful.",
            "So if we look at the problem for a few minutes of multi label classification for documents and here are three datasets that are fairly widely used in the literature, but they're actually relatively small, particularly in terms of the number of unique labels that are in the datasets.",
            "So this is the Reuters data set, about 100 labels, and the Yahoo Arts and health datasets.",
            "Which each have order of 20 labels and the median number of documents per individual label.",
            "So every label is associated with certain number of documents is quite high, hundreds or thousands, and so discriminative methods tend to work fine here.",
            "SVM's one versus all, and so forth."
        ],
        [
            "If you start to look at, you know real world datasets.",
            "And let's look at 2 examples here.",
            "This is the year like status at a European legal document set that has labels that has come out in the last few years.",
            "And this is the New York Times annotated data set.",
            "You very nice data set with very rich annotations.",
            "That's also being made available the last few years.",
            "It the numbers are almost reversed.",
            "Now you have thousands of unique labels.",
            "In fact, both of these datasets are really the sum of the publicly available versions that we used in experiments.",
            "There's really orders of magnitude more labels that you could be using.",
            "And the median number of documents per label is now much smaller.",
            "It's just you know, order of you know 10 or one even."
        ],
        [
            "In fact, if we look at this graph here for these two corpora, this is where we sort the labels by how many documents are associated with each one.",
            "So if you have a dot over here, it means this is a label.",
            "We have one label with 10 to the three documents associated with it.",
            "Over here we have.",
            "Label that has one document associated with it and there's over 10 to three such labels.",
            "So you see this sort of dramatic power log type picture or distribution for both of these datasets.",
            "In other words, there's many more labels down here.",
            "There's a huge number of labels that have only a single document in the corpus associated with them, and then smaller number that have two documents in the smaller number of three, but it's very skewed towards the left, so most of your labels have almost.",
            "No data and so discriminative methods are going to have a problem if there's only one."
        ],
        [
            "I say training document one positive example.",
            "So this idea is being picked up on by our group and others.",
            "Very very simple idea.",
            "So we take topic modeling and we associate each label with the topic.",
            "And during learning during that Gibbs sampling, where if you remember the learning algorithm is going through an for every word token it's saying OK, which given everything else, I know which label should be assigned to which topic should be assigned to.",
            "So instead of looking at all the possible topics, are all the possible labels here, you just restrict it to the assigned labels for that particular document.",
            "So instead of 1000 possible labels, you say no human said there's this document is about these three labels, so I just sample from those.",
            "So what in effect the?",
            "Topic model does.",
            "Here is it learns to associate the labels in the multi label Document said with which words within documents go with it.",
            "It comes up with probability distributions and in fact it's nice because it's much faster than the typical topic model which has to consider all of the possible topics.",
            "Now it's being told to focusing on the few and the key difference with the discriminative method here is that it's you're doing the labeling at the word level, not at the document level.",
            "And we'll see that that gives you some advantages with these.",
            "Very large label sets in a few minutes.",
            "The other thing that's useful here is to build in label dependencies.",
            "If you work with multi label document classification, you know that it's a difficult problem to figure out how to build in dependencies between labels and in the probabilistic framework we have ways to do that and it's fairly straightforward manner and it does improve performance significantly, so let me."
        ],
        [
            "Give you an example of how this works in practice.",
            "Here's an article from the New York Times, at least the first few lines of it, and it has three labels attached to it.",
            "It's an article about lawsuits and video games, so maybe a patent lawsuit?",
            "Antitrust, I guess in this case, and this is how many other?",
            "Well, how many documents in total have these labels?",
            "So the first 2 have or 1967, but the video games label itself.",
            "This is the only documented peers in so.",
            "Which you imagine might happen is that a discriminative classifier here is going to have trouble figuring out what words go with video games, just given that it has one label and has two other labels attached to this document as well."
        ],
        [
            "And sure enough, when we look at the SVM and look at the the words that appear to be relevant, there's some words associated with video games in Nintendo.",
            "But then there's a lot of other somewhat random words here.",
            "An LDA is able to pick out essentially the right words here, and it's the idea you know that in your pearls book, if you remember if you've looked at explaining away the other words have been the non videogame words have already been explained away by.",
            "The models for antitrust actions and suits and litigation, and the remaining words are the ones that are picked up here by LDA, so it works out quite nicely."
        ],
        [
            "And indeed, when we look at if you remember, we had three datasets and these are the three where there's a lot of data for a small number of labels, and these are the ones where there's a very large number of labels on average, very few documents per label, and this is how much better LDA is doing versus SVM.",
            "These SPMS were tuned to be as best as possible.",
            "They're not sort of.",
            "We try to be absolutely as far as we can here, and there was definitely a significant difference these guys are doing.",
            "The Ellie's doing well here precisely because of the example in the previous slide it can.",
            "Pick up on these documents or labels for their very few documents when you start to get over to the data, set Rogers that had a lot of documents per individual label.",
            "Are the SVM start to pick up an advantage because their discriminative?"
        ],
        [
            "Another way to look at this is we in this paper, which came out fairly recently.",
            "We measured a lot of different metrics.",
            "There's a number if you do multiple document classification, there are quite a few different metrics people use, so we looked at pretty much all of them and at one end SVM is we're doing clearly better.",
            "24 out of 25 SPS are doing better.",
            "At the other end it was the opposite, so we definitely saw this difference, and I think the way to go here obviously is some kind of hybrid generative discriminative model that combines both.",
            "Trying to get one of my students to write a paper on that he hasn't, so it's it's sitting out there available for somebody to do."
        ],
        [
            "Something interesting in that context.",
            "Alright, let me give you a second example of using supervised information in a potentially interesting way.",
            "So here's a topic that was learned from Corpus, and if we look at the high probability words, we say, OK, this is probably about families, family life, child, father, etc."
        ],
        [
            "What's interesting is.",
            "Go to it thesaurus.",
            "You can also get a definition of a family, and these are the words in blue.",
            "And it's a different sort of a representation of the concept of family.",
            "It has a lot of words that we don't see very commonly in English.",
            "Birthright, brood heirloom etc dynastic and so forth.",
            "But it doesn't have any notion of a distribution.",
            "How often these words are used, so both sort of sources of information here potentially useful going forward if the topic model could know about these other words, it could generalize better to datasets and perhaps learn better if it.",
            "Use them as prior knowledge and you can also imagine that if I were somehow able to put probabilities on these words for to fit the source information to data, I could then look at how well does it Thesaurus match particular data set, or map documents into the thesaurus, and it turns out you can do this very simply, it it's not.",
            "It's fairly straightforward thing to do and some work in the last few."
        ],
        [
            "As we've done it so you just treat the terraces as prior knowledge.",
            "An associate each concept in the Thesaurus as a topic in your topic model.",
            "And so now the words associated with the Service Concept Act essentially as a prior for the topic.",
            "And there's different ways you can do that.",
            "You know one of the questions, of course, is how strong should that prior be?",
            "And I'm not going to get into that story, but you can basically do different types of things to figure that out from the topic learning POV.",
            "You've got this great starting point.",
            "You know about what concepts may you may be looking for, so you can also learn.",
            "You know you can have free topics that are not associated with the concepts in the source and see what that picks up as well and there.",
            "From the Taurus POV, you're essentially overlaying a probabilistic model on top of something that was up to that point.",
            "Essentially more of a logical type of represent."
        ],
        [
            "Mission.",
            "So here's an example using.",
            "The science part of a large concept set, and there may be, you know, 10s of thousands of these concepts in some of these thesauri.",
            "So we've learned which the probabilities of words in terms of how they're associated with concepts that these are the concepts appearing in particular."
        ],
        [
            "Acument and then you can go into the document and hear their color coded with which words go now with.",
            "These are not just arbitrary tropics, these are these are concepts that are from particular sources that knows about science and so words like charged and so forth get picked up here is it being associated with physics in the right context rather than, you know some other particular concept you can do."
        ],
        [
            "Things like map documents into the tesouras, so a lot of these tests or I or knowledge representation techniques will have hierarchies, and so we've done some work where you again are.",
            "Each of these nodes.",
            "Here is a concept and a set of words are associated with it.",
            "We fit that to a data set and then we get to probabilistic representation of this and then we can take new text and figure out where it fits.",
            "And this is just showing this would be a huge tree, but it's just showing this particular article is about neonatal development.",
            "And breathing and lungs and so forth.",
            "So again, you might not be able to see that, but it's picked out of chemistry node it's picked out.",
            "I can hardly see this myself.",
            "Pregnancy and birth and breathing, so it's sort of focused in in the hours what's relevant from the text itself, and I think there's a lot more that could be done here.",
            "Actually, this is just some work that we did, and if you're into."
        ],
        [
            "Addiction I said earlier that in principle that we should get better quality topics out by initializing it with the these concepts and this is an experiment showing the log probability of new documents where here we just learn topics without any any tesouras dotted one.",
            "We added this Cambridge International Dictionary Thesaurus and this is the Open Directory project where we just took the words on the web pages associated with particular concepts.",
            "And because we are looking at scientific data here, this did the best it.",
            "Really, I did very well and the topic model itself started overfish after awhile."
        ],
        [
            "OK, so this is just a picture from Wikipedia.",
            "I think there's a ton of information out there that's very noisy.",
            "I mean, if you look at Wikipedia categories and Wikipedia pages, there is a lot of SKU and noise there, but I think there's a lot more that could be done with combining these types of."
        ],
        [
            "Alright, so now into the second part of the talk.",
            "Let's see how we're doing on time or what's the got about 15 minutes maybe.",
            "50 OK alright good man.",
            "So I'm going to shift gears, but hopefully I will be able to persuade you that the techniques and ideas are really not radically different from what we've been talking about that.",
            "In fact, they're very in many respects."
        ],
        [
            "So we need a little bit of notation, so we'll start off by talking about.",
            "And I'm really talking mostly about social networks alone.",
            "Some of my examples documents will be the nodes, but I'm thinking more about social networks, so the notation will reflect that.",
            "So we have N in the social Network World.",
            "You call them actors.",
            "This is the node set, and we'll assume that even if we're looking at things overtime, that we magically know the set of actors and the apriori, and that is fixed.",
            "And it's an interesting problem.",
            "In a real world, problem of actors come and go.",
            "So people who attend the ECM LTC today.",
            "Conference from year to year.",
            "That's not a static set, but we'll ignore that problem as everybody else.",
            "Most other people do.",
            "The edges between the actors will think of initially, at least in the standard case, as an adjacency matrix, Y&N by N matrix.",
            "So why Subscripti J, which will see in some of the upcoming slides, indicates an edge between actor Ryan Active J.",
            "In the simplest case, you could think of that as being binary undirected.",
            "Maybe it's directed and more complicated case.",
            "It might be account.",
            "And we also talk about covariates and attributes.",
            "And again, this is very prevalent in real world data.",
            "You just don't have the network you have.",
            "For example, for each individual you might have the age of the person.",
            "If you're a company gathering data about your customers.",
            "If you're a scientist looking at the scientific literature, you might have Co author relations and you have the text documents that people wrote and also on edges.",
            "The edges themselves may have attributes that could have text associated with them.",
            "If we're looking at email, for example."
        ],
        [
            "So an example of exactly that this is from Hewlett-Packard Labs.",
            "Data collected over six months and we see some interesting structure here of sort of core dense cores and people on the periphery.",
            "And again, there's a lot of additional information here about how many emails were sent when, where they sent that we can sort of ignore and just collapse this into an aggregate graph for them."
        ],
        [
            "Moment at least.",
            "Another way to look at this type of data.",
            "Here is a matrix of senders and receivers at a large study that was done 3000 people over three months and there's a dot with depending on how big the how many emails were sent.",
            "You can see some of the senders sent.",
            "You know this is probably somebody broadcasting some, maybe some kind of spam account or conference announcement, or 7 announcement account.",
            "This was that particular campus.",
            "There's a lot of potential structure."
        ],
        [
            "This data so modeling the you know there's a long history in social network analysis of trying to model these kinds of datasets.",
            "This is not something that computer scientists, machine learning data mining people have come onto.",
            "And latent variable models turned out to be very useful in this context.",
            "If there is a rich history of looking at what are called exponential random graph models, but they're really hard to work with if you think if you know a little bit about Markov random fields, imagine much harder version of Markov random fields for the normalization constant is even even like exponentially harder than that to estimate.",
            "And so even though exponential random graph models are very nice and some sense of Canonical model for social network data.",
            "They don't scale well and they have a fair number of issues that might change, but that's the state of the world as we have it so latent variable models, again like in the text data where we were assuming conditional dependences words given the Z variables and conditional dependence disease given the document mixtures here we're going to choose some kind of latent variable.",
            "I haven't told you what yet, so that the edges are conditionally independent given the latent variables.",
            "So if I somehow magically can find the latent variables, then it will simplify the rest of my model, so that's going to be the aim of the game here.",
            "2 examples in a minute, in fact."
        ],
        [
            "First example is.",
            "Let's embed the nodes in a K dimensional real valued space, so every node will be represented by a K dimensional vector of real numbers.",
            "And the probability of an edge between any pair will be proportional to how or a function of how close they are together.",
            "So if you think in two dimensions, we're essentially trying to find an embedding in a 2 dimensional plane and then.",
            "From that point on, once we know where each node is located, the probabilities of edge is just a function of the distance between nodes, and it's a very simple model for saying what's the probability of edges.",
            "The trick is to find the embedding."
        ],
        [
            "So the details of this this was proposed in about 10 years ago.",
            "Is if we look at the log odds, which is the probability of that there's an edge divided by 1 minus the probability of an edge.",
            "We can write this in this sort of additive form here.",
            "Is the distance between nodes I&J that ZI is the latent position of?",
            "No Dyan, CJ, that position or J.",
            "This is some overall network density parameters, so we can put in a parameter that says yeah, on average we expect even if I know nothing else, any random pair of nodes, there's a probability of an edge between them, so that's nice.",
            "And we can also put in an.",
            "This is sort of something you see in the statistical literature.",
            "Lot is sort of trying to set things up as a regression problem in a regression framework, so you can add in other things you know.",
            "And here in this model they put in.",
            "These are coefficients that we learn and this is a vector for every pair of attributes, so pairwise attributes it might be are the two actors the same gender?",
            "Are they the same age?",
            "Things like that, or how different in age are day and then you can try to learn here.",
            "You'll be learning while the network density parameter, but they how important or relevant or these various attributes, and then the embeddings as well disease and so basically they just set this up right down.",
            "The likelihood for this and say OK off we go optimized.",
            "And the thing works quite nicely."
        ],
        [
            "So this is the likelihood function.",
            "So given disease, you have a very simple form.",
            "It's all the pairs are conditionally independent if you know where it is easier and so you go ahead.",
            "And this is a latent variable optimization problem.",
            "You can do.",
            "You can do gradient descent directly if you want.",
            "They did this in the 1st paper and later paper.",
            "They became more beige and did it with priors and so forth.",
            "There is one thing that they didn't mention or didn't really pay much attention to in the paper.",
            "This product is over all pairs, not just where the edges are, but.",
            "All non edges as well and this comes up when you start to look at these models for if you want to scale them up and there are ways to get around.",
            "This will not sort of obvious idea that has been developed is you break this into a product of terms of the actual edges, which is usually fairly small and then the many many many pairs where there are no edges and that second term of the noise you can approximate that by some kind of sampling you don't actually have to look at every non edge and so that idea you can.",
            "Normalize that and it works.",
            "It works quite well, but if you don't do that you're looking at order N squared, and if your Facebook you're dead in the water.",
            "If you're not even Facebook smaller like even 1000 nodes, you're pretty much it's going to be very slow, so this is the kind of thing that."
        ],
        [
            "Get an in fact, if you know multidimensional scaling where you embed nodes, you insert in two dimensions.",
            "This is very similar, but in a probabilistic statistical framework, so we can add in things like covariates in a principled way."
        ],
        [
            "Now here's a model that Harkins back to what I talked earlier.",
            "An looks very different.",
            "In fact you say, well, wait a minute.",
            "This is not a network model.",
            "I mean how does this relate to our latent embedding ideas?",
            "But I'm going to show you that this is in fact very similar to the model we just talked about.",
            "This is China Bly few years ago, so it's a topic model.",
            "This is the graphical model, much the same as what we were talking about earlier, for they've added in relations between the documents, so maybe citations or Co.",
            "Author relations.",
            "And so these new variables here.",
            "These wise, the same wise we were just talking about our one or zero and there one if an edge exists and zero if an edge doesn't exist and they are influenced by the topics of the pair of documents.",
            "In this case that we're thinking of connecting.",
            "And so if documents are more similar, there's this probability.",
            "Here goes up, and one of the nice things about this model.",
            "It's a nicely written paper you that the topics influenced the links.",
            "If you don't know where the links are, the topics can tell you about where the links might be and the other direction as well, the links influence the topics of two documents are linked, their topics are more likely to be similar, and they show experimentally.",
            "They get some nice results here."
        ],
        [
            "So I'm running through here some different examples where I'm headed is to show you that the fact you can look at them all in the same way and another idea in terms of latent variable modeling is to basically do clustering, but with some probabilities attached to it.",
            "And this is called stochastic block modeling in the social network analysis literature.",
            "So we partition our nodes into K blocks or groups that are considered structurally equivalent in the sense that they they act the same in the network, and so P of YIJ now is replaced by subscripts, kin KJ.",
            "You just inherit the.",
            "Probabilities of your group, so you're not an individual anymore if you're in the professor group, your probability of having an edge to a student is just generic for professors to students, not you to the individual student, and so you reduce the number of parameters hugely down to K by K and."
        ],
        [
            "Nice interpretations as well as learning roles and so forth.",
            "And here's a picture."
        ],
        [
            "Show something about that so you have two levels of parameters.",
            "You have this K by K matrix of.",
            "Pairwise probabilities between the groups which have K is small enough is not too hard to estimate, and then you need the indicator variables telling you which group each node is in, and you can.",
            "You can use your favorite estimation technique and there's all kinds of extensions of this one that maybe I don't mention is what is important is this mixed membership stochastic block model, where they generalize this idea to say instead of, you know instead of clustering type approach where you're in one group and that's it.",
            "That's your role in totality.",
            "Let's think of an LDA like way of looking at this and say no, you have a distribution over groups, so there's a probability distribution.",
            "Now over the K groups and that gives us a bit more flexibility, particularly in datasets where there's time involved where people may play different roles.",
            "So they today there a professor, but at the weekend there are fishermen, and so that eyes can.",
            "The inherited from different groups and so forth.",
            "OK, so you."
        ],
        [
            "Side view of all of this is possible, and this is not my idea, but Peterhof and other people have shown that basically you can write down the probability of of particular edge as some function.",
            "Maybe it's a logistic function.",
            "Really this F function here is just playing the role of getting us 201 scale, so it's just some kind of solution is called a link function of some distance.",
            "the G is going to be some way of basically comparing ZI and ZJ.",
            "Sociology are going to be latent vectors representing where.",
            "Individual line interviewer Jr.",
            "This is these are covariates over here and this is our density parameter, so we've seen examples like this so."
        ],
        [
            "As I said, this is the combining function.",
            "These are the latent vectors."
        ],
        [
            "And then the other guys here are covariance.",
            "Now where did all the models I've told you about differ is basically and how they picked this G function and what disease represent how they represent the variables."
        ],
        [
            "And of course, the likelihood is quite simple here, it's just you know to get the probability of all of the data.",
            "I just take the product of all of these guys conditioned on knowing disease and the betas."
        ],
        [
            "So the first example of the G function if we just put in the G function as being the distance between the in the Euclidean space, the K dimensional space, I get that general equation up above the."
        ],
        [
            "Others are maybe not quite so obvious.",
            "Peter Hope is a nice example.",
            "Instead of an additive representation, you can think of a multiplicative type of representation where maybe the fact that people are similar means they are less likely to have connections that this would be relationships like husband and wife.",
            "Are those kinds of partnership types of things, and so he's allows kind of a multiplicative relationship which is modulated by this W matrix here, and he has nice examples of where that can be a better model."
        ],
        [
            "The relational topic model.",
            "I promised you.",
            "I'd show you that that's very similar.",
            "Well, disease are the topics of the two nodes.",
            "If you like and you can take Elementwise product of the true topic distributions, and you get basically this representation here, and you can go."
        ],
        [
            "True, you know the stochastic block model.",
            "We have.",
            "the G function is now basically an indicator telling us which.",
            "Of the elements of the K by K matrix we should select."
        ],
        [
            "The mixed membership model can be written the same."
        ],
        [
            "Way there's a binary feature model which is very similar to hopes multiplicative model, except disease are restricted.",
            "Now to be binary and the W doesn't have to be diagonal, and so in some sense when you put up these slides and show that all these models can be written in the form of a more general equation, you can say, well, we don't need to write anymore papers on this stuff.",
            "It's all done, we just need to know the top equation.",
            "Of course everybody's going to keep writing papers including our group, but still it's nice to see that how these things can be linked together.",
            "It suggests different ways that they combine and also the strengths of each."
        ],
        [
            "OK, so that was static networks and most of those are as an static network modeling in some sense, static networks are the hardest thing to work with.",
            "If you think about networks overtime you can start to do things like prediction and evaluate models in terms of how well they're predicting what happens next, and so there's certainly 2 broad ways to look at dynamic networks, discrete time or.",
            "Snapshots where we have, for example, to actors in a school and.",
            "We're looking at every month.",
            "We, you know, the social scientist goes in and measures who's friends with who this month.",
            "So the first year of students in a secondary school, we're looking at how those friendships evolved.",
            "And so here often you're looking at how does the matrix the Y matrix evolved from discrete time to discrete time and people like Tom Snyder's in the Netherlands and Eric staying at CMU, and others have some nice ideas for this."
        ],
        [
            "So if we if we want to use latent variables here, which is natural, then we can start out with our general form up here and you know one of the ideas is you allow disease to move around and put some dynamics on disease and that could be Markov or it could be for example Andrew Moore and a student did some nice sort of linear dynamics for the latent space model.",
            "Different ways to do that."
        ],
        [
            "I we did some work where we looked at the binary representation that the original work by Miller and Griffiths in Jordan and we allowed an individual features in this latent space to have a Markov dynamics overtime.",
            "And so you might be feature.",
            "Might be that you're interested in fishing or you're interested in soccer or something like that, but that might change.",
            "Ending as as you get over so features can turn on persistor.",
            "Turn off and do inference.",
            "I won't say much more than that, but basically.",
            "You have tools that allow you now to take these latent variable models and think about what kind of dynamics you want to put in them overtime and in some sense."
        ],
        [
            "More interesting is the continuous time version of dynamic networks and what I mean by that is that every edge now has a time stamp.",
            "With it you may have birth death edges where you establish friendship or your stab Lish relationship with somebody, and then at last for maybe weeks, months, what have you.",
            "But then it ends, which be one sort of continuous time.",
            "Also of interest, though is instantaneous edges and an example would be email communication or instant messaging or text messaging and so forth.",
            "So there's a lot of data like that.",
            "We can always aggregate this data if we want to.",
            "So, but in a sense, you know the aggregation can cause problems at what unit of time should we aggregate if we're looking at students sending email or text messages to each other?",
            "And were social scientists at any level of aggregation is potentially problematic 'cause we might miss some important feature of the data, so building it by day or by our bye week could be potentially a problem.",
            "So we're interested in techniques that can actually look at the raw data without having to be in it at all.",
            "And here it turns out that you know classic ideas.",
            "What's the simplest idea for counts overtime?",
            "So the.",
            "Plus on stochastic processes is very natural, and so in a sense, can we think of rates between individuals?",
            "So we have an individual instead of thinking of a binary edge or a weighted edge between them, can we think of the relationship between them as a possum process where there's a rate at which they communicate and that might change overtime?",
            "And so this is, you know how people can start with simple ideas and build out and so."
        ],
        [
            "Yes, 5 minutes alright great good and you know so we could think of just doing this if we had a small number of individuals and a huge amount of data we could think of sort of a nonparametric kind of way of doing it.",
            "We just model each pair separately and come up with a cross on rates and maybe we allowed to change overtime.",
            "The real problem here is that is N squared.",
            "You know we're not leaving, would like a way to somehow combine information about individuals.",
            "If the possum right between me and Melo is something the possum right between me and Peter.",
            "Maybe it's related 'cause I'm connected and they're connected, so we should be able to leverage that somehow, so we'd like to parameterise this and do something more interesting than just using raw counts."
        ],
        [
            "And so one nice model in this direction is my colleague at UC Irvine Carter Butts.",
            "Where again, that you could start to see the sort of statistical influence here of thinking of things in terms of regression equations, lambdas, these lambdas for POS on rates are non negative, so we work with the log of them rather than with the quantity itself.",
            "And that means that thing on the right can be positive or negative, and so here you have a sort of a sort of a network effect of how often do people communicate in general and then relative to that there is a communication rate for individual i.e communication rate for individual J.",
            "And then some.",
            "Parameters of basically some parameters and some features that represent what's going on in the network, so the features are specific to inj, for example.",
            "Are they?",
            "It's measuring maybe how persistent their conversations are.",
            "Turn taking maybe static attributes of them, and then the betas are learn.",
            "How important is it in this network?",
            "How are these important or these different features?",
            "So I'm probably not doing a great job at explaining this, but I think you're getting the general gist of this that these features here are changing overtime depending on what's going on in the network.",
            "The betas are telling us how important each one is, and so we're getting changes in the lambdas.",
            "As the network goes forward so."
        ],
        [
            "This basically piecewise constant or inhomogeneous network possible process.",
            "The likelihood is sort of interesting, and again because it wasn't written by machine learning data mining person, the original paper, it doesn't really point out that there is an N squared term in here at every.",
            "For every event you look at all possible pairs of events, they edge that did happen at that time an all N ^2 -- 1.",
            "That didn't happen.",
            "And then you do that for every possible event.",
            "You might have millions of emails being sent, so this this algorithm does not scale up well, but there's computational tricks you can do, and a lot of the ideas here if you're interested in this kind of stuff.",
            "They're building ideas from in survival and event history analysis, basically taking those ideas, putting them in a network context, and there's certainly some interesting techniques that I think we can.",
            "We can learn from there."
        ],
        [
            "We're looking at this for classroom dynamics with an education professor at Stanford and McFarland interesting data set.",
            "We have multiple different sessions with multiple different people were using Bayesian techniques to pull information and using this relation event model, I'm not."
        ],
        [
            "Say anymore about that, let me let me get to the end here and just advertise one other.",
            "Piece of work that we're currently doing.",
            "I talked about Stephen Wolfram's email and were the students I'm looking at.",
            "Are you know your own personal email history?",
            "What kind of tell us about what's going on?",
            "You'd like to be able to take this data.",
            "And also there's other information here about who were these emails being sent to groups of people that were in contact with projects and so forth and get some insight into.",
            "You know what the email doing to our lives, overtime and so."
        ],
        [
            "I have a paper.",
            "It's just coming out where this is looking at different users and I'll deanonymize myself.",
            "I'm this user here and this is over.",
            "I guess I forgot to put in the time axis.",
            "This is about five years in both of these cases and this is about two years over here and this is the raw traffic, the raw counts and these are in Ferd groups and when the groups were active, so you'd see these groups become active.",
            "This person changed institutions and these groups then persist, just sort of.",
            "Good news for Peter and Mellow and title is this was when I was chair of the KDD Conference and the nice thing I noticed is as soon as the conference was over, the traffic went to zero.",
            "I stopped answering emails about the conference.",
            "I figured.",
            "So you guys are on your own after Friday.",
            "I think if if this is anything to go by, this is sort of another instrument to project over party that was funded over five years.",
            "And of course it picked out that group and then looked at the pulse on rates for that group and I notice there was a lot more email in writing the proposal and there wasn't actually doing the project.",
            "I'm not going to say anymore about that, but.",
            "I think professors will relate to that so."
        ],
        [
            "Including comments and let us get to the museum for some some snacks and so forth."
        ],
        [
            "One thing a few things I didn't comment on.",
            "Just want to wrap up the time complexity.",
            "The learning algorithms, which is actually a big deal, especially given what I started with some large datasets.",
            "So just some notation.",
            "M is the number of documents tractors B is the vocabulary size, K dimension hidden variables L is going to be average document their average degree in networks and T. If we have time data, the number of events and."
        ],
        [
            "So basically these this is the complexities.",
            "Time complexities you get, so LDA pair iterations, all of these algorithms EM Gibbs sampling are iterative and here we have an L which is the average number of words per document or average degree.",
            "And that's good because basically end times L is just the number of tokens that we have in the in the document.",
            "Said times K. But when you go to many of these network models this L becomes an end instead of looking at in a document the words that did appear, you're looking at, as it were.",
            "The worst that did appear in all the words that didn't appear.",
            "Or in the network, all the edges that it appears you get this N squared and definitely have to work around it and then as I mentioned in the relation event models, sometimes that's there's a T put on and on top of that, which is all the possible events that could occur."
        ],
        [
            "Things that people are probably who know a lot about this and people in the audience.",
            "We've done a lot of work and this is probably squirming in their seats he didn't mention.",
            "Dimensionality the hidden variables.",
            "How do we select that?",
            "Well, there's a great work on nonparametric Bayesian techniques that I didn't talk about it all.",
            "You can do other Bayesian model selection, pains, hyperparameters and smoothing.",
            "So all of these multinomial's and I purposely left it out because I didn't want there was just not enough time as the bells start to go about with these priors.",
            "How do you pick them?",
            "Do you want to learn the hyperparameters?",
            "It's very important.",
            "See the paper by Wallach and McCallum and the paper from my group where this is discussed.",
            "And if you're using these models seriously you should look at this and think about it.",
            "Not just use out of the box techniques and then interpretability versus black box prediction.",
            "Well, I'll leave that for talking after it's alright."
        ],
        [
            "I need to get off the stage here, but hopefully I've convinced you that latent variable models are useful in analyzing what essentially are high dimensional datasets with a lot of structure in them.",
            "In the last 10 years there has been a lot of progress.",
            "The representational aspect.",
            "This notion of multi membership that instead of objects be coming from a single cluster that they can be sort of admixtures composed of mixtures of things, is very useful and the learning algorithms are certainly progress.",
            "There we can now learn these models fairly routinely from large datasets.",
            "Although there are scalability issues, so there's many many different models in any conference you go to.",
            "There's you know LDA, this and social network that, but there really, you know.",
            "At least I would argue only a few underlying key ideas.",
            "I mean to get our papers accepted, we have to make them look as different as possible to each other.",
            "But if you know, in a sense there's only so many ways to skin the cash, and so I'm hopefully convinced you that there's there's a few underlying ideas that underlie many of these models, an in terms of research opportunities, I think there are underexplored areas, I think semi supervised learning safer LDA is underexplored, I think dynamic networks people are already realizing that's that's a really rich area.",
            "There's a lot more to be done there, and I think applications, for example with the topic modeling stuff instead of developing yet another graphical model with, you know, that's almost impossible to fit on a page.",
            "Go find some real applications.",
            "Some collaborators in social Sciences and humanities, and some people are doing that.",
            "I think that's what needs to be done.",
            "Alright, I'll leave a few pointers to further work that I got.",
            "Some of my ideas from and finish at that point.",
            "Thank you.",
            "You mentioned that you assume that that information in a static network is in conditional independence, but I doubt there where.",
            "It is.",
            "Assumption is sound curse.",
            "You know that the formation of beings might be can.",
            "It might be dependent on the formation of other links.",
            "Especially it might be proven by some social theories.",
            "So I wonder whether this kind of model could model this kind of phenomenon.",
            "Yes, excellent point.",
            "All of these conditional independence assumptions are gross assumptions.",
            "They're essentially like naive Bayes and supervised learning.",
            "I think.",
            "I you know what?",
            "What social scientists do in statisticians do is they approached on a case by case basis?",
            "And if there's a data set where it's clearly grossly violated, then they wouldn't use a latent variable model with conditional independence.",
            "But I people have found that it's not.",
            "It's not too bad, it's you know, so it's the usual issue of tradeoff between a model that's parsimonious and we can work with versus it is a violating what's really going on.",
            "But maybe the 1st order it can capture what's going on, so, but you are correct that it's in the real world.",
            "These assumptions will be will be violated, certainly.",
            "And another question, miss in your welcome dynamic networks you mentioned the the person read is very varying, but how do how do I?",
            "How do I detect on and evaluate this kind of varying?",
            "I mean is it possible to?",
            "To to to estimate the real real rate real person rate and try to compare this.",
            "Compare radio estimated.",
            "Yeah, I think we can talk in detail afterwards about that, but there's a few different ways.",
            "In the email example I showed at the end we do piecewise.",
            "We do a change point type of analysis where we look for significant jumps in the parcel rates which you tend to see you know for projects.",
            "Suddenly there's a lot of activity and then and then.",
            "In other cases, it can be more of a smooth process, so there's different ways that you can do it.",
            "I'm happy to chat and point you in more detail.",
            "We can talk about that after casing.",
            "Could next person do we have one?",
            "Anybody?",
            "When it will be time for you to chat with them in a in the reception they used.",
            "Is the next meeting in the museum next door for reception.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This evening I will be talking about text and social network data.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I first should acknowledge a lot of the work that I'll be describing.",
                    "label": 0
                },
                {
                    "sent": "His work with collaborators, indeed, other peoples work as well, so I've listed here some of my students, collaborators, and various funding agencies that have helped support all of this work.",
                    "label": 0
                },
                {
                    "sent": "Few minutes ago, the 10 year award was mentioned from Helsinki.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in fact, I spoke at that meeting.",
                    "label": 0
                },
                {
                    "sent": "It was a lot of fun and basically spoke for an hour about this equation, which is basically equation for a finite mixture model where we have a density function on X and we decompose that into components and weights for each of the K different components and today.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After 10 years, I'm coming back with a new equation very similar, except you'll notice that there's a dependence on an extra variable here, D, so hopefully the have a little bit more to say than this, but as we get older, we realized there are a lot of commonality's in what we do, so this is basically the topic model equation, where now the mixture coefficients depend on the individual document rather than being global.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me start by motivating the talk with a few examples and I just pick Facebook.",
                    "label": 0
                },
                {
                    "sent": "Just because there had to be some nice slides flying around.",
                    "label": 0
                },
                {
                    "sent": "But there are many other datasets like this and I think computer science really is changing rapidly and machine learning is a very big part of that, and data mining as these datasets are kind of forcing function on us in terms of what are we going to do with all of this data.",
                    "label": 0
                },
                {
                    "sent": "So Facebook has, you know, by their own accounts, order of 500 million users give or take a few 100 million and the amount of data that they collect.",
                    "label": 0
                },
                {
                    "sent": "For example the graph that they have on average each.",
                    "label": 0
                },
                {
                    "sent": "User has about 130 other users that are connected to so you get a graph with 60 billion edges and you can imagine trying to analyze that kind of data in addition to the network.",
                    "label": 0
                },
                {
                    "sent": "They also get, you know, by their own slides, 30 billion pieces of content every month and 3 billion photos.",
                    "label": 0
                },
                {
                    "sent": "So this kind of interlinked data is provides all kinds of interesting opportunities that even ten years ago when many of us were in Helsinki, it was unimaginable that we could get access to data like this and.",
                    "label": 0
                },
                {
                    "sent": "Course we may not have Facebook data, but there are many other datasets similar to this that we can think of working with.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Even at the individual level, if we're not a Facebook, we just have our own data.",
                    "label": 0
                },
                {
                    "sent": "The amount of data that we're collecting is still staggering.",
                    "label": 0
                },
                {
                    "sent": "Some of you may be aware of Stephen Wolfram on his blog he posted recently.",
                    "label": 0
                },
                {
                    "sent": "All of his email data.",
                    "label": 1
                },
                {
                    "sent": "This is from 1990 up to 2012.",
                    "label": 0
                },
                {
                    "sent": "Every dot.",
                    "label": 0
                },
                {
                    "sent": "Here is an email sent.",
                    "label": 0
                },
                {
                    "sent": "Email received, I don't remember exactly as a function of time of day an it's interesting to look at sort of his behavior over this time.",
                    "label": 0
                },
                {
                    "sent": "For example, this is when he slept.",
                    "label": 0
                },
                {
                    "sent": "There was relatively little email and it was getting later and later and then I believe this is when one of his books came out, so he kind of did a re reboot and got back into more normal schedule.",
                    "label": 0
                },
                {
                    "sent": "You can also see things like evening time and so forth so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There were going to increasing to be collecting this kind of data as most of us are aware not just for things like email but also help type of data.",
                    "label": 0
                },
                {
                    "sent": "Exercise with centers all around us and so forth.",
                    "label": 0
                },
                {
                    "sent": "So this kind of individual data analysis is going to become more and more important.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not just computer scientists and that are collecting this kind of data.",
                    "label": 0
                },
                {
                    "sent": "Again, if you start walking around your campus, you'll realize that all your colleagues in other departments are becoming involved in data collection, so this is an example from history.",
                    "label": 0
                },
                {
                    "sent": "One of the interesting things that's happening in history is there digitising many texts, turning them into ASCII.",
                    "label": 0
                },
                {
                    "sent": "Previously historian would have to go to the library.",
                    "label": 0
                },
                {
                    "sent": "Look at microfiche would be a slow process.",
                    "label": 0
                },
                {
                    "sent": "Now they have access to all of their data at their fingertips.",
                    "label": 0
                },
                {
                    "sent": "So the Pennsylvania Gazette is probably the most famous newspaper in the United States in its time was founded by Benjamin Franklin and a lot of very interesting information there.",
                    "label": 0
                },
                {
                    "sent": "For historians studying US history recently became available in the last few years online, and you can imagine trying to study this just using keyword search.",
                    "label": 0
                },
                {
                    "sent": "Not so easy.",
                    "label": 0
                },
                {
                    "sent": "So historians are interested in humanities in general, looking for tools that can help them analyze these very large amounts of text.",
                    "label": 0
                },
                {
                    "sent": "And rather than having to read every single document.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a common themes that you're going to see, and again, many of you are very familiar with these ideas.",
                    "label": 0
                },
                {
                    "sent": "In fact, there's many people in the audience who worked on many of the techniques that will talk about will be, you know, taking this rich data and what we do in machine learning and data mining is flatten it and make it as simple as possible.",
                    "label": 0
                },
                {
                    "sent": "Put it into vector and matrix form so we can do use our usual nice analytical tools, so we'll be talking about matrices within documents, W words, bag of words, type representation, or for social network data, square matrices where we're looking at perhaps just binary entries, or perhaps.",
                    "label": 1
                },
                {
                    "sent": "Accounts and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What where I think it gets interesting is when we have additional side information which you pretty much always do in practice, so we don't usually just have the documents in the words or the social network.",
                    "label": 0
                },
                {
                    "sent": "We often also have metadata in the sense of time when they coined the information was recorded, or we may have multiple copies of the information at different times.",
                    "label": 0
                },
                {
                    "sent": "We may have covariates, attributes, alters of the documents, what Journal did appear in, or what source and sometimes will have supervised information as well.",
                    "label": 0
                },
                {
                    "sent": "I'd say a little bit more about that later.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why use latent variables?",
                    "label": 1
                },
                {
                    "sent": "Part of the issue is with many of these datasets.",
                    "label": 0
                },
                {
                    "sent": "There really isn't.",
                    "label": 0
                },
                {
                    "sent": "You can't just turn it into a standard classification or regression problem.",
                    "label": 0
                },
                {
                    "sent": "You're dealing with data that there just isn't an obvious target variable to work with and.",
                    "label": 0
                },
                {
                    "sent": "So latent variables when you're trying to model high dimensional data can be can be very handy.",
                    "label": 0
                },
                {
                    "sent": "Very useful, at least the way that there tend to be used and one of the things from a probabilistic point of view is that we use some kind of conditional independence assumption too.",
                    "label": 0
                },
                {
                    "sent": "It simplified the model and essentially make the say the the nodes in a network or the words in a document conditionally independent of each other given the latent variable.",
                    "label": 1
                },
                {
                    "sent": "And you'll see this pattern over and over again with all of these latent variable models.",
                    "label": 0
                },
                {
                    "sent": "One of the things I've tried to get across in my talk is even though these models look very different in a very different terminology, there's really a few basic principles guiding them, and one of them is find a latent variable that if you know the value of the latent variable, everything becomes conditionally independent, which which makes your likelihood and or objective function.",
                    "label": 1
                },
                {
                    "sent": "Much more simple to work with.",
                    "label": 0
                },
                {
                    "sent": "So the representation of form of these models is often very very simple.",
                    "label": 0
                },
                {
                    "sent": "Learning the models can be more complicated, and there's been a fair bit of progress on that in the last few years.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting things when we use latent variable or hidden variable models.",
                    "label": 0
                },
                {
                    "sent": "What is the purpose?",
                    "label": 1
                },
                {
                    "sent": "What is our interpretation of the latent variables?",
                    "label": 0
                },
                {
                    "sent": "And there are situations computer vision would be a good example for the latent variables represent something we could actually measure if we had access to the scene that the camera where the camera was positioned, how far away is the object?",
                    "label": 0
                },
                {
                    "sent": "How many objects were there?",
                    "label": 0
                },
                {
                    "sent": "There is an actual true answer to the question if we just go back in.",
                    "label": 0
                },
                {
                    "sent": "Had enough data to measure it, but in many cases things are not measurable, and in that situation it may be that we're using our latent variable.",
                    "label": 1
                },
                {
                    "sent": "Modeling is a black box, for example principal component analysis or hidden Markov models might be examples there and then.",
                    "label": 0
                },
                {
                    "sent": "In other situations, the latent variables are not measurable, but we don't.",
                    "label": 0
                },
                {
                    "sent": "We do want to interpret what they are.",
                    "label": 0
                },
                {
                    "sent": "We're looking for some insight into the data, and scientists like this.",
                    "label": 0
                },
                {
                    "sent": "If you work with scientists, biologists or even historians.",
                    "label": 0
                },
                {
                    "sent": "For example, they love having tools that can suggest theories that that might be in the data.",
                    "label": 0
                },
                {
                    "sent": "In fact, those of you that have worked with scientists will find that they're very willing to adopt latent variable models like principal components and give them physical interpretations that might not be really justified.",
                    "label": 0
                },
                {
                    "sent": "But that's something you have to be aware aware of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And doing this in a statistical framework, at least I find, is very useful.",
                    "label": 0
                },
                {
                    "sent": "There are many of the problems I'll talk about there are.",
                    "label": 0
                },
                {
                    "sent": "You can approach them in a statistical probabilistic way, or you can use a more optimization.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe a clustering algorithm that doesn't really have any probabilistic semantics.",
                    "label": 0
                },
                {
                    "sent": "I find that the probabilistic approach very useful.",
                    "label": 0
                },
                {
                    "sent": "There's a rich literature you can build on many ideas.",
                    "label": 0
                },
                {
                    "sent": "Many of these ideas going back a long way before people had the computational tools to use them.",
                    "label": 0
                },
                {
                    "sent": "And you have you have a foundation, particularly when you start putting in things like time into the into these models you have all of the techniques from time series from Markov modeling, stochastic processes that you can leverage and take advantage of, so it gives you a great starting point.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you're trying to estimate parameters, then you have that machinery as well.",
                    "label": 0
                },
                {
                    "sent": "But it is important, and I think we sometimes get carried away that all of this comes with a cost.",
                    "label": 0
                },
                {
                    "sent": "So if person A comes along with K means and they can take a data set and analyze it and be done and go home and eat their dinner and.",
                    "label": 0
                },
                {
                    "sent": "It's all night.",
                    "label": 0
                },
                {
                    "sent": "The business person or the scientist is nice and happy.",
                    "label": 0
                },
                {
                    "sent": "Versus perhaps somebody comes along with a very complicated during the process thing and requires 6 PhD students to go with the algorithm to make it work well.",
                    "label": 0
                },
                {
                    "sent": "You know, we need to keep that in mind so so there is a kind of a cautionary note that the complexity of some of these models can get quite complicated and similar models sometimes are better.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of that was by way of introduction what I'm going to talk about is essentially in two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part will be about text, and particularly about topic models.",
                    "label": 0
                },
                {
                    "sent": "I'll review basic concepts and I I'm thinking most people in the audience familiar with this, but I want to make sure everybody is on the same page, so were some of the basic concepts, and then I'll talk about some newer results that you might not be familiar with in semi supervised learning with topic models that I think are quite interesting.",
                    "label": 0
                },
                {
                    "sent": "I then switch over to looking at network data, but hopefully in a way where you'll see the connections that the models for network data or not so different from.",
                    "label": 1
                },
                {
                    "sent": "The types of things I talked about for text Ann will talk, time permitting about both static networks an an area where I think there's a lot more interesting things to be done, which is dynamic.",
                    "label": 0
                },
                {
                    "sent": "Networks where you observe a network overtime and then we finish up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's start to talk about text and just some very very simple concepts here.",
                    "label": 0
                },
                {
                    "sent": "Will talk a lot about multinomial distributions and just what is a multinomial?",
                    "label": 0
                },
                {
                    "sent": "Well, let's say this is a particular example here.",
                    "label": 0
                },
                {
                    "sent": "This was in fact learn from a data set where we have the most likely words.",
                    "label": 0
                },
                {
                    "sent": "Now keep in mind here this is just the top 10 or so words for this multinomial.",
                    "label": 0
                },
                {
                    "sent": "They each have a probability the probabilities sum to one and then the number of words might be in the 10s of thousands.",
                    "label": 0
                },
                {
                    "sent": "So we're just looking at a few words here that have most of the probability mass.",
                    "label": 0
                },
                {
                    "sent": "The words at the bottom of the list.",
                    "label": 0
                },
                {
                    "sent": "We went way down in the ground would have 10 to the minus 5 or 10.",
                    "label": 0
                },
                {
                    "sent": "To the minus you know.",
                    "label": 0
                },
                {
                    "sent": "We have very little probability, so this is a distribution that's very heavily skewed, sort or certain words, and the way we use it in our model is.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a die.",
                    "label": 0
                },
                {
                    "sent": "So a die has six sides with each is equally likely.",
                    "label": 0
                },
                {
                    "sent": "This particular die will have maybe 50,000 sides.",
                    "label": 0
                },
                {
                    "sent": "It will have aside for each word, and it's not in each of the science is not equally likely they're going to be skewed in this manner.",
                    "label": 0
                },
                {
                    "sent": "So the word president is much more likely than some of the other words.",
                    "label": 0
                },
                {
                    "sent": "OK, so when we talk about multinomial, that's what we'll be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the way this is used.",
                    "label": 0
                },
                {
                    "sent": "It has it's.",
                    "label": 0
                },
                {
                    "sent": "There's a conditional independence assumption that like just like in a dire a coin.",
                    "label": 0
                },
                {
                    "sent": "When we use the model, the next word doesn't depend on the last word.",
                    "label": 0
                },
                {
                    "sent": "It only depends on the parameters and.",
                    "label": 0
                },
                {
                    "sent": "So that's what this picture is telling us.",
                    "label": 0
                },
                {
                    "sent": "This graphical model saying we have the word probabilities which sum to one vector of them and each different word.",
                    "label": 1
                },
                {
                    "sent": "The first word, second word third word only depends on the model parameters, not on the last worked.",
                    "label": 0
                },
                {
                    "sent": "Now they're obviously this is the bag of words assumption, and so we're not looking at the sequence of words in the text where looks just looking at the counts essentially.",
                    "label": 0
                },
                {
                    "sent": "And for some.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Felicity will use plate diagram so we don't have to keep rewriting these things an if you haven't seen play targets just means that this is replicated end times given the parent here, so it's the same as the previous picture.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then replications.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to do some anything interesting.",
                    "label": 0
                },
                {
                    "sent": "We don't want to have a single multinomial we want.",
                    "label": 0
                },
                {
                    "sent": "We'd like to have multiple multinomial's.",
                    "label": 0
                },
                {
                    "sent": "Just having one multinomial for language would not be that interesting, and so here we have two an these these are going to be our topics, so when you hear about topic models, the topics themselves are multinomial distributions over words, and this is the one we had before and over here we have a completely different set of high probability words, and these are all about colors.",
                    "label": 0
                },
                {
                    "sent": "And this in fact was a topic that was learned from some.",
                    "label": 0
                },
                {
                    "sent": "Educational text, and so a very different focus in different very different set of words.",
                    "label": 0
                },
                {
                    "sent": "But these other words would appear somewhere on the list with very low probability, but they're very different probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to represent that in our graphical model, this is the same picture we had before, but we've added a little bit.",
                    "label": 0
                },
                {
                    "sent": "We have an indicator variable now that can take say K values.",
                    "label": 0
                },
                {
                    "sent": "K is our number of topics that we're going to have here.",
                    "label": 0
                },
                {
                    "sent": "It might be hundred 200 something like that.",
                    "label": 0
                },
                {
                    "sent": "And well, OK, so it's called sort of K. It's T and over here now we need T different distributions and so we have a plate representing the T distributions and so basically this is just going to switch if Z is equal to 7 then we look up the 7th.",
                    "label": 0
                },
                {
                    "sent": "Multinomial over here.",
                    "label": 0
                },
                {
                    "sent": "So this is a nice notation for these kinds of models.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this to do clustering?",
                    "label": 0
                },
                {
                    "sent": "We just sort of keep building up the picture here and we've added another plate here.",
                    "label": 0
                },
                {
                    "sent": "Now for D documents so words are conditionally independent inside of a document, their Z variable associated with each document.",
                    "label": 1
                },
                {
                    "sent": "So that's the class label or cluster label for that document.",
                    "label": 0
                },
                {
                    "sent": "Each has its own associated with one of these distributions.",
                    "label": 0
                },
                {
                    "sent": "Over here and then, because we have a place here, these documents are assumed to be independent of each other and they depend on disease.",
                    "label": 0
                },
                {
                    "sent": "Depend on some.",
                    "label": 0
                },
                {
                    "sent": "Overall, mixture weights, so this would have been the story, probably about 10 or 15 years ago.",
                    "label": 1
                },
                {
                    "sent": "In terms of probabilistic clustering of documents, and pretty much anybody used this model realized that yeah, it's OK, but documents often have multiple teams in the multiple topics.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple change representation Lee.",
                    "label": 0
                },
                {
                    "sent": "Which we've done here is to move the Z variable into the word level, so previously it was out here at the document level.",
                    "label": 0
                },
                {
                    "sent": "All we've done here is we've moved it inside, and we've also moved the mixture weights inside this plate.",
                    "label": 0
                },
                {
                    "sent": "So now each document has a distribution over topics, and now each Z variable, each word is can be coming from a different topic if we want.",
                    "label": 0
                },
                {
                    "sent": "So we might have a document that's focused on topics one and seven, with probabilities .5, and then we sample words from those two topics.",
                    "label": 0
                },
                {
                    "sent": "Given given disease so very, very simple, but actually quite powerful and it's sort of interesting that people knew about this model.",
                    "label": 0
                },
                {
                    "sent": "Prior to 2000, but we didn't really have good ways to estimate it.",
                    "label": 0
                },
                {
                    "sent": "It wasn't that easy to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model, so just to clarify, you know an example of how this works.",
                    "label": 0
                },
                {
                    "sent": "Here is a paper from your body and colleagues that was at nips many years ago, maybe 94 or so, and it's interesting because it really is bridging two different fields.",
                    "label": 0
                },
                {
                    "sent": "It's taking hidden Markov models and using them for sequence analysis and bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "Now today that's very standard, but at the time this was really combining two things that were very different to each other.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you use clustering, we just ran this 2K means we get a cluster of.",
                    "label": 0
                },
                {
                    "sent": "These are the sort of important words in the cluster that are, well.",
                    "label": 0
                },
                {
                    "sent": "They're kind of a mixture of different words about neural networks and hidden Markov, and so forward.",
                    "label": 0
                },
                {
                    "sent": "It's not particularly good representation if we.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These topics it's assigned to these two different topics, so we get a much nicer representation of the document.",
                    "label": 0
                },
                {
                    "sent": "And again, many of you have used topic models.",
                    "label": 0
                },
                {
                    "sent": "You're aware of this, but I just want to bring everybody sort of up to speed on this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is the equation that's representing things, so the probability of word on the left here is represented by probability of the different under different topics.",
                    "label": 1
                },
                {
                    "sent": "We sum out over the topic variable Z here and we have the probability of the different topics occurring in that document.",
                    "label": 0
                },
                {
                    "sent": "Fairly simple, but we still haven't said how we're going to learn this from data.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also think of this as a matrix decomposition or matrix factorization, which can have its advantages, and it's similar to PCA or SVD, but a little bit different in that the matrix on the left is.",
                    "label": 0
                },
                {
                    "sent": "The probabilities of words given the document, so each row would be a probability of award.",
                    "label": 0
                },
                {
                    "sent": "Given that document rather than the actual counts themselves and on the right we have two sort of skinny matrices, where are if you like, dimension reduction is happening with the topics here, so the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First one is the coefficients.",
                    "label": 0
                },
                {
                    "sent": "If you like for each document and the second one each.",
                    "label": 0
                },
                {
                    "sent": "Role here is a given topic and then it's the multinomial that has the different probabilities of the different words, and so if we sort of look at how this works with the equation probability, particular word in a particular document is basically the dot product of the coefficients for that document across topics, and then the probabilities of words given particular word given those different topics.",
                    "label": 0
                },
                {
                    "sent": "So column from the from the second matrix, so they're very nice analogy here with things that we.",
                    "label": 0
                },
                {
                    "sent": "Do in other contexts, such as principle components and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And indeed, if you look at the history of this model, people had used the idea of matrix decomposition for text as LSA.",
                    "label": 0
                },
                {
                    "sent": "And the problem with that was that we're modeling counts, and LSA is based on principle components, SPD, which is really suited for real valued data.",
                    "label": 0
                },
                {
                    "sent": "So Thomas Huffman was really the person who brought topic modeling to forward his papers back in the late 1990s, the idea had been circulating around in image analysis, say at NIPS and other places, and also was present in some statistics papers.",
                    "label": 0
                },
                {
                    "sent": "But people really haven't figured out how to fit this model, so image analysis, for example, the idea that.",
                    "label": 0
                },
                {
                    "sent": "An object is composed of multiple parts, is very appealing, and so people had been thinking a lot about that kind of a model.",
                    "label": 0
                },
                {
                    "sent": "These kind of factor models.",
                    "label": 0
                },
                {
                    "sent": "Thomas applied it to text and I think immediately a lot of people said this is interesting.",
                    "label": 0
                },
                {
                    "sent": "This is this is taking us beyond clustering.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting, sort of.",
                    "label": 0
                },
                {
                    "sent": "Historically, the paper to get most of the attention is the David Blank paper, which is a great paper that came along later, but it was really building very strongly on.",
                    "label": 0
                },
                {
                    "sent": "Thomas is Thomas's work and then you had the Gibbs sampling extension, which allowed people to easily estimate these models.",
                    "label": 0
                },
                {
                    "sent": "And then many, many, many extensions and applications since the.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we haven't yet said anything about how we learn this model, and again, there are many people in the audience who know a lot about this, so, but some may not.",
                    "label": 0
                },
                {
                    "sent": "So what do we actually need to learn?",
                    "label": 1
                },
                {
                    "sent": "So this is back to our graphical model and we have we have these distributions of words given the topics we have.",
                    "label": 0
                },
                {
                    "sent": "What else do we have?",
                    "label": 1
                },
                {
                    "sent": "We have the mixtures for each document of the topics for that particular document, and we have these latent variables.",
                    "label": 0
                },
                {
                    "sent": "These every single word hassey attached to it.",
                    "label": 0
                },
                {
                    "sent": "So in theory, if you didn't know anything about learning, unsupervised learning would say this.",
                    "label": 0
                },
                {
                    "sent": "Maybe is a little bit.",
                    "label": 0
                },
                {
                    "sent": "Unconstrained, but the structure of texts actually one of the things as you know, if you work with these models is that you it works quite remarkably well, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the ways to think about how did the learning algorithm they especially give sampling learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Pretend if we knew disease if we didn't know disease, what would you be able to do?",
                    "label": 0
                },
                {
                    "sent": "Well, the problem simplifies straight away, so I've colored in green as if disease were known.",
                    "label": 1
                },
                {
                    "sent": "Now I know he Z the topic of every word in every document, so estimating the topic distributions the topic mixtures for any document distributor just count up.",
                    "label": 0
                },
                {
                    "sent": "And similarly I know across the corpus for every word what it's assigned to, so I can easily figure out just again by counting.",
                    "label": 0
                },
                {
                    "sent": "For a given topic, what is distribution over words is so getting a disease gives us an easy way to get Peter and five the other parameters and.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's that's how the Gibbs sampling learning algorithm essentially.",
                    "label": 0
                },
                {
                    "sent": "So we have our bag of words input.",
                    "label": 1
                },
                {
                    "sent": "We have the number of topics we have, no labels, the.",
                    "label": 1
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The collapse keep sampling basically technical trick in sampling techniques to integrate out Peter Anfi and just figure out if I just wanted to know disease.",
                    "label": 0
                },
                {
                    "sent": "What would I do if from abrasion POV and you get a very simple update equation that update equation has to be executed on every word in every document, maybe hundreds or thousands of times you do this kind of collapsed Gibbs sampling, but it's linear in the number of word tokens and then T and then once you have samples of these you can get.",
                    "label": 1
                },
                {
                    "sent": "Point estimates of Theta and Phi.",
                    "label": 0
                },
                {
                    "sent": "So that's basically the Gibbs sampling in Matlab.",
                    "label": 0
                },
                {
                    "sent": "You can write it in a few lines which has been part of the appeal of topic models that it's very easy to use.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Outputs then or the topic word.",
                    "label": 0
                },
                {
                    "sent": "Probably situations.",
                    "label": 0
                },
                {
                    "sent": "The document topic, probability distributions and if you want to look at the individual disease you have them.",
                    "label": 0
                },
                {
                    "sent": "They tend to be quite noisy, but you can average over them.",
                    "label": 0
                },
                {
                    "sent": "Get sentence level word level, section level, etc.",
                    "label": 0
                },
                {
                    "sent": "I should also say there are other learning algorithms variationally in other techniques, but the collapsed Gibbs sampling is so easy that people have.",
                    "label": 0
                },
                {
                    "sent": "Used it.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think topic modeling is at a point where really it needs to find applications.",
                    "label": 0
                },
                {
                    "sent": "There were so many papers being published on topic modeling, including my own group, but it's at a point now where we really start saying how can we use this to help people solve problems.",
                    "label": 0
                },
                {
                    "sent": "One of the projects in my group right now is funded by the US government.",
                    "label": 0
                },
                {
                    "sent": "Is we're looking at large amounts of historical scientific literature to try to track the emergence of ideas.",
                    "label": 0
                },
                {
                    "sent": "We're working with people who study the history of science and technology is quite interesting, so as an example.",
                    "label": 0
                },
                {
                    "sent": "We have 49,000 abstracts that are related to the area of DNA microarrays and we're interested in trying to figure out when did than you wanted new ideas.",
                    "label": 1
                },
                {
                    "sent": "How did this area of DNA microarrays in biology?",
                    "label": 0
                },
                {
                    "sent": "How did it emerge?",
                    "label": 0
                },
                {
                    "sent": "What was the process that happened?",
                    "label": 0
                },
                {
                    "sent": "And so if you run the topic model, you get, you know the usual set of different types of topics.",
                    "label": 0
                },
                {
                    "sent": "Ranging and I've ordered them here.",
                    "label": 0
                },
                {
                    "sent": "Just picked 5 and these are the top five words in each topic.",
                    "label": 1
                },
                {
                    "sent": "Ties probability.",
                    "label": 1
                },
                {
                    "sent": "I'm sort of very basic technology to things that are more application oriented.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning you have topics like just the basic technology of the microarray chips.",
                    "label": 0
                },
                {
                    "sent": "Then you have the emergence of topics that are more.",
                    "label": 0
                },
                {
                    "sent": "How do you use the data coming off the chips for a computer scientist started, get involved and then.",
                    "label": 0
                },
                {
                    "sent": "Over here you have biologists turning to use actually use the data and clinicians to solve real problems.",
                    "label": 0
                },
                {
                    "sent": "So what's nice is if you.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at this through the lens of the topic model.",
                    "label": 0
                },
                {
                    "sent": "Let me explain how this graph works.",
                    "label": 0
                },
                {
                    "sent": "So this is time going this direction and this is the fraction of words per year assigned to a particular topic.",
                    "label": 1
                },
                {
                    "sent": "There were 100 topics in the model, so if everything was equally likely and we took all the documents from, say, the year 2000, then everything would be here where we're showing.",
                    "label": 0
                },
                {
                    "sent": "Here are four topics that are over represented in that year.",
                    "label": 0
                },
                {
                    "sent": "In fact, the account these four account for nearly 50% of all the words and these topics.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you can read it.",
                    "label": 0
                },
                {
                    "sent": "But they're very technology device physics oriented.",
                    "label": 0
                },
                {
                    "sent": "There the papers that we published at that time were very focused on just how do you build these gadgets and get them to work, and then what's interesting is overtime.",
                    "label": 0
                },
                {
                    "sent": "The prevalence of these went down and since these.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Refraction something else must be going up and you see patients and treatment classification pathways and networks themselves.",
                    "label": 1
                },
                {
                    "sent": "You know much more biological clinical application types of topics emerging, and there's quite a quite a pronounced change here, and so people were collaborating with are very interested in.",
                    "label": 0
                },
                {
                    "sent": "Can you quantify, you know the papers that they tend to write in the history of science and technology are somewhat hand WAVY.",
                    "label": 0
                },
                {
                    "sent": "They do some keyword searches, and this potentially gives them much more powerful tools to try to quantify what was going on.",
                    "label": 0
                },
                {
                    "sent": "And so the funding agencies, for example, are very interested in this type of analysis.",
                    "label": 0
                },
                {
                    "sent": "There are limits, of course, to what you can do, but it gives them something more quantitative where they can look at funding programs and see what was happening.",
                    "label": 0
                },
                {
                    "sent": "So we're also looking at patterns here and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coming back to the historical example, the Pennsylvania Gazette data.",
                    "label": 0
                },
                {
                    "sent": "This is from a colleague of mine, Dave Newman, where again the types that this particular topic that they found in the data is about constitutional law.",
                    "label": 0
                },
                {
                    "sent": "And of course you know it's nice that in 1776 this starts to to increase and.",
                    "label": 0
                },
                {
                    "sent": "The you can see how historians would be interested in using this as a new tool and are using it as a new tool in analyzing this kind of data.",
                    "label": 0
                },
                {
                    "sent": "I should also say that the examples I'm showing here, the model knows nothing about time.",
                    "label": 0
                },
                {
                    "sent": "This is very naive, very simple, where we're just fitting a model to all of the data and then retrospectively binning the data by year and counting a more sophisticated approach here and there are many such algorithms would be to put time into the model, but they tend to be a little bit trickier to get to work.",
                    "label": 0
                },
                {
                    "sent": "There's a variety of such techniques out there.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you another example.",
                    "label": 0
                },
                {
                    "sent": "This is the Enron email data set, and for any young people interested in going out and making a lot of money, let me suggest a good start up would be used.",
                    "label": 0
                },
                {
                    "sent": "Some kind of text analysis techniques and sell them to lawyers to legal companies.",
                    "label": 0
                },
                {
                    "sent": "So if you go to work for any law firm, say as a consultant, you'll find that there's still buried in paper and PDF files, and they use keyword search and really could use a lot of help, and so the reason I'm mentioning this is because the Enron email data set became available.",
                    "label": 0
                },
                {
                    "sent": "Through the legal action of the US government who prosecuted them several years ago and 250,000 emails were subpoenaed and arrived at the US Justice Department's desk.",
                    "label": 0
                },
                {
                    "sent": "And you can imagine being a lawyer working, say, junior lawyer in the office.",
                    "label": 0
                },
                {
                    "sent": "You come in on Monday morning.",
                    "label": 0
                },
                {
                    "sent": "Your boss is great.",
                    "label": 0
                },
                {
                    "sent": "We just got all the emails from Enron, or at least some of the emails from Enron.",
                    "label": 0
                },
                {
                    "sent": "And you're thrilled.",
                    "label": 0
                },
                {
                    "sent": "That's great.",
                    "label": 0
                },
                {
                    "sent": "And then that your boss says an were in court on Wednesday.",
                    "label": 0
                },
                {
                    "sent": "And I need you to find the you know the 100 emails that are most relevant and use them in court.",
                    "label": 0
                },
                {
                    "sent": "And so if you had to use keyword search searching through all these emails it will be very difficult.",
                    "label": 0
                },
                {
                    "sent": "So again, techniques clustering, topic modeling and so forth.",
                    "label": 0
                },
                {
                    "sent": "I think have a lot of potential there, but we can't just sort of throw them over the fence to people in lower in history that we need to work with them, collaborate with them to build useful tools.",
                    "label": 0
                },
                {
                    "sent": "So the types of topics that you will get if you run an Enron email are things like.",
                    "label": 0
                },
                {
                    "sent": "Sorry, come back here.",
                    "label": 0
                },
                {
                    "sent": "Standard sort of things about constructing, you know, construction.",
                    "label": 0
                },
                {
                    "sent": "The environmental issues about air quality.",
                    "label": 0
                },
                {
                    "sent": "These are the types of topics you might expect.",
                    "label": 0
                },
                {
                    "sent": "Enron was a large energy company, and so these are typical of what we might.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expect maybe we wouldn't expect, but in hindsight is obvious would be there are these personal topics, so holiday parties and Ron is in Texas and they like their football American football, so they bet a lot on these.",
                    "label": 0
                },
                {
                    "sent": "There's a whole topic about that.",
                    "label": 0
                },
                {
                    "sent": "There was a Christian topic and was warned about online shopping.",
                    "label": 0
                },
                {
                    "sent": "So again, if you are the lawyer, you might say, OK, I'm not interested in any of this stuff.",
                    "label": 0
                },
                {
                    "sent": "This is just not relevant.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You do find sort of political topics.",
                    "label": 0
                },
                {
                    "sent": "There was a big issue about power in California, and in particular bankruptcy aspect of that.",
                    "label": 0
                },
                {
                    "sent": "Also, there's an interesting topic about lobbying in Washington, some topic about lawsuits and what's nice is you can go in and say where the emails that go with these particular topics, and there's actually some very interesting emails in that data set.",
                    "label": 0
                },
                {
                    "sent": "If you have time to go and look.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so many of you have seen much of this before, and so from here on.",
                    "label": 0
                },
                {
                    "sent": "Hopefully some of this would be newer.",
                    "label": 0
                },
                {
                    "sent": "There are many, many extensions that people have come up with.",
                    "label": 0
                },
                {
                    "sent": "Some people probably in this room and I can't mention everybody's name, but some of the highlights would be, you know, putting time into the model so you can properly take time into account putting correlation into it.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric techniques so we don't fix the number of topics ahead of time, author, topic models and an idea that my group has picked up on and I think is very useful and maybe underappreciated.",
                    "label": 0
                },
                {
                    "sent": "Is this directly?",
                    "label": 0
                },
                {
                    "sent": "Normal regression idea from David Mill and Andrew McCallum, where you can pretty much add in arbitrary metadata into the model using sort of a general regression framework, and that seems quite nice.",
                    "label": 0
                },
                {
                    "sent": "It's sort of students meant much of the earlier work like alter topics and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me let me move on to some work that maybe is less familiar to you and let me motivate this by saying.",
                    "label": 0
                },
                {
                    "sent": "You know one of the nice things about topic models clustering techniques is you just throw data added.",
                    "label": 0
                },
                {
                    "sent": "You don't need any prior knowledge, it's just purely data driven and that's great.",
                    "label": 1
                },
                {
                    "sent": "But when you start to work with people in applications you realize there is in fact often a lot of prior knowledge available that you might want to use.",
                    "label": 0
                },
                {
                    "sent": "In particular, dealing with text.",
                    "label": 0
                },
                {
                    "sent": "In some sense, it seems almost silly.",
                    "label": 0
                },
                {
                    "sent": "You know your 4 year old would say you know Daddy where you're trying to learn all this just from data.",
                    "label": 0
                },
                {
                    "sent": "When there's a lot of information out there already that structured.",
                    "label": 0
                },
                {
                    "sent": "So let me expand a little bit on what I mean.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first example is going to talk about is labeled data, which you know for text documents is often small, but still very, very useful.",
                    "label": 0
                },
                {
                    "sent": "So if we look at the problem for a few minutes of multi label classification for documents and here are three datasets that are fairly widely used in the literature, but they're actually relatively small, particularly in terms of the number of unique labels that are in the datasets.",
                    "label": 0
                },
                {
                    "sent": "So this is the Reuters data set, about 100 labels, and the Yahoo Arts and health datasets.",
                    "label": 1
                },
                {
                    "sent": "Which each have order of 20 labels and the median number of documents per individual label.",
                    "label": 1
                },
                {
                    "sent": "So every label is associated with certain number of documents is quite high, hundreds or thousands, and so discriminative methods tend to work fine here.",
                    "label": 0
                },
                {
                    "sent": "SVM's one versus all, and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you start to look at, you know real world datasets.",
                    "label": 0
                },
                {
                    "sent": "And let's look at 2 examples here.",
                    "label": 0
                },
                {
                    "sent": "This is the year like status at a European legal document set that has labels that has come out in the last few years.",
                    "label": 0
                },
                {
                    "sent": "And this is the New York Times annotated data set.",
                    "label": 1
                },
                {
                    "sent": "You very nice data set with very rich annotations.",
                    "label": 0
                },
                {
                    "sent": "That's also being made available the last few years.",
                    "label": 0
                },
                {
                    "sent": "It the numbers are almost reversed.",
                    "label": 0
                },
                {
                    "sent": "Now you have thousands of unique labels.",
                    "label": 0
                },
                {
                    "sent": "In fact, both of these datasets are really the sum of the publicly available versions that we used in experiments.",
                    "label": 0
                },
                {
                    "sent": "There's really orders of magnitude more labels that you could be using.",
                    "label": 0
                },
                {
                    "sent": "And the median number of documents per label is now much smaller.",
                    "label": 1
                },
                {
                    "sent": "It's just you know, order of you know 10 or one even.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, if we look at this graph here for these two corpora, this is where we sort the labels by how many documents are associated with each one.",
                    "label": 0
                },
                {
                    "sent": "So if you have a dot over here, it means this is a label.",
                    "label": 0
                },
                {
                    "sent": "We have one label with 10 to the three documents associated with it.",
                    "label": 0
                },
                {
                    "sent": "Over here we have.",
                    "label": 0
                },
                {
                    "sent": "Label that has one document associated with it and there's over 10 to three such labels.",
                    "label": 0
                },
                {
                    "sent": "So you see this sort of dramatic power log type picture or distribution for both of these datasets.",
                    "label": 0
                },
                {
                    "sent": "In other words, there's many more labels down here.",
                    "label": 0
                },
                {
                    "sent": "There's a huge number of labels that have only a single document in the corpus associated with them, and then smaller number that have two documents in the smaller number of three, but it's very skewed towards the left, so most of your labels have almost.",
                    "label": 1
                },
                {
                    "sent": "No data and so discriminative methods are going to have a problem if there's only one.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I say training document one positive example.",
                    "label": 0
                },
                {
                    "sent": "So this idea is being picked up on by our group and others.",
                    "label": 0
                },
                {
                    "sent": "Very very simple idea.",
                    "label": 0
                },
                {
                    "sent": "So we take topic modeling and we associate each label with the topic.",
                    "label": 1
                },
                {
                    "sent": "And during learning during that Gibbs sampling, where if you remember the learning algorithm is going through an for every word token it's saying OK, which given everything else, I know which label should be assigned to which topic should be assigned to.",
                    "label": 1
                },
                {
                    "sent": "So instead of looking at all the possible topics, are all the possible labels here, you just restrict it to the assigned labels for that particular document.",
                    "label": 0
                },
                {
                    "sent": "So instead of 1000 possible labels, you say no human said there's this document is about these three labels, so I just sample from those.",
                    "label": 0
                },
                {
                    "sent": "So what in effect the?",
                    "label": 0
                },
                {
                    "sent": "Topic model does.",
                    "label": 0
                },
                {
                    "sent": "Here is it learns to associate the labels in the multi label Document said with which words within documents go with it.",
                    "label": 0
                },
                {
                    "sent": "It comes up with probability distributions and in fact it's nice because it's much faster than the typical topic model which has to consider all of the possible topics.",
                    "label": 1
                },
                {
                    "sent": "Now it's being told to focusing on the few and the key difference with the discriminative method here is that it's you're doing the labeling at the word level, not at the document level.",
                    "label": 1
                },
                {
                    "sent": "And we'll see that that gives you some advantages with these.",
                    "label": 0
                },
                {
                    "sent": "Very large label sets in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "The other thing that's useful here is to build in label dependencies.",
                    "label": 0
                },
                {
                    "sent": "If you work with multi label document classification, you know that it's a difficult problem to figure out how to build in dependencies between labels and in the probabilistic framework we have ways to do that and it's fairly straightforward manner and it does improve performance significantly, so let me.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give you an example of how this works in practice.",
                    "label": 0
                },
                {
                    "sent": "Here's an article from the New York Times, at least the first few lines of it, and it has three labels attached to it.",
                    "label": 0
                },
                {
                    "sent": "It's an article about lawsuits and video games, so maybe a patent lawsuit?",
                    "label": 0
                },
                {
                    "sent": "Antitrust, I guess in this case, and this is how many other?",
                    "label": 0
                },
                {
                    "sent": "Well, how many documents in total have these labels?",
                    "label": 0
                },
                {
                    "sent": "So the first 2 have or 1967, but the video games label itself.",
                    "label": 0
                },
                {
                    "sent": "This is the only documented peers in so.",
                    "label": 0
                },
                {
                    "sent": "Which you imagine might happen is that a discriminative classifier here is going to have trouble figuring out what words go with video games, just given that it has one label and has two other labels attached to this document as well.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And sure enough, when we look at the SVM and look at the the words that appear to be relevant, there's some words associated with video games in Nintendo.",
                    "label": 0
                },
                {
                    "sent": "But then there's a lot of other somewhat random words here.",
                    "label": 0
                },
                {
                    "sent": "An LDA is able to pick out essentially the right words here, and it's the idea you know that in your pearls book, if you remember if you've looked at explaining away the other words have been the non videogame words have already been explained away by.",
                    "label": 0
                },
                {
                    "sent": "The models for antitrust actions and suits and litigation, and the remaining words are the ones that are picked up here by LDA, so it works out quite nicely.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And indeed, when we look at if you remember, we had three datasets and these are the three where there's a lot of data for a small number of labels, and these are the ones where there's a very large number of labels on average, very few documents per label, and this is how much better LDA is doing versus SVM.",
                    "label": 0
                },
                {
                    "sent": "These SPMS were tuned to be as best as possible.",
                    "label": 0
                },
                {
                    "sent": "They're not sort of.",
                    "label": 0
                },
                {
                    "sent": "We try to be absolutely as far as we can here, and there was definitely a significant difference these guys are doing.",
                    "label": 0
                },
                {
                    "sent": "The Ellie's doing well here precisely because of the example in the previous slide it can.",
                    "label": 0
                },
                {
                    "sent": "Pick up on these documents or labels for their very few documents when you start to get over to the data, set Rogers that had a lot of documents per individual label.",
                    "label": 0
                },
                {
                    "sent": "Are the SVM start to pick up an advantage because their discriminative?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way to look at this is we in this paper, which came out fairly recently.",
                    "label": 0
                },
                {
                    "sent": "We measured a lot of different metrics.",
                    "label": 0
                },
                {
                    "sent": "There's a number if you do multiple document classification, there are quite a few different metrics people use, so we looked at pretty much all of them and at one end SVM is we're doing clearly better.",
                    "label": 0
                },
                {
                    "sent": "24 out of 25 SPS are doing better.",
                    "label": 0
                },
                {
                    "sent": "At the other end it was the opposite, so we definitely saw this difference, and I think the way to go here obviously is some kind of hybrid generative discriminative model that combines both.",
                    "label": 0
                },
                {
                    "sent": "Trying to get one of my students to write a paper on that he hasn't, so it's it's sitting out there available for somebody to do.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Something interesting in that context.",
                    "label": 0
                },
                {
                    "sent": "Alright, let me give you a second example of using supervised information in a potentially interesting way.",
                    "label": 0
                },
                {
                    "sent": "So here's a topic that was learned from Corpus, and if we look at the high probability words, we say, OK, this is probably about families, family life, child, father, etc.",
                    "label": 1
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's interesting is.",
                    "label": 0
                },
                {
                    "sent": "Go to it thesaurus.",
                    "label": 0
                },
                {
                    "sent": "You can also get a definition of a family, and these are the words in blue.",
                    "label": 0
                },
                {
                    "sent": "And it's a different sort of a representation of the concept of family.",
                    "label": 0
                },
                {
                    "sent": "It has a lot of words that we don't see very commonly in English.",
                    "label": 0
                },
                {
                    "sent": "Birthright, brood heirloom etc dynastic and so forth.",
                    "label": 1
                },
                {
                    "sent": "But it doesn't have any notion of a distribution.",
                    "label": 0
                },
                {
                    "sent": "How often these words are used, so both sort of sources of information here potentially useful going forward if the topic model could know about these other words, it could generalize better to datasets and perhaps learn better if it.",
                    "label": 0
                },
                {
                    "sent": "Use them as prior knowledge and you can also imagine that if I were somehow able to put probabilities on these words for to fit the source information to data, I could then look at how well does it Thesaurus match particular data set, or map documents into the thesaurus, and it turns out you can do this very simply, it it's not.",
                    "label": 0
                },
                {
                    "sent": "It's fairly straightforward thing to do and some work in the last few.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we've done it so you just treat the terraces as prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "An associate each concept in the Thesaurus as a topic in your topic model.",
                    "label": 1
                },
                {
                    "sent": "And so now the words associated with the Service Concept Act essentially as a prior for the topic.",
                    "label": 0
                },
                {
                    "sent": "And there's different ways you can do that.",
                    "label": 0
                },
                {
                    "sent": "You know one of the questions, of course, is how strong should that prior be?",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to get into that story, but you can basically do different types of things to figure that out from the topic learning POV.",
                    "label": 0
                },
                {
                    "sent": "You've got this great starting point.",
                    "label": 0
                },
                {
                    "sent": "You know about what concepts may you may be looking for, so you can also learn.",
                    "label": 0
                },
                {
                    "sent": "You know you can have free topics that are not associated with the concepts in the source and see what that picks up as well and there.",
                    "label": 0
                },
                {
                    "sent": "From the Taurus POV, you're essentially overlaying a probabilistic model on top of something that was up to that point.",
                    "label": 0
                },
                {
                    "sent": "Essentially more of a logical type of represent.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "So here's an example using.",
                    "label": 0
                },
                {
                    "sent": "The science part of a large concept set, and there may be, you know, 10s of thousands of these concepts in some of these thesauri.",
                    "label": 0
                },
                {
                    "sent": "So we've learned which the probabilities of words in terms of how they're associated with concepts that these are the concepts appearing in particular.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acument and then you can go into the document and hear their color coded with which words go now with.",
                    "label": 0
                },
                {
                    "sent": "These are not just arbitrary tropics, these are these are concepts that are from particular sources that knows about science and so words like charged and so forth get picked up here is it being associated with physics in the right context rather than, you know some other particular concept you can do.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things like map documents into the tesouras, so a lot of these tests or I or knowledge representation techniques will have hierarchies, and so we've done some work where you again are.",
                    "label": 0
                },
                {
                    "sent": "Each of these nodes.",
                    "label": 0
                },
                {
                    "sent": "Here is a concept and a set of words are associated with it.",
                    "label": 0
                },
                {
                    "sent": "We fit that to a data set and then we get to probabilistic representation of this and then we can take new text and figure out where it fits.",
                    "label": 0
                },
                {
                    "sent": "And this is just showing this would be a huge tree, but it's just showing this particular article is about neonatal development.",
                    "label": 0
                },
                {
                    "sent": "And breathing and lungs and so forth.",
                    "label": 0
                },
                {
                    "sent": "So again, you might not be able to see that, but it's picked out of chemistry node it's picked out.",
                    "label": 0
                },
                {
                    "sent": "I can hardly see this myself.",
                    "label": 0
                },
                {
                    "sent": "Pregnancy and birth and breathing, so it's sort of focused in in the hours what's relevant from the text itself, and I think there's a lot more that could be done here.",
                    "label": 0
                },
                {
                    "sent": "Actually, this is just some work that we did, and if you're into.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Addiction I said earlier that in principle that we should get better quality topics out by initializing it with the these concepts and this is an experiment showing the log probability of new documents where here we just learn topics without any any tesouras dotted one.",
                    "label": 0
                },
                {
                    "sent": "We added this Cambridge International Dictionary Thesaurus and this is the Open Directory project where we just took the words on the web pages associated with particular concepts.",
                    "label": 0
                },
                {
                    "sent": "And because we are looking at scientific data here, this did the best it.",
                    "label": 0
                },
                {
                    "sent": "Really, I did very well and the topic model itself started overfish after awhile.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is just a picture from Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "I think there's a ton of information out there that's very noisy.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you look at Wikipedia categories and Wikipedia pages, there is a lot of SKU and noise there, but I think there's a lot more that could be done with combining these types of.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now into the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "Let's see how we're doing on time or what's the got about 15 minutes maybe.",
                    "label": 0
                },
                {
                    "sent": "50 OK alright good man.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to shift gears, but hopefully I will be able to persuade you that the techniques and ideas are really not radically different from what we've been talking about that.",
                    "label": 0
                },
                {
                    "sent": "In fact, they're very in many respects.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need a little bit of notation, so we'll start off by talking about.",
                    "label": 0
                },
                {
                    "sent": "And I'm really talking mostly about social networks alone.",
                    "label": 0
                },
                {
                    "sent": "Some of my examples documents will be the nodes, but I'm thinking more about social networks, so the notation will reflect that.",
                    "label": 0
                },
                {
                    "sent": "So we have N in the social Network World.",
                    "label": 0
                },
                {
                    "sent": "You call them actors.",
                    "label": 0
                },
                {
                    "sent": "This is the node set, and we'll assume that even if we're looking at things overtime, that we magically know the set of actors and the apriori, and that is fixed.",
                    "label": 1
                },
                {
                    "sent": "And it's an interesting problem.",
                    "label": 0
                },
                {
                    "sent": "In a real world, problem of actors come and go.",
                    "label": 0
                },
                {
                    "sent": "So people who attend the ECM LTC today.",
                    "label": 0
                },
                {
                    "sent": "Conference from year to year.",
                    "label": 0
                },
                {
                    "sent": "That's not a static set, but we'll ignore that problem as everybody else.",
                    "label": 0
                },
                {
                    "sent": "Most other people do.",
                    "label": 1
                },
                {
                    "sent": "The edges between the actors will think of initially, at least in the standard case, as an adjacency matrix, Y&N by N matrix.",
                    "label": 0
                },
                {
                    "sent": "So why Subscripti J, which will see in some of the upcoming slides, indicates an edge between actor Ryan Active J.",
                    "label": 1
                },
                {
                    "sent": "In the simplest case, you could think of that as being binary undirected.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's directed and more complicated case.",
                    "label": 0
                },
                {
                    "sent": "It might be account.",
                    "label": 1
                },
                {
                    "sent": "And we also talk about covariates and attributes.",
                    "label": 0
                },
                {
                    "sent": "And again, this is very prevalent in real world data.",
                    "label": 0
                },
                {
                    "sent": "You just don't have the network you have.",
                    "label": 0
                },
                {
                    "sent": "For example, for each individual you might have the age of the person.",
                    "label": 0
                },
                {
                    "sent": "If you're a company gathering data about your customers.",
                    "label": 0
                },
                {
                    "sent": "If you're a scientist looking at the scientific literature, you might have Co author relations and you have the text documents that people wrote and also on edges.",
                    "label": 0
                },
                {
                    "sent": "The edges themselves may have attributes that could have text associated with them.",
                    "label": 0
                },
                {
                    "sent": "If we're looking at email, for example.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an example of exactly that this is from Hewlett-Packard Labs.",
                    "label": 0
                },
                {
                    "sent": "Data collected over six months and we see some interesting structure here of sort of core dense cores and people on the periphery.",
                    "label": 0
                },
                {
                    "sent": "And again, there's a lot of additional information here about how many emails were sent when, where they sent that we can sort of ignore and just collapse this into an aggregate graph for them.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moment at least.",
                    "label": 0
                },
                {
                    "sent": "Another way to look at this type of data.",
                    "label": 0
                },
                {
                    "sent": "Here is a matrix of senders and receivers at a large study that was done 3000 people over three months and there's a dot with depending on how big the how many emails were sent.",
                    "label": 0
                },
                {
                    "sent": "You can see some of the senders sent.",
                    "label": 0
                },
                {
                    "sent": "You know this is probably somebody broadcasting some, maybe some kind of spam account or conference announcement, or 7 announcement account.",
                    "label": 0
                },
                {
                    "sent": "This was that particular campus.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of potential structure.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This data so modeling the you know there's a long history in social network analysis of trying to model these kinds of datasets.",
                    "label": 0
                },
                {
                    "sent": "This is not something that computer scientists, machine learning data mining people have come onto.",
                    "label": 0
                },
                {
                    "sent": "And latent variable models turned out to be very useful in this context.",
                    "label": 1
                },
                {
                    "sent": "If there is a rich history of looking at what are called exponential random graph models, but they're really hard to work with if you think if you know a little bit about Markov random fields, imagine much harder version of Markov random fields for the normalization constant is even even like exponentially harder than that to estimate.",
                    "label": 0
                },
                {
                    "sent": "And so even though exponential random graph models are very nice and some sense of Canonical model for social network data.",
                    "label": 0
                },
                {
                    "sent": "They don't scale well and they have a fair number of issues that might change, but that's the state of the world as we have it so latent variable models, again like in the text data where we were assuming conditional dependences words given the Z variables and conditional dependence disease given the document mixtures here we're going to choose some kind of latent variable.",
                    "label": 0
                },
                {
                    "sent": "I haven't told you what yet, so that the edges are conditionally independent given the latent variables.",
                    "label": 1
                },
                {
                    "sent": "So if I somehow magically can find the latent variables, then it will simplify the rest of my model, so that's going to be the aim of the game here.",
                    "label": 0
                },
                {
                    "sent": "2 examples in a minute, in fact.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First example is.",
                    "label": 0
                },
                {
                    "sent": "Let's embed the nodes in a K dimensional real valued space, so every node will be represented by a K dimensional vector of real numbers.",
                    "label": 1
                },
                {
                    "sent": "And the probability of an edge between any pair will be proportional to how or a function of how close they are together.",
                    "label": 0
                },
                {
                    "sent": "So if you think in two dimensions, we're essentially trying to find an embedding in a 2 dimensional plane and then.",
                    "label": 1
                },
                {
                    "sent": "From that point on, once we know where each node is located, the probabilities of edge is just a function of the distance between nodes, and it's a very simple model for saying what's the probability of edges.",
                    "label": 0
                },
                {
                    "sent": "The trick is to find the embedding.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the details of this this was proposed in about 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Is if we look at the log odds, which is the probability of that there's an edge divided by 1 minus the probability of an edge.",
                    "label": 0
                },
                {
                    "sent": "We can write this in this sort of additive form here.",
                    "label": 0
                },
                {
                    "sent": "Is the distance between nodes I&J that ZI is the latent position of?",
                    "label": 1
                },
                {
                    "sent": "No Dyan, CJ, that position or J.",
                    "label": 0
                },
                {
                    "sent": "This is some overall network density parameters, so we can put in a parameter that says yeah, on average we expect even if I know nothing else, any random pair of nodes, there's a probability of an edge between them, so that's nice.",
                    "label": 1
                },
                {
                    "sent": "And we can also put in an.",
                    "label": 0
                },
                {
                    "sent": "This is sort of something you see in the statistical literature.",
                    "label": 0
                },
                {
                    "sent": "Lot is sort of trying to set things up as a regression problem in a regression framework, so you can add in other things you know.",
                    "label": 0
                },
                {
                    "sent": "And here in this model they put in.",
                    "label": 0
                },
                {
                    "sent": "These are coefficients that we learn and this is a vector for every pair of attributes, so pairwise attributes it might be are the two actors the same gender?",
                    "label": 0
                },
                {
                    "sent": "Are they the same age?",
                    "label": 0
                },
                {
                    "sent": "Things like that, or how different in age are day and then you can try to learn here.",
                    "label": 0
                },
                {
                    "sent": "You'll be learning while the network density parameter, but they how important or relevant or these various attributes, and then the embeddings as well disease and so basically they just set this up right down.",
                    "label": 0
                },
                {
                    "sent": "The likelihood for this and say OK off we go optimized.",
                    "label": 0
                },
                {
                    "sent": "And the thing works quite nicely.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So given disease, you have a very simple form.",
                    "label": 0
                },
                {
                    "sent": "It's all the pairs are conditionally independent if you know where it is easier and so you go ahead.",
                    "label": 0
                },
                {
                    "sent": "And this is a latent variable optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You can do.",
                    "label": 0
                },
                {
                    "sent": "You can do gradient descent directly if you want.",
                    "label": 0
                },
                {
                    "sent": "They did this in the 1st paper and later paper.",
                    "label": 0
                },
                {
                    "sent": "They became more beige and did it with priors and so forth.",
                    "label": 1
                },
                {
                    "sent": "There is one thing that they didn't mention or didn't really pay much attention to in the paper.",
                    "label": 0
                },
                {
                    "sent": "This product is over all pairs, not just where the edges are, but.",
                    "label": 1
                },
                {
                    "sent": "All non edges as well and this comes up when you start to look at these models for if you want to scale them up and there are ways to get around.",
                    "label": 0
                },
                {
                    "sent": "This will not sort of obvious idea that has been developed is you break this into a product of terms of the actual edges, which is usually fairly small and then the many many many pairs where there are no edges and that second term of the noise you can approximate that by some kind of sampling you don't actually have to look at every non edge and so that idea you can.",
                    "label": 0
                },
                {
                    "sent": "Normalize that and it works.",
                    "label": 0
                },
                {
                    "sent": "It works quite well, but if you don't do that you're looking at order N squared, and if your Facebook you're dead in the water.",
                    "label": 0
                },
                {
                    "sent": "If you're not even Facebook smaller like even 1000 nodes, you're pretty much it's going to be very slow, so this is the kind of thing that.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get an in fact, if you know multidimensional scaling where you embed nodes, you insert in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "This is very similar, but in a probabilistic statistical framework, so we can add in things like covariates in a principled way.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here's a model that Harkins back to what I talked earlier.",
                    "label": 0
                },
                {
                    "sent": "An looks very different.",
                    "label": 0
                },
                {
                    "sent": "In fact you say, well, wait a minute.",
                    "label": 0
                },
                {
                    "sent": "This is not a network model.",
                    "label": 0
                },
                {
                    "sent": "I mean how does this relate to our latent embedding ideas?",
                    "label": 0
                },
                {
                    "sent": "But I'm going to show you that this is in fact very similar to the model we just talked about.",
                    "label": 0
                },
                {
                    "sent": "This is China Bly few years ago, so it's a topic model.",
                    "label": 1
                },
                {
                    "sent": "This is the graphical model, much the same as what we were talking about earlier, for they've added in relations between the documents, so maybe citations or Co.",
                    "label": 0
                },
                {
                    "sent": "Author relations.",
                    "label": 0
                },
                {
                    "sent": "And so these new variables here.",
                    "label": 0
                },
                {
                    "sent": "These wise, the same wise we were just talking about our one or zero and there one if an edge exists and zero if an edge doesn't exist and they are influenced by the topics of the pair of documents.",
                    "label": 0
                },
                {
                    "sent": "In this case that we're thinking of connecting.",
                    "label": 0
                },
                {
                    "sent": "And so if documents are more similar, there's this probability.",
                    "label": 0
                },
                {
                    "sent": "Here goes up, and one of the nice things about this model.",
                    "label": 0
                },
                {
                    "sent": "It's a nicely written paper you that the topics influenced the links.",
                    "label": 0
                },
                {
                    "sent": "If you don't know where the links are, the topics can tell you about where the links might be and the other direction as well, the links influence the topics of two documents are linked, their topics are more likely to be similar, and they show experimentally.",
                    "label": 1
                },
                {
                    "sent": "They get some nice results here.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm running through here some different examples where I'm headed is to show you that the fact you can look at them all in the same way and another idea in terms of latent variable modeling is to basically do clustering, but with some probabilities attached to it.",
                    "label": 0
                },
                {
                    "sent": "And this is called stochastic block modeling in the social network analysis literature.",
                    "label": 0
                },
                {
                    "sent": "So we partition our nodes into K blocks or groups that are considered structurally equivalent in the sense that they they act the same in the network, and so P of YIJ now is replaced by subscripts, kin KJ.",
                    "label": 1
                },
                {
                    "sent": "You just inherit the.",
                    "label": 0
                },
                {
                    "sent": "Probabilities of your group, so you're not an individual anymore if you're in the professor group, your probability of having an edge to a student is just generic for professors to students, not you to the individual student, and so you reduce the number of parameters hugely down to K by K and.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice interpretations as well as learning roles and so forth.",
                    "label": 0
                },
                {
                    "sent": "And here's a picture.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show something about that so you have two levels of parameters.",
                    "label": 0
                },
                {
                    "sent": "You have this K by K matrix of.",
                    "label": 1
                },
                {
                    "sent": "Pairwise probabilities between the groups which have K is small enough is not too hard to estimate, and then you need the indicator variables telling you which group each node is in, and you can.",
                    "label": 0
                },
                {
                    "sent": "You can use your favorite estimation technique and there's all kinds of extensions of this one that maybe I don't mention is what is important is this mixed membership stochastic block model, where they generalize this idea to say instead of, you know instead of clustering type approach where you're in one group and that's it.",
                    "label": 1
                },
                {
                    "sent": "That's your role in totality.",
                    "label": 0
                },
                {
                    "sent": "Let's think of an LDA like way of looking at this and say no, you have a distribution over groups, so there's a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Now over the K groups and that gives us a bit more flexibility, particularly in datasets where there's time involved where people may play different roles.",
                    "label": 0
                },
                {
                    "sent": "So they today there a professor, but at the weekend there are fishermen, and so that eyes can.",
                    "label": 0
                },
                {
                    "sent": "The inherited from different groups and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so you.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Side view of all of this is possible, and this is not my idea, but Peterhof and other people have shown that basically you can write down the probability of of particular edge as some function.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a logistic function.",
                    "label": 0
                },
                {
                    "sent": "Really this F function here is just playing the role of getting us 201 scale, so it's just some kind of solution is called a link function of some distance.",
                    "label": 0
                },
                {
                    "sent": "the G is going to be some way of basically comparing ZI and ZJ.",
                    "label": 0
                },
                {
                    "sent": "Sociology are going to be latent vectors representing where.",
                    "label": 0
                },
                {
                    "sent": "Individual line interviewer Jr.",
                    "label": 0
                },
                {
                    "sent": "This is these are covariates over here and this is our density parameter, so we've seen examples like this so.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said, this is the combining function.",
                    "label": 0
                },
                {
                    "sent": "These are the latent vectors.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the other guys here are covariance.",
                    "label": 0
                },
                {
                    "sent": "Now where did all the models I've told you about differ is basically and how they picked this G function and what disease represent how they represent the variables.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, the likelihood is quite simple here, it's just you know to get the probability of all of the data.",
                    "label": 0
                },
                {
                    "sent": "I just take the product of all of these guys conditioned on knowing disease and the betas.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first example of the G function if we just put in the G function as being the distance between the in the Euclidean space, the K dimensional space, I get that general equation up above the.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Others are maybe not quite so obvious.",
                    "label": 0
                },
                {
                    "sent": "Peter Hope is a nice example.",
                    "label": 0
                },
                {
                    "sent": "Instead of an additive representation, you can think of a multiplicative type of representation where maybe the fact that people are similar means they are less likely to have connections that this would be relationships like husband and wife.",
                    "label": 0
                },
                {
                    "sent": "Are those kinds of partnership types of things, and so he's allows kind of a multiplicative relationship which is modulated by this W matrix here, and he has nice examples of where that can be a better model.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The relational topic model.",
                    "label": 0
                },
                {
                    "sent": "I promised you.",
                    "label": 0
                },
                {
                    "sent": "I'd show you that that's very similar.",
                    "label": 0
                },
                {
                    "sent": "Well, disease are the topics of the two nodes.",
                    "label": 0
                },
                {
                    "sent": "If you like and you can take Elementwise product of the true topic distributions, and you get basically this representation here, and you can go.",
                    "label": 1
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "True, you know the stochastic block model.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "the G function is now basically an indicator telling us which.",
                    "label": 0
                },
                {
                    "sent": "Of the elements of the K by K matrix we should select.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The mixed membership model can be written the same.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way there's a binary feature model which is very similar to hopes multiplicative model, except disease are restricted.",
                    "label": 0
                },
                {
                    "sent": "Now to be binary and the W doesn't have to be diagonal, and so in some sense when you put up these slides and show that all these models can be written in the form of a more general equation, you can say, well, we don't need to write anymore papers on this stuff.",
                    "label": 0
                },
                {
                    "sent": "It's all done, we just need to know the top equation.",
                    "label": 0
                },
                {
                    "sent": "Of course everybody's going to keep writing papers including our group, but still it's nice to see that how these things can be linked together.",
                    "label": 0
                },
                {
                    "sent": "It suggests different ways that they combine and also the strengths of each.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was static networks and most of those are as an static network modeling in some sense, static networks are the hardest thing to work with.",
                    "label": 0
                },
                {
                    "sent": "If you think about networks overtime you can start to do things like prediction and evaluate models in terms of how well they're predicting what happens next, and so there's certainly 2 broad ways to look at dynamic networks, discrete time or.",
                    "label": 1
                },
                {
                    "sent": "Snapshots where we have, for example, to actors in a school and.",
                    "label": 0
                },
                {
                    "sent": "We're looking at every month.",
                    "label": 0
                },
                {
                    "sent": "We, you know, the social scientist goes in and measures who's friends with who this month.",
                    "label": 0
                },
                {
                    "sent": "So the first year of students in a secondary school, we're looking at how those friendships evolved.",
                    "label": 0
                },
                {
                    "sent": "And so here often you're looking at how does the matrix the Y matrix evolved from discrete time to discrete time and people like Tom Snyder's in the Netherlands and Eric staying at CMU, and others have some nice ideas for this.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we if we want to use latent variables here, which is natural, then we can start out with our general form up here and you know one of the ideas is you allow disease to move around and put some dynamics on disease and that could be Markov or it could be for example Andrew Moore and a student did some nice sort of linear dynamics for the latent space model.",
                    "label": 0
                },
                {
                    "sent": "Different ways to do that.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I we did some work where we looked at the binary representation that the original work by Miller and Griffiths in Jordan and we allowed an individual features in this latent space to have a Markov dynamics overtime.",
                    "label": 0
                },
                {
                    "sent": "And so you might be feature.",
                    "label": 0
                },
                {
                    "sent": "Might be that you're interested in fishing or you're interested in soccer or something like that, but that might change.",
                    "label": 0
                },
                {
                    "sent": "Ending as as you get over so features can turn on persistor.",
                    "label": 1
                },
                {
                    "sent": "Turn off and do inference.",
                    "label": 0
                },
                {
                    "sent": "I won't say much more than that, but basically.",
                    "label": 0
                },
                {
                    "sent": "You have tools that allow you now to take these latent variable models and think about what kind of dynamics you want to put in them overtime and in some sense.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More interesting is the continuous time version of dynamic networks and what I mean by that is that every edge now has a time stamp.",
                    "label": 1
                },
                {
                    "sent": "With it you may have birth death edges where you establish friendship or your stab Lish relationship with somebody, and then at last for maybe weeks, months, what have you.",
                    "label": 0
                },
                {
                    "sent": "But then it ends, which be one sort of continuous time.",
                    "label": 0
                },
                {
                    "sent": "Also of interest, though is instantaneous edges and an example would be email communication or instant messaging or text messaging and so forth.",
                    "label": 1
                },
                {
                    "sent": "So there's a lot of data like that.",
                    "label": 1
                },
                {
                    "sent": "We can always aggregate this data if we want to.",
                    "label": 0
                },
                {
                    "sent": "So, but in a sense, you know the aggregation can cause problems at what unit of time should we aggregate if we're looking at students sending email or text messages to each other?",
                    "label": 0
                },
                {
                    "sent": "And were social scientists at any level of aggregation is potentially problematic 'cause we might miss some important feature of the data, so building it by day or by our bye week could be potentially a problem.",
                    "label": 0
                },
                {
                    "sent": "So we're interested in techniques that can actually look at the raw data without having to be in it at all.",
                    "label": 0
                },
                {
                    "sent": "And here it turns out that you know classic ideas.",
                    "label": 0
                },
                {
                    "sent": "What's the simplest idea for counts overtime?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Plus on stochastic processes is very natural, and so in a sense, can we think of rates between individuals?",
                    "label": 1
                },
                {
                    "sent": "So we have an individual instead of thinking of a binary edge or a weighted edge between them, can we think of the relationship between them as a possum process where there's a rate at which they communicate and that might change overtime?",
                    "label": 0
                },
                {
                    "sent": "And so this is, you know how people can start with simple ideas and build out and so.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, 5 minutes alright great good and you know so we could think of just doing this if we had a small number of individuals and a huge amount of data we could think of sort of a nonparametric kind of way of doing it.",
                    "label": 0
                },
                {
                    "sent": "We just model each pair separately and come up with a cross on rates and maybe we allowed to change overtime.",
                    "label": 0
                },
                {
                    "sent": "The real problem here is that is N squared.",
                    "label": 0
                },
                {
                    "sent": "You know we're not leaving, would like a way to somehow combine information about individuals.",
                    "label": 0
                },
                {
                    "sent": "If the possum right between me and Melo is something the possum right between me and Peter.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's related 'cause I'm connected and they're connected, so we should be able to leverage that somehow, so we'd like to parameterise this and do something more interesting than just using raw counts.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so one nice model in this direction is my colleague at UC Irvine Carter Butts.",
                    "label": 0
                },
                {
                    "sent": "Where again, that you could start to see the sort of statistical influence here of thinking of things in terms of regression equations, lambdas, these lambdas for POS on rates are non negative, so we work with the log of them rather than with the quantity itself.",
                    "label": 0
                },
                {
                    "sent": "And that means that thing on the right can be positive or negative, and so here you have a sort of a sort of a network effect of how often do people communicate in general and then relative to that there is a communication rate for individual i.e communication rate for individual J.",
                    "label": 0
                },
                {
                    "sent": "And then some.",
                    "label": 0
                },
                {
                    "sent": "Parameters of basically some parameters and some features that represent what's going on in the network, so the features are specific to inj, for example.",
                    "label": 0
                },
                {
                    "sent": "Are they?",
                    "label": 0
                },
                {
                    "sent": "It's measuring maybe how persistent their conversations are.",
                    "label": 0
                },
                {
                    "sent": "Turn taking maybe static attributes of them, and then the betas are learn.",
                    "label": 0
                },
                {
                    "sent": "How important is it in this network?",
                    "label": 0
                },
                {
                    "sent": "How are these important or these different features?",
                    "label": 0
                },
                {
                    "sent": "So I'm probably not doing a great job at explaining this, but I think you're getting the general gist of this that these features here are changing overtime depending on what's going on in the network.",
                    "label": 0
                },
                {
                    "sent": "The betas are telling us how important each one is, and so we're getting changes in the lambdas.",
                    "label": 0
                },
                {
                    "sent": "As the network goes forward so.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This basically piecewise constant or inhomogeneous network possible process.",
                    "label": 1
                },
                {
                    "sent": "The likelihood is sort of interesting, and again because it wasn't written by machine learning data mining person, the original paper, it doesn't really point out that there is an N squared term in here at every.",
                    "label": 1
                },
                {
                    "sent": "For every event you look at all possible pairs of events, they edge that did happen at that time an all N ^2 -- 1.",
                    "label": 0
                },
                {
                    "sent": "That didn't happen.",
                    "label": 0
                },
                {
                    "sent": "And then you do that for every possible event.",
                    "label": 0
                },
                {
                    "sent": "You might have millions of emails being sent, so this this algorithm does not scale up well, but there's computational tricks you can do, and a lot of the ideas here if you're interested in this kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "They're building ideas from in survival and event history analysis, basically taking those ideas, putting them in a network context, and there's certainly some interesting techniques that I think we can.",
                    "label": 1
                },
                {
                    "sent": "We can learn from there.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're looking at this for classroom dynamics with an education professor at Stanford and McFarland interesting data set.",
                    "label": 0
                },
                {
                    "sent": "We have multiple different sessions with multiple different people were using Bayesian techniques to pull information and using this relation event model, I'm not.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say anymore about that, let me let me get to the end here and just advertise one other.",
                    "label": 0
                },
                {
                    "sent": "Piece of work that we're currently doing.",
                    "label": 0
                },
                {
                    "sent": "I talked about Stephen Wolfram's email and were the students I'm looking at.",
                    "label": 0
                },
                {
                    "sent": "Are you know your own personal email history?",
                    "label": 1
                },
                {
                    "sent": "What kind of tell us about what's going on?",
                    "label": 0
                },
                {
                    "sent": "You'd like to be able to take this data.",
                    "label": 0
                },
                {
                    "sent": "And also there's other information here about who were these emails being sent to groups of people that were in contact with projects and so forth and get some insight into.",
                    "label": 0
                },
                {
                    "sent": "You know what the email doing to our lives, overtime and so.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a paper.",
                    "label": 0
                },
                {
                    "sent": "It's just coming out where this is looking at different users and I'll deanonymize myself.",
                    "label": 0
                },
                {
                    "sent": "I'm this user here and this is over.",
                    "label": 0
                },
                {
                    "sent": "I guess I forgot to put in the time axis.",
                    "label": 0
                },
                {
                    "sent": "This is about five years in both of these cases and this is about two years over here and this is the raw traffic, the raw counts and these are in Ferd groups and when the groups were active, so you'd see these groups become active.",
                    "label": 0
                },
                {
                    "sent": "This person changed institutions and these groups then persist, just sort of.",
                    "label": 0
                },
                {
                    "sent": "Good news for Peter and Mellow and title is this was when I was chair of the KDD Conference and the nice thing I noticed is as soon as the conference was over, the traffic went to zero.",
                    "label": 0
                },
                {
                    "sent": "I stopped answering emails about the conference.",
                    "label": 0
                },
                {
                    "sent": "I figured.",
                    "label": 0
                },
                {
                    "sent": "So you guys are on your own after Friday.",
                    "label": 0
                },
                {
                    "sent": "I think if if this is anything to go by, this is sort of another instrument to project over party that was funded over five years.",
                    "label": 0
                },
                {
                    "sent": "And of course it picked out that group and then looked at the pulse on rates for that group and I notice there was a lot more email in writing the proposal and there wasn't actually doing the project.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say anymore about that, but.",
                    "label": 0
                },
                {
                    "sent": "I think professors will relate to that so.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Including comments and let us get to the museum for some some snacks and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing a few things I didn't comment on.",
                    "label": 0
                },
                {
                    "sent": "Just want to wrap up the time complexity.",
                    "label": 0
                },
                {
                    "sent": "The learning algorithms, which is actually a big deal, especially given what I started with some large datasets.",
                    "label": 0
                },
                {
                    "sent": "So just some notation.",
                    "label": 0
                },
                {
                    "sent": "M is the number of documents tractors B is the vocabulary size, K dimension hidden variables L is going to be average document their average degree in networks and T. If we have time data, the number of events and.",
                    "label": 1
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically these this is the complexities.",
                    "label": 0
                },
                {
                    "sent": "Time complexities you get, so LDA pair iterations, all of these algorithms EM Gibbs sampling are iterative and here we have an L which is the average number of words per document or average degree.",
                    "label": 1
                },
                {
                    "sent": "And that's good because basically end times L is just the number of tokens that we have in the in the document.",
                    "label": 0
                },
                {
                    "sent": "Said times K. But when you go to many of these network models this L becomes an end instead of looking at in a document the words that did appear, you're looking at, as it were.",
                    "label": 0
                },
                {
                    "sent": "The worst that did appear in all the words that didn't appear.",
                    "label": 0
                },
                {
                    "sent": "Or in the network, all the edges that it appears you get this N squared and definitely have to work around it and then as I mentioned in the relation event models, sometimes that's there's a T put on and on top of that, which is all the possible events that could occur.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things that people are probably who know a lot about this and people in the audience.",
                    "label": 0
                },
                {
                    "sent": "We've done a lot of work and this is probably squirming in their seats he didn't mention.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "How do we select that?",
                    "label": 1
                },
                {
                    "sent": "Well, there's a great work on nonparametric Bayesian techniques that I didn't talk about it all.",
                    "label": 1
                },
                {
                    "sent": "You can do other Bayesian model selection, pains, hyperparameters and smoothing.",
                    "label": 0
                },
                {
                    "sent": "So all of these multinomial's and I purposely left it out because I didn't want there was just not enough time as the bells start to go about with these priors.",
                    "label": 0
                },
                {
                    "sent": "How do you pick them?",
                    "label": 1
                },
                {
                    "sent": "Do you want to learn the hyperparameters?",
                    "label": 0
                },
                {
                    "sent": "It's very important.",
                    "label": 0
                },
                {
                    "sent": "See the paper by Wallach and McCallum and the paper from my group where this is discussed.",
                    "label": 0
                },
                {
                    "sent": "And if you're using these models seriously you should look at this and think about it.",
                    "label": 0
                },
                {
                    "sent": "Not just use out of the box techniques and then interpretability versus black box prediction.",
                    "label": 1
                },
                {
                    "sent": "Well, I'll leave that for talking after it's alright.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I need to get off the stage here, but hopefully I've convinced you that latent variable models are useful in analyzing what essentially are high dimensional datasets with a lot of structure in them.",
                    "label": 0
                },
                {
                    "sent": "In the last 10 years there has been a lot of progress.",
                    "label": 1
                },
                {
                    "sent": "The representational aspect.",
                    "label": 0
                },
                {
                    "sent": "This notion of multi membership that instead of objects be coming from a single cluster that they can be sort of admixtures composed of mixtures of things, is very useful and the learning algorithms are certainly progress.",
                    "label": 0
                },
                {
                    "sent": "There we can now learn these models fairly routinely from large datasets.",
                    "label": 0
                },
                {
                    "sent": "Although there are scalability issues, so there's many many different models in any conference you go to.",
                    "label": 0
                },
                {
                    "sent": "There's you know LDA, this and social network that, but there really, you know.",
                    "label": 0
                },
                {
                    "sent": "At least I would argue only a few underlying key ideas.",
                    "label": 0
                },
                {
                    "sent": "I mean to get our papers accepted, we have to make them look as different as possible to each other.",
                    "label": 0
                },
                {
                    "sent": "But if you know, in a sense there's only so many ways to skin the cash, and so I'm hopefully convinced you that there's there's a few underlying ideas that underlie many of these models, an in terms of research opportunities, I think there are underexplored areas, I think semi supervised learning safer LDA is underexplored, I think dynamic networks people are already realizing that's that's a really rich area.",
                    "label": 1
                },
                {
                    "sent": "There's a lot more to be done there, and I think applications, for example with the topic modeling stuff instead of developing yet another graphical model with, you know, that's almost impossible to fit on a page.",
                    "label": 0
                },
                {
                    "sent": "Go find some real applications.",
                    "label": 0
                },
                {
                    "sent": "Some collaborators in social Sciences and humanities, and some people are doing that.",
                    "label": 0
                },
                {
                    "sent": "I think that's what needs to be done.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'll leave a few pointers to further work that I got.",
                    "label": 0
                },
                {
                    "sent": "Some of my ideas from and finish at that point.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that you assume that that information in a static network is in conditional independence, but I doubt there where.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "Assumption is sound curse.",
                    "label": 0
                },
                {
                    "sent": "You know that the formation of beings might be can.",
                    "label": 0
                },
                {
                    "sent": "It might be dependent on the formation of other links.",
                    "label": 0
                },
                {
                    "sent": "Especially it might be proven by some social theories.",
                    "label": 0
                },
                {
                    "sent": "So I wonder whether this kind of model could model this kind of phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Yes, excellent point.",
                    "label": 0
                },
                {
                    "sent": "All of these conditional independence assumptions are gross assumptions.",
                    "label": 0
                },
                {
                    "sent": "They're essentially like naive Bayes and supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I you know what?",
                    "label": 0
                },
                {
                    "sent": "What social scientists do in statisticians do is they approached on a case by case basis?",
                    "label": 0
                },
                {
                    "sent": "And if there's a data set where it's clearly grossly violated, then they wouldn't use a latent variable model with conditional independence.",
                    "label": 0
                },
                {
                    "sent": "But I people have found that it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not too bad, it's you know, so it's the usual issue of tradeoff between a model that's parsimonious and we can work with versus it is a violating what's really going on.",
                    "label": 0
                },
                {
                    "sent": "But maybe the 1st order it can capture what's going on, so, but you are correct that it's in the real world.",
                    "label": 0
                },
                {
                    "sent": "These assumptions will be will be violated, certainly.",
                    "label": 0
                },
                {
                    "sent": "And another question, miss in your welcome dynamic networks you mentioned the the person read is very varying, but how do how do I?",
                    "label": 0
                },
                {
                    "sent": "How do I detect on and evaluate this kind of varying?",
                    "label": 0
                },
                {
                    "sent": "I mean is it possible to?",
                    "label": 0
                },
                {
                    "sent": "To to to estimate the real real rate real person rate and try to compare this.",
                    "label": 0
                },
                {
                    "sent": "Compare radio estimated.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I think we can talk in detail afterwards about that, but there's a few different ways.",
                    "label": 0
                },
                {
                    "sent": "In the email example I showed at the end we do piecewise.",
                    "label": 0
                },
                {
                    "sent": "We do a change point type of analysis where we look for significant jumps in the parcel rates which you tend to see you know for projects.",
                    "label": 0
                },
                {
                    "sent": "Suddenly there's a lot of activity and then and then.",
                    "label": 0
                },
                {
                    "sent": "In other cases, it can be more of a smooth process, so there's different ways that you can do it.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to chat and point you in more detail.",
                    "label": 0
                },
                {
                    "sent": "We can talk about that after casing.",
                    "label": 0
                },
                {
                    "sent": "Could next person do we have one?",
                    "label": 0
                },
                {
                    "sent": "Anybody?",
                    "label": 0
                },
                {
                    "sent": "When it will be time for you to chat with them in a in the reception they used.",
                    "label": 0
                },
                {
                    "sent": "Is the next meeting in the museum next door for reception.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}