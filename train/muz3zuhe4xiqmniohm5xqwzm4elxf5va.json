{
    "id": "muz3zuhe4xiqmniohm5xqwzm4elxf5va",
    "title": "No Size Fits All - Running the Star Schema Benchmark with SPARQL and RDF Aggregate Views",
    "info": {
        "introducer": [
            "Axel Polleres, Institute for Information Business, Vienna University of Economics and Business"
        ],
        "author": [
            "Benedikt K\u00e4mpgen, Institute of Applied Informatics and Formal Description Methods (AIFB), Karlsruhe Institute of Technology (KIT)"
        ],
        "published": "July 8, 2013",
        "recorded": "May 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2013_kaempgen_views/",
    "segmentation": [
        [
            "So I'm Benedict Chemkin from kaytee.",
            "It cost for Germany and I would like to present your work on running the Star Schema benchmark with SPAR QL and RDF aggregate views and this."
        ],
        [
            "Work is motivated by the fact that more and more statistical link data is out there.",
            "So statistics published, for example from World Bank from OECD, World Health Organization, Euro Stat SEC, experience finance data or for example Yahoo Finance data with here more concretely stock market trading prices of certain companies are published as linked data and.",
            "What we want to have is a query engine that allows to have analytical queries on such data and return the results to be displayed in resolution visualizations such as you see here line charge of trading prices for selected companies or trading prices in the lower chart pivot table.",
            "The trading price is aggregated overtime and to make."
        ],
        [
            "It more clear what is done here.",
            "So what you have published on the statistical data is basically a data set that has comparable columns like.",
            "I mean it's represented in RDF, but still it is representing a table of companies dates and then there the average trading price on a certain date and it's published using the RDF data Cube vocabulary and this query engine in this case to have this line chart.",
            "Displayed would need to filter for selected for specific companies in this case.",
            "Here for for Visa incorporation and master card.",
            "So you see the trading price the this is the orange line.",
            "This is visa incorporation where you see the average trading price over time.",
            "And the error."
        ],
        [
            "Other example of the pivot table.",
            "Here you are aggregating so you you take the the original data from some statistical data and then you aggregate over dates so the dates are not interesting for you anymore and in the pivot table you look at per row.",
            "You'll see the companies and in the column in the column cells you have the average trading price and of course these two operations filtering and aggregating.",
            "They seem very easy now on this little.",
            "Example data set, but if you increase the number of the size of the table.",
            "Dramatically, these these operations actually are a problem, so you need and this is the black box in the middle.",
            "You need a query engine that allows you to implement such such such such operations efficiently."
        ],
        [
            "So the problem is that typically, OK. How do you now implement such a query engine and one approach that is often used in industry is roll up.",
            "So you would have typical all up clients, issue all up queries to a rollup engine, and that roll up engine would translate the OLAP queries into SQL on relational database that has been pre filled with tables from statistical data in our fixed our schema I will talk about the Star schema more later.",
            "But what you can also do here is you can optimize the these queries further.",
            "If you do pre aggregations.",
            "If you pre aggregate certain values from the original table and store them in aggregate tables.",
            "And the approach now that we have proposed is to, instead of using a rollup engine to use an all up for LD engine, that would also use get OLAP queries, but would translate them into SPARQL queries on a triple store that has been filled with RDF in general.",
            "So in this case we cannot only we only import the data into the Triple Store and also the metadata of the RDF data cube vocabulary, but also we can.",
            "For example, load RDF data from DB PEDIA or Freebase if, for example, the companies are linked to Freebase, Freebase, or DB Pedia.",
            "So the motivation is that in this case you can do more better, more expressive queries so you can for example dynamically or ad hoc ask for you can filter for companies from cities with more than 100,000 inhabitants.",
            "This is possible if this fact.",
            "Sam is represented in the RDF or also you can filter for companies that have a CEO who is younger than 20, so this is the motivation of really using RDF in this case, but I don't want to go into details of this motivation, but rather I want to go a step before that.",
            "'cause we really have the question of what is now the performance difference between using a roll up engine Anna Star schema instead of using this all up for the approach of using a triple store.",
            "And also what we are interested in is whether we can actually use materialized aggregate views.",
            "So whether we can apply something like aggregate tables in RDF."
        ],
        [
            "So what we want to present to you today in this talk is, first of all, a comparison between the rollup approach, which means this is the so that's the first red error and on the left side where we here compare rollup which uses a relational database.",
            "My SQL in this case and which users mandrian as a way to translate the OLAP queries into SQL.",
            "And this is a non much realization approach and this we want to compare with our OLAP foil D approach where we use open virtuoso as the representative triple store and Spark QL for querying the data and for translating the OLAP queries into SPARQL we use our own system all up foil D and then there's in the second part of the talk.",
            "I will go into details of the comparison of our OLAP ALDI approach in comparison with materialization."
        ],
        [
            "So regarding related work, what we do is we compare roll up and all up on a common triplestore, whereas of course there's lots of work on optimizing queries on RDF, like using MapReduce or column stores.",
            "And also we use the RDF data cube vocabulary to represent aggregate views in RDF, which we think no one else has done before.",
            "But of course there's lots of work on representing statistics generally in as linked data and also we materialize aggregate views with.",
            "Insert queries and run a realistic benchmark with more than more than 100 million triples.",
            "But of course there is a lot of work on managing views on RDF datasets."
        ],
        [
            "So in the reminder of this talk, I will first present you the Star Schema benchmark that we have used to get our results.",
            "Then I will go into the details of the 1st result comparing roll up with our olive oil.",
            "And then.",
            "Evaluate the effect of RDF aggregate views on our approach and then I will conclude."
        ],
        [
            "So this task, him a benchmark, is based on TPC H, which is why we have chosen is becausw.",
            "TCH is very often used in relational databases afterward to benchmark and analytical systems, and it gives us metadata about end data about data cube with line orders of a specific company.",
            "So each line order would contain measures, for example the sum of revenues.",
            "Also it would have dimensions with hierarchies.",
            "And for example, each measure would each line order would have dimensions like dates when the order was taken, and this has a hierarchy of every date has a month a year, and it can also be aggregated over.",
            "Also we have supplier dimension for example, which also have a higher CRI of city which is in a nation within a region and can also be aggregated over the oil level.",
            "And also we have chosen these task more benchmark because it specifically uses a logical schema in a star schema which is very often used in industry because it's very easy to use and it's very performant in relational databases typically.",
            "And here we have a huge fact table.",
            "You can you see their schema on the left side on the actual data on the right side we have a huge fact table, in this case with six million line orders and these fact table is linked to.",
            "Dimension tables these dimension tables are denormalized and they represent the entire hierarchy of the specific dimensions."
        ],
        [
            "Also this task in my benchmark gives us the queries.",
            "So for example we have a query that asks for the revenues per year and per product brands of a specific category and a specific supplier and this can be translated into the operation that I've shown you before so we can aggregate.",
            "So we need to aggregate here from dates to the years when we need to aggregate from pasta products to the brand level and we want we are not interested in the customer at all, we just.",
            "Aggregate over it to the oil level and we filter for a specific category in a specific region here.",
            "And this query, if this is run on our OLAP engine then we the results we can display for example in a pivot table as you see on the left lower corner.",
            "Here we have in each row the years the different years and the columns the different brands and the single cells now are created from the original fact table by filtering and aggregation."
        ],
        [
            "In total these SSBB benchmark gives us four equivalent flights.",
            "Query flights means specific types of queries.",
            "The all of these queries of 1 query flight out of the same type and only the selectivity and the granularity changes and we have 13 queries in total and one way to measure the complexity of this queries would be for example to the number of dimensions that are used for filtering or aggregation.",
            "And here you see.",
            "That the query flights actually get more complex."
        ],
        [
            "Our experimental setup is that we use every time the same hardware strong server, but that is not too important, but rather which is important is the setup of the software.",
            "So we use a limited amount of memory for each of the databases because we think it's unrealistic to load the entire data into main memory.",
            "Then the SB data we translate into for the appropriate approach.",
            "So we would for example.",
            "And create the star schema or we would transform the data into RDF.",
            "According to the approach or we would for example, create the materialized views and we have the SB queries that we translate according to our approaches.",
            "So either monitoring and SQL or all up for the inspection well and then the results are taken.",
            "So we measure the elapsed query times on hot ones with the business intelligence benchmark framework and all of this information you also find on the paper website so."
        ],
        [
            "Now coming to the results first as a reminder, we now want to compare roll up with olive oil D and the results are."
        ],
        [
            "That we are the all up for our OLAP faulty approach is 12 times slower than the roll up for executing all queries.",
            "So why is that?"
        ],
        [
            "Here you see on the upper diagram these results and why are we slower and we think our hypothesis is because of the numbers joints for QB hierarchies that are needed.",
            "I will show you in a minute and."
        ],
        [
            "Pacific use, specifically, you see that in the query Flight 4 where we have, especially in comparison to the other queries, you see that on the lower diagram you see the number of joints needed for the SQL and the SPARQL queries and we see that we have many more joints in the spiritual version.",
            "That is why we are much lower in this.",
            "Why we think we are much slower in this case."
        ],
        [
            "And why is that is exactly so in the roll up we have this fact table that would, for example, for aggregating to the years level, we need to do a join from the fact table to the dates dimension and dates table, and this requires exactly 1 join.",
            "If we now look."
        ],
        [
            "In the RDF version here we have also the resource for each of the line orders, and then we have this cost narrow path describing the hierarchies and you can see here with the red line that these are requires many more joints and we have made additional experiments to support our claim.",
            "So for example, we have represented the RDF closer resembling the roll up."
        ],
        [
            "The roll up structure of the star schema.",
            "This is this you see now here on the right side, where we represent the hierarchies differently by just pointing literal values from the actual date, and this works much faster, but of course it doesn't use the RDF data cube recovery, so this is really strong.",
            "Reassembling the star schema and it's much faster, but still this is not re using a standard vocabulary and vote for example not allow data integration too easily."
        ],
        [
            "OK, so this was about comparing rollup in all up Friday and Secondly about I want to 1st introduce area of aggregate views and then I want to present you the performance evaluation."
        ],
        [
            "So idea of aggregate views are basically sets of line orders on the same level of granularity, and you see that on the left lower corner where we have a view that would only contain line orders on the year level, and would for example aggregate overall customers.",
            "These views now can be connected to each other via rollup operations.",
            "So for example, you would hear from the view you would roll up, so you would aggregate from the part brand level to a higher level the product category level.",
            "And the good thing about defining views like that is that you can actually represent the entire data cube in a data cube lattice.",
            "And."
        ],
        [
            "This data cube letters knows has nice features, so for example of you is always computable from any view on a lower level on a roller path.",
            "So for example, you see that here.",
            "So we have a view here.",
            "In this view can be computed from.",
            "This view can be computed from this view, but also from this view.",
            "And another property is that the higher the view and the lettuce, the fewer line orders it contains.",
            "Also, the number of use typically is quite large, so in our line order data could be a 3000 line 3000 views and also so is so are the number of possible line orders in the entire data cube.",
            "So we have two point 6 * 10 to the power of 19th possible line orders in our data cube, and so we have the problem of deciding which view to select and how to compute it.",
            "This is a typical.",
            "Problem."
        ],
        [
            "So for each about selection, for each query we select the highest view from which a query can be computed.",
            "In this we simply do because we take the apophysis that the cost of answering a query is proportional to the number of line orders that need to be scanned.",
            "So for each single query in our workload, we create a single for each query in our workload we create a view that is highest in the lattice.",
            "And of course you can say OK, that's a heuristic, and you often cannot do that.",
            "But we have chosen this setting because we think it will speed up queries.",
            "Best as we can do.",
            "Secondly, we compute the views via Sparkle Sparkle insert queries.",
            "This is why we have identified that we can represent each view M as a cube slice using the RDF data cube recovery and that is because every view would fix certain dimensions to the oil level.",
            "So we I showed you these view that would aggregate to have only line orders on the year level and would aggregate overall customers and this view.",
            "Is represented as a slice that fixes the customer dimension to the OR level.",
            "This is this slice you see on the right lower corner.",
            "So."
        ],
        [
            "What is the result of introducing at F aggregate use in our setting?"
        ],
        [
            "So as a reminder, we now want to compare all up quality with all up for DM."
        ],
        [
            "So bad news.",
            "First overall we are still two times slower or no.",
            "We have two times slower with dematerialization approach then our approach without much realization.",
            "But good news.",
            "The most complex query query query, flight query four, we were able to optimize.",
            "In fact we were able to have a 13 times faster in a comparison rollup M to use aggregate tables in the.",
            "Relational database, it was able to speed up query execution by 50, so there is much optimization still to get.",
            "But I focused more on the reason of RDF materialization."
        ],
        [
            "So why exactly are we tried two times slower than all up already?",
            "And this is because we think we think it's becausw.",
            "All up for the M need still to scan all the line orders, although a view only contains a fraction of line orders."
        ],
        [
            "So here for for query 2.1 you'll see that our whole apology M is actually much slower, and this is although the actual view you see that on the lower diagram and the view actually only contains a very low number of line orders in this view.",
            "But our assumption is that still the triple Store needs to basically scan most of them and so why is that?"
        ],
        [
            "How is this done in Rollup M?",
            "Here we have created an aggregate table for each of the queries.",
            "So if we want to execute this query, the answer this query we only need to do that on this aggregate table and we don't need to do any joints so that is why the speedup is much faster.",
            "But in the RDF representation we still have here on the left side we have still did the original line orders in the graph.",
            "And those line orders are attached to a data set.",
            "This data set is attached to a slice and this slice now points to all the observations in the view.",
            "So you'll see we had to actually in order to use the RDF data cube recovery, we had to insert all the aggregated line orders into the same graph and This is why we think that the queries much queries are typically much slow."
        ],
        [
            "But we have one.",
            "So why are we in for query for Huawei?",
            "Are we much slower if much faster here?",
            "So where we able to optimize for this is because here we indeed what you."
        ],
        [
            "You have what I missed you to tell you is that on the right side for the aggregated line orders, we were able to reduce the Scots narrow paths, so we were able to reduce the joints that are needed for this RDF representation."
        ],
        [
            "And for query Flight 4 we the reduction in joints is much higher than for the other queries.",
            "So apparently the reduction in joints makes UPS makes up for the increased number of triples."
        ],
        [
            "For conclusion.",
            "It was written in the in the title, so no size fits all and what we mean with that is that if we have such a data analytics scenario, we always have to consider the tradeoff between efficiency and modeling.",
            "So with efficiency I mean that the triple stores are Presian, not worse for analytical queries, But if so, if we model data similar to a star schema in relational database.",
            "Then query processing is just as fast as a non relational database.",
            "But if we now model the data using a standard vocabulary such as the RDF data cube recovery, then statistics of course can be more easily be integrated with other data sources because we conform to a specific vocabulary, but in this case we require numerous joins for hierarchies.",
            "We can represent audio aggregate views, but those aggregated line orders are added to the same graph so that in total we were able to successfully optimize some queries, but most others were much slower.",
            "So summing up, we think that more work is needed to be done here.",
            "So regarding analytical queries on statistical data called for more optimized query engine's.",
            "Thank."
        ],
        [
            "Very much.",
            "So I have two questions, one.",
            "If you go back to the previous slide.",
            "You, I don't see any any evidence that you're providing that supports at first claim.",
            "It may be true, but, but."
        ],
        [
            "Yeah, maybe you missed that, but I included it.",
            "This is because of this slide, so I showed you the representation of this all up for DSB on the right side.",
            "This is a different modeling of the same information in RDF and this representation led up and in the paper you actually.",
            "This is an own approach so you really have the comparison of these approaches and this representing the RDF this way would result in almost as fast queries as the roll up engine.",
            "OK, so then I guess that kind of answers my next question, but I'm going to ask.",
            "It is if you have all this existing RDF data that that does all the statistics and stuff.",
            "What would you recommend?",
            "Should I just translate that and put into a star schema relational database?",
            "Or should I go your approach right now?",
            "What would you recommend?",
            "I would recommend use RDF now, but I would currently I would recommend still.",
            "It is a tricky question.",
            "It really depends on the use case.",
            "So you can either if you have a use case where the schema is very fixed, just write a script that would load the RDF into a star schema and then probably.",
            "I would recommend to use a relational database, but if you have a use case where you actually think about loading knew RDF triples.",
            "Consequently, into and you want to use them in queries.",
            "Then I think you would rather use triple stock, right?",
            "Thank you.",
            "Hi ever interesting talk thanks.",
            "So do you think that the output of this work is that we might think about changing the datacube vocabulary?",
            "I mean can this bring up because I find it quite complex.",
            "Actually at the vocabulary and now you are showing that is not even very efficient so.",
            "OK yeah yeah yeah yeah.",
            "I'm involved in the audio and in the government link data working group of the W as we see that is currently standardizing the RDF data cube vocabulary and of course we have a lot of discussions about it, especially with Richard Sagon Yogan Dave Reynolds that are the main drivers and editors.",
            "But The thing is, they are more concerned about the semantics of the data.",
            "So if you have a hierarchy and you have a date that is or let's say another example, you have a company and this company is now in a specific standard standard industrial classification, then you want to represent this relationship and you want to keep both as resources.",
            "You don't want to have one of those represent and represent it as literal values.",
            "So I think the audio data chip recovery is pretty fine.",
            "It's pretty nice it's combining different vocabularies for representing statistics.",
            "So I don't think this this.",
            "It's rather that maybe the audio data recovery is for certain use cases not not not that much appropriate for other use cases.",
            "Maybe this would be 1?",
            "One answer.",
            "OK, so I have a couple of comments on this, so we did the exact same experiments at open link with each other column store and 30 times the scale, and we compared our sequel to the Sparkle on the same engine.",
            "And the good news is that we just cut the IDF tags.",
            "You said a factor of 12.",
            "We cut it down to five and even better news is that when we run SQL on virtual.",
            "So I mean, most people know Virtualizer and RDF store, but we also do sequel and sequel with Columnstore we beat.",
            "Let's say Monet DB, which is a well respected analytics engine by a factor of two.",
            "So we completely obliterate my SQL and.",
            "Run up there with the absolute best in the analytics column stores in SQL and only five times slower with sparkle.",
            "So it means that right now you can use sparkle to do star schema stuff to do analytics with very reasonable cost and all the flexibility so the game has changed.",
            "But we will talk about that more later.",
            "Thanks for the comment and I'm happy that the RDF store scores go into this direction because it gives people less.",
            "Options to choose from, they can just use RDF right from the beginning.",
            "Anymore questions Oscar we've had.",
            "This is not really a question, but this, yes, I mean to say thank you for making all that data available in a website.",
            "As soon as you have a supplementary material, because that's really, really useful.",
            "In order to understand everything that you have been doing.",
            "Yeah, thanks everybody, everybody else would try to to also follow that.",
            "Really, that mother it took most of the work actually to create this website with where you will find all the queries and the data.",
            "And if you have questions regarding reproducing the results then I would be happy to help you.",
            "OK, so with that we're just in time for lunch and thanks for the talk again to the speaker and thanks for everybody attending."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm Benedict Chemkin from kaytee.",
                    "label": 0
                },
                {
                    "sent": "It cost for Germany and I would like to present your work on running the Star Schema benchmark with SPAR QL and RDF aggregate views and this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work is motivated by the fact that more and more statistical link data is out there.",
                    "label": 0
                },
                {
                    "sent": "So statistics published, for example from World Bank from OECD, World Health Organization, Euro Stat SEC, experience finance data or for example Yahoo Finance data with here more concretely stock market trading prices of certain companies are published as linked data and.",
                    "label": 1
                },
                {
                    "sent": "What we want to have is a query engine that allows to have analytical queries on such data and return the results to be displayed in resolution visualizations such as you see here line charge of trading prices for selected companies or trading prices in the lower chart pivot table.",
                    "label": 1
                },
                {
                    "sent": "The trading price is aggregated overtime and to make.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It more clear what is done here.",
                    "label": 0
                },
                {
                    "sent": "So what you have published on the statistical data is basically a data set that has comparable columns like.",
                    "label": 0
                },
                {
                    "sent": "I mean it's represented in RDF, but still it is representing a table of companies dates and then there the average trading price on a certain date and it's published using the RDF data Cube vocabulary and this query engine in this case to have this line chart.",
                    "label": 1
                },
                {
                    "sent": "Displayed would need to filter for selected for specific companies in this case.",
                    "label": 0
                },
                {
                    "sent": "Here for for Visa incorporation and master card.",
                    "label": 0
                },
                {
                    "sent": "So you see the trading price the this is the orange line.",
                    "label": 0
                },
                {
                    "sent": "This is visa incorporation where you see the average trading price over time.",
                    "label": 0
                },
                {
                    "sent": "And the error.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other example of the pivot table.",
                    "label": 0
                },
                {
                    "sent": "Here you are aggregating so you you take the the original data from some statistical data and then you aggregate over dates so the dates are not interesting for you anymore and in the pivot table you look at per row.",
                    "label": 0
                },
                {
                    "sent": "You'll see the companies and in the column in the column cells you have the average trading price and of course these two operations filtering and aggregating.",
                    "label": 0
                },
                {
                    "sent": "They seem very easy now on this little.",
                    "label": 0
                },
                {
                    "sent": "Example data set, but if you increase the number of the size of the table.",
                    "label": 0
                },
                {
                    "sent": "Dramatically, these these operations actually are a problem, so you need and this is the black box in the middle.",
                    "label": 0
                },
                {
                    "sent": "You need a query engine that allows you to implement such such such such operations efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is that typically, OK. How do you now implement such a query engine and one approach that is often used in industry is roll up.",
                    "label": 0
                },
                {
                    "sent": "So you would have typical all up clients, issue all up queries to a rollup engine, and that roll up engine would translate the OLAP queries into SQL on relational database that has been pre filled with tables from statistical data in our fixed our schema I will talk about the Star schema more later.",
                    "label": 0
                },
                {
                    "sent": "But what you can also do here is you can optimize the these queries further.",
                    "label": 0
                },
                {
                    "sent": "If you do pre aggregations.",
                    "label": 0
                },
                {
                    "sent": "If you pre aggregate certain values from the original table and store them in aggregate tables.",
                    "label": 0
                },
                {
                    "sent": "And the approach now that we have proposed is to, instead of using a rollup engine to use an all up for LD engine, that would also use get OLAP queries, but would translate them into SPARQL queries on a triple store that has been filled with RDF in general.",
                    "label": 0
                },
                {
                    "sent": "So in this case we cannot only we only import the data into the Triple Store and also the metadata of the RDF data cube vocabulary, but also we can.",
                    "label": 1
                },
                {
                    "sent": "For example, load RDF data from DB PEDIA or Freebase if, for example, the companies are linked to Freebase, Freebase, or DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "So the motivation is that in this case you can do more better, more expressive queries so you can for example dynamically or ad hoc ask for you can filter for companies from cities with more than 100,000 inhabitants.",
                    "label": 1
                },
                {
                    "sent": "This is possible if this fact.",
                    "label": 0
                },
                {
                    "sent": "Sam is represented in the RDF or also you can filter for companies that have a CEO who is younger than 20, so this is the motivation of really using RDF in this case, but I don't want to go into details of this motivation, but rather I want to go a step before that.",
                    "label": 1
                },
                {
                    "sent": "'cause we really have the question of what is now the performance difference between using a roll up engine Anna Star schema instead of using this all up for the approach of using a triple store.",
                    "label": 0
                },
                {
                    "sent": "And also what we are interested in is whether we can actually use materialized aggregate views.",
                    "label": 0
                },
                {
                    "sent": "So whether we can apply something like aggregate tables in RDF.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we want to present to you today in this talk is, first of all, a comparison between the rollup approach, which means this is the so that's the first red error and on the left side where we here compare rollup which uses a relational database.",
                    "label": 0
                },
                {
                    "sent": "My SQL in this case and which users mandrian as a way to translate the OLAP queries into SQL.",
                    "label": 0
                },
                {
                    "sent": "And this is a non much realization approach and this we want to compare with our OLAP foil D approach where we use open virtuoso as the representative triple store and Spark QL for querying the data and for translating the OLAP queries into SPARQL we use our own system all up foil D and then there's in the second part of the talk.",
                    "label": 0
                },
                {
                    "sent": "I will go into details of the comparison of our OLAP ALDI approach in comparison with materialization.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So regarding related work, what we do is we compare roll up and all up on a common triplestore, whereas of course there's lots of work on optimizing queries on RDF, like using MapReduce or column stores.",
                    "label": 1
                },
                {
                    "sent": "And also we use the RDF data cube vocabulary to represent aggregate views in RDF, which we think no one else has done before.",
                    "label": 1
                },
                {
                    "sent": "But of course there's lots of work on representing statistics generally in as linked data and also we materialize aggregate views with.",
                    "label": 0
                },
                {
                    "sent": "Insert queries and run a realistic benchmark with more than more than 100 million triples.",
                    "label": 1
                },
                {
                    "sent": "But of course there is a lot of work on managing views on RDF datasets.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the reminder of this talk, I will first present you the Star Schema benchmark that we have used to get our results.",
                    "label": 1
                },
                {
                    "sent": "Then I will go into the details of the 1st result comparing roll up with our olive oil.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Evaluate the effect of RDF aggregate views on our approach and then I will conclude.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this task, him a benchmark, is based on TPC H, which is why we have chosen is becausw.",
                    "label": 0
                },
                {
                    "sent": "TCH is very often used in relational databases afterward to benchmark and analytical systems, and it gives us metadata about end data about data cube with line orders of a specific company.",
                    "label": 1
                },
                {
                    "sent": "So each line order would contain measures, for example the sum of revenues.",
                    "label": 1
                },
                {
                    "sent": "Also it would have dimensions with hierarchies.",
                    "label": 1
                },
                {
                    "sent": "And for example, each measure would each line order would have dimensions like dates when the order was taken, and this has a hierarchy of every date has a month a year, and it can also be aggregated over.",
                    "label": 1
                },
                {
                    "sent": "Also we have supplier dimension for example, which also have a higher CRI of city which is in a nation within a region and can also be aggregated over the oil level.",
                    "label": 0
                },
                {
                    "sent": "And also we have chosen these task more benchmark because it specifically uses a logical schema in a star schema which is very often used in industry because it's very easy to use and it's very performant in relational databases typically.",
                    "label": 0
                },
                {
                    "sent": "And here we have a huge fact table.",
                    "label": 0
                },
                {
                    "sent": "You can you see their schema on the left side on the actual data on the right side we have a huge fact table, in this case with six million line orders and these fact table is linked to.",
                    "label": 0
                },
                {
                    "sent": "Dimension tables these dimension tables are denormalized and they represent the entire hierarchy of the specific dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also this task in my benchmark gives us the queries.",
                    "label": 0
                },
                {
                    "sent": "So for example we have a query that asks for the revenues per year and per product brands of a specific category and a specific supplier and this can be translated into the operation that I've shown you before so we can aggregate.",
                    "label": 1
                },
                {
                    "sent": "So we need to aggregate here from dates to the years when we need to aggregate from pasta products to the brand level and we want we are not interested in the customer at all, we just.",
                    "label": 0
                },
                {
                    "sent": "Aggregate over it to the oil level and we filter for a specific category in a specific region here.",
                    "label": 0
                },
                {
                    "sent": "And this query, if this is run on our OLAP engine then we the results we can display for example in a pivot table as you see on the left lower corner.",
                    "label": 0
                },
                {
                    "sent": "Here we have in each row the years the different years and the columns the different brands and the single cells now are created from the original fact table by filtering and aggregation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In total these SSBB benchmark gives us four equivalent flights.",
                    "label": 0
                },
                {
                    "sent": "Query flights means specific types of queries.",
                    "label": 0
                },
                {
                    "sent": "The all of these queries of 1 query flight out of the same type and only the selectivity and the granularity changes and we have 13 queries in total and one way to measure the complexity of this queries would be for example to the number of dimensions that are used for filtering or aggregation.",
                    "label": 1
                },
                {
                    "sent": "And here you see.",
                    "label": 0
                },
                {
                    "sent": "That the query flights actually get more complex.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our experimental setup is that we use every time the same hardware strong server, but that is not too important, but rather which is important is the setup of the software.",
                    "label": 0
                },
                {
                    "sent": "So we use a limited amount of memory for each of the databases because we think it's unrealistic to load the entire data into main memory.",
                    "label": 0
                },
                {
                    "sent": "Then the SB data we translate into for the appropriate approach.",
                    "label": 0
                },
                {
                    "sent": "So we would for example.",
                    "label": 0
                },
                {
                    "sent": "And create the star schema or we would transform the data into RDF.",
                    "label": 0
                },
                {
                    "sent": "According to the approach or we would for example, create the materialized views and we have the SB queries that we translate according to our approaches.",
                    "label": 1
                },
                {
                    "sent": "So either monitoring and SQL or all up for the inspection well and then the results are taken.",
                    "label": 0
                },
                {
                    "sent": "So we measure the elapsed query times on hot ones with the business intelligence benchmark framework and all of this information you also find on the paper website so.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now coming to the results first as a reminder, we now want to compare roll up with olive oil D and the results are.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we are the all up for our OLAP faulty approach is 12 times slower than the roll up for executing all queries.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you see on the upper diagram these results and why are we slower and we think our hypothesis is because of the numbers joints for QB hierarchies that are needed.",
                    "label": 0
                },
                {
                    "sent": "I will show you in a minute and.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pacific use, specifically, you see that in the query Flight 4 where we have, especially in comparison to the other queries, you see that on the lower diagram you see the number of joints needed for the SQL and the SPARQL queries and we see that we have many more joints in the spiritual version.",
                    "label": 0
                },
                {
                    "sent": "That is why we are much lower in this.",
                    "label": 0
                },
                {
                    "sent": "Why we think we are much slower in this case.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And why is that is exactly so in the roll up we have this fact table that would, for example, for aggregating to the years level, we need to do a join from the fact table to the dates dimension and dates table, and this requires exactly 1 join.",
                    "label": 0
                },
                {
                    "sent": "If we now look.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the RDF version here we have also the resource for each of the line orders, and then we have this cost narrow path describing the hierarchies and you can see here with the red line that these are requires many more joints and we have made additional experiments to support our claim.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have represented the RDF closer resembling the roll up.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The roll up structure of the star schema.",
                    "label": 0
                },
                {
                    "sent": "This is this you see now here on the right side, where we represent the hierarchies differently by just pointing literal values from the actual date, and this works much faster, but of course it doesn't use the RDF data cube recovery, so this is really strong.",
                    "label": 0
                },
                {
                    "sent": "Reassembling the star schema and it's much faster, but still this is not re using a standard vocabulary and vote for example not allow data integration too easily.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this was about comparing rollup in all up Friday and Secondly about I want to 1st introduce area of aggregate views and then I want to present you the performance evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So idea of aggregate views are basically sets of line orders on the same level of granularity, and you see that on the left lower corner where we have a view that would only contain line orders on the year level, and would for example aggregate overall customers.",
                    "label": 1
                },
                {
                    "sent": "These views now can be connected to each other via rollup operations.",
                    "label": 0
                },
                {
                    "sent": "So for example, you would hear from the view you would roll up, so you would aggregate from the part brand level to a higher level the product category level.",
                    "label": 1
                },
                {
                    "sent": "And the good thing about defining views like that is that you can actually represent the entire data cube in a data cube lattice.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This data cube letters knows has nice features, so for example of you is always computable from any view on a lower level on a roller path.",
                    "label": 1
                },
                {
                    "sent": "So for example, you see that here.",
                    "label": 0
                },
                {
                    "sent": "So we have a view here.",
                    "label": 0
                },
                {
                    "sent": "In this view can be computed from.",
                    "label": 0
                },
                {
                    "sent": "This view can be computed from this view, but also from this view.",
                    "label": 1
                },
                {
                    "sent": "And another property is that the higher the view and the lettuce, the fewer line orders it contains.",
                    "label": 0
                },
                {
                    "sent": "Also, the number of use typically is quite large, so in our line order data could be a 3000 line 3000 views and also so is so are the number of possible line orders in the entire data cube.",
                    "label": 0
                },
                {
                    "sent": "So we have two point 6 * 10 to the power of 19th possible line orders in our data cube, and so we have the problem of deciding which view to select and how to compute it.",
                    "label": 0
                },
                {
                    "sent": "This is a typical.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for each about selection, for each query we select the highest view from which a query can be computed.",
                    "label": 1
                },
                {
                    "sent": "In this we simply do because we take the apophysis that the cost of answering a query is proportional to the number of line orders that need to be scanned.",
                    "label": 0
                },
                {
                    "sent": "So for each single query in our workload, we create a single for each query in our workload we create a view that is highest in the lattice.",
                    "label": 0
                },
                {
                    "sent": "And of course you can say OK, that's a heuristic, and you often cannot do that.",
                    "label": 0
                },
                {
                    "sent": "But we have chosen this setting because we think it will speed up queries.",
                    "label": 1
                },
                {
                    "sent": "Best as we can do.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we compute the views via Sparkle Sparkle insert queries.",
                    "label": 0
                },
                {
                    "sent": "This is why we have identified that we can represent each view M as a cube slice using the RDF data cube recovery and that is because every view would fix certain dimensions to the oil level.",
                    "label": 0
                },
                {
                    "sent": "So we I showed you these view that would aggregate to have only line orders on the year level and would aggregate overall customers and this view.",
                    "label": 0
                },
                {
                    "sent": "Is represented as a slice that fixes the customer dimension to the OR level.",
                    "label": 0
                },
                {
                    "sent": "This is this slice you see on the right lower corner.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the result of introducing at F aggregate use in our setting?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as a reminder, we now want to compare all up quality with all up for DM.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So bad news.",
                    "label": 0
                },
                {
                    "sent": "First overall we are still two times slower or no.",
                    "label": 1
                },
                {
                    "sent": "We have two times slower with dematerialization approach then our approach without much realization.",
                    "label": 0
                },
                {
                    "sent": "But good news.",
                    "label": 1
                },
                {
                    "sent": "The most complex query query query, flight query four, we were able to optimize.",
                    "label": 0
                },
                {
                    "sent": "In fact we were able to have a 13 times faster in a comparison rollup M to use aggregate tables in the.",
                    "label": 1
                },
                {
                    "sent": "Relational database, it was able to speed up query execution by 50, so there is much optimization still to get.",
                    "label": 0
                },
                {
                    "sent": "But I focused more on the reason of RDF materialization.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why exactly are we tried two times slower than all up already?",
                    "label": 0
                },
                {
                    "sent": "And this is because we think we think it's becausw.",
                    "label": 0
                },
                {
                    "sent": "All up for the M need still to scan all the line orders, although a view only contains a fraction of line orders.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here for for query 2.1 you'll see that our whole apology M is actually much slower, and this is although the actual view you see that on the lower diagram and the view actually only contains a very low number of line orders in this view.",
                    "label": 0
                },
                {
                    "sent": "But our assumption is that still the triple Store needs to basically scan most of them and so why is that?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How is this done in Rollup M?",
                    "label": 0
                },
                {
                    "sent": "Here we have created an aggregate table for each of the queries.",
                    "label": 0
                },
                {
                    "sent": "So if we want to execute this query, the answer this query we only need to do that on this aggregate table and we don't need to do any joints so that is why the speedup is much faster.",
                    "label": 0
                },
                {
                    "sent": "But in the RDF representation we still have here on the left side we have still did the original line orders in the graph.",
                    "label": 0
                },
                {
                    "sent": "And those line orders are attached to a data set.",
                    "label": 0
                },
                {
                    "sent": "This data set is attached to a slice and this slice now points to all the observations in the view.",
                    "label": 0
                },
                {
                    "sent": "So you'll see we had to actually in order to use the RDF data cube recovery, we had to insert all the aggregated line orders into the same graph and This is why we think that the queries much queries are typically much slow.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we have one.",
                    "label": 0
                },
                {
                    "sent": "So why are we in for query for Huawei?",
                    "label": 0
                },
                {
                    "sent": "Are we much slower if much faster here?",
                    "label": 0
                },
                {
                    "sent": "So where we able to optimize for this is because here we indeed what you.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have what I missed you to tell you is that on the right side for the aggregated line orders, we were able to reduce the Scots narrow paths, so we were able to reduce the joints that are needed for this RDF representation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for query Flight 4 we the reduction in joints is much higher than for the other queries.",
                    "label": 0
                },
                {
                    "sent": "So apparently the reduction in joints makes UPS makes up for the increased number of triples.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For conclusion.",
                    "label": 0
                },
                {
                    "sent": "It was written in the in the title, so no size fits all and what we mean with that is that if we have such a data analytics scenario, we always have to consider the tradeoff between efficiency and modeling.",
                    "label": 0
                },
                {
                    "sent": "So with efficiency I mean that the triple stores are Presian, not worse for analytical queries, But if so, if we model data similar to a star schema in relational database.",
                    "label": 1
                },
                {
                    "sent": "Then query processing is just as fast as a non relational database.",
                    "label": 0
                },
                {
                    "sent": "But if we now model the data using a standard vocabulary such as the RDF data cube recovery, then statistics of course can be more easily be integrated with other data sources because we conform to a specific vocabulary, but in this case we require numerous joins for hierarchies.",
                    "label": 1
                },
                {
                    "sent": "We can represent audio aggregate views, but those aggregated line orders are added to the same graph so that in total we were able to successfully optimize some queries, but most others were much slower.",
                    "label": 1
                },
                {
                    "sent": "So summing up, we think that more work is needed to be done here.",
                    "label": 0
                },
                {
                    "sent": "So regarding analytical queries on statistical data called for more optimized query engine's.",
                    "label": 0
                },
                {
                    "sent": "Thank.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very much.",
                    "label": 0
                },
                {
                    "sent": "So I have two questions, one.",
                    "label": 0
                },
                {
                    "sent": "If you go back to the previous slide.",
                    "label": 0
                },
                {
                    "sent": "You, I don't see any any evidence that you're providing that supports at first claim.",
                    "label": 0
                },
                {
                    "sent": "It may be true, but, but.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, maybe you missed that, but I included it.",
                    "label": 0
                },
                {
                    "sent": "This is because of this slide, so I showed you the representation of this all up for DSB on the right side.",
                    "label": 0
                },
                {
                    "sent": "This is a different modeling of the same information in RDF and this representation led up and in the paper you actually.",
                    "label": 0
                },
                {
                    "sent": "This is an own approach so you really have the comparison of these approaches and this representing the RDF this way would result in almost as fast queries as the roll up engine.",
                    "label": 0
                },
                {
                    "sent": "OK, so then I guess that kind of answers my next question, but I'm going to ask.",
                    "label": 0
                },
                {
                    "sent": "It is if you have all this existing RDF data that that does all the statistics and stuff.",
                    "label": 0
                },
                {
                    "sent": "What would you recommend?",
                    "label": 0
                },
                {
                    "sent": "Should I just translate that and put into a star schema relational database?",
                    "label": 1
                },
                {
                    "sent": "Or should I go your approach right now?",
                    "label": 0
                },
                {
                    "sent": "What would you recommend?",
                    "label": 0
                },
                {
                    "sent": "I would recommend use RDF now, but I would currently I would recommend still.",
                    "label": 0
                },
                {
                    "sent": "It is a tricky question.",
                    "label": 0
                },
                {
                    "sent": "It really depends on the use case.",
                    "label": 0
                },
                {
                    "sent": "So you can either if you have a use case where the schema is very fixed, just write a script that would load the RDF into a star schema and then probably.",
                    "label": 1
                },
                {
                    "sent": "I would recommend to use a relational database, but if you have a use case where you actually think about loading knew RDF triples.",
                    "label": 0
                },
                {
                    "sent": "Consequently, into and you want to use them in queries.",
                    "label": 0
                },
                {
                    "sent": "Then I think you would rather use triple stock, right?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi ever interesting talk thanks.",
                    "label": 0
                },
                {
                    "sent": "So do you think that the output of this work is that we might think about changing the datacube vocabulary?",
                    "label": 0
                },
                {
                    "sent": "I mean can this bring up because I find it quite complex.",
                    "label": 0
                },
                {
                    "sent": "Actually at the vocabulary and now you are showing that is not even very efficient so.",
                    "label": 0
                },
                {
                    "sent": "OK yeah yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I'm involved in the audio and in the government link data working group of the W as we see that is currently standardizing the RDF data cube vocabulary and of course we have a lot of discussions about it, especially with Richard Sagon Yogan Dave Reynolds that are the main drivers and editors.",
                    "label": 0
                },
                {
                    "sent": "But The thing is, they are more concerned about the semantics of the data.",
                    "label": 0
                },
                {
                    "sent": "So if you have a hierarchy and you have a date that is or let's say another example, you have a company and this company is now in a specific standard standard industrial classification, then you want to represent this relationship and you want to keep both as resources.",
                    "label": 0
                },
                {
                    "sent": "You don't want to have one of those represent and represent it as literal values.",
                    "label": 0
                },
                {
                    "sent": "So I think the audio data chip recovery is pretty fine.",
                    "label": 0
                },
                {
                    "sent": "It's pretty nice it's combining different vocabularies for representing statistics.",
                    "label": 0
                },
                {
                    "sent": "So I don't think this this.",
                    "label": 0
                },
                {
                    "sent": "It's rather that maybe the audio data recovery is for certain use cases not not not that much appropriate for other use cases.",
                    "label": 0
                },
                {
                    "sent": "Maybe this would be 1?",
                    "label": 0
                },
                {
                    "sent": "One answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have a couple of comments on this, so we did the exact same experiments at open link with each other column store and 30 times the scale, and we compared our sequel to the Sparkle on the same engine.",
                    "label": 0
                },
                {
                    "sent": "And the good news is that we just cut the IDF tags.",
                    "label": 0
                },
                {
                    "sent": "You said a factor of 12.",
                    "label": 1
                },
                {
                    "sent": "We cut it down to five and even better news is that when we run SQL on virtual.",
                    "label": 0
                },
                {
                    "sent": "So I mean, most people know Virtualizer and RDF store, but we also do sequel and sequel with Columnstore we beat.",
                    "label": 0
                },
                {
                    "sent": "Let's say Monet DB, which is a well respected analytics engine by a factor of two.",
                    "label": 0
                },
                {
                    "sent": "So we completely obliterate my SQL and.",
                    "label": 0
                },
                {
                    "sent": "Run up there with the absolute best in the analytics column stores in SQL and only five times slower with sparkle.",
                    "label": 0
                },
                {
                    "sent": "So it means that right now you can use sparkle to do star schema stuff to do analytics with very reasonable cost and all the flexibility so the game has changed.",
                    "label": 0
                },
                {
                    "sent": "But we will talk about that more later.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the comment and I'm happy that the RDF store scores go into this direction because it gives people less.",
                    "label": 0
                },
                {
                    "sent": "Options to choose from, they can just use RDF right from the beginning.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions Oscar we've had.",
                    "label": 0
                },
                {
                    "sent": "This is not really a question, but this, yes, I mean to say thank you for making all that data available in a website.",
                    "label": 0
                },
                {
                    "sent": "As soon as you have a supplementary material, because that's really, really useful.",
                    "label": 0
                },
                {
                    "sent": "In order to understand everything that you have been doing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks everybody, everybody else would try to to also follow that.",
                    "label": 0
                },
                {
                    "sent": "Really, that mother it took most of the work actually to create this website with where you will find all the queries and the data.",
                    "label": 0
                },
                {
                    "sent": "And if you have questions regarding reproducing the results then I would be happy to help you.",
                    "label": 0
                },
                {
                    "sent": "OK, so with that we're just in time for lunch and thanks for the talk again to the speaker and thanks for everybody attending.",
                    "label": 0
                }
            ]
        }
    }
}