{
    "id": "xjs5kto375c7qcgsyjl3dwmyd6wqhz32",
    "title": "A Scalable Two-Stage Approach for a Class of Dimensionality Reduction Techniques",
    "info": {
        "author": [
            "Liang Sun, Department of Computer Science and Engineering, Arizona State University"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd2010_sun_stsa/",
    "segmentation": [
        [
            "Good morning, I'm grounded from Arizona State, so my talking tone scalable two stage approach for a class of them, salary reduction techniques and this is joint work with virtual server and Professor Japanese."
        ],
        [
            "So high time so data is ubiquitous in real world applications.",
            "Some typical example includes gene expression data.",
            "The handwritten digital image, human face and human brain images.",
            "So a natural question is how to process the high dimensional data."
        ],
        [
            "If it to me.",
            "A popular and effective tool is linear dimensionality reduction in dimensionality reduction.",
            "IT projects the data in high dimensional space onto low damage space by using linear transformation.",
            "So here giving the original data X, which is divine matrix D is dimensionality and number of data points.",
            "So the dimensionality is number of rows.",
            "By using the transformation matrix W, we can transform the.",
            "Original data onto the reduced data express so we can observe that the number of rows is reduced significantly, so we can reduce the dimensionality.",
            "In addition, the transformation matrix W can be computed by optimizing a certain criteria."
        ],
        [
            "What do we need?",
            "Dimensionality reduction?",
            "The reading that are most data.",
            "Many algorithms may not work very well for high dimensional data.",
            "One of the key issues cause the curse of them sanity, which leads to decrease the accuracy and efficiency as damage increases.",
            "In fact, in many real world applications, the intrinsic dimension may be very small, for, for example, the number of genes responsible for a certain type of disease may be small.",
            "In addition, in the visualization of data, it is convenient to project data onto 2 three dimensional space.",
            "In this case, dimensionality reduction is crucial."
        ],
        [
            "The money, time standard reduction algorithms.",
            "We can roughly be divided into three categories, unsupervised, supervised and semi supervised.",
            "In this paper we focus on."
        ],
        [
            "Advise the algorithms.",
            "It turns out that so many dimensionality reduction algorithms reduced to solving a generalized eigenvalue problem or GP.",
            "In this paper, we focus on a special type of GP and.",
            "X is the dependent data matrix, as is undying symmetric matrix and it is it is positive semidefinite in supervised learning as is generally constructed from the label information.",
            "So what we need to solve is the vector D is the vector W becausw.",
            "Regularization is commonly used to improve the generalization performance, so we also have the GP with regularization and we have the.",
            "This formulation, is a positive parameter.",
            "In fact, many popular dimensionality reduction algorithms belong to this type of GP.",
            "Some typical example include Canonical correlation analysis or normalized passion squares have graph spectrum learning and linear discriminant analysis, and we will discuss."
        ],
        [
            "Discuss them later.",
            "While the key issue of the GP is that how to solve it efficiently, unfortunately existing algorithms does not scale well too large that problems that many algorithms in numerical linear algebra, but generally computationally expensive.",
            "On the other hand, are equivalent.",
            "Discussed for Mazing was proposed for this class of DP.",
            "But the disadvantage is that the equivalence relationship only holds under strong assumption.",
            "In addition, we cannot extend the equivalence relationship to the regularization case."
        ],
        [
            "So in this paper we propose a two stage approach for the class of GP mentioned earlier, which includes CCA or small, especially squarehead graph spectrum learning and LDA.",
            "One distinct.",
            "Advantage of the two stage approach is that look assumption is required so it can be applied for any data set.",
            "In addition, we can extend the two stage approach to the resisting case.",
            "I will send radical alloces and empirical studies show that the two stage approach can scale to very."
        ],
        [
            "Had problems.",
            "And this is the outline of the rest of my talk.",
            "First, we will give an overview of some example dimensionality reduction algorithms and then we introduce our two stage approach, including its main procedure, its equivalence relationship with the direct approach and.",
            "Is time complexity analysis to evaluate the proposed algorithm.",
            "We perform extensive empirical studies and unless."
        ],
        [
            "This is conclusive.",
            "So first we introduce Canonical correlation analysis in Canonical correlation analysis, we try to compute two linear projections for two views XX&Y by maximizing the correlation after linear projection.",
            "Under, it can issue that the projection for XWX can be obtained by solving this GP so we can comparing.",
            "We can compare this DP with the GP in standard form so we can get the definition of matrix as an addition.",
            "We can decompose as as the product of HH transpose the definition of matrix H is also giving here."
        ],
        [
            "As normal as the passing score.",
            "Also confused linear projection for two views by maximizing the covariance afternoon protection, it can be sure that the projection for XWX can also be obtained by solving this DP.",
            "So same Danny, we can get the definition of matrix S&H."
        ],
        [
            "Have graph spectral learning?",
            "Is the dimensionality reduction technique for multi label classification so its uses have graph to capture the correlation among different labels.",
            "So for example in this example we have three labels and six instances.",
            "It lends a low dimensional embedding through a linear transformation W. By solving this optimization problem.",
            "Where L is is the so-called have graph map Lassie.",
            "It can issue that.",
            "The transformation value can be obtained the best with this GPU.",
            "So similarly we can get in the definition of S&H."
        ],
        [
            "LDA is a classical technique.",
            "In classification, it tries to minimize the within class scope of various while maximizing the between class variance after linear projection.",
            "I can be sure that the optical optical optimal linear projection correspond to the top eigenvectors of this matrix.",
            "So similarly we can show that it can be formulated as generalized activity problem and the definition of S&HI was giving."
        ],
        [
            "OK, let's the way give a overview of our two stage approach first.",
            "In the first stage, we project the original data ex.",
            "Onto the intermediate data X~ by using the transformation matrix W one and in the first stage W one is computed by solving our least squares problem.",
            "In the second stage, we further Project X~ onto X prime.",
            "The final reduce data by using W2 and in this stage the projection matrix W2.",
            "Is computed by solving a generalized eigenvalue problem of reduced size so.",
            "In the first stage we show the list with problem.",
            "In the second stage will solve a generalized eigenvalue problem of reduced size so.",
            "The whole problem can be solved very efficiently."
        ],
        [
            "This is a detailed procedure of the two stage approach.",
            "Without regularization, the input is the data matrix X and the matrix age.",
            "So in service learning H is generally constructed from the label information.",
            "The output is the projection matrix W. In the first days we should have at least at least squares problem.",
            "In this stage, each transport is the target matrix.",
            "Becauses supers learning H is generally constructed from the label formation.",
            "So each can be considered as the latent target.",
            "In this stage we apply LSQR and concrete gradient algorithm to solve the least squares problem.",
            "OS2 is well known for its scalability, and it is even reliable for your condition problem in the second stage.",
            "Firstly we compute the intermediate data X~ and then.",
            "Where actually what we do is that we replace X in the original generalized eigenvalue problem with the intermediate data access to that and then show off the resulting generalized eigenvalue problem.",
            "So this general generalized eigenvalue problem is also equivalent to this optimization problem, because the size of X~ is reduced significantly in the first stage.",
            "So the computational cost of this stage is very low.",
            "Finally we combine W1W2 and get the final projection matrix double."
        ],
        [
            "OK, Alexa, we gave a time complexity analysis.",
            "The total total computational cost of the first stage is giving here.",
            "So capital N is total number of iterations of the iterative algorithm.",
            "SQL and.",
            "Be cause age is MBK matrix.",
            "So so I need to show off.",
            "So we need to solve Kaylee Scott problem so we have coefficient K here and three N + 50 + 2 Z is cost of.",
            "So solving each iteration in SQR.",
            "And the cost of second stage is crazy.",
            "Plus N squared Katie is the cost of computer X~ and a skirt is cost of solving this optimization problem?",
            "Under the cost of combining WNW, choose discard so the final computation cost is given here."
        ],
        [
            "We rigorously prove the equivalence relationship between the two stage approach and the direct approach.",
            "The proving is quite technical, so I'm not going to the details.",
            "The full proof can be found in the paper.",
            "We just give some key result here.",
            "The theorem one gives the solution of the two States and serum two gives the solution of the director approach without regularization.",
            "So we can observe that they are identical, so the equipments relationship can be approved."
        ],
        [
            "We can further extend the two stage approach into the regularization setting.",
            "In the first stage, becausw the.",
            "Organizations consider that, so we should have a penalized least squares problem using the same target.",
            "We can also rigorously prove the equivalence relationship between the two state project Direct approach in the localization case.",
            "So compared with the existing work, our two stage approach is a significant improvement, cause lower something is required and it can be first extended to the regularization searching.",
            "We can also analyze the time complexity analysis of the two state approach.",
            "In the regulation case, and it turns out it is same as the previous one."
        ],
        [
            "To verify the equipment or internship and demonstrated.",
            "The scalability will perform extensive."
        ],
        [
            "Permits.",
            "Both synthetic and real world datasets are tested in our experiment.",
            "The first 4 devices are synthetic and the remaining 90,000.",
            "Our benchmark data sets in math class learning."
        ],
        [
            "The math label learning.",
            "In this experiment, we compare the class classification performance on each data set.",
            "Increase the regularization parameter the.",
            "Black barcodes, punctual direct approach and right back refund through the what two states approach and we can observe that they achieve the same performance."
        ],
        [
            "In all cases.",
            "And in this experiment we compare.",
            "Compare the two 2 state approach with the direct approach.",
            "In terms of scalability On this date on a CV1V2 datasets we fix the dimensionality as 2000 and increase the sample size from 500 of from 500 to 3000.",
            "The blue curve correspond to Tuesdays and the red curve goes to correspond to the direct approach, so we can observe that the two stage approach is significant.",
            "Significantly more scale."
        ],
        [
            "And in this experiment we fix them to manatee or fix sample size as resultant and increase the dimensionality.",
            "So we can also observe that.",
            "The two states approach is much more."
        ],
        [
            "Visit.",
            "So in summary, in this paper we have established a two stage approach for a class of them, slanted reduction techniques which includes LDA or similar fashion, square, LACA and hypergraphs spectrum learning.",
            "We rigorously prove the equivalence relationship between the two stage approach and the direct approach.",
            "We emphasize the advantages of our two state parts.",
            "So firstly lower something is required so it can be applied for any data set.",
            "In addition it can be further applied in the organization searching.",
            "It's good scalability makes it an idea to to process the height.",
            "Large scale, high dimensional data in terms of future work we plan to extend the two stage approach to some to some other algorithms similar to the GP formulation Mason earlier.",
            "We also plan to develop an online algorithm for the two stage approach."
        ],
        [
            "Thank you.",
            "OK, so in the first days K because we focus on supervised learning in this paper.",
            "So in this paper K is number of labels in multi level learning.",
            "So in the first stage we K it's fixed.",
            "Is number of classics."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning, I'm grounded from Arizona State, so my talking tone scalable two stage approach for a class of them, salary reduction techniques and this is joint work with virtual server and Professor Japanese.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So high time so data is ubiquitous in real world applications.",
                    "label": 1
                },
                {
                    "sent": "Some typical example includes gene expression data.",
                    "label": 0
                },
                {
                    "sent": "The handwritten digital image, human face and human brain images.",
                    "label": 0
                },
                {
                    "sent": "So a natural question is how to process the high dimensional data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If it to me.",
                    "label": 0
                },
                {
                    "sent": "A popular and effective tool is linear dimensionality reduction in dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "IT projects the data in high dimensional space onto low damage space by using linear transformation.",
                    "label": 1
                },
                {
                    "sent": "So here giving the original data X, which is divine matrix D is dimensionality and number of data points.",
                    "label": 1
                },
                {
                    "sent": "So the dimensionality is number of rows.",
                    "label": 0
                },
                {
                    "sent": "By using the transformation matrix W, we can transform the.",
                    "label": 0
                },
                {
                    "sent": "Original data onto the reduced data express so we can observe that the number of rows is reduced significantly, so we can reduce the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "In addition, the transformation matrix W can be computed by optimizing a certain criteria.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do we need?",
                    "label": 0
                },
                {
                    "sent": "Dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "The reading that are most data.",
                    "label": 0
                },
                {
                    "sent": "Many algorithms may not work very well for high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "One of the key issues cause the curse of them sanity, which leads to decrease the accuracy and efficiency as damage increases.",
                    "label": 0
                },
                {
                    "sent": "In fact, in many real world applications, the intrinsic dimension may be very small, for, for example, the number of genes responsible for a certain type of disease may be small.",
                    "label": 1
                },
                {
                    "sent": "In addition, in the visualization of data, it is convenient to project data onto 2 three dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In this case, dimensionality reduction is crucial.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The money, time standard reduction algorithms.",
                    "label": 0
                },
                {
                    "sent": "We can roughly be divided into three categories, unsupervised, supervised and semi supervised.",
                    "label": 0
                },
                {
                    "sent": "In this paper we focus on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Advise the algorithms.",
                    "label": 0
                },
                {
                    "sent": "It turns out that so many dimensionality reduction algorithms reduced to solving a generalized eigenvalue problem or GP.",
                    "label": 1
                },
                {
                    "sent": "In this paper, we focus on a special type of GP and.",
                    "label": 1
                },
                {
                    "sent": "X is the dependent data matrix, as is undying symmetric matrix and it is it is positive semidefinite in supervised learning as is generally constructed from the label information.",
                    "label": 0
                },
                {
                    "sent": "So what we need to solve is the vector D is the vector W becausw.",
                    "label": 0
                },
                {
                    "sent": "Regularization is commonly used to improve the generalization performance, so we also have the GP with regularization and we have the.",
                    "label": 0
                },
                {
                    "sent": "This formulation, is a positive parameter.",
                    "label": 1
                },
                {
                    "sent": "In fact, many popular dimensionality reduction algorithms belong to this type of GP.",
                    "label": 0
                },
                {
                    "sent": "Some typical example include Canonical correlation analysis or normalized passion squares have graph spectrum learning and linear discriminant analysis, and we will discuss.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discuss them later.",
                    "label": 0
                },
                {
                    "sent": "While the key issue of the GP is that how to solve it efficiently, unfortunately existing algorithms does not scale well too large that problems that many algorithms in numerical linear algebra, but generally computationally expensive.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, are equivalent.",
                    "label": 1
                },
                {
                    "sent": "Discussed for Mazing was proposed for this class of DP.",
                    "label": 0
                },
                {
                    "sent": "But the disadvantage is that the equivalence relationship only holds under strong assumption.",
                    "label": 0
                },
                {
                    "sent": "In addition, we cannot extend the equivalence relationship to the regularization case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this paper we propose a two stage approach for the class of GP mentioned earlier, which includes CCA or small, especially squarehead graph spectrum learning and LDA.",
                    "label": 1
                },
                {
                    "sent": "One distinct.",
                    "label": 0
                },
                {
                    "sent": "Advantage of the two stage approach is that look assumption is required so it can be applied for any data set.",
                    "label": 1
                },
                {
                    "sent": "In addition, we can extend the two stage approach to the resisting case.",
                    "label": 0
                },
                {
                    "sent": "I will send radical alloces and empirical studies show that the two stage approach can scale to very.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Had problems.",
                    "label": 0
                },
                {
                    "sent": "And this is the outline of the rest of my talk.",
                    "label": 0
                },
                {
                    "sent": "First, we will give an overview of some example dimensionality reduction algorithms and then we introduce our two stage approach, including its main procedure, its equivalence relationship with the direct approach and.",
                    "label": 1
                },
                {
                    "sent": "Is time complexity analysis to evaluate the proposed algorithm.",
                    "label": 0
                },
                {
                    "sent": "We perform extensive empirical studies and unless.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is conclusive.",
                    "label": 0
                },
                {
                    "sent": "So first we introduce Canonical correlation analysis in Canonical correlation analysis, we try to compute two linear projections for two views XX&Y by maximizing the correlation after linear projection.",
                    "label": 1
                },
                {
                    "sent": "Under, it can issue that the projection for XWX can be obtained by solving this GP so we can comparing.",
                    "label": 0
                },
                {
                    "sent": "We can compare this DP with the GP in standard form so we can get the definition of matrix as an addition.",
                    "label": 0
                },
                {
                    "sent": "We can decompose as as the product of HH transpose the definition of matrix H is also giving here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As normal as the passing score.",
                    "label": 0
                },
                {
                    "sent": "Also confused linear projection for two views by maximizing the covariance afternoon protection, it can be sure that the projection for XWX can also be obtained by solving this DP.",
                    "label": 0
                },
                {
                    "sent": "So same Danny, we can get the definition of matrix S&H.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have graph spectral learning?",
                    "label": 0
                },
                {
                    "sent": "Is the dimensionality reduction technique for multi label classification so its uses have graph to capture the correlation among different labels.",
                    "label": 1
                },
                {
                    "sent": "So for example in this example we have three labels and six instances.",
                    "label": 1
                },
                {
                    "sent": "It lends a low dimensional embedding through a linear transformation W. By solving this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Where L is is the so-called have graph map Lassie.",
                    "label": 0
                },
                {
                    "sent": "It can issue that.",
                    "label": 0
                },
                {
                    "sent": "The transformation value can be obtained the best with this GPU.",
                    "label": 0
                },
                {
                    "sent": "So similarly we can get in the definition of S&H.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "LDA is a classical technique.",
                    "label": 0
                },
                {
                    "sent": "In classification, it tries to minimize the within class scope of various while maximizing the between class variance after linear projection.",
                    "label": 1
                },
                {
                    "sent": "I can be sure that the optical optical optimal linear projection correspond to the top eigenvectors of this matrix.",
                    "label": 0
                },
                {
                    "sent": "So similarly we can show that it can be formulated as generalized activity problem and the definition of S&HI was giving.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's the way give a overview of our two stage approach first.",
                    "label": 0
                },
                {
                    "sent": "In the first stage, we project the original data ex.",
                    "label": 0
                },
                {
                    "sent": "Onto the intermediate data X~ by using the transformation matrix W one and in the first stage W one is computed by solving our least squares problem.",
                    "label": 1
                },
                {
                    "sent": "In the second stage, we further Project X~ onto X prime.",
                    "label": 0
                },
                {
                    "sent": "The final reduce data by using W2 and in this stage the projection matrix W2.",
                    "label": 1
                },
                {
                    "sent": "Is computed by solving a generalized eigenvalue problem of reduced size so.",
                    "label": 0
                },
                {
                    "sent": "In the first stage we show the list with problem.",
                    "label": 0
                },
                {
                    "sent": "In the second stage will solve a generalized eigenvalue problem of reduced size so.",
                    "label": 0
                },
                {
                    "sent": "The whole problem can be solved very efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is a detailed procedure of the two stage approach.",
                    "label": 0
                },
                {
                    "sent": "Without regularization, the input is the data matrix X and the matrix age.",
                    "label": 0
                },
                {
                    "sent": "So in service learning H is generally constructed from the label information.",
                    "label": 0
                },
                {
                    "sent": "The output is the projection matrix W. In the first days we should have at least at least squares problem.",
                    "label": 0
                },
                {
                    "sent": "In this stage, each transport is the target matrix.",
                    "label": 0
                },
                {
                    "sent": "Becauses supers learning H is generally constructed from the label formation.",
                    "label": 0
                },
                {
                    "sent": "So each can be considered as the latent target.",
                    "label": 0
                },
                {
                    "sent": "In this stage we apply LSQR and concrete gradient algorithm to solve the least squares problem.",
                    "label": 0
                },
                {
                    "sent": "OS2 is well known for its scalability, and it is even reliable for your condition problem in the second stage.",
                    "label": 0
                },
                {
                    "sent": "Firstly we compute the intermediate data X~ and then.",
                    "label": 0
                },
                {
                    "sent": "Where actually what we do is that we replace X in the original generalized eigenvalue problem with the intermediate data access to that and then show off the resulting generalized eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "So this general generalized eigenvalue problem is also equivalent to this optimization problem, because the size of X~ is reduced significantly in the first stage.",
                    "label": 0
                },
                {
                    "sent": "So the computational cost of this stage is very low.",
                    "label": 0
                },
                {
                    "sent": "Finally we combine W1W2 and get the final projection matrix double.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, Alexa, we gave a time complexity analysis.",
                    "label": 1
                },
                {
                    "sent": "The total total computational cost of the first stage is giving here.",
                    "label": 1
                },
                {
                    "sent": "So capital N is total number of iterations of the iterative algorithm.",
                    "label": 0
                },
                {
                    "sent": "SQL and.",
                    "label": 0
                },
                {
                    "sent": "Be cause age is MBK matrix.",
                    "label": 0
                },
                {
                    "sent": "So so I need to show off.",
                    "label": 0
                },
                {
                    "sent": "So we need to solve Kaylee Scott problem so we have coefficient K here and three N + 50 + 2 Z is cost of.",
                    "label": 1
                },
                {
                    "sent": "So solving each iteration in SQR.",
                    "label": 1
                },
                {
                    "sent": "And the cost of second stage is crazy.",
                    "label": 0
                },
                {
                    "sent": "Plus N squared Katie is the cost of computer X~ and a skirt is cost of solving this optimization problem?",
                    "label": 0
                },
                {
                    "sent": "Under the cost of combining WNW, choose discard so the final computation cost is given here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We rigorously prove the equivalence relationship between the two stage approach and the direct approach.",
                    "label": 0
                },
                {
                    "sent": "The proving is quite technical, so I'm not going to the details.",
                    "label": 0
                },
                {
                    "sent": "The full proof can be found in the paper.",
                    "label": 0
                },
                {
                    "sent": "We just give some key result here.",
                    "label": 0
                },
                {
                    "sent": "The theorem one gives the solution of the two States and serum two gives the solution of the director approach without regularization.",
                    "label": 1
                },
                {
                    "sent": "So we can observe that they are identical, so the equipments relationship can be approved.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can further extend the two stage approach into the regularization setting.",
                    "label": 0
                },
                {
                    "sent": "In the first stage, becausw the.",
                    "label": 1
                },
                {
                    "sent": "Organizations consider that, so we should have a penalized least squares problem using the same target.",
                    "label": 1
                },
                {
                    "sent": "We can also rigorously prove the equivalence relationship between the two state project Direct approach in the localization case.",
                    "label": 1
                },
                {
                    "sent": "So compared with the existing work, our two stage approach is a significant improvement, cause lower something is required and it can be first extended to the regularization searching.",
                    "label": 0
                },
                {
                    "sent": "We can also analyze the time complexity analysis of the two state approach.",
                    "label": 0
                },
                {
                    "sent": "In the regulation case, and it turns out it is same as the previous one.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To verify the equipment or internship and demonstrated.",
                    "label": 0
                },
                {
                    "sent": "The scalability will perform extensive.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Permits.",
                    "label": 0
                },
                {
                    "sent": "Both synthetic and real world datasets are tested in our experiment.",
                    "label": 0
                },
                {
                    "sent": "The first 4 devices are synthetic and the remaining 90,000.",
                    "label": 0
                },
                {
                    "sent": "Our benchmark data sets in math class learning.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The math label learning.",
                    "label": 0
                },
                {
                    "sent": "In this experiment, we compare the class classification performance on each data set.",
                    "label": 1
                },
                {
                    "sent": "Increase the regularization parameter the.",
                    "label": 1
                },
                {
                    "sent": "Black barcodes, punctual direct approach and right back refund through the what two states approach and we can observe that they achieve the same performance.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In all cases.",
                    "label": 0
                },
                {
                    "sent": "And in this experiment we compare.",
                    "label": 0
                },
                {
                    "sent": "Compare the two 2 state approach with the direct approach.",
                    "label": 0
                },
                {
                    "sent": "In terms of scalability On this date on a CV1V2 datasets we fix the dimensionality as 2000 and increase the sample size from 500 of from 500 to 3000.",
                    "label": 0
                },
                {
                    "sent": "The blue curve correspond to Tuesdays and the red curve goes to correspond to the direct approach, so we can observe that the two stage approach is significant.",
                    "label": 0
                },
                {
                    "sent": "Significantly more scale.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in this experiment we fix them to manatee or fix sample size as resultant and increase the dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So we can also observe that.",
                    "label": 0
                },
                {
                    "sent": "The two states approach is much more.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Visit.",
                    "label": 0
                },
                {
                    "sent": "So in summary, in this paper we have established a two stage approach for a class of them, slanted reduction techniques which includes LDA or similar fashion, square, LACA and hypergraphs spectrum learning.",
                    "label": 1
                },
                {
                    "sent": "We rigorously prove the equivalence relationship between the two stage approach and the direct approach.",
                    "label": 1
                },
                {
                    "sent": "We emphasize the advantages of our two state parts.",
                    "label": 0
                },
                {
                    "sent": "So firstly lower something is required so it can be applied for any data set.",
                    "label": 0
                },
                {
                    "sent": "In addition it can be further applied in the organization searching.",
                    "label": 1
                },
                {
                    "sent": "It's good scalability makes it an idea to to process the height.",
                    "label": 1
                },
                {
                    "sent": "Large scale, high dimensional data in terms of future work we plan to extend the two stage approach to some to some other algorithms similar to the GP formulation Mason earlier.",
                    "label": 1
                },
                {
                    "sent": "We also plan to develop an online algorithm for the two stage approach.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the first days K because we focus on supervised learning in this paper.",
                    "label": 0
                },
                {
                    "sent": "So in this paper K is number of labels in multi level learning.",
                    "label": 0
                },
                {
                    "sent": "So in the first stage we K it's fixed.",
                    "label": 0
                },
                {
                    "sent": "Is number of classics.",
                    "label": 0
                }
            ]
        }
    }
}