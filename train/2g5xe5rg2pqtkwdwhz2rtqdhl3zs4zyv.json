{
    "id": "2g5xe5rg2pqtkwdwhz2rtqdhl3zs4zyv",
    "title": "Nearest Hyperdisk Methods for High-Dimensional Classification",
    "info": {
        "author": [
            "Bill Triggs, Laboratoire Jean Kuntzmann"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_triggs_nhm/",
    "segmentation": [
        [
            "Good, OK, so I'm not talking about distance learning though.",
            "I'm talking about some things that involve ellipsoids and things like that.",
            "This is joint work with how can civic calpus from Turkey and Robbie pull car from?"
        ],
        [
            "University in the United States.",
            "OK, so we're interested in applications like face recognition, content based, image retrieval with.",
            "There are many classes and high dimensional descriptors.",
            "We'd like to be able to have a fairly compact representation for the classes, which allows easy addition removal, updating of the classes.",
            "That suggests that we should take an approach which is like a kind of one class approach generative representation where we have a single model for each class rather than learning something that's discriminative between classes, such as an SVM.",
            "And we expect to have relatively few training examples per class.",
            "Again suggests some kind of approach which is more geometric and less probabilistic because you need a lot of data to learn a probabilistic model, we want something that simple and Colonel isable as well."
        ],
        [
            "OK, so one of the problems if you use very simple classifier such as nearest neighbors water called typically whole art effects.",
            "So if I'm learning if I've got classes which are represented maybe by the red region and the green region, and I have some points near the boundaries.",
            "If I learn a nearest neighbor classifier for this, then the nearest neighbor is often the wrong one in high dimensions, just because there are so many dimensions in which things can happen that often.",
            "This is kind of a whole data set, I didn't happen to get a positive sample in this region here.",
            "So I guess my training example gets assigned to the wrong class.",
            "Now people who like SVM will tell you, but in SVM will solve this problem by learning a nice margin here which will interpolate between these examples and therefore give you the right class.",
            "Well, that's true, but it's not perfect, because if your class regions happen to be convex like this, you're training examples in high dimensional space.",
            "They're actually very sparse in the space, and typically they lie inside the boundary of the class.",
            "And in any case, they won't be able to densely tile or densely fill out the region of the class, so they cut off a chunk of the class and therefore again in SVM might make a mistake because the true example happened to lie outside of the chunk that gets cut off by the training examples.",
            "So SVM also subject to a whole problem, so this just maybe what we'd like to do is to learn some kind of class model that fills in holes very naturally, so that the classes do that.",
            "And SVM fills them in linearly, but maybe we have other."
        ],
        [
            "Wasteful things.",
            "OK, so this is the strategy that will take will approximate each class with a convex region, which is going to be based on the given training samples and then will classify new examples simply to the class with the nearest convex model.",
            "OK, so we take a couple of convex models for the different classes.",
            "We have a new sample, we find the Euclidean distance on modified equal distance.",
            "In our case, we're not distance learning we are learning.",
            "However, some kind of convex model to the class and then we classify to the class that's nearest to the to the convex model.",
            "OK, so we need we need a model that's convex because we need to calculate distances to it.",
            "So we need an efficient calculation for that.",
            "Also, in high dimensions we very rarely have enough data to learn an elaborate model, so we're going to look at very very simple geometric models and see how well they can do an effect will consider 4."
        ],
        [
            "Current models the 1st three are already known.",
            "OK, so the first one is the convex Hull of the training samples.",
            "So I can write that informally with a convex combination of the training samples with positive coefficients, sum to one.",
            "OK, that's the model that makes very few assumptions about the type of the shape of the class.",
            "But for practical numbers of training examples in high dimensions, you really have to realize that the convex Hull lies very, very much inside the class.",
            "Typically, if I'm for example, a class that's a sphere or something, and I have a number of training samples which is less than the dimension of the sphere, then they'll form a simplex and that simplex will have a volume which can be so astronomically small compared to the volume of the sphere.",
            "In high dimensions that you will never lie within the within the sphere, so.",
            "Almost all the classes outside the convex Hull of the training samples, so that's a problem that means that this problem of regions being cut off is very, very difficult for a convex Hull model.",
            "Also, the convex hulls are computationally expensive.",
            "In practice, they have an exponential number of facets, which means that you can't store the whole explicitly, so you have to calculate the appropriate convict, the nearest convex facet for a new sample.",
            "At runtime, which is a quadratic program that you have to solve for each example at runtime, it's a simple quadratic program, but it's still a quadratic program that you need to solve, and therefore the method can be rather slow.",
            "I should just point out the relationship to SVM.",
            "If you take 2 convex hulls for two different classes, and you find the nearest.",
            "Pair of points between them and then the orthogonal separated to that is the SVM direction.",
            "So there's a close relation you can think of the SVM as one facet of the separating surface between the two classes."
        ],
        [
            "OK, so that's one model.",
            "Now let's take an even simpler model.",
            "We simply take the training samples and find their affine Hull, so the linear subspace shifted linear subspace that lies in all the contains all the examples.",
            "Now you may see, say that model is far too simple.",
            "It goes well outside the region of the class.",
            "We don't know where the classes within the affine Hull, so you might expect that to be a very bad method.",
            "In fact, in high dimensions that turned out to be a very good method, surprisingly good, so it's worth thinking about.",
            "It's very easy to calculate, it's just a simple linear calculation, so so that's quite."
        ],
        [
            "Nice method to consider.",
            "OK, Third model the bounding hyper sphere of the training samples.",
            "So here we simply take the samples and we fit the smallest volume sphere that will enclose them.",
            "OK, we can do this again with a quadratic program.",
            "It's a rather loose approximation, in the same way the FN hole is loose because it goes very enlarged.",
            "Directions beyond the class.",
            "The sphere is loose because if the class is sort of flat sphere will typically extend well above and below the class.",
            "OK, so there's a lot of volume in the sphere that's not near the affine Hull of the class, and for that reason this can be sometimes a bad method, but it is also recently simple method, which for example is the motivation for the one class SVM, so it's something we've seen before, and we know that that works quite well in a number of cases.",
            "Another thing is that the distance calculations here are trivial because it's just a sphere, so just shifting the Euclidean distance by."
        ],
        [
            "Radius of the sphere.",
            "OK, So what model will we take?",
            "We're going to take something which is the intersection of the previous two.",
            "We're going to take the bounding hyper disc of the training samples, so we calculate the affine Hull of the samples.",
            "We calculate the bounding hyper sphere of them.",
            "We take the intersection so it's a disc like.",
            "Geometric form in high dimensions.",
            "It's still somewhat of a loose approximation, but it doesn't code both.",
            "Which variables are relevant, so the affine Hull manages to do that, and where within that space of variables the class actually lies.",
            "So the sphere manage to do that."
        ],
        [
            "OK, so that's the model to calculate.",
            "It is pretty straightforward.",
            "You calculate you refine whole project.",
            "The training test samples onto that, so you can use for example and SPD to find a nice orthogonal coordinate system on that projected Hull.",
            "And then you need to solve a quadratic program in order to find the sphere.",
            "You can include slack variables here for outliers if you want to.",
            "That's basically all there is to say about it.",
            "It's a fairly straightforward."
        ],
        [
            "And then once you've got the disk in order to find distances, you simply project onto the affine Hull.",
            "If the projection lies inside the sphere, then it's just the distance the orthogonal distance that counts.",
            "If it lies outside, you have to move back down towards the sphere until you hit the sphere and it's those two just to the square of those two distances that count so straightforward calculation."
        ],
        [
            "That you can write in that form.",
            "OK now one thing about these models is that all of them can be kernelized very easily and.",
            "Maybe it's not worth going through the formula on this, except that a lot of people seem to have a kind of a mental block about what you can and can't calculate with kernelized when you can and can't.",
            "Kernel Lisa space so you reading the textbooks if it's something that you can express in terms of inner product, you Colonel eyes it OK and that's true, but it's very, very misleading for people people think.",
            "Oh gosh, well, I've got this formula can express it in terms of inner product.",
            "So another way to say it is simply anything that you can express in terms of coordinate geometry you can you can kernel eyes.",
            "OK, because you can always express coordinates in terms of always express coordinate coordinates in terms of product.",
            "So here's what we do.",
            "We have the standard things, some implicit feature space embedding the kernel matrix.",
            "That's the product of those two.",
            "We write all the training samples as a nice big implicit matrix Phi.",
            "The kernel matrix of the training samples is the product of those two five things together.",
            "And then, for each new example that we have, if we product five with it some feature vector, we get the kernel vector of the sample.",
            "OK, the standard things that we have now here we are interested in doing a fine computations, not linear ones.",
            "So the first step is always to center the training data, subtract the mean of the training data and you can do that in sample space.",
            "With this matrix Pi, which is just a very simple identity minus one M being A1 matrix of 1 vector of ones within entries.",
            "So very simple kind of subtraction of mean type of operation.",
            "So with that matrix there if we take it's truncated thin SVD, so thin or truncated in order to get rid of dimensions that are irrelevant then.",
            "With EU matrix is a thing that gives us orthogonal coordinates in the space spanned by the examples, so that's essentially what we need to multiply by.",
            "In fact, if we have this kernel vector, or if we express it in features, we take the feature vector of the point, subtract the mean, and then multiply by this.",
            "This gives us an affordable coordinate system within the space spanned by the feature space spanned by the training examples, so that's what we want to do.",
            "Any kind of computation, like distances we can't calculate you explicitly.",
            "Because it's an implicit feature space quantity.",
            "But what we can do is calculate a matrix A, which gives an equivalent set of coordinates in terms of the kernel vector and the way to do that is simply to take your centered kernel matrix, so these projection operators on each side of the kernel do an eigendecomposition of that, a thin one, and then the DMV matrices together with projection give you the appropriate matrix A that you can use for this projection.",
            "OK, so it's very straightforward to calculate rectified orthogonal coordinates in this projection space in terms of the training examples.",
            "And then from there it's a straightforward kind of kernel calculation to find the distance from the point to the kernelized hyper disk.",
            "So."
        ],
        [
            "Can do all those things right?",
            "So now a couple of experiments so this is First Toy Experiment.",
            "We have data which is 300 dimensional simply spheres.",
            "Four spheres in 300 dimensional space, and we squash 200 of the mentions quite a lot.",
            "OK, so we've now got kind of pancake shaped things and here we are projecting on the only two dimensions of this data set which are actually discriminate, one of which is squashed and one of which is not squashed OK."
        ],
        [
            "So that's the data set.",
            "Now what we're going to do here is show the different methods so the hyper disc method, the affine hold method in the convex Hull method.",
            "As we increase the number of training samples and we're looking for recognition rate.",
            "So the amount of time we get the correct prediction of the class, so you can see that once you get up much beyond 100 dimensions, which is the dimension of the.",
            "The lodge non discriminate dimensions.",
            "There are 100, not discriminations.",
            "Once you get a bit about that so you can study estimating the linear spaces correctly, the affine and the hyper disk method are doing much better than the convex Hull method.",
            "OK, and ultimately the comics.",
            "The disk method is doing best now just a couple of points on this data set.",
            "First of all, the sapphire method it looks to be doing really, really well.",
            "In fact the FN method has an asymptotic error rate of 50."
        ],
        [
            "Paint, because if I go back to the starter set, if I learned an exact linear subspace for the set, it would lie exactly on the line.",
            "That also includes.",
            "So this set a would also include the set C, so I wouldn't get any discrimination between A&C or between B&D with the affine method 50%."
        ],
        [
            "Error rate yet in practice we were up around 80%, so this curve will gradually go down as we go asymptotically to Infinity, but in fact with small sample size is fine, is really good OK. And second thing, the convex method asymptotically has good accuracy, but in practice it has very poor accuracy for these sample sizes.",
            "Why is that?",
            "Well, I've got one class sitting here which is a big disk and I've got another class sitting just below it.",
            "Another big disk, the affine holes, are small subset, small, convex.",
            "Polytopes which sit inside those seats, and they'll be kind of randomly oriented and have random vertices at random places.",
            "So in fact what happens is that because the sets are near one another, it's just random whether you happen to be closer to a vertex or to a facet of one class or the other.",
            "So in fact the convex method, because you don't have enough training data to fill out these disks, gives you very, very poor classification accuracy, and these problems and this is something that's generic for the convex Hull method.",
            "It's 22 conservative to tighten approximation to the classes.",
            "So it doesn't work too well.",
            "Why did the affine method do?",
            "Well here will affect what happened is that when you estimate an affine Hull to one of these data sets, the noise makes it tilt slightly."
        ],
        [
            "And because it's tilting slightly, it won't go straight through the middle of the data set.",
            "It will be slightly tilted and will therefore pass at some distance from this other class, but more dimensions.",
            "I have the more things more dimensions in which things can tilt, so the more effective tilt that I have, and therefore in some sense the affine thing, manages to capture what the hyper disc thing does as well.",
            "It captures something about the position of the class within the."
        ],
        [
            "The subspace.",
            "OK, so."
        ],
        [
            "It's a little explanation of why these things are going well now on some real experiment, so here we've got a simple data set.",
            "This is Columbia coil data sets, objects on the turntable against the black background.",
            "We're just using rule pixels as the features here, because the problem is too simple.",
            "Otherwise we are testing a number of other methods here, in particular SVM.",
            "So this is one against the rest SVM versus our methods and you can see that these convex model methods are doing best in this problem."
        ],
        [
            "Particularly hyper disk method.",
            "OK, another data set, so this is again using these linear methods.",
            "Here we've got a set of birds against backgrounds, birds, and no particular position.",
            "They change in scale, they change in pose and everything.",
            "So this is starting to be a real vision data set.",
            "We use bag of words over SIFT features.",
            "I'm not going to go into the details of that if you don't know what it is, talk to me later.",
            "So with this data set here 2000 word dictionary, it turned out that the SVM and the convex Hull method were best.",
            "But you can see still that these the other two convex model methods are doing pretty well.",
            "So again we compared it."
        ],
        [
            "With this theme for this Now, let's look at some low dimensional data set.",
            "So low dimensional being more examples than dimensions and we're using kernelized versions of the methods here.",
            "So before it was linear stream here it's kernelized SVM.",
            "You can see again in this case.",
            "This is data sets from UCI that these these convex model based methods are doing pretty well.",
            "In one class, one case VM wins, but in the others, that's the config."
        ],
        [
            "Tell me if it's doing well.",
            "OK, so the summary of this part overall, this convex class approximation method seems to be going pretty well.",
            "They're often comparable or better than SVM.",
            "It's easy to add, remove and update your classes and the affine version of works surprisingly well, much better than you expect theoretically, although in practice the hyper disc one almost never does worse and sometimes does better, so it's a little bit more reliable as a choice.",
            "The convex Hull methods are less reliable, sometimes it works well and sometimes not.",
            "How many minutes do I have left?",
            "2 minutes."
        ],
        [
            "Yeah, OK so I just wanted to give you a very quick overview of another method that we have which was published in CPR which uses this convex models to find discriminate directions.",
            "So previously we had a very rigid model of the class.",
            "Now what we're going to do is we're going to use these convex models to find project out some discriminate directions and then use nearest neighbor classifier or any other class low dimensional classifier that you want on the output.",
            "So the main idea is that when you need to separate classes.",
            "A good way to to find discriminate directions is to look for separations from your current example to the two other classes and find in some sense of weighted set of those directions that give you good separation, and in particular the waiting has to be something that focuses on the nearby classes.",
            "OK, so that's the basic thing.",
            "We could also look at in sample separations, but we think they'll be too noisy.",
            "This is the whole problem again, or we could look at into class separations into class model separations, but we think there would be too rigid, so we want the compromise of going.",
            "Model 2."
        ],
        [
            "Sample the way we're going to do this is we're going to take local discriminants, local lamp direction vectors from examples to the convex class, model unit vectors of those we're going to wait them with a decreasing function of distance.",
            "We sum over all examples or classes.",
            "That gives us a scatter matrix, which we will then be able to decompose with an S feet with eigen decomposition and the dominant few directions of that will give us our discriminate directions, and there waiting would choose is just the exponent."
        ],
        [
            "So here, OK, so we've applied it for example to a face recognition data set were just doing some normal."
        ],
        [
            "Station here you could ignore that and you can see the results here.",
            "So if this is a multiclass problem, so there's many directions that SVM projects out, and here is our set of.",
            "Sort of convex model based things so you can see that we are on the same envelope is SVM, but we're much more flexible as to where we can lie on that envelope.",
            "So all of those, all of our methods are using simple nearest neighbor in the output."
        ],
        [
            "Space here OK.",
            "Similarly, Pasco Visual Object Challenge class F on or happy just projection and then nearest neighbors and you can see that again.",
            "These things are doing better than SVM."
        ],
        [
            "Example OK again this UCI birds dot UIUC birds data set that we saw before.",
            "Again these finding hyper disc projection methods are doing better than SVM."
        ],
        [
            "OK, so that's just a summary of the second part, using distances to convex class model seems to be a good heuristic.",
            "Finding discriminant projection directions and hyper disk enough on how models again seemed to be equal best and this gives nearest neighbor comparable performance to SVM OK questions.",
            "So.",
            "The SVM, well, the most of these films.",
            "They were linear, so they don't have kernels in them.",
            "When I used a linear method development that I also used a linear SVM and when I just curl method I used kernel SVM.",
            "The kernel once we always use Gaussian kernels, we use the same kernel for all models.",
            "So they will have the same things to tune basically.",
            "Did you try simply modeling these transferred over the Gaussian?",
            "Because hyper viscous.",
            "I was comparing to LDA in some of the slides, which is sort of that.",
            "It's.",
            "It called passed at the same carreras.",
            "Yeah.",
            "You could do quadratic quadratic LDA, qda as well.",
            "The philosophy is somewhat different.",
            "The philosophy is to try to find kind of a model that's well.",
            "First of all our models there, ellipsoids, and certain number of dimensions, but in the other dimensions, they're not Gaussian, they simply flat.",
            "So we have singular covariance in that sense, and we're finding things that bound the examples.",
            "We're not finding things that lie inside them, so those two things together.",
            "I mean, The thing is, with the Gaussian, I don't know how you used in this geometric framework because you need to find the distance from the new example to it.",
            "So if you're in the Gaussian frame it you use Mahalanobis distance.",
            "OK, so you're very much in the kind of distance learning type of framework, whereas we're not in the distance learning framework.",
            "We are in a geometric type of framework.",
            "We simply looking for distances to some geometric approximation to the class.",
            "Yes, these things can be compared, of course.",
            "Anymore questions.",
            "Yes, go ahead.",
            "We talked about kernel Ising it.",
            "Yes, you actually experiment with that.",
            "Yes, the last experiments I showed this.",
            "Let me just do this.",
            "So for the."
        ],
        [
            "Yeah, this one.",
            "These are kernelized experiments OK.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good, OK, so I'm not talking about distance learning though.",
                    "label": 0
                },
                {
                    "sent": "I'm talking about some things that involve ellipsoids and things like that.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with how can civic calpus from Turkey and Robbie pull car from?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "University in the United States.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're interested in applications like face recognition, content based, image retrieval with.",
                    "label": 1
                },
                {
                    "sent": "There are many classes and high dimensional descriptors.",
                    "label": 0
                },
                {
                    "sent": "We'd like to be able to have a fairly compact representation for the classes, which allows easy addition removal, updating of the classes.",
                    "label": 1
                },
                {
                    "sent": "That suggests that we should take an approach which is like a kind of one class approach generative representation where we have a single model for each class rather than learning something that's discriminative between classes, such as an SVM.",
                    "label": 1
                },
                {
                    "sent": "And we expect to have relatively few training examples per class.",
                    "label": 0
                },
                {
                    "sent": "Again suggests some kind of approach which is more geometric and less probabilistic because you need a lot of data to learn a probabilistic model, we want something that simple and Colonel isable as well.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so one of the problems if you use very simple classifier such as nearest neighbors water called typically whole art effects.",
                    "label": 0
                },
                {
                    "sent": "So if I'm learning if I've got classes which are represented maybe by the red region and the green region, and I have some points near the boundaries.",
                    "label": 0
                },
                {
                    "sent": "If I learn a nearest neighbor classifier for this, then the nearest neighbor is often the wrong one in high dimensions, just because there are so many dimensions in which things can happen that often.",
                    "label": 1
                },
                {
                    "sent": "This is kind of a whole data set, I didn't happen to get a positive sample in this region here.",
                    "label": 1
                },
                {
                    "sent": "So I guess my training example gets assigned to the wrong class.",
                    "label": 0
                },
                {
                    "sent": "Now people who like SVM will tell you, but in SVM will solve this problem by learning a nice margin here which will interpolate between these examples and therefore give you the right class.",
                    "label": 0
                },
                {
                    "sent": "Well, that's true, but it's not perfect, because if your class regions happen to be convex like this, you're training examples in high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "They're actually very sparse in the space, and typically they lie inside the boundary of the class.",
                    "label": 0
                },
                {
                    "sent": "And in any case, they won't be able to densely tile or densely fill out the region of the class, so they cut off a chunk of the class and therefore again in SVM might make a mistake because the true example happened to lie outside of the chunk that gets cut off by the training examples.",
                    "label": 0
                },
                {
                    "sent": "So SVM also subject to a whole problem, so this just maybe what we'd like to do is to learn some kind of class model that fills in holes very naturally, so that the classes do that.",
                    "label": 0
                },
                {
                    "sent": "And SVM fills them in linearly, but maybe we have other.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wasteful things.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the strategy that will take will approximate each class with a convex region, which is going to be based on the given training samples and then will classify new examples simply to the class with the nearest convex model.",
                    "label": 1
                },
                {
                    "sent": "OK, so we take a couple of convex models for the different classes.",
                    "label": 0
                },
                {
                    "sent": "We have a new sample, we find the Euclidean distance on modified equal distance.",
                    "label": 0
                },
                {
                    "sent": "In our case, we're not distance learning we are learning.",
                    "label": 0
                },
                {
                    "sent": "However, some kind of convex model to the class and then we classify to the class that's nearest to the to the convex model.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need we need a model that's convex because we need to calculate distances to it.",
                    "label": 0
                },
                {
                    "sent": "So we need an efficient calculation for that.",
                    "label": 1
                },
                {
                    "sent": "Also, in high dimensions we very rarely have enough data to learn an elaborate model, so we're going to look at very very simple geometric models and see how well they can do an effect will consider 4.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Current models the 1st three are already known.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first one is the convex Hull of the training samples.",
                    "label": 1
                },
                {
                    "sent": "So I can write that informally with a convex combination of the training samples with positive coefficients, sum to one.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the model that makes very few assumptions about the type of the shape of the class.",
                    "label": 0
                },
                {
                    "sent": "But for practical numbers of training examples in high dimensions, you really have to realize that the convex Hull lies very, very much inside the class.",
                    "label": 1
                },
                {
                    "sent": "Typically, if I'm for example, a class that's a sphere or something, and I have a number of training samples which is less than the dimension of the sphere, then they'll form a simplex and that simplex will have a volume which can be so astronomically small compared to the volume of the sphere.",
                    "label": 0
                },
                {
                    "sent": "In high dimensions that you will never lie within the within the sphere, so.",
                    "label": 1
                },
                {
                    "sent": "Almost all the classes outside the convex Hull of the training samples, so that's a problem that means that this problem of regions being cut off is very, very difficult for a convex Hull model.",
                    "label": 1
                },
                {
                    "sent": "Also, the convex hulls are computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "In practice, they have an exponential number of facets, which means that you can't store the whole explicitly, so you have to calculate the appropriate convict, the nearest convex facet for a new sample.",
                    "label": 0
                },
                {
                    "sent": "At runtime, which is a quadratic program that you have to solve for each example at runtime, it's a simple quadratic program, but it's still a quadratic program that you need to solve, and therefore the method can be rather slow.",
                    "label": 1
                },
                {
                    "sent": "I should just point out the relationship to SVM.",
                    "label": 0
                },
                {
                    "sent": "If you take 2 convex hulls for two different classes, and you find the nearest.",
                    "label": 0
                },
                {
                    "sent": "Pair of points between them and then the orthogonal separated to that is the SVM direction.",
                    "label": 0
                },
                {
                    "sent": "So there's a close relation you can think of the SVM as one facet of the separating surface between the two classes.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's one model.",
                    "label": 0
                },
                {
                    "sent": "Now let's take an even simpler model.",
                    "label": 0
                },
                {
                    "sent": "We simply take the training samples and find their affine Hull, so the linear subspace shifted linear subspace that lies in all the contains all the examples.",
                    "label": 1
                },
                {
                    "sent": "Now you may see, say that model is far too simple.",
                    "label": 0
                },
                {
                    "sent": "It goes well outside the region of the class.",
                    "label": 0
                },
                {
                    "sent": "We don't know where the classes within the affine Hull, so you might expect that to be a very bad method.",
                    "label": 1
                },
                {
                    "sent": "In fact, in high dimensions that turned out to be a very good method, surprisingly good, so it's worth thinking about.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to calculate, it's just a simple linear calculation, so so that's quite.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice method to consider.",
                    "label": 0
                },
                {
                    "sent": "OK, Third model the bounding hyper sphere of the training samples.",
                    "label": 1
                },
                {
                    "sent": "So here we simply take the samples and we fit the smallest volume sphere that will enclose them.",
                    "label": 1
                },
                {
                    "sent": "OK, we can do this again with a quadratic program.",
                    "label": 0
                },
                {
                    "sent": "It's a rather loose approximation, in the same way the FN hole is loose because it goes very enlarged.",
                    "label": 0
                },
                {
                    "sent": "Directions beyond the class.",
                    "label": 1
                },
                {
                    "sent": "The sphere is loose because if the class is sort of flat sphere will typically extend well above and below the class.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a lot of volume in the sphere that's not near the affine Hull of the class, and for that reason this can be sometimes a bad method, but it is also recently simple method, which for example is the motivation for the one class SVM, so it's something we've seen before, and we know that that works quite well in a number of cases.",
                    "label": 0
                },
                {
                    "sent": "Another thing is that the distance calculations here are trivial because it's just a sphere, so just shifting the Euclidean distance by.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Radius of the sphere.",
                    "label": 0
                },
                {
                    "sent": "OK, So what model will we take?",
                    "label": 0
                },
                {
                    "sent": "We're going to take something which is the intersection of the previous two.",
                    "label": 1
                },
                {
                    "sent": "We're going to take the bounding hyper disc of the training samples, so we calculate the affine Hull of the samples.",
                    "label": 0
                },
                {
                    "sent": "We calculate the bounding hyper sphere of them.",
                    "label": 0
                },
                {
                    "sent": "We take the intersection so it's a disc like.",
                    "label": 1
                },
                {
                    "sent": "Geometric form in high dimensions.",
                    "label": 0
                },
                {
                    "sent": "It's still somewhat of a loose approximation, but it doesn't code both.",
                    "label": 0
                },
                {
                    "sent": "Which variables are relevant, so the affine Hull manages to do that, and where within that space of variables the class actually lies.",
                    "label": 0
                },
                {
                    "sent": "So the sphere manage to do that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the model to calculate.",
                    "label": 0
                },
                {
                    "sent": "It is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "You calculate you refine whole project.",
                    "label": 0
                },
                {
                    "sent": "The training test samples onto that, so you can use for example and SPD to find a nice orthogonal coordinate system on that projected Hull.",
                    "label": 0
                },
                {
                    "sent": "And then you need to solve a quadratic program in order to find the sphere.",
                    "label": 0
                },
                {
                    "sent": "You can include slack variables here for outliers if you want to.",
                    "label": 0
                },
                {
                    "sent": "That's basically all there is to say about it.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly straightforward.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then once you've got the disk in order to find distances, you simply project onto the affine Hull.",
                    "label": 1
                },
                {
                    "sent": "If the projection lies inside the sphere, then it's just the distance the orthogonal distance that counts.",
                    "label": 0
                },
                {
                    "sent": "If it lies outside, you have to move back down towards the sphere until you hit the sphere and it's those two just to the square of those two distances that count so straightforward calculation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That you can write in that form.",
                    "label": 0
                },
                {
                    "sent": "OK now one thing about these models is that all of them can be kernelized very easily and.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not worth going through the formula on this, except that a lot of people seem to have a kind of a mental block about what you can and can't calculate with kernelized when you can and can't.",
                    "label": 0
                },
                {
                    "sent": "Kernel Lisa space so you reading the textbooks if it's something that you can express in terms of inner product, you Colonel eyes it OK and that's true, but it's very, very misleading for people people think.",
                    "label": 0
                },
                {
                    "sent": "Oh gosh, well, I've got this formula can express it in terms of inner product.",
                    "label": 0
                },
                {
                    "sent": "So another way to say it is simply anything that you can express in terms of coordinate geometry you can you can kernel eyes.",
                    "label": 0
                },
                {
                    "sent": "OK, because you can always express coordinates in terms of always express coordinate coordinates in terms of product.",
                    "label": 0
                },
                {
                    "sent": "So here's what we do.",
                    "label": 0
                },
                {
                    "sent": "We have the standard things, some implicit feature space embedding the kernel matrix.",
                    "label": 1
                },
                {
                    "sent": "That's the product of those two.",
                    "label": 0
                },
                {
                    "sent": "We write all the training samples as a nice big implicit matrix Phi.",
                    "label": 1
                },
                {
                    "sent": "The kernel matrix of the training samples is the product of those two five things together.",
                    "label": 0
                },
                {
                    "sent": "And then, for each new example that we have, if we product five with it some feature vector, we get the kernel vector of the sample.",
                    "label": 0
                },
                {
                    "sent": "OK, the standard things that we have now here we are interested in doing a fine computations, not linear ones.",
                    "label": 1
                },
                {
                    "sent": "So the first step is always to center the training data, subtract the mean of the training data and you can do that in sample space.",
                    "label": 1
                },
                {
                    "sent": "With this matrix Pi, which is just a very simple identity minus one M being A1 matrix of 1 vector of ones within entries.",
                    "label": 0
                },
                {
                    "sent": "So very simple kind of subtraction of mean type of operation.",
                    "label": 0
                },
                {
                    "sent": "So with that matrix there if we take it's truncated thin SVD, so thin or truncated in order to get rid of dimensions that are irrelevant then.",
                    "label": 0
                },
                {
                    "sent": "With EU matrix is a thing that gives us orthogonal coordinates in the space spanned by the examples, so that's essentially what we need to multiply by.",
                    "label": 0
                },
                {
                    "sent": "In fact, if we have this kernel vector, or if we express it in features, we take the feature vector of the point, subtract the mean, and then multiply by this.",
                    "label": 1
                },
                {
                    "sent": "This gives us an affordable coordinate system within the space spanned by the feature space spanned by the training examples, so that's what we want to do.",
                    "label": 0
                },
                {
                    "sent": "Any kind of computation, like distances we can't calculate you explicitly.",
                    "label": 0
                },
                {
                    "sent": "Because it's an implicit feature space quantity.",
                    "label": 0
                },
                {
                    "sent": "But what we can do is calculate a matrix A, which gives an equivalent set of coordinates in terms of the kernel vector and the way to do that is simply to take your centered kernel matrix, so these projection operators on each side of the kernel do an eigendecomposition of that, a thin one, and then the DMV matrices together with projection give you the appropriate matrix A that you can use for this projection.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's very straightforward to calculate rectified orthogonal coordinates in this projection space in terms of the training examples.",
                    "label": 0
                },
                {
                    "sent": "And then from there it's a straightforward kind of kernel calculation to find the distance from the point to the kernelized hyper disk.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can do all those things right?",
                    "label": 0
                },
                {
                    "sent": "So now a couple of experiments so this is First Toy Experiment.",
                    "label": 0
                },
                {
                    "sent": "We have data which is 300 dimensional simply spheres.",
                    "label": 0
                },
                {
                    "sent": "Four spheres in 300 dimensional space, and we squash 200 of the mentions quite a lot.",
                    "label": 0
                },
                {
                    "sent": "OK, so we've now got kind of pancake shaped things and here we are projecting on the only two dimensions of this data set which are actually discriminate, one of which is squashed and one of which is not squashed OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the data set.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to do here is show the different methods so the hyper disc method, the affine hold method in the convex Hull method.",
                    "label": 1
                },
                {
                    "sent": "As we increase the number of training samples and we're looking for recognition rate.",
                    "label": 1
                },
                {
                    "sent": "So the amount of time we get the correct prediction of the class, so you can see that once you get up much beyond 100 dimensions, which is the dimension of the.",
                    "label": 0
                },
                {
                    "sent": "The lodge non discriminate dimensions.",
                    "label": 0
                },
                {
                    "sent": "There are 100, not discriminations.",
                    "label": 0
                },
                {
                    "sent": "Once you get a bit about that so you can study estimating the linear spaces correctly, the affine and the hyper disk method are doing much better than the convex Hull method.",
                    "label": 0
                },
                {
                    "sent": "OK, and ultimately the comics.",
                    "label": 0
                },
                {
                    "sent": "The disk method is doing best now just a couple of points on this data set.",
                    "label": 0
                },
                {
                    "sent": "First of all, the sapphire method it looks to be doing really, really well.",
                    "label": 1
                },
                {
                    "sent": "In fact the FN method has an asymptotic error rate of 50.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paint, because if I go back to the starter set, if I learned an exact linear subspace for the set, it would lie exactly on the line.",
                    "label": 0
                },
                {
                    "sent": "That also includes.",
                    "label": 0
                },
                {
                    "sent": "So this set a would also include the set C, so I wouldn't get any discrimination between A&C or between B&D with the affine method 50%.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error rate yet in practice we were up around 80%, so this curve will gradually go down as we go asymptotically to Infinity, but in fact with small sample size is fine, is really good OK. And second thing, the convex method asymptotically has good accuracy, but in practice it has very poor accuracy for these sample sizes.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, I've got one class sitting here which is a big disk and I've got another class sitting just below it.",
                    "label": 0
                },
                {
                    "sent": "Another big disk, the affine holes, are small subset, small, convex.",
                    "label": 0
                },
                {
                    "sent": "Polytopes which sit inside those seats, and they'll be kind of randomly oriented and have random vertices at random places.",
                    "label": 0
                },
                {
                    "sent": "So in fact what happens is that because the sets are near one another, it's just random whether you happen to be closer to a vertex or to a facet of one class or the other.",
                    "label": 0
                },
                {
                    "sent": "So in fact the convex method, because you don't have enough training data to fill out these disks, gives you very, very poor classification accuracy, and these problems and this is something that's generic for the convex Hull method.",
                    "label": 0
                },
                {
                    "sent": "It's 22 conservative to tighten approximation to the classes.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't work too well.",
                    "label": 0
                },
                {
                    "sent": "Why did the affine method do?",
                    "label": 0
                },
                {
                    "sent": "Well here will affect what happened is that when you estimate an affine Hull to one of these data sets, the noise makes it tilt slightly.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because it's tilting slightly, it won't go straight through the middle of the data set.",
                    "label": 0
                },
                {
                    "sent": "It will be slightly tilted and will therefore pass at some distance from this other class, but more dimensions.",
                    "label": 0
                },
                {
                    "sent": "I have the more things more dimensions in which things can tilt, so the more effective tilt that I have, and therefore in some sense the affine thing, manages to capture what the hyper disc thing does as well.",
                    "label": 0
                },
                {
                    "sent": "It captures something about the position of the class within the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The subspace.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a little explanation of why these things are going well now on some real experiment, so here we've got a simple data set.",
                    "label": 0
                },
                {
                    "sent": "This is Columbia coil data sets, objects on the turntable against the black background.",
                    "label": 0
                },
                {
                    "sent": "We're just using rule pixels as the features here, because the problem is too simple.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we are testing a number of other methods here, in particular SVM.",
                    "label": 0
                },
                {
                    "sent": "So this is one against the rest SVM versus our methods and you can see that these convex model methods are doing best in this problem.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Particularly hyper disk method.",
                    "label": 0
                },
                {
                    "sent": "OK, another data set, so this is again using these linear methods.",
                    "label": 0
                },
                {
                    "sent": "Here we've got a set of birds against backgrounds, birds, and no particular position.",
                    "label": 0
                },
                {
                    "sent": "They change in scale, they change in pose and everything.",
                    "label": 0
                },
                {
                    "sent": "So this is starting to be a real vision data set.",
                    "label": 0
                },
                {
                    "sent": "We use bag of words over SIFT features.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details of that if you don't know what it is, talk to me later.",
                    "label": 0
                },
                {
                    "sent": "So with this data set here 2000 word dictionary, it turned out that the SVM and the convex Hull method were best.",
                    "label": 1
                },
                {
                    "sent": "But you can see still that these the other two convex model methods are doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "So again we compared it.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this theme for this Now, let's look at some low dimensional data set.",
                    "label": 0
                },
                {
                    "sent": "So low dimensional being more examples than dimensions and we're using kernelized versions of the methods here.",
                    "label": 0
                },
                {
                    "sent": "So before it was linear stream here it's kernelized SVM.",
                    "label": 0
                },
                {
                    "sent": "You can see again in this case.",
                    "label": 0
                },
                {
                    "sent": "This is data sets from UCI that these these convex model based methods are doing pretty well.",
                    "label": 0
                },
                {
                    "sent": "In one class, one case VM wins, but in the others, that's the config.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tell me if it's doing well.",
                    "label": 0
                },
                {
                    "sent": "OK, so the summary of this part overall, this convex class approximation method seems to be going pretty well.",
                    "label": 0
                },
                {
                    "sent": "They're often comparable or better than SVM.",
                    "label": 1
                },
                {
                    "sent": "It's easy to add, remove and update your classes and the affine version of works surprisingly well, much better than you expect theoretically, although in practice the hyper disc one almost never does worse and sometimes does better, so it's a little bit more reliable as a choice.",
                    "label": 1
                },
                {
                    "sent": "The convex Hull methods are less reliable, sometimes it works well and sometimes not.",
                    "label": 0
                },
                {
                    "sent": "How many minutes do I have left?",
                    "label": 0
                },
                {
                    "sent": "2 minutes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, OK so I just wanted to give you a very quick overview of another method that we have which was published in CPR which uses this convex models to find discriminate directions.",
                    "label": 1
                },
                {
                    "sent": "So previously we had a very rigid model of the class.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to do is we're going to use these convex models to find project out some discriminate directions and then use nearest neighbor classifier or any other class low dimensional classifier that you want on the output.",
                    "label": 0
                },
                {
                    "sent": "So the main idea is that when you need to separate classes.",
                    "label": 0
                },
                {
                    "sent": "A good way to to find discriminate directions is to look for separations from your current example to the two other classes and find in some sense of weighted set of those directions that give you good separation, and in particular the waiting has to be something that focuses on the nearby classes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the basic thing.",
                    "label": 1
                },
                {
                    "sent": "We could also look at in sample separations, but we think they'll be too noisy.",
                    "label": 0
                },
                {
                    "sent": "This is the whole problem again, or we could look at into class separations into class model separations, but we think there would be too rigid, so we want the compromise of going.",
                    "label": 0
                },
                {
                    "sent": "Model 2.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample the way we're going to do this is we're going to take local discriminants, local lamp direction vectors from examples to the convex class, model unit vectors of those we're going to wait them with a decreasing function of distance.",
                    "label": 0
                },
                {
                    "sent": "We sum over all examples or classes.",
                    "label": 0
                },
                {
                    "sent": "That gives us a scatter matrix, which we will then be able to decompose with an S feet with eigen decomposition and the dominant few directions of that will give us our discriminate directions, and there waiting would choose is just the exponent.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here, OK, so we've applied it for example to a face recognition data set were just doing some normal.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Station here you could ignore that and you can see the results here.",
                    "label": 0
                },
                {
                    "sent": "So if this is a multiclass problem, so there's many directions that SVM projects out, and here is our set of.",
                    "label": 0
                },
                {
                    "sent": "Sort of convex model based things so you can see that we are on the same envelope is SVM, but we're much more flexible as to where we can lie on that envelope.",
                    "label": 0
                },
                {
                    "sent": "So all of those, all of our methods are using simple nearest neighbor in the output.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space here OK.",
                    "label": 0
                },
                {
                    "sent": "Similarly, Pasco Visual Object Challenge class F on or happy just projection and then nearest neighbors and you can see that again.",
                    "label": 0
                },
                {
                    "sent": "These things are doing better than SVM.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example OK again this UCI birds dot UIUC birds data set that we saw before.",
                    "label": 0
                },
                {
                    "sent": "Again these finding hyper disc projection methods are doing better than SVM.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's just a summary of the second part, using distances to convex class model seems to be a good heuristic.",
                    "label": 1
                },
                {
                    "sent": "Finding discriminant projection directions and hyper disk enough on how models again seemed to be equal best and this gives nearest neighbor comparable performance to SVM OK questions.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The SVM, well, the most of these films.",
                    "label": 0
                },
                {
                    "sent": "They were linear, so they don't have kernels in them.",
                    "label": 0
                },
                {
                    "sent": "When I used a linear method development that I also used a linear SVM and when I just curl method I used kernel SVM.",
                    "label": 0
                },
                {
                    "sent": "The kernel once we always use Gaussian kernels, we use the same kernel for all models.",
                    "label": 0
                },
                {
                    "sent": "So they will have the same things to tune basically.",
                    "label": 0
                },
                {
                    "sent": "Did you try simply modeling these transferred over the Gaussian?",
                    "label": 0
                },
                {
                    "sent": "Because hyper viscous.",
                    "label": 0
                },
                {
                    "sent": "I was comparing to LDA in some of the slides, which is sort of that.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "It called passed at the same carreras.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You could do quadratic quadratic LDA, qda as well.",
                    "label": 0
                },
                {
                    "sent": "The philosophy is somewhat different.",
                    "label": 0
                },
                {
                    "sent": "The philosophy is to try to find kind of a model that's well.",
                    "label": 0
                },
                {
                    "sent": "First of all our models there, ellipsoids, and certain number of dimensions, but in the other dimensions, they're not Gaussian, they simply flat.",
                    "label": 0
                },
                {
                    "sent": "So we have singular covariance in that sense, and we're finding things that bound the examples.",
                    "label": 0
                },
                {
                    "sent": "We're not finding things that lie inside them, so those two things together.",
                    "label": 0
                },
                {
                    "sent": "I mean, The thing is, with the Gaussian, I don't know how you used in this geometric framework because you need to find the distance from the new example to it.",
                    "label": 0
                },
                {
                    "sent": "So if you're in the Gaussian frame it you use Mahalanobis distance.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're very much in the kind of distance learning type of framework, whereas we're not in the distance learning framework.",
                    "label": 0
                },
                {
                    "sent": "We are in a geometric type of framework.",
                    "label": 0
                },
                {
                    "sent": "We simply looking for distances to some geometric approximation to the class.",
                    "label": 0
                },
                {
                    "sent": "Yes, these things can be compared, of course.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Yes, go ahead.",
                    "label": 0
                },
                {
                    "sent": "We talked about kernel Ising it.",
                    "label": 0
                },
                {
                    "sent": "Yes, you actually experiment with that.",
                    "label": 0
                },
                {
                    "sent": "Yes, the last experiments I showed this.",
                    "label": 0
                },
                {
                    "sent": "Let me just do this.",
                    "label": 0
                },
                {
                    "sent": "So for the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this one.",
                    "label": 0
                },
                {
                    "sent": "These are kernelized experiments OK.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}