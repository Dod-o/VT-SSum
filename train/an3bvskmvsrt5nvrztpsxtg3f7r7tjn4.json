{
    "id": "an3bvskmvsrt5nvrztpsxtg3f7r7tjn4",
    "title": "Classification with Deep Invariant Scattering Networks",
    "info": {
        "author": [
            "St\u00e9phane Mallat, Applied Mathematics - CMAP, \u00c9cole Polytechnique"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nips2012_mallat_classification/",
    "segmentation": [
        [
            "Thank you for the opportunity to give a talk here, which I really appreciate highly, so I'll be speaking today.",
            "Of course about deep neural network, but it's also the result of a lot of work done by a small team of PhD students.",
            "Track him and then Joanne Breen, Aloha Stiff and Iran Walls burger.",
            "Now at a call normal superior."
        ],
        [
            "So I'd like to begin with an example.",
            "Example of high dimensional classification coming from image processing.",
            "So there is a very standard database of images called Caltech 101.",
            "You have 100 classes and this is just a few examples.",
            "5 classes anchors Joshua Tree, Beavers, lot's water Lily and you can see the different images which are within each of these classes.",
            "Now when you look at such a classification problem, so you are given an image, you have to find which class it belongs to, what's immediately striking and well known is the fact that because of this very high dimensionality problem, there is a huge variability within each class.",
            "I mean, a tree can have very different shape and anchor.",
            "The Beaver depends upon lightning position and so second thing Euclidean distances are completely meaningless.",
            "It's if you compute the distance between any of these two.",
            "Any two image it will give you know information whether they belong to the same class.",
            "Or they don't belong to the same class.",
            "Now what comes out of this is that obviously what you'll need to do is to find some kind of invariants within each of the class to kill this variability.",
            "But the problem of invariant is that you're killing information, so you have to make sure that these invariants are sufficiently informative to discriminate the different classes."
        ],
        [
            "So here we are within this problematic of representation of high dimensional signals.",
            "So you are initially in a high dimensional space and because of that, as I said, Euclidean distance is going to be essentially non informative about classification and the dream is to find some kind of representation computed by some kind of nonlinear map five such that within this representation space the distance simple Euclidean distance will give you.",
            "A similarity measure which is now meaningful and hopefully elements of his same class are going to become much closer.",
            "Now of course, if you are able to do that, then we know that classification then becomes relatively easy.",
            "There's many possible approach to classification.",
            "Local approach like nearest neighbors or more global approach such as hyperplane separation with SVM and so on, and basically the static.",
            "Tickle decision theorie about classification.",
            "In such a framework is relatively well set.",
            "I mean, there is constantly new results, but we understand what's happening and we understand the whole range that goes from local methods to global methods.",
            "Now the situation is completely different on the representation side.",
            "Basically it's or not.",
            "You have many very smart ideas which have been developed.",
            "You have engineering features to build such a representation.",
            "For example in audio it's called MFC.",
            "In image processing it would be called sift.",
            "Then you have somewhat more generic methods such as Gaussian mixture models, clustering algorithm, histogram ING.",
            "There is an image processing all these family of algorithms called bag of words.",
            "Which are using these tools and the deep neural network which more recently have obtained pretty remarkable success.",
            "So the key problem now is to try to understand how can one think of this issue of representation.",
            "We are in very high dimensional space.",
            "What are the tools?",
            "What are the underlying concept that will allow us to understand how to build such a good representation and why?",
            "For example, deep neural networks are of any good for doing so."
        ],
        [
            "I just think so.",
            "Let me briefly summarize the idea of deep neural networks, so there's these very beautiful ideas that have been developed during the last 1015 years.",
            "First with Geoff Hinton, which had the idea of saying, well, if you want to build the norm, that you probably need to stack up several layers, and the intuition is that that will allow you to build global structure with a yard equal approach and these global structure will basically probably build invariant.",
            "Structure over your classification problem.",
            "Now from there yet Luca had this next very important idea, which is to say well.",
            "But the problem is essentially invariant to translation.",
            "What's going to happen here is probably similar than what's going to happen here, so probably the filters of your neural network should be translation invariant.",
            "Translation invariant filters means you are going to use convolution.",
            "And here you have this global structure which is obtained by doing convolutions and then within each of the layer you apply a series of nonlinear transform rectification, contrast renormalization, and then there is this so called pooling stage, which is going basically to aggregate several features, hopefully to build some kind of invariant.",
            "So this is the basic transformation, and then it's repeated.",
            "And of course you have many free parameters, all the para meters that will define your filters and the para meters which will define the pulling and these parameters.",
            "You learn them.",
            "So initially the idea was to learn them with back propagation algorithms to optimize the classify the classification.",
            "So in the last layer basically you have more or less standard classification classifier such as radial basis functions or NS via.",
            "Now the problem of.",
            "Doing a back propagation is that you don't have so much supervised samples, and it often gets trapped in local minima."
        ],
        [
            "So there was a next very important idea, which is to say that you can in fact learn the parameters of the network with the almost supervised unsupervised approach.",
            "And that was this idea of autoencoders proposed by Hinton, which is to say, well basically the parameters of the filters.",
            "You can view them as a way to encode one layer from the next one.",
            "And this idea was refined by number of people such as Yoshua Bengio runs at odds, and so on.",
            "And what came out of it is the fact that sparsity appears to be very important to do that.",
            "So this idea of how to build the filters do a sparse encoding of one layer from the other.",
            "And if you do that for example on the first layer you almost systematically get filters which looks like wavelets and I'll come back about what is wave lights, but it looks like that.",
            "So then a lot of work has been done around these ideas, and more recently you had this huge network that was developed by layer runs at oh and the whole Google team.",
            "So that's a bit of a Google style research, huge network, huge amount of memory and amazing results.",
            "Amazing results with one billion variables.",
            "In fact they were able to show that on this last layer you have detectors which are learned in a completely unsupervised way.",
            "But looks like what so called grandmother cells.",
            "Name these cells which are very specialized and very environment.",
            "For example to detect face or detect bodies and they take different type of elements that will happen very often within your unsupervised database.",
            "And that became pretty popular.",
            "You had about two weeks ago.",
            "Very nice article from the New York Times announcing stunning results.",
            "And indeed it has some very good results on number of applications.",
            "But there is a basic question is why?",
            "I mean, why does it work?",
            "If you look at that, it's horribly complicated.",
            "You have a whole series of linear nonlinear algorithms with many para meters.",
            "And to make this thing work, you have to be pretty good in the area.",
            "That's the question I'd like to discuss why an in what sense by looking at these architecture, can we learn at the generic problem of high dimensional representations for classification?"
        ],
        [
            "OK, so the core issue is intraclass variability.",
            "You want to kill this intraclass variability, but the first thing is how to think about intraclass variability.",
            "What it is one way to think of intraclass variability is to look at the set of operators that is going to leave invariant your signal set, so if you look at all the operators that lives in variant, your signal set.",
            "That's a group that's called a symmetric group.",
            "So maybe maybe a comment.",
            "I until now I've seen a lot of things on groups and I always had this feeling that it was a somewhat pedantic way to describe things that are already working without needing any group theory, so I'm sure many of you shares this.",
            "What I would like is just to give me a chance and I would like to try to show that in fact it may be of some use.",
            "To understand intuitively what's happening behind these networks, OK, so let's look at the supervised learning.",
            "I mean to try to do such a thing.",
            "Learning a group is hopeless.",
            "A group is a huge thing.",
            "You have very few training samples.",
            "There's no way you're going to learn a group.",
            "OK, what about unsupervised learning and supervised learning has the advantage of having much more data.",
            "You can get data and YouTube as many images as you want, and maybe you could learn a group, but then it's not going to be the symmetric group that leaves invariant each of the class.",
            "The only thing that you know is the union of all classes, so you may be able to learn the symmetric group of the Union of all classes.",
            "Now this group, as you know, any group can be factorized.",
            "So maybe if you factorize this group you are going to get elements which will give you information about the structure of each group of each class.",
            "So maybe it's a reasonable way to try to attack the problem.",
            "That means you are learning about the world of signals, but by looking at the structure of the world of signals, you'll be able to better understand the structure of the different classes.",
            "Now, in order not to remain abstract, let me try to see what are these groups."
        ],
        [
            "So if you look at patterns, for example, digit recognition, of course digits are translated.",
            "But the digits are not just translated, they are deformed.",
            "So the groups that leave these things in violence is the group of translation.",
            "This is a very simple 2 dimensional group.",
            "The problems are deformations.",
            "Deformation corresponds to much, much more para meters.",
            "So you have basically an infinite dimensional group.",
            "If you look at textures while the texture, these are two identical textures.",
            "They correspond to, let's say risation of stationary processes stationary means.",
            "Invariant to translation.",
            "So again, you see the translation invariants.",
            "The problem is you also have the formation whenever texture is going to be map on the surface, it's going to be deformed, so you are not just going to handle translation, but you also need to handle deformations.",
            "And."
        ],
        [
            "Then you have many other sources of variability, for example rotation.",
            "The objects can be rotated, but not just rotated.",
            "You are going to also have deformations within your rotations and therefore you have this before morphism group scaling.",
            "If you look at an image you have a lot of scaling depending upon the distance, but the scale is also going to vary in space, so again you see appearing this issue of deformation, but now it's on the scaling group."
        ],
        [
            "Think of speech.",
            "So here's 2 sounds.",
            "These are the spectrogram in log scale time and frequency.",
            "So the first sound encyclopedias.",
            "Second one down encyclopedias OK.",
            "Same words pronounced by a man and a woman, and what do you see?",
            "What you see is first of all, that the spectrogram has moved in frequency because these are lower frequencies, the man voice, but also of course, it's not exactly time aligned.",
            "So here you have another group which is time and frequency translation, which is called the Heisenberg Group."
        ],
        [
            "Now if you look at it, let me just superimpose the two.",
            "You see how it moves, it's not."
        ],
        [
            "Translating in time and frequency, but it's deforming again, you see, again appearing translation, but deformations coming in.",
            "OK, So what do?"
        ],
        [
            "We have we have the fact that the signal variability has multiple codes.",
            "Of course you have translation, rotation, scaling, translation in frequencies.",
            "All these sources of variabilities are basically provided by the physics of the world around us.",
            "Now, the reason why the problem is much more difficult is that you have many, many other sources of variability.",
            "For example, you may pronounce a word with different possible accent.",
            "Different phonemes can be pronounced different way if you take for example, a chair.",
            "A chair can have many different shapes.",
            "So you have variation.",
            "You have structural variation of the objects, so all these many many different sources of variability will need to be learned because they are not given to you.",
            "By the physics, and therefore if you think in terms of group, what you see is that basically you variability can be factorized.",
            "You are going to have basically the physics and already you have pretty complex group coming in.",
            "But then you have all these groups which are completely unknown and that have to be learned basically.",
            "OK, so what's the relation with deep neural network?",
            "Well, if you look at the problem that way, the thing that you may think is, well, you have a cascade of group.",
            "So you are going to put layers and each layer is going to build an environment for each of the group.",
            "But it doesn't work quite that way, and that doesn't work.",
            "So what I'm?"
        ],
        [
            "To try to do is.",
            "Come from these basic principle idea and try to see how we very naturally arrives to a deep neural network structure.",
            "But to do that I'm going to go relatively progressively and first of all I'd like to try to understand how can we build an invariant when you have such a complex variability problem, which is not just for example translation, but all possible deformation.",
            "How can you do that without killing all the information?",
            "And what you're going to see is that to do that, in fact, you need a multilayer structure and something which looks very much like a deep neural networks with you'll see wavelengths.",
            "Then the next question that will be easier to address once we understand that is how do you cascade groups?",
            "How can you cascade different environments on different groups?",
            "And then we'll finish on the learning problem once we basically understand what we are after, will try to understand how can we learn and how come these very nice idea of sparse ento encoders indeed seems to work very well to learn these type of representation.",
            "And I'll give examples on images and audio on."
        ],
        [
            "Wait OK, so let me be."
        ],
        [
            "In with this idea of invariant representation.",
            "So let's think for example of translation.",
            "You have an image of pattern which is going to be translated, so when it translate you can see the image as a point in your in your space.",
            "When it translate, it's going to describe a whole.",
            "Line or a 2 dimensional surface which is the orbit within your space.",
            "So each of these points corresponds to the image translated.",
            "Now an invariant representation mean."
        ],
        [
            "That you are going to take all this orbit and transform it into a single point.",
            "Now the."
        ],
        [
            "Problem is that the formation, because the deformation is going to be a much bigger set of images.",
            "And what you want to do is to transform these things, not into an invariant representation.",
            "That would be very dangerous, because one can be deformed into a 7, so you don't want to be invariant to the formation you want to be stable to deformation and transform all that into a neighborhood of points near your original point.",
            "If you do that."
        ],
        [
            "Then you will be able to kill the formation at the supervised stage with a simple linear operator.",
            "If all your deform points are nearby, then."
        ],
        [
            "You can just project on the orthogonal space to your manifold, which is approximated by the tangent space and."
        ],
        [
            "But you can learn with your supervised learning, so the formation will be addressed with supervised learning.",
            "If you did well, the job having something which is stable to deformation."
        ],
        [
            "Suppose you have another pattern.",
            "Of course, you don't want it to be mapped to the same point, so you won't fight to be discriminants.",
            "Although."
        ],
        [
            "8 should be mapped."
        ],
        [
            "To the neighborhood of the Origonal.",
            "8 and."
        ],
        [
            "Your supervised stage, you can learn the invariant for the specific date."
        ],
        [
            "Formation of the 8."
        ],
        [
            "By projecting it."
        ],
        [
            "And then of course discrimination is very easy.",
            "Question how can you do that?",
            "How can you?"
        ],
        [
            "Build such a file."
        ],
        [
            "OK, so let's look at the case of translation.",
            "You have a translated signal and you want that the representation of the translated signal should be the representation of the original signal.",
            "For example, these are two translated signals.",
            "You can just make."
        ],
        [
            "A simple registration.",
            "A signal, another approach.",
            "You can compute the four."
        ],
        [
            "Transform which is going to be transformed by phase and kill the faith.",
            "So that's very easy.",
            "The problem is when you have a deformation."
        ],
        [
            "If now your signal is deformed, what you would like?"
        ],
        [
            "Is that the representation of the deform signal should be close to the representation of the original signal.",
            "If the deformation is small, that's the key point, so it should be of the order of this size of the deformation.",
            "Now suppose."
        ],
        [
            "That you do a registration.",
            "If you had a formation, you maybe align one peak, but the others are not going to be aligned.",
            "The distance distance is going to be very large.",
            "Registration is not stable to deformation."
        ],
        [
            "If you make a Fourier transform, same problem, high frequencies are going to move.",
            "It doesn't work.",
            "So we are facing this strange problem which is there is a lot of math on environment groups, but there is no standard tools to build something which is invariant.",
            "Sorry, stable today."
        ],
        [
            "Nation.",
            "OK, what I propose here is to replace the sine wave, which are delocalized of the Fourier transform with something localized to be stable today formation.",
            "And these are the wavelengths.",
            "So wave light will be here complex function with a real imaginary part with quadratic phase.",
            "Then you take your wavelet and you scale it with all possible scale here.",
            "And in the for your domain, a wavelet is basically a bandpass filter.",
            "So when you scale it you cover different frequency bands over there.",
            "And the low frequencies are covered by a low frequency filter."
        ],
        [
            "OK, So what do you do?",
            "You take your signal and you filter it simple convolution with your wavelength.",
            "So basically you decompose it in different frequency bands.",
            "So the way they transform is just a bunch of convolutions with different filters, different position.",
            "You have a vector of coefficients.",
            "And if you design well, your wave let's this vector is unitary.",
            "The norm is the same as the norm of your original signal.",
            "OK, so why wave lights?"
        ],
        [
            "There will be 2 reasons.",
            "One, the wavelet dictionary, the set of all wavelet translated is environed by translation, that's good.",
            "But more important now, if you deform a wavelength, you're going to get a new wavelet, which is going to be very close to the original one if.",
            "The deformation was small.",
            "And this idea is going to be important because generalizing it will understand.",
            "Allow us to understand how do you learn."
        ],
        [
            "OK, in 2D, how do you build a wavelet?",
            "Same thing but time is now the two variable T1T2 you have a complex wave let in 2D we are going to rotate the wave light.",
            "So you see here the different wavelets scaled and they're all rotated or possible rotation, real part, imaginary parts.",
            "And the wavelet transform is just a bunch of convolutions with each of these wavelets, real and imaginary parts, low frequency, and you preserve the norm.",
            "So that's a very simple and classical tool."
        ],
        [
            "OK, now how do you build an environment?",
            "Take your signal.",
            "Make a convolution with the wave, let you have your real and imaginary part.",
            "You have something which is oscillating.",
            "If you want to kill the translation, one way is to do like the Fourier transform, kill the phase which is sensitive to translation."
        ],
        [
            "If you do that, that's what Jen Luker would be calling a pooling, so you just get the envelope.",
            "You get the regular ambulance so it's not inviting by translation, but just to very smell more translations.",
            "If you want to build something much more environmental."
        ],
        [
            "Translation one way to do it is you just take this function and now you average it.",
            "With a window file.",
            "If you do that now, you're going to get something much more regular, which is going to become to be invited to translation, which are relatively small, relatively to your window.",
            "Now let's your window increase in size.",
            "If the window fire increase in size and converge to one.",
            "This is basically going to converge to the integral of the wavelet transform modulus.",
            "So the L1 norm.",
            "This is completely invariant to translation.",
            "And we'll see stable today.",
            "Formation great.",
            "But you've lost a lot of information.",
            "You had a function.",
            "Now you just have a bunch of L1 norm, so all the information almost has disappeared.",
            "So the question is."
        ],
        [
            "How can you recover this lost information?",
            "OK, the problem is you've taken your modulus an you averaged it.",
            "So you lost all the low frequencies.",
            "How to recover?",
            "Sorry to all the high frequencies you've done.",
            "A low frequency filtering.",
            "How to recover them?",
            "Well, the low frequency, that's just the first component of a wavelet transform.",
            "The high frequencies.",
            "You can recover them by getting the wavelet coefficients of this modelers OK.",
            "These are the high frequencies, but the high frequencies are not invariant to translation.",
            "How can you make them invariant to translation?",
            "Kill the face.",
            "An average, you just do the same thing Hilda Phase and you average and now you have a whole bunch of environments for any wavelength Lambda one any Lambda to.",
            "These are a whole set of invariants.",
            "What are we doing?",
            "We are."
        ],
        [
            "Implementing here a deep convolution network.",
            "We begin from X."
        ],
        [
            "Get the first environments.",
            "The average over there and then all the wavelet coefficients, which are the high frequencies.",
            "Take the modelers to make them environment now."
        ],
        [
            "Now the next layer of environment is here.",
            "What have you lost?",
            "The high frequencies which are here.",
            "Make these high frequency invariants."
        ],
        [
            "You average them.",
            "What have you lost the next week?",
            "It's.",
            "And then suddenly you see appearing this norm.",
            "That's deep neural net.",
            "It's not because you have many groups, you just have one group.",
            "But it's because you want to recover the lost information each time you've implemented your environment."
        ],
        [
            "So that's the network output.",
            "It's a whole set of coefficients that will be called the scattering transform."
        ],
        [
            "The first layer are just the standard MFC C or sip coefficient.",
            "The next layers will be.",
            "The compliment will look at them."
        ],
        [
            "OK, so let's look at an example you have here.",
            "The spectrogram of his sound.",
            "Now you have a tremolo vibrato OK."
        ],
        [
            "First layer of coefficients because you are averaging.",
            "What are you going to get?",
            "You are going to get the basic frequency distribution of coefficients, but the averaging has killed all the difference between these different nodes, which are very different when you hear them.",
            "Because you averaged.",
            "Where is this information you?"
        ],
        [
            "Look at the second layer of coefficients, which gives you the time variation of these and you see the attack here, which was very different, appears because you have a lot of high frequencies in the second layer, the tremolo appears with this high frequency element.",
            "Vibrato appears through this spectrum, so the second layer will give you the information which differentiate these nodes, let's look."
        ],
        [
            "And image these two images are two textures.",
            "They have exactly the same 2nd order moments.",
            "If you look at them through there's power spectrum.",
            "This power spectrum is absolutely identical because they have same 2nd order moment.",
            "If you look at the 1st."
        ],
        [
            "If your network, these are the wavelet coefficient, it basically gives you the same information.",
            "Then the power spectrum, but a little bit average, so no difference.",
            "If you look at the."
        ],
        [
            "Can layer they are very different.",
            "These are all the second layer coefficients and because this is much more sparse than this, the coefficient have a very different distribution.",
            "So you see it being the fact that these second layers and next gives a lot of information."
        ],
        [
            "OK, can we understand what's behind all that and can we analyze the properties while in fact we are doing something simple?",
            "What are we doing?",
            "We are taking the signal taking a simple unit unitary transform.",
            "The way they transform which is unitary and the only thing that we."
        ],
        [
            "Changing is where taking the absolute value.",
            "So now we have a non linear operator but it remains contractive because the absolute value is contracted.",
            "And it keeps the normal and then we are just iterating on this operator."
        ],
        [
            "We are taking the average.",
            "Each of these coefficients.",
            "We make it environment and that's the residue.",
            "The next layer we reapply the operator, we get the next layer of environment output and inside node, which is the next layer and we reapply and.",
            "So that's how you build your network.",
            "Now, one very important property you can prove that this is going to converge to 0.",
            "What does that mean?",
            "All the information is going to get out in the output.",
            "So that's summarized."
        ],
        [
            "By the following theorem, which says if you look at your scattering vector, the output of your network first of all you have something contractive.",
            "If you look at this as a vector, the difference if you take a signal X&Y between these two vectors will be smaller than X -- Y because it's implemented as a cascade of contractive operators.",
            "The second thing is, all the energy will get out.",
            "The norm is the same as the norm of the original signal.",
            "Because you've implemented that with something which preserves the norm and it converge to 0.",
            "But much more important, you are stable to the formation.",
            "If you know your signal is deformed, the representation is going to be very close if.",
            "The deformation is small."
        ],
        [
            "OK, but have you lost in form?"
        ],
        [
            "It's not because you keep the norm that you haven't lost information.",
            "What did you do?",
            "You iterated on this operator and this operator kills the phase.",
            "Are you losing information?",
            "But there is a very nice result of Iran Wildberger which shows that no, this operator is invertible.",
            "So if you take this signal, you have the modulus of the wavelet transform.",
            "You can recover exactly the original signal.",
            "You can invert it."
        ],
        [
            "What does that mean?",
            "That means that now you can try to reconstruct by inverting.",
            "This operator between each layer.",
            "Problem."
        ],
        [
            "You don't have all the layers at one point, you will stop.",
            "I will show you reconstruction from the two layers, so you've lost all the rest.",
            "You have an error here and this area is going to propagate.",
            "How bad is that going to be?",
            "Let me show you."
        ],
        [
            "Example.",
            "Reconstruction just from the."
        ],
        [
            "Slayer.",
            "So you see this spectrum slowly varying because the window where 3 seconds OK reconstruction from.",
            "If you add the second layer."
        ],
        [
            "You have a huge amount of information in second layer.",
            "That's what gives you basically the transient structure.",
            "That's why you do need to have these multilayer structures."
        ],
        [
            "OK, so classification there is number of very nice things done by Jean Bruno.",
            "So basically the idea is now you have a representation which is translation invariant and stable to deformation.",
            "The deformations are going to be studied at the supervised stage and killed for example with a simple SVM.",
            "So that's where you will learn the linear operator that kills the deformation.",
            "And you can look at the result so it's interesting to compare with convolution network on amnesty.",
            "The state of the art was hauled by the team of yellow can with convolution networks, and that's what we're getting with the scattering.",
            "Basically, we do improve without learning anything besides this simple SVM.",
            "So what's the interpretation?",
            "There's two ways to think about it.",
            "One, well, you have something much faster and you get better results because you know in advance what to learn so you don't need to learn.",
            "You know that what you need here are wave.",
            "Let's let's put the wavelets and use them.",
            "On the other hand, you can think of it differently.",
            "They didn't impose any filters, and they were able to learn the filters, and in fact they got very good result.",
            "So that shows that learning is working here.",
            "In the case you know where to get, it seems that you do get that.",
            "Although of course you'd better get it directly when you know."
        ],
        [
            "But it's.",
            "To show that the learning does work.",
            "Now if you take a second type of problem textures.",
            "So this is a well known database in Berkeley, curate with 61 classes of textures.",
            "This is 3 classes and exactly same thing.",
            "You build a representation of your texture and then your supervised learning state of the art was basically obtained by fully spectral man features with histogram errors of about one person.",
            "There are textures that you can't discriminate with the power spectrum.",
            "If you do it with a scattering, there is a big improvement.",
            "Why?",
            "Because now suddenly you can discriminate textures having exactly the same 2nd order moment.",
            "So basically it also as a side effect gives you different way to look at stationary processes and classify them."
        ],
        [
            "OK, now we basically understand how to deal with one environment.",
            "How do you put a second invite?",
            "A second group?",
            "One first idea that one you may think is, let's kill the first group.",
            "First environment and then let's kill the second group.",
            "For example translation environments, then rotation invariant.",
            "It doesn't work.",
            "Now why it doesn't work?",
            "Take these two images 2 textures.",
            "There are totally different.",
            "Now if you make a translation invariant 1st and then a rotation invariants, they will be exactly having the same representation because they differ they have the same distribution of orientation, it's just that the position is different.",
            "It's the joint position between rotation and translation.",
            "So what you need to do is to keep the joint information."
        ],
        [
            "So you cannot deal with this group by separating into two independent components.",
            "You really need to deal with the full roto Translation Group, which includes a rotation and the translation.",
            "So if you have a rotation translation acting on the signal, it means you take your signal, translate it, and rotate it OK."
        ],
        [
            "Now what is the group multiplication?",
            "Well, if you do a first rotation translation, a second rotation translation.",
            "The rotation will be the product of the two rotation, but rotation will act on translation, so it's not commutative.",
            "It's a little bit more difficult."
        ],
        [
            "How can you make an environment?",
            "Well, it's the same thing.",
            "You make a convolution, you just average over the group.",
            "How do you do an averaging?",
            "So if you want to do an ad."
        ],
        [
            "Charging in time to do translation invariants, you make a convolution your signal average with your window translated.",
            "Well, if you deal with another group, it's the same."
        ],
        [
            "You take your signal, which is a function of rotation translation and you average it with a window which is translated over the group.",
            "That's a convolution, it's just a convolution on a different group."
        ],
        [
            "How do you do wavelet transform to recover the lost information while you are going to define a wavelength on the group?",
            "Ann, you're going to make a wavelet transform.",
            "Basically, these are the averaging you've lost the high frequencies and the high frequencies you'll recover them with your wavelengths.",
            "It's exactly the same idea.",
            "And you can have a unitary transform.",
            "You need now to have an invariant.",
            "How are you?"
        ],
        [
            "To build an invariant kelda phase because the phases where you encode the local translation.",
            "So you just take the modulus of this wavelet transform.",
            "Computed over this group with the convolution on the group."
        ],
        [
            "OK. Let's begin with the signal.",
            "First layer translation invariants.",
            "So you have your translation wavelet convolution.",
            "You make the environment by averaging, and that's the last information.",
            "Now this last information is a function of rotation because the wavelength was rotated.",
            "Scale and time.",
            "Let's consider it as a signal as a function of rotation in time.",
            "If you want something which is rotation, invite."
        ],
        [
            "Let's just make.",
            "You apply this wavelet transform modulus operator, make a convolution on the group.",
            "Average on the group and you get an invariant to rotation and translation, so it's the same idea, but now you add one other layer which corresponds to the rotation and to the translation."
        ],
        [
            "What if you want to do that with scale?",
            "Same thing, but now you are also going to do a convolution relatively to the scale.",
            "Here you will need also renormalization.",
            "I will come back to that when I look at the very general problem.",
            "So the idea is you just cascade convolution is exactly as gender can thought, but you just change your definition of convolution depending upon the type of environments you want to do."
        ],
        [
            "OK, let's look for example data classification problem of textures.",
            "This is a database where the textures are also deformed, rotated, translated, of course, but also scaling.",
            "If you just have a translation invariant network, 20% affairs if you add up rotation 3% if you add up scaling environments, you go back to zero point 6%, which just shows.",
            "Once you have your invariant classification is indeed really easy.",
            "OK."
        ],
        [
            "The problem is what invites and in this simple example that I gave from physics you know them, but what about learning them?",
            "So basically we are in a situation where we have few groups that we know and few groups that you need to learn OK.",
            "The question is how can you learn a group?",
            "Now a group is nice because it has a very global structure.",
            "The group is entirely defined by a simpler thing, which is a Lie algebra.",
            "What is the Lee algebra?",
            "A group is a series of operators which moves your signal well.",
            "You can look at the Infinity symbol movement and the infinitesimal movement is carried by a simple linear space, which is called the linear algebra OK, and that's what entirely specifies the group.",
            "So, for example, if you have a translation.",
            "That the algebra is just a 1D linear space which corresponds to the derivative.",
            "The derivative operator, because a translation can be obtained by series of increments.",
            "So basically series of derivatives.",
            "If the group includes all possible deformation, then the larger prize much more complicated because it's infinite dimensional.",
            "You have the multiplication by NEC one function.",
            "OK."
        ],
        [
            "So how to think about learning one key element will be don't learn the groups.",
            "What you need is to learn the parts exactly like in the bag of words.",
            "Ideas OK, what is apart apart is like a center of symmetry is a family of functions which are stable.",
            "If you apply your group transformation.",
            "So if you apply an element of the algebra, basically the function is not going to change, it's just going to be multiplied by a constant.",
            "So it's something which.",
            "Most doesn't change when you apply your group operator.",
            "What does that mean?",
            "That means that if you look at your element of your algebra and you represent it on your family or function, you have a very sparse matrix.",
            "Basically not diagonal, but a band matrix."
        ],
        [
            "OK, why is that useful?",
            "Because once you have that, you can build an environment.",
            "Why?",
            "Because if you have a signal which is transformed by your element, actually algebra and you represent it with your pseudo wavelength, which I called here, this table part, it's just going to be changed by a multiplicative factor.",
            "So to get an invariant you just kill the phase and you re normalize.",
            "And that's not just true for translation and so on.",
            "It's going to be true for any kind of variations.",
            "So you can think.",
            "Of these network filters as just being the stable parts which almost diagonalize your algebra."
        ],
        [
            "Now wavelets are exactly that.",
            "They're just a particular case if you consider the problem of translation and deformation, there are the stable parts of your group."
        ],
        [
            "Now the nice thing is you don't have to learn the groups.",
            "In fact it would be unfeasible because the groups is much too big.",
            "You just have to learn these center of symmetry of you.",
            "You prefer these table parts exactly a little bit like these people are sinking in this bag of words algorithms suppose and how to do that, and that's where you see this ad ideas of sparse autoencoders.",
            "Suppose you have a way to represent your signal set in a very sparse way.",
            "OK, I'm not dealing with groups, just representing the signal set in a sparse way.",
            "That means that your signal is going to represent it by very few non zero inner product.",
            "OK and if you change your signal that's still in your signal set.",
            "It's very sparse.",
            "What does that mean?"
        ],
        [
            "It means that the operator which goes from here to here has to be sparse, otherwise this one wouldn't be sparse.",
            "So that means that too sparse.",
            "If I to find the sparse elements of the group of the group operation, you just have to specify the signal.",
            "And that means."
        ],
        [
            "But it's a very subtle strategy to find the invites, you first find a way to make your signal sparse.",
            "Indirectly.",
            "You found a way to represent efficiently your variability, and then the environment you just build it by killing the phase and renormalizing, which is exactly what is done by these deep neural networks.",
            "So that's the way you can see it, and one question is maybe this is basically a very generic approach for understanding representation in general of very high dam."
        ],
        [
            "Social spaces.",
            "So conclusion, one way basically to see all that is really to think of learning as a way to reduce intra variability.",
            "Once you think of it that way, the notion of variability is related in a way or another, to the notion of group, and then it's really an issue of learning the group and learning a way to build an environment.",
            "The problem again, the groups are not so simple because they're not just simple translation rotation, they're very big.",
            "You have all the deformations and one of the key thing is that to build an environment of these very complex groups, you need these stable parts.",
            "It happens that the stable parts are wavelets for translation or roto translation, but you can define this table parts in a very generic weight and what are going to be the stable part?",
            "That's what you are going to observe very often in your signals precisely because they are stable.",
            "They are going to remain there.",
            "Once you found these tables, basically the conjecture is that you can learn them with sparsity.",
            "Theta conjecture because mathematics are not done, but I think it's one of the way to think of why sparsity is so important and why these structures are getting such beautiful results.",
            "So they are all kind of.",
            "Results around these things.",
            "One thing is if anybody is interested in that kind of research we are looking for postdoc.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the opportunity to give a talk here, which I really appreciate highly, so I'll be speaking today.",
                    "label": 0
                },
                {
                    "sent": "Of course about deep neural network, but it's also the result of a lot of work done by a small team of PhD students.",
                    "label": 0
                },
                {
                    "sent": "Track him and then Joanne Breen, Aloha Stiff and Iran Walls burger.",
                    "label": 0
                },
                {
                    "sent": "Now at a call normal superior.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'd like to begin with an example.",
                    "label": 0
                },
                {
                    "sent": "Example of high dimensional classification coming from image processing.",
                    "label": 1
                },
                {
                    "sent": "So there is a very standard database of images called Caltech 101.",
                    "label": 0
                },
                {
                    "sent": "You have 100 classes and this is just a few examples.",
                    "label": 0
                },
                {
                    "sent": "5 classes anchors Joshua Tree, Beavers, lot's water Lily and you can see the different images which are within each of these classes.",
                    "label": 0
                },
                {
                    "sent": "Now when you look at such a classification problem, so you are given an image, you have to find which class it belongs to, what's immediately striking and well known is the fact that because of this very high dimensionality problem, there is a huge variability within each class.",
                    "label": 0
                },
                {
                    "sent": "I mean, a tree can have very different shape and anchor.",
                    "label": 1
                },
                {
                    "sent": "The Beaver depends upon lightning position and so second thing Euclidean distances are completely meaningless.",
                    "label": 0
                },
                {
                    "sent": "It's if you compute the distance between any of these two.",
                    "label": 0
                },
                {
                    "sent": "Any two image it will give you know information whether they belong to the same class.",
                    "label": 0
                },
                {
                    "sent": "Or they don't belong to the same class.",
                    "label": 0
                },
                {
                    "sent": "Now what comes out of this is that obviously what you'll need to do is to find some kind of invariants within each of the class to kill this variability.",
                    "label": 0
                },
                {
                    "sent": "But the problem of invariant is that you're killing information, so you have to make sure that these invariants are sufficiently informative to discriminate the different classes.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we are within this problematic of representation of high dimensional signals.",
                    "label": 0
                },
                {
                    "sent": "So you are initially in a high dimensional space and because of that, as I said, Euclidean distance is going to be essentially non informative about classification and the dream is to find some kind of representation computed by some kind of nonlinear map five such that within this representation space the distance simple Euclidean distance will give you.",
                    "label": 0
                },
                {
                    "sent": "A similarity measure which is now meaningful and hopefully elements of his same class are going to become much closer.",
                    "label": 0
                },
                {
                    "sent": "Now of course, if you are able to do that, then we know that classification then becomes relatively easy.",
                    "label": 0
                },
                {
                    "sent": "There's many possible approach to classification.",
                    "label": 0
                },
                {
                    "sent": "Local approach like nearest neighbors or more global approach such as hyperplane separation with SVM and so on, and basically the static.",
                    "label": 0
                },
                {
                    "sent": "Tickle decision theorie about classification.",
                    "label": 0
                },
                {
                    "sent": "In such a framework is relatively well set.",
                    "label": 0
                },
                {
                    "sent": "I mean, there is constantly new results, but we understand what's happening and we understand the whole range that goes from local methods to global methods.",
                    "label": 0
                },
                {
                    "sent": "Now the situation is completely different on the representation side.",
                    "label": 0
                },
                {
                    "sent": "Basically it's or not.",
                    "label": 0
                },
                {
                    "sent": "You have many very smart ideas which have been developed.",
                    "label": 0
                },
                {
                    "sent": "You have engineering features to build such a representation.",
                    "label": 1
                },
                {
                    "sent": "For example in audio it's called MFC.",
                    "label": 0
                },
                {
                    "sent": "In image processing it would be called sift.",
                    "label": 0
                },
                {
                    "sent": "Then you have somewhat more generic methods such as Gaussian mixture models, clustering algorithm, histogram ING.",
                    "label": 0
                },
                {
                    "sent": "There is an image processing all these family of algorithms called bag of words.",
                    "label": 1
                },
                {
                    "sent": "Which are using these tools and the deep neural network which more recently have obtained pretty remarkable success.",
                    "label": 0
                },
                {
                    "sent": "So the key problem now is to try to understand how can one think of this issue of representation.",
                    "label": 1
                },
                {
                    "sent": "We are in very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "What are the tools?",
                    "label": 0
                },
                {
                    "sent": "What are the underlying concept that will allow us to understand how to build such a good representation and why?",
                    "label": 0
                },
                {
                    "sent": "For example, deep neural networks are of any good for doing so.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just think so.",
                    "label": 0
                },
                {
                    "sent": "Let me briefly summarize the idea of deep neural networks, so there's these very beautiful ideas that have been developed during the last 1015 years.",
                    "label": 0
                },
                {
                    "sent": "First with Geoff Hinton, which had the idea of saying, well, if you want to build the norm, that you probably need to stack up several layers, and the intuition is that that will allow you to build global structure with a yard equal approach and these global structure will basically probably build invariant.",
                    "label": 0
                },
                {
                    "sent": "Structure over your classification problem.",
                    "label": 0
                },
                {
                    "sent": "Now from there yet Luca had this next very important idea, which is to say well.",
                    "label": 0
                },
                {
                    "sent": "But the problem is essentially invariant to translation.",
                    "label": 0
                },
                {
                    "sent": "What's going to happen here is probably similar than what's going to happen here, so probably the filters of your neural network should be translation invariant.",
                    "label": 0
                },
                {
                    "sent": "Translation invariant filters means you are going to use convolution.",
                    "label": 0
                },
                {
                    "sent": "And here you have this global structure which is obtained by doing convolutions and then within each of the layer you apply a series of nonlinear transform rectification, contrast renormalization, and then there is this so called pooling stage, which is going basically to aggregate several features, hopefully to build some kind of invariant.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic transformation, and then it's repeated.",
                    "label": 0
                },
                {
                    "sent": "And of course you have many free parameters, all the para meters that will define your filters and the para meters which will define the pulling and these parameters.",
                    "label": 0
                },
                {
                    "sent": "You learn them.",
                    "label": 0
                },
                {
                    "sent": "So initially the idea was to learn them with back propagation algorithms to optimize the classify the classification.",
                    "label": 0
                },
                {
                    "sent": "So in the last layer basically you have more or less standard classification classifier such as radial basis functions or NS via.",
                    "label": 0
                },
                {
                    "sent": "Now the problem of.",
                    "label": 0
                },
                {
                    "sent": "Doing a back propagation is that you don't have so much supervised samples, and it often gets trapped in local minima.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there was a next very important idea, which is to say that you can in fact learn the parameters of the network with the almost supervised unsupervised approach.",
                    "label": 0
                },
                {
                    "sent": "And that was this idea of autoencoders proposed by Hinton, which is to say, well basically the parameters of the filters.",
                    "label": 0
                },
                {
                    "sent": "You can view them as a way to encode one layer from the next one.",
                    "label": 0
                },
                {
                    "sent": "And this idea was refined by number of people such as Yoshua Bengio runs at odds, and so on.",
                    "label": 0
                },
                {
                    "sent": "And what came out of it is the fact that sparsity appears to be very important to do that.",
                    "label": 0
                },
                {
                    "sent": "So this idea of how to build the filters do a sparse encoding of one layer from the other.",
                    "label": 0
                },
                {
                    "sent": "And if you do that for example on the first layer you almost systematically get filters which looks like wavelets and I'll come back about what is wave lights, but it looks like that.",
                    "label": 0
                },
                {
                    "sent": "So then a lot of work has been done around these ideas, and more recently you had this huge network that was developed by layer runs at oh and the whole Google team.",
                    "label": 0
                },
                {
                    "sent": "So that's a bit of a Google style research, huge network, huge amount of memory and amazing results.",
                    "label": 0
                },
                {
                    "sent": "Amazing results with one billion variables.",
                    "label": 0
                },
                {
                    "sent": "In fact they were able to show that on this last layer you have detectors which are learned in a completely unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "But looks like what so called grandmother cells.",
                    "label": 0
                },
                {
                    "sent": "Name these cells which are very specialized and very environment.",
                    "label": 0
                },
                {
                    "sent": "For example to detect face or detect bodies and they take different type of elements that will happen very often within your unsupervised database.",
                    "label": 0
                },
                {
                    "sent": "And that became pretty popular.",
                    "label": 0
                },
                {
                    "sent": "You had about two weeks ago.",
                    "label": 0
                },
                {
                    "sent": "Very nice article from the New York Times announcing stunning results.",
                    "label": 0
                },
                {
                    "sent": "And indeed it has some very good results on number of applications.",
                    "label": 0
                },
                {
                    "sent": "But there is a basic question is why?",
                    "label": 0
                },
                {
                    "sent": "I mean, why does it work?",
                    "label": 0
                },
                {
                    "sent": "If you look at that, it's horribly complicated.",
                    "label": 0
                },
                {
                    "sent": "You have a whole series of linear nonlinear algorithms with many para meters.",
                    "label": 0
                },
                {
                    "sent": "And to make this thing work, you have to be pretty good in the area.",
                    "label": 0
                },
                {
                    "sent": "That's the question I'd like to discuss why an in what sense by looking at these architecture, can we learn at the generic problem of high dimensional representations for classification?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the core issue is intraclass variability.",
                    "label": 1
                },
                {
                    "sent": "You want to kill this intraclass variability, but the first thing is how to think about intraclass variability.",
                    "label": 0
                },
                {
                    "sent": "What it is one way to think of intraclass variability is to look at the set of operators that is going to leave invariant your signal set, so if you look at all the operators that lives in variant, your signal set.",
                    "label": 0
                },
                {
                    "sent": "That's a group that's called a symmetric group.",
                    "label": 0
                },
                {
                    "sent": "So maybe maybe a comment.",
                    "label": 0
                },
                {
                    "sent": "I until now I've seen a lot of things on groups and I always had this feeling that it was a somewhat pedantic way to describe things that are already working without needing any group theory, so I'm sure many of you shares this.",
                    "label": 0
                },
                {
                    "sent": "What I would like is just to give me a chance and I would like to try to show that in fact it may be of some use.",
                    "label": 0
                },
                {
                    "sent": "To understand intuitively what's happening behind these networks, OK, so let's look at the supervised learning.",
                    "label": 0
                },
                {
                    "sent": "I mean to try to do such a thing.",
                    "label": 0
                },
                {
                    "sent": "Learning a group is hopeless.",
                    "label": 0
                },
                {
                    "sent": "A group is a huge thing.",
                    "label": 0
                },
                {
                    "sent": "You have very few training samples.",
                    "label": 1
                },
                {
                    "sent": "There's no way you're going to learn a group.",
                    "label": 0
                },
                {
                    "sent": "OK, what about unsupervised learning and supervised learning has the advantage of having much more data.",
                    "label": 0
                },
                {
                    "sent": "You can get data and YouTube as many images as you want, and maybe you could learn a group, but then it's not going to be the symmetric group that leaves invariant each of the class.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you know is the union of all classes, so you may be able to learn the symmetric group of the Union of all classes.",
                    "label": 1
                },
                {
                    "sent": "Now this group, as you know, any group can be factorized.",
                    "label": 0
                },
                {
                    "sent": "So maybe if you factorize this group you are going to get elements which will give you information about the structure of each group of each class.",
                    "label": 0
                },
                {
                    "sent": "So maybe it's a reasonable way to try to attack the problem.",
                    "label": 0
                },
                {
                    "sent": "That means you are learning about the world of signals, but by looking at the structure of the world of signals, you'll be able to better understand the structure of the different classes.",
                    "label": 0
                },
                {
                    "sent": "Now, in order not to remain abstract, let me try to see what are these groups.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you look at patterns, for example, digit recognition, of course digits are translated.",
                    "label": 1
                },
                {
                    "sent": "But the digits are not just translated, they are deformed.",
                    "label": 0
                },
                {
                    "sent": "So the groups that leave these things in violence is the group of translation.",
                    "label": 0
                },
                {
                    "sent": "This is a very simple 2 dimensional group.",
                    "label": 0
                },
                {
                    "sent": "The problems are deformations.",
                    "label": 0
                },
                {
                    "sent": "Deformation corresponds to much, much more para meters.",
                    "label": 0
                },
                {
                    "sent": "So you have basically an infinite dimensional group.",
                    "label": 0
                },
                {
                    "sent": "If you look at textures while the texture, these are two identical textures.",
                    "label": 0
                },
                {
                    "sent": "They correspond to, let's say risation of stationary processes stationary means.",
                    "label": 0
                },
                {
                    "sent": "Invariant to translation.",
                    "label": 0
                },
                {
                    "sent": "So again, you see the translation invariants.",
                    "label": 0
                },
                {
                    "sent": "The problem is you also have the formation whenever texture is going to be map on the surface, it's going to be deformed, so you are not just going to handle translation, but you also need to handle deformations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you have many other sources of variability, for example rotation.",
                    "label": 0
                },
                {
                    "sent": "The objects can be rotated, but not just rotated.",
                    "label": 0
                },
                {
                    "sent": "You are going to also have deformations within your rotations and therefore you have this before morphism group scaling.",
                    "label": 0
                },
                {
                    "sent": "If you look at an image you have a lot of scaling depending upon the distance, but the scale is also going to vary in space, so again you see appearing this issue of deformation, but now it's on the scaling group.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think of speech.",
                    "label": 0
                },
                {
                    "sent": "So here's 2 sounds.",
                    "label": 0
                },
                {
                    "sent": "These are the spectrogram in log scale time and frequency.",
                    "label": 0
                },
                {
                    "sent": "So the first sound encyclopedias.",
                    "label": 0
                },
                {
                    "sent": "Second one down encyclopedias OK.",
                    "label": 0
                },
                {
                    "sent": "Same words pronounced by a man and a woman, and what do you see?",
                    "label": 0
                },
                {
                    "sent": "What you see is first of all, that the spectrogram has moved in frequency because these are lower frequencies, the man voice, but also of course, it's not exactly time aligned.",
                    "label": 0
                },
                {
                    "sent": "So here you have another group which is time and frequency translation, which is called the Heisenberg Group.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now if you look at it, let me just superimpose the two.",
                    "label": 0
                },
                {
                    "sent": "You see how it moves, it's not.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Translating in time and frequency, but it's deforming again, you see, again appearing translation, but deformations coming in.",
                    "label": 0
                },
                {
                    "sent": "OK, So what do?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have we have the fact that the signal variability has multiple codes.",
                    "label": 1
                },
                {
                    "sent": "Of course you have translation, rotation, scaling, translation in frequencies.",
                    "label": 0
                },
                {
                    "sent": "All these sources of variabilities are basically provided by the physics of the world around us.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason why the problem is much more difficult is that you have many, many other sources of variability.",
                    "label": 0
                },
                {
                    "sent": "For example, you may pronounce a word with different possible accent.",
                    "label": 0
                },
                {
                    "sent": "Different phonemes can be pronounced different way if you take for example, a chair.",
                    "label": 0
                },
                {
                    "sent": "A chair can have many different shapes.",
                    "label": 0
                },
                {
                    "sent": "So you have variation.",
                    "label": 0
                },
                {
                    "sent": "You have structural variation of the objects, so all these many many different sources of variability will need to be learned because they are not given to you.",
                    "label": 0
                },
                {
                    "sent": "By the physics, and therefore if you think in terms of group, what you see is that basically you variability can be factorized.",
                    "label": 0
                },
                {
                    "sent": "You are going to have basically the physics and already you have pretty complex group coming in.",
                    "label": 0
                },
                {
                    "sent": "But then you have all these groups which are completely unknown and that have to be learned basically.",
                    "label": 1
                },
                {
                    "sent": "OK, so what's the relation with deep neural network?",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at the problem that way, the thing that you may think is, well, you have a cascade of group.",
                    "label": 0
                },
                {
                    "sent": "So you are going to put layers and each layer is going to build an environment for each of the group.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't work quite that way, and that doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So what I'm?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To try to do is.",
                    "label": 0
                },
                {
                    "sent": "Come from these basic principle idea and try to see how we very naturally arrives to a deep neural network structure.",
                    "label": 0
                },
                {
                    "sent": "But to do that I'm going to go relatively progressively and first of all I'd like to try to understand how can we build an invariant when you have such a complex variability problem, which is not just for example translation, but all possible deformation.",
                    "label": 0
                },
                {
                    "sent": "How can you do that without killing all the information?",
                    "label": 0
                },
                {
                    "sent": "And what you're going to see is that to do that, in fact, you need a multilayer structure and something which looks very much like a deep neural networks with you'll see wavelengths.",
                    "label": 0
                },
                {
                    "sent": "Then the next question that will be easier to address once we understand that is how do you cascade groups?",
                    "label": 0
                },
                {
                    "sent": "How can you cascade different environments on different groups?",
                    "label": 0
                },
                {
                    "sent": "And then we'll finish on the learning problem once we basically understand what we are after, will try to understand how can we learn and how come these very nice idea of sparse ento encoders indeed seems to work very well to learn these type of representation.",
                    "label": 0
                },
                {
                    "sent": "And I'll give examples on images and audio on.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait OK, so let me be.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In with this idea of invariant representation.",
                    "label": 0
                },
                {
                    "sent": "So let's think for example of translation.",
                    "label": 0
                },
                {
                    "sent": "You have an image of pattern which is going to be translated, so when it translate you can see the image as a point in your in your space.",
                    "label": 0
                },
                {
                    "sent": "When it translate, it's going to describe a whole.",
                    "label": 0
                },
                {
                    "sent": "Line or a 2 dimensional surface which is the orbit within your space.",
                    "label": 0
                },
                {
                    "sent": "So each of these points corresponds to the image translated.",
                    "label": 0
                },
                {
                    "sent": "Now an invariant representation mean.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you are going to take all this orbit and transform it into a single point.",
                    "label": 0
                },
                {
                    "sent": "Now the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is that the formation, because the deformation is going to be a much bigger set of images.",
                    "label": 0
                },
                {
                    "sent": "And what you want to do is to transform these things, not into an invariant representation.",
                    "label": 0
                },
                {
                    "sent": "That would be very dangerous, because one can be deformed into a 7, so you don't want to be invariant to the formation you want to be stable to deformation and transform all that into a neighborhood of points near your original point.",
                    "label": 0
                },
                {
                    "sent": "If you do that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you will be able to kill the formation at the supervised stage with a simple linear operator.",
                    "label": 0
                },
                {
                    "sent": "If all your deform points are nearby, then.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can just project on the orthogonal space to your manifold, which is approximated by the tangent space and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can learn with your supervised learning, so the formation will be addressed with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "If you did well, the job having something which is stable to deformation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Suppose you have another pattern.",
                    "label": 0
                },
                {
                    "sent": "Of course, you don't want it to be mapped to the same point, so you won't fight to be discriminants.",
                    "label": 0
                },
                {
                    "sent": "Although.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "8 should be mapped.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the neighborhood of the Origonal.",
                    "label": 0
                },
                {
                    "sent": "8 and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Your supervised stage, you can learn the invariant for the specific date.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formation of the 8.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By projecting it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then of course discrimination is very easy.",
                    "label": 0
                },
                {
                    "sent": "Question how can you do that?",
                    "label": 0
                },
                {
                    "sent": "How can you?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Build such a file.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at the case of translation.",
                    "label": 0
                },
                {
                    "sent": "You have a translated signal and you want that the representation of the translated signal should be the representation of the original signal.",
                    "label": 0
                },
                {
                    "sent": "For example, these are two translated signals.",
                    "label": 0
                },
                {
                    "sent": "You can just make.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A simple registration.",
                    "label": 0
                },
                {
                    "sent": "A signal, another approach.",
                    "label": 0
                },
                {
                    "sent": "You can compute the four.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transform which is going to be transformed by phase and kill the faith.",
                    "label": 0
                },
                {
                    "sent": "So that's very easy.",
                    "label": 0
                },
                {
                    "sent": "The problem is when you have a deformation.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If now your signal is deformed, what you would like?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the representation of the deform signal should be close to the representation of the original signal.",
                    "label": 0
                },
                {
                    "sent": "If the deformation is small, that's the key point, so it should be of the order of this size of the deformation.",
                    "label": 0
                },
                {
                    "sent": "Now suppose.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you do a registration.",
                    "label": 0
                },
                {
                    "sent": "If you had a formation, you maybe align one peak, but the others are not going to be aligned.",
                    "label": 0
                },
                {
                    "sent": "The distance distance is going to be very large.",
                    "label": 0
                },
                {
                    "sent": "Registration is not stable to deformation.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you make a Fourier transform, same problem, high frequencies are going to move.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "So we are facing this strange problem which is there is a lot of math on environment groups, but there is no standard tools to build something which is invariant.",
                    "label": 0
                },
                {
                    "sent": "Sorry, stable today.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation.",
                    "label": 0
                },
                {
                    "sent": "OK, what I propose here is to replace the sine wave, which are delocalized of the Fourier transform with something localized to be stable today formation.",
                    "label": 0
                },
                {
                    "sent": "And these are the wavelengths.",
                    "label": 0
                },
                {
                    "sent": "So wave light will be here complex function with a real imaginary part with quadratic phase.",
                    "label": 0
                },
                {
                    "sent": "Then you take your wavelet and you scale it with all possible scale here.",
                    "label": 0
                },
                {
                    "sent": "And in the for your domain, a wavelet is basically a bandpass filter.",
                    "label": 0
                },
                {
                    "sent": "So when you scale it you cover different frequency bands over there.",
                    "label": 0
                },
                {
                    "sent": "And the low frequencies are covered by a low frequency filter.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what do you do?",
                    "label": 0
                },
                {
                    "sent": "You take your signal and you filter it simple convolution with your wavelength.",
                    "label": 0
                },
                {
                    "sent": "So basically you decompose it in different frequency bands.",
                    "label": 0
                },
                {
                    "sent": "So the way they transform is just a bunch of convolutions with different filters, different position.",
                    "label": 0
                },
                {
                    "sent": "You have a vector of coefficients.",
                    "label": 0
                },
                {
                    "sent": "And if you design well, your wave let's this vector is unitary.",
                    "label": 0
                },
                {
                    "sent": "The norm is the same as the norm of your original signal.",
                    "label": 0
                },
                {
                    "sent": "OK, so why wave lights?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There will be 2 reasons.",
                    "label": 0
                },
                {
                    "sent": "One, the wavelet dictionary, the set of all wavelet translated is environed by translation, that's good.",
                    "label": 1
                },
                {
                    "sent": "But more important now, if you deform a wavelength, you're going to get a new wavelet, which is going to be very close to the original one if.",
                    "label": 0
                },
                {
                    "sent": "The deformation was small.",
                    "label": 1
                },
                {
                    "sent": "And this idea is going to be important because generalizing it will understand.",
                    "label": 0
                },
                {
                    "sent": "Allow us to understand how do you learn.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, in 2D, how do you build a wavelet?",
                    "label": 0
                },
                {
                    "sent": "Same thing but time is now the two variable T1T2 you have a complex wave let in 2D we are going to rotate the wave light.",
                    "label": 0
                },
                {
                    "sent": "So you see here the different wavelets scaled and they're all rotated or possible rotation, real part, imaginary parts.",
                    "label": 0
                },
                {
                    "sent": "And the wavelet transform is just a bunch of convolutions with each of these wavelets, real and imaginary parts, low frequency, and you preserve the norm.",
                    "label": 1
                },
                {
                    "sent": "So that's a very simple and classical tool.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now how do you build an environment?",
                    "label": 0
                },
                {
                    "sent": "Take your signal.",
                    "label": 0
                },
                {
                    "sent": "Make a convolution with the wave, let you have your real and imaginary part.",
                    "label": 0
                },
                {
                    "sent": "You have something which is oscillating.",
                    "label": 0
                },
                {
                    "sent": "If you want to kill the translation, one way is to do like the Fourier transform, kill the phase which is sensitive to translation.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do that, that's what Jen Luker would be calling a pooling, so you just get the envelope.",
                    "label": 0
                },
                {
                    "sent": "You get the regular ambulance so it's not inviting by translation, but just to very smell more translations.",
                    "label": 0
                },
                {
                    "sent": "If you want to build something much more environmental.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Translation one way to do it is you just take this function and now you average it.",
                    "label": 0
                },
                {
                    "sent": "With a window file.",
                    "label": 0
                },
                {
                    "sent": "If you do that now, you're going to get something much more regular, which is going to become to be invited to translation, which are relatively small, relatively to your window.",
                    "label": 1
                },
                {
                    "sent": "Now let's your window increase in size.",
                    "label": 0
                },
                {
                    "sent": "If the window fire increase in size and converge to one.",
                    "label": 0
                },
                {
                    "sent": "This is basically going to converge to the integral of the wavelet transform modulus.",
                    "label": 1
                },
                {
                    "sent": "So the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "This is completely invariant to translation.",
                    "label": 1
                },
                {
                    "sent": "And we'll see stable today.",
                    "label": 0
                },
                {
                    "sent": "Formation great.",
                    "label": 0
                },
                {
                    "sent": "But you've lost a lot of information.",
                    "label": 0
                },
                {
                    "sent": "You had a function.",
                    "label": 0
                },
                {
                    "sent": "Now you just have a bunch of L1 norm, so all the information almost has disappeared.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How can you recover this lost information?",
                    "label": 1
                },
                {
                    "sent": "OK, the problem is you've taken your modulus an you averaged it.",
                    "label": 0
                },
                {
                    "sent": "So you lost all the low frequencies.",
                    "label": 0
                },
                {
                    "sent": "How to recover?",
                    "label": 0
                },
                {
                    "sent": "Sorry to all the high frequencies you've done.",
                    "label": 1
                },
                {
                    "sent": "A low frequency filtering.",
                    "label": 0
                },
                {
                    "sent": "How to recover them?",
                    "label": 0
                },
                {
                    "sent": "Well, the low frequency, that's just the first component of a wavelet transform.",
                    "label": 1
                },
                {
                    "sent": "The high frequencies.",
                    "label": 0
                },
                {
                    "sent": "You can recover them by getting the wavelet coefficients of this modelers OK.",
                    "label": 0
                },
                {
                    "sent": "These are the high frequencies, but the high frequencies are not invariant to translation.",
                    "label": 0
                },
                {
                    "sent": "How can you make them invariant to translation?",
                    "label": 0
                },
                {
                    "sent": "Kill the face.",
                    "label": 0
                },
                {
                    "sent": "An average, you just do the same thing Hilda Phase and you average and now you have a whole bunch of environments for any wavelength Lambda one any Lambda to.",
                    "label": 0
                },
                {
                    "sent": "These are a whole set of invariants.",
                    "label": 0
                },
                {
                    "sent": "What are we doing?",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Implementing here a deep convolution network.",
                    "label": 0
                },
                {
                    "sent": "We begin from X.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get the first environments.",
                    "label": 0
                },
                {
                    "sent": "The average over there and then all the wavelet coefficients, which are the high frequencies.",
                    "label": 0
                },
                {
                    "sent": "Take the modelers to make them environment now.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the next layer of environment is here.",
                    "label": 0
                },
                {
                    "sent": "What have you lost?",
                    "label": 0
                },
                {
                    "sent": "The high frequencies which are here.",
                    "label": 0
                },
                {
                    "sent": "Make these high frequency invariants.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You average them.",
                    "label": 0
                },
                {
                    "sent": "What have you lost the next week?",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "And then suddenly you see appearing this norm.",
                    "label": 0
                },
                {
                    "sent": "That's deep neural net.",
                    "label": 0
                },
                {
                    "sent": "It's not because you have many groups, you just have one group.",
                    "label": 0
                },
                {
                    "sent": "But it's because you want to recover the lost information each time you've implemented your environment.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the network output.",
                    "label": 0
                },
                {
                    "sent": "It's a whole set of coefficients that will be called the scattering transform.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first layer are just the standard MFC C or sip coefficient.",
                    "label": 0
                },
                {
                    "sent": "The next layers will be.",
                    "label": 0
                },
                {
                    "sent": "The compliment will look at them.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at an example you have here.",
                    "label": 0
                },
                {
                    "sent": "The spectrogram of his sound.",
                    "label": 0
                },
                {
                    "sent": "Now you have a tremolo vibrato OK.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First layer of coefficients because you are averaging.",
                    "label": 0
                },
                {
                    "sent": "What are you going to get?",
                    "label": 0
                },
                {
                    "sent": "You are going to get the basic frequency distribution of coefficients, but the averaging has killed all the difference between these different nodes, which are very different when you hear them.",
                    "label": 0
                },
                {
                    "sent": "Because you averaged.",
                    "label": 0
                },
                {
                    "sent": "Where is this information you?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the second layer of coefficients, which gives you the time variation of these and you see the attack here, which was very different, appears because you have a lot of high frequencies in the second layer, the tremolo appears with this high frequency element.",
                    "label": 0
                },
                {
                    "sent": "Vibrato appears through this spectrum, so the second layer will give you the information which differentiate these nodes, let's look.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And image these two images are two textures.",
                    "label": 0
                },
                {
                    "sent": "They have exactly the same 2nd order moments.",
                    "label": 0
                },
                {
                    "sent": "If you look at them through there's power spectrum.",
                    "label": 1
                },
                {
                    "sent": "This power spectrum is absolutely identical because they have same 2nd order moment.",
                    "label": 0
                },
                {
                    "sent": "If you look at the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If your network, these are the wavelet coefficient, it basically gives you the same information.",
                    "label": 0
                },
                {
                    "sent": "Then the power spectrum, but a little bit average, so no difference.",
                    "label": 1
                },
                {
                    "sent": "If you look at the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can layer they are very different.",
                    "label": 0
                },
                {
                    "sent": "These are all the second layer coefficients and because this is much more sparse than this, the coefficient have a very different distribution.",
                    "label": 0
                },
                {
                    "sent": "So you see it being the fact that these second layers and next gives a lot of information.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, can we understand what's behind all that and can we analyze the properties while in fact we are doing something simple?",
                    "label": 0
                },
                {
                    "sent": "What are we doing?",
                    "label": 0
                },
                {
                    "sent": "We are taking the signal taking a simple unit unitary transform.",
                    "label": 0
                },
                {
                    "sent": "The way they transform which is unitary and the only thing that we.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Changing is where taking the absolute value.",
                    "label": 0
                },
                {
                    "sent": "So now we have a non linear operator but it remains contractive because the absolute value is contracted.",
                    "label": 0
                },
                {
                    "sent": "And it keeps the normal and then we are just iterating on this operator.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are taking the average.",
                    "label": 0
                },
                {
                    "sent": "Each of these coefficients.",
                    "label": 0
                },
                {
                    "sent": "We make it environment and that's the residue.",
                    "label": 0
                },
                {
                    "sent": "The next layer we reapply the operator, we get the next layer of environment output and inside node, which is the next layer and we reapply and.",
                    "label": 0
                },
                {
                    "sent": "So that's how you build your network.",
                    "label": 0
                },
                {
                    "sent": "Now, one very important property you can prove that this is going to converge to 0.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "All the information is going to get out in the output.",
                    "label": 0
                },
                {
                    "sent": "So that's summarized.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By the following theorem, which says if you look at your scattering vector, the output of your network first of all you have something contractive.",
                    "label": 0
                },
                {
                    "sent": "If you look at this as a vector, the difference if you take a signal X&Y between these two vectors will be smaller than X -- Y because it's implemented as a cascade of contractive operators.",
                    "label": 0
                },
                {
                    "sent": "The second thing is, all the energy will get out.",
                    "label": 0
                },
                {
                    "sent": "The norm is the same as the norm of the original signal.",
                    "label": 0
                },
                {
                    "sent": "Because you've implemented that with something which preserves the norm and it converge to 0.",
                    "label": 0
                },
                {
                    "sent": "But much more important, you are stable to the formation.",
                    "label": 0
                },
                {
                    "sent": "If you know your signal is deformed, the representation is going to be very close if.",
                    "label": 0
                },
                {
                    "sent": "The deformation is small.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, but have you lost in form?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not because you keep the norm that you haven't lost information.",
                    "label": 0
                },
                {
                    "sent": "What did you do?",
                    "label": 0
                },
                {
                    "sent": "You iterated on this operator and this operator kills the phase.",
                    "label": 0
                },
                {
                    "sent": "Are you losing information?",
                    "label": 0
                },
                {
                    "sent": "But there is a very nice result of Iran Wildberger which shows that no, this operator is invertible.",
                    "label": 0
                },
                {
                    "sent": "So if you take this signal, you have the modulus of the wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "You can recover exactly the original signal.",
                    "label": 0
                },
                {
                    "sent": "You can invert it.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that now you can try to reconstruct by inverting.",
                    "label": 0
                },
                {
                    "sent": "This operator between each layer.",
                    "label": 0
                },
                {
                    "sent": "Problem.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You don't have all the layers at one point, you will stop.",
                    "label": 0
                },
                {
                    "sent": "I will show you reconstruction from the two layers, so you've lost all the rest.",
                    "label": 0
                },
                {
                    "sent": "You have an error here and this area is going to propagate.",
                    "label": 0
                },
                {
                    "sent": "How bad is that going to be?",
                    "label": 0
                },
                {
                    "sent": "Let me show you.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "Reconstruction just from the.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slayer.",
                    "label": 0
                },
                {
                    "sent": "So you see this spectrum slowly varying because the window where 3 seconds OK reconstruction from.",
                    "label": 1
                },
                {
                    "sent": "If you add the second layer.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have a huge amount of information in second layer.",
                    "label": 0
                },
                {
                    "sent": "That's what gives you basically the transient structure.",
                    "label": 0
                },
                {
                    "sent": "That's why you do need to have these multilayer structures.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so classification there is number of very nice things done by Jean Bruno.",
                    "label": 0
                },
                {
                    "sent": "So basically the idea is now you have a representation which is translation invariant and stable to deformation.",
                    "label": 1
                },
                {
                    "sent": "The deformations are going to be studied at the supervised stage and killed for example with a simple SVM.",
                    "label": 0
                },
                {
                    "sent": "So that's where you will learn the linear operator that kills the deformation.",
                    "label": 0
                },
                {
                    "sent": "And you can look at the result so it's interesting to compare with convolution network on amnesty.",
                    "label": 0
                },
                {
                    "sent": "The state of the art was hauled by the team of yellow can with convolution networks, and that's what we're getting with the scattering.",
                    "label": 0
                },
                {
                    "sent": "Basically, we do improve without learning anything besides this simple SVM.",
                    "label": 0
                },
                {
                    "sent": "So what's the interpretation?",
                    "label": 0
                },
                {
                    "sent": "There's two ways to think about it.",
                    "label": 0
                },
                {
                    "sent": "One, well, you have something much faster and you get better results because you know in advance what to learn so you don't need to learn.",
                    "label": 0
                },
                {
                    "sent": "You know that what you need here are wave.",
                    "label": 0
                },
                {
                    "sent": "Let's let's put the wavelets and use them.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you can think of it differently.",
                    "label": 0
                },
                {
                    "sent": "They didn't impose any filters, and they were able to learn the filters, and in fact they got very good result.",
                    "label": 0
                },
                {
                    "sent": "So that shows that learning is working here.",
                    "label": 0
                },
                {
                    "sent": "In the case you know where to get, it seems that you do get that.",
                    "label": 0
                },
                {
                    "sent": "Although of course you'd better get it directly when you know.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But it's.",
                    "label": 0
                },
                {
                    "sent": "To show that the learning does work.",
                    "label": 0
                },
                {
                    "sent": "Now if you take a second type of problem textures.",
                    "label": 0
                },
                {
                    "sent": "So this is a well known database in Berkeley, curate with 61 classes of textures.",
                    "label": 1
                },
                {
                    "sent": "This is 3 classes and exactly same thing.",
                    "label": 0
                },
                {
                    "sent": "You build a representation of your texture and then your supervised learning state of the art was basically obtained by fully spectral man features with histogram errors of about one person.",
                    "label": 0
                },
                {
                    "sent": "There are textures that you can't discriminate with the power spectrum.",
                    "label": 0
                },
                {
                    "sent": "If you do it with a scattering, there is a big improvement.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because now suddenly you can discriminate textures having exactly the same 2nd order moment.",
                    "label": 1
                },
                {
                    "sent": "So basically it also as a side effect gives you different way to look at stationary processes and classify them.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we basically understand how to deal with one environment.",
                    "label": 1
                },
                {
                    "sent": "How do you put a second invite?",
                    "label": 0
                },
                {
                    "sent": "A second group?",
                    "label": 0
                },
                {
                    "sent": "One first idea that one you may think is, let's kill the first group.",
                    "label": 0
                },
                {
                    "sent": "First environment and then let's kill the second group.",
                    "label": 0
                },
                {
                    "sent": "For example translation environments, then rotation invariant.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Now why it doesn't work?",
                    "label": 0
                },
                {
                    "sent": "Take these two images 2 textures.",
                    "label": 0
                },
                {
                    "sent": "There are totally different.",
                    "label": 0
                },
                {
                    "sent": "Now if you make a translation invariant 1st and then a rotation invariants, they will be exactly having the same representation because they differ they have the same distribution of orientation, it's just that the position is different.",
                    "label": 0
                },
                {
                    "sent": "It's the joint position between rotation and translation.",
                    "label": 1
                },
                {
                    "sent": "So what you need to do is to keep the joint information.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you cannot deal with this group by separating into two independent components.",
                    "label": 0
                },
                {
                    "sent": "You really need to deal with the full roto Translation Group, which includes a rotation and the translation.",
                    "label": 0
                },
                {
                    "sent": "So if you have a rotation translation acting on the signal, it means you take your signal, translate it, and rotate it OK.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now what is the group multiplication?",
                    "label": 1
                },
                {
                    "sent": "Well, if you do a first rotation translation, a second rotation translation.",
                    "label": 1
                },
                {
                    "sent": "The rotation will be the product of the two rotation, but rotation will act on translation, so it's not commutative.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can you make an environment?",
                    "label": 0
                },
                {
                    "sent": "Well, it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "You make a convolution, you just average over the group.",
                    "label": 0
                },
                {
                    "sent": "How do you do an averaging?",
                    "label": 0
                },
                {
                    "sent": "So if you want to do an ad.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Charging in time to do translation invariants, you make a convolution your signal average with your window translated.",
                    "label": 0
                },
                {
                    "sent": "Well, if you deal with another group, it's the same.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You take your signal, which is a function of rotation translation and you average it with a window which is translated over the group.",
                    "label": 0
                },
                {
                    "sent": "That's a convolution, it's just a convolution on a different group.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do you do wavelet transform to recover the lost information while you are going to define a wavelength on the group?",
                    "label": 0
                },
                {
                    "sent": "Ann, you're going to make a wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "Basically, these are the averaging you've lost the high frequencies and the high frequencies you'll recover them with your wavelengths.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same idea.",
                    "label": 0
                },
                {
                    "sent": "And you can have a unitary transform.",
                    "label": 0
                },
                {
                    "sent": "You need now to have an invariant.",
                    "label": 0
                },
                {
                    "sent": "How are you?",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To build an invariant kelda phase because the phases where you encode the local translation.",
                    "label": 0
                },
                {
                    "sent": "So you just take the modulus of this wavelet transform.",
                    "label": 0
                },
                {
                    "sent": "Computed over this group with the convolution on the group.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Let's begin with the signal.",
                    "label": 0
                },
                {
                    "sent": "First layer translation invariants.",
                    "label": 0
                },
                {
                    "sent": "So you have your translation wavelet convolution.",
                    "label": 0
                },
                {
                    "sent": "You make the environment by averaging, and that's the last information.",
                    "label": 0
                },
                {
                    "sent": "Now this last information is a function of rotation because the wavelength was rotated.",
                    "label": 0
                },
                {
                    "sent": "Scale and time.",
                    "label": 0
                },
                {
                    "sent": "Let's consider it as a signal as a function of rotation in time.",
                    "label": 0
                },
                {
                    "sent": "If you want something which is rotation, invite.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's just make.",
                    "label": 0
                },
                {
                    "sent": "You apply this wavelet transform modulus operator, make a convolution on the group.",
                    "label": 0
                },
                {
                    "sent": "Average on the group and you get an invariant to rotation and translation, so it's the same idea, but now you add one other layer which corresponds to the rotation and to the translation.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What if you want to do that with scale?",
                    "label": 0
                },
                {
                    "sent": "Same thing, but now you are also going to do a convolution relatively to the scale.",
                    "label": 0
                },
                {
                    "sent": "Here you will need also renormalization.",
                    "label": 0
                },
                {
                    "sent": "I will come back to that when I look at the very general problem.",
                    "label": 0
                },
                {
                    "sent": "So the idea is you just cascade convolution is exactly as gender can thought, but you just change your definition of convolution depending upon the type of environments you want to do.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let's look for example data classification problem of textures.",
                    "label": 0
                },
                {
                    "sent": "This is a database where the textures are also deformed, rotated, translated, of course, but also scaling.",
                    "label": 0
                },
                {
                    "sent": "If you just have a translation invariant network, 20% affairs if you add up rotation 3% if you add up scaling environments, you go back to zero point 6%, which just shows.",
                    "label": 0
                },
                {
                    "sent": "Once you have your invariant classification is indeed really easy.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is what invites and in this simple example that I gave from physics you know them, but what about learning them?",
                    "label": 0
                },
                {
                    "sent": "So basically we are in a situation where we have few groups that we know and few groups that you need to learn OK.",
                    "label": 0
                },
                {
                    "sent": "The question is how can you learn a group?",
                    "label": 0
                },
                {
                    "sent": "Now a group is nice because it has a very global structure.",
                    "label": 0
                },
                {
                    "sent": "The group is entirely defined by a simpler thing, which is a Lie algebra.",
                    "label": 1
                },
                {
                    "sent": "What is the Lee algebra?",
                    "label": 0
                },
                {
                    "sent": "A group is a series of operators which moves your signal well.",
                    "label": 0
                },
                {
                    "sent": "You can look at the Infinity symbol movement and the infinitesimal movement is carried by a simple linear space, which is called the linear algebra OK, and that's what entirely specifies the group.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you have a translation.",
                    "label": 0
                },
                {
                    "sent": "That the algebra is just a 1D linear space which corresponds to the derivative.",
                    "label": 0
                },
                {
                    "sent": "The derivative operator, because a translation can be obtained by series of increments.",
                    "label": 0
                },
                {
                    "sent": "So basically series of derivatives.",
                    "label": 0
                },
                {
                    "sent": "If the group includes all possible deformation, then the larger prize much more complicated because it's infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "You have the multiplication by NEC one function.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how to think about learning one key element will be don't learn the groups.",
                    "label": 0
                },
                {
                    "sent": "What you need is to learn the parts exactly like in the bag of words.",
                    "label": 0
                },
                {
                    "sent": "Ideas OK, what is apart apart is like a center of symmetry is a family of functions which are stable.",
                    "label": 0
                },
                {
                    "sent": "If you apply your group transformation.",
                    "label": 0
                },
                {
                    "sent": "So if you apply an element of the algebra, basically the function is not going to change, it's just going to be multiplied by a constant.",
                    "label": 0
                },
                {
                    "sent": "So it's something which.",
                    "label": 0
                },
                {
                    "sent": "Most doesn't change when you apply your group operator.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that if you look at your element of your algebra and you represent it on your family or function, you have a very sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "Basically not diagonal, but a band matrix.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, why is that useful?",
                    "label": 0
                },
                {
                    "sent": "Because once you have that, you can build an environment.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Because if you have a signal which is transformed by your element, actually algebra and you represent it with your pseudo wavelength, which I called here, this table part, it's just going to be changed by a multiplicative factor.",
                    "label": 0
                },
                {
                    "sent": "So to get an invariant you just kill the phase and you re normalize.",
                    "label": 0
                },
                {
                    "sent": "And that's not just true for translation and so on.",
                    "label": 0
                },
                {
                    "sent": "It's going to be true for any kind of variations.",
                    "label": 0
                },
                {
                    "sent": "So you can think.",
                    "label": 0
                },
                {
                    "sent": "Of these network filters as just being the stable parts which almost diagonalize your algebra.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now wavelets are exactly that.",
                    "label": 0
                },
                {
                    "sent": "They're just a particular case if you consider the problem of translation and deformation, there are the stable parts of your group.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the nice thing is you don't have to learn the groups.",
                    "label": 0
                },
                {
                    "sent": "In fact it would be unfeasible because the groups is much too big.",
                    "label": 0
                },
                {
                    "sent": "You just have to learn these center of symmetry of you.",
                    "label": 0
                },
                {
                    "sent": "You prefer these table parts exactly a little bit like these people are sinking in this bag of words algorithms suppose and how to do that, and that's where you see this ad ideas of sparse autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Suppose you have a way to represent your signal set in a very sparse way.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm not dealing with groups, just representing the signal set in a sparse way.",
                    "label": 1
                },
                {
                    "sent": "That means that your signal is going to represent it by very few non zero inner product.",
                    "label": 0
                },
                {
                    "sent": "OK and if you change your signal that's still in your signal set.",
                    "label": 0
                },
                {
                    "sent": "It's very sparse.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It means that the operator which goes from here to here has to be sparse, otherwise this one wouldn't be sparse.",
                    "label": 0
                },
                {
                    "sent": "So that means that too sparse.",
                    "label": 0
                },
                {
                    "sent": "If I to find the sparse elements of the group of the group operation, you just have to specify the signal.",
                    "label": 0
                },
                {
                    "sent": "And that means.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's a very subtle strategy to find the invites, you first find a way to make your signal sparse.",
                    "label": 0
                },
                {
                    "sent": "Indirectly.",
                    "label": 0
                },
                {
                    "sent": "You found a way to represent efficiently your variability, and then the environment you just build it by killing the phase and renormalizing, which is exactly what is done by these deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "So that's the way you can see it, and one question is maybe this is basically a very generic approach for understanding representation in general of very high dam.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Social spaces.",
                    "label": 0
                },
                {
                    "sent": "So conclusion, one way basically to see all that is really to think of learning as a way to reduce intra variability.",
                    "label": 0
                },
                {
                    "sent": "Once you think of it that way, the notion of variability is related in a way or another, to the notion of group, and then it's really an issue of learning the group and learning a way to build an environment.",
                    "label": 0
                },
                {
                    "sent": "The problem again, the groups are not so simple because they're not just simple translation rotation, they're very big.",
                    "label": 0
                },
                {
                    "sent": "You have all the deformations and one of the key thing is that to build an environment of these very complex groups, you need these stable parts.",
                    "label": 1
                },
                {
                    "sent": "It happens that the stable parts are wavelets for translation or roto translation, but you can define this table parts in a very generic weight and what are going to be the stable part?",
                    "label": 0
                },
                {
                    "sent": "That's what you are going to observe very often in your signals precisely because they are stable.",
                    "label": 0
                },
                {
                    "sent": "They are going to remain there.",
                    "label": 1
                },
                {
                    "sent": "Once you found these tables, basically the conjecture is that you can learn them with sparsity.",
                    "label": 0
                },
                {
                    "sent": "Theta conjecture because mathematics are not done, but I think it's one of the way to think of why sparsity is so important and why these structures are getting such beautiful results.",
                    "label": 0
                },
                {
                    "sent": "So they are all kind of.",
                    "label": 0
                },
                {
                    "sent": "Results around these things.",
                    "label": 0
                },
                {
                    "sent": "One thing is if anybody is interested in that kind of research we are looking for postdoc.",
                    "label": 1
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}