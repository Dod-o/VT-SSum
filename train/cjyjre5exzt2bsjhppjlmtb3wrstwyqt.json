{
    "id": "cjyjre5exzt2bsjhppjlmtb3wrstwyqt",
    "title": "Convex Risk Minimization and Conditional Probability Estimation",
    "info": {
        "author": [
            "Matus Telgarsky, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Sept. 9, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_telgarsky_conditional_probability/",
    "segmentation": [
        [
            "Thanks, this is joint work with Russia.",
            "Merodach.",
            "So let me."
        ],
        [
            "You in three sentences with the short summary is.",
            "So first the basic object is we have sequences of linear predictors that optimize some convex risk.",
            "The guarantee we want is a we want to make a guarantee about this sequence.",
            "Some convergence properties for the classification risk, and the reason this problem is not trivial is a it's general theorem, so it's an infinite dimension, and two, there's no, there's no boundedness or regularization or strong convexity or any any of these kinds of identifiability.",
            "Any of these kinds of assumptions."
        ],
        [
            "OK, so let me just tell you a little bit more precisely what the setting is.",
            "So when I say something is linear, I just mean I sum up a bunch of basic functions of this telegraphic H. That's my basic predictors, and so by this notation, if you choose H in some way, you can get regression.",
            "You choose another way you get boosting.",
            "So for all these standard kind of linear ISH algorithms and the second thing is so standard kind of margin definition of risk, the only thing I want to highlight about this standard definition is that notice that the losses in this paper they all asymptotes to 0 on one side.",
            "What that means is that in general, the optimum is going to be off at Infinity.",
            "You don't have an optimum.",
            "And so when I say some things are risk minimizing sequence.",
            "That just means that the sequence of all these risks goes to the optimal one."
        ],
        [
            "And in order to explain what the convergence result will actually be, the converged result will be in terms not of these functions F, But actually in terms of something called conditional probability models.",
            "So this is a standard translation of a predictor where where you for the logistic loss is just this sigmoidal type thing.",
            "So it's a very nice smooth functions.",
            "You're on one side 1 on the other, and then there's a way to do it for a generic L. And this is a standard standard translation, and the important thing is that disagrees with the predictor always.",
            "So if the predictor says I'm going to say this is a positive example, so does this conditional probability model.",
            "So we can use these to reason about the classification properties of the predictors themselves.",
            "So here's the 1st result.",
            "If you minimize the convex risk then you converge this thing at a bar.",
            "Now what is Ada Bareda bars defined by the other parts that promise for any?",
            "There's a unique Ada bar such that any minimizing sequence will converge.",
            "This at a bar.",
            "That's the 1st result, and so let me just remind you of the motivation.",
            "You handed me a risk minimizing sequence, but there's no evidence that this actually had any relevance to our original problem of classification, so we want to say some classification relevant, and that's captured by the definition of the Zeta's, so we can use at a bar, and I can tell you about this at the post you can use at a bar to reason about the classification properties of the eyes."
        ],
        [
            "And again, the reason this is nontrivial is because first it's infinite dimensional.",
            "So for instance, the set of possible ETA, ETA ETA bars is not a compact set, even though I've made some boundedness on the, you know the prediction and I mean on the output spacer between 01.",
            "And two, there are no no boundedness assumptions.",
            "And so this result applies to unregularized unconstrained algorithms.",
            "Things like boosting, but it also applies if you if you regularize something, you stick the regularization strength down down to 0.",
            "The sex."
        ],
        [
            "Result that we include.",
            "Unfortunately, in the finite dimensional setting, but it is a uniform deviation inequality, so it says that the gap between you and the ADA bar is the quantity that's controlled only.",
            "So look at the right hand side.",
            "I've got the usual kind of 1 / N stuff, but then I have just an excess risk.",
            "So in particular if you tried to prove this by hitting it with Rademacher, you'd get a norm dependence and you can't have a norm dependence because the optimum is often Infinity, so again, the right hand side only has depends on the excess risk.",
            "So if you say, let's say, just.",
            "Run your favorite adigrat or whatever you just tell it to optimize 1 / N that right hand side is constant times 1 / N. Square root.",
            "OK. Let me just tell you and just 'cause I somehow I have to see how we get around boundedness so."
        ],
        [
            "One cute trick is you set up your duality correctly.",
            "Always have a dual optimum and everywhere that we needed an optimum.",
            "We just use the development.",
            "Didn't matter that there was no primal optimum, so that's one cute."
        ],
        [
            "Fact second, any problem of this type has the following decomposition.",
            "There's a unique half space.",
            "Which is this or unique subspace, this gamma?",
            "Where?",
            "I'm going to just talk about points along Gamma and all the other points."
        ],
        [
            "The points that are not on gamma.",
            "They're nicely separated, so here I can use kind of marginal reasoning for the deviations.",
            "I can just go to control them like 0.",
            "One things I can use the VC theorem.",
            "It doesn't matter if they go off to Infinity and then on."
        ],
        [
            "Set.",
            "It doesn't make sense to pick it.",
            "Pick a large predictor, because convex losses hate making mistakes, so the loss itself prevents you from picking a solution with big big norms.",
            "So here we just use regular local router marker for for Lipschitz, so please come to the poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks, this is joint work with Russia.",
                    "label": 0
                },
                {
                    "sent": "Merodach.",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You in three sentences with the short summary is.",
                    "label": 0
                },
                {
                    "sent": "So first the basic object is we have sequences of linear predictors that optimize some convex risk.",
                    "label": 1
                },
                {
                    "sent": "The guarantee we want is a we want to make a guarantee about this sequence.",
                    "label": 0
                },
                {
                    "sent": "Some convergence properties for the classification risk, and the reason this problem is not trivial is a it's general theorem, so it's an infinite dimension, and two, there's no, there's no boundedness or regularization or strong convexity or any any of these kinds of identifiability.",
                    "label": 0
                },
                {
                    "sent": "Any of these kinds of assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me just tell you a little bit more precisely what the setting is.",
                    "label": 0
                },
                {
                    "sent": "So when I say something is linear, I just mean I sum up a bunch of basic functions of this telegraphic H. That's my basic predictors, and so by this notation, if you choose H in some way, you can get regression.",
                    "label": 0
                },
                {
                    "sent": "You choose another way you get boosting.",
                    "label": 0
                },
                {
                    "sent": "So for all these standard kind of linear ISH algorithms and the second thing is so standard kind of margin definition of risk, the only thing I want to highlight about this standard definition is that notice that the losses in this paper they all asymptotes to 0 on one side.",
                    "label": 0
                },
                {
                    "sent": "What that means is that in general, the optimum is going to be off at Infinity.",
                    "label": 0
                },
                {
                    "sent": "You don't have an optimum.",
                    "label": 0
                },
                {
                    "sent": "And so when I say some things are risk minimizing sequence.",
                    "label": 0
                },
                {
                    "sent": "That just means that the sequence of all these risks goes to the optimal one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in order to explain what the convergence result will actually be, the converged result will be in terms not of these functions F, But actually in terms of something called conditional probability models.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard translation of a predictor where where you for the logistic loss is just this sigmoidal type thing.",
                    "label": 0
                },
                {
                    "sent": "So it's a very nice smooth functions.",
                    "label": 0
                },
                {
                    "sent": "You're on one side 1 on the other, and then there's a way to do it for a generic L. And this is a standard standard translation, and the important thing is that disagrees with the predictor always.",
                    "label": 0
                },
                {
                    "sent": "So if the predictor says I'm going to say this is a positive example, so does this conditional probability model.",
                    "label": 1
                },
                {
                    "sent": "So we can use these to reason about the classification properties of the predictors themselves.",
                    "label": 0
                },
                {
                    "sent": "So here's the 1st result.",
                    "label": 1
                },
                {
                    "sent": "If you minimize the convex risk then you converge this thing at a bar.",
                    "label": 0
                },
                {
                    "sent": "Now what is Ada Bareda bars defined by the other parts that promise for any?",
                    "label": 1
                },
                {
                    "sent": "There's a unique Ada bar such that any minimizing sequence will converge.",
                    "label": 0
                },
                {
                    "sent": "This at a bar.",
                    "label": 0
                },
                {
                    "sent": "That's the 1st result, and so let me just remind you of the motivation.",
                    "label": 0
                },
                {
                    "sent": "You handed me a risk minimizing sequence, but there's no evidence that this actually had any relevance to our original problem of classification, so we want to say some classification relevant, and that's captured by the definition of the Zeta's, so we can use at a bar, and I can tell you about this at the post you can use at a bar to reason about the classification properties of the eyes.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And again, the reason this is nontrivial is because first it's infinite dimensional.",
                    "label": 1
                },
                {
                    "sent": "So for instance, the set of possible ETA, ETA ETA bars is not a compact set, even though I've made some boundedness on the, you know the prediction and I mean on the output spacer between 01.",
                    "label": 1
                },
                {
                    "sent": "And two, there are no no boundedness assumptions.",
                    "label": 0
                },
                {
                    "sent": "And so this result applies to unregularized unconstrained algorithms.",
                    "label": 1
                },
                {
                    "sent": "Things like boosting, but it also applies if you if you regularize something, you stick the regularization strength down down to 0.",
                    "label": 0
                },
                {
                    "sent": "The sex.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result that we include.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, in the finite dimensional setting, but it is a uniform deviation inequality, so it says that the gap between you and the ADA bar is the quantity that's controlled only.",
                    "label": 0
                },
                {
                    "sent": "So look at the right hand side.",
                    "label": 0
                },
                {
                    "sent": "I've got the usual kind of 1 / N stuff, but then I have just an excess risk.",
                    "label": 0
                },
                {
                    "sent": "So in particular if you tried to prove this by hitting it with Rademacher, you'd get a norm dependence and you can't have a norm dependence because the optimum is often Infinity, so again, the right hand side only has depends on the excess risk.",
                    "label": 0
                },
                {
                    "sent": "So if you say, let's say, just.",
                    "label": 0
                },
                {
                    "sent": "Run your favorite adigrat or whatever you just tell it to optimize 1 / N that right hand side is constant times 1 / N. Square root.",
                    "label": 0
                },
                {
                    "sent": "OK. Let me just tell you and just 'cause I somehow I have to see how we get around boundedness so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One cute trick is you set up your duality correctly.",
                    "label": 0
                },
                {
                    "sent": "Always have a dual optimum and everywhere that we needed an optimum.",
                    "label": 1
                },
                {
                    "sent": "We just use the development.",
                    "label": 0
                },
                {
                    "sent": "Didn't matter that there was no primal optimum, so that's one cute.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fact second, any problem of this type has the following decomposition.",
                    "label": 0
                },
                {
                    "sent": "There's a unique half space.",
                    "label": 0
                },
                {
                    "sent": "Which is this or unique subspace, this gamma?",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "I'm going to just talk about points along Gamma and all the other points.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The points that are not on gamma.",
                    "label": 0
                },
                {
                    "sent": "They're nicely separated, so here I can use kind of marginal reasoning for the deviations.",
                    "label": 0
                },
                {
                    "sent": "I can just go to control them like 0.",
                    "label": 0
                },
                {
                    "sent": "One things I can use the VC theorem.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter if they go off to Infinity and then on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "It doesn't make sense to pick it.",
                    "label": 0
                },
                {
                    "sent": "Pick a large predictor, because convex losses hate making mistakes, so the loss itself prevents you from picking a solution with big big norms.",
                    "label": 0
                },
                {
                    "sent": "So here we just use regular local router marker for for Lipschitz, so please come to the poster.",
                    "label": 0
                }
            ]
        }
    }
}