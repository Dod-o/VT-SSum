{
    "id": "3dmezvukplgd7uwznqkgi2ttmnwkjp4x",
    "title": "Introduction to Torch",
    "info": {
        "author": [
            "Alex Wiltschko, Twitter, Inc."
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_wiltschko_torch/",
    "segmentation": [
        [
            "So this talk has two kind of main parts."
        ],
        [
            "Part is just torch basics, so just using torch for doing scientific computing in an overview of how to train neural networks with torch.",
            "And I'll kind of stay at a high level when discussing that and then point you to some material if you really want to dive in.",
            "And of course there's the practical session and torch that comes afterwards.",
            "If anybody is interested.",
            "And in the second half will talk more about automatic differentiation, which we've heard a couple talks touch on as back propagation, but I'll go into a little more detail and then torch autograde, which is Twitter's kind of opinionated implementation of automatic differentiation, and I'll talk a bit more about.",
            "What's gone into that and what it enables when you kind of take particular opinion of how you should implement automatic differentiation.",
            "Seriously, so before I get started, who has heard of the language Lua?",
            "OK, it's good amount who is heard of Torch?",
            "Probably the same people.",
            "And who has used torch?",
            "OK, so a little bit less.",
            "OK, maybe like a third.",
            "OK cool so I think some of these details might be interesting too that haven't used it to kind of get a sense of what it's all about and those that do know it might be just a good refresher.",
            "So torch is.",
            "A."
        ],
        [
            "An array programming library for Lua.",
            "It looks a lot like Matlab because it was directly inspired by Matlab.",
            "It just happens to be in a different language with different kinds of tradeoffs, and because it looks like MATLAB, it also looks a lot like NUM py, so if you're familiar with any of these array programming libraries, you'll probably be very comfortable working in torch and will be able to get up to speed very quickly if it's something that interests you.",
            "So Torch is an interactive."
        ],
        [
            "Scientific computing framework in Lua, and this is just like a really quick take on what looks like.",
            "It looks very, very similar to Python, so we've got strings like you would want them.",
            "You can print stuff, there's only one associated data type in Lua, and that's the table, so you can build lists and dictionaries and sets out of this one data type.",
            "But really, there's just one kind of single associated data type in Lua, and of course we've got four loops and all that kind of basic stuff."
        ],
        [
            "Torch itself has 150 or so tensor functions, so it's kind of the same level of coverage is MATLAB and NUM py for doing linear algebra.",
            "And of course convolutions because Torch has a really strong focus on deep learning or some of the packages do tensor manipulation, like reshaping, slicing everything that you might expect from an array library.",
            "And then logical operators for doing masking and stuff like that.",
            "And a whole bunch more.",
            "And it's all fully documented on GitHub.",
            "These slides are on the MHW Dem GitHub page, so if you're interested in following along there on that summer school project under the torch directory.",
            "The kind of similarities between torch, an Matlab and NUM py extend."
        ],
        [
            "Beyond just the functions that they implement, there's also nice plotting everything can be done inside of Jupiter or Ipython notebook, so you can do interact really nice interactive programming with torch.",
            "And."
        ],
        [
            "You know, just as to run through some quick examples of what kind of common torch code looks like here will just make an identity matrix define a couple of scalars, and we can do scalar tensor math.",
            "With that we can take the Max and do clamping, and these are all really basic stuff, But if you're comfortable with NUM, PY or with Matlab, they should probably be very comfortable with torch kind of getting started should be very easy."
        ],
        [
            "We also have Boolean functions, so you can do masking so you can mask an array in the index into another array."
        ],
        [
            "We support all the special functions that you might want like besselian gamma and all geometric functions like a 10.",
            "Two those come from the Southeast Library, just as a note, NUM, PY actually implements these functions by linking into the Southeast Library as well, so we're kind of sharing a common scientific computing code base."
        ],
        [
            "And we also have.",
            "Are you able to sample from random distributions?",
            "We've got a whole set of the most common ones you might want to sample from here.",
            "I'm just sampling from a negative binomial, grabbing 10,000 samples and then using the I torch plotting functionality to plot a histogram of those values.",
            "So again, this should all be very familiar and comfortable to you if you're used to an umpire or Matlab."
        ],
        [
            "We also have inline help so you can take a torch function and then pre Pend a question mark and it will print out the documentation for that function.",
            "So if you're exploring through all these torch functions it's very easy to see what the call signatures might be.",
            "So you can begin to figure out how to use those functions.",
            "So."
        ],
        [
            "Torches in Lua, which is maybe it's most peculiar feature and Lua is an interesting language for a couple of reasons, and it's a good choice for machine learning for a couple of reasons.",
            "First is that Lua, as a language has very little overhead, specially compared to interpreted languages like Python And Matlab.",
            "There's a JIT compiled version of Lua called Lua JIT, and if you're familiar with Python, the way you should think about it is pie pie is to see Python as Lua JIT is to Lua.",
            "If you're familiar with those packages in Lua JIT, and in some extent Lou itself for loops are basically free there.",
            "They happen at C speed, so this is very unusual for an interpreted language.",
            "For you to kind of turn to writing a for loop before trying to vectorize an operation, but in Lua JIT and this is part of the package.",
            "This is code that's like running on.",
            "You know, large systems in GPUs and shipped in the torch library.",
            "There's just a big fat for loop right there, and there's actually really no speed penalty for doing that.",
            "If you're really a freak about speed, you can drop down into see and I'll get to that in a second and the whole language of Lou is about 10,000 lines of C, which is really small for a language, so and also the just the features of the language are also kind of minimal, so it's just a very simple, clean language that's very fast for an interpreted language.",
            "It's maybe one of the fastest interpreted languages that's out there.",
            "And it's really easy also to interoperate."
        ],
        [
            "With C and in fact Lua was designed from the very beginning to interoperate with.",
            "C was designed first of all to be a small and simple language, and second of all to easily be called from C code and make it easy for you to call into C code.",
            "There's something called the FFI or foreign function interface that's available in Lua.",
            "JIT in lua.",
            "That makes calling into C code, you know, look like basically plain Lua, so it's very, very easy for you to compile some C code and then begin to use that from Louis Olu is really in practice.",
            "This very thin layer on top of a whole ton of C code, so torch is a big C&CC library and loses this thin layer on top of it, so you're very close to the metal, so integrating with C code or Fortran code for that matter really doesn't require you to do any Cython or swig wrapping, which could be sometimes cumbersome to do that kind of gluing and.",
            "Python.",
            "It's really straightforward and actually was originally designed for this purpose.",
            "Was originally designed to interoperate with C, and it's used in a lot of places as an embedded language.",
            "What that means is there's a really high performance code, and then there's a part where you need to do some scripting so World of Warcraft, the graphics engine is in presumably see, but all of the scripting, like when the boss comes out of The Cave and all that stuff.",
            "That's all done in blue, and if you're writing macros for World of Warcraft, that's also in Lua.",
            "Adobe Lightroom is another program where the heavy lifting it's like a app for photographers.",
            "All the heavy image processing is done in CNC, but all the UI elements in the glue is done in Lua and then Redis and Engine X, Witcher 2.",
            "Programs for high performance web programming.",
            "There also implemented in C++, but all the scripting and the kind of the user facing script ability is done in Lua, so lots of different applications have chosen to turn to Lua to glue together high performance code, and that's kind of what we do a lot of in deep learning is we have some very efficient implementations of primitive operations and we want to glue them together somehow.",
            "Glue convolution to matrix, multiply for instance.",
            "In Lou is actually originally chosen four torch because there was a need to do embedded machine learning.",
            "So the story that I've heard is clyma far away.",
            "Who is one of the inventors of Torch was trying to put a convolutional neural net in an FPGA, which is like a reprogrammable circuit board so that he could have a confident on his bike helmet when he's driving around and identifying objects and getting that to work in Python is apparently a pain in the butt.",
            "I never tried it, but it was apparently very easy to do that in Lua, so that's kind of the Genesis of using Lua and Torch.",
            "And now Torch runs in all kinds of places, like on very large server farms at Twitter and on mobile phones, right?",
            "So it can kind of run anywhere."
        ],
        [
            "And as I mentioned, there's very easy integration into and from C and so this is production code calling into khudian enan.",
            "This Lua code for calling this function is about as long as if you were to just write it and see most of the craft here is just actually putting the types in the right order and everything.",
            "Really disting."
        ],
        [
            "Pushing feature related to Cody and N is that there's strong first class GPU support in torch and I think this is really differentiating factor.",
            "For torch you can just require this coup torch library and requires like Louis Import statement for Python And then all of a sudden you have access to this new type.",
            "This CUDA tensor type and all you have to do is cast your tensors using this member function CUDA and that tensor has been moved on onto the GPU and then everything you do with that tensor at that point happens a GPU operation.",
            "Right in the way this works under the hood is we have a whole lot of C implementations for code on the CPU, and then we have a whole lot of CUDA kernels that mirror the functionality of that, and so there's a lot of heavy work on the back end that you never see, so that things look very light and transparent and you could be running on the GPU and maybe not even know it.",
            "So it's really, really easy to get your code on the GPU, just switch types.",
            "I personally think that using types to do kind of multi device support and multiple dispatches probably.",
            "I think that's the kind of latest.",
            "And most elegant way of dealing with multi device support and in torch supports that.",
            "And Torch also has a very large community."
        ],
        [
            "So there's a lot of industrial contributors, but it is not industry owned and that's the really important distinction, at least for me.",
            "So NVIDIA and Facebook and Twitter all contribute a lot to the open source community, and we use these tools internally, but we don't own the direction of this tool, it's community driven.",
            "Facebook AI uses it for their research labs and we use it both for research but also for production.",
            "So all the deep learning that happens at Twitter happens in torch and so that means any image that might get uploaded to a torch piece of piece of source code is probably seeing that image.",
            "There's a ton of academic support as well."
        ],
        [
            "Another nice aspect of a very large and vibrant community, especially in the deep learning community in torch, is that as soon as a paper is published with some cool cutting edge result or with some new functionality, there's almost always a GitHub repository that pops up that actually implements that paper.",
            "Sometimes the paper comes out at the same time as the GitHub repository, so it's usually the case that you can find working code you can download and run for really cutting edge stuff that might have even coming out that day, and you're able to run that in torch, so there's a whole variety of.",
            "Of networks that are available for you just to download and run or modify or integrate into your own."
        ],
        [
            "Checked.",
            "A couple of examples.",
            "There's many different implementations of image capturing that you can download and run."
        ],
        [
            "There's also many different implementations of neural style transfer that you can download and run, so there's a lot of different flavors of this that people have just kind of for fun implemented, but also some more reference implementations of a publication."
        ],
        [
            "There's also neural conversation models.",
            "You can build a chatbot and torch by just downloading the code, and this is all available on GitHub already and then."
        ],
        [
            "In a lot of variants of generative adversarial networks you can download that code on torch.",
            "So where does torch fit in?",
            "Like the larger scientific computing order?"
        ],
        [
            "The science ecosystem.",
            "It's really clear that Python is kind of the biggest language when it comes to doing scientific computing and doing data analysis and data science defined however you want.",
            "Lua and Torch is much smaller than Python, however torch is disproportionately strong for its size in deep learning.",
            "And so you'll find a lot of resources for that specifically.",
            "And before I learned about Lou, I've been programming Python for about 10 years and switching to Lua from Python was very smooth, 'cause the languages are very similar and you can sit down and read the manual for the whole language in like an afternoon.",
            "So there's really not a lot to learn there in terms of becoming familiar with the language, so if you're worried about that, you should be able to pick it up very quickly.",
            "In terms of where torch fits with respect to deep learning libraries specifically the first kind."
        ],
        [
            "Going to preface it all with is that there's really no Silver bullet library.",
            "Each library has its strengths and its weaknesses.",
            "Depending on what you want to get out of it.",
            "So if you're really interested in running just convolutional neural networks in production with high throughput, then you should probably pick something like Cafe, but if you want to try weird architectures and do research, you might pick something more like torture, Theano, and now Tensorflow.",
            "I think as well.",
            "But really, each of these has its strength and minus is, I think, torches unique along with tensor flow, in that it's mostly used for research, but we've kind of at least a Twitter.",
            "We've demonstrated that can be used at scale in production, so if you're interested in building a startup around torch and running that on mobile phones, or running that on servers, that's something that has been done so you can do it as well.",
            "And development in torch in terms of adding new functionality or adding modules to the code."
        ],
        [
            "System we have like a core philosophy that I think most contributions follow, which is that.",
            "Programming and computing with torch should be really interactive.",
            "You should be able to try things out in an interactive setting to see if things fail very quickly.",
            "So that means there should be no compilation time, so this is a choice that torch makes that we won't be building a compiler to run your neural network as we would like to have our computation be interactive, and also we think that we prefer the imperative programming style.",
            "So that means the torch code should look like torch code and you shouldn't be writing in like a mini domain specific language or something kind of separate.",
            "So everything is also very close to the metal, so if you are running a given torch function and you're wondering about how it's implemented, or you know why it's slower, fast in a given case, if you go to GitHub and look up that function, usually in one or two different hop one or hops down the code, you can find the C code that's actually running it, so it's actually very easy to introspect what's going on underneath the hood, so if you're that kind of a person, kind of a hacker mentality and you want to understand what's really happening.",
            "Torches really great library for that.",
            "And also there's maximal flexibility, so composing different modules together is actually pretty easy.",
            "So that's kind of a high level overview of the torch core.",
            "And I think it would be good just to give an overview for like the fundamental data types and torch, you can have a sense of how things are happening on the computer when you're running code."
        ],
        [
            "So there's two fundamental data types in torch, there's a tensor and a storage, and torch is unique in that it's a D coupled this, whereas in NUM.",
            "Py it's it's actually fused, so a storage is a linear array of memory that just has a length and type, so each element is afloat or a double orucuta float, and a tensor is a view into that storage, right?",
            "So a tensor is an end dimensional view into that storage, that is row major in memory.",
            "So that means that you access the first row and then you get linear access to the next element of that row, and then you have to skip to the next row and so this tensor.",
            "Here is a four by six view into the underlying storage, with the stride of six by one.",
            "It means that as you go down the row you have to skip 6 elements in the linear memory in order to go down each row, and you have to skip just one to go across the columns."
        ],
        [
            "And Lua, like Matlab, is 1 indexed.",
            "So if we call the select function for instance on this tensor we're selecting from the first dimension the Rose, the third element, so that green row and we'll get back a tensor.",
            "What we're getting back actually is a view into the storage, right?",
            "So tensors are views into storage.",
            "So now we have two tensors pointing at the same kind of block of memory.",
            "Now, if we were."
        ],
        [
            "Two and also the offset is now 13, right?",
            "So that's a third property the tensor can accumulate from a storage."
        ],
        [
            "If we were to select from the second dimension, the third index will be grabbing that third column.",
            "Now we have a size 4 tensor with the stride of six.",
            "As we go linearly through this tensor, we need to skip 6 elements in the underlying storage.",
            "That's something to be aware of if you're trying to make really efficient code.",
            "If you induce a stride, you can slow down some of your operations.",
            "So."
        ],
        [
            "So how you actually instantiate this practically is we might grab a tensor a from a double tensor.",
            "That's 4 by 6.",
            "At this point our memory is completely uninitialized, so there could be anything in there.",
            "And then we can fill it with uniform noise an if we print the result.",
            "We've got some values in a that are uniform.",
            "And."
        ],
        [
            "If we select out that third row and call it B and we print that, we just got that third row in B."
        ],
        [
            "And if we fill B with some number like 3, recall that this new tensor is just a view on the same piece of storage.",
            "So what we've actually done is overwritten the values in a as well as a is just a view into the underlying storage, so this can be a gotcha.",
            "Sometimes it's maybe a little bit different from how you think about it in, NUM, PY.",
            "Sometimes these slicing operations induce copies in NUM.",
            "Py, kind of silently and in torch you really are kind of dealing with the computers memory.",
            "Kind of closer to the metal."
        ],
        [
            "And then, as I mentioned before, we have GPU support for all operations, so if you just require COO torch, which you get is this new type shows up in torch this CUDA tensor and that's just the float tensor for the GPU.",
            "So if you cast if you create a CUDA tensor uninitialized and fill it with uniform values, you can do exactly the same operation.",
            "So anything you can do on the CPU, you can do exactly the same on the GPU, so oftentimes moving a whole codebase from the CPU to GPU is just a one line change at the very beginning where you cast the types of the variables that you're.",
            "Operating on.",
            "So that's a quick view of the fundamentals of memory management in Torch and I'll talk at a very high level about kind of the libraries and approaches that you might use when training neural networks and torch so."
        ],
        [
            "So the.",
            "Training cycle for training model specifically in neural network and torch usually load data from your hard drive and then cue that up and then you might have some piece of code that coordinates a neural network.",
            "A cost function at the end of that neural network, and then an optimizer which can take gradients from that neural network and make updates to the parameters."
        ],
        [
            "And in the torch ecosystem there's three packages that can kind of handle this for you, so threads which I won't talk about and then will handle the specification of your neural network and your cost function and optim will handle the optimization of your neural network.",
            "So."
        ],
        [
            "CNN package handles specifying your neural network in your cost function and also calculating gradients."
        ],
        [
            "So Hugo yesterday talked about two different ways you might think about phrasing a neural network.",
            "The first was writing a set of nested mathematical expressions, kind of writing things down symbolically, and then another way that he phrased, specifying neural network was as a connected graph of nodes, where each node is a function that's taking in edges, which are data, and so you can think of a neural network as a directed acyclic graph of computation and really buys into this idea.",
            "Pretty heavily, so it's really easy to build neural networks in, and by composing different building blocks on top of each other.",
            "So it's really like snapping Legos together.",
            "So in this example we start with a sequential container, so we'll have a container in which we can snap blocks inside of it, and then here we'll just define a convolutional neural networks.",
            "A lot of convolution and non linearity, some pooling, some contrasted normalization in on in the network until the end of log softmax at the end, right?",
            "So it's kind of a linear set of constructions for building a neural network.",
            "This is a sequential container, so we're just adding blocks one after the other, so the computation proceeds."
        ],
        [
            "But we can also compose networks like Lego Blocks and other types of configurations, so have parallel streams in a network or branch them out, or concatenate them in.",
            "So these are all different types of containers you can play with."
        ],
        [
            "In the end, package and what we care about when training neural networks.",
            "Are there gradients in their outputs on the end.",
            "Package exposes 2 main methods that you can call on.",
            "The containers are also on the modules for pushing activations through network and then also for pulling the gradients from the loss back to the parameters and that the first is update output.",
            "So that will take output fed into a network and push it all the way through and then update grad input is a call that you will see which pulls the gradients all the way back.",
            "And sometimes you'll see at grad parameters which actually updates the parameters with the gradients.",
            "Um?",
            "And."
        ],
        [
            "We also have a corresponding package for an, then called Coonan.",
            "So with torch you just import coup torch and then you have this new type and with kuenen it's exactly the same thing, right?",
            "So every single layer that's been implemented in end has a CUDA equivalent, so all you have to do is require kuenen and then cast the model right there online 9 to the CUDA type and the whole network gets moved to the GPU and the network runs on the GPU at that point.",
            "So it's really just a one line change to move all of your computation to the GPU.",
            "Of course you have to cast your input type as well to CUDA tensor.",
            "So I've described like building linear graphs of computation, but you might want to do something more complicated."
        ],
        [
            "There is something called the NN graph package.",
            "So instead of clicking Lego blocks together, you will instantiate a module and wire it to some other module as long as it's a cyclic and this let's."
        ],
        [
            "Build more complicated directed acyclic graphs of computation in your neural network.",
            "We don't really use this at Twitter, we use a pack."
        ],
        [
            "Is called torch autograde.",
            "That's because I think that explicitly building your compute graph can get a little cumbersome.",
            "We say OK, give me this module and wire to this one.",
            "What's more natural, at least to me, is just to write down the code that you want implemented.",
            "So just write in an imperative style.",
            "I want to take the 10 H of this output, and then I want to take the output of that tension.",
            "Do something else with it, so I think code written in this imperative style is a lot more readable and allows me to kind of iterate a lot faster, and so we use this torch autograph package that I'll talk a bit more about later.",
            "To implement automatic differentiation on arbitrary compute graphs.",
            "So I'll discuss that a bit more."
        ],
        [
            "I'll talk briefly about the opt in package without going into too much detail, so the opt in package implem."
        ],
        [
            "It's a whole host of 1st order optimization methods including our friends to casted gradient descent, which is always with us and some line search methods like elegs and then some newer things like Adam an RMS prop."
        ],
        [
            "And optimism is a little kind of changes the way that you think about writing your code just a little bit.",
            "So there on the bottom there.",
            "I've got that optim SGD function, and it's got three input parameters.",
            "Will go kind of from right to left, so the config is like the learning rate and momentum.",
            "The things that parameterized your optimization and then X is the parameters of your neural network and then the first input is a function that takes in as an argument your parameters and then outputs the Lausanne the gradients.",
            "So you'll notice that nowhere here is data loading or the specification of the model that all gets closed over by this function.",
            "So in that function need to load your data and you need to evaluate a neural network and grab the grab the gradients so it can be kind of a rearrangement of how you ordinarily think about writing this type of code.",
            "But once you have it written in this style, you can replace SGD with Adam with deleting 3 characters and adding four, and so you can actually.",
            "Sometimes it's so easy you can treat the optimizer as like a categorical variable to optimize the hyperparameter.",
            "If that's something that you're interested in doing."
        ],
        [
            "Another thing to think about when using the optim package is that it expects that all the parameters that you're trying to optimize or contiguous in memory, and this happens automatically when you're using torch NN.",
            "So the weights and biases of layer one or right next to the weights and biases of layer two in the memory and the storage for that neural network.",
            "But if you're optimizing something that's not in end, you'll have to kind of pack all of your parameters together and provide it to end and to Optum, so that's something just kind of consider.",
            "So NN and Optum together are kind of like packages that take advantage of torch.",
            "Building the foundation of Torch to provide neural networks and the ability to optimize those neural networks on both the CPU or the GPU, and it's kind of a high level overview of what's available and what the functionalities are, so that's the first half of the talk or the first part of the talk."
        ],
        [
            "I'd like to switch to talk a bit more about this package torch autograde, so yeah.",
            "Explain the punctuation of optimal or what did I do?"
        ],
        [
            "Yeah, so you're passing in a function here, and this function is spitting out the loss and the gradients, and so you have to close over the model so the model has to be an up value that's declared outside of the function.",
            "So it's just a different style of writing.",
            "Instead of writing things in an object oriented style, you're kind of using a closure style, which you can do the same thing in both styles is that I don't know if that answers your question.",
            "Yeah, OK."
        ],
        [
            "So just a show of hands who's heard of automatic differentiation.",
            "OK, everybody has anybody heard of Autograde the package before?",
            "OK, so some folks cool so autograde is Twitters.",
            "It's really an industrial strength but extremely flexible implementation of automatic differentiation and we try really crazy ideas and autograde.",
            "So instead of writing computation as a linear set of blocks you can really write anything down any numeric function and expect to be able to get the gradients of that function.",
            "I was inspired by the original version in Python from Ryan Adams Group at Harvard Dougal Mclaurin.",
            "David do venonat.",
            "Johnson were the kind of original developers of the Python version.",
            "So just taking kind of a step."
        ],
        [
            "Back to appreciate the before I get into automatic differentiation to appreciate this stable abstractions that all of machine learning is built on so we don't really think about arrays anymore, and that was kind of solved in the late 50s.",
            "We don't really think about how to call linear algebra routines that was solved by lapack and we kind of take for granted the idea that we shouldn't really be programming doing scientific computing and less.",
            "We have all of the kind of primitive functions that we would want like.",
            "I'm not going to re implement my own exponential.",
            "I'm not going to reimplement sampling from.",
            "You know a normal distribution kind of take advantage of that.",
            "All should just be there in Matlab and NUM.",
            "Py kind of gave us that expectation and we really should take these abstractions for granted in order for us to stay same like we don't want to go back down to the networking stack and reimplant reimplement everything by hand, we really should kind of build on top of these really strong abstractions."
        ],
        [
            "But machine learning has has other abstractions that I think are still kind of nascent that I'm really interested to see how how they kind of evolved in solidify.",
            "I would say that automatic differentiation really is a V abstraction for gradient based machine learning.",
            "Automatic differentiation.",
            "It's a process that mechanically calculates derivatives as functions expressed his computer programs at machine precision, an with complexity guarantees, so there's like 3 parts to that statement.",
            "The first is that auto differ.",
            "Just a D should take in as an input.",
            "A program you can run that will give you a number output and it should output a new program that will give you the gradient as well as the original output.",
            "So it's a transformation of a program you can run.",
            "It has machine precision, so automatic differentiation guarantees that your new function that calculates gradients will have the same numerical stability as the original function, right?",
            "And also it has complexity guarantees, so it'll just be some constant factor, so some it will be at most three times the complexity of the original function.",
            "This distinguishes it from finite differences, so there was a question in Hugo's talk about why can't you just train neural networks using finite differences.",
            "Apparently people actually did this in the 60s, so this book called Talking Nets in an interview with Woodrow, which is one of the chapters.",
            "It's a book of like interviews with people that were luminaries in neural networks.",
            "It's a great book if you haven't read it.",
            "Woodrow described actually training neural networks with finite differences and kind of the pain and difficulty of having to train neural networks in that fashion because they're incredibly numerically stable.",
            "To do that, the process of finite differences means perturbing your input a little bit, and then passing it through a whole composure of a bunch of functions.",
            "Which increases the numerical instability.",
            "So kind of exacerbates the problem is also very inefficient, is not symbolic differentiation, so it's not something that you would like type in a mathematical expression to Mathematica and say, give me the partial derivative of this expression.",
            "There's no complexity guarantee with that, so for highly nested functions, which is every neural net.",
            "The size of the expression you get back from naive symbolic differentiation can swell very, very quickly, and that's not the case with automatic differentiation."
        ],
        [
            "So this idea has been rediscovered several times.",
            "There's kind of a review paper there.",
            "The first implementation that I'm aware of where the input to auto diff is a program in the output is a program.",
            "Is this guy Burt Steele pending thesis in 1980 he wrote this thesis that describe auto diff in perfect clarity and then just disappeared for like 30 years and then kind of gave a talk at some auditing conference.",
            "At some other point.",
            "So he's just disappeared from the field.",
            "There was an earlier implementation of variant called Forward mode that I'll describe about 15 years earlier.",
            "It was, I guess, popularized in connectionist machine learning as back back propagation by Rumelhart.",
            "Although there is some debate as to who came first and who is responsible for the invention of this algorithm, but one thing to note is that in other fields like nuclear science and atmospheric Sciences like meteorology and computational fluid dynamics, these guys use a D all the time, and in fact there are tools for doing for using ADR.",
            "Actually a lot more sophisticated than the ones that we have in machine learning, so I think we actually have a lot to important to learn from these different fields.",
            "They have been happily humming along and using a D to tune their models to train them."
        ],
        [
            "And also I would make a distinction that we don't use a D. We use reverse mode AD, so there's there's."
        ],
        [
            "2 main modes of automatic differentiation.",
            "Two ways of calculating derivatives or too extreme variants.",
            "The first is called Forward Mode, which will almost never seen machine learning 'cause it's a really bad idea and I'll explain why it's a bad idea in machine learning and then reverse mode, which is synonymous with backpropagation.",
            "And these are really just different orders of applying the chain rule.",
            "So."
        ],
        [
            "So here's like a symbolic view of forward mode automatic differentiation.",
            "So say we've got a neural network that takes in an image and predicts the probabilities of classes.",
            "That's a composition of functions, and so we have this chain of partial derivatives that we want to calculate the derivative of the loss with respect to the parameters, and so we need to apply the chain rule to those partial derivatives in the forward mode.",
            "With this we can apply the chain rule in any direction that we like, right?",
            "So we can start from the left and go to the right.",
            "You can kind of see immediately that that's a bad idea 'cause you have these big matrix matrix multiplies.",
            "So going left to right is a bad idea, especially when it is a bad idea when you're going from many parameters to a scalar loss, right?",
            "It's actually a good idea when you're going from a small set of parameters to a lot of numbers, right?",
            "And that's the case when you want like vector flow fields or something like that, but you never do that an optimization because the output is always some scalar loss."
        ],
        [
            "We can view this in a different way as a like a program view, so here's a program we want to differentiate, so we have FAB&C if B is greater than C, then return a times the sign of B, otherwise return a + B * C. You can back propagate through this.",
            "There's an if statement, but we can actually kind of ignore it and then we can ignore it is by just recording the trace of the operations that were actually run by the program and this is called a Wengert list.",
            "So we first assign A to three to A and two to B and then one to see and then we basically erase the IF statement because that's not, it's not a numerical function an we return.",
            "Eight times the sign of be right, so this is a trace of this program, you know, a run through this program and if we wanted to do the forward mode of backpropagation with this program, we would define AS 3 and then we instantiate the derivative of a with respect to a.",
            "So we're starting the chain rule from the left, right?",
            "And two is B and then the derivative of be with respect to a is zero and see is 1 and then its derivative.",
            "See with respect to a 0.",
            "And then we'll run D and then we'll get the derivative of deal with respect to a 'cause we have all the information at that point.",
            "This is a really, really simple example.",
            "But I think you can appreciate that if you wanted to get the derivative of the youth respect to be, you'd have to do a whole another sweep through this program.",
            "You have to initialize derivative of be with respect to be as one.",
            "This is really bad for neural networks, so if you have a million parameters I mean you have to do a million sweeps of forward mode automatic differentiation in order to get the gradients that you need to make an update.",
            "So this is a really bad idea.",
            "However if you had some other function of a B&C that you wanted to calculate partial derivatives for.",
            "At the very end of this evaluation, you have that all available to you, so if you have fan out in your computation graph, so if you go from very few parameters to very many Ford modes, a good idea, it's just not something that happens in machine learning a lot."
        ],
        [
            "So reverse mode is the same idea, except for evaluating the chain rule from right to left.",
            "So we've got these nice matrix vector products that keep things small."
        ],
        [
            "Much better complexity when you have a scalar valued output.",
            "And we can build a Wengert list for the same function using reverse mode.",
            "So since we're going from right to left, we actually need all of the.",
            "Need the whole trace to be available before we can start calculating the gradients, so we'll define AB&C&D and that's our our trace of this program.",
            "And then when we want to calculate the derivatives or do backpropagation, will actually run the whole program and then start the derivative of the with respect to D as one and then build up our gradient from the right side of the chain rule all the way to the left.",
            "So we actually need to tape the whole program.",
            "We need to record the whole program before we can do back propagation.",
            "And then we can return the gradients there.",
            "I think you can appreciate here if we want the gradients of D with respect to be.",
            "That's just an extra single step here.",
            "Whereas before we have to go through the whole program again, and as your program becomes larger and more complicated, this becomes a more severe penalty in forward mode, but definitely not in reverse mode.",
            "So that's kind of the the differences between forward mode in reverse mode and why you really never see forward mode in machine learning and why you see reverse mode.",
            "And that's back propagation.",
            "So autumn torch Autograde is an implementation of reverse mode auto diff."
        ],
        [
            "And so this is an entire function for training a three layer MLP in torch autograde.",
            "So we'll define the parameters at the top.",
            "Define our neural network is nonlinearities interspersed between matrix multiplies and additions, and then a loss function at the end and then this."
        ],
        [
            "Is really the entire T of the API of Autograde, so you import auto grad and then you call grad on the function you would like the gradients of and it gives you back a new function and then down at line 24 will actually call that new function and instead of returning the loss, it will return the gradients of the loss with respect to the parameters.",
            "So we've just transformed this function into a function that will now give us the gradients for free so we don't have to really worry about writing down any partial derivatives or doing any math, so as long as your function is differentiable.",
            "Autograde will give you back the gradients of that function."
        ],
        [
            "So what's actually happening under the scenes is as this function is running.",
            "Like I said before, we're actually keeping track of the whole tape.",
            "So on the first line we're doing matrix multiply, and so we're going to save W an input and the function as well as the output, and then when we add each one in Paramus dot B to make H2, we will save all that information as well.",
            "So we're saving the results of this compute graph, kind of behind the scenes while you're running this numeric function and the way we do this."
        ],
        [
            "This Villa operator overloading, so for every function in torch we're going to overload that function with one that keeps track of computation, builds up a linked list of computation, so will save the original function, and then in our new redefinition of torch, some will check if there's a special type being passed to the function, and if not, will just return the original function, and if not, if there is a special type, then will unpack that type, grab the output value, and then pack it back into this special type.",
            "And this special type keeps track of the function that was run.",
            "The arguments that were passed to that function and then the outputs, because really, that's all you need along with the gradient that's coming in to calculate the.",
            "Partial derivatives at that particular step.",
            "So."
        ],
        [
            "We keep track of this compute graph, and then when we need to evaluate the partial derivatives, we have access to all of the local information we'd need to calculate that gradient to pass it back down to compute function was actually."
        ],
        [
            "Happening when we calculate gradients back down, the compute graph is we're looking up in a big big table.",
            "All of the partial derivatives for every torch function that is available.",
            "So we written all these partial derivatives by hand so that you can compose them together and really any way that you please right?",
            "So if we hit a square root function in this compute graph, would just go look up the gradient of the first argument of square roots, only one argument, and then we can use that to calculate the gradient and then propagated through the graph.",
            "So behind the scenes we've done all this work to have individual partial derivatives.",
            "And then by using the chain rule in reverse mode, auto diff, we can actually calculate the gradients through an entire function that's been composed together.",
            "Yep.",
            "So.",
            "If I want to back up to the Internet, but I will have to use the grading function for each of the functions for each of the components.",
            "So is there somewhere I can just call one function and it will automatically compute gradients through the entire network?",
            "Yep, that's that is the grad function, so I'll show you in a little bit, but we've exposed the ability to use NN modules, so if you have a network that you've written end torch NN, you can wrap that and use that in auto grad and then you can combine networks however you want, so you could plug a CNN into an LTM and do image captioning.",
            "An autograph will just figure out how to calculate the derivative through all of that so you can compose anything an autograph will take care of calculating the derivative through those functions.",
            "Calling the function and loss that was the last part of the network, so I thought you have to call back for every part of the network loss.",
            "Actually, let me show you that again.",
            "Whoops"
        ],
        [
            "So.",
            "Loss is actually calling into the neural network function.",
            "So when we go into that function in called neural network, which is defined up, there were actually taping the contents of that function as well.",
            "So we're keeping track of everything that's running all the functions that are kind of recursively called.",
            "Do do."
        ],
        [
            "Do do do do do do.",
            "Um?",
            "So just show you a couple of examples, kind of increasing in complexity of what you can do with autograde, so of course we can back propagate through really basic arithmetic.",
            "You just write that down, you know.",
            "Would using normal touch code you can get the gradients of that arithmetic expression."
        ],
        [
            "Like I mentioned before, you can have control flow, so you can just have an IF statement there, not have to worry about phrasing it as a Condor, doing anything really complicated like you have to do in maybe Theano or Tensorflow.",
            "You can just write plain code and AutoCAD will figure out the derivatives of this code for you."
        ],
        [
            "And I've been using scalars, but of course this all works with tensors, right?",
            "So wait, wait several network are matrices and everything is works for matrices as well as scalars.",
            "So."
        ],
        [
            "I think Autograde really shines when you start to use more complicated to control flow, so a for loop for instance, is trivial.",
            "In autograph, you just write a for loop and will take the gradients with all the way through that for loop.",
            "True, re phrasing of your computation, you have to do.",
            "There's no restrictions.",
            "This could be a nested for loop.",
            "You could nested five times.",
            "It doesn't matter.",
            "Auto grade will figure out the gradients."
        ],
        [
            "So one thing that I actually don't think is possible in any other automatic differentiation package is ability to call yourself recursively or to have recursive control flow.",
            "So this actually implements the same thing as the previous for loop.",
            "So here I'm just multiplying a by itself and storing that as A and I'm going to do that B times here.",
            "I have a function where if B is zero, return a, otherwise.",
            "Take the F 8 * A and decrement B so it does the same thing.",
            "It just uses a recursive calling style, but I'm calling F inside of F, right?",
            "So I have this recursive function, but autograde has no problem with that, so you can take gradient through recursive functions and you could, you know, compose all these things together and if statements for loops or cursive function calls and the gradients will just be figured out for you.",
            "Don't really have to worry about the specifics there, so it allows you to really naturally phrase your computation.",
            "1."
        ],
        [
            "Thing that I think a little bit neat is adding your own gradients or playing with the gradients is actually pretty easy to do an autograph.",
            "So why would you want to do that?",
            "Gradient clipping?",
            "For instance, you might want to have a function that on the forward passes the identity function, but on the backwards passes a clamp.",
            "Make sure that your gradients don't grow too much, so you can easily make a function in autograde that has a partial derivative that doesn't look like the actual partial derivative of the forward pass.",
            "It's pretty easy, so in this example I'm taking the sum of the floor of eight of the third, and floor is a rounding operation, so its gradient is zero almost everywhere.",
            "And the gradients passing through this function are correctly 0, right?",
            "So this is bottleneck that's crushing the gradient.",
            "Maybe I want to back propagate through floor, like for instance the JPEG compression algorithm has a quantization step and maybe I want to take the gradients of the JPEG compression algorithm 'cause I'm interested in doing that for instance so I can make the gradient floor disappear really easily so."
        ],
        [
            "Will require autograde.",
            "I'll make my own special module and I'll add a floor method to my special module.",
            "An internally floor is just calling torched out floor like before and then I'll overload my module that I call special and I'll define the gradient of floor inside that module and I'll have the gradient.",
            "Just return G. So just passed through the gradient as a floor doesn't exist and then I can call."
        ],
        [
            "This original function actually have gradients passed through that, so it's really trivial to play with the kind of internals and the mechanisms of automatic differentiation in autograde, so you can kind of imagine crazy ideas and just implement them very, very quickly.",
            "Um?",
            "So those are a couple of examples of using autograde in kind of the more exotic features that differentiate it from the alternatives.",
            "So what?"
        ],
        [
            "Is actually differentiating these different neural network libraries or what's 1 axis along which all these different libraries are separate?",
            "One way I like to think about it is the granularity at which they implement auto def, right?",
            "So something like scikit learn you get a whole model and you actually can't even compose them, so you have a whole model and that's it.",
            "Was something like torch NN you get these big layers and you compose the layers like Lego blocks, but you have no access to the operations that are inside those Lego blocks.",
            "And what I would call full automatic differentiation which is implemented by Autograde in Theano and Tensorflow.",
            "You can use really any operation in a host language and autograph that host language happens to be torch and Theano and Tensorflow.",
            "That host language is a language inside of Python, which I think is a distinction that needs to be made.",
            "And."
        ],
        [
            "Practically, this level of granularity is enforced by what partial derivatives the developers happen to expose to you, right?",
            "So you get layers and you can only compose layers because those are the partial derivatives that the developers of those libraries have exposed to.",
            "In order for you to compose them together to use an automatic differentiation.",
            "But sometimes people spend a lot of time implementing a very efficient layer, like a convolution on the GPU that engineers add in video.",
            "Or Nevada systems have spent a long time making really really fast, and you want to use that layer, but you have some weird loss function and there is no fast implementation of that.",
            "But you just need to correct implementation.",
            "So perhaps you want to compose these different styles together so we."
        ],
        [
            "We don't want any limits on how what style we need to use, and we can actually write neural networks in an arbitrary style in autograde, so he."
        ],
        [
            "Just kind of three different styles of writing a three layer multilayer perceptron using Full Auto diff.",
            "So this is the most granular level where we're defining all of the.",
            "Parameters ahead of time and then we're using torch functions to do the matrix multiplications in the tenacious and the loss function."
        ],
        [
            "We can also pop one level up in this kind of get your question, which is in autograde.",
            "We have this submodule called NN so it's like Autograph Dot North and inside of that module or every single engine module that's normally exposed in torch NN.",
            "So you can use everything that has been implemented kind of prior.",
            "There's a ton of work that's gone into that and so you can use a functionalized version of all these modules where you might instantiate, for instance end linear and you get back a function which implements the linear layer.",
            "And the parameters for that linear layer.",
            "And so you just build up your whole network here and then you can call these functions where the first argument are the parameters in the second or the inputs and you can compose these as big layers here, so you can use codeine and convolutions here, along with whatever arbitrary torch could you please."
        ],
        [
            "And also sometimes you just want a neural network or sometimes you just want a continent and auto grad.",
            "We have a module submodel submodule where you can just grab entire networks.",
            "So if you want to compose a CNN with analysis TM or build a generative adversarial network and composed two networks together, that way it's very easy to grab the whole network and then compose them together.",
            "Um?",
            "So that's kind of the high level.",
            "That's like some tour.",
            "A tour through torch autograde"
        ],
        [
            "At Twitter, practically this has allowed us to kind of prototype without fear, so we can dive into really weird ideas without worrying about getting bogged down in the details.",
            "So you can imagine some crazy loss function and just write it down in plain code and expect to get the gradients correct the first time.",
            "What that means practically is we do try crazier, potentially high payoff ideas more often, because it's kind of free to get the gradient so we don't have to worry about doing that context switch where you go from programming mode to math mode, which can actually kind of be painful, at least for me sometimes.",
            "And also, once you're done training your network, autograde just completely disappears.",
            "It's just regular numeric code at that point that you can just run just as fast as you know, as if AutoCAD didn't exist.",
            "We've used autograph to train very large models that run on a lot of media that's uploaded to Twitter and then in terms of like the speed penalty, which is probably one of the main criticisms of autograph, it's usually fast enough.",
            "We do have an optimized mode that's a sensually as fast as using regular North.",
            "It has some caveats that I can get into if you're interested in that.",
            "And that's effectively nearly a compiler.",
            "That's kind of happening at runtime, so it's a just in time compiler for neural networks.",
            "So in autograph is allowed us to really try new stuff a lot faster, and it's one of the first tools that we reach for.",
            "There's a lot of other."
        ],
        [
            "And if ideas that haven't made their way from nuclear science and atmospheric science, computational field dynamics into machine learning, and there's a lot of really cool ideas that could accelerate the pace of development.",
            "But also the speed at which networks are trained.",
            "So here's a couple ideas that I think would be interesting to look into.",
            "The first is checkpointing, and this is actually been implemented, I think only by MX net at this point in the idea here is you don't actually need to save all the intermediate computation in order to back propagate.",
            "You might actually delete.",
            "Or not, save some of your intermediate results and then recompute them as you need them, and this can save you a ton of memory.",
            "So if you just decided to save every tenth layer, for instance in a Resnet, and recompute 10 layers at a time when you're doing your backwards pass, you pay in computation, but you can save enormously in memory.",
            "It turns out to be a little tricky to implement this well, and solving this like fully is an NP hard problem, but there's good heuristics for checkpointing schedules.",
            "Also potentially is faster.",
            "For instance, you can throw away the intermediate results from like nonlinearities or pointwise functions, because you confuse those operations in CUDA, and so it might be actually cheaper to just do the computation again, then load all that from memory onto the registers that you're doing your computation with.",
            "Another idea is mixing forward mode in reverse mode, so I mentioned forward modes.",
            "Really bad forward mode is really bad for doing your entire network as you start with many parameters and you end up with one scalar loss, but in the middle your graph could be any shape.",
            "So if your graph starts from something small and blows up something very large.",
            "If you're generating an image for instance, and then you're coming down to a scalar loss, you had this diamond shaped network.",
            "You might imagine actually doing reverse mode into the explosion point and then doing forward mode up until that point.",
            "And kind of merging the results.",
            "So you don't have to just do forward mode or reverse mode.",
            "You can mix the two.",
            "There's a name for it, it's called Cross Country elimination.",
            "Again, it's tricky to implement, but there is potentially giant speedups that are available using technology that just hasn't made it into machine learning at this point.",
            "Stencils, I won't talk too much about, but you can phrase every element wise computation as a stencil, which is like a convolution or a blur.",
            "You can talk about those stencils implementing gradients for those efficiently and automatically is tough, and some people in computer graphics do work on this.",
            "So Brian Gunter and Zack de Vito have some work on this, that's I think pretty compelling, could result in big speedups.",
            "Source to source compilation, I think is also a very interesting idea.",
            "So what I've described to you is what's called a tape based reverse mode, automatic differentiation, where we're recording the computation that occurs at runtime, and then reversing that computation to get the gradients at runtime.",
            "You could imagine, however, looking at the whole source code and then printing out the gradients for that source code next to it, concatenating them, and then returning a new string of a function, and then having that be your gradient.",
            "So that's called source to source transformation.",
            "That's the original way that auto diff was implemented.",
            "You know, back in the 80s and Fortran and people haven't really done this a lot lately, because operator overloading is a whole lot easier than building like Alexa and a parser in compiler.",
            "So reverse mode with the tape is just easier to implement.",
            "However, it is the gold standard for performance in these other fields that make use of automatic differentiation a lot, so I think we're actually missing out on potentially a really fast technique for doing auto diff.",
            "The challenge there is how do you reverse an if statement?",
            "How do you reverse a for loop?",
            "These are challenges that kind of have to just be tackled.",
            "Also, higher order gradients, so if you want the Hessian, you might want to phrase that's taking the grad of the graph of F, and indeed this works in Python autograde and with some caveats and torch autograde, but there's really not that many efficient implementations, so I think the exploration with second order methods or habitual explanation of exploration of 2nd order methods would really expand if it was a lot easier to get Hessian vector products.",
            "Right now it's just a pain, you just have to spend a lot of time thinking about that there's not many efficient implementations of this.",
            "But there are libraries that will do this for you.",
            "But I think there's room in making this something that's really fast.",
            "You actually want to reach for and like.",
            "Try to experiment with."
        ],
        [
            "It's actually really easy to get started with all this stuff, anacondas kind of the defacto distribution for scientific Python And all of this is now installable an Anaconda, so if you have Anaconda installed, it's a one liner there online.",
            "9 to get all these packages and start playing with it.",
            "So that's all the material that I have."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this talk has two kind of main parts.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Part is just torch basics, so just using torch for doing scientific computing in an overview of how to train neural networks with torch.",
                    "label": 0
                },
                {
                    "sent": "And I'll kind of stay at a high level when discussing that and then point you to some material if you really want to dive in.",
                    "label": 0
                },
                {
                    "sent": "And of course there's the practical session and torch that comes afterwards.",
                    "label": 0
                },
                {
                    "sent": "If anybody is interested.",
                    "label": 0
                },
                {
                    "sent": "And in the second half will talk more about automatic differentiation, which we've heard a couple talks touch on as back propagation, but I'll go into a little more detail and then torch autograde, which is Twitter's kind of opinionated implementation of automatic differentiation, and I'll talk a bit more about.",
                    "label": 0
                },
                {
                    "sent": "What's gone into that and what it enables when you kind of take particular opinion of how you should implement automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "Seriously, so before I get started, who has heard of the language Lua?",
                    "label": 0
                },
                {
                    "sent": "OK, it's good amount who is heard of Torch?",
                    "label": 0
                },
                {
                    "sent": "Probably the same people.",
                    "label": 0
                },
                {
                    "sent": "And who has used torch?",
                    "label": 0
                },
                {
                    "sent": "OK, so a little bit less.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe like a third.",
                    "label": 0
                },
                {
                    "sent": "OK cool so I think some of these details might be interesting too that haven't used it to kind of get a sense of what it's all about and those that do know it might be just a good refresher.",
                    "label": 0
                },
                {
                    "sent": "So torch is.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An array programming library for Lua.",
                    "label": 1
                },
                {
                    "sent": "It looks a lot like Matlab because it was directly inspired by Matlab.",
                    "label": 0
                },
                {
                    "sent": "It just happens to be in a different language with different kinds of tradeoffs, and because it looks like MATLAB, it also looks a lot like NUM py, so if you're familiar with any of these array programming libraries, you'll probably be very comfortable working in torch and will be able to get up to speed very quickly if it's something that interests you.",
                    "label": 0
                },
                {
                    "sent": "So Torch is an interactive.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scientific computing framework in Lua, and this is just like a really quick take on what looks like.",
                    "label": 1
                },
                {
                    "sent": "It looks very, very similar to Python, so we've got strings like you would want them.",
                    "label": 0
                },
                {
                    "sent": "You can print stuff, there's only one associated data type in Lua, and that's the table, so you can build lists and dictionaries and sets out of this one data type.",
                    "label": 0
                },
                {
                    "sent": "But really, there's just one kind of single associated data type in Lua, and of course we've got four loops and all that kind of basic stuff.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Torch itself has 150 or so tensor functions, so it's kind of the same level of coverage is MATLAB and NUM py for doing linear algebra.",
                    "label": 1
                },
                {
                    "sent": "And of course convolutions because Torch has a really strong focus on deep learning or some of the packages do tensor manipulation, like reshaping, slicing everything that you might expect from an array library.",
                    "label": 1
                },
                {
                    "sent": "And then logical operators for doing masking and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "And a whole bunch more.",
                    "label": 1
                },
                {
                    "sent": "And it's all fully documented on GitHub.",
                    "label": 0
                },
                {
                    "sent": "These slides are on the MHW Dem GitHub page, so if you're interested in following along there on that summer school project under the torch directory.",
                    "label": 0
                },
                {
                    "sent": "The kind of similarities between torch, an Matlab and NUM py extend.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beyond just the functions that they implement, there's also nice plotting everything can be done inside of Jupiter or Ipython notebook, so you can do interact really nice interactive programming with torch.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, just as to run through some quick examples of what kind of common torch code looks like here will just make an identity matrix define a couple of scalars, and we can do scalar tensor math.",
                    "label": 0
                },
                {
                    "sent": "With that we can take the Max and do clamping, and these are all really basic stuff, But if you're comfortable with NUM, PY or with Matlab, they should probably be very comfortable with torch kind of getting started should be very easy.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have Boolean functions, so you can do masking so you can mask an array in the index into another array.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We support all the special functions that you might want like besselian gamma and all geometric functions like a 10.",
                    "label": 0
                },
                {
                    "sent": "Two those come from the Southeast Library, just as a note, NUM, PY actually implements these functions by linking into the Southeast Library as well, so we're kind of sharing a common scientific computing code base.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also have.",
                    "label": 0
                },
                {
                    "sent": "Are you able to sample from random distributions?",
                    "label": 0
                },
                {
                    "sent": "We've got a whole set of the most common ones you might want to sample from here.",
                    "label": 0
                },
                {
                    "sent": "I'm just sampling from a negative binomial, grabbing 10,000 samples and then using the I torch plotting functionality to plot a histogram of those values.",
                    "label": 0
                },
                {
                    "sent": "So again, this should all be very familiar and comfortable to you if you're used to an umpire or Matlab.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have inline help so you can take a torch function and then pre Pend a question mark and it will print out the documentation for that function.",
                    "label": 0
                },
                {
                    "sent": "So if you're exploring through all these torch functions it's very easy to see what the call signatures might be.",
                    "label": 0
                },
                {
                    "sent": "So you can begin to figure out how to use those functions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Torches in Lua, which is maybe it's most peculiar feature and Lua is an interesting language for a couple of reasons, and it's a good choice for machine learning for a couple of reasons.",
                    "label": 0
                },
                {
                    "sent": "First is that Lua, as a language has very little overhead, specially compared to interpreted languages like Python And Matlab.",
                    "label": 0
                },
                {
                    "sent": "There's a JIT compiled version of Lua called Lua JIT, and if you're familiar with Python, the way you should think about it is pie pie is to see Python as Lua JIT is to Lua.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with those packages in Lua JIT, and in some extent Lou itself for loops are basically free there.",
                    "label": 0
                },
                {
                    "sent": "They happen at C speed, so this is very unusual for an interpreted language.",
                    "label": 0
                },
                {
                    "sent": "For you to kind of turn to writing a for loop before trying to vectorize an operation, but in Lua JIT and this is part of the package.",
                    "label": 0
                },
                {
                    "sent": "This is code that's like running on.",
                    "label": 0
                },
                {
                    "sent": "You know, large systems in GPUs and shipped in the torch library.",
                    "label": 0
                },
                {
                    "sent": "There's just a big fat for loop right there, and there's actually really no speed penalty for doing that.",
                    "label": 0
                },
                {
                    "sent": "If you're really a freak about speed, you can drop down into see and I'll get to that in a second and the whole language of Lou is about 10,000 lines of C, which is really small for a language, so and also the just the features of the language are also kind of minimal, so it's just a very simple, clean language that's very fast for an interpreted language.",
                    "label": 0
                },
                {
                    "sent": "It's maybe one of the fastest interpreted languages that's out there.",
                    "label": 0
                },
                {
                    "sent": "And it's really easy also to interoperate.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With C and in fact Lua was designed from the very beginning to interoperate with.",
                    "label": 0
                },
                {
                    "sent": "C was designed first of all to be a small and simple language, and second of all to easily be called from C code and make it easy for you to call into C code.",
                    "label": 0
                },
                {
                    "sent": "There's something called the FFI or foreign function interface that's available in Lua.",
                    "label": 0
                },
                {
                    "sent": "JIT in lua.",
                    "label": 0
                },
                {
                    "sent": "That makes calling into C code, you know, look like basically plain Lua, so it's very, very easy for you to compile some C code and then begin to use that from Louis Olu is really in practice.",
                    "label": 0
                },
                {
                    "sent": "This very thin layer on top of a whole ton of C code, so torch is a big C&CC library and loses this thin layer on top of it, so you're very close to the metal, so integrating with C code or Fortran code for that matter really doesn't require you to do any Cython or swig wrapping, which could be sometimes cumbersome to do that kind of gluing and.",
                    "label": 0
                },
                {
                    "sent": "Python.",
                    "label": 0
                },
                {
                    "sent": "It's really straightforward and actually was originally designed for this purpose.",
                    "label": 0
                },
                {
                    "sent": "Was originally designed to interoperate with C, and it's used in a lot of places as an embedded language.",
                    "label": 1
                },
                {
                    "sent": "What that means is there's a really high performance code, and then there's a part where you need to do some scripting so World of Warcraft, the graphics engine is in presumably see, but all of the scripting, like when the boss comes out of The Cave and all that stuff.",
                    "label": 1
                },
                {
                    "sent": "That's all done in blue, and if you're writing macros for World of Warcraft, that's also in Lua.",
                    "label": 0
                },
                {
                    "sent": "Adobe Lightroom is another program where the heavy lifting it's like a app for photographers.",
                    "label": 0
                },
                {
                    "sent": "All the heavy image processing is done in CNC, but all the UI elements in the glue is done in Lua and then Redis and Engine X, Witcher 2.",
                    "label": 0
                },
                {
                    "sent": "Programs for high performance web programming.",
                    "label": 0
                },
                {
                    "sent": "There also implemented in C++, but all the scripting and the kind of the user facing script ability is done in Lua, so lots of different applications have chosen to turn to Lua to glue together high performance code, and that's kind of what we do a lot of in deep learning is we have some very efficient implementations of primitive operations and we want to glue them together somehow.",
                    "label": 0
                },
                {
                    "sent": "Glue convolution to matrix, multiply for instance.",
                    "label": 1
                },
                {
                    "sent": "In Lou is actually originally chosen four torch because there was a need to do embedded machine learning.",
                    "label": 0
                },
                {
                    "sent": "So the story that I've heard is clyma far away.",
                    "label": 0
                },
                {
                    "sent": "Who is one of the inventors of Torch was trying to put a convolutional neural net in an FPGA, which is like a reprogrammable circuit board so that he could have a confident on his bike helmet when he's driving around and identifying objects and getting that to work in Python is apparently a pain in the butt.",
                    "label": 0
                },
                {
                    "sent": "I never tried it, but it was apparently very easy to do that in Lua, so that's kind of the Genesis of using Lua and Torch.",
                    "label": 0
                },
                {
                    "sent": "And now Torch runs in all kinds of places, like on very large server farms at Twitter and on mobile phones, right?",
                    "label": 0
                },
                {
                    "sent": "So it can kind of run anywhere.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I mentioned, there's very easy integration into and from C and so this is production code calling into khudian enan.",
                    "label": 1
                },
                {
                    "sent": "This Lua code for calling this function is about as long as if you were to just write it and see most of the craft here is just actually putting the types in the right order and everything.",
                    "label": 0
                },
                {
                    "sent": "Really disting.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pushing feature related to Cody and N is that there's strong first class GPU support in torch and I think this is really differentiating factor.",
                    "label": 0
                },
                {
                    "sent": "For torch you can just require this coup torch library and requires like Louis Import statement for Python And then all of a sudden you have access to this new type.",
                    "label": 0
                },
                {
                    "sent": "This CUDA tensor type and all you have to do is cast your tensors using this member function CUDA and that tensor has been moved on onto the GPU and then everything you do with that tensor at that point happens a GPU operation.",
                    "label": 0
                },
                {
                    "sent": "Right in the way this works under the hood is we have a whole lot of C implementations for code on the CPU, and then we have a whole lot of CUDA kernels that mirror the functionality of that, and so there's a lot of heavy work on the back end that you never see, so that things look very light and transparent and you could be running on the GPU and maybe not even know it.",
                    "label": 0
                },
                {
                    "sent": "So it's really, really easy to get your code on the GPU, just switch types.",
                    "label": 0
                },
                {
                    "sent": "I personally think that using types to do kind of multi device support and multiple dispatches probably.",
                    "label": 0
                },
                {
                    "sent": "I think that's the kind of latest.",
                    "label": 0
                },
                {
                    "sent": "And most elegant way of dealing with multi device support and in torch supports that.",
                    "label": 0
                },
                {
                    "sent": "And Torch also has a very large community.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a lot of industrial contributors, but it is not industry owned and that's the really important distinction, at least for me.",
                    "label": 0
                },
                {
                    "sent": "So NVIDIA and Facebook and Twitter all contribute a lot to the open source community, and we use these tools internally, but we don't own the direction of this tool, it's community driven.",
                    "label": 0
                },
                {
                    "sent": "Facebook AI uses it for their research labs and we use it both for research but also for production.",
                    "label": 0
                },
                {
                    "sent": "So all the deep learning that happens at Twitter happens in torch and so that means any image that might get uploaded to a torch piece of piece of source code is probably seeing that image.",
                    "label": 0
                },
                {
                    "sent": "There's a ton of academic support as well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another nice aspect of a very large and vibrant community, especially in the deep learning community in torch, is that as soon as a paper is published with some cool cutting edge result or with some new functionality, there's almost always a GitHub repository that pops up that actually implements that paper.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the paper comes out at the same time as the GitHub repository, so it's usually the case that you can find working code you can download and run for really cutting edge stuff that might have even coming out that day, and you're able to run that in torch, so there's a whole variety of.",
                    "label": 0
                },
                {
                    "sent": "Of networks that are available for you just to download and run or modify or integrate into your own.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Checked.",
                    "label": 0
                },
                {
                    "sent": "A couple of examples.",
                    "label": 0
                },
                {
                    "sent": "There's many different implementations of image capturing that you can download and run.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's also many different implementations of neural style transfer that you can download and run, so there's a lot of different flavors of this that people have just kind of for fun implemented, but also some more reference implementations of a publication.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's also neural conversation models.",
                    "label": 0
                },
                {
                    "sent": "You can build a chatbot and torch by just downloading the code, and this is all available on GitHub already and then.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a lot of variants of generative adversarial networks you can download that code on torch.",
                    "label": 0
                },
                {
                    "sent": "So where does torch fit in?",
                    "label": 0
                },
                {
                    "sent": "Like the larger scientific computing order?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The science ecosystem.",
                    "label": 0
                },
                {
                    "sent": "It's really clear that Python is kind of the biggest language when it comes to doing scientific computing and doing data analysis and data science defined however you want.",
                    "label": 0
                },
                {
                    "sent": "Lua and Torch is much smaller than Python, however torch is disproportionately strong for its size in deep learning.",
                    "label": 1
                },
                {
                    "sent": "And so you'll find a lot of resources for that specifically.",
                    "label": 0
                },
                {
                    "sent": "And before I learned about Lou, I've been programming Python for about 10 years and switching to Lua from Python was very smooth, 'cause the languages are very similar and you can sit down and read the manual for the whole language in like an afternoon.",
                    "label": 0
                },
                {
                    "sent": "So there's really not a lot to learn there in terms of becoming familiar with the language, so if you're worried about that, you should be able to pick it up very quickly.",
                    "label": 0
                },
                {
                    "sent": "In terms of where torch fits with respect to deep learning libraries specifically the first kind.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to preface it all with is that there's really no Silver bullet library.",
                    "label": 1
                },
                {
                    "sent": "Each library has its strengths and its weaknesses.",
                    "label": 0
                },
                {
                    "sent": "Depending on what you want to get out of it.",
                    "label": 0
                },
                {
                    "sent": "So if you're really interested in running just convolutional neural networks in production with high throughput, then you should probably pick something like Cafe, but if you want to try weird architectures and do research, you might pick something more like torture, Theano, and now Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "I think as well.",
                    "label": 0
                },
                {
                    "sent": "But really, each of these has its strength and minus is, I think, torches unique along with tensor flow, in that it's mostly used for research, but we've kind of at least a Twitter.",
                    "label": 1
                },
                {
                    "sent": "We've demonstrated that can be used at scale in production, so if you're interested in building a startup around torch and running that on mobile phones, or running that on servers, that's something that has been done so you can do it as well.",
                    "label": 0
                },
                {
                    "sent": "And development in torch in terms of adding new functionality or adding modules to the code.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System we have like a core philosophy that I think most contributions follow, which is that.",
                    "label": 1
                },
                {
                    "sent": "Programming and computing with torch should be really interactive.",
                    "label": 0
                },
                {
                    "sent": "You should be able to try things out in an interactive setting to see if things fail very quickly.",
                    "label": 0
                },
                {
                    "sent": "So that means there should be no compilation time, so this is a choice that torch makes that we won't be building a compiler to run your neural network as we would like to have our computation be interactive, and also we think that we prefer the imperative programming style.",
                    "label": 1
                },
                {
                    "sent": "So that means the torch code should look like torch code and you shouldn't be writing in like a mini domain specific language or something kind of separate.",
                    "label": 0
                },
                {
                    "sent": "So everything is also very close to the metal, so if you are running a given torch function and you're wondering about how it's implemented, or you know why it's slower, fast in a given case, if you go to GitHub and look up that function, usually in one or two different hop one or hops down the code, you can find the C code that's actually running it, so it's actually very easy to introspect what's going on underneath the hood, so if you're that kind of a person, kind of a hacker mentality and you want to understand what's really happening.",
                    "label": 0
                },
                {
                    "sent": "Torches really great library for that.",
                    "label": 1
                },
                {
                    "sent": "And also there's maximal flexibility, so composing different modules together is actually pretty easy.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a high level overview of the torch core.",
                    "label": 0
                },
                {
                    "sent": "And I think it would be good just to give an overview for like the fundamental data types and torch, you can have a sense of how things are happening on the computer when you're running code.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's two fundamental data types in torch, there's a tensor and a storage, and torch is unique in that it's a D coupled this, whereas in NUM.",
                    "label": 0
                },
                {
                    "sent": "Py it's it's actually fused, so a storage is a linear array of memory that just has a length and type, so each element is afloat or a double orucuta float, and a tensor is a view into that storage, right?",
                    "label": 0
                },
                {
                    "sent": "So a tensor is an end dimensional view into that storage, that is row major in memory.",
                    "label": 0
                },
                {
                    "sent": "So that means that you access the first row and then you get linear access to the next element of that row, and then you have to skip to the next row and so this tensor.",
                    "label": 0
                },
                {
                    "sent": "Here is a four by six view into the underlying storage, with the stride of six by one.",
                    "label": 0
                },
                {
                    "sent": "It means that as you go down the row you have to skip 6 elements in the linear memory in order to go down each row, and you have to skip just one to go across the columns.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Lua, like Matlab, is 1 indexed.",
                    "label": 0
                },
                {
                    "sent": "So if we call the select function for instance on this tensor we're selecting from the first dimension the Rose, the third element, so that green row and we'll get back a tensor.",
                    "label": 0
                },
                {
                    "sent": "What we're getting back actually is a view into the storage, right?",
                    "label": 0
                },
                {
                    "sent": "So tensors are views into storage.",
                    "label": 0
                },
                {
                    "sent": "So now we have two tensors pointing at the same kind of block of memory.",
                    "label": 0
                },
                {
                    "sent": "Now, if we were.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two and also the offset is now 13, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a third property the tensor can accumulate from a storage.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we were to select from the second dimension, the third index will be grabbing that third column.",
                    "label": 0
                },
                {
                    "sent": "Now we have a size 4 tensor with the stride of six.",
                    "label": 0
                },
                {
                    "sent": "As we go linearly through this tensor, we need to skip 6 elements in the underlying storage.",
                    "label": 0
                },
                {
                    "sent": "That's something to be aware of if you're trying to make really efficient code.",
                    "label": 0
                },
                {
                    "sent": "If you induce a stride, you can slow down some of your operations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how you actually instantiate this practically is we might grab a tensor a from a double tensor.",
                    "label": 0
                },
                {
                    "sent": "That's 4 by 6.",
                    "label": 0
                },
                {
                    "sent": "At this point our memory is completely uninitialized, so there could be anything in there.",
                    "label": 0
                },
                {
                    "sent": "And then we can fill it with uniform noise an if we print the result.",
                    "label": 0
                },
                {
                    "sent": "We've got some values in a that are uniform.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we select out that third row and call it B and we print that, we just got that third row in B.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we fill B with some number like 3, recall that this new tensor is just a view on the same piece of storage.",
                    "label": 0
                },
                {
                    "sent": "So what we've actually done is overwritten the values in a as well as a is just a view into the underlying storage, so this can be a gotcha.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's maybe a little bit different from how you think about it in, NUM, PY.",
                    "label": 0
                },
                {
                    "sent": "Sometimes these slicing operations induce copies in NUM.",
                    "label": 0
                },
                {
                    "sent": "Py, kind of silently and in torch you really are kind of dealing with the computers memory.",
                    "label": 0
                },
                {
                    "sent": "Kind of closer to the metal.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then, as I mentioned before, we have GPU support for all operations, so if you just require COO torch, which you get is this new type shows up in torch this CUDA tensor and that's just the float tensor for the GPU.",
                    "label": 1
                },
                {
                    "sent": "So if you cast if you create a CUDA tensor uninitialized and fill it with uniform values, you can do exactly the same operation.",
                    "label": 0
                },
                {
                    "sent": "So anything you can do on the CPU, you can do exactly the same on the GPU, so oftentimes moving a whole codebase from the CPU to GPU is just a one line change at the very beginning where you cast the types of the variables that you're.",
                    "label": 0
                },
                {
                    "sent": "Operating on.",
                    "label": 0
                },
                {
                    "sent": "So that's a quick view of the fundamentals of memory management in Torch and I'll talk at a very high level about kind of the libraries and approaches that you might use when training neural networks and torch so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Training cycle for training model specifically in neural network and torch usually load data from your hard drive and then cue that up and then you might have some piece of code that coordinates a neural network.",
                    "label": 0
                },
                {
                    "sent": "A cost function at the end of that neural network, and then an optimizer which can take gradients from that neural network and make updates to the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the torch ecosystem there's three packages that can kind of handle this for you, so threads which I won't talk about and then will handle the specification of your neural network and your cost function and optim will handle the optimization of your neural network.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "CNN package handles specifying your neural network in your cost function and also calculating gradients.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Hugo yesterday talked about two different ways you might think about phrasing a neural network.",
                    "label": 0
                },
                {
                    "sent": "The first was writing a set of nested mathematical expressions, kind of writing things down symbolically, and then another way that he phrased, specifying neural network was as a connected graph of nodes, where each node is a function that's taking in edges, which are data, and so you can think of a neural network as a directed acyclic graph of computation and really buys into this idea.",
                    "label": 0
                },
                {
                    "sent": "Pretty heavily, so it's really easy to build neural networks in, and by composing different building blocks on top of each other.",
                    "label": 1
                },
                {
                    "sent": "So it's really like snapping Legos together.",
                    "label": 0
                },
                {
                    "sent": "So in this example we start with a sequential container, so we'll have a container in which we can snap blocks inside of it, and then here we'll just define a convolutional neural networks.",
                    "label": 0
                },
                {
                    "sent": "A lot of convolution and non linearity, some pooling, some contrasted normalization in on in the network until the end of log softmax at the end, right?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a linear set of constructions for building a neural network.",
                    "label": 0
                },
                {
                    "sent": "This is a sequential container, so we're just adding blocks one after the other, so the computation proceeds.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can also compose networks like Lego Blocks and other types of configurations, so have parallel streams in a network or branch them out, or concatenate them in.",
                    "label": 0
                },
                {
                    "sent": "So these are all different types of containers you can play with.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the end, package and what we care about when training neural networks.",
                    "label": 0
                },
                {
                    "sent": "Are there gradients in their outputs on the end.",
                    "label": 0
                },
                {
                    "sent": "Package exposes 2 main methods that you can call on.",
                    "label": 0
                },
                {
                    "sent": "The containers are also on the modules for pushing activations through network and then also for pulling the gradients from the loss back to the parameters and that the first is update output.",
                    "label": 0
                },
                {
                    "sent": "So that will take output fed into a network and push it all the way through and then update grad input is a call that you will see which pulls the gradients all the way back.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you'll see at grad parameters which actually updates the parameters with the gradients.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have a corresponding package for an, then called Coonan.",
                    "label": 0
                },
                {
                    "sent": "So with torch you just import coup torch and then you have this new type and with kuenen it's exactly the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "So every single layer that's been implemented in end has a CUDA equivalent, so all you have to do is require kuenen and then cast the model right there online 9 to the CUDA type and the whole network gets moved to the GPU and the network runs on the GPU at that point.",
                    "label": 0
                },
                {
                    "sent": "So it's really just a one line change to move all of your computation to the GPU.",
                    "label": 0
                },
                {
                    "sent": "Of course you have to cast your input type as well to CUDA tensor.",
                    "label": 0
                },
                {
                    "sent": "So I've described like building linear graphs of computation, but you might want to do something more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is something called the NN graph package.",
                    "label": 0
                },
                {
                    "sent": "So instead of clicking Lego blocks together, you will instantiate a module and wire it to some other module as long as it's a cyclic and this let's.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Build more complicated directed acyclic graphs of computation in your neural network.",
                    "label": 0
                },
                {
                    "sent": "We don't really use this at Twitter, we use a pack.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is called torch autograde.",
                    "label": 0
                },
                {
                    "sent": "That's because I think that explicitly building your compute graph can get a little cumbersome.",
                    "label": 0
                },
                {
                    "sent": "We say OK, give me this module and wire to this one.",
                    "label": 0
                },
                {
                    "sent": "What's more natural, at least to me, is just to write down the code that you want implemented.",
                    "label": 0
                },
                {
                    "sent": "So just write in an imperative style.",
                    "label": 0
                },
                {
                    "sent": "I want to take the 10 H of this output, and then I want to take the output of that tension.",
                    "label": 0
                },
                {
                    "sent": "Do something else with it, so I think code written in this imperative style is a lot more readable and allows me to kind of iterate a lot faster, and so we use this torch autograph package that I'll talk a bit more about later.",
                    "label": 0
                },
                {
                    "sent": "To implement automatic differentiation on arbitrary compute graphs.",
                    "label": 0
                },
                {
                    "sent": "So I'll discuss that a bit more.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll talk briefly about the opt in package without going into too much detail, so the opt in package implem.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a whole host of 1st order optimization methods including our friends to casted gradient descent, which is always with us and some line search methods like elegs and then some newer things like Adam an RMS prop.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And optimism is a little kind of changes the way that you think about writing your code just a little bit.",
                    "label": 0
                },
                {
                    "sent": "So there on the bottom there.",
                    "label": 0
                },
                {
                    "sent": "I've got that optim SGD function, and it's got three input parameters.",
                    "label": 0
                },
                {
                    "sent": "Will go kind of from right to left, so the config is like the learning rate and momentum.",
                    "label": 0
                },
                {
                    "sent": "The things that parameterized your optimization and then X is the parameters of your neural network and then the first input is a function that takes in as an argument your parameters and then outputs the Lausanne the gradients.",
                    "label": 0
                },
                {
                    "sent": "So you'll notice that nowhere here is data loading or the specification of the model that all gets closed over by this function.",
                    "label": 0
                },
                {
                    "sent": "So in that function need to load your data and you need to evaluate a neural network and grab the grab the gradients so it can be kind of a rearrangement of how you ordinarily think about writing this type of code.",
                    "label": 0
                },
                {
                    "sent": "But once you have it written in this style, you can replace SGD with Adam with deleting 3 characters and adding four, and so you can actually.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's so easy you can treat the optimizer as like a categorical variable to optimize the hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "If that's something that you're interested in doing.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing to think about when using the optim package is that it expects that all the parameters that you're trying to optimize or contiguous in memory, and this happens automatically when you're using torch NN.",
                    "label": 1
                },
                {
                    "sent": "So the weights and biases of layer one or right next to the weights and biases of layer two in the memory and the storage for that neural network.",
                    "label": 0
                },
                {
                    "sent": "But if you're optimizing something that's not in end, you'll have to kind of pack all of your parameters together and provide it to end and to Optum, so that's something just kind of consider.",
                    "label": 0
                },
                {
                    "sent": "So NN and Optum together are kind of like packages that take advantage of torch.",
                    "label": 0
                },
                {
                    "sent": "Building the foundation of Torch to provide neural networks and the ability to optimize those neural networks on both the CPU or the GPU, and it's kind of a high level overview of what's available and what the functionalities are, so that's the first half of the talk or the first part of the talk.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'd like to switch to talk a bit more about this package torch autograde, so yeah.",
                    "label": 0
                },
                {
                    "sent": "Explain the punctuation of optimal or what did I do?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so you're passing in a function here, and this function is spitting out the loss and the gradients, and so you have to close over the model so the model has to be an up value that's declared outside of the function.",
                    "label": 0
                },
                {
                    "sent": "So it's just a different style of writing.",
                    "label": 0
                },
                {
                    "sent": "Instead of writing things in an object oriented style, you're kind of using a closure style, which you can do the same thing in both styles is that I don't know if that answers your question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a show of hands who's heard of automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "OK, everybody has anybody heard of Autograde the package before?",
                    "label": 0
                },
                {
                    "sent": "OK, so some folks cool so autograde is Twitters.",
                    "label": 0
                },
                {
                    "sent": "It's really an industrial strength but extremely flexible implementation of automatic differentiation and we try really crazy ideas and autograde.",
                    "label": 1
                },
                {
                    "sent": "So instead of writing computation as a linear set of blocks you can really write anything down any numeric function and expect to be able to get the gradients of that function.",
                    "label": 0
                },
                {
                    "sent": "I was inspired by the original version in Python from Ryan Adams Group at Harvard Dougal Mclaurin.",
                    "label": 0
                },
                {
                    "sent": "David do venonat.",
                    "label": 0
                },
                {
                    "sent": "Johnson were the kind of original developers of the Python version.",
                    "label": 0
                },
                {
                    "sent": "So just taking kind of a step.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back to appreciate the before I get into automatic differentiation to appreciate this stable abstractions that all of machine learning is built on so we don't really think about arrays anymore, and that was kind of solved in the late 50s.",
                    "label": 0
                },
                {
                    "sent": "We don't really think about how to call linear algebra routines that was solved by lapack and we kind of take for granted the idea that we shouldn't really be programming doing scientific computing and less.",
                    "label": 0
                },
                {
                    "sent": "We have all of the kind of primitive functions that we would want like.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to re implement my own exponential.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to reimplement sampling from.",
                    "label": 0
                },
                {
                    "sent": "You know a normal distribution kind of take advantage of that.",
                    "label": 0
                },
                {
                    "sent": "All should just be there in Matlab and NUM.",
                    "label": 0
                },
                {
                    "sent": "Py kind of gave us that expectation and we really should take these abstractions for granted in order for us to stay same like we don't want to go back down to the networking stack and reimplant reimplement everything by hand, we really should kind of build on top of these really strong abstractions.",
                    "label": 1
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But machine learning has has other abstractions that I think are still kind of nascent that I'm really interested to see how how they kind of evolved in solidify.",
                    "label": 1
                },
                {
                    "sent": "I would say that automatic differentiation really is a V abstraction for gradient based machine learning.",
                    "label": 0
                },
                {
                    "sent": "Automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "It's a process that mechanically calculates derivatives as functions expressed his computer programs at machine precision, an with complexity guarantees, so there's like 3 parts to that statement.",
                    "label": 1
                },
                {
                    "sent": "The first is that auto differ.",
                    "label": 0
                },
                {
                    "sent": "Just a D should take in as an input.",
                    "label": 0
                },
                {
                    "sent": "A program you can run that will give you a number output and it should output a new program that will give you the gradient as well as the original output.",
                    "label": 0
                },
                {
                    "sent": "So it's a transformation of a program you can run.",
                    "label": 0
                },
                {
                    "sent": "It has machine precision, so automatic differentiation guarantees that your new function that calculates gradients will have the same numerical stability as the original function, right?",
                    "label": 0
                },
                {
                    "sent": "And also it has complexity guarantees, so it'll just be some constant factor, so some it will be at most three times the complexity of the original function.",
                    "label": 0
                },
                {
                    "sent": "This distinguishes it from finite differences, so there was a question in Hugo's talk about why can't you just train neural networks using finite differences.",
                    "label": 0
                },
                {
                    "sent": "Apparently people actually did this in the 60s, so this book called Talking Nets in an interview with Woodrow, which is one of the chapters.",
                    "label": 0
                },
                {
                    "sent": "It's a book of like interviews with people that were luminaries in neural networks.",
                    "label": 0
                },
                {
                    "sent": "It's a great book if you haven't read it.",
                    "label": 0
                },
                {
                    "sent": "Woodrow described actually training neural networks with finite differences and kind of the pain and difficulty of having to train neural networks in that fashion because they're incredibly numerically stable.",
                    "label": 0
                },
                {
                    "sent": "To do that, the process of finite differences means perturbing your input a little bit, and then passing it through a whole composure of a bunch of functions.",
                    "label": 0
                },
                {
                    "sent": "Which increases the numerical instability.",
                    "label": 0
                },
                {
                    "sent": "So kind of exacerbates the problem is also very inefficient, is not symbolic differentiation, so it's not something that you would like type in a mathematical expression to Mathematica and say, give me the partial derivative of this expression.",
                    "label": 1
                },
                {
                    "sent": "There's no complexity guarantee with that, so for highly nested functions, which is every neural net.",
                    "label": 0
                },
                {
                    "sent": "The size of the expression you get back from naive symbolic differentiation can swell very, very quickly, and that's not the case with automatic differentiation.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this idea has been rediscovered several times.",
                    "label": 1
                },
                {
                    "sent": "There's kind of a review paper there.",
                    "label": 0
                },
                {
                    "sent": "The first implementation that I'm aware of where the input to auto diff is a program in the output is a program.",
                    "label": 0
                },
                {
                    "sent": "Is this guy Burt Steele pending thesis in 1980 he wrote this thesis that describe auto diff in perfect clarity and then just disappeared for like 30 years and then kind of gave a talk at some auditing conference.",
                    "label": 0
                },
                {
                    "sent": "At some other point.",
                    "label": 0
                },
                {
                    "sent": "So he's just disappeared from the field.",
                    "label": 0
                },
                {
                    "sent": "There was an earlier implementation of variant called Forward mode that I'll describe about 15 years earlier.",
                    "label": 1
                },
                {
                    "sent": "It was, I guess, popularized in connectionist machine learning as back back propagation by Rumelhart.",
                    "label": 0
                },
                {
                    "sent": "Although there is some debate as to who came first and who is responsible for the invention of this algorithm, but one thing to note is that in other fields like nuclear science and atmospheric Sciences like meteorology and computational fluid dynamics, these guys use a D all the time, and in fact there are tools for doing for using ADR.",
                    "label": 1
                },
                {
                    "sent": "Actually a lot more sophisticated than the ones that we have in machine learning, so I think we actually have a lot to important to learn from these different fields.",
                    "label": 0
                },
                {
                    "sent": "They have been happily humming along and using a D to tune their models to train them.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also I would make a distinction that we don't use a D. We use reverse mode AD, so there's there's.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "2 main modes of automatic differentiation.",
                    "label": 1
                },
                {
                    "sent": "Two ways of calculating derivatives or too extreme variants.",
                    "label": 0
                },
                {
                    "sent": "The first is called Forward Mode, which will almost never seen machine learning 'cause it's a really bad idea and I'll explain why it's a bad idea in machine learning and then reverse mode, which is synonymous with backpropagation.",
                    "label": 1
                },
                {
                    "sent": "And these are really just different orders of applying the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's like a symbolic view of forward mode automatic differentiation.",
                    "label": 1
                },
                {
                    "sent": "So say we've got a neural network that takes in an image and predicts the probabilities of classes.",
                    "label": 0
                },
                {
                    "sent": "That's a composition of functions, and so we have this chain of partial derivatives that we want to calculate the derivative of the loss with respect to the parameters, and so we need to apply the chain rule to those partial derivatives in the forward mode.",
                    "label": 0
                },
                {
                    "sent": "With this we can apply the chain rule in any direction that we like, right?",
                    "label": 0
                },
                {
                    "sent": "So we can start from the left and go to the right.",
                    "label": 0
                },
                {
                    "sent": "You can kind of see immediately that that's a bad idea 'cause you have these big matrix matrix multiplies.",
                    "label": 0
                },
                {
                    "sent": "So going left to right is a bad idea, especially when it is a bad idea when you're going from many parameters to a scalar loss, right?",
                    "label": 0
                },
                {
                    "sent": "It's actually a good idea when you're going from a small set of parameters to a lot of numbers, right?",
                    "label": 0
                },
                {
                    "sent": "And that's the case when you want like vector flow fields or something like that, but you never do that an optimization because the output is always some scalar loss.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can view this in a different way as a like a program view, so here's a program we want to differentiate, so we have FAB&C if B is greater than C, then return a times the sign of B, otherwise return a + B * C. You can back propagate through this.",
                    "label": 0
                },
                {
                    "sent": "There's an if statement, but we can actually kind of ignore it and then we can ignore it is by just recording the trace of the operations that were actually run by the program and this is called a Wengert list.",
                    "label": 1
                },
                {
                    "sent": "So we first assign A to three to A and two to B and then one to see and then we basically erase the IF statement because that's not, it's not a numerical function an we return.",
                    "label": 0
                },
                {
                    "sent": "Eight times the sign of be right, so this is a trace of this program, you know, a run through this program and if we wanted to do the forward mode of backpropagation with this program, we would define AS 3 and then we instantiate the derivative of a with respect to a.",
                    "label": 0
                },
                {
                    "sent": "So we're starting the chain rule from the left, right?",
                    "label": 0
                },
                {
                    "sent": "And two is B and then the derivative of be with respect to a is zero and see is 1 and then its derivative.",
                    "label": 0
                },
                {
                    "sent": "See with respect to a 0.",
                    "label": 0
                },
                {
                    "sent": "And then we'll run D and then we'll get the derivative of deal with respect to a 'cause we have all the information at that point.",
                    "label": 0
                },
                {
                    "sent": "This is a really, really simple example.",
                    "label": 0
                },
                {
                    "sent": "But I think you can appreciate that if you wanted to get the derivative of the youth respect to be, you'd have to do a whole another sweep through this program.",
                    "label": 0
                },
                {
                    "sent": "You have to initialize derivative of be with respect to be as one.",
                    "label": 0
                },
                {
                    "sent": "This is really bad for neural networks, so if you have a million parameters I mean you have to do a million sweeps of forward mode automatic differentiation in order to get the gradients that you need to make an update.",
                    "label": 0
                },
                {
                    "sent": "So this is a really bad idea.",
                    "label": 0
                },
                {
                    "sent": "However if you had some other function of a B&C that you wanted to calculate partial derivatives for.",
                    "label": 0
                },
                {
                    "sent": "At the very end of this evaluation, you have that all available to you, so if you have fan out in your computation graph, so if you go from very few parameters to very many Ford modes, a good idea, it's just not something that happens in machine learning a lot.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So reverse mode is the same idea, except for evaluating the chain rule from right to left.",
                    "label": 0
                },
                {
                    "sent": "So we've got these nice matrix vector products that keep things small.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much better complexity when you have a scalar valued output.",
                    "label": 0
                },
                {
                    "sent": "And we can build a Wengert list for the same function using reverse mode.",
                    "label": 0
                },
                {
                    "sent": "So since we're going from right to left, we actually need all of the.",
                    "label": 0
                },
                {
                    "sent": "Need the whole trace to be available before we can start calculating the gradients, so we'll define AB&C&D and that's our our trace of this program.",
                    "label": 0
                },
                {
                    "sent": "And then when we want to calculate the derivatives or do backpropagation, will actually run the whole program and then start the derivative of the with respect to D as one and then build up our gradient from the right side of the chain rule all the way to the left.",
                    "label": 0
                },
                {
                    "sent": "So we actually need to tape the whole program.",
                    "label": 0
                },
                {
                    "sent": "We need to record the whole program before we can do back propagation.",
                    "label": 0
                },
                {
                    "sent": "And then we can return the gradients there.",
                    "label": 0
                },
                {
                    "sent": "I think you can appreciate here if we want the gradients of D with respect to be.",
                    "label": 0
                },
                {
                    "sent": "That's just an extra single step here.",
                    "label": 0
                },
                {
                    "sent": "Whereas before we have to go through the whole program again, and as your program becomes larger and more complicated, this becomes a more severe penalty in forward mode, but definitely not in reverse mode.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the the differences between forward mode in reverse mode and why you really never see forward mode in machine learning and why you see reverse mode.",
                    "label": 0
                },
                {
                    "sent": "And that's back propagation.",
                    "label": 0
                },
                {
                    "sent": "So autumn torch Autograde is an implementation of reverse mode auto diff.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so this is an entire function for training a three layer MLP in torch autograde.",
                    "label": 1
                },
                {
                    "sent": "So we'll define the parameters at the top.",
                    "label": 1
                },
                {
                    "sent": "Define our neural network is nonlinearities interspersed between matrix multiplies and additions, and then a loss function at the end and then this.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is really the entire T of the API of Autograde, so you import auto grad and then you call grad on the function you would like the gradients of and it gives you back a new function and then down at line 24 will actually call that new function and instead of returning the loss, it will return the gradients of the loss with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "So we've just transformed this function into a function that will now give us the gradients for free so we don't have to really worry about writing down any partial derivatives or doing any math, so as long as your function is differentiable.",
                    "label": 0
                },
                {
                    "sent": "Autograde will give you back the gradients of that function.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's actually happening under the scenes is as this function is running.",
                    "label": 1
                },
                {
                    "sent": "Like I said before, we're actually keeping track of the whole tape.",
                    "label": 0
                },
                {
                    "sent": "So on the first line we're doing matrix multiply, and so we're going to save W an input and the function as well as the output, and then when we add each one in Paramus dot B to make H2, we will save all that information as well.",
                    "label": 1
                },
                {
                    "sent": "So we're saving the results of this compute graph, kind of behind the scenes while you're running this numeric function and the way we do this.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This Villa operator overloading, so for every function in torch we're going to overload that function with one that keeps track of computation, builds up a linked list of computation, so will save the original function, and then in our new redefinition of torch, some will check if there's a special type being passed to the function, and if not, will just return the original function, and if not, if there is a special type, then will unpack that type, grab the output value, and then pack it back into this special type.",
                    "label": 1
                },
                {
                    "sent": "And this special type keeps track of the function that was run.",
                    "label": 0
                },
                {
                    "sent": "The arguments that were passed to that function and then the outputs, because really, that's all you need along with the gradient that's coming in to calculate the.",
                    "label": 0
                },
                {
                    "sent": "Partial derivatives at that particular step.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We keep track of this compute graph, and then when we need to evaluate the partial derivatives, we have access to all of the local information we'd need to calculate that gradient to pass it back down to compute function was actually.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Happening when we calculate gradients back down, the compute graph is we're looking up in a big big table.",
                    "label": 0
                },
                {
                    "sent": "All of the partial derivatives for every torch function that is available.",
                    "label": 1
                },
                {
                    "sent": "So we written all these partial derivatives by hand so that you can compose them together and really any way that you please right?",
                    "label": 1
                },
                {
                    "sent": "So if we hit a square root function in this compute graph, would just go look up the gradient of the first argument of square roots, only one argument, and then we can use that to calculate the gradient and then propagated through the graph.",
                    "label": 1
                },
                {
                    "sent": "So behind the scenes we've done all this work to have individual partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "And then by using the chain rule in reverse mode, auto diff, we can actually calculate the gradients through an entire function that's been composed together.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I want to back up to the Internet, but I will have to use the grading function for each of the functions for each of the components.",
                    "label": 0
                },
                {
                    "sent": "So is there somewhere I can just call one function and it will automatically compute gradients through the entire network?",
                    "label": 0
                },
                {
                    "sent": "Yep, that's that is the grad function, so I'll show you in a little bit, but we've exposed the ability to use NN modules, so if you have a network that you've written end torch NN, you can wrap that and use that in auto grad and then you can combine networks however you want, so you could plug a CNN into an LTM and do image captioning.",
                    "label": 0
                },
                {
                    "sent": "An autograph will just figure out how to calculate the derivative through all of that so you can compose anything an autograph will take care of calculating the derivative through those functions.",
                    "label": 0
                },
                {
                    "sent": "Calling the function and loss that was the last part of the network, so I thought you have to call back for every part of the network loss.",
                    "label": 0
                },
                {
                    "sent": "Actually, let me show you that again.",
                    "label": 0
                },
                {
                    "sent": "Whoops",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Loss is actually calling into the neural network function.",
                    "label": 0
                },
                {
                    "sent": "So when we go into that function in called neural network, which is defined up, there were actually taping the contents of that function as well.",
                    "label": 0
                },
                {
                    "sent": "So we're keeping track of everything that's running all the functions that are kind of recursively called.",
                    "label": 0
                },
                {
                    "sent": "Do do.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do do do do do do.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So just show you a couple of examples, kind of increasing in complexity of what you can do with autograde, so of course we can back propagate through really basic arithmetic.",
                    "label": 0
                },
                {
                    "sent": "You just write that down, you know.",
                    "label": 0
                },
                {
                    "sent": "Would using normal touch code you can get the gradients of that arithmetic expression.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like I mentioned before, you can have control flow, so you can just have an IF statement there, not have to worry about phrasing it as a Condor, doing anything really complicated like you have to do in maybe Theano or Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "You can just write plain code and AutoCAD will figure out the derivatives of this code for you.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I've been using scalars, but of course this all works with tensors, right?",
                    "label": 0
                },
                {
                    "sent": "So wait, wait several network are matrices and everything is works for matrices as well as scalars.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think Autograde really shines when you start to use more complicated to control flow, so a for loop for instance, is trivial.",
                    "label": 0
                },
                {
                    "sent": "In autograph, you just write a for loop and will take the gradients with all the way through that for loop.",
                    "label": 0
                },
                {
                    "sent": "True, re phrasing of your computation, you have to do.",
                    "label": 1
                },
                {
                    "sent": "There's no restrictions.",
                    "label": 0
                },
                {
                    "sent": "This could be a nested for loop.",
                    "label": 0
                },
                {
                    "sent": "You could nested five times.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Auto grade will figure out the gradients.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one thing that I actually don't think is possible in any other automatic differentiation package is ability to call yourself recursively or to have recursive control flow.",
                    "label": 0
                },
                {
                    "sent": "So this actually implements the same thing as the previous for loop.",
                    "label": 0
                },
                {
                    "sent": "So here I'm just multiplying a by itself and storing that as A and I'm going to do that B times here.",
                    "label": 0
                },
                {
                    "sent": "I have a function where if B is zero, return a, otherwise.",
                    "label": 0
                },
                {
                    "sent": "Take the F 8 * A and decrement B so it does the same thing.",
                    "label": 0
                },
                {
                    "sent": "It just uses a recursive calling style, but I'm calling F inside of F, right?",
                    "label": 0
                },
                {
                    "sent": "So I have this recursive function, but autograde has no problem with that, so you can take gradient through recursive functions and you could, you know, compose all these things together and if statements for loops or cursive function calls and the gradients will just be figured out for you.",
                    "label": 0
                },
                {
                    "sent": "Don't really have to worry about the specifics there, so it allows you to really naturally phrase your computation.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing that I think a little bit neat is adding your own gradients or playing with the gradients is actually pretty easy to do an autograph.",
                    "label": 0
                },
                {
                    "sent": "So why would you want to do that?",
                    "label": 0
                },
                {
                    "sent": "Gradient clipping?",
                    "label": 0
                },
                {
                    "sent": "For instance, you might want to have a function that on the forward passes the identity function, but on the backwards passes a clamp.",
                    "label": 0
                },
                {
                    "sent": "Make sure that your gradients don't grow too much, so you can easily make a function in autograde that has a partial derivative that doesn't look like the actual partial derivative of the forward pass.",
                    "label": 0
                },
                {
                    "sent": "It's pretty easy, so in this example I'm taking the sum of the floor of eight of the third, and floor is a rounding operation, so its gradient is zero almost everywhere.",
                    "label": 0
                },
                {
                    "sent": "And the gradients passing through this function are correctly 0, right?",
                    "label": 0
                },
                {
                    "sent": "So this is bottleneck that's crushing the gradient.",
                    "label": 0
                },
                {
                    "sent": "Maybe I want to back propagate through floor, like for instance the JPEG compression algorithm has a quantization step and maybe I want to take the gradients of the JPEG compression algorithm 'cause I'm interested in doing that for instance so I can make the gradient floor disappear really easily so.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will require autograde.",
                    "label": 0
                },
                {
                    "sent": "I'll make my own special module and I'll add a floor method to my special module.",
                    "label": 0
                },
                {
                    "sent": "An internally floor is just calling torched out floor like before and then I'll overload my module that I call special and I'll define the gradient of floor inside that module and I'll have the gradient.",
                    "label": 0
                },
                {
                    "sent": "Just return G. So just passed through the gradient as a floor doesn't exist and then I can call.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This original function actually have gradients passed through that, so it's really trivial to play with the kind of internals and the mechanisms of automatic differentiation in autograde, so you can kind of imagine crazy ideas and just implement them very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So those are a couple of examples of using autograde in kind of the more exotic features that differentiate it from the alternatives.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is actually differentiating these different neural network libraries or what's 1 axis along which all these different libraries are separate?",
                    "label": 0
                },
                {
                    "sent": "One way I like to think about it is the granularity at which they implement auto def, right?",
                    "label": 1
                },
                {
                    "sent": "So something like scikit learn you get a whole model and you actually can't even compose them, so you have a whole model and that's it.",
                    "label": 0
                },
                {
                    "sent": "Was something like torch NN you get these big layers and you compose the layers like Lego blocks, but you have no access to the operations that are inside those Lego blocks.",
                    "label": 0
                },
                {
                    "sent": "And what I would call full automatic differentiation which is implemented by Autograde in Theano and Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "You can use really any operation in a host language and autograph that host language happens to be torch and Theano and Tensorflow.",
                    "label": 0
                },
                {
                    "sent": "That host language is a language inside of Python, which I think is a distinction that needs to be made.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Practically, this level of granularity is enforced by what partial derivatives the developers happen to expose to you, right?",
                    "label": 0
                },
                {
                    "sent": "So you get layers and you can only compose layers because those are the partial derivatives that the developers of those libraries have exposed to.",
                    "label": 0
                },
                {
                    "sent": "In order for you to compose them together to use an automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "But sometimes people spend a lot of time implementing a very efficient layer, like a convolution on the GPU that engineers add in video.",
                    "label": 0
                },
                {
                    "sent": "Or Nevada systems have spent a long time making really really fast, and you want to use that layer, but you have some weird loss function and there is no fast implementation of that.",
                    "label": 0
                },
                {
                    "sent": "But you just need to correct implementation.",
                    "label": 0
                },
                {
                    "sent": "So perhaps you want to compose these different styles together so we.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't want any limits on how what style we need to use, and we can actually write neural networks in an arbitrary style in autograde, so he.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just kind of three different styles of writing a three layer multilayer perceptron using Full Auto diff.",
                    "label": 0
                },
                {
                    "sent": "So this is the most granular level where we're defining all of the.",
                    "label": 1
                },
                {
                    "sent": "Parameters ahead of time and then we're using torch functions to do the matrix multiplications in the tenacious and the loss function.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also pop one level up in this kind of get your question, which is in autograde.",
                    "label": 0
                },
                {
                    "sent": "We have this submodule called NN so it's like Autograph Dot North and inside of that module or every single engine module that's normally exposed in torch NN.",
                    "label": 0
                },
                {
                    "sent": "So you can use everything that has been implemented kind of prior.",
                    "label": 0
                },
                {
                    "sent": "There's a ton of work that's gone into that and so you can use a functionalized version of all these modules where you might instantiate, for instance end linear and you get back a function which implements the linear layer.",
                    "label": 0
                },
                {
                    "sent": "And the parameters for that linear layer.",
                    "label": 0
                },
                {
                    "sent": "And so you just build up your whole network here and then you can call these functions where the first argument are the parameters in the second or the inputs and you can compose these as big layers here, so you can use codeine and convolutions here, along with whatever arbitrary torch could you please.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also sometimes you just want a neural network or sometimes you just want a continent and auto grad.",
                    "label": 0
                },
                {
                    "sent": "We have a module submodel submodule where you can just grab entire networks.",
                    "label": 1
                },
                {
                    "sent": "So if you want to compose a CNN with analysis TM or build a generative adversarial network and composed two networks together, that way it's very easy to grab the whole network and then compose them together.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of the high level.",
                    "label": 0
                },
                {
                    "sent": "That's like some tour.",
                    "label": 0
                },
                {
                    "sent": "A tour through torch autograde",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At Twitter, practically this has allowed us to kind of prototype without fear, so we can dive into really weird ideas without worrying about getting bogged down in the details.",
                    "label": 1
                },
                {
                    "sent": "So you can imagine some crazy loss function and just write it down in plain code and expect to get the gradients correct the first time.",
                    "label": 0
                },
                {
                    "sent": "What that means practically is we do try crazier, potentially high payoff ideas more often, because it's kind of free to get the gradient so we don't have to worry about doing that context switch where you go from programming mode to math mode, which can actually kind of be painful, at least for me sometimes.",
                    "label": 1
                },
                {
                    "sent": "And also, once you're done training your network, autograde just completely disappears.",
                    "label": 1
                },
                {
                    "sent": "It's just regular numeric code at that point that you can just run just as fast as you know, as if AutoCAD didn't exist.",
                    "label": 0
                },
                {
                    "sent": "We've used autograph to train very large models that run on a lot of media that's uploaded to Twitter and then in terms of like the speed penalty, which is probably one of the main criticisms of autograph, it's usually fast enough.",
                    "label": 0
                },
                {
                    "sent": "We do have an optimized mode that's a sensually as fast as using regular North.",
                    "label": 0
                },
                {
                    "sent": "It has some caveats that I can get into if you're interested in that.",
                    "label": 1
                },
                {
                    "sent": "And that's effectively nearly a compiler.",
                    "label": 0
                },
                {
                    "sent": "That's kind of happening at runtime, so it's a just in time compiler for neural networks.",
                    "label": 0
                },
                {
                    "sent": "So in autograph is allowed us to really try new stuff a lot faster, and it's one of the first tools that we reach for.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of other.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if ideas that haven't made their way from nuclear science and atmospheric science, computational field dynamics into machine learning, and there's a lot of really cool ideas that could accelerate the pace of development.",
                    "label": 1
                },
                {
                    "sent": "But also the speed at which networks are trained.",
                    "label": 0
                },
                {
                    "sent": "So here's a couple ideas that I think would be interesting to look into.",
                    "label": 1
                },
                {
                    "sent": "The first is checkpointing, and this is actually been implemented, I think only by MX net at this point in the idea here is you don't actually need to save all the intermediate computation in order to back propagate.",
                    "label": 1
                },
                {
                    "sent": "You might actually delete.",
                    "label": 0
                },
                {
                    "sent": "Or not, save some of your intermediate results and then recompute them as you need them, and this can save you a ton of memory.",
                    "label": 0
                },
                {
                    "sent": "So if you just decided to save every tenth layer, for instance in a Resnet, and recompute 10 layers at a time when you're doing your backwards pass, you pay in computation, but you can save enormously in memory.",
                    "label": 0
                },
                {
                    "sent": "It turns out to be a little tricky to implement this well, and solving this like fully is an NP hard problem, but there's good heuristics for checkpointing schedules.",
                    "label": 0
                },
                {
                    "sent": "Also potentially is faster.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can throw away the intermediate results from like nonlinearities or pointwise functions, because you confuse those operations in CUDA, and so it might be actually cheaper to just do the computation again, then load all that from memory onto the registers that you're doing your computation with.",
                    "label": 0
                },
                {
                    "sent": "Another idea is mixing forward mode in reverse mode, so I mentioned forward modes.",
                    "label": 0
                },
                {
                    "sent": "Really bad forward mode is really bad for doing your entire network as you start with many parameters and you end up with one scalar loss, but in the middle your graph could be any shape.",
                    "label": 0
                },
                {
                    "sent": "So if your graph starts from something small and blows up something very large.",
                    "label": 0
                },
                {
                    "sent": "If you're generating an image for instance, and then you're coming down to a scalar loss, you had this diamond shaped network.",
                    "label": 0
                },
                {
                    "sent": "You might imagine actually doing reverse mode into the explosion point and then doing forward mode up until that point.",
                    "label": 0
                },
                {
                    "sent": "And kind of merging the results.",
                    "label": 0
                },
                {
                    "sent": "So you don't have to just do forward mode or reverse mode.",
                    "label": 0
                },
                {
                    "sent": "You can mix the two.",
                    "label": 0
                },
                {
                    "sent": "There's a name for it, it's called Cross Country elimination.",
                    "label": 1
                },
                {
                    "sent": "Again, it's tricky to implement, but there is potentially giant speedups that are available using technology that just hasn't made it into machine learning at this point.",
                    "label": 1
                },
                {
                    "sent": "Stencils, I won't talk too much about, but you can phrase every element wise computation as a stencil, which is like a convolution or a blur.",
                    "label": 0
                },
                {
                    "sent": "You can talk about those stencils implementing gradients for those efficiently and automatically is tough, and some people in computer graphics do work on this.",
                    "label": 0
                },
                {
                    "sent": "So Brian Gunter and Zack de Vito have some work on this, that's I think pretty compelling, could result in big speedups.",
                    "label": 1
                },
                {
                    "sent": "Source to source compilation, I think is also a very interesting idea.",
                    "label": 1
                },
                {
                    "sent": "So what I've described to you is what's called a tape based reverse mode, automatic differentiation, where we're recording the computation that occurs at runtime, and then reversing that computation to get the gradients at runtime.",
                    "label": 0
                },
                {
                    "sent": "You could imagine, however, looking at the whole source code and then printing out the gradients for that source code next to it, concatenating them, and then returning a new string of a function, and then having that be your gradient.",
                    "label": 0
                },
                {
                    "sent": "So that's called source to source transformation.",
                    "label": 1
                },
                {
                    "sent": "That's the original way that auto diff was implemented.",
                    "label": 0
                },
                {
                    "sent": "You know, back in the 80s and Fortran and people haven't really done this a lot lately, because operator overloading is a whole lot easier than building like Alexa and a parser in compiler.",
                    "label": 1
                },
                {
                    "sent": "So reverse mode with the tape is just easier to implement.",
                    "label": 0
                },
                {
                    "sent": "However, it is the gold standard for performance in these other fields that make use of automatic differentiation a lot, so I think we're actually missing out on potentially a really fast technique for doing auto diff.",
                    "label": 0
                },
                {
                    "sent": "The challenge there is how do you reverse an if statement?",
                    "label": 0
                },
                {
                    "sent": "How do you reverse a for loop?",
                    "label": 0
                },
                {
                    "sent": "These are challenges that kind of have to just be tackled.",
                    "label": 1
                },
                {
                    "sent": "Also, higher order gradients, so if you want the Hessian, you might want to phrase that's taking the grad of the graph of F, and indeed this works in Python autograde and with some caveats and torch autograde, but there's really not that many efficient implementations, so I think the exploration with second order methods or habitual explanation of exploration of 2nd order methods would really expand if it was a lot easier to get Hessian vector products.",
                    "label": 0
                },
                {
                    "sent": "Right now it's just a pain, you just have to spend a lot of time thinking about that there's not many efficient implementations of this.",
                    "label": 0
                },
                {
                    "sent": "But there are libraries that will do this for you.",
                    "label": 0
                },
                {
                    "sent": "But I think there's room in making this something that's really fast.",
                    "label": 0
                },
                {
                    "sent": "You actually want to reach for and like.",
                    "label": 0
                },
                {
                    "sent": "Try to experiment with.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's actually really easy to get started with all this stuff, anacondas kind of the defacto distribution for scientific Python And all of this is now installable an Anaconda, so if you have Anaconda installed, it's a one liner there online.",
                    "label": 1
                },
                {
                    "sent": "9 to get all these packages and start playing with it.",
                    "label": 0
                },
                {
                    "sent": "So that's all the material that I have.",
                    "label": 0
                }
            ]
        }
    }
}