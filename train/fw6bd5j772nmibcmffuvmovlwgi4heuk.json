{
    "id": "fw6bd5j772nmibcmffuvmovlwgi4heuk",
    "title": "Deep Generative Models",
    "info": {
        "author": [
            "Yoshua Bengio, Department of Computer Science and Operations Research, University of Montreal"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_bengio_generative_models/",
    "segmentation": [
        [
            "Thank you Roland.",
            "So for this last lecture I'm going to tell you about something very dear to my heart which is.",
            "The question of how do we train better?",
            "Unsupervised models, probably generative models of the world out there.",
            "And I think this is a crucial component.",
            "Of AI, which is the reason that I'm in this business?"
        ],
        [
            "I'm so.",
            "I have the.",
            "The intuition that over the decades that I've been in research, one thing that happens over and over again is that researchers are humans, and very often they look more to the short term an like a bit of ambition, and very often that's what prevents us from actually reaching our most difficult goals.",
            "So I think many of the issues we see with the current type of machine learning and deep learning.",
            "Are due to our lack of ambition of trying to solve the short term problems and that we might be lucky an make big strides by actually going for the big prize of building machines that understand the world where we live.",
            "So for example, if you think about the adversarial examples issue that Ian Goodfellow raised.",
            "The strategies that one can come up with may help like strategies like some kinds of adversarial negative examples or some kind of smoothing methods.",
            "They might help a bit, but at the end of the day.",
            "What we see with this adversarial examples is not some new configurations.",
            "Give, surprisingly, you know, make the model fail in this surprising ways.",
            "That's basically we see, but there are more trivial ways in which the model fails simply because it just didn't really, really understand the content.",
            "Really understand the image or really understand the text.",
            "So when we we test our like caption generation models, it's very easy to fool it by putting like a background that doesn't correspond to the foreground image object.",
            "If we look at images, there are easy ways to fool them and text the same thing.",
            "We see that it uses a lot of statistical regularity's to try to solve the problem, but very often it misses the point of the actual kind of causal relationship that matter and that allow us to build a mental model that is predictive of the future.",
            "That's what also science is trying to do right to build theories about the world.",
            "That are causal, that we can use to predict what will happen and we can have a long chain of these predictions.",
            "That's called reasoning.",
            "But that only works if you have a fairly good model of the world.",
            "OK, so this is I think we should aim for the traditional view of neural Nets.",
            "Is that neural Nets are good for pattern recognition, but I think we need to go much beyond that in order to approach AI.",
            "And we've seen some steps in that direction.",
            "For example, you've heard about these memory networks and things like that which are based on extending recurrent Nets and using attention mechanisms to try to move towards the idea of reasoning and so reasoning.",
            "Planning all these things are really important for AI and they have been kind of neglected in the traditional view of neural Nets of the 80s and before.",
            "OK, so one of the ingredients for getting there is to have a model that can really kind of simulate.",
            "You know what would happen if.",
            "An innocence that's what generative models are aiming to do."
        ],
        [
            "And so we build these models whether they are generative or discriminative, that create different levels of representations.",
            "And we hope that the higher levels of representation capture the underlying sort of factors.",
            "The what would I like to call the abstractions behind what we see?",
            "The idea is that if.",
            "We are able to do that.",
            "Basically we're able to explain better.",
            "In a sense, that's more accurate rendering of what actually happens in the world.",
            "What is going on, and this is really the key to AAI 2.",
            "Two really good generalization it for any kind of task.",
            "OK, it's not enough to have that it's not enough to have depth.",
            "It's also that these higher level.",
            "Factors that we're discovering actually explain in a causal sense what's going on.",
            "So yeah, so I've used these word this word disentangle in several papers and it's one way to express this idea about what we're trying to discover with deep representations are the those factors that are kind of mixed up in the data we're seeing in the pixels and the characters.",
            "But we'd like to separate them out to disentangle them so."
        ],
        [
            "It's interesting to distinguish that notion of disentanglement from the notion of invariants that is closely related, but is different and has been a very important theme in areas like computer vision or speech recognition.",
            "So when you're doing supervised learning, you want to predict in a typical supervised learning where you have a few outputs corresponding to categories you would like to find features that are as invariant as possible to the things you don't care.",
            "Like translating your image or changing the lighting and to focus on the things that you care about.",
            "That's that's the idea of invariants.",
            "But if you're doing a supervised learning, and if you're trying to understand the world around you, you're basically doing unsupervised learning.",
            "By the way we're trying to find the relationships between everything you're observing so that you could, you know.",
            "Simulate it in your mind so that you can really understand what is going on.",
            "So if you're doing unsupervised learning then the notion of invariants isn't sufficient.",
            "You need we need to extend it.",
            "So invariant says get rid of the things we don't care, but really, if you're going to explain everything, what you want to do is to separate out those factors, right?",
            "So some of them might be the ones that relate to your classification problems, but the other ones, like translation, gives you position.",
            "Well, if you're doing object classification, you don't care bout position, but maybe you want to do object detection and then you do care.",
            "Bout position seems true in speech recognition, right?",
            "If you want to do speech recognition, you don't care about the.",
            "The speaker identity.",
            "But maybe there is another task which is speaker identification where you do care.",
            "So what you'd like to have is a system that listens to a lot of speech.",
            "Anne Anne learns that there are these factors, one of which is the semantic content of what the person is trying to say, and the other is who that person is.",
            "What kind of room this is in, and what kind of noise there is, and so on.",
            "And we we can do that.",
            "OK, so this entangling is a bit different from from invariants, but it's real."
        ],
        [
            "I did.",
            "And we and others have been thinking that this is something we would really like to have in our models.",
            "But really, we don't know how to build models that would have these properties.",
            "We would like that and short of actually building models that do that, we try to measure if some representations that was learned by our unsupervised learning approaches.",
            "Would actually disentangle the underlying factors and you can do that if you kind of.",
            "If you know some of those factors.",
            "Very often we do.",
            "So in the case of the 1st paper here by Ian, when he was at Stanford.",
            "You do know something because this is.",
            "These are videos and you know the camera movement and you know what the objects are so you can check whether the features that are being detected.",
            "Some of them would be very sensitive to some things, but very insensitive to other factors.",
            "So you can measure that and we did something similar a couple of years later on in the domain of texts where we discovered that our denoising auto encoders are stacks of noise.",
            "Quarters where discovering features that were that tended to specialize to some of the underlying factors.",
            "We knew like these were.",
            "These were comments and Amazon on books and music and things like that and so.",
            "So we knew the underlying sentiment because this was actually the task we cared about.",
            "But we also knew where the text was.",
            "The text about.",
            "Is it a comment about a book or is it a comment about video?",
            "And so it turned out that as you go, as you went on higher up, you saw those features that tended to specialize in the underlying factor.",
            "So this is, it seems that some disentangling is happening, but we don't fully understand why and how.",
            "One coincidence maybe is that these early models were using sparse representation, so there was a sparsity penalty like an L1 penalty.",
            "And today nobody talks about sparsity anymore.",
            "It's out of fashion, but maybe we should think more about it."
        ],
        [
            "I think I showed a variant of this slide in my first lecture which talks about the relationship with causality and unsupervised learning.",
            "So.",
            "So there is something funny going on when you're trying to model.",
            "Say pairs X&Y depending on whether X is a cause of Y or Y, is the cause of X.",
            "The way that you're going to model those relationships that the conditional distributions is going to is going to behave differently.",
            "You're going to get different results in particular.",
            "If Y is one of the causes of X.",
            "Which is, I think, what happens for most of the problems we care about, so we want to predict the cause given the effects we see an image we want to know.",
            "You know what are the objects in it.",
            "We hear sound.",
            "We want to know what was the intention.",
            "That's the cause, right of the person speaking.",
            "So for very, very often we're trying to do this.",
            "We're trying to recover some of the causes, given the effects, and if that's the case, then.",
            "P of Y given X which is at the end of the day.",
            "What we want to do.",
            "We want to predict Y given X. Um can be rewritten in this, you know, usual Bayes rule form.",
            "And where P of X is just a normalization of the numerator with P of X given YNP of Y, so of course we can always choose to write, you know our joint in any way we want, But when we write it.",
            "Like this, if X is really A cause of Y.",
            "Then I'm sorry if X is really an effect.",
            "And why is the cause then P of X incorporates information about why, right?",
            "So because P of X is a sum over the wise of of this numerator.",
            "In other words.",
            "Of course you can always write this join to any way you want, but if you write it this way, it's the.",
            "It's the natural way that the data was generated an you would expect that you can build a more compact model.",
            "Furthermore, if you only observe X or you have many examples that are unlabeled, you only observe X, then that will contain information implicitly about why, because the the form of P of X, the best form of P of X is one that includes Y in it.",
            "So think about the clustering problem.",
            "I think I showed that example.",
            "This is like a very simple minded view of what really is happening.",
            "Of course you have say two clusters.",
            "Two Gaussians are well separated.",
            "Cluster one corresponds to y = 0.",
            "Cluster two to correspond to y = 1.",
            "And.",
            "You know a really good model is one that implicitly represents this.",
            "This P of X as.",
            "The mixture between these two cases, so there is like an implicit.",
            "Discrete variable that tells you in which cluster you are.",
            "An if you think about object recognition and unsupervised learning for object recognition is very much like this.",
            "There are natural categories of objects like cats and dogs and tables and people.",
            "And even if we don't get the labels.",
            "We, because of the statistical structure, we get a sense of what these categories are.",
            "This is the way that a child was never seen a picture of some African animals.",
            "They will be able to recognize from a single label case the category in new cases, right?",
            "So you show pictures of zebra, the child.",
            "I've never seen a zebra, but China is seen.",
            "Other animals, a child as built.",
            "An underlying model of how objects and animals look like and you just need one label to get it right after that.",
            "OK, so this motivates.",
            "Model that go in the direction from Y to X even though at the end of the day we want to go from X to Y very often.",
            "So that means if we don't observe the wise all the time, especially that.",
            "What we would need to do is to discover.",
            "Some latent variables which may be related to Y, hopefully that which we're going to call H. In general for hidden units.",
            "That explain X. ANAN hopefully are related to why and if we do semi supervised learning.",
            "Then we can make that connection you know clear and strong.",
            "And we saw a bit of dot in the semi supervised model that Aaron presented this morning with the denoising autoencoder."
        ],
        [
            "So as soon as you the thing however, is that as soon as you introduce latent variables in your models and you try to go predicting X.",
            "Given Y or X given H. Rather than predicting why granex things get harder for some reason.",
            "Latent variables are in a sense.",
            "Really, really important too.",
            "Deal with the machine learning problem, so I wrote here.",
            "It helps to avoid the curse of dimensionality.",
            "This.",
            "This required would require a bit more discussion, but I remember in my first lecture I talked about how in this representations.",
            "And you heard later about our BMS, how you can.",
            "You can represent a very complicated distribution that has an exponential number of components using a fairly compact representation.",
            "Now, so unfortunately this these advantages come with intractability, so exact inference, in other words, predicting age given X.",
            "If you have a model that goes from.",
            "From H2X and trying to go backwards in general is intractable and you need that when you want to use the model and it turns out you also need that in many learning approaches like the variational autoencoder or the Helmholtz machine, which is the ancestor of that thing.",
            "Um?",
            "So that's for directed models in the next few slides, I'm going to tell you about undirected models, which for which this is easier.",
            "The inference is easier, but you have another problem, which is this normalizing constant you've heard about with some of the lectures we had earlier.",
            "So that this normalizing constants is intractable, and unfortunately, because the normalizing constant also depends on the parameters and you want to compute the derivative with respect to parameters, the gradient of the long black dude is going to be intractable, so a lot of what I'll be discussing today and what you've already heard about a bit with Aaron's lecture.",
            "Is an exploration of alternatives to the the."
        ],
        [
            "Instead of standard log likehood maximization approach.",
            "That's common in statistics and and the most common in machine learning as well.",
            "So to see a bit more where this normalization constant hurts us.",
            "So think about.",
            "A model like this where the marginal distribution on X, so we have X&H.",
            "We have the joint distribution, but we marginalized it, meaning that we look at the aggregating over all the values of H. What do we get?",
            "We get P of X.",
            "And so there's this intractable sum here.",
            "And there's another intractable some in here, which is some over all the X and age or integral.",
            "In case there are continuous.",
            "And these these.",
            "Both of these sums that show up in the likelihood.",
            "They also show up in the gradient, obviously.",
            "But it's interesting to see a little bit more detail when we look at the gradient people.",
            "I guess this comes maybe to Geoff Hinton in the 80s have broken the gradient into two components, and I think Hung Black talked about this.",
            "There is what's called a positive phase component of the gradient and the negative phase component of gradient.",
            "And the positive phase basically means how do I change the parameters so as to make the energy of the of X energy is just minus log probability are normalized.",
            "How to make that energy lower and the negative says phase says?",
            "How do we make the energy of everyone else higher in particular?",
            "Putting more emphasis on those at the model.",
            "Currently things are good, so this is kind of funny and it takes awhile to understand that really, the long lanky gradient tells you you want to make the training examples more probably more.",
            "I have a better score, but you want to make the score of the configurations that the model likes to be worse.",
            "And you know it may not be obvious why you do that, but the algebra is very straightforward and you can generalize that to the case where you have a joint distribution.",
            "Now, in addition to this, you have a some overage coming everywhere.",
            "An in general, it turns out well for different kinds of problems that the positive phase is often easy to compute, or you're going to have reasonable approximation for that sum.",
            "But but that part is usually very hard because it requires sampling.",
            "I mean, we can't do this sum exactly for sure typically, but we could use Monte Carlo machines.",
            "We need to find an X~ here sample from P or an, X~ H~ from the joint, and that.",
            "It is typically hard, unfortunately, for many models."
        ],
        [
            "So this is a thing that I've been trying to figure out.",
            "Over the years I spent a lot of years trying to do.",
            "Unsupervised learning with both machines and their variants.",
            "In fact, I started working on both machines in my Masters in 86.",
            "But I still think that there are great models, but maybe maximum likelihood is not the right way to train them.",
            "So what happens with maximum like you are the approximations that we typically use is we need to sample these what we call negative examples.",
            "You know, in in in this part.",
            "Here we need to sample these X tildes or X tildes.",
            "I still does.",
            "And as I said, sampling exactly is typically impossible, but.",
            "You can always use Monte Carlo Markov chain methods, and I guess you didn't get a course in multicolor Markov chain methods.",
            "But one thing to know about them is that we're going to.",
            "In order to sample from the thing we care about, we're going to produce a sequence of samples, and that's called a Markov chain because you only use the previous sample tell you how to get the next one.",
            "At the market part, and as you do these you sample one thing after the other.",
            "Eventually the distribution of the samples you get are what you want.",
            "However, it might take infinite time to get the right distribution.",
            "It doesn't, but.",
            "Turns out that things get harder.",
            "In other words, it becomes more difficult to get good samples when the distribution you're trying to learn from his sharper.",
            "So what I mean by sharper is that it has these regions of very high probability and an other regions are very low probability.",
            "So the if you think of the density function is a function, it's a function that's not smooth at all.",
            "Remember, we talked about manifolds alot.",
            "Pascal talked about manifolds in particular, so if the data is concentrating near these manifolds, it means the density is very sharp.",
            "Is nonsmooth right?",
            "Because you have this abrupt transition from something likely to something unlikely?",
            "So if you've taken an English sentence and you you switch some?",
            "Words randomly.",
            "I mean, you just switch one word and it may become nonsensical.",
            "So you go out of the manifold of natural language sentences you use which you know half of the words and it's total garbage.",
            "And for pixels, well, you have to flip more than than half of the pixels something before you start seeing that doesn't make any sense, but, but in fact.",
            "Most configurations of pixels are unlike natural images, which means that the true.",
            "Distribution function is very, very concentrated.",
            "Very very sharp, is much sharper than in this figure.",
            "And that's a problem, because these MCMC these chains they you know they make small steps.",
            "They have to make small steps, otherwise the whole thing just can't work.",
            "The volume of space is too large and an and and the other constraint is that at each point in the chain the Markov chains are trying to sample more and more probable things.",
            "So they go from a configuration to one that's slightly more probable, and that's how they climb towards good configurations.",
            "And then they stay.",
            "Among good configurations, but you can see that if you have many modes that are disconnected is going to be very difficult with this kind of process to visit all the modes.",
            "I mean you're going to find one of the modes, and then you're going to stuck there because you can't.",
            "You can't go through these unlikely examples by making small steps to reach this mode from this mode.",
            "So the result is the negative phase of the gradient in these undirected graphical models is going to.",
            "Is going to become bad is going to be a poor reflection of the true gradient, and maybe that's one reason why we are having trouble.",
            "Sort of scaling up these models beyond hymnists."
        ],
        [
            "One glimmer of Hope was given by this paper a couple of years ago.",
            "Where we found that if you train RBM's.",
            "Or denoising warters.",
            "By stacking one on top of the other and you look at the Markov chains that are generating so you can have one PBM or one denoising order.",
            "I'll show you later, or how you can sample from a denoising order or contract with encoder.",
            "But in any case, you get these Markov chains in both cases.",
            "And so if you if you look at the GBM at the low level.",
            "It really has as training progresses it becomes hard for it too.",
            "To move from mode to mode.",
            "So if you think about.",
            "Faces it will lock on some type of face and then it will move very slowly and stay.",
            "You know in that region of faces.",
            "However, if you stack another RBM on top.",
            "When you run the Markov chain there, you see that now it mixes between different types of faces or different digits.",
            "In the case of M nests much more easily.",
            "So that was interesting, right?",
            "So it looks like.",
            "The representations were getting higher up.",
            "Are more amenable to mixing between modes than the low level representation or the raw data.",
            "So why is that?",
            "Well, so I have a. I have a theory about."
        ],
        [
            "This and we've tried to kind of validate it in that paper.",
            "Basically, the theory is that.",
            "Even though in the input space the manifolds are very thin and concentrated in some places.",
            "And could be far from each other.",
            "In the space of learned representations with our BMS and autoencoders and stuff like that, things are different.",
            "The regions of high probability occupy a larger fraction of the total volume.",
            "And the different modes corresponding or different manifolds maybe corresponding to different categories are somehow closer to each other.",
            "To get a sense of what's so, to try to validate this mental picture, we run some experiments that I'm going to try to explain to explain here.",
            "So this image of a 9 here in this image of a three are coming from the training set of M. Nists.",
            "And imagine you run a linear interpolation between those points in the pixel space.",
            "So what is a linear interpolation is just adding up two vectors with a convex combination here in between.",
            "So it's just adding up the image of a nine with the image of a three and what you get, you get something that's doesn't look like an ominous digit.",
            "So it means that.",
            "If you take the manifold of digits, it's curvy, right?",
            "If it was flat.",
            "Then when I do a linear interpolation, the things inbetween should also be likely.",
            "Like like here, right?",
            "So if you have a flat space and I take 2 points and interpolate the means that the things in between are also on the manifold.",
            "And so I should see plausable things.",
            "But it's not what we see.",
            "We see that the things in between are implausible.",
            "That means the manifolds are really curved.",
            "Now let's see what happens.",
            "So this explains the first row here.",
            "Now let's do the linear interpolation in the space of the first level representation.",
            "In the second level representation, and so on.",
            "That was learned by these autoencoders.",
            "And we see these curves.",
            "So if you look at the second level interpolation, what you see is that.",
            "Now almost every image on the route from here to here.",
            "Looks like something that could come from the training set.",
            "OK, so how do we get these images?",
            "By the way?",
            "Of course now this is in the space of hidden representations and I do a linear interpolation between those vectors, But then of course I have a decoder so I can map back to the input space.",
            "And so I can visualize those what those representations correspond to in the pixel space, and that's this is how we get these pictures.",
            "So the mental picture I have based on this is that the.",
            "The representations we're getting correspond to kind of coordinate system that follows the shape of the manifold and and the decoder does this kind of nonlinear transformation that that that goes from this space with this base.",
            "Sorry from this space, this space and the encoder goes in the other direction, so the encoder and decoder basically folding or and folding the manifold.",
            "Depending on.",
            "At which you look."
        ],
        [
            "So you've heard about autoencoders.",
            "Andy noisy one quarters.",
            "I'll remind you that the way we train the denoising order is that we just.",
            "Add extra noise in input and then we try to reconstruct the clean input and there is some intermediate code in between and.",
            "Instead of thinking about reconstruction error as like mean squared error, you can think of it as maximizing the log probability of the clean input given the code and the code you know would have been sampled from some encoder distribution, which could be deterministic.",
            "Or in the case of the variational autoencoder you've seen, it could be actually injecting noise as well here.",
            "OK.",
            "So."
        ],
        [
            "One idea to try to go away from the traditional undirected graphical models.",
            "Is to think of the model of the problem of learning a generative model more directly so usually.",
            "In in classical machine learning and statistics, we think of learning the density or the probability function P of X and we have a a parametric form for P of X.",
            "But if at the end of the day we want to do is not be valid P of X, but to generate samples, maybe another way is to have some kind of black box, maybe neural net that takes in parameters random numbers and output samples.",
            "Right, that would be a true generative model.",
            "'cause if you think about how we get samples in a Boston machine or these Android graphical models, it's actually a very complicated process that we said was failing.",
            "So maybe we could just train a machine that generates our samples directly.",
            "One variant of that is to have a machine that you know actually constructs a Markov chain.",
            "And that's what the generative stochastic networks and also the denoising interpretation of denoising orders I'll give you, fall into that family, so later I'll tell you about the generative adversarial networks which are like this.",
            "Basically, it's a it's a neural net here regular.",
            "Feedforward network it has inputs parameters.",
            "Well I mean usual parameters an but it has random numbers like Gaussian or uniform and it outputs images or whatever you want.",
            "And also tell you about these kinds of guys.",
            "I think that's where I'm going to start with, like the denoising encoder actually falls in this category where we can interpret what the denoising auto encoder is doing and it's extensions that generative stochastic networks.",
            "As learning one you know one step of a Markov chain an what's different from this guy is that now we have introduced this notion of state.",
            "So the box, in addition to outputting samples, outputs a next state and it has of course the previous state's extra input.",
            "And now you can change that and get a kind of recurrent network that has noise injected at every step and also produces samples at every step.",
            "So I think our brain is such a machine now.",
            "What's the principle by which we train it?",
            "We don't know.",
            "Oak."
        ],
        [
            "Made one of the alternatives to.",
            "Maximizing the like you, that's really interesting is called score matching.",
            "Actually, I should say that one of the things that really.",
            "Stimulating in this area of research of unsupervised generative models and deep learning is that there's an incredible variety of approaches that people have looked at for training them with different training principles, and we don't have to stick to the good old maximum likelihood framework.",
            "So here's one which can be modified in various ways, called score matching, and the initial idea is very simple.",
            "Instead of making using the KL divergent between the estimated density and and the data.",
            "Density, which we approximate with the empirical distribution.",
            "We're going to match, not the density, but it's it's derivative or its derivative of its log.",
            "OK, so we have our model P. We know how to compute its derivative with respect to the input.",
            "And let's say somebody gave us not samples but the derivative of the true data generating density with respect to its input.",
            "Then we could just do squared error, minimize that and why is that interesting?",
            "Because when we take the derivative with respect to the input.",
            "The normalizing constant goes away 'cause the normalizing constant remember.",
            "Is the integral over X of some expression, so there's no X in it.",
            "It has the parameters, but it has no X.",
            "So when we take the derivative with respect to X here.",
            "The normalizing constant goes away, so now if if P of X is something that has a reasonably simple form up to this.",
            "Intractable, normalizing constant, then we're in business, right?",
            "We've eliminated the problem, so that was the objective of Aaron when he proposed this 10 years ago.",
            "Now the question is how do we get this guy?",
            "We don't know the truth.",
            "Model score.",
            "Is that true?",
            "Data score.",
            "Well, it turns out that you can rearrange the do a bit of algebra and get this expression.",
            "Where you see these two terms, so the first one is not very surprising.",
            "It says that.",
            "At the data point.",
            "So the X here now is coming from the data distribution.",
            "So at the data points, you'd like the density, so that's what I'm showing here that you'd like at the data points, you want the density to have a critical point.",
            "You want the derivative to be 0, right?",
            "So because these are the places where the probability should be high.",
            "I mean, if you think about what maximum likelihood is trying to do is trying to put a high probability on the examples.",
            "That means ideally it wants to have a mode on each example.",
            "So this is doing the same thing.",
            "It's saying OK, so let's put a critical point at each example and let's make sure that.",
            "These are second derivatives.",
            "Let's make sure that it's curving up in every direction.",
            "So this is not the full Hessian.",
            "This is something that's actually easy to compute, well, not too expensive to compute.",
            "Because it's.",
            "Per per dimension is coming over to dimensions of the input vector.",
            "OK, so this has been used and there's a literature on this.",
            "And the interesting thing is that it's connected to.",
            "That"
        ],
        [
            "Annoying 1/4 and I'm going to try to explain that this connection was made by Pascal Vincent, whom you heard during this summer school.",
            "In a paper in 2011.",
            "So.",
            "Here's the first sort of the manifold interpretation of what the nosing around quarter does, and then we'll make the connection with score matching.",
            "So imagine your data are concentrated on a manifold or near manifold and not exactly exactly on it, right?",
            "And So what we do within the denoising order framework as we take a training example an we add noise.",
            "So in this sort of simplistic view, let's add Gaussian noise so it can go anywhere in this ball.",
            "And in a high dimensional space, the manifold is a low dimensional object, so this this is almost always going to be outside of the manifold.",
            "So it means that when we add noise we produce a configuration that's less likely according to the true data during distribution.",
            "Then the original.",
            "And then what are we going to learn?",
            "So this is how we prepare our data.",
            "When you have the input.",
            "I mean we have the input and the target and we're going to learn the backwards arrow that goes from the corrupted input.",
            "To our target, which is the clean input.",
            "And so we learned this vector field, which everywhere in space is pointing towards the clean stuff.",
            "Which is where the manifold concentrates.",
            "And I can show you."
        ],
        [
            "Immediately a picture."
        ],
        [
            "What you get?",
            "So this is.",
            "This was done by Guillaume alone was sitting somewhere here.",
            "And.",
            "We have this 2 dimensional data with a 1 dimensional manifold and he trained a denoising order and then when we see these little arrows are.",
            "What the autoencoder is learning?",
            "It's the difference between the reconstruction.",
            "And the input.",
            "So if we start at that point and we do encode decode, we get here.",
            "So that's what this vector field tells us.",
            "So when you look at this, you should get the feeling that, well, we've basically learned what the density was, right?",
            "We know what to do to go to places of high probability which is just follow the arrows.",
            "Once you get there, the arrows are, you know the norm.",
            "You can see like little dots here is because the arrows are becoming very very small, which is normal, like the derivative should be 0.",
            "On the manifold.",
            "Just as the score matching was asking us to do.",
            "Anne."
        ],
        [
            "And let's see.",
            "It's."
        ],
        [
            "Yes.",
            "So.",
            "In a later paper bag, Yum and I showed this identity slightly more general context where.",
            "We find that as.",
            "As the amount of noise becomes small, an as the amount of training data goes to Infinity and the capacity goes to Infinity.",
            "So like the nonparametric setting, so in the best of all possible words, which is what the typical.",
            "Statistical analysis is about.",
            "We find that the difference between the reconstruction and the input, which are you know these vectors I showed before.",
            "Should converge to this.",
            "Delock PDX, Ann.",
            "You should recognize this is the score of the well I wear P here.",
            "I should have.",
            "I should have said Q.",
            "This should be Q, so this should be that the score of the true data generating distribution.",
            "So if you have enough data and enough capacity and you let Sigma go to zero as the amount of data goes to Infinity, then you recovered the true score.",
            "So this vector field is really pointing.",
            "In what in the direction where the probability increases the most, which is what?"
        ],
        [
            "What we see in this picture, right?",
            "Do you have questions about this?",
            "Um?",
            "What do you think about this guy?",
            "Huh?",
            "Yeah, so what's the magnitude of derivative here, you think?",
            "Yeah, so this is this.",
            "These are Maxima of the probability, but this should be a minimum of the probability.",
            "Anne, remember the vector were gay?"
        ],
        [
            "Thing is proportional to the derivative of the log probability, so it's going to be 0 either at Maxima or minima of the probability.",
            "Which is interesting because take the square difference here.",
            "Take the norm of this on both sides.",
            "And this is just reconstruction error, right?",
            "So it means that reconstruction error.",
            "Is not exactly the same as energy.",
            "In other words, reconstruction error could be low, either because you have you're in a place near the manifold where the derivative should be small, but it could also be low in."
        ],
        [
            "These funny places where you have a maximum of the probability.",
            "Anyways."
        ],
        [
            "It's just parentesis.",
            "Let's go back to the connection with score matching.",
            "So remember, we're trying to.",
            "Take the difference between the model score, what the model thinks of you, know which direction to go to, increase probability, and the what the data tells us.",
            "So one way to get it, what the data tells us, is to take the difference between the clean example and it's corrupted.",
            "Version right?",
            "Because you think about it.",
            "If I if I have a clean sorry if I have a clean example and I corrupt it.",
            "The arrow going in the other direction basically tells me in which direction I need to go in order to increase probability.",
            "Now there's going to be a little bit of bias here, but it turns out that in average it's going to point in the right direction so.",
            "So now we basically."
        ],
        [
            "An applied directly this original formula.",
            "Case.",
            "We have a way to estimate the model score and we have a way to estimate.",
            "The data score, the data distribution score."
        ],
        [
            "So."
        ],
        [
            "The data distribution score is going to be given by the difference between the cleaner, corrupted and the model score is going to be given by this difference between reconstruction and the original at the corrupted location, and if you subtract 1 from the other, which is what you want to get the square difference, then you get a cancellation and you just get this denoising reconstruction error, which is different from the straight reconstruction error.",
            "So this is a kind of quick and dirty.",
            "Demonstration of the connection between score matching, an denoising autoencoder objective function.",
            "It's not exactly score matching.",
            "Actually is score matching on a smooth version of the original data distribution."
        ],
        [
            "But that's a detail.",
            "Another interesting aspect of these denoising decoders is that.",
            "So let's do a 1 dimensional denoising order or contract.",
            "It will behave pretty much the same.",
            "Um?",
            "So what they're trying to do remember is to have a reconstruction.",
            "Function, I actually didn't say that it's here.",
            "They want to have a reconstruction function that's.",
            "It has a derivative of zero at the data .1 way to see this is if you look at.",
            "The denoising reconstruction and you do a Taylor expansion assuming Sigma is small.",
            "So around X you get the normal reconstruction error plus a term in Sigma squared times the.",
            "The norm of the Frobenius norm of the Jacobian of the reconstruction.",
            "So this is a vector of reconstructions.",
            "However, the spec to the input and that matrix that Jacobian matrix.",
            "If you somehow the square of all the elements, that's what this means.",
            "So basically it says it wants the derivatives of the reconstructions with respect to the inputs to be small.",
            "And it wants to have small reconstruction error.",
            "So how do you get these two objectives together?",
            "Well, you want the derivative to be flat at the examples, but you want the examples to line on this identity line, right?",
            "The because that's you know if get R of X = X, you want the reconstruction of X1 to be X1.",
            "The reconstruction of X2 to be X2, and so on.",
            "So so you need to have points that are at the same time on the identity line.",
            "And you want to have flatness around the examples, that's the regularizer.",
            "The regularizer here is something that wants locally constant features, and if you remember.",
            "The adversarial examples we're trying to also get you locally constant features.",
            "OK, so that's it's kind of it kind of helps to see in a very toy way what what the.",
            "What does denoising thing is really doing."
        ],
        [
            "Now.",
            "The cool thing is."
        ],
        [
            "OK, I'm going to skip that.",
            "The cool thing is not only we get this interpretation of the denoising order is telling us in which direction the density is increasing, but you can also use these to sample once you've trained them, and so we define a Markov chain, which is very, very simple and intuitive at each step of the Markov chain we corrupt.",
            "So let's say add Gaussian noise or put masking noise, whatever.",
            "Encode, decode and then sample from the reconstruction distribution.",
            "So we for this to this sample means you know we come back to the definition where we said that."
        ],
        [
            "We're doing here with the decoder is produce a conditional distribution of X given age.",
            "So if we think of it like this, we can sample X given age, which means in the case in the Gaussian case, which it means we add an amount of noise that's proportional to our reconstruction error.",
            "OK, so so that defines a Markov chain."
        ],
        [
            "And it turns out."
        ],
        [
            "So let me show you a sort of more example of this Markov chain.",
            "So let's say your data is sort of on this manifold an you start from the point on the manifold, you corrupt it, so you get the X~ Then you encoded decoded.",
            "You go back in in the space of X.",
            "And what you get is it is a distribution, let's say a Gaussian distribution.",
            "So now you take your mean.",
            "You add some noise within that covariance matrix and you get one of these guys an you do it again and that's going to be a random walk.",
            "That's good, tend to stay on the manifold an visit.",
            "The manifold like you would expect of any Markov chain.",
            "OK, and."
        ],
        [
            "And one reason you might want to do this while is that.",
            "Although P of X might be very complicated because it has many modes.",
            "P of the Markov chain P of the next state given the previous state might be much simpler.",
            "So to illustrate this, which is what we are trying to learn right instead of learning P of X directly, we're trying to learn one step of the Markov chain.",
            "So you know the true P of X might be this blue thing with many modes, just as an illustration.",
            "But you could generate that distribution with a Markov chain that goes with local moves that.",
            "And then each of those steps could be very simple.",
            "Maybe this bimodal distribution here, so by conditioning on the previous state we make our the generative part of the model.",
            "Hopefully much simpler because it doesn't have to deal with all the modes, only those in some neighborhoods.",
            "So that's one motivation for learning a Markov chain rather than learning directly P of X."
        ],
        [
            "And so the good news with these denoising order is that we can show that this Markov chain.",
            "I was telling you about in uh, again, in a symbiotic sense, estimates the true data general distribution.",
            "So more precisely, the theorems we have say that if your denoising model which goes from the corrupted to the clean as a conditional distribution approaches the true conditional distribution, the true denoising distribution.",
            "Then the the stationary distribution of the Markov chain.",
            "Becomes the true distribution.",
            "The data generating distribution.",
            "So we don't need to model P of X directly, we just need to model the denoising distribution and that's good enough."
        ],
        [
            "OK, and then you can generalize this this Markov chain, which went directly from XT2XT plus one by introducing some latent variables.",
            "And this is what the generalized derivatives with stochastic networks are supposed to be."
        ],
        [
            "Let me show you an example of that.",
            "So remember in the denoising order we only add noise in the input.",
            "Well now you can imagine a more complicated scenario where we're going to generate the Markov chain, but there's going to be noise in the hidden layers, and Furthermore those in layers could be connected to each other, so this is unfolding in time of this kind of recurrent net that has noise injected everywhere and generates a sequence of samples that would converge to the distribution.",
            "We care we're trying to estimate.",
            "And the reason for this particular architecture is that it mimics the debost machine gives chain where you have exactly the same graph except you, you have binary units an at different sampling process at each step, but basically the same computational graph and the way you train this is basically by back prop.",
            "So you unfold the chain from some initial training example through a few samples, and then you say, oh, I'd like that guy to be close to the original, so that the denoising objective used backdrop with the whole thing.",
            "An update the parameters, so that's what basically generative stochastic networks are, and we verified that."
        ],
        [
            "They are able to learn conditional.",
            "I mean to learn to learn distributions both on like Poly data like this."
        ],
        [
            "And images like EM Nists and TFD.",
            "But up to now, I would say.",
            "They're not the best narrative models.",
            "There's one."
        ],
        [
            "That bothered me with with these models as well as with the deep belief Nets and the both machines you've already heard about and Aaron talked about that quite a bit, so I'm not going system much, which is that many of the models we've been considering in the past, including the GSM like I showed before, have this property that the when you think about how an image would be generated, you would maybe have a deep model an in the case of the GSN, it it has like a recurrence.",
            "So it goes through many steps in the Markov chain.",
            "But it has these multiple levels and at every level you would add noise.",
            "And that's bad.",
            "That's bad, because if you think about.",
            "Natural images so.",
            "Like this?",
            "There is no noise, it's all very sharp and clean, so these models are not a problem.",
            "We don't want to.",
            "Ideally we don't want to add noise along the way.",
            "We would like to push the injection of noise higher up so that you get a chance to do a very non linear transformation that Maps the noise to the image is if you just add noise in the low level is just like adding white noise to your images so you're never going to get sharp clean images if the noise is injected everywhere along the way, it has to be injected.",
            "As high as possible.",
            "I mean there may be some kinds of noise that actually are sort of lower level, but most of the factors of variation we care about, like the identity of objects and their transparent their operations that have been applied to them and so on.",
            "These these are very high level objects and you know there's a very strong year relationship between those factors and the input.",
            "I call this noise, but maybe it's a misnomer, it's just.",
            "Random Bibles that are sampled when you generate and in fact usually we think of noises at low level things and the high level things are not really thought of noise but more as the generative factors, yes.",
            "I don't know how to do that.",
            "So one of the very common issues with almost all of the models we've played with is that they tend to generate blurry images.",
            "And so you know, it's not obvious how to get rid of that.",
            "Typically, the better the models, the less you have this issue."
        ],
        [
            "So Aaron told you a lot about this family of models, which is structured like this, where you have an end quarter and a decoder.",
            "And they have this property that the noise is injected completely at the top.",
            "When you generate, you generate from the top here and then you have a deterministic set of transformations to get the visible.",
            "So this is different from the classical host machine where you were injecting noise all the way.",
            "And also the training objective is slightly different and there's been a lot of work and you know Aaron has told you about some of that.",
            "There's more like recent work like the Reweighted Way Sleep algorithm which was presented at last ICM, which is in the same family but uses different training objective.",
            "And we've been exploring other related things.",
            "I'm soon going to tell you about an interesting model called Nice that Aaron mentioned that also.",
            "Has the property that it doesn't have noise along the way, but just just at the top."
        ],
        [
            "OK, Anne, and here's a picture to try to yes."
        ],
        [
            "Yes, it's like it will not really noise.",
            "It's abstractions when it when it gets to be there, yeah.",
            "Oh, you mean in the GSN?",
            "Well, actually you can add noise anywhere you want, so in the GSM is very easy to fix this problem you just.",
            "Put zero noise in the lower layers.",
            "Well now it's not that easy actually, because."
        ],
        [
            "Said something wrong, let me."
        ],
        [
            "Show you the picture."
        ],
        [
            "Again, so The thing is, if you don't add noise in the lower levels then it can kind of cheat, right?",
            "It has.",
            "If there's no noise at all, like, especially if you don't put noise, no corruption, then it can cheat and it can.",
            "Just kind of copy things.",
            "So well, so you either.",
            "Have noise in the input.",
            "Or noise in the middle layers, but it's not enough to have noise on the top, and for these kinds of models, but noise in the input is is OK, so this is actually the best results we got were actually with no noise in the lower levels, an corruption in the input."
        ],
        [
            "Alright."
        ],
        [
            "So let me go to this kind of geometric picture that.",
            "At least has helped me to understand what I think we should be doing.",
            "If you go back to this manifold picture.",
            "Remember I said what we have in the data space is a very complicated manifold like I drew this wire here.",
            "It's actually in a high dimensional space, so it doesn't really cross on itself typically.",
            "And.",
            "If you think about what's going on with the variational, autoencoder or other models.",
            "What we really hope is that the representative, very simple, like a Gaussian.",
            "And Furthermore, it's a Gaussian that's.",
            "Basically, has dimensions where the the variance is very small.",
            "And dimensions where the variance can be large.",
            "Basically that's what your manifold is, right?",
            "So it's a tiny tiny variance in the directions orthogonal to the manifold, and then in the direction of the manifold you have lots of variance because you can move around as you want on the manifold.",
            "So like one very flat Gaussian.",
            "That would be one manifold.",
            "Of course, you could have many manifolds, but let's say you have one manifold.",
            "So what you want to do is to take a twisted curve and untwist it to get a flat one that you can model with the Gaussian.",
            "So one way to think about this is we're trying to find a two way transformation between these two spaces that takes the manifold.",
            "The twisted thing here, and flattens it, and of course can go backwards.",
            "If we could do that, then we'd be in business.",
            "An you can think of.",
            "You can think of making these steps stochastic and then you have conditional distributions rather than functions.",
            "And some approaches do that."
        ],
        [
            "So the nice approach haha.",
            "Nonlinear independent component estimation nice.",
            "It was trying to do exactly that, so but you're looking at a terministic version of the transformation, and we wanted that that transformation to be invertible.",
            "Turns out that if you have an invertible function, so you have F and G = F -- 1.",
            "Then things become really easy, at least in principle in terms of maximizing the likelihood.",
            "So there's a very very simple equality here that comes from the change of variable equation.",
            "You know, if you have an integral an you change variable from this space that space, then the density index can be basically mapped to a density in the H space when you just map X2F.",
            "But now you have to pay a price for sort of how much volume.",
            "Change you're getting when you go from the X Space H space that's given by the determinant of the Jacobian of this encoder.",
            "So this is an equality.",
            "The question is whether we can compute that determinant easily.",
            "And also whether we can get an F such that we can compute its inverse, because you notice that if it's not invertible, then that will be 0, and then you have log of zero and it is just doesn't work.",
            "Maybe there's a way to fix that, by the way, but anyways, the straight formula looks like you need to have an invertible encoder an it has to have the same dimension so you can have a Jacobian as the original space.",
            "So we aimed to do that and we found a way to do it.",
            "Um?",
            "Oh so at the top level we want something really, really simple, so we're going to ask the transform data to be to have a factorized prior distribution.",
            "So this is very common in all of these models, like in the variational encoder and the original Hamels machine that we assume that the underlying factors at the top level are independent of each other.",
            "It's very strong.",
            "Assumption is probably not correct, but the first approximation.",
            "This is a very good thing to aim for.",
            "OK, and then we know that any distribution can always be generated that way, right?",
            "So you know how are random numbers, random variates generated in the computer.",
            "You start with uniform numbers, and then you do some operations on them and you get whatever you want.",
            "This is how all of the random numbers generators work, right?",
            "So so you think of that like this, we have uniform numbers here or Gaussian, and we apply a nonlinear transformation and then we get some whatever distribution we want.",
            "So we want to learn that mapping and back such that what we get here is the data distribution."
        ],
        [
            "Alright, so here are some samples from that model.",
            "MNIST is OK. See far is not OK as VHN is not OK but.",
            "It does something sensible."
        ],
        [
            "I also do inpainting.",
            "Meaning that you can, because we have a formula for the likelihood, we can just do gradient descent on it.",
            "If I give you only the bottom part of an image, I can change the inputs here by just increasing the likelihood by just doing gradient descent on the likelihood gradient descent on the likelihood an in find values.",
            "Like like this year.",
            "So the top part has been filled here that here the bottom part has been filled and so on.",
            "The left or right whatever.",
            "Even in the bottom I think is like one pixel out of two that's been corrupted, so it seems to be just noise.",
            "But if you do great in the sense you.",
            "You get nice pictures."
        ],
        [
            "Alright."
        ],
        [
            "Now how did we get that to work?",
            "Involved a Riddle, which is how we have a parameterized function.",
            "That is going to have a determinant that we can compute, say, determinant of 1.",
            "Ann is invertible and we can compute the inverse easily.",
            "That seems like a tall order an before we started this, we didn't know how to do that, and so there's a trick.",
            "Basically, we're going to.",
            "You can think of the trick is making the function kind of lower diagonal.",
            "If you think about its different components.",
            "So think about.",
            "Two scalars or two vectors, so we can decompose the input into X1 and X2, and we're going to.",
            "Decompose our final function as the composition of a bunch of stages, and each of these stages is going to have this form.",
            "It takes X1 and X2 and it produces Y1 and Y2 and and what it has here is just a copy.",
            "So why one is a copy of X1 and Y2 is just a copy of X2 plus some nonlinear function of X1 and that gives rise to a Jacobian matrix like this.",
            "Right, because the copy means you have identity blocks and you have zero here because there is no connection between X2 and Y1 and so the determinant is easy is 1 and it doesn't depend on the parameters, so we're not.",
            "Each such stage is not changing the volume at all, right is keeping.",
            "Volume properties.",
            "But it's moving things around nonlinearly, and it turns out that if you stack enough of these stages, not that many, like 510 or something you can, you can learn the kinds of distributions I showed you before.",
            "Now I I somehow don't think this is the right way of solving the problem and the reason I think it's a little bit problematic and this is just an intuition, is that the wise here are in a sense in the same space as the axis, right?",
            "First of all they have the same dimension.",
            "And you see, because of the copies that they're kind of in the same space.",
            "Now they're more like residuals, right?",
            "If you think about it, this is, like, think of this like you're predicting Y 2 from X1.",
            "And you subtracting X2, we adding, but it's the same thing.",
            "So what you're getting is like a prediction error of Y2.",
            "Given X one and you look at the error.",
            "A prediction and you basically have the original 1 and you have the error in predicting you original X1.",
            "You have the error in predicting X2 using X1 and this is what you pass.",
            "So you just compute these predictions of things given other things.",
            "And you, you pass the residuals and eventually those residuals become Gaussian.",
            "If you do things well.",
            "But there I have the feeling that they're not going to be high level abstractions, just going to be residuals.",
            "OK."
        ],
        [
            "So let's go now to the generative adversarial networks.",
            "I don't have a lot of time.",
            "Anne.",
            "So this was published."
        ],
        [
            "Adds NIPS 2014 just the last nips.",
            "Anne, it's yet a completely different way of thinking about learning Java models.",
            "So again, like I said before, we're going to skip having a P of X, unlike in the nice where we had an explicit P of X.",
            "And we're going to skip the Markov chain idea as well, so it's going to be."
        ],
        [
            "Like this, just neural net takes random numbers output samples.",
            "Question is how we train this guy?"
        ],
        [
            "So the way we're going to train it is by playing a game.",
            "And there's no variational bound either."
        ],
        [
            "So we're going to play a game.",
            "The game has two players.",
            "The generator, which is the thing we want to train at the end, right?",
            "So that's the neural net that takes random numbers and outputs images.",
            "And the discriminator.",
            "So we're going to call the generator G and this quarter D An what the discriminator does is try to figure out.",
            "Try to evaluate how good examples from the generator an it is in the December is going to be a classifier, as its name indicates, an it's going to be trained to discriminate between a sample from the data distribution, like your training examples and the sample from the generator.",
            "So that looks like a very simple idea right away to measure the quality of a generative model is that if somebody looks at the generated samples, the whole distribution right?",
            "So you have a.",
            "100,000 samples coming from your model and looks at the training data and can't find a way to classify and discriminate one against the other.",
            "That means you must have a very good classifier.",
            "I mean a very good.",
            "Not a good well means you have a good generator.",
            "You want to have a good classifier.",
            "Otherwise your measure of goodness is not going to be very useful.",
            "So this computer has to be very good.",
            "It's trying to so let me see there."
        ],
        [
            "This is a nice."
        ],
        [
            "Yes, you can.",
            "Think of it like a game between the police and the counterfeiter, right?",
            "So the generator is trying to counterfeit the real money coming from the data.",
            "And and this community is, like you know, the police trying to figure out.",
            "Is this a real bill or it's made up?",
            "So they are constantly fighting each other, trying to beat each other, and you can define an objective function.",
            "Which is like a game theoretical or mini Max or saddle point or whatever objective function where one of the players wants to minimize it and the other ones to maximize it.",
            "And the game is basically what you would expect that the discriminator is trying to predict whether the data comes from one or the other thing, and the generator is sending inputs to discriminator and trying to make it provide the wrong answer.",
            "This is what this equation is saying, just in algebra.",
            "And you can.",
            "You can prove what the optimal discriminator would be.",
            "It's just by Bayes rule.",
            "What if you knew the data distribution and you knew the model solution?",
            "If you have the density is a formula, then you could you could find the optimal deevak.",
            "So first we have neither P Dayton or P model in our case, so we can't actually compute this.",
            "But we can train a model that classifieds between the two distributions."
        ],
        [
            "So.",
            "OK, sorry, so in terms of operational terms, the discriminator is going to be trained on two kinds of data, so we have X either sample from the data and then send to the discriminator and we would like it to output category one or X comes from the generator which takes input noise.",
            "Nonlinear transforms it.",
            "An gives these counter feed samples, and then the discriminator is supposed to say 00 now.",
            "On on the funny part is on the examples here.",
            "D is trying to improve its classification, but G is trying to make that guy make more mistakes.",
            "So that was the second part of the objective function that I showed here.",
            "I know this guy is trying to make the log like you'd better for himself, and she's trying to make it worse because we are minimizing over Gmail.",
            "Yeah, so yeah.",
            "So you have these two neural Nets and you just, you know stick one to the input of the other an backdrop and basically you insert a a -- 1 in the gradient here when you back propan.",
            "Or you change the sign of the updates here whatever.",
            "And that's very simple to implement."
        ],
        [
            "Um?"
        ],
        [
            "What happens during training though is a little bit funny.",
            "So and it took us some time to understand what was going on and also to understand its failure modes so.",
            "Here's a picture to try to understand what is ideally going on.",
            "Think of.",
            "The Latent Space Z as a 1 dimensional real line.",
            "So you have a distribution.",
            "An initially you say, so there's at a uniform right?",
            "And we're trying to learn is a transformation that Maps every point on the real line.",
            "Here to a point on the X line.",
            "So this is the data space and what the transformation function the G does is.",
            "It's trying to map the uniform distribution to distribution, which puts maybe a lot of mass here, and no mass here, right?",
            "So that you can get that by, you know some kind of nonlinear function like this.",
            "So initially G is bad and these bad.",
            "But let's say we keep G fix and we can optimize D, which is the right way of thinking about it.",
            "So now, given the current GD is trying to discriminate between the data distribution which is putting mass in the middle here the black dots.",
            "And the samples coming from G and because they are more black dots on the left and on the right.",
            "Then it learns this.",
            "Maybe this, you know classifier that says, oh here's the decision surface and so on.",
            "So this is the output probability for the discriminator.",
            "Now it's done a good job and now that we have a good D. We can update G. So now G is going to try to fool D. How could it fool G?",
            "Well, it needs to shift some of these arrows to the left so that they will pass for good bills, according to the discriminator.",
            "So.",
            "So you do a step of gradient in that direction and then the green distribution moves a bit to the left.",
            "Now you can see right away where it can get wrong.",
            "How can we get wrong here?",
            "I mean completely wrong if we're not careful, yes.",
            "Yeah, it could go completely to the left, so this is great here.",
            "I mean the problem is even higher and this is totally wrong.",
            "Of course right?",
            "So what you see in this illustration is that.",
            "You have to make a little step in G for every full optimization of deities.",
            "That's what the theory says.",
            "In practice, you have to play with the learning rates so that D can keep up with the changes you're making in G. Otherwise you get into completely wrong results and you can see that happening if you're not careful.",
            "So that's the main thing to keep in mind.",
            "And of course, if you do these carefully, then at the end of the day you have this so-called mixed strategy equilibrium where the game everybody every player in the game is happy and they can't change.",
            "And get happy."
        ],
        [
            "So we get these kinds of samples.",
            "They're good, they're better than the nice.",
            "For example that I showed you.",
            "But they're not as good as what I'm going to show you next."
        ],
        [
            "Blah blah blah."
        ],
        [
            "More samples, so this is this is nice movies that Ian made.",
            "Where?",
            "I you.",
            "You take 2 random points in the Dead Space.",
            "You trace a line.",
            "Remember, we said we can interpolate.",
            "In the edge space and we should be able to see nice samples, right?",
            "And especially here because the distribution is Gaussian or uniform, which this should work.",
            "And then for each point on the line we we project down into the image space and we visualize that.",
            "So now we have a smooth transformation of the image that tells you what happens when you go from one place to another place.",
            "You can see this for the MNIST digits and you can see this for the faces.",
            "There's still like a lot of noise that we'd like to get rid of."
        ],
        [
            "OK, and you can do that for cifar images, so it's it's getting the idea that there are like objects in the middle and the background, but these objects are not really sharp as we would like.",
            "There are much sharper than what we get with the VA though.",
            "And so some smart people at NYU, you and Facebook."
        ],
        [
            "There was a poster about this here at this summer school.",
            "Use this type of generator in a kind of hierarchical cascades.",
            "And I think they say this is currently the best.",
            "Giant models of images.",
            "I'll show you some samples later.",
            "So.",
            "So they figured that."
        ],
        [
            "Generating directly the image is difficult.",
            "But maybe we can break the problem.",
            "Of generating images into."
        ],
        [
            "Into pieces.",
            "By considering a bunch of conditional distributions.",
            "So.",
            "So instead of generating the image directly from scratch from a zed, we're going to create some intermediate.",
            "Variables that are actually deterministic functions of the input an.",
            "In their case, those deterministic functions of the input are the low resolution versions of the image.",
            "OK, so we have the image.",
            "And instead of trying to model directly, we're going to consider a bunch of other random variables which are not terministic function of that original image, and it's just low resolution versions of the image.",
            "And now we're going to consider the problem of conditional distribution right, which is always easier, right?",
            "You should.",
            "Eventually, figure out that learning conditional distribution is always easier than than an unconditional one because those conditional distributions are have less diversity.",
            "Less modes.",
            "Remember when I said learn a Markov chain is conditional in previous state and that should be easier because we condition on something.",
            "Now we have less choices, so it's going to be easier to model that.",
            "So so they exploit this, and now we're going to condition on each level we're going to predict the next one.",
            "The description of the next one.",
            "So we just train a bunch of conditional distributions.",
            "Each of them is going to be.",
            "Basically you're you're gone.",
            "Your generative adversarial network.",
            "That that's the G here.",
            "I guess that takes.",
            "Takes the usual zed, but is conditioned on extra inputs to generate a.",
            "Well, not exactly.",
            "The low resolution version, but the difference between the two images, something you can do with with images that's called applausi and pyramid, so they're trying to generate these differences rather than the low resolution images themselves.",
            "The differences between those, which is kind of.",
            "Well known feature representation for images.",
            "So.",
            "So basically you start by predicting the with the regular ground.",
            "The very low resolution image not predicting, but generating it using using one normal Gan.",
            "This one so it takes is edit produces the low resolution image and of course you train a discriminator that learns to make the difference between the generated distribution and the true ones.",
            "And now you can take those those images.",
            "And.",
            "And use them to condition the next stage, which is going to produce the difference between the sort of low resolution and high resolution version of the image.",
            "And so you sample, you learn to sample the conditional that goes from here to here and then another one, another model.",
            "It can be trained completely separately.",
            "That goes from here to here and here to here.",
            "And."
        ],
        [
            "You can see that process you generate low resolution version of the image and then.",
            "That's unconditional and then generate the next one conditionally, and so on, and eventually you get a high resolution version of the image.",
            "And then the amazing thing is that these images are very good and if you ask people to make the difference between these images and the training images, so we played the Gan game with humans, which is basically what a Turing test is.",
            "At 40% of the time the humans are confusing the generated images for the real one, so we're not exactly doing as well as humans, but this is, I think, very impressive.",
            "Alright, in the last few minutes, let me tell you about another model which is not generative.",
            "But it is related to the denoising auto encoder.",
            "And seems to be beating everything else in terms of learning good features for classification."
        ],
        [
            "In a semi supervised setting.",
            "So this is called the ladder network.",
            "It's very recent work.",
            "So by the way, if you are not used to archive, you can read the time when the paper was submitted the first time.",
            "So 15 Zero 7 means julai 2015 and then the papers are ordered sequentially.",
            "So if somebody tells you my paper is 15 zero 703 something, that means that this one came before.",
            "This is really useful.",
            "One of the tricks you can learn in summer school.",
            "OK, So what is this model?",
            "So you can think of it like a stack of denoising auto encoders with lateral connections.",
            "Along with a feedforward net that uses the end quarters to predict the thing you care about, the wise that you want to predict for classification.",
            "So let's see.",
            "So we have three things here.",
            "We have the end quarter use used during training.",
            "For the purpose of.",
            "Basically reconstruction, so we have encode, encode, decode, decode.",
            "But you see these lateral connections.",
            "So when you're going to decode, you're allowed to use.",
            "Lateral connections here.",
            "You have to use what's going on here.",
            "That essentially frees up the high level from trying to do everything.",
            "So in the old models.",
            "All of the models you've seen up to now, you don't have these lateral connections and what it means is that every level needs to capture everything about the input, especially in the sort of autoencoder type of framework.",
            "Whereas.",
            "You can think of the information to generate.",
            "Here will come either from.",
            "Sort of low level noise.",
            "Or high level abstractions?",
            "And you're combining the two things to generate the image.",
            "OK, but the way they actually.",
            "Design this man presented in the paper is not a generative model.",
            "It's like a feature, semi supervised algorithm, so we're going to have.",
            "Basically two terms with the objective function.",
            "One that says, well, pretty quiet, as well as you can.",
            "This is target given X.",
            "Requires well, as you can actually on this, using the corrupted input, so it's just like.",
            "Dropout network, right?",
            "So you've got a feedforward net with noise injected along the way.",
            "And try to make these.",
            "Zed hat here.",
            "Good reconstructions?",
            "Of the clean path here zed that you get without any noise, so you've got, you know denoising.",
            "Remember we want to clean versus the corrupted so they run this network which is pure clean.",
            "It's also the same network you used to actually do classification on test examples, so no noise.",
            "And we want to match this CD.",
            "Here is the cost that they're adding here.",
            "The squared error, so that's the normal reconstruction error.",
            "But we have one for each level, so compared to clean path activations to the.",
            "Sort of.",
            "That level reconstruction coming from above.",
            "And they optimize this.",
            "Anne."
        ],
        [
            "And they get failure.",
            "Amazing results on the semi supervised and also the regular M lists.",
            "So on the regular amnestied train with everything, so these are a previous results.",
            "It's actually nice to put all of these things in one big table.",
            "So you can see.",
            "The adversarial Nets here point 78 that we're doing quite well.",
            "Um?",
            "And I think hung like talked about the pseudo level pseudo label algorithm.",
            "Well, I guess it's only the semi supervised case.",
            "What else did you hear about?",
            "DGN here is not the variational encoder.",
            "I'm not sure.",
            "Aaron is not there I guess.",
            "Yes.",
            "Oh sorry, sorry, sorry.",
            "Did you write?",
            "This is after survival training?",
            "I got confused.",
            "Yes, thanks.",
            "Um anyways.",
            "Yes, um.",
            "Was do you know what this DGN is with point 96?",
            "OK. OK.",
            "So M1 plus M2 blah blah blah.",
            "OK so yeah, actually, but I remember you had like .96 and anyways.",
            "Anyway, OK, so they get .61 which is currently the state of the art on what's called permutation invariant amnis, where you don't use any information about the fact that it's an image.",
            "And but then, the really amazing thing is that you can do this with a handle labeled example and they get one point 13% error.",
            "So.",
            "You know when I got into this business of Hymnists?",
            "In the old days.",
            "We would have died for a number like this on the full data set.",
            "I mean like not 100 examples, but 60,000.",
            "So now they're getting this number with only hand labeled examples for the user.",
            "60,000 unlabeled examples as well.",
            "OK, so this is something to follow up on I think."
        ],
        [
            "And to conclude.",
            "The log likelihood objective function for high dimensional.",
            "Vector is he trying to model with complicated solution is is intractable and so people have been proposing your kinds of approximations and you've heard at the beginning of the summer school about Monte Carlo, Markov chain methods, and both machines.",
            "But the good thing is not there's being a rich variety of different approaches that have been proposed that avoid this intractability, that actually are not just approximations to the log likelihood, but completely different approaches like score matching and the generative adversarial networks and ice and so on.",
            "So, and this is not a complete portrait, there more so that's very interesting as a space to explore because it's like we're discovering a new world basically.",
            "Another thing I hope you've learned from this is that although the autoencoders were initially introduced as some something, a bit hacky that we didn't have a probabilistic interpretation for, we now have fairly solid interpretation for what they do.",
            "They allow to find the directions in which you should move to go towards more probable configurations, and if you if you do this iteratively, then you move towards and you add noise.",
            "You basically get a Markov chain from which you can sample and you get you get fairly decent.",
            "Samples from that.",
            "I guess I initially had slides on the variational encoder, but Aaron covered that very well.",
            "I also told you about the generative adversarial networks, which again really follow a completely different approach.",
            "Which you can think as optimizing a kind of Turing test, which I think is a really interesting way to think about what generative models should be optimizing.",
            "And currently the basis for the best genitive models of images, but who knows, in two weeks from now, someone will come up with a better model.",
            "So thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you Roland.",
                    "label": 0
                },
                {
                    "sent": "So for this last lecture I'm going to tell you about something very dear to my heart which is.",
                    "label": 0
                },
                {
                    "sent": "The question of how do we train better?",
                    "label": 0
                },
                {
                    "sent": "Unsupervised models, probably generative models of the world out there.",
                    "label": 0
                },
                {
                    "sent": "And I think this is a crucial component.",
                    "label": 0
                },
                {
                    "sent": "Of AI, which is the reason that I'm in this business?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm so.",
                    "label": 0
                },
                {
                    "sent": "I have the.",
                    "label": 0
                },
                {
                    "sent": "The intuition that over the decades that I've been in research, one thing that happens over and over again is that researchers are humans, and very often they look more to the short term an like a bit of ambition, and very often that's what prevents us from actually reaching our most difficult goals.",
                    "label": 0
                },
                {
                    "sent": "So I think many of the issues we see with the current type of machine learning and deep learning.",
                    "label": 0
                },
                {
                    "sent": "Are due to our lack of ambition of trying to solve the short term problems and that we might be lucky an make big strides by actually going for the big prize of building machines that understand the world where we live.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you think about the adversarial examples issue that Ian Goodfellow raised.",
                    "label": 0
                },
                {
                    "sent": "The strategies that one can come up with may help like strategies like some kinds of adversarial negative examples or some kind of smoothing methods.",
                    "label": 0
                },
                {
                    "sent": "They might help a bit, but at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "What we see with this adversarial examples is not some new configurations.",
                    "label": 0
                },
                {
                    "sent": "Give, surprisingly, you know, make the model fail in this surprising ways.",
                    "label": 0
                },
                {
                    "sent": "That's basically we see, but there are more trivial ways in which the model fails simply because it just didn't really, really understand the content.",
                    "label": 0
                },
                {
                    "sent": "Really understand the image or really understand the text.",
                    "label": 0
                },
                {
                    "sent": "So when we we test our like caption generation models, it's very easy to fool it by putting like a background that doesn't correspond to the foreground image object.",
                    "label": 0
                },
                {
                    "sent": "If we look at images, there are easy ways to fool them and text the same thing.",
                    "label": 0
                },
                {
                    "sent": "We see that it uses a lot of statistical regularity's to try to solve the problem, but very often it misses the point of the actual kind of causal relationship that matter and that allow us to build a mental model that is predictive of the future.",
                    "label": 0
                },
                {
                    "sent": "That's what also science is trying to do right to build theories about the world.",
                    "label": 0
                },
                {
                    "sent": "That are causal, that we can use to predict what will happen and we can have a long chain of these predictions.",
                    "label": 1
                },
                {
                    "sent": "That's called reasoning.",
                    "label": 0
                },
                {
                    "sent": "But that only works if you have a fairly good model of the world.",
                    "label": 1
                },
                {
                    "sent": "OK, so this is I think we should aim for the traditional view of neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Is that neural Nets are good for pattern recognition, but I think we need to go much beyond that in order to approach AI.",
                    "label": 0
                },
                {
                    "sent": "And we've seen some steps in that direction.",
                    "label": 0
                },
                {
                    "sent": "For example, you've heard about these memory networks and things like that which are based on extending recurrent Nets and using attention mechanisms to try to move towards the idea of reasoning and so reasoning.",
                    "label": 0
                },
                {
                    "sent": "Planning all these things are really important for AI and they have been kind of neglected in the traditional view of neural Nets of the 80s and before.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the ingredients for getting there is to have a model that can really kind of simulate.",
                    "label": 0
                },
                {
                    "sent": "You know what would happen if.",
                    "label": 0
                },
                {
                    "sent": "An innocence that's what generative models are aiming to do.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we build these models whether they are generative or discriminative, that create different levels of representations.",
                    "label": 1
                },
                {
                    "sent": "And we hope that the higher levels of representation capture the underlying sort of factors.",
                    "label": 1
                },
                {
                    "sent": "The what would I like to call the abstractions behind what we see?",
                    "label": 0
                },
                {
                    "sent": "The idea is that if.",
                    "label": 0
                },
                {
                    "sent": "We are able to do that.",
                    "label": 0
                },
                {
                    "sent": "Basically we're able to explain better.",
                    "label": 0
                },
                {
                    "sent": "In a sense, that's more accurate rendering of what actually happens in the world.",
                    "label": 0
                },
                {
                    "sent": "What is going on, and this is really the key to AAI 2.",
                    "label": 0
                },
                {
                    "sent": "Two really good generalization it for any kind of task.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not enough to have that it's not enough to have depth.",
                    "label": 0
                },
                {
                    "sent": "It's also that these higher level.",
                    "label": 0
                },
                {
                    "sent": "Factors that we're discovering actually explain in a causal sense what's going on.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so I've used these word this word disentangle in several papers and it's one way to express this idea about what we're trying to discover with deep representations are the those factors that are kind of mixed up in the data we're seeing in the pixels and the characters.",
                    "label": 0
                },
                {
                    "sent": "But we'd like to separate them out to disentangle them so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's interesting to distinguish that notion of disentanglement from the notion of invariants that is closely related, but is different and has been a very important theme in areas like computer vision or speech recognition.",
                    "label": 0
                },
                {
                    "sent": "So when you're doing supervised learning, you want to predict in a typical supervised learning where you have a few outputs corresponding to categories you would like to find features that are as invariant as possible to the things you don't care.",
                    "label": 0
                },
                {
                    "sent": "Like translating your image or changing the lighting and to focus on the things that you care about.",
                    "label": 0
                },
                {
                    "sent": "That's that's the idea of invariants.",
                    "label": 0
                },
                {
                    "sent": "But if you're doing a supervised learning, and if you're trying to understand the world around you, you're basically doing unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "By the way we're trying to find the relationships between everything you're observing so that you could, you know.",
                    "label": 0
                },
                {
                    "sent": "Simulate it in your mind so that you can really understand what is going on.",
                    "label": 0
                },
                {
                    "sent": "So if you're doing unsupervised learning then the notion of invariants isn't sufficient.",
                    "label": 0
                },
                {
                    "sent": "You need we need to extend it.",
                    "label": 0
                },
                {
                    "sent": "So invariant says get rid of the things we don't care, but really, if you're going to explain everything, what you want to do is to separate out those factors, right?",
                    "label": 0
                },
                {
                    "sent": "So some of them might be the ones that relate to your classification problems, but the other ones, like translation, gives you position.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're doing object classification, you don't care bout position, but maybe you want to do object detection and then you do care.",
                    "label": 0
                },
                {
                    "sent": "Bout position seems true in speech recognition, right?",
                    "label": 0
                },
                {
                    "sent": "If you want to do speech recognition, you don't care about the.",
                    "label": 0
                },
                {
                    "sent": "The speaker identity.",
                    "label": 0
                },
                {
                    "sent": "But maybe there is another task which is speaker identification where you do care.",
                    "label": 0
                },
                {
                    "sent": "So what you'd like to have is a system that listens to a lot of speech.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne learns that there are these factors, one of which is the semantic content of what the person is trying to say, and the other is who that person is.",
                    "label": 0
                },
                {
                    "sent": "What kind of room this is in, and what kind of noise there is, and so on.",
                    "label": 0
                },
                {
                    "sent": "And we we can do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this entangling is a bit different from from invariants, but it's real.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I did.",
                    "label": 0
                },
                {
                    "sent": "And we and others have been thinking that this is something we would really like to have in our models.",
                    "label": 0
                },
                {
                    "sent": "But really, we don't know how to build models that would have these properties.",
                    "label": 0
                },
                {
                    "sent": "We would like that and short of actually building models that do that, we try to measure if some representations that was learned by our unsupervised learning approaches.",
                    "label": 0
                },
                {
                    "sent": "Would actually disentangle the underlying factors and you can do that if you kind of.",
                    "label": 0
                },
                {
                    "sent": "If you know some of those factors.",
                    "label": 0
                },
                {
                    "sent": "Very often we do.",
                    "label": 0
                },
                {
                    "sent": "So in the case of the 1st paper here by Ian, when he was at Stanford.",
                    "label": 0
                },
                {
                    "sent": "You do know something because this is.",
                    "label": 0
                },
                {
                    "sent": "These are videos and you know the camera movement and you know what the objects are so you can check whether the features that are being detected.",
                    "label": 0
                },
                {
                    "sent": "Some of them would be very sensitive to some things, but very insensitive to other factors.",
                    "label": 0
                },
                {
                    "sent": "So you can measure that and we did something similar a couple of years later on in the domain of texts where we discovered that our denoising auto encoders are stacks of noise.",
                    "label": 0
                },
                {
                    "sent": "Quarters where discovering features that were that tended to specialize to some of the underlying factors.",
                    "label": 0
                },
                {
                    "sent": "We knew like these were.",
                    "label": 0
                },
                {
                    "sent": "These were comments and Amazon on books and music and things like that and so.",
                    "label": 0
                },
                {
                    "sent": "So we knew the underlying sentiment because this was actually the task we cared about.",
                    "label": 0
                },
                {
                    "sent": "But we also knew where the text was.",
                    "label": 0
                },
                {
                    "sent": "The text about.",
                    "label": 0
                },
                {
                    "sent": "Is it a comment about a book or is it a comment about video?",
                    "label": 0
                },
                {
                    "sent": "And so it turned out that as you go, as you went on higher up, you saw those features that tended to specialize in the underlying factor.",
                    "label": 0
                },
                {
                    "sent": "So this is, it seems that some disentangling is happening, but we don't fully understand why and how.",
                    "label": 0
                },
                {
                    "sent": "One coincidence maybe is that these early models were using sparse representation, so there was a sparsity penalty like an L1 penalty.",
                    "label": 0
                },
                {
                    "sent": "And today nobody talks about sparsity anymore.",
                    "label": 0
                },
                {
                    "sent": "It's out of fashion, but maybe we should think more about it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think I showed a variant of this slide in my first lecture which talks about the relationship with causality and unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So there is something funny going on when you're trying to model.",
                    "label": 0
                },
                {
                    "sent": "Say pairs X&Y depending on whether X is a cause of Y or Y, is the cause of X.",
                    "label": 0
                },
                {
                    "sent": "The way that you're going to model those relationships that the conditional distributions is going to is going to behave differently.",
                    "label": 0
                },
                {
                    "sent": "You're going to get different results in particular.",
                    "label": 0
                },
                {
                    "sent": "If Y is one of the causes of X.",
                    "label": 0
                },
                {
                    "sent": "Which is, I think, what happens for most of the problems we care about, so we want to predict the cause given the effects we see an image we want to know.",
                    "label": 0
                },
                {
                    "sent": "You know what are the objects in it.",
                    "label": 0
                },
                {
                    "sent": "We hear sound.",
                    "label": 0
                },
                {
                    "sent": "We want to know what was the intention.",
                    "label": 0
                },
                {
                    "sent": "That's the cause, right of the person speaking.",
                    "label": 0
                },
                {
                    "sent": "So for very, very often we're trying to do this.",
                    "label": 0
                },
                {
                    "sent": "We're trying to recover some of the causes, given the effects, and if that's the case, then.",
                    "label": 0
                },
                {
                    "sent": "P of Y given X which is at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "What we want to do.",
                    "label": 0
                },
                {
                    "sent": "We want to predict Y given X. Um can be rewritten in this, you know, usual Bayes rule form.",
                    "label": 0
                },
                {
                    "sent": "And where P of X is just a normalization of the numerator with P of X given YNP of Y, so of course we can always choose to write, you know our joint in any way we want, But when we write it.",
                    "label": 0
                },
                {
                    "sent": "Like this, if X is really A cause of Y.",
                    "label": 0
                },
                {
                    "sent": "Then I'm sorry if X is really an effect.",
                    "label": 0
                },
                {
                    "sent": "And why is the cause then P of X incorporates information about why, right?",
                    "label": 0
                },
                {
                    "sent": "So because P of X is a sum over the wise of of this numerator.",
                    "label": 0
                },
                {
                    "sent": "In other words.",
                    "label": 0
                },
                {
                    "sent": "Of course you can always write this join to any way you want, but if you write it this way, it's the.",
                    "label": 0
                },
                {
                    "sent": "It's the natural way that the data was generated an you would expect that you can build a more compact model.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, if you only observe X or you have many examples that are unlabeled, you only observe X, then that will contain information implicitly about why, because the the form of P of X, the best form of P of X is one that includes Y in it.",
                    "label": 1
                },
                {
                    "sent": "So think about the clustering problem.",
                    "label": 0
                },
                {
                    "sent": "I think I showed that example.",
                    "label": 0
                },
                {
                    "sent": "This is like a very simple minded view of what really is happening.",
                    "label": 0
                },
                {
                    "sent": "Of course you have say two clusters.",
                    "label": 0
                },
                {
                    "sent": "Two Gaussians are well separated.",
                    "label": 0
                },
                {
                    "sent": "Cluster one corresponds to y = 0.",
                    "label": 0
                },
                {
                    "sent": "Cluster two to correspond to y = 1.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You know a really good model is one that implicitly represents this.",
                    "label": 0
                },
                {
                    "sent": "This P of X as.",
                    "label": 0
                },
                {
                    "sent": "The mixture between these two cases, so there is like an implicit.",
                    "label": 0
                },
                {
                    "sent": "Discrete variable that tells you in which cluster you are.",
                    "label": 0
                },
                {
                    "sent": "An if you think about object recognition and unsupervised learning for object recognition is very much like this.",
                    "label": 0
                },
                {
                    "sent": "There are natural categories of objects like cats and dogs and tables and people.",
                    "label": 0
                },
                {
                    "sent": "And even if we don't get the labels.",
                    "label": 0
                },
                {
                    "sent": "We, because of the statistical structure, we get a sense of what these categories are.",
                    "label": 0
                },
                {
                    "sent": "This is the way that a child was never seen a picture of some African animals.",
                    "label": 0
                },
                {
                    "sent": "They will be able to recognize from a single label case the category in new cases, right?",
                    "label": 0
                },
                {
                    "sent": "So you show pictures of zebra, the child.",
                    "label": 0
                },
                {
                    "sent": "I've never seen a zebra, but China is seen.",
                    "label": 0
                },
                {
                    "sent": "Other animals, a child as built.",
                    "label": 0
                },
                {
                    "sent": "An underlying model of how objects and animals look like and you just need one label to get it right after that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this motivates.",
                    "label": 0
                },
                {
                    "sent": "Model that go in the direction from Y to X even though at the end of the day we want to go from X to Y very often.",
                    "label": 0
                },
                {
                    "sent": "So that means if we don't observe the wise all the time, especially that.",
                    "label": 0
                },
                {
                    "sent": "What we would need to do is to discover.",
                    "label": 0
                },
                {
                    "sent": "Some latent variables which may be related to Y, hopefully that which we're going to call H. In general for hidden units.",
                    "label": 0
                },
                {
                    "sent": "That explain X. ANAN hopefully are related to why and if we do semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Then we can make that connection you know clear and strong.",
                    "label": 0
                },
                {
                    "sent": "And we saw a bit of dot in the semi supervised model that Aaron presented this morning with the denoising autoencoder.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as soon as you the thing however, is that as soon as you introduce latent variables in your models and you try to go predicting X.",
                    "label": 0
                },
                {
                    "sent": "Given Y or X given H. Rather than predicting why granex things get harder for some reason.",
                    "label": 0
                },
                {
                    "sent": "Latent variables are in a sense.",
                    "label": 1
                },
                {
                    "sent": "Really, really important too.",
                    "label": 0
                },
                {
                    "sent": "Deal with the machine learning problem, so I wrote here.",
                    "label": 0
                },
                {
                    "sent": "It helps to avoid the curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "This required would require a bit more discussion, but I remember in my first lecture I talked about how in this representations.",
                    "label": 0
                },
                {
                    "sent": "And you heard later about our BMS, how you can.",
                    "label": 0
                },
                {
                    "sent": "You can represent a very complicated distribution that has an exponential number of components using a fairly compact representation.",
                    "label": 0
                },
                {
                    "sent": "Now, so unfortunately this these advantages come with intractability, so exact inference, in other words, predicting age given X.",
                    "label": 0
                },
                {
                    "sent": "If you have a model that goes from.",
                    "label": 0
                },
                {
                    "sent": "From H2X and trying to go backwards in general is intractable and you need that when you want to use the model and it turns out you also need that in many learning approaches like the variational autoencoder or the Helmholtz machine, which is the ancestor of that thing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So that's for directed models in the next few slides, I'm going to tell you about undirected models, which for which this is easier.",
                    "label": 0
                },
                {
                    "sent": "The inference is easier, but you have another problem, which is this normalizing constant you've heard about with some of the lectures we had earlier.",
                    "label": 0
                },
                {
                    "sent": "So that this normalizing constants is intractable, and unfortunately, because the normalizing constant also depends on the parameters and you want to compute the derivative with respect to parameters, the gradient of the long black dude is going to be intractable, so a lot of what I'll be discussing today and what you've already heard about a bit with Aaron's lecture.",
                    "label": 0
                },
                {
                    "sent": "Is an exploration of alternatives to the the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead of standard log likehood maximization approach.",
                    "label": 0
                },
                {
                    "sent": "That's common in statistics and and the most common in machine learning as well.",
                    "label": 0
                },
                {
                    "sent": "So to see a bit more where this normalization constant hurts us.",
                    "label": 0
                },
                {
                    "sent": "So think about.",
                    "label": 0
                },
                {
                    "sent": "A model like this where the marginal distribution on X, so we have X&H.",
                    "label": 0
                },
                {
                    "sent": "We have the joint distribution, but we marginalized it, meaning that we look at the aggregating over all the values of H. What do we get?",
                    "label": 0
                },
                {
                    "sent": "We get P of X.",
                    "label": 0
                },
                {
                    "sent": "And so there's this intractable sum here.",
                    "label": 0
                },
                {
                    "sent": "And there's another intractable some in here, which is some over all the X and age or integral.",
                    "label": 0
                },
                {
                    "sent": "In case there are continuous.",
                    "label": 1
                },
                {
                    "sent": "And these these.",
                    "label": 0
                },
                {
                    "sent": "Both of these sums that show up in the likelihood.",
                    "label": 0
                },
                {
                    "sent": "They also show up in the gradient, obviously.",
                    "label": 0
                },
                {
                    "sent": "But it's interesting to see a little bit more detail when we look at the gradient people.",
                    "label": 0
                },
                {
                    "sent": "I guess this comes maybe to Geoff Hinton in the 80s have broken the gradient into two components, and I think Hung Black talked about this.",
                    "label": 0
                },
                {
                    "sent": "There is what's called a positive phase component of the gradient and the negative phase component of gradient.",
                    "label": 1
                },
                {
                    "sent": "And the positive phase basically means how do I change the parameters so as to make the energy of the of X energy is just minus log probability are normalized.",
                    "label": 0
                },
                {
                    "sent": "How to make that energy lower and the negative says phase says?",
                    "label": 0
                },
                {
                    "sent": "How do we make the energy of everyone else higher in particular?",
                    "label": 0
                },
                {
                    "sent": "Putting more emphasis on those at the model.",
                    "label": 0
                },
                {
                    "sent": "Currently things are good, so this is kind of funny and it takes awhile to understand that really, the long lanky gradient tells you you want to make the training examples more probably more.",
                    "label": 0
                },
                {
                    "sent": "I have a better score, but you want to make the score of the configurations that the model likes to be worse.",
                    "label": 0
                },
                {
                    "sent": "And you know it may not be obvious why you do that, but the algebra is very straightforward and you can generalize that to the case where you have a joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Now, in addition to this, you have a some overage coming everywhere.",
                    "label": 0
                },
                {
                    "sent": "An in general, it turns out well for different kinds of problems that the positive phase is often easy to compute, or you're going to have reasonable approximation for that sum.",
                    "label": 0
                },
                {
                    "sent": "But but that part is usually very hard because it requires sampling.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can't do this sum exactly for sure typically, but we could use Monte Carlo machines.",
                    "label": 0
                },
                {
                    "sent": "We need to find an X~ here sample from P or an, X~ H~ from the joint, and that.",
                    "label": 0
                },
                {
                    "sent": "It is typically hard, unfortunately, for many models.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a thing that I've been trying to figure out.",
                    "label": 0
                },
                {
                    "sent": "Over the years I spent a lot of years trying to do.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning with both machines and their variants.",
                    "label": 0
                },
                {
                    "sent": "In fact, I started working on both machines in my Masters in 86.",
                    "label": 0
                },
                {
                    "sent": "But I still think that there are great models, but maybe maximum likelihood is not the right way to train them.",
                    "label": 1
                },
                {
                    "sent": "So what happens with maximum like you are the approximations that we typically use is we need to sample these what we call negative examples.",
                    "label": 0
                },
                {
                    "sent": "You know, in in in this part.",
                    "label": 0
                },
                {
                    "sent": "Here we need to sample these X tildes or X tildes.",
                    "label": 0
                },
                {
                    "sent": "I still does.",
                    "label": 0
                },
                {
                    "sent": "And as I said, sampling exactly is typically impossible, but.",
                    "label": 0
                },
                {
                    "sent": "You can always use Monte Carlo Markov chain methods, and I guess you didn't get a course in multicolor Markov chain methods.",
                    "label": 0
                },
                {
                    "sent": "But one thing to know about them is that we're going to.",
                    "label": 0
                },
                {
                    "sent": "In order to sample from the thing we care about, we're going to produce a sequence of samples, and that's called a Markov chain because you only use the previous sample tell you how to get the next one.",
                    "label": 0
                },
                {
                    "sent": "At the market part, and as you do these you sample one thing after the other.",
                    "label": 0
                },
                {
                    "sent": "Eventually the distribution of the samples you get are what you want.",
                    "label": 0
                },
                {
                    "sent": "However, it might take infinite time to get the right distribution.",
                    "label": 0
                },
                {
                    "sent": "It doesn't, but.",
                    "label": 0
                },
                {
                    "sent": "Turns out that things get harder.",
                    "label": 0
                },
                {
                    "sent": "In other words, it becomes more difficult to get good samples when the distribution you're trying to learn from his sharper.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by sharper is that it has these regions of very high probability and an other regions are very low probability.",
                    "label": 0
                },
                {
                    "sent": "So the if you think of the density function is a function, it's a function that's not smooth at all.",
                    "label": 0
                },
                {
                    "sent": "Remember, we talked about manifolds alot.",
                    "label": 0
                },
                {
                    "sent": "Pascal talked about manifolds in particular, so if the data is concentrating near these manifolds, it means the density is very sharp.",
                    "label": 0
                },
                {
                    "sent": "Is nonsmooth right?",
                    "label": 0
                },
                {
                    "sent": "Because you have this abrupt transition from something likely to something unlikely?",
                    "label": 0
                },
                {
                    "sent": "So if you've taken an English sentence and you you switch some?",
                    "label": 0
                },
                {
                    "sent": "Words randomly.",
                    "label": 0
                },
                {
                    "sent": "I mean, you just switch one word and it may become nonsensical.",
                    "label": 0
                },
                {
                    "sent": "So you go out of the manifold of natural language sentences you use which you know half of the words and it's total garbage.",
                    "label": 0
                },
                {
                    "sent": "And for pixels, well, you have to flip more than than half of the pixels something before you start seeing that doesn't make any sense, but, but in fact.",
                    "label": 0
                },
                {
                    "sent": "Most configurations of pixels are unlike natural images, which means that the true.",
                    "label": 0
                },
                {
                    "sent": "Distribution function is very, very concentrated.",
                    "label": 0
                },
                {
                    "sent": "Very very sharp, is much sharper than in this figure.",
                    "label": 0
                },
                {
                    "sent": "And that's a problem, because these MCMC these chains they you know they make small steps.",
                    "label": 0
                },
                {
                    "sent": "They have to make small steps, otherwise the whole thing just can't work.",
                    "label": 0
                },
                {
                    "sent": "The volume of space is too large and an and and the other constraint is that at each point in the chain the Markov chains are trying to sample more and more probable things.",
                    "label": 0
                },
                {
                    "sent": "So they go from a configuration to one that's slightly more probable, and that's how they climb towards good configurations.",
                    "label": 0
                },
                {
                    "sent": "And then they stay.",
                    "label": 0
                },
                {
                    "sent": "Among good configurations, but you can see that if you have many modes that are disconnected is going to be very difficult with this kind of process to visit all the modes.",
                    "label": 0
                },
                {
                    "sent": "I mean you're going to find one of the modes, and then you're going to stuck there because you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't go through these unlikely examples by making small steps to reach this mode from this mode.",
                    "label": 0
                },
                {
                    "sent": "So the result is the negative phase of the gradient in these undirected graphical models is going to.",
                    "label": 1
                },
                {
                    "sent": "Is going to become bad is going to be a poor reflection of the true gradient, and maybe that's one reason why we are having trouble.",
                    "label": 0
                },
                {
                    "sent": "Sort of scaling up these models beyond hymnists.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One glimmer of Hope was given by this paper a couple of years ago.",
                    "label": 0
                },
                {
                    "sent": "Where we found that if you train RBM's.",
                    "label": 0
                },
                {
                    "sent": "Or denoising warters.",
                    "label": 0
                },
                {
                    "sent": "By stacking one on top of the other and you look at the Markov chains that are generating so you can have one PBM or one denoising order.",
                    "label": 0
                },
                {
                    "sent": "I'll show you later, or how you can sample from a denoising order or contract with encoder.",
                    "label": 0
                },
                {
                    "sent": "But in any case, you get these Markov chains in both cases.",
                    "label": 0
                },
                {
                    "sent": "And so if you if you look at the GBM at the low level.",
                    "label": 0
                },
                {
                    "sent": "It really has as training progresses it becomes hard for it too.",
                    "label": 0
                },
                {
                    "sent": "To move from mode to mode.",
                    "label": 0
                },
                {
                    "sent": "So if you think about.",
                    "label": 0
                },
                {
                    "sent": "Faces it will lock on some type of face and then it will move very slowly and stay.",
                    "label": 0
                },
                {
                    "sent": "You know in that region of faces.",
                    "label": 0
                },
                {
                    "sent": "However, if you stack another RBM on top.",
                    "label": 0
                },
                {
                    "sent": "When you run the Markov chain there, you see that now it mixes between different types of faces or different digits.",
                    "label": 0
                },
                {
                    "sent": "In the case of M nests much more easily.",
                    "label": 0
                },
                {
                    "sent": "So that was interesting, right?",
                    "label": 0
                },
                {
                    "sent": "So it looks like.",
                    "label": 0
                },
                {
                    "sent": "The representations were getting higher up.",
                    "label": 0
                },
                {
                    "sent": "Are more amenable to mixing between modes than the low level representation or the raw data.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "Well, so I have a. I have a theory about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This and we've tried to kind of validate it in that paper.",
                    "label": 0
                },
                {
                    "sent": "Basically, the theory is that.",
                    "label": 0
                },
                {
                    "sent": "Even though in the input space the manifolds are very thin and concentrated in some places.",
                    "label": 0
                },
                {
                    "sent": "And could be far from each other.",
                    "label": 0
                },
                {
                    "sent": "In the space of learned representations with our BMS and autoencoders and stuff like that, things are different.",
                    "label": 0
                },
                {
                    "sent": "The regions of high probability occupy a larger fraction of the total volume.",
                    "label": 0
                },
                {
                    "sent": "And the different modes corresponding or different manifolds maybe corresponding to different categories are somehow closer to each other.",
                    "label": 0
                },
                {
                    "sent": "To get a sense of what's so, to try to validate this mental picture, we run some experiments that I'm going to try to explain to explain here.",
                    "label": 0
                },
                {
                    "sent": "So this image of a 9 here in this image of a three are coming from the training set of M. Nists.",
                    "label": 0
                },
                {
                    "sent": "And imagine you run a linear interpolation between those points in the pixel space.",
                    "label": 1
                },
                {
                    "sent": "So what is a linear interpolation is just adding up two vectors with a convex combination here in between.",
                    "label": 0
                },
                {
                    "sent": "So it's just adding up the image of a nine with the image of a three and what you get, you get something that's doesn't look like an ominous digit.",
                    "label": 0
                },
                {
                    "sent": "So it means that.",
                    "label": 0
                },
                {
                    "sent": "If you take the manifold of digits, it's curvy, right?",
                    "label": 0
                },
                {
                    "sent": "If it was flat.",
                    "label": 0
                },
                {
                    "sent": "Then when I do a linear interpolation, the things inbetween should also be likely.",
                    "label": 0
                },
                {
                    "sent": "Like like here, right?",
                    "label": 0
                },
                {
                    "sent": "So if you have a flat space and I take 2 points and interpolate the means that the things in between are also on the manifold.",
                    "label": 0
                },
                {
                    "sent": "And so I should see plausable things.",
                    "label": 0
                },
                {
                    "sent": "But it's not what we see.",
                    "label": 0
                },
                {
                    "sent": "We see that the things in between are implausible.",
                    "label": 0
                },
                {
                    "sent": "That means the manifolds are really curved.",
                    "label": 1
                },
                {
                    "sent": "Now let's see what happens.",
                    "label": 0
                },
                {
                    "sent": "So this explains the first row here.",
                    "label": 0
                },
                {
                    "sent": "Now let's do the linear interpolation in the space of the first level representation.",
                    "label": 0
                },
                {
                    "sent": "In the second level representation, and so on.",
                    "label": 0
                },
                {
                    "sent": "That was learned by these autoencoders.",
                    "label": 0
                },
                {
                    "sent": "And we see these curves.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the second level interpolation, what you see is that.",
                    "label": 0
                },
                {
                    "sent": "Now almost every image on the route from here to here.",
                    "label": 0
                },
                {
                    "sent": "Looks like something that could come from the training set.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we get these images?",
                    "label": 0
                },
                {
                    "sent": "By the way?",
                    "label": 1
                },
                {
                    "sent": "Of course now this is in the space of hidden representations and I do a linear interpolation between those vectors, But then of course I have a decoder so I can map back to the input space.",
                    "label": 0
                },
                {
                    "sent": "And so I can visualize those what those representations correspond to in the pixel space, and that's this is how we get these pictures.",
                    "label": 0
                },
                {
                    "sent": "So the mental picture I have based on this is that the.",
                    "label": 0
                },
                {
                    "sent": "The representations we're getting correspond to kind of coordinate system that follows the shape of the manifold and and the decoder does this kind of nonlinear transformation that that that goes from this space with this base.",
                    "label": 0
                },
                {
                    "sent": "Sorry from this space, this space and the encoder goes in the other direction, so the encoder and decoder basically folding or and folding the manifold.",
                    "label": 0
                },
                {
                    "sent": "Depending on.",
                    "label": 0
                },
                {
                    "sent": "At which you look.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you've heard about autoencoders.",
                    "label": 0
                },
                {
                    "sent": "Andy noisy one quarters.",
                    "label": 0
                },
                {
                    "sent": "I'll remind you that the way we train the denoising order is that we just.",
                    "label": 0
                },
                {
                    "sent": "Add extra noise in input and then we try to reconstruct the clean input and there is some intermediate code in between and.",
                    "label": 1
                },
                {
                    "sent": "Instead of thinking about reconstruction error as like mean squared error, you can think of it as maximizing the log probability of the clean input given the code and the code you know would have been sampled from some encoder distribution, which could be deterministic.",
                    "label": 1
                },
                {
                    "sent": "Or in the case of the variational autoencoder you've seen, it could be actually injecting noise as well here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One idea to try to go away from the traditional undirected graphical models.",
                    "label": 0
                },
                {
                    "sent": "Is to think of the model of the problem of learning a generative model more directly so usually.",
                    "label": 0
                },
                {
                    "sent": "In in classical machine learning and statistics, we think of learning the density or the probability function P of X and we have a a parametric form for P of X.",
                    "label": 0
                },
                {
                    "sent": "But if at the end of the day we want to do is not be valid P of X, but to generate samples, maybe another way is to have some kind of black box, maybe neural net that takes in parameters random numbers and output samples.",
                    "label": 0
                },
                {
                    "sent": "Right, that would be a true generative model.",
                    "label": 0
                },
                {
                    "sent": "'cause if you think about how we get samples in a Boston machine or these Android graphical models, it's actually a very complicated process that we said was failing.",
                    "label": 0
                },
                {
                    "sent": "So maybe we could just train a machine that generates our samples directly.",
                    "label": 0
                },
                {
                    "sent": "One variant of that is to have a machine that you know actually constructs a Markov chain.",
                    "label": 1
                },
                {
                    "sent": "And that's what the generative stochastic networks and also the denoising interpretation of denoising orders I'll give you, fall into that family, so later I'll tell you about the generative adversarial networks which are like this.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's a it's a neural net here regular.",
                    "label": 0
                },
                {
                    "sent": "Feedforward network it has inputs parameters.",
                    "label": 0
                },
                {
                    "sent": "Well I mean usual parameters an but it has random numbers like Gaussian or uniform and it outputs images or whatever you want.",
                    "label": 0
                },
                {
                    "sent": "And also tell you about these kinds of guys.",
                    "label": 0
                },
                {
                    "sent": "I think that's where I'm going to start with, like the denoising encoder actually falls in this category where we can interpret what the denoising auto encoder is doing and it's extensions that generative stochastic networks.",
                    "label": 1
                },
                {
                    "sent": "As learning one you know one step of a Markov chain an what's different from this guy is that now we have introduced this notion of state.",
                    "label": 0
                },
                {
                    "sent": "So the box, in addition to outputting samples, outputs a next state and it has of course the previous state's extra input.",
                    "label": 0
                },
                {
                    "sent": "And now you can change that and get a kind of recurrent network that has noise injected at every step and also produces samples at every step.",
                    "label": 0
                },
                {
                    "sent": "So I think our brain is such a machine now.",
                    "label": 0
                },
                {
                    "sent": "What's the principle by which we train it?",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Made one of the alternatives to.",
                    "label": 0
                },
                {
                    "sent": "Maximizing the like you, that's really interesting is called score matching.",
                    "label": 1
                },
                {
                    "sent": "Actually, I should say that one of the things that really.",
                    "label": 0
                },
                {
                    "sent": "Stimulating in this area of research of unsupervised generative models and deep learning is that there's an incredible variety of approaches that people have looked at for training them with different training principles, and we don't have to stick to the good old maximum likelihood framework.",
                    "label": 0
                },
                {
                    "sent": "So here's one which can be modified in various ways, called score matching, and the initial idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "Instead of making using the KL divergent between the estimated density and and the data.",
                    "label": 0
                },
                {
                    "sent": "Density, which we approximate with the empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "We're going to match, not the density, but it's it's derivative or its derivative of its log.",
                    "label": 1
                },
                {
                    "sent": "OK, so we have our model P. We know how to compute its derivative with respect to the input.",
                    "label": 0
                },
                {
                    "sent": "And let's say somebody gave us not samples but the derivative of the true data generating density with respect to its input.",
                    "label": 0
                },
                {
                    "sent": "Then we could just do squared error, minimize that and why is that interesting?",
                    "label": 0
                },
                {
                    "sent": "Because when we take the derivative with respect to the input.",
                    "label": 0
                },
                {
                    "sent": "The normalizing constant goes away 'cause the normalizing constant remember.",
                    "label": 0
                },
                {
                    "sent": "Is the integral over X of some expression, so there's no X in it.",
                    "label": 0
                },
                {
                    "sent": "It has the parameters, but it has no X.",
                    "label": 0
                },
                {
                    "sent": "So when we take the derivative with respect to X here.",
                    "label": 0
                },
                {
                    "sent": "The normalizing constant goes away, so now if if P of X is something that has a reasonably simple form up to this.",
                    "label": 0
                },
                {
                    "sent": "Intractable, normalizing constant, then we're in business, right?",
                    "label": 0
                },
                {
                    "sent": "We've eliminated the problem, so that was the objective of Aaron when he proposed this 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how do we get this guy?",
                    "label": 0
                },
                {
                    "sent": "We don't know the truth.",
                    "label": 0
                },
                {
                    "sent": "Model score.",
                    "label": 0
                },
                {
                    "sent": "Is that true?",
                    "label": 0
                },
                {
                    "sent": "Data score.",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that you can rearrange the do a bit of algebra and get this expression.",
                    "label": 0
                },
                {
                    "sent": "Where you see these two terms, so the first one is not very surprising.",
                    "label": 0
                },
                {
                    "sent": "It says that.",
                    "label": 0
                },
                {
                    "sent": "At the data point.",
                    "label": 0
                },
                {
                    "sent": "So the X here now is coming from the data distribution.",
                    "label": 0
                },
                {
                    "sent": "So at the data points, you'd like the density, so that's what I'm showing here that you'd like at the data points, you want the density to have a critical point.",
                    "label": 0
                },
                {
                    "sent": "You want the derivative to be 0, right?",
                    "label": 1
                },
                {
                    "sent": "So because these are the places where the probability should be high.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you think about what maximum likelihood is trying to do is trying to put a high probability on the examples.",
                    "label": 0
                },
                {
                    "sent": "That means ideally it wants to have a mode on each example.",
                    "label": 0
                },
                {
                    "sent": "So this is doing the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's saying OK, so let's put a critical point at each example and let's make sure that.",
                    "label": 0
                },
                {
                    "sent": "These are second derivatives.",
                    "label": 0
                },
                {
                    "sent": "Let's make sure that it's curving up in every direction.",
                    "label": 0
                },
                {
                    "sent": "So this is not the full Hessian.",
                    "label": 0
                },
                {
                    "sent": "This is something that's actually easy to compute, well, not too expensive to compute.",
                    "label": 0
                },
                {
                    "sent": "Because it's.",
                    "label": 0
                },
                {
                    "sent": "Per per dimension is coming over to dimensions of the input vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so this has been used and there's a literature on this.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that it's connected to.",
                    "label": 0
                },
                {
                    "sent": "That",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Annoying 1/4 and I'm going to try to explain that this connection was made by Pascal Vincent, whom you heard during this summer school.",
                    "label": 0
                },
                {
                    "sent": "In a paper in 2011.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's the first sort of the manifold interpretation of what the nosing around quarter does, and then we'll make the connection with score matching.",
                    "label": 1
                },
                {
                    "sent": "So imagine your data are concentrated on a manifold or near manifold and not exactly exactly on it, right?",
                    "label": 0
                },
                {
                    "sent": "And So what we do within the denoising order framework as we take a training example an we add noise.",
                    "label": 0
                },
                {
                    "sent": "So in this sort of simplistic view, let's add Gaussian noise so it can go anywhere in this ball.",
                    "label": 0
                },
                {
                    "sent": "And in a high dimensional space, the manifold is a low dimensional object, so this this is almost always going to be outside of the manifold.",
                    "label": 0
                },
                {
                    "sent": "So it means that when we add noise we produce a configuration that's less likely according to the true data during distribution.",
                    "label": 0
                },
                {
                    "sent": "Then the original.",
                    "label": 0
                },
                {
                    "sent": "And then what are we going to learn?",
                    "label": 0
                },
                {
                    "sent": "So this is how we prepare our data.",
                    "label": 0
                },
                {
                    "sent": "When you have the input.",
                    "label": 0
                },
                {
                    "sent": "I mean we have the input and the target and we're going to learn the backwards arrow that goes from the corrupted input.",
                    "label": 1
                },
                {
                    "sent": "To our target, which is the clean input.",
                    "label": 0
                },
                {
                    "sent": "And so we learned this vector field, which everywhere in space is pointing towards the clean stuff.",
                    "label": 1
                },
                {
                    "sent": "Which is where the manifold concentrates.",
                    "label": 0
                },
                {
                    "sent": "And I can show you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Immediately a picture.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you get?",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This was done by Guillaume alone was sitting somewhere here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We have this 2 dimensional data with a 1 dimensional manifold and he trained a denoising order and then when we see these little arrows are.",
                    "label": 0
                },
                {
                    "sent": "What the autoencoder is learning?",
                    "label": 0
                },
                {
                    "sent": "It's the difference between the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "And the input.",
                    "label": 0
                },
                {
                    "sent": "So if we start at that point and we do encode decode, we get here.",
                    "label": 0
                },
                {
                    "sent": "So that's what this vector field tells us.",
                    "label": 1
                },
                {
                    "sent": "So when you look at this, you should get the feeling that, well, we've basically learned what the density was, right?",
                    "label": 0
                },
                {
                    "sent": "We know what to do to go to places of high probability which is just follow the arrows.",
                    "label": 0
                },
                {
                    "sent": "Once you get there, the arrows are, you know the norm.",
                    "label": 0
                },
                {
                    "sent": "You can see like little dots here is because the arrows are becoming very very small, which is normal, like the derivative should be 0.",
                    "label": 0
                },
                {
                    "sent": "On the manifold.",
                    "label": 1
                },
                {
                    "sent": "Just as the score matching was asking us to do.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let's see.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In a later paper bag, Yum and I showed this identity slightly more general context where.",
                    "label": 0
                },
                {
                    "sent": "We find that as.",
                    "label": 0
                },
                {
                    "sent": "As the amount of noise becomes small, an as the amount of training data goes to Infinity and the capacity goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So like the nonparametric setting, so in the best of all possible words, which is what the typical.",
                    "label": 0
                },
                {
                    "sent": "Statistical analysis is about.",
                    "label": 0
                },
                {
                    "sent": "We find that the difference between the reconstruction and the input, which are you know these vectors I showed before.",
                    "label": 0
                },
                {
                    "sent": "Should converge to this.",
                    "label": 0
                },
                {
                    "sent": "Delock PDX, Ann.",
                    "label": 0
                },
                {
                    "sent": "You should recognize this is the score of the well I wear P here.",
                    "label": 0
                },
                {
                    "sent": "I should have.",
                    "label": 0
                },
                {
                    "sent": "I should have said Q.",
                    "label": 0
                },
                {
                    "sent": "This should be Q, so this should be that the score of the true data generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you have enough data and enough capacity and you let Sigma go to zero as the amount of data goes to Infinity, then you recovered the true score.",
                    "label": 0
                },
                {
                    "sent": "So this vector field is really pointing.",
                    "label": 0
                },
                {
                    "sent": "In what in the direction where the probability increases the most, which is what?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we see in this picture, right?",
                    "label": 0
                },
                {
                    "sent": "Do you have questions about this?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What do you think about this guy?",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so what's the magnitude of derivative here, you think?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is this.",
                    "label": 0
                },
                {
                    "sent": "These are Maxima of the probability, but this should be a minimum of the probability.",
                    "label": 0
                },
                {
                    "sent": "Anne, remember the vector were gay?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing is proportional to the derivative of the log probability, so it's going to be 0 either at Maxima or minima of the probability.",
                    "label": 0
                },
                {
                    "sent": "Which is interesting because take the square difference here.",
                    "label": 0
                },
                {
                    "sent": "Take the norm of this on both sides.",
                    "label": 0
                },
                {
                    "sent": "And this is just reconstruction error, right?",
                    "label": 0
                },
                {
                    "sent": "So it means that reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "Is not exactly the same as energy.",
                    "label": 0
                },
                {
                    "sent": "In other words, reconstruction error could be low, either because you have you're in a place near the manifold where the derivative should be small, but it could also be low in.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These funny places where you have a maximum of the probability.",
                    "label": 0
                },
                {
                    "sent": "Anyways.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's just parentesis.",
                    "label": 0
                },
                {
                    "sent": "Let's go back to the connection with score matching.",
                    "label": 0
                },
                {
                    "sent": "So remember, we're trying to.",
                    "label": 0
                },
                {
                    "sent": "Take the difference between the model score, what the model thinks of you, know which direction to go to, increase probability, and the what the data tells us.",
                    "label": 0
                },
                {
                    "sent": "So one way to get it, what the data tells us, is to take the difference between the clean example and it's corrupted.",
                    "label": 0
                },
                {
                    "sent": "Version right?",
                    "label": 0
                },
                {
                    "sent": "Because you think about it.",
                    "label": 0
                },
                {
                    "sent": "If I if I have a clean sorry if I have a clean example and I corrupt it.",
                    "label": 0
                },
                {
                    "sent": "The arrow going in the other direction basically tells me in which direction I need to go in order to increase probability.",
                    "label": 0
                },
                {
                    "sent": "Now there's going to be a little bit of bias here, but it turns out that in average it's going to point in the right direction so.",
                    "label": 0
                },
                {
                    "sent": "So now we basically.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An applied directly this original formula.",
                    "label": 0
                },
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "We have a way to estimate the model score and we have a way to estimate.",
                    "label": 0
                },
                {
                    "sent": "The data score, the data distribution score.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data distribution score is going to be given by the difference between the cleaner, corrupted and the model score is going to be given by this difference between reconstruction and the original at the corrupted location, and if you subtract 1 from the other, which is what you want to get the square difference, then you get a cancellation and you just get this denoising reconstruction error, which is different from the straight reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "So this is a kind of quick and dirty.",
                    "label": 0
                },
                {
                    "sent": "Demonstration of the connection between score matching, an denoising autoencoder objective function.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly score matching.",
                    "label": 0
                },
                {
                    "sent": "Actually is score matching on a smooth version of the original data distribution.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But that's a detail.",
                    "label": 0
                },
                {
                    "sent": "Another interesting aspect of these denoising decoders is that.",
                    "label": 0
                },
                {
                    "sent": "So let's do a 1 dimensional denoising order or contract.",
                    "label": 0
                },
                {
                    "sent": "It will behave pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what they're trying to do remember is to have a reconstruction.",
                    "label": 0
                },
                {
                    "sent": "Function, I actually didn't say that it's here.",
                    "label": 0
                },
                {
                    "sent": "They want to have a reconstruction function that's.",
                    "label": 0
                },
                {
                    "sent": "It has a derivative of zero at the data .1 way to see this is if you look at.",
                    "label": 0
                },
                {
                    "sent": "The denoising reconstruction and you do a Taylor expansion assuming Sigma is small.",
                    "label": 0
                },
                {
                    "sent": "So around X you get the normal reconstruction error plus a term in Sigma squared times the.",
                    "label": 0
                },
                {
                    "sent": "The norm of the Frobenius norm of the Jacobian of the reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So this is a vector of reconstructions.",
                    "label": 0
                },
                {
                    "sent": "However, the spec to the input and that matrix that Jacobian matrix.",
                    "label": 0
                },
                {
                    "sent": "If you somehow the square of all the elements, that's what this means.",
                    "label": 0
                },
                {
                    "sent": "So basically it says it wants the derivatives of the reconstructions with respect to the inputs to be small.",
                    "label": 0
                },
                {
                    "sent": "And it wants to have small reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "So how do you get these two objectives together?",
                    "label": 0
                },
                {
                    "sent": "Well, you want the derivative to be flat at the examples, but you want the examples to line on this identity line, right?",
                    "label": 0
                },
                {
                    "sent": "The because that's you know if get R of X = X, you want the reconstruction of X1 to be X1.",
                    "label": 0
                },
                {
                    "sent": "The reconstruction of X2 to be X2, and so on.",
                    "label": 0
                },
                {
                    "sent": "So so you need to have points that are at the same time on the identity line.",
                    "label": 0
                },
                {
                    "sent": "And you want to have flatness around the examples, that's the regularizer.",
                    "label": 0
                },
                {
                    "sent": "The regularizer here is something that wants locally constant features, and if you remember.",
                    "label": 0
                },
                {
                    "sent": "The adversarial examples we're trying to also get you locally constant features.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's it's kind of it kind of helps to see in a very toy way what what the.",
                    "label": 0
                },
                {
                    "sent": "What does denoising thing is really doing.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The cool thing is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to skip that.",
                    "label": 0
                },
                {
                    "sent": "The cool thing is not only we get this interpretation of the denoising order is telling us in which direction the density is increasing, but you can also use these to sample once you've trained them, and so we define a Markov chain, which is very, very simple and intuitive at each step of the Markov chain we corrupt.",
                    "label": 0
                },
                {
                    "sent": "So let's say add Gaussian noise or put masking noise, whatever.",
                    "label": 0
                },
                {
                    "sent": "Encode, decode and then sample from the reconstruction distribution.",
                    "label": 0
                },
                {
                    "sent": "So we for this to this sample means you know we come back to the definition where we said that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're doing here with the decoder is produce a conditional distribution of X given age.",
                    "label": 0
                },
                {
                    "sent": "So if we think of it like this, we can sample X given age, which means in the case in the Gaussian case, which it means we add an amount of noise that's proportional to our reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "OK, so so that defines a Markov chain.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me show you a sort of more example of this Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So let's say your data is sort of on this manifold an you start from the point on the manifold, you corrupt it, so you get the X~ Then you encoded decoded.",
                    "label": 0
                },
                {
                    "sent": "You go back in in the space of X.",
                    "label": 0
                },
                {
                    "sent": "And what you get is it is a distribution, let's say a Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So now you take your mean.",
                    "label": 0
                },
                {
                    "sent": "You add some noise within that covariance matrix and you get one of these guys an you do it again and that's going to be a random walk.",
                    "label": 0
                },
                {
                    "sent": "That's good, tend to stay on the manifold an visit.",
                    "label": 0
                },
                {
                    "sent": "The manifold like you would expect of any Markov chain.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one reason you might want to do this while is that.",
                    "label": 0
                },
                {
                    "sent": "Although P of X might be very complicated because it has many modes.",
                    "label": 0
                },
                {
                    "sent": "P of the Markov chain P of the next state given the previous state might be much simpler.",
                    "label": 0
                },
                {
                    "sent": "So to illustrate this, which is what we are trying to learn right instead of learning P of X directly, we're trying to learn one step of the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "So you know the true P of X might be this blue thing with many modes, just as an illustration.",
                    "label": 0
                },
                {
                    "sent": "But you could generate that distribution with a Markov chain that goes with local moves that.",
                    "label": 1
                },
                {
                    "sent": "And then each of those steps could be very simple.",
                    "label": 0
                },
                {
                    "sent": "Maybe this bimodal distribution here, so by conditioning on the previous state we make our the generative part of the model.",
                    "label": 0
                },
                {
                    "sent": "Hopefully much simpler because it doesn't have to deal with all the modes, only those in some neighborhoods.",
                    "label": 0
                },
                {
                    "sent": "So that's one motivation for learning a Markov chain rather than learning directly P of X.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the good news with these denoising order is that we can show that this Markov chain.",
                    "label": 0
                },
                {
                    "sent": "I was telling you about in uh, again, in a symbiotic sense, estimates the true data general distribution.",
                    "label": 0
                },
                {
                    "sent": "So more precisely, the theorems we have say that if your denoising model which goes from the corrupted to the clean as a conditional distribution approaches the true conditional distribution, the true denoising distribution.",
                    "label": 0
                },
                {
                    "sent": "Then the the stationary distribution of the Markov chain.",
                    "label": 1
                },
                {
                    "sent": "Becomes the true distribution.",
                    "label": 0
                },
                {
                    "sent": "The data generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So we don't need to model P of X directly, we just need to model the denoising distribution and that's good enough.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then you can generalize this this Markov chain, which went directly from XT2XT plus one by introducing some latent variables.",
                    "label": 0
                },
                {
                    "sent": "And this is what the generalized derivatives with stochastic networks are supposed to be.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you an example of that.",
                    "label": 0
                },
                {
                    "sent": "So remember in the denoising order we only add noise in the input.",
                    "label": 0
                },
                {
                    "sent": "Well now you can imagine a more complicated scenario where we're going to generate the Markov chain, but there's going to be noise in the hidden layers, and Furthermore those in layers could be connected to each other, so this is unfolding in time of this kind of recurrent net that has noise injected everywhere and generates a sequence of samples that would converge to the distribution.",
                    "label": 0
                },
                {
                    "sent": "We care we're trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "And the reason for this particular architecture is that it mimics the debost machine gives chain where you have exactly the same graph except you, you have binary units an at different sampling process at each step, but basically the same computational graph and the way you train this is basically by back prop.",
                    "label": 0
                },
                {
                    "sent": "So you unfold the chain from some initial training example through a few samples, and then you say, oh, I'd like that guy to be close to the original, so that the denoising objective used backdrop with the whole thing.",
                    "label": 0
                },
                {
                    "sent": "An update the parameters, so that's what basically generative stochastic networks are, and we verified that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are able to learn conditional.",
                    "label": 0
                },
                {
                    "sent": "I mean to learn to learn distributions both on like Poly data like this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And images like EM Nists and TFD.",
                    "label": 0
                },
                {
                    "sent": "But up to now, I would say.",
                    "label": 0
                },
                {
                    "sent": "They're not the best narrative models.",
                    "label": 0
                },
                {
                    "sent": "There's one.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That bothered me with with these models as well as with the deep belief Nets and the both machines you've already heard about and Aaron talked about that quite a bit, so I'm not going system much, which is that many of the models we've been considering in the past, including the GSM like I showed before, have this property that the when you think about how an image would be generated, you would maybe have a deep model an in the case of the GSN, it it has like a recurrence.",
                    "label": 0
                },
                {
                    "sent": "So it goes through many steps in the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "But it has these multiple levels and at every level you would add noise.",
                    "label": 0
                },
                {
                    "sent": "And that's bad.",
                    "label": 0
                },
                {
                    "sent": "That's bad, because if you think about.",
                    "label": 0
                },
                {
                    "sent": "Natural images so.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "There is no noise, it's all very sharp and clean, so these models are not a problem.",
                    "label": 0
                },
                {
                    "sent": "We don't want to.",
                    "label": 0
                },
                {
                    "sent": "Ideally we don't want to add noise along the way.",
                    "label": 0
                },
                {
                    "sent": "We would like to push the injection of noise higher up so that you get a chance to do a very non linear transformation that Maps the noise to the image is if you just add noise in the low level is just like adding white noise to your images so you're never going to get sharp clean images if the noise is injected everywhere along the way, it has to be injected.",
                    "label": 0
                },
                {
                    "sent": "As high as possible.",
                    "label": 0
                },
                {
                    "sent": "I mean there may be some kinds of noise that actually are sort of lower level, but most of the factors of variation we care about, like the identity of objects and their transparent their operations that have been applied to them and so on.",
                    "label": 0
                },
                {
                    "sent": "These these are very high level objects and you know there's a very strong year relationship between those factors and the input.",
                    "label": 0
                },
                {
                    "sent": "I call this noise, but maybe it's a misnomer, it's just.",
                    "label": 0
                },
                {
                    "sent": "Random Bibles that are sampled when you generate and in fact usually we think of noises at low level things and the high level things are not really thought of noise but more as the generative factors, yes.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "So one of the very common issues with almost all of the models we've played with is that they tend to generate blurry images.",
                    "label": 0
                },
                {
                    "sent": "And so you know, it's not obvious how to get rid of that.",
                    "label": 0
                },
                {
                    "sent": "Typically, the better the models, the less you have this issue.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Aaron told you a lot about this family of models, which is structured like this, where you have an end quarter and a decoder.",
                    "label": 0
                },
                {
                    "sent": "And they have this property that the noise is injected completely at the top.",
                    "label": 0
                },
                {
                    "sent": "When you generate, you generate from the top here and then you have a deterministic set of transformations to get the visible.",
                    "label": 0
                },
                {
                    "sent": "So this is different from the classical host machine where you were injecting noise all the way.",
                    "label": 0
                },
                {
                    "sent": "And also the training objective is slightly different and there's been a lot of work and you know Aaron has told you about some of that.",
                    "label": 0
                },
                {
                    "sent": "There's more like recent work like the Reweighted Way Sleep algorithm which was presented at last ICM, which is in the same family but uses different training objective.",
                    "label": 0
                },
                {
                    "sent": "And we've been exploring other related things.",
                    "label": 0
                },
                {
                    "sent": "I'm soon going to tell you about an interesting model called Nice that Aaron mentioned that also.",
                    "label": 0
                },
                {
                    "sent": "Has the property that it doesn't have noise along the way, but just just at the top.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, Anne, and here's a picture to try to yes.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, it's like it will not really noise.",
                    "label": 0
                },
                {
                    "sent": "It's abstractions when it when it gets to be there, yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh, you mean in the GSN?",
                    "label": 0
                },
                {
                    "sent": "Well, actually you can add noise anywhere you want, so in the GSM is very easy to fix this problem you just.",
                    "label": 0
                },
                {
                    "sent": "Put zero noise in the lower layers.",
                    "label": 0
                },
                {
                    "sent": "Well now it's not that easy actually, because.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Said something wrong, let me.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you the picture.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, so The thing is, if you don't add noise in the lower levels then it can kind of cheat, right?",
                    "label": 1
                },
                {
                    "sent": "It has.",
                    "label": 0
                },
                {
                    "sent": "If there's no noise at all, like, especially if you don't put noise, no corruption, then it can cheat and it can.",
                    "label": 0
                },
                {
                    "sent": "Just kind of copy things.",
                    "label": 0
                },
                {
                    "sent": "So well, so you either.",
                    "label": 0
                },
                {
                    "sent": "Have noise in the input.",
                    "label": 0
                },
                {
                    "sent": "Or noise in the middle layers, but it's not enough to have noise on the top, and for these kinds of models, but noise in the input is is OK, so this is actually the best results we got were actually with no noise in the lower levels, an corruption in the input.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me go to this kind of geometric picture that.",
                    "label": 0
                },
                {
                    "sent": "At least has helped me to understand what I think we should be doing.",
                    "label": 0
                },
                {
                    "sent": "If you go back to this manifold picture.",
                    "label": 0
                },
                {
                    "sent": "Remember I said what we have in the data space is a very complicated manifold like I drew this wire here.",
                    "label": 0
                },
                {
                    "sent": "It's actually in a high dimensional space, so it doesn't really cross on itself typically.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "If you think about what's going on with the variational, autoencoder or other models.",
                    "label": 0
                },
                {
                    "sent": "What we really hope is that the representative, very simple, like a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, it's a Gaussian that's.",
                    "label": 0
                },
                {
                    "sent": "Basically, has dimensions where the the variance is very small.",
                    "label": 0
                },
                {
                    "sent": "And dimensions where the variance can be large.",
                    "label": 0
                },
                {
                    "sent": "Basically that's what your manifold is, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a tiny tiny variance in the directions orthogonal to the manifold, and then in the direction of the manifold you have lots of variance because you can move around as you want on the manifold.",
                    "label": 0
                },
                {
                    "sent": "So like one very flat Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That would be one manifold.",
                    "label": 0
                },
                {
                    "sent": "Of course, you could have many manifolds, but let's say you have one manifold.",
                    "label": 0
                },
                {
                    "sent": "So what you want to do is to take a twisted curve and untwist it to get a flat one that you can model with the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So one way to think about this is we're trying to find a two way transformation between these two spaces that takes the manifold.",
                    "label": 0
                },
                {
                    "sent": "The twisted thing here, and flattens it, and of course can go backwards.",
                    "label": 0
                },
                {
                    "sent": "If we could do that, then we'd be in business.",
                    "label": 0
                },
                {
                    "sent": "An you can think of.",
                    "label": 0
                },
                {
                    "sent": "You can think of making these steps stochastic and then you have conditional distributions rather than functions.",
                    "label": 0
                },
                {
                    "sent": "And some approaches do that.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the nice approach haha.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear independent component estimation nice.",
                    "label": 0
                },
                {
                    "sent": "It was trying to do exactly that, so but you're looking at a terministic version of the transformation, and we wanted that that transformation to be invertible.",
                    "label": 0
                },
                {
                    "sent": "Turns out that if you have an invertible function, so you have F and G = F -- 1.",
                    "label": 0
                },
                {
                    "sent": "Then things become really easy, at least in principle in terms of maximizing the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So there's a very very simple equality here that comes from the change of variable equation.",
                    "label": 0
                },
                {
                    "sent": "You know, if you have an integral an you change variable from this space that space, then the density index can be basically mapped to a density in the H space when you just map X2F.",
                    "label": 0
                },
                {
                    "sent": "But now you have to pay a price for sort of how much volume.",
                    "label": 0
                },
                {
                    "sent": "Change you're getting when you go from the X Space H space that's given by the determinant of the Jacobian of this encoder.",
                    "label": 0
                },
                {
                    "sent": "So this is an equality.",
                    "label": 0
                },
                {
                    "sent": "The question is whether we can compute that determinant easily.",
                    "label": 0
                },
                {
                    "sent": "And also whether we can get an F such that we can compute its inverse, because you notice that if it's not invertible, then that will be 0, and then you have log of zero and it is just doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's a way to fix that, by the way, but anyways, the straight formula looks like you need to have an invertible encoder an it has to have the same dimension so you can have a Jacobian as the original space.",
                    "label": 0
                },
                {
                    "sent": "So we aimed to do that and we found a way to do it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Oh so at the top level we want something really, really simple, so we're going to ask the transform data to be to have a factorized prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is very common in all of these models, like in the variational encoder and the original Hamels machine that we assume that the underlying factors at the top level are independent of each other.",
                    "label": 0
                },
                {
                    "sent": "It's very strong.",
                    "label": 0
                },
                {
                    "sent": "Assumption is probably not correct, but the first approximation.",
                    "label": 0
                },
                {
                    "sent": "This is a very good thing to aim for.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we know that any distribution can always be generated that way, right?",
                    "label": 0
                },
                {
                    "sent": "So you know how are random numbers, random variates generated in the computer.",
                    "label": 0
                },
                {
                    "sent": "You start with uniform numbers, and then you do some operations on them and you get whatever you want.",
                    "label": 0
                },
                {
                    "sent": "This is how all of the random numbers generators work, right?",
                    "label": 0
                },
                {
                    "sent": "So so you think of that like this, we have uniform numbers here or Gaussian, and we apply a nonlinear transformation and then we get some whatever distribution we want.",
                    "label": 0
                },
                {
                    "sent": "So we want to learn that mapping and back such that what we get here is the data distribution.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so here are some samples from that model.",
                    "label": 0
                },
                {
                    "sent": "MNIST is OK. See far is not OK as VHN is not OK but.",
                    "label": 0
                },
                {
                    "sent": "It does something sensible.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I also do inpainting.",
                    "label": 0
                },
                {
                    "sent": "Meaning that you can, because we have a formula for the likelihood, we can just do gradient descent on it.",
                    "label": 0
                },
                {
                    "sent": "If I give you only the bottom part of an image, I can change the inputs here by just increasing the likelihood by just doing gradient descent on the likelihood gradient descent on the likelihood an in find values.",
                    "label": 0
                },
                {
                    "sent": "Like like this year.",
                    "label": 0
                },
                {
                    "sent": "So the top part has been filled here that here the bottom part has been filled and so on.",
                    "label": 0
                },
                {
                    "sent": "The left or right whatever.",
                    "label": 0
                },
                {
                    "sent": "Even in the bottom I think is like one pixel out of two that's been corrupted, so it seems to be just noise.",
                    "label": 0
                },
                {
                    "sent": "But if you do great in the sense you.",
                    "label": 0
                },
                {
                    "sent": "You get nice pictures.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now how did we get that to work?",
                    "label": 0
                },
                {
                    "sent": "Involved a Riddle, which is how we have a parameterized function.",
                    "label": 0
                },
                {
                    "sent": "That is going to have a determinant that we can compute, say, determinant of 1.",
                    "label": 0
                },
                {
                    "sent": "Ann is invertible and we can compute the inverse easily.",
                    "label": 0
                },
                {
                    "sent": "That seems like a tall order an before we started this, we didn't know how to do that, and so there's a trick.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're going to.",
                    "label": 0
                },
                {
                    "sent": "You can think of the trick is making the function kind of lower diagonal.",
                    "label": 0
                },
                {
                    "sent": "If you think about its different components.",
                    "label": 0
                },
                {
                    "sent": "So think about.",
                    "label": 0
                },
                {
                    "sent": "Two scalars or two vectors, so we can decompose the input into X1 and X2, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "Decompose our final function as the composition of a bunch of stages, and each of these stages is going to have this form.",
                    "label": 0
                },
                {
                    "sent": "It takes X1 and X2 and it produces Y1 and Y2 and and what it has here is just a copy.",
                    "label": 0
                },
                {
                    "sent": "So why one is a copy of X1 and Y2 is just a copy of X2 plus some nonlinear function of X1 and that gives rise to a Jacobian matrix like this.",
                    "label": 0
                },
                {
                    "sent": "Right, because the copy means you have identity blocks and you have zero here because there is no connection between X2 and Y1 and so the determinant is easy is 1 and it doesn't depend on the parameters, so we're not.",
                    "label": 0
                },
                {
                    "sent": "Each such stage is not changing the volume at all, right is keeping.",
                    "label": 0
                },
                {
                    "sent": "Volume properties.",
                    "label": 0
                },
                {
                    "sent": "But it's moving things around nonlinearly, and it turns out that if you stack enough of these stages, not that many, like 510 or something you can, you can learn the kinds of distributions I showed you before.",
                    "label": 0
                },
                {
                    "sent": "Now I I somehow don't think this is the right way of solving the problem and the reason I think it's a little bit problematic and this is just an intuition, is that the wise here are in a sense in the same space as the axis, right?",
                    "label": 0
                },
                {
                    "sent": "First of all they have the same dimension.",
                    "label": 0
                },
                {
                    "sent": "And you see, because of the copies that they're kind of in the same space.",
                    "label": 0
                },
                {
                    "sent": "Now they're more like residuals, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about it, this is, like, think of this like you're predicting Y 2 from X1.",
                    "label": 0
                },
                {
                    "sent": "And you subtracting X2, we adding, but it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "So what you're getting is like a prediction error of Y2.",
                    "label": 0
                },
                {
                    "sent": "Given X one and you look at the error.",
                    "label": 0
                },
                {
                    "sent": "A prediction and you basically have the original 1 and you have the error in predicting you original X1.",
                    "label": 0
                },
                {
                    "sent": "You have the error in predicting X2 using X1 and this is what you pass.",
                    "label": 0
                },
                {
                    "sent": "So you just compute these predictions of things given other things.",
                    "label": 0
                },
                {
                    "sent": "And you, you pass the residuals and eventually those residuals become Gaussian.",
                    "label": 0
                },
                {
                    "sent": "If you do things well.",
                    "label": 0
                },
                {
                    "sent": "But there I have the feeling that they're not going to be high level abstractions, just going to be residuals.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go now to the generative adversarial networks.",
                    "label": 0
                },
                {
                    "sent": "I don't have a lot of time.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So this was published.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adds NIPS 2014 just the last nips.",
                    "label": 0
                },
                {
                    "sent": "Anne, it's yet a completely different way of thinking about learning Java models.",
                    "label": 0
                },
                {
                    "sent": "So again, like I said before, we're going to skip having a P of X, unlike in the nice where we had an explicit P of X.",
                    "label": 0
                },
                {
                    "sent": "And we're going to skip the Markov chain idea as well, so it's going to be.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, just neural net takes random numbers output samples.",
                    "label": 0
                },
                {
                    "sent": "Question is how we train this guy?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way we're going to train it is by playing a game.",
                    "label": 0
                },
                {
                    "sent": "And there's no variational bound either.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to play a game.",
                    "label": 0
                },
                {
                    "sent": "The game has two players.",
                    "label": 0
                },
                {
                    "sent": "The generator, which is the thing we want to train at the end, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the neural net that takes random numbers and outputs images.",
                    "label": 0
                },
                {
                    "sent": "And the discriminator.",
                    "label": 0
                },
                {
                    "sent": "So we're going to call the generator G and this quarter D An what the discriminator does is try to figure out.",
                    "label": 0
                },
                {
                    "sent": "Try to evaluate how good examples from the generator an it is in the December is going to be a classifier, as its name indicates, an it's going to be trained to discriminate between a sample from the data distribution, like your training examples and the sample from the generator.",
                    "label": 0
                },
                {
                    "sent": "So that looks like a very simple idea right away to measure the quality of a generative model is that if somebody looks at the generated samples, the whole distribution right?",
                    "label": 0
                },
                {
                    "sent": "So you have a.",
                    "label": 0
                },
                {
                    "sent": "100,000 samples coming from your model and looks at the training data and can't find a way to classify and discriminate one against the other.",
                    "label": 0
                },
                {
                    "sent": "That means you must have a very good classifier.",
                    "label": 0
                },
                {
                    "sent": "I mean a very good.",
                    "label": 0
                },
                {
                    "sent": "Not a good well means you have a good generator.",
                    "label": 0
                },
                {
                    "sent": "You want to have a good classifier.",
                    "label": 0
                },
                {
                    "sent": "Otherwise your measure of goodness is not going to be very useful.",
                    "label": 0
                },
                {
                    "sent": "So this computer has to be very good.",
                    "label": 0
                },
                {
                    "sent": "It's trying to so let me see there.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a nice.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, you can.",
                    "label": 0
                },
                {
                    "sent": "Think of it like a game between the police and the counterfeiter, right?",
                    "label": 1
                },
                {
                    "sent": "So the generator is trying to counterfeit the real money coming from the data.",
                    "label": 0
                },
                {
                    "sent": "And and this community is, like you know, the police trying to figure out.",
                    "label": 0
                },
                {
                    "sent": "Is this a real bill or it's made up?",
                    "label": 0
                },
                {
                    "sent": "So they are constantly fighting each other, trying to beat each other, and you can define an objective function.",
                    "label": 0
                },
                {
                    "sent": "Which is like a game theoretical or mini Max or saddle point or whatever objective function where one of the players wants to minimize it and the other ones to maximize it.",
                    "label": 0
                },
                {
                    "sent": "And the game is basically what you would expect that the discriminator is trying to predict whether the data comes from one or the other thing, and the generator is sending inputs to discriminator and trying to make it provide the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "This is what this equation is saying, just in algebra.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "You can prove what the optimal discriminator would be.",
                    "label": 0
                },
                {
                    "sent": "It's just by Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "What if you knew the data distribution and you knew the model solution?",
                    "label": 1
                },
                {
                    "sent": "If you have the density is a formula, then you could you could find the optimal deevak.",
                    "label": 0
                },
                {
                    "sent": "So first we have neither P Dayton or P model in our case, so we can't actually compute this.",
                    "label": 0
                },
                {
                    "sent": "But we can train a model that classifieds between the two distributions.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, sorry, so in terms of operational terms, the discriminator is going to be trained on two kinds of data, so we have X either sample from the data and then send to the discriminator and we would like it to output category one or X comes from the generator which takes input noise.",
                    "label": 1
                },
                {
                    "sent": "Nonlinear transforms it.",
                    "label": 0
                },
                {
                    "sent": "An gives these counter feed samples, and then the discriminator is supposed to say 00 now.",
                    "label": 0
                },
                {
                    "sent": "On on the funny part is on the examples here.",
                    "label": 0
                },
                {
                    "sent": "D is trying to improve its classification, but G is trying to make that guy make more mistakes.",
                    "label": 0
                },
                {
                    "sent": "So that was the second part of the objective function that I showed here.",
                    "label": 0
                },
                {
                    "sent": "I know this guy is trying to make the log like you'd better for himself, and she's trying to make it worse because we are minimizing over Gmail.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So you have these two neural Nets and you just, you know stick one to the input of the other an backdrop and basically you insert a a -- 1 in the gradient here when you back propan.",
                    "label": 0
                },
                {
                    "sent": "Or you change the sign of the updates here whatever.",
                    "label": 0
                },
                {
                    "sent": "And that's very simple to implement.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens during training though is a little bit funny.",
                    "label": 0
                },
                {
                    "sent": "So and it took us some time to understand what was going on and also to understand its failure modes so.",
                    "label": 0
                },
                {
                    "sent": "Here's a picture to try to understand what is ideally going on.",
                    "label": 0
                },
                {
                    "sent": "Think of.",
                    "label": 0
                },
                {
                    "sent": "The Latent Space Z as a 1 dimensional real line.",
                    "label": 0
                },
                {
                    "sent": "So you have a distribution.",
                    "label": 0
                },
                {
                    "sent": "An initially you say, so there's at a uniform right?",
                    "label": 0
                },
                {
                    "sent": "And we're trying to learn is a transformation that Maps every point on the real line.",
                    "label": 0
                },
                {
                    "sent": "Here to a point on the X line.",
                    "label": 0
                },
                {
                    "sent": "So this is the data space and what the transformation function the G does is.",
                    "label": 0
                },
                {
                    "sent": "It's trying to map the uniform distribution to distribution, which puts maybe a lot of mass here, and no mass here, right?",
                    "label": 0
                },
                {
                    "sent": "So that you can get that by, you know some kind of nonlinear function like this.",
                    "label": 0
                },
                {
                    "sent": "So initially G is bad and these bad.",
                    "label": 0
                },
                {
                    "sent": "But let's say we keep G fix and we can optimize D, which is the right way of thinking about it.",
                    "label": 0
                },
                {
                    "sent": "So now, given the current GD is trying to discriminate between the data distribution which is putting mass in the middle here the black dots.",
                    "label": 0
                },
                {
                    "sent": "And the samples coming from G and because they are more black dots on the left and on the right.",
                    "label": 0
                },
                {
                    "sent": "Then it learns this.",
                    "label": 0
                },
                {
                    "sent": "Maybe this, you know classifier that says, oh here's the decision surface and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is the output probability for the discriminator.",
                    "label": 0
                },
                {
                    "sent": "Now it's done a good job and now that we have a good D. We can update G. So now G is going to try to fool D. How could it fool G?",
                    "label": 0
                },
                {
                    "sent": "Well, it needs to shift some of these arrows to the left so that they will pass for good bills, according to the discriminator.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you do a step of gradient in that direction and then the green distribution moves a bit to the left.",
                    "label": 0
                },
                {
                    "sent": "Now you can see right away where it can get wrong.",
                    "label": 0
                },
                {
                    "sent": "How can we get wrong here?",
                    "label": 0
                },
                {
                    "sent": "I mean completely wrong if we're not careful, yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it could go completely to the left, so this is great here.",
                    "label": 0
                },
                {
                    "sent": "I mean the problem is even higher and this is totally wrong.",
                    "label": 0
                },
                {
                    "sent": "Of course right?",
                    "label": 0
                },
                {
                    "sent": "So what you see in this illustration is that.",
                    "label": 0
                },
                {
                    "sent": "You have to make a little step in G for every full optimization of deities.",
                    "label": 0
                },
                {
                    "sent": "That's what the theory says.",
                    "label": 0
                },
                {
                    "sent": "In practice, you have to play with the learning rates so that D can keep up with the changes you're making in G. Otherwise you get into completely wrong results and you can see that happening if you're not careful.",
                    "label": 0
                },
                {
                    "sent": "So that's the main thing to keep in mind.",
                    "label": 0
                },
                {
                    "sent": "And of course, if you do these carefully, then at the end of the day you have this so-called mixed strategy equilibrium where the game everybody every player in the game is happy and they can't change.",
                    "label": 0
                },
                {
                    "sent": "And get happy.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we get these kinds of samples.",
                    "label": 0
                },
                {
                    "sent": "They're good, they're better than the nice.",
                    "label": 0
                },
                {
                    "sent": "For example that I showed you.",
                    "label": 0
                },
                {
                    "sent": "But they're not as good as what I'm going to show you next.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Blah blah blah.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More samples, so this is this is nice movies that Ian made.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "I you.",
                    "label": 0
                },
                {
                    "sent": "You take 2 random points in the Dead Space.",
                    "label": 0
                },
                {
                    "sent": "You trace a line.",
                    "label": 0
                },
                {
                    "sent": "Remember, we said we can interpolate.",
                    "label": 0
                },
                {
                    "sent": "In the edge space and we should be able to see nice samples, right?",
                    "label": 0
                },
                {
                    "sent": "And especially here because the distribution is Gaussian or uniform, which this should work.",
                    "label": 0
                },
                {
                    "sent": "And then for each point on the line we we project down into the image space and we visualize that.",
                    "label": 0
                },
                {
                    "sent": "So now we have a smooth transformation of the image that tells you what happens when you go from one place to another place.",
                    "label": 0
                },
                {
                    "sent": "You can see this for the MNIST digits and you can see this for the faces.",
                    "label": 0
                },
                {
                    "sent": "There's still like a lot of noise that we'd like to get rid of.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and you can do that for cifar images, so it's it's getting the idea that there are like objects in the middle and the background, but these objects are not really sharp as we would like.",
                    "label": 0
                },
                {
                    "sent": "There are much sharper than what we get with the VA though.",
                    "label": 0
                },
                {
                    "sent": "And so some smart people at NYU, you and Facebook.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There was a poster about this here at this summer school.",
                    "label": 0
                },
                {
                    "sent": "Use this type of generator in a kind of hierarchical cascades.",
                    "label": 0
                },
                {
                    "sent": "And I think they say this is currently the best.",
                    "label": 0
                },
                {
                    "sent": "Giant models of images.",
                    "label": 0
                },
                {
                    "sent": "I'll show you some samples later.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So they figured that.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generating directly the image is difficult.",
                    "label": 0
                },
                {
                    "sent": "But maybe we can break the problem.",
                    "label": 0
                },
                {
                    "sent": "Of generating images into.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into pieces.",
                    "label": 0
                },
                {
                    "sent": "By considering a bunch of conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So instead of generating the image directly from scratch from a zed, we're going to create some intermediate.",
                    "label": 0
                },
                {
                    "sent": "Variables that are actually deterministic functions of the input an.",
                    "label": 0
                },
                {
                    "sent": "In their case, those deterministic functions of the input are the low resolution versions of the image.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have the image.",
                    "label": 0
                },
                {
                    "sent": "And instead of trying to model directly, we're going to consider a bunch of other random variables which are not terministic function of that original image, and it's just low resolution versions of the image.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to consider the problem of conditional distribution right, which is always easier, right?",
                    "label": 0
                },
                {
                    "sent": "You should.",
                    "label": 0
                },
                {
                    "sent": "Eventually, figure out that learning conditional distribution is always easier than than an unconditional one because those conditional distributions are have less diversity.",
                    "label": 0
                },
                {
                    "sent": "Less modes.",
                    "label": 0
                },
                {
                    "sent": "Remember when I said learn a Markov chain is conditional in previous state and that should be easier because we condition on something.",
                    "label": 0
                },
                {
                    "sent": "Now we have less choices, so it's going to be easier to model that.",
                    "label": 0
                },
                {
                    "sent": "So so they exploit this, and now we're going to condition on each level we're going to predict the next one.",
                    "label": 0
                },
                {
                    "sent": "The description of the next one.",
                    "label": 0
                },
                {
                    "sent": "So we just train a bunch of conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "Each of them is going to be.",
                    "label": 0
                },
                {
                    "sent": "Basically you're you're gone.",
                    "label": 0
                },
                {
                    "sent": "Your generative adversarial network.",
                    "label": 0
                },
                {
                    "sent": "That that's the G here.",
                    "label": 0
                },
                {
                    "sent": "I guess that takes.",
                    "label": 0
                },
                {
                    "sent": "Takes the usual zed, but is conditioned on extra inputs to generate a.",
                    "label": 0
                },
                {
                    "sent": "Well, not exactly.",
                    "label": 0
                },
                {
                    "sent": "The low resolution version, but the difference between the two images, something you can do with with images that's called applausi and pyramid, so they're trying to generate these differences rather than the low resolution images themselves.",
                    "label": 0
                },
                {
                    "sent": "The differences between those, which is kind of.",
                    "label": 0
                },
                {
                    "sent": "Well known feature representation for images.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically you start by predicting the with the regular ground.",
                    "label": 0
                },
                {
                    "sent": "The very low resolution image not predicting, but generating it using using one normal Gan.",
                    "label": 0
                },
                {
                    "sent": "This one so it takes is edit produces the low resolution image and of course you train a discriminator that learns to make the difference between the generated distribution and the true ones.",
                    "label": 0
                },
                {
                    "sent": "And now you can take those those images.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And use them to condition the next stage, which is going to produce the difference between the sort of low resolution and high resolution version of the image.",
                    "label": 0
                },
                {
                    "sent": "And so you sample, you learn to sample the conditional that goes from here to here and then another one, another model.",
                    "label": 0
                },
                {
                    "sent": "It can be trained completely separately.",
                    "label": 0
                },
                {
                    "sent": "That goes from here to here and here to here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that process you generate low resolution version of the image and then.",
                    "label": 0
                },
                {
                    "sent": "That's unconditional and then generate the next one conditionally, and so on, and eventually you get a high resolution version of the image.",
                    "label": 0
                },
                {
                    "sent": "And then the amazing thing is that these images are very good and if you ask people to make the difference between these images and the training images, so we played the Gan game with humans, which is basically what a Turing test is.",
                    "label": 0
                },
                {
                    "sent": "At 40% of the time the humans are confusing the generated images for the real one, so we're not exactly doing as well as humans, but this is, I think, very impressive.",
                    "label": 0
                },
                {
                    "sent": "Alright, in the last few minutes, let me tell you about another model which is not generative.",
                    "label": 0
                },
                {
                    "sent": "But it is related to the denoising auto encoder.",
                    "label": 0
                },
                {
                    "sent": "And seems to be beating everything else in terms of learning good features for classification.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a semi supervised setting.",
                    "label": 0
                },
                {
                    "sent": "So this is called the ladder network.",
                    "label": 0
                },
                {
                    "sent": "It's very recent work.",
                    "label": 0
                },
                {
                    "sent": "So by the way, if you are not used to archive, you can read the time when the paper was submitted the first time.",
                    "label": 0
                },
                {
                    "sent": "So 15 Zero 7 means julai 2015 and then the papers are ordered sequentially.",
                    "label": 0
                },
                {
                    "sent": "So if somebody tells you my paper is 15 zero 703 something, that means that this one came before.",
                    "label": 0
                },
                {
                    "sent": "This is really useful.",
                    "label": 0
                },
                {
                    "sent": "One of the tricks you can learn in summer school.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is this model?",
                    "label": 0
                },
                {
                    "sent": "So you can think of it like a stack of denoising auto encoders with lateral connections.",
                    "label": 0
                },
                {
                    "sent": "Along with a feedforward net that uses the end quarters to predict the thing you care about, the wise that you want to predict for classification.",
                    "label": 0
                },
                {
                    "sent": "So let's see.",
                    "label": 0
                },
                {
                    "sent": "So we have three things here.",
                    "label": 0
                },
                {
                    "sent": "We have the end quarter use used during training.",
                    "label": 0
                },
                {
                    "sent": "For the purpose of.",
                    "label": 0
                },
                {
                    "sent": "Basically reconstruction, so we have encode, encode, decode, decode.",
                    "label": 0
                },
                {
                    "sent": "But you see these lateral connections.",
                    "label": 0
                },
                {
                    "sent": "So when you're going to decode, you're allowed to use.",
                    "label": 0
                },
                {
                    "sent": "Lateral connections here.",
                    "label": 0
                },
                {
                    "sent": "You have to use what's going on here.",
                    "label": 0
                },
                {
                    "sent": "That essentially frees up the high level from trying to do everything.",
                    "label": 0
                },
                {
                    "sent": "So in the old models.",
                    "label": 0
                },
                {
                    "sent": "All of the models you've seen up to now, you don't have these lateral connections and what it means is that every level needs to capture everything about the input, especially in the sort of autoencoder type of framework.",
                    "label": 0
                },
                {
                    "sent": "Whereas.",
                    "label": 0
                },
                {
                    "sent": "You can think of the information to generate.",
                    "label": 0
                },
                {
                    "sent": "Here will come either from.",
                    "label": 0
                },
                {
                    "sent": "Sort of low level noise.",
                    "label": 0
                },
                {
                    "sent": "Or high level abstractions?",
                    "label": 0
                },
                {
                    "sent": "And you're combining the two things to generate the image.",
                    "label": 0
                },
                {
                    "sent": "OK, but the way they actually.",
                    "label": 0
                },
                {
                    "sent": "Design this man presented in the paper is not a generative model.",
                    "label": 0
                },
                {
                    "sent": "It's like a feature, semi supervised algorithm, so we're going to have.",
                    "label": 0
                },
                {
                    "sent": "Basically two terms with the objective function.",
                    "label": 0
                },
                {
                    "sent": "One that says, well, pretty quiet, as well as you can.",
                    "label": 0
                },
                {
                    "sent": "This is target given X.",
                    "label": 0
                },
                {
                    "sent": "Requires well, as you can actually on this, using the corrupted input, so it's just like.",
                    "label": 0
                },
                {
                    "sent": "Dropout network, right?",
                    "label": 0
                },
                {
                    "sent": "So you've got a feedforward net with noise injected along the way.",
                    "label": 0
                },
                {
                    "sent": "And try to make these.",
                    "label": 0
                },
                {
                    "sent": "Zed hat here.",
                    "label": 0
                },
                {
                    "sent": "Good reconstructions?",
                    "label": 0
                },
                {
                    "sent": "Of the clean path here zed that you get without any noise, so you've got, you know denoising.",
                    "label": 0
                },
                {
                    "sent": "Remember we want to clean versus the corrupted so they run this network which is pure clean.",
                    "label": 0
                },
                {
                    "sent": "It's also the same network you used to actually do classification on test examples, so no noise.",
                    "label": 0
                },
                {
                    "sent": "And we want to match this CD.",
                    "label": 0
                },
                {
                    "sent": "Here is the cost that they're adding here.",
                    "label": 0
                },
                {
                    "sent": "The squared error, so that's the normal reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "But we have one for each level, so compared to clean path activations to the.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "That level reconstruction coming from above.",
                    "label": 0
                },
                {
                    "sent": "And they optimize this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they get failure.",
                    "label": 0
                },
                {
                    "sent": "Amazing results on the semi supervised and also the regular M lists.",
                    "label": 0
                },
                {
                    "sent": "So on the regular amnestied train with everything, so these are a previous results.",
                    "label": 0
                },
                {
                    "sent": "It's actually nice to put all of these things in one big table.",
                    "label": 0
                },
                {
                    "sent": "So you can see.",
                    "label": 0
                },
                {
                    "sent": "The adversarial Nets here point 78 that we're doing quite well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And I think hung like talked about the pseudo level pseudo label algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, I guess it's only the semi supervised case.",
                    "label": 0
                },
                {
                    "sent": "What else did you hear about?",
                    "label": 0
                },
                {
                    "sent": "DGN here is not the variational encoder.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Aaron is not there I guess.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, sorry, sorry.",
                    "label": 0
                },
                {
                    "sent": "Did you write?",
                    "label": 0
                },
                {
                    "sent": "This is after survival training?",
                    "label": 0
                },
                {
                    "sent": "I got confused.",
                    "label": 0
                },
                {
                    "sent": "Yes, thanks.",
                    "label": 0
                },
                {
                    "sent": "Um anyways.",
                    "label": 0
                },
                {
                    "sent": "Yes, um.",
                    "label": 0
                },
                {
                    "sent": "Was do you know what this DGN is with point 96?",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                },
                {
                    "sent": "So M1 plus M2 blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "OK so yeah, actually, but I remember you had like .96 and anyways.",
                    "label": 0
                },
                {
                    "sent": "Anyway, OK, so they get .61 which is currently the state of the art on what's called permutation invariant amnis, where you don't use any information about the fact that it's an image.",
                    "label": 0
                },
                {
                    "sent": "And but then, the really amazing thing is that you can do this with a handle labeled example and they get one point 13% error.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know when I got into this business of Hymnists?",
                    "label": 0
                },
                {
                    "sent": "In the old days.",
                    "label": 0
                },
                {
                    "sent": "We would have died for a number like this on the full data set.",
                    "label": 0
                },
                {
                    "sent": "I mean like not 100 examples, but 60,000.",
                    "label": 0
                },
                {
                    "sent": "So now they're getting this number with only hand labeled examples for the user.",
                    "label": 0
                },
                {
                    "sent": "60,000 unlabeled examples as well.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is something to follow up on I think.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to conclude.",
                    "label": 0
                },
                {
                    "sent": "The log likelihood objective function for high dimensional.",
                    "label": 0
                },
                {
                    "sent": "Vector is he trying to model with complicated solution is is intractable and so people have been proposing your kinds of approximations and you've heard at the beginning of the summer school about Monte Carlo, Markov chain methods, and both machines.",
                    "label": 0
                },
                {
                    "sent": "But the good thing is not there's being a rich variety of different approaches that have been proposed that avoid this intractability, that actually are not just approximations to the log likelihood, but completely different approaches like score matching and the generative adversarial networks and ice and so on.",
                    "label": 0
                },
                {
                    "sent": "So, and this is not a complete portrait, there more so that's very interesting as a space to explore because it's like we're discovering a new world basically.",
                    "label": 0
                },
                {
                    "sent": "Another thing I hope you've learned from this is that although the autoencoders were initially introduced as some something, a bit hacky that we didn't have a probabilistic interpretation for, we now have fairly solid interpretation for what they do.",
                    "label": 0
                },
                {
                    "sent": "They allow to find the directions in which you should move to go towards more probable configurations, and if you if you do this iteratively, then you move towards and you add noise.",
                    "label": 0
                },
                {
                    "sent": "You basically get a Markov chain from which you can sample and you get you get fairly decent.",
                    "label": 0
                },
                {
                    "sent": "Samples from that.",
                    "label": 0
                },
                {
                    "sent": "I guess I initially had slides on the variational encoder, but Aaron covered that very well.",
                    "label": 0
                },
                {
                    "sent": "I also told you about the generative adversarial networks, which again really follow a completely different approach.",
                    "label": 0
                },
                {
                    "sent": "Which you can think as optimizing a kind of Turing test, which I think is a really interesting way to think about what generative models should be optimizing.",
                    "label": 0
                },
                {
                    "sent": "And currently the basis for the best genitive models of images, but who knows, in two weeks from now, someone will come up with a better model.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}