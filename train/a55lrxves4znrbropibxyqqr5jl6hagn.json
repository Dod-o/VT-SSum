{
    "id": "a55lrxves4znrbropibxyqqr5jl6hagn",
    "title": "Graphical Models and message-passing algorithms",
    "info": {
        "author": [
            "Martin J. Wainwright, UC Berkeley"
        ],
        "published": "Oct. 12, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/mlss2011_wainwright_messagepassing/",
    "segmentation": [
        [
            "So I'll be talking about graphical models, I guess some aspects of graphical models appear up there, things like.",
            "Graphical models trees bounds reweighted Gaussian will see algorithm will see many of these things in these tutorial talks.",
            "So the plan is the first talk today is going to be somewhat higher level.",
            "Just giving the basics of graphical models.",
            "Just giving you a feel what they can be used for.",
            "Start drilling down a little bit towards the end of the lecture today and then tomorrow we'll get more in detail on message passing and also what are known as variational methods.",
            "I'm kind of fortunate, I think, 'cause I'm the last lecture, so I can essentially build on what everyone else has done.",
            "I assume that you know everything that every other lecture that preceded me.",
            "So there will be some things that are relevant, probably for tomorrow things that leave in Vandenberghe taught you about optimization.",
            "It could be helpful if you just glanced at the definition of conjugate dual function again from his notes, I'll go through it again tomorrow, but we're going to use it will see it actually is very important in graphical models, so that's a nice connection to what you heard from leaving.",
            "OK, so let's get started."
        ],
        [
            "I'm going to use a combination of these slides, but then to slow me down I'm going to write things down on the overhead as we go.",
            "Actually, just for curiosity, how many people know about graphical models, as in, have either read about them or had a course already?",
            "OK, so why am I here?",
            "Well, that's fine, but I'm just trying to set the level.",
            "You don't know, OK?",
            "No so.",
            "Is always this sort of multiresolution aspect when you're teaching so today I'm going to start from the basics, probably by later tomorrow afternoon.",
            "We can.",
            "Maybe you know, take the gloves off a little bit.",
            "I think that's sort of a fair, fair compromise.",
            "OK, so graphical models have a pretty long history.",
            "They used a lot in machine learning, but actually the history lies in different areas that earlier history.",
            "Probably the earliest history is in statistical physics.",
            "Will talk in a minute about something called the Eising model, which was proposed around the 1920s, but also in areas like phylogenetic's people studying trees the way organisms evolve and split.",
            "It's very natural that they started using graphs as well.",
            "And I'll talk a little bit.",
            "Also in communication these models were used a lot in the 50s, so in some ways artificial intelligence and machine learning is a bit of a latecomer to the field is really in the 80s that Udaya Pearl, in his book started talking a lot about graphs and algorithms on graphs and things like that, and since then, of course the area is exploded.",
            "There's a very active literature.",
            "OK, so as the name suggests, graphical models involve graphs.",
            "There's different kinds of graphs.",
            "I could sort of spend a lot of time talking about different kinds of graphs, but just to keep things focused here, we're going to talk exclusively about undirected graphs and what the purpose of the graphical model is.",
            "It's to allow us to model the dependencies among a large collection of random variables in a relatively simple way.",
            "And so the way we do that is we take our graph, so here's an ordinary graph.",
            "This is a different object.",
            "I'll talk about it in a minute.",
            "A graph has nodes or vertices, and we're going to assign a random variable to every one of those things.",
            "And what we'll see is that the edges, or actually the absence of edges, those are going to allow us to include what kind of dependency structure we see in a collection of random variables.",
            "OK, so what's the motivation for doing this?",
            "Well, the motivation is, let's just do a little thought experiment.",
            "If I just gave you, let's say N is 100, you have 100 variables and the variables are disk coin flips there.",
            "This Bernoulli an if you didn't have any structure whatsoever on the distribution, how many numbers would you need to represent it?",
            "So 100 binary variables?",
            "Two to the 100, or if you want to be minus one, yes, right?",
            "So it doesn't really matter two to the 100 is probably larger than it is larger than the number of atoms in the visible universe, so there's no way that you could ever represent a distribution you couldn't even store it.",
            "You can't compute with it.",
            "And 100 is a small a small problem.",
            "I'll show you a simulation in a moment that involves something like 10,000 variables."
        ],
        [
            "So that's the point of these.",
            "Graphical models is going to allow us to really simplify and specify simpler factorizations, in particular of distributions.",
            "OK, so.",
            "What we're going to do now?",
            "We want to start relating the structure of the graph to the dependency structure of the random variables.",
            "There's sort of two ways of doing this.",
            "One way of doing it is in terms of what's known as the clique structure of a graph, so just out of curiosity, how many people know what a clique is?",
            "OK, so some subset of you, but you don't.",
            "I admire you, actually, 'cause you admit what you don't know.",
            "If you do it a little experiment, any audience.",
            "I do this often you say who knows about X and then some fraction raises their hand.",
            "You say who does not know about X?",
            "It's definitely not the one minus the fraction that just raised their hand.",
            "Nothing wrong with not knowing anything.",
            "What's wrong is when you don't know what you don't know.",
            "Starting to sound like Donald Rumsfeld for those of you who know him.",
            "OK, so let's write a little bit here.",
            "Week oh is our friend back.",
            "How did that happen?",
            "He was gone a minute ago.",
            "It's OK, he'll give you some.",
            "That's true.",
            "Sometimes I feel like that too.",
            "OK, so we want to link move that doesn't work.",
            "Hopefully this one works.",
            "We want to link graphs an probabilities right that's.",
            "Is that better?",
            "Good graphs plus probability?",
            "That's what we're doing.",
            "Um?",
            "Well, so we've got an undirected graph.",
            "Right, so all throughout all, use V for vertices or nodes.",
            "Just think about that is the set one through N. We're just labeling the nodes of our graph, and then E is a bunch of edges, so edges for an undirected graph, or just unordered pairs.",
            "These are edges IJI not equal to J. Hi there.",
            "OK, and we've got a random variable sitting at each node of the graph.",
            "Right, so random variable X at node I. OK, and So what we want to do is we've got.",
            "We've got this random vector.",
            "Let me introduce the shorthand that I'll use sometimes I'll call XAV.",
            "Whenever I use a subset like V or A or anything, it just means this is a big subset of random variables that are indexed by the vertices in this case.",
            "So we've got a big random vector and we'd like to specify a distribution over it.",
            "So a clique.",
            "C. The subset of vertices.",
            "That is fully connected.",
            "Right, so it just means that I J is in the edge set for all I&J that are in the vertex set.",
            "OK, so that's what I've defined up here.",
            "So let's let's look at this graph that I've drawn up here, and let's figure out what the cliques are.",
            "Anyone?",
            "Well, I've already put them there for you.",
            "It's rather helpful.",
            "Right, but for instance 123 is a clique because 1, two, and three all talk.",
            "What about 1234?",
            "Is that a clique?",
            "No, it's not, because four should be friends with two and one.",
            "So in this case the maximal cliques.",
            "Those are cliques that aren't contained within anything else.",
            "Those are 123.",
            "This triangle 345456 and four 7.",
            "So it's 2 three o'clock for instance.",
            "Right, it is a clique, but it's not what we call a maximal clique.",
            "We could make it bigger by adding one to it.",
            "So sometimes we'll talk about cliques, sometimes about maximal cliques.",
            "The other thing we're going to need, I'll just I'm just here just rewriting what I've got on the slide just so we can refer to it later.",
            "We're going to vertex cut set right, so vertex cut set is.",
            "It's a subset.",
            "It's a different kind of subset.",
            "So it's a subset of vertices who when you remove them it splits the graph into two or more.",
            "Um, disjoint parts, right?",
            "That's slightly vague.",
            "You can make that precise, but the intuition is given here this set here.",
            "If I remove this pair of vertices from the graph, that's vertices four and five in this graph, then it basically breaks the graph into two pieces, and the two pieces are set of variables A and the set of variables B. OK, so those are two graph theoretic properties and we're going to use those graph theoretic properties.",
            "We're going to use them to specify how distributions should somehow respect the structure of the graph.",
            "Um?",
            "So one way will do it.",
            "This slide says, so we're going to remember those two."
        ],
        [
            "Guns.",
            "So there's sort of two ways in which we can use the graph to build structured representations of our random variables.",
            "One is that we can impose what's known as a Markov property.",
            "So how many people know about Markov chains?",
            "Just to just be sure?",
            "OK, so a little bit.",
            "Right, so Markov chain is actually a very special kind of graphical model.",
            "Let's let's actually do this before I go on.",
            "Right, somehow this it's really historically where these things came from.",
            "A Markov chain is as you'd expect, it's it's a graph, that's a chain, right?",
            "So you've just got variables ordered like this.",
            "It's very natural if you have things like time series.",
            "See this old technology is kind of, well, it's nice in some ways but annoying in others.",
            "Right?",
            "I'll come down in a second.",
            "Right, so in a Markov chain, you've got a bunch of random variables.",
            "Think about a time series.",
            "Think about for instance, you're trying to track the position of a person walking in a camera in a sequence video sequence.",
            "So X one could be positioned at time 21X2 position at time two, and so on, and the natural sort of model you'd have is that that's evolving according to something like a Markov chain.",
            "Right, so where are the vertex cut sets in this very simple graph here?",
            "Right, basically any variables a vertex cut set.",
            "The interesting ones.",
            "This is not an interesting one, but the interesting vertex cut sets.",
            "If I remove this from my graph, right if I call that S. If I remove that, then that's going to break the graph into two pieces, and in this case they have a nice interpretation.",
            "You would get the past.",
            "If the cut set is the present, then the present splits the graph into the past and the future.",
            "Right, so that's that's one way of defining Markov chains, right?",
            "You say that it's a process that if you know the present, then everything else in the past tells you nothing more about the future.",
            "Right, but this is a simple instance of this more general property that I'm about to introduce.",
            "Because this is a cut set, this cut sets split the graph into two pieces, the past and the future, and what you had is a conditional independence property.",
            "And the notation we use for this.",
            "We say that X of a.",
            "That's the random variables in the past is conditionally independent of X of.",
            "I think I've messed up my notation.",
            "I was using B to call that B, thanks.",
            "Yeah, This is why I hate old technology.",
            "I can guarantee you you should look at me at dinner.",
            "I'll have paint on my face.",
            "Right so past.",
            "Future.",
            "And present.",
            "Right, So what this notation means?",
            "This notation means that the set of variables indexed by the past by the set A are conditionally independent of the future given the present right?",
            "So this is shorthand for X of A is conditionally independent of X of B given past.",
            "So how many people have seen that property before?",
            "OK.",
            "So there's ways to slow down professors, write questions.",
            "Always slow down professors, ways to speed them up is sort of, you know, audible snoring's or sleeping noises tend to speed.",
            "Or they might anger Professor, but probably would speed them up.",
            "OK, so the more general property is that if you have any graph right not just a Markov chain, what you say is that if you have a vertex cut set then you want the parts that get split.",
            "You want the random variables in those parts to be conditionally."
        ],
        [
            "Right, so in this case, what it would mean to say that the random variables respect the graph structure.",
            "One way of saying that is that if we conditioned on the variables in this cut set, if you knew those variables, then the variables here are conditionally independent of the variables there.",
            "Given this cut set.",
            "Right, so that's that's what's called the Markov properties of the graph, and you can see it's just a natural generalization of the Markov chain.",
            "Oops, sorry bout that."
        ],
        [
            "OK, so that's one characterization, right?",
            "So we look back here.",
            "I've introduced two graph theoretic properties.",
            "One was the property of a vertex cut set.",
            "And what we've seen is that vertex cut sets give us conditional independence properties.",
            "Right, so we've seen that the statement we want is that XA and is conditionally independent of X be given access, right?",
            "So A&B were the two disconnected parts.",
            "So that's sort of one way in which we could constrain our distribution.",
            "The other way is in terms of cliques, right?",
            "So cliques are fully connected subsets of variables.",
            "And the way we constraint with cliques is that we say.",
            "We would like a probability distribution.",
            "Remember, in general this could have.",
            "Many, many degrees of freedom.",
            "But what we say now is we say that you're only allowed to build this probability distribution, so it's a function over N variables and it should be built in such a way that you're only allowed to take a product over cliques and every clique.",
            "You can put a function on that clique.",
            "And that function is a function.",
            "It's a local function that depends only on the variables in that clique.",
            "Right, these are sometimes called compatibility functions.",
            "OK, so if we go back to this this Markov chain.",
            "Alright, So what are the maximal cliques for this guy?",
            "Right, the edges are the maximal cliques.",
            "There's nothing else.",
            "So what this would say for this Markov chain?",
            "It would say that you've got a distribution over 4 variables.",
            "And it would say that it should factorize this Z that I'm writing is just a normalization term.",
            "It's to make the probability distribution normalize.",
            "It would say this should factorize in this way.",
            "Right so we gotta click on one, two gotta click on 2 three.",
            "And you've gotta click on 3 four.",
            "Right, so it's saying that you can sort of choose three functions here.",
            "I've done it, one for every maximal clique, and the functions depend only on two variables.",
            "The whole distribution depends on four, but it's it's a product of pairwise terms.",
            "Right, so suppose these variables were binary.",
            "We argued before that if I didn't have any structure.",
            "I would need something like 16 numbers here.",
            "Right with no structure, I'd need two to four.",
            "How many numbers would I need for these guys?",
            "You need at most 12 right?",
            "'cause the function on two binary variables.",
            "Each pair needs at most 4 numbers.",
            "Three is actually enough if you think about it.",
            "So in fact you need at most 9 numbers here, so 16 to 9.",
            "That doesn't seem like a big deal, but if it hadn't been for if I could draw, you know 100, we would have gone from 2 to 300 down to something like 4 * 100.",
            "So you go from 2 to 100 more than the number of atoms in the universe to roughly 400 numbers.",
            "And that's quite reasonable, right?",
            "If you think about if you were trying to fit one of these models today to 400 parameters is not unreasonable, whereas 2 to 100.",
            "Well, datasets are big these days, but they're not that big, right?",
            "So that's very important.",
            "That kind of compression in terms of the parameterisation of the distribution.",
            "Is anyone bothered by this factorization?",
            "Is there an inconsistency between what I wrote and what stated up there?",
            "But here I was only using maximal cliques, right?",
            "There I sort of said it's overall cliques, right?",
            "So the additional clicks that we left over the nodes.",
            "The nodes are cliques.",
            "But is it wrong what I've written here?",
            "Yes, absorb you're a man of few words but correct words, which is good, alright?",
            "So we could always absorb these things so we haven't said this is unique.",
            "If you sort of strip parameters out of it, you can get unique factorizations, but in the way we've written it, these factorizations aren't unique.",
            "But that's not what's actually important right now.",
            "What's important is that the factorization is very compact.",
            "OK, so.",
            "I'm an important result in this area that I'll come back to later.",
            "Actually says you might be wondering.",
            "OK, so high level just recapping.",
            "We said graphs plus probability.",
            "We said, OK, I can look at vertex cut sets and I can look at conditional independence, or I can look at cliques and I can look at factorization.",
            "Now you might say, well, why did I introduce both of them?",
            "Well, introduce both, because they're actually equivalent.",
            "This is a theorem known as the Hammersley Clifford theorem.",
            "It doesn't matter which way you go if you start with a fixed graph, and you say that you want it to factorize across the cliques, that's the same as you want, as saying that all these conditional independent statements hold.",
            "So the Markov chain is a particular example of it, right the Markov chain.",
            "You know when you build Markov chains, you don't actually typically write them like this.",
            "You typically write them as an initial distribution.",
            "Is a conditional distribution X2 given X one X3 given X2, but that's a special case of this kind of factorization, right?",
            "So that's one way to understand Markov chains and the other way is in terms of the conditioning on present breaks the graph into 2.",
            "Right, so for Markov chains we can sort of see that, which doesn't matter where we start, we end up at the same answer, but Hammersley Clifford says, actually, for a general graph that that's also true, and that's not obvious.",
            "One Direction is not hard.",
            "I'll do the not hard direction with you in a minute.",
            "The other direction is actually quite difficult.",
            "Not quite difficult, but it needs some work.",
            "OK, so I'll come back to the."
        ],
        [
            "Minute let me just start giving you some examples.",
            "This is an example that we've I've already just mentioned the Markov chain.",
            "What I'm doing here is actually amending it to make it what's known as a hidden Markov model.",
            "Alright, so hidden Markov model you have both these variables X that are evolving overtime and you also have some variables hanging off the X is these are wise.",
            "These are shaded in because that's typically what you observe.",
            "Right, so actually the biggest success of Markov models are one of the earlier big successes in automatic speech recognition, which I personally don't like because it's part of the reason that you know whenever you call a phone number for a company, you get a computer.",
            "These days you don't get customer service right, but speech recognition, speech models, way speech involves at least humans can be modeled very naturally as a Markov process.",
            "Right, there's sort of a sequential dependence in the way that people are forming phonemes, and the problem of speech recognition is exactly it's well modeled by exactly this graph.",
            "Of course, along graph.",
            "So these are the sort of hidden phonemes the sort of units of speech that you're trying to infer, and the shaded circles.",
            "Those are other random variables.",
            "That's what you observe.",
            "So if you're speaking over the phone, you're getting some sort of noisy version of what the person is really saying, or things like you know people might not be fluent speakers.",
            "If I speak French, there would be some.",
            "Certain amount of noise should we say for the speech recognition device and so on.",
            "Right, so it's it's an instance of this that what the computer observes is this noisy instance.",
            "What is trying to infer as the actual phonemes, and then it would parse them to get words and what is exploiting is that there's a lot of temporal dependence in the sequence of words.",
            "Phonemes at all levels of language.",
            "There's tons of temporal dependence, so really the success of automatic speech recognition is an instance of this, and it's an instance of some algorithms that will talk about tomorrow.",
            "Some of these message passing algorithms.",
            "Um?",
            "So Markov models are great, but.",
            "There's many things you'd like to do in practice that are more complicated.",
            "What various people have done an interesting sort of extension of this speech recognition is if you have video that has both audio tracks, an it has speaking and let's assume that multiple people are speaking an hence multiples people's lips are moving, or maybe it's slightly different times.",
            "If you wanted to solve solve different names for it, but the cocktail party problem right?",
            "You have many people speaking.",
            "And you'd like to separate the audio track into multiple tracks to get each person's voice.",
            "One way you can do that better is if you have video and then you can somehow use the synchrony between my lips, moving, informing certain things and what's being said on the audio.",
            "So now what you have is you have two temporal models, you have two Markov chains.",
            "You have the video evolving overtime and you have the audio evolving overtime, so you'd like to model both the temporal dependencies.",
            "You'd also like to synchronize.",
            "You'd like to synchronize across streams, and you might actually have multiple cameras if you're lucky, so you might have camera 1, camera 2 audio one, and you have a whole coupled set of Markov chains, so this is a much richer model and we're going to see that that algorithmically this poses significant challenges that this model does not.",
            "The reason it poses challenges is because this model is what's known as a tree.",
            "It doesn't have cycles, right?",
            "There's no way of following a cycle around, but this model has many cycles, right?",
            "I can walk around a cycle like that.",
            "There's a lot of cycles in this MoD.",
            "It's not a tree, and we'll see for that reason that when it comes to.",
            "Solving computational inference problems.",
            "This model is much harder than that one.",
            "Yes.",
            "Yes.",
            "Yes.",
            "So yes, I like you you you catch me so true I'm only going to talk about undirected, but some of my pictures have directed have arrows on them for what we're going to do later.",
            "The Arrows aren't going to play any role later when I talk about inference, the first thing that you would normally do is what's called moralized.",
            "The graph more lies is a cool name.",
            "If you have a directed graph.",
            "If you have two nodes that come into the same node, it's as if you have two people.",
            "Who have had a baby and they're not married.",
            "So if you're in certain parts of the United States, that's immoral.",
            "So therefore you moralize the graph by adding an edge between these parents and then you drop the arrows.",
            "So you're correct that directed graphs are very interesting.",
            "I don't mean to say that they're not, but for what we're going to talk about in the rest, I'm going to think about undirected.",
            "So you should basically just ignore the arrows here.",
            "You're also correct that Markov chains, if you think about time series, there's a natural sense of moving forward versus backward, but again, for many of the sort of core problems that will solve that difference is not actually essential, it is from the modeling interpretation point of view, but from the computational point of view, it actually turns out not to be.",
            "That's a good question.",
            "Anything else?",
            "Skip.",
            "Yes.",
            "It depends how you interpret it.",
            "The way the way this graph should be interpreted is that the colored nodes have what you've observed, which is what you should think is the hidden stuff is, you're trying to infer.",
            "Let's say the identity of the speakers you're trying to infer the unstream.",
            "The unmixed audio stream, and also unmixed from the video, but you don't observe that.",
            "The audio might have, you know this.",
            "We could just do with audio.",
            "You might have three audio streams, as in speakers who are evolving independently, but you don't observe them.",
            "You have observed their mixture and that's what the colored nodes would be similar with audio plus video.",
            "Anything else?",
            "Yep.",
            "Any other distribution other than.",
            "Um?",
            "With certain exceptions, and this is probably related to what are nodes spoke to you about or noticed?",
            "Set things like particle filtering.",
            "So tomorrow we're going to talk about message passing.",
            "These are algorithms that are, for instance, one thing you might like to do from since if you're doing speech recognition, is you'd like to predict.",
            "Perhaps you might like to predict what the next utterance would be, or actually maybe a more modern examples when you're typing on your iPad or your phone, right?",
            "It's actually again running a Markov chain.",
            "It's trying to predict, based on language.",
            "Temporal dependence is what character you're going to strike next based on.",
            "Frequency and other things.",
            "So if you wanted to predict the next character, for instance, you'd like to solve an inference problem you'd like to compute a marginal over that.",
            "We're going to talk about algorithms that are tractable for discrete and Gaussian.",
            "There's essentially closed form if you have continuous but non Gaussian, then you have to resort to things like particle filtering or other kinds of MCMC or sampling based methods.",
            "Those problems are much harder.",
            "So yeah, we will talk mainly about discrete and Gaussian, but the talk of Arno would be connected to the more general problems.",
            "Did he talk about non parametric belief propagation or did he talk exclusively about time series?",
            "Say no, that was an either or so we talked about time series.",
            "I would imagine right, but much of what he said can be done on more general graphs and there has been a fair bit of work on that.",
            "If you search for non parametric belief propagation, you'll find some interesting hits."
        ],
        [
            "OK, so social network analysis this is another one quite topical, so we've got a bunch of individuals, individuals or engaging in some kind of behavior and you'd like to model that.",
            "So sort of nice now we've got massive datasets.",
            "We've got things like Facebook.",
            "We've got Amazon collecting huge amounts of data on.",
            "You know what people like and what they don't like, so there's very large amounts of data to model.",
            "Let me just give you a simple demonstration of this.",
            "This actually will illustrate what's known as the eising model.",
            "So just to be concrete.",
            "So this is not what the eising model was used for, but one thing you could think about using it for is.",
            "You could think about it for modeling how epidemics spread.",
            "It's.",
            "So the using model is an instance of a graphical model.",
            "It's a graphical model in which.",
            "You've got a bunch of variables that are just.",
            "Let's make them minus 1 + 1.",
            "They're binary.",
            "You can think about them as indicator variables for the state of something.",
            "So if we wanted to model diseases.",
            "Then we could think about XI being plus one.",
            "That means that individual I.",
            "So that's the person at node I is infected.",
            "And we could think of minus one if I is healthy.",
            "Alright, so we're going to a pattern of plus or minus ones and it's interesting to figure out, for instance, how infectious diseases, right?",
            "You sort of expect.",
            "Well with most diseases, that spatial proximity, if you're close to someone who's infected, then the likelihood that you get infected goes up.",
            "For instance, every time I travel in a plane, I worry about this right?",
            "You're sitting next to someone who happens to have a horrible cough.",
            "Then after 10 hours of that.",
            "The likelihood of you having a horrible cough is non negligible.",
            "Right, so if we were modeling this, we'd sort of think about.",
            "We've got some graph.",
            "If you wanted to model in a plane, we'd have a plane like this.",
            "Um, no, I don't travel first class.",
            "I just can't draw enough of these nodes, but you'd have nodes like this and you sort of.",
            "You could imagine a grid that would be a simplest simple instance of it, and you can imagine somebody in the middle of that that's infected, so we would have sitting here.",
            "We would have a function.",
            "One of these compatibility functions.",
            "Right, so how might this compatibility function look?",
            "Well, there's different ways we might parameterise it, but one way we could is we could give every edge of weight.",
            "Let's call that Omega and we could parameterise it like this.",
            "This would be a simple parameterisation.",
            "Right, So what this means is.",
            "This corner would be let's say plus 1 + 1 and the bottom corner would be minus 1 -- 1, right?",
            "So I'm just listing the four numbers that you can possibly get out of a pair of binary variables.",
            "Right so.",
            "The Omega, of course, would have a physical meaning if we're talking about infectious diseases, what do you expect about Omega?",
            "It would be 1.",
            "Be large andwich sign.",
            "Right, so it's probably going to be large and positive, right?",
            "Because if Omega is large positive than the two diagonals, the sort of both infected or both healthy are going to be very.",
            "You're going to get a lot of weight on those in your factorization of the probability distribution.",
            "Right, so likely you would have.",
            "If a mega was zero, you would sort of have no infection and beaten independence model.",
            "Anifa Mega was less than one.",
            "It could arise, but maybe not in disease modeling, but you have sort of an anti infectious model.",
            "If you're diseased, then I'm likely to be healthy.",
            "It's not likely for diseases, but you can also use this model to model the way that politicians vote, right?",
            "You could say XYZ plus one if that person votes yes and minus one if they vote no.",
            "And in that case a negative weight makes a lot of sense.",
            "It just means that you have two people who disagree.",
            "A lot of the time.",
            "OK, so let's let's do a little simulation just to look at what happens.",
            "Um, let me see if I can get this right.",
            "OK, So what I'm showing you here.",
            "A bit of Matlab code, but the important thing is this variable.",
            "Here it's called sig E. It's it's the Omega term, so I'm setting that to be quite small.",
            "It's going to .1.",
            "So that's.",
            ".1 it's positive.",
            "And we could run Gibbs sampling, for instance, to sample from this model, right?",
            "And I'm sampling it from a grid which has 32 nodes by 32 nodes.",
            "So you're sort of seeing a spatial pattern of plus and minus ones.",
            "Right, so if it's .1 then there's sort of some infectious nature to the disease, but it's not highly infectious.",
            "Whereas if I increase that infection probability to something like .5 four point 4, let's say do the same thing.",
            "Now you can see this.",
            "Qualitatively it's changing.",
            "It's getting much block here.",
            "You're getting bigger pattern splotches of infected people.",
            "Bigger splotches of uninfected people, and if we change it a lot, if we send it to .8, let's say, then we essentially well.",
            "In that case, everybody except a few isolated individuals are infected.",
            "So just with a single parameter, we're sort of getting a whole range.",
            "There's a different sample now.",
            "Everyone in this side basically got infected, and everyone on the top was healthy.",
            "If we take black as being maybe black should be infected like the black plague.",
            "Right, so you sort of get a range of behavior if you said it negative again, that wouldn't make so much sense for diseases.",
            "So actually what kind of pattern do you expect?",
            "If I have a negative wait here?",
            "Right to be checkerboard.",
            "Because the grid is bipartite, it sort of has two pieces.",
            "Right, so it's not a perfect checkerboard, but it's getting there, right?",
            "If I'm healthy, that makes it unlikely.",
            "Makes it more likely that you're infected, and vice versa.",
            "OK, so that's a simple model.",
            "And what we are running, there are no spoke about MCMC algorithms.",
            "Do you speak about Gibbs sampling?",
            "For instance, right?",
            "So we're running just essentially a form of Gibbs sampling on this graph.",
            "This is a reasonably small problem, just something like 1000 nodes.",
            "So Gibbs sampling is OK, but on larger problems actually it does become hard to even sample from these.",
            "So I'll skip this example."
        ],
        [
            "Um?",
            "Let me just talk about one other application and then I'm going to come back to Hammersley Clifford.",
            "How many people have heard about error correction coding?",
            "Things like Turbo codes?",
            "Interesting background.",
            "How many people are actually in communications here?",
            "Very few, but you sort of heard about it.",
            "So error control codes just means they're all over there all in your computer in your laptops and your hard drive.",
            "Your cell phones, all of them use error control codes.",
            "Even the grocery store.",
            "So whenever you scan something like a barcode, right?",
            "If you use a scanner, if the scanner is misaligned, you might miss read the digits of the barcode.",
            "So what barcodes actually have in them?",
            "They have some extra parity checks.",
            "They have some extra numbers that say this first, second, third number should sum up to something.",
            "Maybe that's zero and MoD 2.",
            "That would be a simple example of a parity check code, right?",
            "So it means that when the scanner is misaligned, even if it gets a few digits wrong, those parity checks allow it to correct or fix those errors.",
            "Now it turns out that a lot of the best codes, the things that are actually used in practice."
        ],
        [
            "These are based on graphs.",
            "Here I'm showing you a slightly different kind of graphical model, but something that you'll also see in the literature.",
            "Something called a factor graph.",
            "Right, so in a factor graph, you have nodes.",
            "These are what we seen before.",
            "These represent variables.",
            "In the case we're talking about, these would represent things that are being transmitted.",
            "For instance, when your cell phone works, uses radio waves, but it transmits essentially a sequence encoded form a sequence of zeros and ones.",
            "So this is what's being transmitted, or a noisy version of this is being transmitted.",
            "And what these boxes represent, these boxes represent parity checks, so this box is saying I'm going to look at 1535 and seven, and whatever you send, I wanted to be the case that adds up to zero and MoD two there should be an even number of ones.",
            "Right, so how many?",
            "So a code word is something that satisfies all of these parity checks, right?",
            "So it's a sequence of zeros and ones of length 7.",
            "In this case, such that the 1st, third, 5th and 7th.",
            "If you add them up is zero and MoD 2, and so on for the other 3 ones.",
            "So how many code words?",
            "How many sort of valid 01 sequences can you send in this case?",
            "Alright, there's two to the seven possible sequences, but not all of them are valid, right?",
            "This guy is not valid 001.",
            "One is not valid.",
            "Why is that?",
            "He must violate something.",
            "Yeah, so he would violate this parity check because only seven is 1 and 5, three and one or zero.",
            "So he has an even number of 1 so he would not be valid.",
            "Right, so the total number of codewords is 2 to the four.",
            "You started with two to seven.",
            "You sort of had 7 degrees of freedom, and you lossed one for every parity check.",
            "So 7 -- 3 you had three checks.",
            "You end up with four, so in this case there's 16 codewords total that could be sent in a much larger space of two to 7.",
            "Now, in practice, people don't do this just with seven variables.",
            "They do it with something like 10,000 or 100,000 variables.",
            "And what really has revolutionized and what's implemented in many of your devices is the sum product algorithm.",
            "So how many people have heard of the sum product algorithm?",
            "OK, you're OK.",
            "I understand we're going to talk about it tomorrow.",
            "It's a message passing algorithm.",
            "What's important about it?",
            "Message passing means that it does local operations at the nodes of the graph.",
            "It sends things along the edges.",
            "It's an extremely efficient algorithm, and it's very parallelizable.",
            "So actually many of your cell phones are running a version of some product right now, and it's very, very fast, because obviously if you're speaking you want to decode the message in real time, you don't want to speak and then wait.",
            "For a minute or so, until the message is decoded, it has to be done now.",
            "So this message passing algorithm will talk about.",
            "Let me just demonstrate it in practice for you on one of these codes.",
            "So what I'm showing you here is.",
            "So it's a little demonstration of how you might clean up an image, so I'm showing you a code that has roughly 10,000 bits, so it means it's a graphical model that has roughly 10,000 of those circular variables.",
            "And the way I've built it, it's got 5000 of those square variables, those factor nodes, or those parity checks.",
            "So it's a big graph.",
            "Anne, what you observe is you observe this image which has been corrupted by some special noise.",
            "And what we're going to do is we're going to use the sum product algorithm.",
            "We're going to run it on the underlying graph.",
            "We'll see tomorrow what it does.",
            "And you'll see that it cleans up the image.",
            "Right, so it's running iterations and it's done.",
            "So 14 iterations there on 10,000.",
            "It essentially recovered the.",
            "The code word in this case, which was the clean image.",
            "It recovered it perfectly.",
            "Yep.",
            "No, no trigraphs.",
            "That's the interesting thing.",
            "So you'd like it to be a tree graph, because then sum product algorithm would be exact.",
            "Will see this tomorrow, but it turns out that if you use trees to build codes, they're not good codes.",
            "There are sort of bad codes in a sense that can be made precise, so all good codes things like Turbo codes.",
            "What this is what I'm showing you is called a low density parity check code.",
            "All good codes have cycles, so this is one of the reasons why belief propagation or some product on graphs with cycles.",
            "This is probably.",
            "The single largest success story of it in any application, right?",
            "Because it's it's in all of your devices as we speak, so it's key that it has cycles.",
            "Um?",
            "Any other questions?",
            "Let's just look at this code quickly.",
            "Let me just explain what I was doing.",
            "So what this means was when I was generating the noisy version, I was flipping six.",
            "I was on average.",
            "I was flipping a coin with probability .06 of making it a bad pixel, so I was corrupting only 6%.",
            "6% of the pixels, so you might ask, well, you made it too easy so we can try a slightly harder problem.",
            "I'll corrupt about 7 1/2%.",
            "I don't know what's going to happen because it's random.",
            "It may work, it may not.",
            "This is sort of near the threshold of what this code can do.",
            "Um?",
            "OK, so that worked if I ran it again it might not work at that level.",
            "If I go too high, if I crop something like 10 or 12%, let's do 10% it will it will fail.",
            "I think almost assuredly right, so now it's even more corrupted and will see that the algorithm because it's on a graph with cycles.",
            "Yeah, you see that it's sort of partly cleaning up a bit of the image, but now it's just kind of oscillating.",
            "It's bouncing back and forth, so the graph is not on a tree, so we're going to see tomorrow.",
            "It doesn't always converge, and in this case the problem is too hard for it failed to decode properly.",
            "But that's not surprising, because for those of you know about Shannon theory, the Shannon limit for this problem.",
            "No method, whatever you do, can do better than 11%.",
            "So if you're at 10%, this code, about 8%, that's actually quite good, it's reasonable.",
            "OK, so any other questions?",
            "Yep.",
            "So I took the image and I padded it with.",
            "Yeah, essentially I did a linear algebraic trick.",
            "The images some sequence of zeros and ones, but by shifting all the words by the image I can make it the all zeros vector and then it's a valid code word.",
            "I can show you the code afterwards, it's just a little trick, so I took the image.",
            "You're correct, I made it a code word that's tricky and then I added some redundancy bits in the check and what we were running with some product.",
            "Right, so it's doing about.",
            "It's not optimized, but it's doing 10 iterations on something like 15,000 nodes and you can see it's reasonably fast.",
            "People do use this algorithm in practice."
        ],
        [
            "OK, so."
        ],
        [
            "Those are some applications I just happen to cherry pick those.",
            "There's many others, right?",
            "Natural language processing things like parts of speech tagging, graphical models are used all the time.",
            "Statistical image processing, computer vision, things like image denoising, image segmentation, image disparity, computation, people use graphs all the time.",
            "There they actually use grids like this often because those are good for modeling spatial things.",
            "Some other examples phylogenetic's computational biology, genomics.",
            "These all use graphical models as well.",
            "So there's many applications and the applications depend both on somehow the simplicity that we get from the graph structure and also these algorithms that I'll talk about tomorrow.",
            "What I'd like to do in this last part is.",
            "I'd like to come back to the Hammersley Clifford theorem.",
            "I'd like to drill down into that a little bit more.",
            "We've sort of been high level, but I'd like to understand.",
            "Why it is the case that if you have something that factorizes like this, it must satisfy all these conditional independence properties.",
            "So let's let's start drilling down a little bit to understand that.",
            "Right, so if we remember the statement, the statement says if you start with the graph, whatever undirected graph you want, and you build a distribution that factorizes over the cliques of the graph.",
            "Then that's the same as if you started with the graph and said that you had all these Markov properties holding.",
            "So let's let's work through that One Direction of that argument.",
            "Any questions before I sort of segue into that.",
            "Hammersley and Clifford so they were, say probabilists at Oxford.",
            "The original paper is actually unpublished.",
            "I think you can get your hands on a scan version of it, but yeah, it's never been published, so they were probabilists, and they were sort of interested in essentially generalizations, Markov processes on chains play such an important role in probability, and they were interested in these kinds of generalizations to graphs.",
            "So I think one of the first published proofs was by Julian Besag.",
            "Unfortunately passed away I think last year, but he did a lot of the pioneering work early work in the 70s and graphical models, among other things.",
            "He was very good at naming his papers.",
            "He had a great paper called the statistical analysis of Dirty Pictures.",
            "By which he just meant image denoising exactly what I did with codes that you have pictures that are corrupted and he was going to be.",
            "Noise them with the graphical model so.",
            "It did, yeah, I think the British kind of have a naughty sense of humor.",
            "They like those kinds of jokes.",
            "Americans are just more crude, you know.",
            "More direct about it.",
            "Other nice proof, I think was a student Grimmett in 70.",
            "Three had a very nice proof.",
            "I believe he was a student of one of the two I don't remember, but um.",
            "So let's let's do One Direction.",
            "Anything else?",
            "That's a good question.",
            "I don't know that to be honest.",
            "I always wonder.",
            "I feel it has to be an American.",
            "It's not a Scandinavian because Scandinavians are not scandalized by unmarried parents having children so.",
            "Maybe Perla, but I don't want to cast aspersions so.",
            "Someone could search it and find out, I'm sure so.",
            "OK, so let's let me just work through just because I think it's useful to do some of the technical work.",
            "So let's show the following.",
            "Let's show that the factorization property implies the Markov property on any graph.",
            "Alright, so I'm not writing the statement out, but what we're trying to show is One Direction of this.",
            "So what I'm going to assume is that you've got a distribution that factorizes in the way we've written there.",
            "Right, so I'm assuming that I've got my nice factorization, so I've got a bunch of N variables and it breaks into a product of terms over the cliques like I've written up there.",
            "So I'll use a script C for the clique set an regular C for the clique.",
            "And So what I want to show is.",
            "I'd like to show that if I take a vertex cut set, let's say S an, I splits the graph into two parts.",
            "A&B right, so I want to take a vertex cut set.",
            "It splits the graph into two subsets, A&B.",
            "Those are separate, and so I've sort of got three disjoint subsets AB&S.",
            "Let me flip back.",
            "We can just look at this picture right this pic."
        ],
        [
            "Sure is sort of Canonical.",
            "You've got three disjoint subsets, AB&S and so on.",
            "The right here S splits the graph into two parts, and So what?",
            "I'd like to show is I'd like to show that the conditional independence property holds.",
            "OK, so to do that we should go back to the definition of conditional independence.",
            "Right, So what is conditional independence mean?",
            "It means that while there's different definitions, but one way of thinking about it is that means that X of AXB we condition that on X of S then that should break into two pieces that should be X of a given X of S&X of B given access.",
            "Right, that's what we'd like to show.",
            "OK, so.",
            "To do that, what I'm going to do is I'm going to take the set of cliques right?",
            "What we have to work with is we have this partitioning of the clique set.",
            "Well, we have this factorization of the clique set.",
            "So what I'm going to do is I'm going to use these subsets of vertices AB&S to partition the cliques.",
            "Alright, so I'm going to define three sets of cliques.",
            "Right, so the first set of cliques are cliques that have some intersection with a right?",
            "So a is some subset, C is a clique, it's some other subset of nodes, and I want to look at those that intersect with a in a nontrivial way.",
            "Same thing for script C of BI.",
            "Want to look at cliques that intersect with being a nontrivial way.",
            "Not equal to write.",
            "I want that not to be empty and the last set is.",
            "I want to look at cliques that belonged to the clique set an are subsets of the vertex cut set.",
            "It's about three sets of cliques there.",
            "Right, so in the graph that I've shown there, if I use the numberings on the left for instance, what is C of S?",
            "See, this is the set of cliques that are contained within the separator.",
            "Set the vertex cut set, so in that case only four five.",
            "That's a clique, and it's of course contained.",
            "I guess if we also four and five separately, those are not maximal cliques.",
            "Those are contained within S as well.",
            "OK, So what can we say about these three sets?",
            "So they cover all the cliques in the graph Y.",
            "Is that the claim is actually that they cover every clique is contained in one of those sets an exactly 1, so they form a disjoint cover of the clique set.",
            "Now, that's not always true.",
            "We have to use some property of S. We know something about S. That is true.",
            "Right, so this I'm going to sort of leave it for you to think about on your own a little bit, but this essentially uses the fact that S is a vertex cut set or separator set.",
            "Right, it means for one we can't have any direct edges between A&B.",
            "Right, so you can work through a little argument to show that these that every clique is contained in exactly one of these sets, and the sets are disjoint.",
            "Right, so that's useful to us.",
            "Why's that useful?",
            "It's useful because we have a factorization up here that's over all the cliques of the graph, and we can now split it into three separate factorizations, one for each of these terms, right?",
            "So let's do that.",
            "Speak dirty.",
            "Right, so if we use that disjoint partition, then what we can conclude is that our joint distribution is on my end variables in the graph.",
            "I can break it into three terms.",
            "Well, I have the normalization constant that's just to make things sum to one.",
            "I've got a product overall clicks that are in the first set.",
            "A product of cliques in the second subset indexed by B.",
            "In a product of cliques in the third set indexed by S. Right, that's using the fact that I have a disjoint union of my cliques.",
            "So let me give these things some names.",
            "My letters got one.",
            "What's mixed up?",
            "Thank you, yes I was confused.",
            "OK so sorry bout that.",
            "That should be 5 CXC so this is the product over the set of cliques indexed by S. What can I say about this thing which which variables can it depend on?",
            "Right, it can only depend on the variables in S, so you can sort of think about this as some.",
            "That's because every clique in this set is contained within S, so it can only depend on things in S, so we can think about this as some kind of super function.",
            "Um, are there any?",
            "Yeah, I'm very good at destroying pens.",
            "How many PHD's does it take to open up?",
            "OK, so this guy is some big function.",
            "Let's call him what is that?",
            "That's a Phi Phi X of S right?",
            "He depends only on the variables of an excess.",
            "What can we say about this guy here?",
            "The term indexed by a.",
            "Right, he can only depend on things in A&S.",
            "Right, that's again, using part of the Cutset property right?",
            "There?",
            "Can't be any connection between A&B, so there's no way that a bee term will somehow pollute me and this guy can only depend on.",
            "I should get these different names.",
            "This is S. This is a S and this is fi BS.",
            "So that depends only on B&S.",
            "OK, so who can tell me why that's useful?",
            "So what we sort of done is we've just taken our factorization.",
            "We've collapsed it into three terms, one for a, one for B and one for the overlap the cut set.",
            "Right, so if we remember what we're trying to prove right, what we're trying to prove is this guy right here.",
            "We're trying to prove that when we condition on X of S that it becomes a product of two terms, so I won't write out the algebra.",
            "I'll leave that for you guys, but essentially the proof is done because what happens now is if you take this and you condition, you're going to fix this.",
            "This just becomes some constant that you don't care about.",
            "You fix some of these variables and fix these variables here, but after conditioning what you have is just a separate product of A and terms indexed by B, so you can see exactly the splitting of.",
            "After conditioning you get exactly the splitting that you wanted.",
            "So somehow that decomposition that was the key in this particular argument.",
            "And of course that decomposition that was using this claim that you should verify that we did in fact have a disjoint union that was using the fact that we had a vertex cut set.",
            "OK, so that's that sort of shows us One Direction that shows us the factorization property.",
            "Um?",
            "Implies Markov, so just summarizing.",
            "If we condition.",
            "Then what you'll see is that PXAXB if we condition on X of S. If I write proportional 2 and I'm just going to drop things that depend only on S because those conditioning those are fixed, I'm going to get something that's basically proportional to feess.",
            "XA Let's put bar because we're conditioning on it and I guess.",
            "And VXB access right?",
            "They'll be some constants floating around, but we don't care about those.",
            "That's enough to show us that this thing breaks into the product that we wanted.",
            "OK, so any questions about that?",
            "OK, so that actually was the easier direction as you might have thought.",
            "Probably just in the interest of time.",
            "I won't prove the other direction 'cause it would take a fair bit of work, but I can sort of tell you how it works.",
            "The other direction is a bit harder, so if you look at the assumption, the assumption of the theorem include that you have a strictly positive distribution.",
            "But this argument, I mean, you might worry a little bit about dividing by zero when I condition, but if you look carefully at this argument, it actually holds for any distribution it holds for distributions that might assign zero probability to some configurations.",
            "But the reverse step, if you have a Markov, if you have the Markov property, the factorization property doesn't always hold, and less the distribution is strictly positive.",
            "So if you look in the book by Lauritsen, it's a book called Graphical Models.",
            "You can find all these sort of weird kind of counterexamples.",
            "What the proof uses is it uses something called the Mobius inversion formula.",
            "Alright, so this is what I said.",
            "It certainly needs this assumption that P of X is bigger than 0.",
            "It's it's a nice proof.",
            "It's kind of combinatorial.",
            "It uses something called Mobius inversion.",
            "And, um, I won't go into details on it here.",
            "Um?",
            "You can, you can read it.",
            "The proof of this.",
            "So in the directory that I set up for this course on the first page of your slides, there's a web address.",
            "I left a set of introductory lectures that actually will go will review this proof that I just did, but also state the Mobius inversion and go for the backwards direction too.",
            "So if you're interested, you can have a look at that if you haven't ever seen it, I think it's worth looking at just once, because well, historically it's interesting, and it's a nice argument.",
            "It's nice to understand.",
            "Why these two things are equivalent?",
            "OK, so any questions."
        ],
        [
            "OK, So what I'd like to start talking about now and what we'll spend a lot of tomorrow morning talking about is.",
            "More the kinds of computational questions that arise when you apply a graphical model.",
            "Right, and we've already seen some instances of the kinds of computational problems that you'd like to solve.",
            "Um, one of them that you'd like to solve is basically a.",
            "It's sort of simply stated.",
            "It's basically just a giant summation or a giant integral.",
            "Alright, I haven't said much so far about what this mysterious Z quantity IS0Z, right, but if you sort of, I said that it's the thing that makes the distribution normalize.",
            "And so if you had a discrete set of variables, that would be a sum.",
            "If you had continuous ones, it would be an integral, it's just a constant that you get by summing or integrating over all configurations.",
            "This product of all the terms on your cliques.",
            "So it seems kind of boring, but in any learning problem that thing will be something like the log likelihood of your data.",
            "If you take the log of it, it would be the log likelihood.",
            "So things like maximum likelihood or expectation maximization.",
            "All of these things care very much.",
            "They are only efficient if you are able to compute this quantity quickly.",
            "Alright, so it looks kind of innocuous, but um, it's actually very important.",
            "Right, so if we think about the complexity even in the discrete case, you can see it's exponential, right?",
            "Because even if you just had two states, a binary variable at every node, you have two to the end summation.",
            "So that thing if you think about it naively, is very hard.",
            "What we're going to see is that if you're clever about it, if you exploit the graph structure when you have trees, then you can do this exactly.",
            "You can do it by essentially.",
            "It's essentially a form of dynamic programming.",
            "It's known as the Max product or the sum product algorithm, and that's what we'll see when you have cycles.",
            "It's actually quite hard.",
            "There's many complexity theoretic results in the literature that tell you that it's very unlikely that people are going to be able to compute these sums in general exactly for general problems.",
            "OK, the other problem that you might be interested in is also a summation or an integration problem.",
            "Let me just go back to the examples that we had."
        ],
        [
            "Um?",
            "Right, so when we were talking about speech recognition or we were talking about predicting typewriter sequences on your on your laptop or your iPad, what you'd like to do is you maybe have observed all the keystrokes up to some time T and you'd like to predict the next time T + 1.",
            "So what you'd like to do is marginalized efficiently.",
            "You'd like to sum or integrate over all these variables to try and predict what the marginal distribution is at that node, right?",
            "So it's again a kind of summation problem.",
            "It's a summation or an integration problem.",
            "What's another problem that we might want to solve in this case?",
            "How many people know about sequence alignment problems in computational biology?",
            "OK, So what kind of problem might you want to solve in that area?",
            "Sorry.",
            "You could do that, but maybe let's let's be more concrete, you might like to do a sequence alignment problem.",
            "You might like to search efficiently overall alignments and match alignments you might like to find the alignment that maximizes the probability of being matched under some probability model, so problems like sequence alignment.",
            "Many of them you can formulate them as you're essentially building a graphical model over sequences.",
            "Ann, you're penalizing certain transitions.",
            "You're saying some transitions are more likely than others, and what you'd like to do.",
            "It's kind of like a batch computation.",
            "It's not an online computation you'd like to search overall sequences over the entire length of the sequence you'd like to search all sequences and maximize, find the sequence that has the highest probability of match of matching the observed data.",
            "Right, so that's like this problem.",
            "I defined it's instead."
        ],
        [
            "The summation that's a problem down here right?",
            "In this problem, we wanted to sum over all the variables.",
            "In this problem, we want to maximize over all of the variables, so there's sort of two different types of problems.",
            "Those that involve summations and those that involve maximization's.",
            "And it sort of depends on your application.",
            "Sometimes you might want to compute the likelihood of data, or you might want to compute a marginal distribution to make predictions.",
            "In other times, you might want to search the whole space to find the best alignment, or for computer vision you might want to search over all pairs of images to find the one that best matches the depth between the two.",
            "So those are more search problems when you have a maximization.",
            "OK, so both sets of problems are hard in general.",
            "This one is hard because you have again an exponential number of summations, or you have a very high dimensional integral.",
            "Right integrals are very easy if they're in one dimension or two, but they rapidly become very hard if you have higher dimensional ones.",
            "Right, so these are generalizations of the problems that I think our node would have spoken about.",
            "He's probably spoke about the problem of filtering that you have a sequence, something like a timeseries evolving overtime and you want to compute or approximate the marginal distribution given all the data up to time T. That would be this problem here.",
            "This marginalization problem if you were on a Markov chain.",
            "We're going to be interested in it for more general graphs."
        ],
        [
            "If I go back to this example so that that demo that I showed you what was I doing or what was the algorithm approximately doing?",
            "Really, there were extra nodes here.",
            "You were observing noisy versions, right?",
            "You had the clean image that was sitting there.",
            "You didn't see that.",
            "You saw a noisy image, so there'd be some shaded nodes there that show what you observe and what that algorithm was doing was it was approximately summing very quickly.",
            "Overall, code words that are possible, and for every node it was finding out is it more likely to be a one, or to be a 0.",
            "So it was computing the marginal at every node, and if it was more likely to be a one then it declared A1 and vice versa.",
            "So that's what that algorithm was doing.",
            "It wasn't doing it exactly because as somebody mentioned, this graph actually has cycles, but if it had been a tree.",
            "The algorithm that will present will do it exactly and do it very efficiently."
        ],
        [
            "OK, so those are the problems we'd like to solve."
        ],
        [
            "Let me come back to this tomorrow."
        ],
        [
            "About variational methods.",
            "Let me just start talking about the Max product algorithm.",
            "OK so the Max product algorithm.",
            "How many people know about the Viterbi algorithm?",
            "OK, so the verb algorithm is a special case of the Max product algorithm.",
            "One reason graphical models are useful is because they essentially generalize many classical algorithms.",
            "There's a list of at least 10 to 15 algorithms that are special cases of what we're going to do now, so if you've heard of atterby, that's a special case.",
            "Forward.",
            "Backward is a special case.",
            "Alpha Beta is a special case.",
            "Common filtering is a special case.",
            "Fast Fourier Transform is a special case.",
            "You can essentially go down and name many influential algorithms there.",
            "All this a special case of what we're going to do now that's part of the utility of graphical models.",
            "They somehow allowed you see what the key ideas in all of these prob.",
            "Zara OK so Max product.",
            "What is it?",
            "It's an algorithm for trying to solve."
        ],
        [
            "Of problem #3 right?",
            "It's trying to solve this maximization problems we have.",
            "What we'd like to do is maximize over all the nodes of the graph.",
            "This product of terms.",
            "That's where it gets its name.",
            "You'll see in a minute because it has a Max Anna product and it the algorithm.",
            "All it does is play games with those two."
        ],
        [
            "Arms.",
            "OK, so let's let me do this concrete example.",
            "OK so Max product.",
            "Right, so the simple example I'm showing you, we've got three nodes, 1, two and three, so we got.",
            "Three variables, and so I know that I have a factorization over these three variables.",
            "I'm going to write it in a certain way because it's useful to me.",
            "Later, I'm going to write it as.",
            "Proportional to a product of three terms, one for every node.",
            "It should be an S sorry.",
            "And I'm going to have.",
            "In my case, I'm just going to have two edges.",
            "OK, So what I've done here is you can either write these things as compatibility functions as size, or if they're strictly positive, you could always write them as the exponential of some other function, which I'll call Theta.",
            "The reason I'm going to do that is it when we talk about approximate methods tomorrow it's more convenient, often to work with these thetas.",
            "If you know what an exponential family is, then it shows you that this is just an instance of an exponential family.",
            "A physicist would call this sort of the Gibbs representation of the distribution.",
            "For those of you who are have some physics background.",
            "OK, So what we want to do is we want to compute a maximum we want.",
            "We'd like to maximize over these three terms.",
            "Right, so if we think about it.",
            "Right?",
            "So I've got X one X2 and X3.",
            "We can sort of do this successively, maybe first all Max over X2 or first do it over X3.",
            "If I maximize over X3.",
            "What do we know about Max and product?",
            "Right so Max and products are interchangeable.",
            "You can sort of distribute Maxima over products and vice versa.",
            "So in this case, all I'm saying is you have a maximum over X3, But you can basically ignore any term, for instance PSI one of X1 has no dependence on X3, so you can push the maximization inside right?",
            "So all we're doing is we're using the fact that maximization distributes.",
            "So what I can do is I can just extract the terms that depend on PSI three.",
            "In this case they are.",
            "Or X3 so I would get a size 3X3 and also Sai 23X2X3 those terms depend on X3.",
            "And now the other terms that I have.",
            "This is proportional to.",
            "Other terms I have left or side 1X1 side 2X2.",
            "Anssi 12X1X2.",
            "Right, so that the key here, though, is these curly braces mean the Max is now only over a limited set of terms, right?",
            "It's only over 2 terms, one that sits at node 3 right there, and one that sits on the edge between 2:00 and 3:00.",
            "OK, so after I do this operation.",
            "What do I get?",
            "I get if you said a function of X1 and X2 it's.",
            "It is a function.",
            "It is a function of X1 and X2, but actually even simpler.",
            "It's only a function of X2 in this case.",
            "Right, because if there had been an edge between three and one, there would have been an extra term and it would have been a function of X1 and X2.",
            "But because there's no edge, this is only a function of X2.",
            "So the way you should think about this, you sort of think I put a number in there and I solve this maximization problem.",
            "I get a number I can do that for every value of X2 and I get a function of X2.",
            "Alright, so this function has a name.",
            "It's called a message and.",
            "You might call it the message that's passed from three to two.",
            "Right, because it's a function of X2.",
            "Right, So what we're going to think about when the algorithm works here?",
            "I'm just writing algebra, but when the algorithm is working the way it's going to be working is that X3 is going to be doing the that maximization that I just said.",
            "And it's going to be passing the result of its computation as a message from three to two.",
            "So compute and pass.",
            "That's why it's called message.",
            "Passing.",
            "Messages are just functions of random variables that are summarizing what X2 needs to know.",
            "OK, what should I maximize over next if I want to make my life easy?",
            "Someone said X one, so why is X1 better somehow?",
            "Well, it's better because of symmetry in this case, but in general it's there's a general algorithm that I'll talk about tomorrow elimination algorithm.",
            "It's sort of proceeds from the outer boundary of the graph, and it eliminates nodes by maximizing their, summing them, and it strips the graph down to a single node.",
            "So I could just as easily do this same operation.",
            "Let's use a different color.",
            "I could maximize this guy over X1.",
            "And what's nice now is that maximum passes into here.",
            "And I haven't ordered my terms properly.",
            "Let's erase this 'cause it doesn't depend on.",
            "X1 and put it outside.",
            "Right, so I can now do another maximization over X1.",
            "And now I have a message.",
            "This message is a function of X2 as well.",
            "And this guy also flows in two X2.",
            "Right, so the way you can think about it now is that no two basically now has all the information that he or she needs to solve the problem because node two can just do a maximum over X2 of these two quantities of the message from three to two X2.",
            "The other guy.",
            "One to two X2 and then the last thing is this piece of information his own local information or local evidence.",
            "He needs to maximize over that.",
            "And the problem is solved.",
            "Yep.",
            "For this example, if you summed out over X2 first, if you think about the key thing in this argument, was the messages that I create.",
            "How big are they?",
            "If I somehow over X2 first, so I sum out X2, I end up with a function of X1 and X3.",
            "So instead of just being a vector, it would now be a matrix.",
            "Now, that doesn't seem bad, because this is a toy example, but I could make a very bad graph such that if you here is perhaps the worst graph that you could imagine.",
            "This is also a tree.",
            "But it's like a star.",
            "Right, so if you wanted to solve this problem.",
            "The right thing to do would be to pick one of the leaves.",
            "These are the leaves of the tree.",
            "For obvious reasons.",
            "You would sum that out and you get a message that filters up a message that filters up a message that filters up, and those would all be vectors.",
            "They'd be very compact.",
            "If I summed this Sky out, it would have a disastrous effect.",
            "It would couple all of these guys together.",
            "You would get a giant message, in this case on five variables I could have made it a star with 100 variables, But you would get a very large dimensional message and you eliminate all the sort of savings that we're after.",
            "What we're after, and we're going to drive tomorrow is for any tree we're going to have an algorithm that all it does at every round.",
            "Every node does a little bit of summation and passes a single vector along every edge, right?",
            "So that's important because the things being passed or very easy to store and very easy to pass.",
            "You don't want to pass messages that are huge tables, right?",
            "Huge table.",
            "If you had 100 variables in the leaves and you sum that guy out, you have to pass something that was maybe 2 to the 100 dimensional that's.",
            "That's definitely not what we want.",
            "Any other questions?",
            "Right, So what we've done here?",
            "Um?",
            "What we've done here is basically just organize the structure of our computation right?",
            "We've just played simple games with maximum that we can move.",
            "For instance, this maximum can get moved in effect only those terms.",
            "The way to think about it is what we're doing is we're structuring the computation in the right way that we have to do the minimal number of maximization's and products.",
            "If we don't structured in this way, right?",
            "If you're naive about it and you didn't structure, then you have to do 8 maximization's.",
            "If you had binary variables and every time you have to do a product if you do a little bit of algebra discounting operations, you will see that this is already starting to save us.",
            "Maybe not so much because it was an example with three nodes, but.",
            "Example, if you imagine a much longer chain like you would have if you were doing a parsing problem, you can see that the computer the savings are going to get exponential we're going to drop the complexity from being exponential in the graph size to being linear in the graph size."
        ],
        [
            "So wrap up now.",
            "'cause probably everyone is hungry, but what you might want to think about is.",
            "I've sort of given you the first building block.",
            "I did three nodes for you.",
            "That's a very simple graph.",
            "Now you want to sort of think if I had a somewhat larger graph with five nodes and I had additional people beyond three, I need to play the same game.",
            "I need to structure my Max and my products in the right way and I need to pass messages.",
            "And if you play games, you'll find out that you can derive.",
            "Rule that tells you how the messages should be updated.",
            "And the way they should be updated is you should take messages that are incoming to like these red guys.",
            "You should multiply them.",
            "That's a product and then you should do a maximization that involves your local evidence and the evidence on the edges.",
            "So that update right there for this simple graph, that's called the Max product update, right?",
            "It's sort of clear why it's a maximum and you have a product that if you did it on a chain, that's exactly what the Viterbi algorithm is doing, except the Viterbi algorithm is doing it.",
            "Usually they take the log of the problem, and they do something like the Max sum algorithm or them in some algorithm.",
            "But they're all the same.",
            "Algorithm.",
            "Doesn't matter whether you maximize a product or if you maximize the log of the product, that's.",
            "Course the same thing.",
            "OK so tomorrow will finish driving that this sort of simple update rule and we'll see that it's exact on trees.",
            "But then we're going to want to understand.",
            "It's also used on graphs with cycles where it need not be exact, but it can often behave well, so we'd like to try and understand that a little bit tomorrow.",
            "OK, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll be talking about graphical models, I guess some aspects of graphical models appear up there, things like.",
                    "label": 0
                },
                {
                    "sent": "Graphical models trees bounds reweighted Gaussian will see algorithm will see many of these things in these tutorial talks.",
                    "label": 0
                },
                {
                    "sent": "So the plan is the first talk today is going to be somewhat higher level.",
                    "label": 0
                },
                {
                    "sent": "Just giving the basics of graphical models.",
                    "label": 1
                },
                {
                    "sent": "Just giving you a feel what they can be used for.",
                    "label": 1
                },
                {
                    "sent": "Start drilling down a little bit towards the end of the lecture today and then tomorrow we'll get more in detail on message passing and also what are known as variational methods.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of fortunate, I think, 'cause I'm the last lecture, so I can essentially build on what everyone else has done.",
                    "label": 0
                },
                {
                    "sent": "I assume that you know everything that every other lecture that preceded me.",
                    "label": 0
                },
                {
                    "sent": "So there will be some things that are relevant, probably for tomorrow things that leave in Vandenberghe taught you about optimization.",
                    "label": 0
                },
                {
                    "sent": "It could be helpful if you just glanced at the definition of conjugate dual function again from his notes, I'll go through it again tomorrow, but we're going to use it will see it actually is very important in graphical models, so that's a nice connection to what you heard from leaving.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to use a combination of these slides, but then to slow me down I'm going to write things down on the overhead as we go.",
                    "label": 0
                },
                {
                    "sent": "Actually, just for curiosity, how many people know about graphical models, as in, have either read about them or had a course already?",
                    "label": 0
                },
                {
                    "sent": "OK, so why am I here?",
                    "label": 0
                },
                {
                    "sent": "Well, that's fine, but I'm just trying to set the level.",
                    "label": 0
                },
                {
                    "sent": "You don't know, OK?",
                    "label": 0
                },
                {
                    "sent": "No so.",
                    "label": 0
                },
                {
                    "sent": "Is always this sort of multiresolution aspect when you're teaching so today I'm going to start from the basics, probably by later tomorrow afternoon.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know, take the gloves off a little bit.",
                    "label": 0
                },
                {
                    "sent": "I think that's sort of a fair, fair compromise.",
                    "label": 0
                },
                {
                    "sent": "OK, so graphical models have a pretty long history.",
                    "label": 0
                },
                {
                    "sent": "They used a lot in machine learning, but actually the history lies in different areas that earlier history.",
                    "label": 1
                },
                {
                    "sent": "Probably the earliest history is in statistical physics.",
                    "label": 1
                },
                {
                    "sent": "Will talk in a minute about something called the Eising model, which was proposed around the 1920s, but also in areas like phylogenetic's people studying trees the way organisms evolve and split.",
                    "label": 0
                },
                {
                    "sent": "It's very natural that they started using graphs as well.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk a little bit.",
                    "label": 0
                },
                {
                    "sent": "Also in communication these models were used a lot in the 50s, so in some ways artificial intelligence and machine learning is a bit of a latecomer to the field is really in the 80s that Udaya Pearl, in his book started talking a lot about graphs and algorithms on graphs and things like that, and since then, of course the area is exploded.",
                    "label": 0
                },
                {
                    "sent": "There's a very active literature.",
                    "label": 0
                },
                {
                    "sent": "OK, so as the name suggests, graphical models involve graphs.",
                    "label": 0
                },
                {
                    "sent": "There's different kinds of graphs.",
                    "label": 0
                },
                {
                    "sent": "I could sort of spend a lot of time talking about different kinds of graphs, but just to keep things focused here, we're going to talk exclusively about undirected graphs and what the purpose of the graphical model is.",
                    "label": 0
                },
                {
                    "sent": "It's to allow us to model the dependencies among a large collection of random variables in a relatively simple way.",
                    "label": 0
                },
                {
                    "sent": "And so the way we do that is we take our graph, so here's an ordinary graph.",
                    "label": 0
                },
                {
                    "sent": "This is a different object.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about it in a minute.",
                    "label": 0
                },
                {
                    "sent": "A graph has nodes or vertices, and we're going to assign a random variable to every one of those things.",
                    "label": 0
                },
                {
                    "sent": "And what we'll see is that the edges, or actually the absence of edges, those are going to allow us to include what kind of dependency structure we see in a collection of random variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's the motivation for doing this?",
                    "label": 0
                },
                {
                    "sent": "Well, the motivation is, let's just do a little thought experiment.",
                    "label": 0
                },
                {
                    "sent": "If I just gave you, let's say N is 100, you have 100 variables and the variables are disk coin flips there.",
                    "label": 0
                },
                {
                    "sent": "This Bernoulli an if you didn't have any structure whatsoever on the distribution, how many numbers would you need to represent it?",
                    "label": 0
                },
                {
                    "sent": "So 100 binary variables?",
                    "label": 0
                },
                {
                    "sent": "Two to the 100, or if you want to be minus one, yes, right?",
                    "label": 0
                },
                {
                    "sent": "So it doesn't really matter two to the 100 is probably larger than it is larger than the number of atoms in the visible universe, so there's no way that you could ever represent a distribution you couldn't even store it.",
                    "label": 0
                },
                {
                    "sent": "You can't compute with it.",
                    "label": 0
                },
                {
                    "sent": "And 100 is a small a small problem.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a simulation in a moment that involves something like 10,000 variables.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the point of these.",
                    "label": 0
                },
                {
                    "sent": "Graphical models is going to allow us to really simplify and specify simpler factorizations, in particular of distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do now?",
                    "label": 0
                },
                {
                    "sent": "We want to start relating the structure of the graph to the dependency structure of the random variables.",
                    "label": 0
                },
                {
                    "sent": "There's sort of two ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "One way of doing it is in terms of what's known as the clique structure of a graph, so just out of curiosity, how many people know what a clique is?",
                    "label": 0
                },
                {
                    "sent": "OK, so some subset of you, but you don't.",
                    "label": 0
                },
                {
                    "sent": "I admire you, actually, 'cause you admit what you don't know.",
                    "label": 0
                },
                {
                    "sent": "If you do it a little experiment, any audience.",
                    "label": 0
                },
                {
                    "sent": "I do this often you say who knows about X and then some fraction raises their hand.",
                    "label": 0
                },
                {
                    "sent": "You say who does not know about X?",
                    "label": 0
                },
                {
                    "sent": "It's definitely not the one minus the fraction that just raised their hand.",
                    "label": 0
                },
                {
                    "sent": "Nothing wrong with not knowing anything.",
                    "label": 0
                },
                {
                    "sent": "What's wrong is when you don't know what you don't know.",
                    "label": 0
                },
                {
                    "sent": "Starting to sound like Donald Rumsfeld for those of you who know him.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's write a little bit here.",
                    "label": 0
                },
                {
                    "sent": "Week oh is our friend back.",
                    "label": 0
                },
                {
                    "sent": "How did that happen?",
                    "label": 0
                },
                {
                    "sent": "He was gone a minute ago.",
                    "label": 0
                },
                {
                    "sent": "It's OK, he'll give you some.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "Sometimes I feel like that too.",
                    "label": 0
                },
                {
                    "sent": "OK, so we want to link move that doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Hopefully this one works.",
                    "label": 0
                },
                {
                    "sent": "We want to link graphs an probabilities right that's.",
                    "label": 0
                },
                {
                    "sent": "Is that better?",
                    "label": 0
                },
                {
                    "sent": "Good graphs plus probability?",
                    "label": 0
                },
                {
                    "sent": "That's what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, so we've got an undirected graph.",
                    "label": 0
                },
                {
                    "sent": "Right, so all throughout all, use V for vertices or nodes.",
                    "label": 0
                },
                {
                    "sent": "Just think about that is the set one through N. We're just labeling the nodes of our graph, and then E is a bunch of edges, so edges for an undirected graph, or just unordered pairs.",
                    "label": 0
                },
                {
                    "sent": "These are edges IJI not equal to J. Hi there.",
                    "label": 0
                },
                {
                    "sent": "OK, and we've got a random variable sitting at each node of the graph.",
                    "label": 1
                },
                {
                    "sent": "Right, so random variable X at node I. OK, and So what we want to do is we've got.",
                    "label": 1
                },
                {
                    "sent": "We've got this random vector.",
                    "label": 0
                },
                {
                    "sent": "Let me introduce the shorthand that I'll use sometimes I'll call XAV.",
                    "label": 0
                },
                {
                    "sent": "Whenever I use a subset like V or A or anything, it just means this is a big subset of random variables that are indexed by the vertices in this case.",
                    "label": 0
                },
                {
                    "sent": "So we've got a big random vector and we'd like to specify a distribution over it.",
                    "label": 0
                },
                {
                    "sent": "So a clique.",
                    "label": 0
                },
                {
                    "sent": "C. The subset of vertices.",
                    "label": 0
                },
                {
                    "sent": "That is fully connected.",
                    "label": 0
                },
                {
                    "sent": "Right, so it just means that I J is in the edge set for all I&J that are in the vertex set.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what I've defined up here.",
                    "label": 0
                },
                {
                    "sent": "So let's let's look at this graph that I've drawn up here, and let's figure out what the cliques are.",
                    "label": 0
                },
                {
                    "sent": "Anyone?",
                    "label": 0
                },
                {
                    "sent": "Well, I've already put them there for you.",
                    "label": 0
                },
                {
                    "sent": "It's rather helpful.",
                    "label": 0
                },
                {
                    "sent": "Right, but for instance 123 is a clique because 1, two, and three all talk.",
                    "label": 0
                },
                {
                    "sent": "What about 1234?",
                    "label": 0
                },
                {
                    "sent": "Is that a clique?",
                    "label": 1
                },
                {
                    "sent": "No, it's not, because four should be friends with two and one.",
                    "label": 0
                },
                {
                    "sent": "So in this case the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "Those are cliques that aren't contained within anything else.",
                    "label": 0
                },
                {
                    "sent": "Those are 123.",
                    "label": 0
                },
                {
                    "sent": "This triangle 345456 and four 7.",
                    "label": 0
                },
                {
                    "sent": "So it's 2 three o'clock for instance.",
                    "label": 1
                },
                {
                    "sent": "Right, it is a clique, but it's not what we call a maximal clique.",
                    "label": 0
                },
                {
                    "sent": "We could make it bigger by adding one to it.",
                    "label": 0
                },
                {
                    "sent": "So sometimes we'll talk about cliques, sometimes about maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "The other thing we're going to need, I'll just I'm just here just rewriting what I've got on the slide just so we can refer to it later.",
                    "label": 0
                },
                {
                    "sent": "We're going to vertex cut set right, so vertex cut set is.",
                    "label": 0
                },
                {
                    "sent": "It's a subset.",
                    "label": 0
                },
                {
                    "sent": "It's a different kind of subset.",
                    "label": 0
                },
                {
                    "sent": "So it's a subset of vertices who when you remove them it splits the graph into two or more.",
                    "label": 1
                },
                {
                    "sent": "Um, disjoint parts, right?",
                    "label": 0
                },
                {
                    "sent": "That's slightly vague.",
                    "label": 0
                },
                {
                    "sent": "You can make that precise, but the intuition is given here this set here.",
                    "label": 0
                },
                {
                    "sent": "If I remove this pair of vertices from the graph, that's vertices four and five in this graph, then it basically breaks the graph into two pieces, and the two pieces are set of variables A and the set of variables B. OK, so those are two graph theoretic properties and we're going to use those graph theoretic properties.",
                    "label": 0
                },
                {
                    "sent": "We're going to use them to specify how distributions should somehow respect the structure of the graph.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So one way will do it.",
                    "label": 0
                },
                {
                    "sent": "This slide says, so we're going to remember those two.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Guns.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of two ways in which we can use the graph to build structured representations of our random variables.",
                    "label": 0
                },
                {
                    "sent": "One is that we can impose what's known as a Markov property.",
                    "label": 1
                },
                {
                    "sent": "So how many people know about Markov chains?",
                    "label": 0
                },
                {
                    "sent": "Just to just be sure?",
                    "label": 0
                },
                {
                    "sent": "OK, so a little bit.",
                    "label": 0
                },
                {
                    "sent": "Right, so Markov chain is actually a very special kind of graphical model.",
                    "label": 0
                },
                {
                    "sent": "Let's let's actually do this before I go on.",
                    "label": 0
                },
                {
                    "sent": "Right, somehow this it's really historically where these things came from.",
                    "label": 0
                },
                {
                    "sent": "A Markov chain is as you'd expect, it's it's a graph, that's a chain, right?",
                    "label": 0
                },
                {
                    "sent": "So you've just got variables ordered like this.",
                    "label": 0
                },
                {
                    "sent": "It's very natural if you have things like time series.",
                    "label": 0
                },
                {
                    "sent": "See this old technology is kind of, well, it's nice in some ways but annoying in others.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I'll come down in a second.",
                    "label": 0
                },
                {
                    "sent": "Right, so in a Markov chain, you've got a bunch of random variables.",
                    "label": 0
                },
                {
                    "sent": "Think about a time series.",
                    "label": 0
                },
                {
                    "sent": "Think about for instance, you're trying to track the position of a person walking in a camera in a sequence video sequence.",
                    "label": 0
                },
                {
                    "sent": "So X one could be positioned at time 21X2 position at time two, and so on, and the natural sort of model you'd have is that that's evolving according to something like a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Right, so where are the vertex cut sets in this very simple graph here?",
                    "label": 0
                },
                {
                    "sent": "Right, basically any variables a vertex cut set.",
                    "label": 0
                },
                {
                    "sent": "The interesting ones.",
                    "label": 0
                },
                {
                    "sent": "This is not an interesting one, but the interesting vertex cut sets.",
                    "label": 0
                },
                {
                    "sent": "If I remove this from my graph, right if I call that S. If I remove that, then that's going to break the graph into two pieces, and in this case they have a nice interpretation.",
                    "label": 0
                },
                {
                    "sent": "You would get the past.",
                    "label": 0
                },
                {
                    "sent": "If the cut set is the present, then the present splits the graph into the past and the future.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's one way of defining Markov chains, right?",
                    "label": 0
                },
                {
                    "sent": "You say that it's a process that if you know the present, then everything else in the past tells you nothing more about the future.",
                    "label": 0
                },
                {
                    "sent": "Right, but this is a simple instance of this more general property that I'm about to introduce.",
                    "label": 0
                },
                {
                    "sent": "Because this is a cut set, this cut sets split the graph into two pieces, the past and the future, and what you had is a conditional independence property.",
                    "label": 1
                },
                {
                    "sent": "And the notation we use for this.",
                    "label": 0
                },
                {
                    "sent": "We say that X of a.",
                    "label": 0
                },
                {
                    "sent": "That's the random variables in the past is conditionally independent of X of.",
                    "label": 0
                },
                {
                    "sent": "I think I've messed up my notation.",
                    "label": 0
                },
                {
                    "sent": "I was using B to call that B, thanks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, This is why I hate old technology.",
                    "label": 0
                },
                {
                    "sent": "I can guarantee you you should look at me at dinner.",
                    "label": 0
                },
                {
                    "sent": "I'll have paint on my face.",
                    "label": 0
                },
                {
                    "sent": "Right so past.",
                    "label": 0
                },
                {
                    "sent": "Future.",
                    "label": 0
                },
                {
                    "sent": "And present.",
                    "label": 0
                },
                {
                    "sent": "Right, So what this notation means?",
                    "label": 0
                },
                {
                    "sent": "This notation means that the set of variables indexed by the past by the set A are conditionally independent of the future given the present right?",
                    "label": 0
                },
                {
                    "sent": "So this is shorthand for X of A is conditionally independent of X of B given past.",
                    "label": 0
                },
                {
                    "sent": "So how many people have seen that property before?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there's ways to slow down professors, write questions.",
                    "label": 0
                },
                {
                    "sent": "Always slow down professors, ways to speed them up is sort of, you know, audible snoring's or sleeping noises tend to speed.",
                    "label": 0
                },
                {
                    "sent": "Or they might anger Professor, but probably would speed them up.",
                    "label": 0
                },
                {
                    "sent": "OK, so the more general property is that if you have any graph right not just a Markov chain, what you say is that if you have a vertex cut set then you want the parts that get split.",
                    "label": 0
                },
                {
                    "sent": "You want the random variables in those parts to be conditionally.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so in this case, what it would mean to say that the random variables respect the graph structure.",
                    "label": 0
                },
                {
                    "sent": "One way of saying that is that if we conditioned on the variables in this cut set, if you knew those variables, then the variables here are conditionally independent of the variables there.",
                    "label": 0
                },
                {
                    "sent": "Given this cut set.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's that's what's called the Markov properties of the graph, and you can see it's just a natural generalization of the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Oops, sorry bout that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's one characterization, right?",
                    "label": 0
                },
                {
                    "sent": "So we look back here.",
                    "label": 0
                },
                {
                    "sent": "I've introduced two graph theoretic properties.",
                    "label": 0
                },
                {
                    "sent": "One was the property of a vertex cut set.",
                    "label": 0
                },
                {
                    "sent": "And what we've seen is that vertex cut sets give us conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "Right, so we've seen that the statement we want is that XA and is conditionally independent of X be given access, right?",
                    "label": 0
                },
                {
                    "sent": "So A&B were the two disconnected parts.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of one way in which we could constrain our distribution.",
                    "label": 0
                },
                {
                    "sent": "The other way is in terms of cliques, right?",
                    "label": 0
                },
                {
                    "sent": "So cliques are fully connected subsets of variables.",
                    "label": 0
                },
                {
                    "sent": "And the way we constraint with cliques is that we say.",
                    "label": 0
                },
                {
                    "sent": "We would like a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Remember, in general this could have.",
                    "label": 0
                },
                {
                    "sent": "Many, many degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "But what we say now is we say that you're only allowed to build this probability distribution, so it's a function over N variables and it should be built in such a way that you're only allowed to take a product over cliques and every clique.",
                    "label": 1
                },
                {
                    "sent": "You can put a function on that clique.",
                    "label": 0
                },
                {
                    "sent": "And that function is a function.",
                    "label": 0
                },
                {
                    "sent": "It's a local function that depends only on the variables in that clique.",
                    "label": 0
                },
                {
                    "sent": "Right, these are sometimes called compatibility functions.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we go back to this this Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what are the maximal cliques for this guy?",
                    "label": 0
                },
                {
                    "sent": "Right, the edges are the maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "There's nothing else.",
                    "label": 0
                },
                {
                    "sent": "So what this would say for this Markov chain?",
                    "label": 0
                },
                {
                    "sent": "It would say that you've got a distribution over 4 variables.",
                    "label": 0
                },
                {
                    "sent": "And it would say that it should factorize this Z that I'm writing is just a normalization term.",
                    "label": 0
                },
                {
                    "sent": "It's to make the probability distribution normalize.",
                    "label": 0
                },
                {
                    "sent": "It would say this should factorize in this way.",
                    "label": 0
                },
                {
                    "sent": "Right so we gotta click on one, two gotta click on 2 three.",
                    "label": 0
                },
                {
                    "sent": "And you've gotta click on 3 four.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's saying that you can sort of choose three functions here.",
                    "label": 0
                },
                {
                    "sent": "I've done it, one for every maximal clique, and the functions depend only on two variables.",
                    "label": 0
                },
                {
                    "sent": "The whole distribution depends on four, but it's it's a product of pairwise terms.",
                    "label": 0
                },
                {
                    "sent": "Right, so suppose these variables were binary.",
                    "label": 0
                },
                {
                    "sent": "We argued before that if I didn't have any structure.",
                    "label": 0
                },
                {
                    "sent": "I would need something like 16 numbers here.",
                    "label": 0
                },
                {
                    "sent": "Right with no structure, I'd need two to four.",
                    "label": 0
                },
                {
                    "sent": "How many numbers would I need for these guys?",
                    "label": 0
                },
                {
                    "sent": "You need at most 12 right?",
                    "label": 1
                },
                {
                    "sent": "'cause the function on two binary variables.",
                    "label": 0
                },
                {
                    "sent": "Each pair needs at most 4 numbers.",
                    "label": 0
                },
                {
                    "sent": "Three is actually enough if you think about it.",
                    "label": 0
                },
                {
                    "sent": "So in fact you need at most 9 numbers here, so 16 to 9.",
                    "label": 0
                },
                {
                    "sent": "That doesn't seem like a big deal, but if it hadn't been for if I could draw, you know 100, we would have gone from 2 to 300 down to something like 4 * 100.",
                    "label": 0
                },
                {
                    "sent": "So you go from 2 to 100 more than the number of atoms in the universe to roughly 400 numbers.",
                    "label": 0
                },
                {
                    "sent": "And that's quite reasonable, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about if you were trying to fit one of these models today to 400 parameters is not unreasonable, whereas 2 to 100.",
                    "label": 0
                },
                {
                    "sent": "Well, datasets are big these days, but they're not that big, right?",
                    "label": 0
                },
                {
                    "sent": "So that's very important.",
                    "label": 0
                },
                {
                    "sent": "That kind of compression in terms of the parameterisation of the distribution.",
                    "label": 0
                },
                {
                    "sent": "Is anyone bothered by this factorization?",
                    "label": 0
                },
                {
                    "sent": "Is there an inconsistency between what I wrote and what stated up there?",
                    "label": 0
                },
                {
                    "sent": "But here I was only using maximal cliques, right?",
                    "label": 0
                },
                {
                    "sent": "There I sort of said it's overall cliques, right?",
                    "label": 0
                },
                {
                    "sent": "So the additional clicks that we left over the nodes.",
                    "label": 0
                },
                {
                    "sent": "The nodes are cliques.",
                    "label": 0
                },
                {
                    "sent": "But is it wrong what I've written here?",
                    "label": 0
                },
                {
                    "sent": "Yes, absorb you're a man of few words but correct words, which is good, alright?",
                    "label": 0
                },
                {
                    "sent": "So we could always absorb these things so we haven't said this is unique.",
                    "label": 0
                },
                {
                    "sent": "If you sort of strip parameters out of it, you can get unique factorizations, but in the way we've written it, these factorizations aren't unique.",
                    "label": 1
                },
                {
                    "sent": "But that's not what's actually important right now.",
                    "label": 0
                },
                {
                    "sent": "What's important is that the factorization is very compact.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I'm an important result in this area that I'll come back to later.",
                    "label": 0
                },
                {
                    "sent": "Actually says you might be wondering.",
                    "label": 0
                },
                {
                    "sent": "OK, so high level just recapping.",
                    "label": 0
                },
                {
                    "sent": "We said graphs plus probability.",
                    "label": 0
                },
                {
                    "sent": "We said, OK, I can look at vertex cut sets and I can look at conditional independence, or I can look at cliques and I can look at factorization.",
                    "label": 1
                },
                {
                    "sent": "Now you might say, well, why did I introduce both of them?",
                    "label": 0
                },
                {
                    "sent": "Well, introduce both, because they're actually equivalent.",
                    "label": 0
                },
                {
                    "sent": "This is a theorem known as the Hammersley Clifford theorem.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter which way you go if you start with a fixed graph, and you say that you want it to factorize across the cliques, that's the same as you want, as saying that all these conditional independent statements hold.",
                    "label": 0
                },
                {
                    "sent": "So the Markov chain is a particular example of it, right the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "You know when you build Markov chains, you don't actually typically write them like this.",
                    "label": 0
                },
                {
                    "sent": "You typically write them as an initial distribution.",
                    "label": 0
                },
                {
                    "sent": "Is a conditional distribution X2 given X one X3 given X2, but that's a special case of this kind of factorization, right?",
                    "label": 1
                },
                {
                    "sent": "So that's one way to understand Markov chains and the other way is in terms of the conditioning on present breaks the graph into 2.",
                    "label": 0
                },
                {
                    "sent": "Right, so for Markov chains we can sort of see that, which doesn't matter where we start, we end up at the same answer, but Hammersley Clifford says, actually, for a general graph that that's also true, and that's not obvious.",
                    "label": 0
                },
                {
                    "sent": "One Direction is not hard.",
                    "label": 0
                },
                {
                    "sent": "I'll do the not hard direction with you in a minute.",
                    "label": 0
                },
                {
                    "sent": "The other direction is actually quite difficult.",
                    "label": 0
                },
                {
                    "sent": "Not quite difficult, but it needs some work.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll come back to the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minute let me just start giving you some examples.",
                    "label": 0
                },
                {
                    "sent": "This is an example that we've I've already just mentioned the Markov chain.",
                    "label": 0
                },
                {
                    "sent": "What I'm doing here is actually amending it to make it what's known as a hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Alright, so hidden Markov model you have both these variables X that are evolving overtime and you also have some variables hanging off the X is these are wise.",
                    "label": 0
                },
                {
                    "sent": "These are shaded in because that's typically what you observe.",
                    "label": 0
                },
                {
                    "sent": "Right, so actually the biggest success of Markov models are one of the earlier big successes in automatic speech recognition, which I personally don't like because it's part of the reason that you know whenever you call a phone number for a company, you get a computer.",
                    "label": 0
                },
                {
                    "sent": "These days you don't get customer service right, but speech recognition, speech models, way speech involves at least humans can be modeled very naturally as a Markov process.",
                    "label": 0
                },
                {
                    "sent": "Right, there's sort of a sequential dependence in the way that people are forming phonemes, and the problem of speech recognition is exactly it's well modeled by exactly this graph.",
                    "label": 0
                },
                {
                    "sent": "Of course, along graph.",
                    "label": 0
                },
                {
                    "sent": "So these are the sort of hidden phonemes the sort of units of speech that you're trying to infer, and the shaded circles.",
                    "label": 0
                },
                {
                    "sent": "Those are other random variables.",
                    "label": 0
                },
                {
                    "sent": "That's what you observe.",
                    "label": 0
                },
                {
                    "sent": "So if you're speaking over the phone, you're getting some sort of noisy version of what the person is really saying, or things like you know people might not be fluent speakers.",
                    "label": 0
                },
                {
                    "sent": "If I speak French, there would be some.",
                    "label": 0
                },
                {
                    "sent": "Certain amount of noise should we say for the speech recognition device and so on.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's it's an instance of this that what the computer observes is this noisy instance.",
                    "label": 0
                },
                {
                    "sent": "What is trying to infer as the actual phonemes, and then it would parse them to get words and what is exploiting is that there's a lot of temporal dependence in the sequence of words.",
                    "label": 0
                },
                {
                    "sent": "Phonemes at all levels of language.",
                    "label": 0
                },
                {
                    "sent": "There's tons of temporal dependence, so really the success of automatic speech recognition is an instance of this, and it's an instance of some algorithms that will talk about tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Some of these message passing algorithms.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So Markov models are great, but.",
                    "label": 0
                },
                {
                    "sent": "There's many things you'd like to do in practice that are more complicated.",
                    "label": 0
                },
                {
                    "sent": "What various people have done an interesting sort of extension of this speech recognition is if you have video that has both audio tracks, an it has speaking and let's assume that multiple people are speaking an hence multiples people's lips are moving, or maybe it's slightly different times.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to solve solve different names for it, but the cocktail party problem right?",
                    "label": 0
                },
                {
                    "sent": "You have many people speaking.",
                    "label": 0
                },
                {
                    "sent": "And you'd like to separate the audio track into multiple tracks to get each person's voice.",
                    "label": 0
                },
                {
                    "sent": "One way you can do that better is if you have video and then you can somehow use the synchrony between my lips, moving, informing certain things and what's being said on the audio.",
                    "label": 0
                },
                {
                    "sent": "So now what you have is you have two temporal models, you have two Markov chains.",
                    "label": 0
                },
                {
                    "sent": "You have the video evolving overtime and you have the audio evolving overtime, so you'd like to model both the temporal dependencies.",
                    "label": 0
                },
                {
                    "sent": "You'd also like to synchronize.",
                    "label": 0
                },
                {
                    "sent": "You'd like to synchronize across streams, and you might actually have multiple cameras if you're lucky, so you might have camera 1, camera 2 audio one, and you have a whole coupled set of Markov chains, so this is a much richer model and we're going to see that that algorithmically this poses significant challenges that this model does not.",
                    "label": 0
                },
                {
                    "sent": "The reason it poses challenges is because this model is what's known as a tree.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have cycles, right?",
                    "label": 0
                },
                {
                    "sent": "There's no way of following a cycle around, but this model has many cycles, right?",
                    "label": 0
                },
                {
                    "sent": "I can walk around a cycle like that.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of cycles in this MoD.",
                    "label": 0
                },
                {
                    "sent": "It's not a tree, and we'll see for that reason that when it comes to.",
                    "label": 0
                },
                {
                    "sent": "Solving computational inference problems.",
                    "label": 0
                },
                {
                    "sent": "This model is much harder than that one.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So yes, I like you you you catch me so true I'm only going to talk about undirected, but some of my pictures have directed have arrows on them for what we're going to do later.",
                    "label": 0
                },
                {
                    "sent": "The Arrows aren't going to play any role later when I talk about inference, the first thing that you would normally do is what's called moralized.",
                    "label": 0
                },
                {
                    "sent": "The graph more lies is a cool name.",
                    "label": 0
                },
                {
                    "sent": "If you have a directed graph.",
                    "label": 0
                },
                {
                    "sent": "If you have two nodes that come into the same node, it's as if you have two people.",
                    "label": 0
                },
                {
                    "sent": "Who have had a baby and they're not married.",
                    "label": 0
                },
                {
                    "sent": "So if you're in certain parts of the United States, that's immoral.",
                    "label": 0
                },
                {
                    "sent": "So therefore you moralize the graph by adding an edge between these parents and then you drop the arrows.",
                    "label": 0
                },
                {
                    "sent": "So you're correct that directed graphs are very interesting.",
                    "label": 0
                },
                {
                    "sent": "I don't mean to say that they're not, but for what we're going to talk about in the rest, I'm going to think about undirected.",
                    "label": 0
                },
                {
                    "sent": "So you should basically just ignore the arrows here.",
                    "label": 0
                },
                {
                    "sent": "You're also correct that Markov chains, if you think about time series, there's a natural sense of moving forward versus backward, but again, for many of the sort of core problems that will solve that difference is not actually essential, it is from the modeling interpretation point of view, but from the computational point of view, it actually turns out not to be.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "Skip.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It depends how you interpret it.",
                    "label": 0
                },
                {
                    "sent": "The way the way this graph should be interpreted is that the colored nodes have what you've observed, which is what you should think is the hidden stuff is, you're trying to infer.",
                    "label": 0
                },
                {
                    "sent": "Let's say the identity of the speakers you're trying to infer the unstream.",
                    "label": 0
                },
                {
                    "sent": "The unmixed audio stream, and also unmixed from the video, but you don't observe that.",
                    "label": 0
                },
                {
                    "sent": "The audio might have, you know this.",
                    "label": 0
                },
                {
                    "sent": "We could just do with audio.",
                    "label": 0
                },
                {
                    "sent": "You might have three audio streams, as in speakers who are evolving independently, but you don't observe them.",
                    "label": 0
                },
                {
                    "sent": "You have observed their mixture and that's what the colored nodes would be similar with audio plus video.",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Any other distribution other than.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "With certain exceptions, and this is probably related to what are nodes spoke to you about or noticed?",
                    "label": 0
                },
                {
                    "sent": "Set things like particle filtering.",
                    "label": 0
                },
                {
                    "sent": "So tomorrow we're going to talk about message passing.",
                    "label": 0
                },
                {
                    "sent": "These are algorithms that are, for instance, one thing you might like to do from since if you're doing speech recognition, is you'd like to predict.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you might like to predict what the next utterance would be, or actually maybe a more modern examples when you're typing on your iPad or your phone, right?",
                    "label": 0
                },
                {
                    "sent": "It's actually again running a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "It's trying to predict, based on language.",
                    "label": 0
                },
                {
                    "sent": "Temporal dependence is what character you're going to strike next based on.",
                    "label": 0
                },
                {
                    "sent": "Frequency and other things.",
                    "label": 0
                },
                {
                    "sent": "So if you wanted to predict the next character, for instance, you'd like to solve an inference problem you'd like to compute a marginal over that.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about algorithms that are tractable for discrete and Gaussian.",
                    "label": 0
                },
                {
                    "sent": "There's essentially closed form if you have continuous but non Gaussian, then you have to resort to things like particle filtering or other kinds of MCMC or sampling based methods.",
                    "label": 0
                },
                {
                    "sent": "Those problems are much harder.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we will talk mainly about discrete and Gaussian, but the talk of Arno would be connected to the more general problems.",
                    "label": 0
                },
                {
                    "sent": "Did he talk about non parametric belief propagation or did he talk exclusively about time series?",
                    "label": 0
                },
                {
                    "sent": "Say no, that was an either or so we talked about time series.",
                    "label": 0
                },
                {
                    "sent": "I would imagine right, but much of what he said can be done on more general graphs and there has been a fair bit of work on that.",
                    "label": 0
                },
                {
                    "sent": "If you search for non parametric belief propagation, you'll find some interesting hits.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so social network analysis this is another one quite topical, so we've got a bunch of individuals, individuals or engaging in some kind of behavior and you'd like to model that.",
                    "label": 1
                },
                {
                    "sent": "So sort of nice now we've got massive datasets.",
                    "label": 0
                },
                {
                    "sent": "We've got things like Facebook.",
                    "label": 0
                },
                {
                    "sent": "We've got Amazon collecting huge amounts of data on.",
                    "label": 0
                },
                {
                    "sent": "You know what people like and what they don't like, so there's very large amounts of data to model.",
                    "label": 0
                },
                {
                    "sent": "Let me just give you a simple demonstration of this.",
                    "label": 0
                },
                {
                    "sent": "This actually will illustrate what's known as the eising model.",
                    "label": 0
                },
                {
                    "sent": "So just to be concrete.",
                    "label": 0
                },
                {
                    "sent": "So this is not what the eising model was used for, but one thing you could think about using it for is.",
                    "label": 0
                },
                {
                    "sent": "You could think about it for modeling how epidemics spread.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "So the using model is an instance of a graphical model.",
                    "label": 0
                },
                {
                    "sent": "It's a graphical model in which.",
                    "label": 0
                },
                {
                    "sent": "You've got a bunch of variables that are just.",
                    "label": 0
                },
                {
                    "sent": "Let's make them minus 1 + 1.",
                    "label": 0
                },
                {
                    "sent": "They're binary.",
                    "label": 0
                },
                {
                    "sent": "You can think about them as indicator variables for the state of something.",
                    "label": 0
                },
                {
                    "sent": "So if we wanted to model diseases.",
                    "label": 0
                },
                {
                    "sent": "Then we could think about XI being plus one.",
                    "label": 0
                },
                {
                    "sent": "That means that individual I.",
                    "label": 0
                },
                {
                    "sent": "So that's the person at node I is infected.",
                    "label": 0
                },
                {
                    "sent": "And we could think of minus one if I is healthy.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're going to a pattern of plus or minus ones and it's interesting to figure out, for instance, how infectious diseases, right?",
                    "label": 0
                },
                {
                    "sent": "You sort of expect.",
                    "label": 0
                },
                {
                    "sent": "Well with most diseases, that spatial proximity, if you're close to someone who's infected, then the likelihood that you get infected goes up.",
                    "label": 1
                },
                {
                    "sent": "For instance, every time I travel in a plane, I worry about this right?",
                    "label": 0
                },
                {
                    "sent": "You're sitting next to someone who happens to have a horrible cough.",
                    "label": 0
                },
                {
                    "sent": "Then after 10 hours of that.",
                    "label": 0
                },
                {
                    "sent": "The likelihood of you having a horrible cough is non negligible.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we were modeling this, we'd sort of think about.",
                    "label": 0
                },
                {
                    "sent": "We've got some graph.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to model in a plane, we'd have a plane like this.",
                    "label": 0
                },
                {
                    "sent": "Um, no, I don't travel first class.",
                    "label": 0
                },
                {
                    "sent": "I just can't draw enough of these nodes, but you'd have nodes like this and you sort of.",
                    "label": 0
                },
                {
                    "sent": "You could imagine a grid that would be a simplest simple instance of it, and you can imagine somebody in the middle of that that's infected, so we would have sitting here.",
                    "label": 0
                },
                {
                    "sent": "We would have a function.",
                    "label": 0
                },
                {
                    "sent": "One of these compatibility functions.",
                    "label": 0
                },
                {
                    "sent": "Right, so how might this compatibility function look?",
                    "label": 0
                },
                {
                    "sent": "Well, there's different ways we might parameterise it, but one way we could is we could give every edge of weight.",
                    "label": 0
                },
                {
                    "sent": "Let's call that Omega and we could parameterise it like this.",
                    "label": 0
                },
                {
                    "sent": "This would be a simple parameterisation.",
                    "label": 0
                },
                {
                    "sent": "Right, So what this means is.",
                    "label": 0
                },
                {
                    "sent": "This corner would be let's say plus 1 + 1 and the bottom corner would be minus 1 -- 1, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm just listing the four numbers that you can possibly get out of a pair of binary variables.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "The Omega, of course, would have a physical meaning if we're talking about infectious diseases, what do you expect about Omega?",
                    "label": 0
                },
                {
                    "sent": "It would be 1.",
                    "label": 0
                },
                {
                    "sent": "Be large andwich sign.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's probably going to be large and positive, right?",
                    "label": 0
                },
                {
                    "sent": "Because if Omega is large positive than the two diagonals, the sort of both infected or both healthy are going to be very.",
                    "label": 0
                },
                {
                    "sent": "You're going to get a lot of weight on those in your factorization of the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so likely you would have.",
                    "label": 0
                },
                {
                    "sent": "If a mega was zero, you would sort of have no infection and beaten independence model.",
                    "label": 0
                },
                {
                    "sent": "Anifa Mega was less than one.",
                    "label": 0
                },
                {
                    "sent": "It could arise, but maybe not in disease modeling, but you have sort of an anti infectious model.",
                    "label": 0
                },
                {
                    "sent": "If you're diseased, then I'm likely to be healthy.",
                    "label": 0
                },
                {
                    "sent": "It's not likely for diseases, but you can also use this model to model the way that politicians vote, right?",
                    "label": 0
                },
                {
                    "sent": "You could say XYZ plus one if that person votes yes and minus one if they vote no.",
                    "label": 0
                },
                {
                    "sent": "And in that case a negative weight makes a lot of sense.",
                    "label": 0
                },
                {
                    "sent": "It just means that you have two people who disagree.",
                    "label": 0
                },
                {
                    "sent": "A lot of the time.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's do a little simulation just to look at what happens.",
                    "label": 0
                },
                {
                    "sent": "Um, let me see if I can get this right.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm showing you here.",
                    "label": 0
                },
                {
                    "sent": "A bit of Matlab code, but the important thing is this variable.",
                    "label": 0
                },
                {
                    "sent": "Here it's called sig E. It's it's the Omega term, so I'm setting that to be quite small.",
                    "label": 0
                },
                {
                    "sent": "It's going to .1.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": ".1 it's positive.",
                    "label": 0
                },
                {
                    "sent": "And we could run Gibbs sampling, for instance, to sample from this model, right?",
                    "label": 0
                },
                {
                    "sent": "And I'm sampling it from a grid which has 32 nodes by 32 nodes.",
                    "label": 0
                },
                {
                    "sent": "So you're sort of seeing a spatial pattern of plus and minus ones.",
                    "label": 0
                },
                {
                    "sent": "Right, so if it's .1 then there's sort of some infectious nature to the disease, but it's not highly infectious.",
                    "label": 0
                },
                {
                    "sent": "Whereas if I increase that infection probability to something like .5 four point 4, let's say do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Now you can see this.",
                    "label": 0
                },
                {
                    "sent": "Qualitatively it's changing.",
                    "label": 0
                },
                {
                    "sent": "It's getting much block here.",
                    "label": 0
                },
                {
                    "sent": "You're getting bigger pattern splotches of infected people.",
                    "label": 0
                },
                {
                    "sent": "Bigger splotches of uninfected people, and if we change it a lot, if we send it to .8, let's say, then we essentially well.",
                    "label": 0
                },
                {
                    "sent": "In that case, everybody except a few isolated individuals are infected.",
                    "label": 0
                },
                {
                    "sent": "So just with a single parameter, we're sort of getting a whole range.",
                    "label": 0
                },
                {
                    "sent": "There's a different sample now.",
                    "label": 0
                },
                {
                    "sent": "Everyone in this side basically got infected, and everyone on the top was healthy.",
                    "label": 0
                },
                {
                    "sent": "If we take black as being maybe black should be infected like the black plague.",
                    "label": 0
                },
                {
                    "sent": "Right, so you sort of get a range of behavior if you said it negative again, that wouldn't make so much sense for diseases.",
                    "label": 0
                },
                {
                    "sent": "So actually what kind of pattern do you expect?",
                    "label": 0
                },
                {
                    "sent": "If I have a negative wait here?",
                    "label": 0
                },
                {
                    "sent": "Right to be checkerboard.",
                    "label": 0
                },
                {
                    "sent": "Because the grid is bipartite, it sort of has two pieces.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's not a perfect checkerboard, but it's getting there, right?",
                    "label": 0
                },
                {
                    "sent": "If I'm healthy, that makes it unlikely.",
                    "label": 0
                },
                {
                    "sent": "Makes it more likely that you're infected, and vice versa.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a simple model.",
                    "label": 0
                },
                {
                    "sent": "And what we are running, there are no spoke about MCMC algorithms.",
                    "label": 0
                },
                {
                    "sent": "Do you speak about Gibbs sampling?",
                    "label": 0
                },
                {
                    "sent": "For instance, right?",
                    "label": 0
                },
                {
                    "sent": "So we're running just essentially a form of Gibbs sampling on this graph.",
                    "label": 0
                },
                {
                    "sent": "This is a reasonably small problem, just something like 1000 nodes.",
                    "label": 0
                },
                {
                    "sent": "So Gibbs sampling is OK, but on larger problems actually it does become hard to even sample from these.",
                    "label": 0
                },
                {
                    "sent": "So I'll skip this example.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let me just talk about one other application and then I'm going to come back to Hammersley Clifford.",
                    "label": 0
                },
                {
                    "sent": "How many people have heard about error correction coding?",
                    "label": 0
                },
                {
                    "sent": "Things like Turbo codes?",
                    "label": 0
                },
                {
                    "sent": "Interesting background.",
                    "label": 0
                },
                {
                    "sent": "How many people are actually in communications here?",
                    "label": 0
                },
                {
                    "sent": "Very few, but you sort of heard about it.",
                    "label": 0
                },
                {
                    "sent": "So error control codes just means they're all over there all in your computer in your laptops and your hard drive.",
                    "label": 0
                },
                {
                    "sent": "Your cell phones, all of them use error control codes.",
                    "label": 0
                },
                {
                    "sent": "Even the grocery store.",
                    "label": 0
                },
                {
                    "sent": "So whenever you scan something like a barcode, right?",
                    "label": 0
                },
                {
                    "sent": "If you use a scanner, if the scanner is misaligned, you might miss read the digits of the barcode.",
                    "label": 0
                },
                {
                    "sent": "So what barcodes actually have in them?",
                    "label": 0
                },
                {
                    "sent": "They have some extra parity checks.",
                    "label": 0
                },
                {
                    "sent": "They have some extra numbers that say this first, second, third number should sum up to something.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's zero and MoD 2.",
                    "label": 0
                },
                {
                    "sent": "That would be a simple example of a parity check code, right?",
                    "label": 0
                },
                {
                    "sent": "So it means that when the scanner is misaligned, even if it gets a few digits wrong, those parity checks allow it to correct or fix those errors.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that a lot of the best codes, the things that are actually used in practice.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are based on graphs.",
                    "label": 0
                },
                {
                    "sent": "Here I'm showing you a slightly different kind of graphical model, but something that you'll also see in the literature.",
                    "label": 0
                },
                {
                    "sent": "Something called a factor graph.",
                    "label": 1
                },
                {
                    "sent": "Right, so in a factor graph, you have nodes.",
                    "label": 0
                },
                {
                    "sent": "These are what we seen before.",
                    "label": 0
                },
                {
                    "sent": "These represent variables.",
                    "label": 0
                },
                {
                    "sent": "In the case we're talking about, these would represent things that are being transmitted.",
                    "label": 0
                },
                {
                    "sent": "For instance, when your cell phone works, uses radio waves, but it transmits essentially a sequence encoded form a sequence of zeros and ones.",
                    "label": 0
                },
                {
                    "sent": "So this is what's being transmitted, or a noisy version of this is being transmitted.",
                    "label": 0
                },
                {
                    "sent": "And what these boxes represent, these boxes represent parity checks, so this box is saying I'm going to look at 1535 and seven, and whatever you send, I wanted to be the case that adds up to zero and MoD two there should be an even number of ones.",
                    "label": 0
                },
                {
                    "sent": "Right, so how many?",
                    "label": 0
                },
                {
                    "sent": "So a code word is something that satisfies all of these parity checks, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a sequence of zeros and ones of length 7.",
                    "label": 0
                },
                {
                    "sent": "In this case, such that the 1st, third, 5th and 7th.",
                    "label": 0
                },
                {
                    "sent": "If you add them up is zero and MoD 2, and so on for the other 3 ones.",
                    "label": 0
                },
                {
                    "sent": "So how many code words?",
                    "label": 0
                },
                {
                    "sent": "How many sort of valid 01 sequences can you send in this case?",
                    "label": 0
                },
                {
                    "sent": "Alright, there's two to the seven possible sequences, but not all of them are valid, right?",
                    "label": 0
                },
                {
                    "sent": "This guy is not valid 001.",
                    "label": 0
                },
                {
                    "sent": "One is not valid.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "He must violate something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so he would violate this parity check because only seven is 1 and 5, three and one or zero.",
                    "label": 0
                },
                {
                    "sent": "So he has an even number of 1 so he would not be valid.",
                    "label": 0
                },
                {
                    "sent": "Right, so the total number of codewords is 2 to the four.",
                    "label": 0
                },
                {
                    "sent": "You started with two to seven.",
                    "label": 0
                },
                {
                    "sent": "You sort of had 7 degrees of freedom, and you lossed one for every parity check.",
                    "label": 0
                },
                {
                    "sent": "So 7 -- 3 you had three checks.",
                    "label": 0
                },
                {
                    "sent": "You end up with four, so in this case there's 16 codewords total that could be sent in a much larger space of two to 7.",
                    "label": 0
                },
                {
                    "sent": "Now, in practice, people don't do this just with seven variables.",
                    "label": 0
                },
                {
                    "sent": "They do it with something like 10,000 or 100,000 variables.",
                    "label": 0
                },
                {
                    "sent": "And what really has revolutionized and what's implemented in many of your devices is the sum product algorithm.",
                    "label": 0
                },
                {
                    "sent": "So how many people have heard of the sum product algorithm?",
                    "label": 0
                },
                {
                    "sent": "OK, you're OK.",
                    "label": 0
                },
                {
                    "sent": "I understand we're going to talk about it tomorrow.",
                    "label": 0
                },
                {
                    "sent": "It's a message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "What's important about it?",
                    "label": 0
                },
                {
                    "sent": "Message passing means that it does local operations at the nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "It sends things along the edges.",
                    "label": 0
                },
                {
                    "sent": "It's an extremely efficient algorithm, and it's very parallelizable.",
                    "label": 0
                },
                {
                    "sent": "So actually many of your cell phones are running a version of some product right now, and it's very, very fast, because obviously if you're speaking you want to decode the message in real time, you don't want to speak and then wait.",
                    "label": 0
                },
                {
                    "sent": "For a minute or so, until the message is decoded, it has to be done now.",
                    "label": 0
                },
                {
                    "sent": "So this message passing algorithm will talk about.",
                    "label": 0
                },
                {
                    "sent": "Let me just demonstrate it in practice for you on one of these codes.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing you here is.",
                    "label": 0
                },
                {
                    "sent": "So it's a little demonstration of how you might clean up an image, so I'm showing you a code that has roughly 10,000 bits, so it means it's a graphical model that has roughly 10,000 of those circular variables.",
                    "label": 0
                },
                {
                    "sent": "And the way I've built it, it's got 5000 of those square variables, those factor nodes, or those parity checks.",
                    "label": 0
                },
                {
                    "sent": "So it's a big graph.",
                    "label": 0
                },
                {
                    "sent": "Anne, what you observe is you observe this image which has been corrupted by some special noise.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do is we're going to use the sum product algorithm.",
                    "label": 0
                },
                {
                    "sent": "We're going to run it on the underlying graph.",
                    "label": 0
                },
                {
                    "sent": "We'll see tomorrow what it does.",
                    "label": 0
                },
                {
                    "sent": "And you'll see that it cleans up the image.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's running iterations and it's done.",
                    "label": 0
                },
                {
                    "sent": "So 14 iterations there on 10,000.",
                    "label": 0
                },
                {
                    "sent": "It essentially recovered the.",
                    "label": 0
                },
                {
                    "sent": "The code word in this case, which was the clean image.",
                    "label": 0
                },
                {
                    "sent": "It recovered it perfectly.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "No, no trigraphs.",
                    "label": 0
                },
                {
                    "sent": "That's the interesting thing.",
                    "label": 0
                },
                {
                    "sent": "So you'd like it to be a tree graph, because then sum product algorithm would be exact.",
                    "label": 0
                },
                {
                    "sent": "Will see this tomorrow, but it turns out that if you use trees to build codes, they're not good codes.",
                    "label": 0
                },
                {
                    "sent": "There are sort of bad codes in a sense that can be made precise, so all good codes things like Turbo codes.",
                    "label": 1
                },
                {
                    "sent": "What this is what I'm showing you is called a low density parity check code.",
                    "label": 0
                },
                {
                    "sent": "All good codes have cycles, so this is one of the reasons why belief propagation or some product on graphs with cycles.",
                    "label": 0
                },
                {
                    "sent": "This is probably.",
                    "label": 0
                },
                {
                    "sent": "The single largest success story of it in any application, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's it's in all of your devices as we speak, so it's key that it has cycles.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Let's just look at this code quickly.",
                    "label": 0
                },
                {
                    "sent": "Let me just explain what I was doing.",
                    "label": 0
                },
                {
                    "sent": "So what this means was when I was generating the noisy version, I was flipping six.",
                    "label": 0
                },
                {
                    "sent": "I was on average.",
                    "label": 0
                },
                {
                    "sent": "I was flipping a coin with probability .06 of making it a bad pixel, so I was corrupting only 6%.",
                    "label": 0
                },
                {
                    "sent": "6% of the pixels, so you might ask, well, you made it too easy so we can try a slightly harder problem.",
                    "label": 0
                },
                {
                    "sent": "I'll corrupt about 7 1/2%.",
                    "label": 0
                },
                {
                    "sent": "I don't know what's going to happen because it's random.",
                    "label": 0
                },
                {
                    "sent": "It may work, it may not.",
                    "label": 0
                },
                {
                    "sent": "This is sort of near the threshold of what this code can do.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so that worked if I ran it again it might not work at that level.",
                    "label": 0
                },
                {
                    "sent": "If I go too high, if I crop something like 10 or 12%, let's do 10% it will it will fail.",
                    "label": 0
                },
                {
                    "sent": "I think almost assuredly right, so now it's even more corrupted and will see that the algorithm because it's on a graph with cycles.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you see that it's sort of partly cleaning up a bit of the image, but now it's just kind of oscillating.",
                    "label": 0
                },
                {
                    "sent": "It's bouncing back and forth, so the graph is not on a tree, so we're going to see tomorrow.",
                    "label": 0
                },
                {
                    "sent": "It doesn't always converge, and in this case the problem is too hard for it failed to decode properly.",
                    "label": 0
                },
                {
                    "sent": "But that's not surprising, because for those of you know about Shannon theory, the Shannon limit for this problem.",
                    "label": 0
                },
                {
                    "sent": "No method, whatever you do, can do better than 11%.",
                    "label": 0
                },
                {
                    "sent": "So if you're at 10%, this code, about 8%, that's actually quite good, it's reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So I took the image and I padded it with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, essentially I did a linear algebraic trick.",
                    "label": 0
                },
                {
                    "sent": "The images some sequence of zeros and ones, but by shifting all the words by the image I can make it the all zeros vector and then it's a valid code word.",
                    "label": 0
                },
                {
                    "sent": "I can show you the code afterwards, it's just a little trick, so I took the image.",
                    "label": 0
                },
                {
                    "sent": "You're correct, I made it a code word that's tricky and then I added some redundancy bits in the check and what we were running with some product.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's doing about.",
                    "label": 0
                },
                {
                    "sent": "It's not optimized, but it's doing 10 iterations on something like 15,000 nodes and you can see it's reasonably fast.",
                    "label": 0
                },
                {
                    "sent": "People do use this algorithm in practice.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those are some applications I just happen to cherry pick those.",
                    "label": 0
                },
                {
                    "sent": "There's many others, right?",
                    "label": 0
                },
                {
                    "sent": "Natural language processing things like parts of speech tagging, graphical models are used all the time.",
                    "label": 0
                },
                {
                    "sent": "Statistical image processing, computer vision, things like image denoising, image segmentation, image disparity, computation, people use graphs all the time.",
                    "label": 0
                },
                {
                    "sent": "There they actually use grids like this often because those are good for modeling spatial things.",
                    "label": 0
                },
                {
                    "sent": "Some other examples phylogenetic's computational biology, genomics.",
                    "label": 0
                },
                {
                    "sent": "These all use graphical models as well.",
                    "label": 0
                },
                {
                    "sent": "So there's many applications and the applications depend both on somehow the simplicity that we get from the graph structure and also these algorithms that I'll talk about tomorrow.",
                    "label": 0
                },
                {
                    "sent": "What I'd like to do in this last part is.",
                    "label": 0
                },
                {
                    "sent": "I'd like to come back to the Hammersley Clifford theorem.",
                    "label": 0
                },
                {
                    "sent": "I'd like to drill down into that a little bit more.",
                    "label": 0
                },
                {
                    "sent": "We've sort of been high level, but I'd like to understand.",
                    "label": 0
                },
                {
                    "sent": "Why it is the case that if you have something that factorizes like this, it must satisfy all these conditional independence properties.",
                    "label": 0
                },
                {
                    "sent": "So let's let's start drilling down a little bit to understand that.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we remember the statement, the statement says if you start with the graph, whatever undirected graph you want, and you build a distribution that factorizes over the cliques of the graph.",
                    "label": 0
                },
                {
                    "sent": "Then that's the same as if you started with the graph and said that you had all these Markov properties holding.",
                    "label": 0
                },
                {
                    "sent": "So let's let's work through that One Direction of that argument.",
                    "label": 0
                },
                {
                    "sent": "Any questions before I sort of segue into that.",
                    "label": 0
                },
                {
                    "sent": "Hammersley and Clifford so they were, say probabilists at Oxford.",
                    "label": 0
                },
                {
                    "sent": "The original paper is actually unpublished.",
                    "label": 0
                },
                {
                    "sent": "I think you can get your hands on a scan version of it, but yeah, it's never been published, so they were probabilists, and they were sort of interested in essentially generalizations, Markov processes on chains play such an important role in probability, and they were interested in these kinds of generalizations to graphs.",
                    "label": 0
                },
                {
                    "sent": "So I think one of the first published proofs was by Julian Besag.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately passed away I think last year, but he did a lot of the pioneering work early work in the 70s and graphical models, among other things.",
                    "label": 0
                },
                {
                    "sent": "He was very good at naming his papers.",
                    "label": 0
                },
                {
                    "sent": "He had a great paper called the statistical analysis of Dirty Pictures.",
                    "label": 0
                },
                {
                    "sent": "By which he just meant image denoising exactly what I did with codes that you have pictures that are corrupted and he was going to be.",
                    "label": 0
                },
                {
                    "sent": "Noise them with the graphical model so.",
                    "label": 0
                },
                {
                    "sent": "It did, yeah, I think the British kind of have a naughty sense of humor.",
                    "label": 0
                },
                {
                    "sent": "They like those kinds of jokes.",
                    "label": 0
                },
                {
                    "sent": "Americans are just more crude, you know.",
                    "label": 0
                },
                {
                    "sent": "More direct about it.",
                    "label": 0
                },
                {
                    "sent": "Other nice proof, I think was a student Grimmett in 70.",
                    "label": 0
                },
                {
                    "sent": "Three had a very nice proof.",
                    "label": 0
                },
                {
                    "sent": "I believe he was a student of one of the two I don't remember, but um.",
                    "label": 0
                },
                {
                    "sent": "So let's let's do One Direction.",
                    "label": 0
                },
                {
                    "sent": "Anything else?",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "I don't know that to be honest.",
                    "label": 0
                },
                {
                    "sent": "I always wonder.",
                    "label": 0
                },
                {
                    "sent": "I feel it has to be an American.",
                    "label": 0
                },
                {
                    "sent": "It's not a Scandinavian because Scandinavians are not scandalized by unmarried parents having children so.",
                    "label": 0
                },
                {
                    "sent": "Maybe Perla, but I don't want to cast aspersions so.",
                    "label": 0
                },
                {
                    "sent": "Someone could search it and find out, I'm sure so.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let me just work through just because I think it's useful to do some of the technical work.",
                    "label": 0
                },
                {
                    "sent": "So let's show the following.",
                    "label": 0
                },
                {
                    "sent": "Let's show that the factorization property implies the Markov property on any graph.",
                    "label": 1
                },
                {
                    "sent": "Alright, so I'm not writing the statement out, but what we're trying to show is One Direction of this.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to assume is that you've got a distribution that factorizes in the way we've written there.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'm assuming that I've got my nice factorization, so I've got a bunch of N variables and it breaks into a product of terms over the cliques like I've written up there.",
                    "label": 0
                },
                {
                    "sent": "So I'll use a script C for the clique set an regular C for the clique.",
                    "label": 0
                },
                {
                    "sent": "And So what I want to show is.",
                    "label": 0
                },
                {
                    "sent": "I'd like to show that if I take a vertex cut set, let's say S an, I splits the graph into two parts.",
                    "label": 0
                },
                {
                    "sent": "A&B right, so I want to take a vertex cut set.",
                    "label": 0
                },
                {
                    "sent": "It splits the graph into two subsets, A&B.",
                    "label": 0
                },
                {
                    "sent": "Those are separate, and so I've sort of got three disjoint subsets AB&S.",
                    "label": 0
                },
                {
                    "sent": "Let me flip back.",
                    "label": 0
                },
                {
                    "sent": "We can just look at this picture right this pic.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure is sort of Canonical.",
                    "label": 0
                },
                {
                    "sent": "You've got three disjoint subsets, AB&S and so on.",
                    "label": 0
                },
                {
                    "sent": "The right here S splits the graph into two parts, and So what?",
                    "label": 1
                },
                {
                    "sent": "I'd like to show is I'd like to show that the conditional independence property holds.",
                    "label": 0
                },
                {
                    "sent": "OK, so to do that we should go back to the definition of conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Right, So what is conditional independence mean?",
                    "label": 0
                },
                {
                    "sent": "It means that while there's different definitions, but one way of thinking about it is that means that X of AXB we condition that on X of S then that should break into two pieces that should be X of a given X of S&X of B given access.",
                    "label": 0
                },
                {
                    "sent": "Right, that's what we'd like to show.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "To do that, what I'm going to do is I'm going to take the set of cliques right?",
                    "label": 0
                },
                {
                    "sent": "What we have to work with is we have this partitioning of the clique set.",
                    "label": 0
                },
                {
                    "sent": "Well, we have this factorization of the clique set.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do is I'm going to use these subsets of vertices AB&S to partition the cliques.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm going to define three sets of cliques.",
                    "label": 0
                },
                {
                    "sent": "Right, so the first set of cliques are cliques that have some intersection with a right?",
                    "label": 1
                },
                {
                    "sent": "So a is some subset, C is a clique, it's some other subset of nodes, and I want to look at those that intersect with a in a nontrivial way.",
                    "label": 0
                },
                {
                    "sent": "Same thing for script C of BI.",
                    "label": 0
                },
                {
                    "sent": "Want to look at cliques that intersect with being a nontrivial way.",
                    "label": 0
                },
                {
                    "sent": "Not equal to write.",
                    "label": 0
                },
                {
                    "sent": "I want that not to be empty and the last set is.",
                    "label": 0
                },
                {
                    "sent": "I want to look at cliques that belonged to the clique set an are subsets of the vertex cut set.",
                    "label": 0
                },
                {
                    "sent": "It's about three sets of cliques there.",
                    "label": 0
                },
                {
                    "sent": "Right, so in the graph that I've shown there, if I use the numberings on the left for instance, what is C of S?",
                    "label": 0
                },
                {
                    "sent": "See, this is the set of cliques that are contained within the separator.",
                    "label": 0
                },
                {
                    "sent": "Set the vertex cut set, so in that case only four five.",
                    "label": 1
                },
                {
                    "sent": "That's a clique, and it's of course contained.",
                    "label": 0
                },
                {
                    "sent": "I guess if we also four and five separately, those are not maximal cliques.",
                    "label": 0
                },
                {
                    "sent": "Those are contained within S as well.",
                    "label": 0
                },
                {
                    "sent": "OK, So what can we say about these three sets?",
                    "label": 0
                },
                {
                    "sent": "So they cover all the cliques in the graph Y.",
                    "label": 0
                },
                {
                    "sent": "Is that the claim is actually that they cover every clique is contained in one of those sets an exactly 1, so they form a disjoint cover of the clique set.",
                    "label": 0
                },
                {
                    "sent": "Now, that's not always true.",
                    "label": 0
                },
                {
                    "sent": "We have to use some property of S. We know something about S. That is true.",
                    "label": 0
                },
                {
                    "sent": "Right, so this I'm going to sort of leave it for you to think about on your own a little bit, but this essentially uses the fact that S is a vertex cut set or separator set.",
                    "label": 0
                },
                {
                    "sent": "Right, it means for one we can't have any direct edges between A&B.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can work through a little argument to show that these that every clique is contained in exactly one of these sets, and the sets are disjoint.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's useful to us.",
                    "label": 0
                },
                {
                    "sent": "Why's that useful?",
                    "label": 0
                },
                {
                    "sent": "It's useful because we have a factorization up here that's over all the cliques of the graph, and we can now split it into three separate factorizations, one for each of these terms, right?",
                    "label": 0
                },
                {
                    "sent": "So let's do that.",
                    "label": 0
                },
                {
                    "sent": "Speak dirty.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we use that disjoint partition, then what we can conclude is that our joint distribution is on my end variables in the graph.",
                    "label": 0
                },
                {
                    "sent": "I can break it into three terms.",
                    "label": 0
                },
                {
                    "sent": "Well, I have the normalization constant that's just to make things sum to one.",
                    "label": 0
                },
                {
                    "sent": "I've got a product overall clicks that are in the first set.",
                    "label": 0
                },
                {
                    "sent": "A product of cliques in the second subset indexed by B.",
                    "label": 0
                },
                {
                    "sent": "In a product of cliques in the third set indexed by S. Right, that's using the fact that I have a disjoint union of my cliques.",
                    "label": 0
                },
                {
                    "sent": "So let me give these things some names.",
                    "label": 0
                },
                {
                    "sent": "My letters got one.",
                    "label": 0
                },
                {
                    "sent": "What's mixed up?",
                    "label": 0
                },
                {
                    "sent": "Thank you, yes I was confused.",
                    "label": 0
                },
                {
                    "sent": "OK so sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "That should be 5 CXC so this is the product over the set of cliques indexed by S. What can I say about this thing which which variables can it depend on?",
                    "label": 0
                },
                {
                    "sent": "Right, it can only depend on the variables in S, so you can sort of think about this as some.",
                    "label": 0
                },
                {
                    "sent": "That's because every clique in this set is contained within S, so it can only depend on things in S, so we can think about this as some kind of super function.",
                    "label": 0
                },
                {
                    "sent": "Um, are there any?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm very good at destroying pens.",
                    "label": 0
                },
                {
                    "sent": "How many PHD's does it take to open up?",
                    "label": 0
                },
                {
                    "sent": "OK, so this guy is some big function.",
                    "label": 0
                },
                {
                    "sent": "Let's call him what is that?",
                    "label": 0
                },
                {
                    "sent": "That's a Phi Phi X of S right?",
                    "label": 0
                },
                {
                    "sent": "He depends only on the variables of an excess.",
                    "label": 0
                },
                {
                    "sent": "What can we say about this guy here?",
                    "label": 0
                },
                {
                    "sent": "The term indexed by a.",
                    "label": 0
                },
                {
                    "sent": "Right, he can only depend on things in A&S.",
                    "label": 0
                },
                {
                    "sent": "Right, that's again, using part of the Cutset property right?",
                    "label": 0
                },
                {
                    "sent": "There?",
                    "label": 0
                },
                {
                    "sent": "Can't be any connection between A&B, so there's no way that a bee term will somehow pollute me and this guy can only depend on.",
                    "label": 1
                },
                {
                    "sent": "I should get these different names.",
                    "label": 0
                },
                {
                    "sent": "This is S. This is a S and this is fi BS.",
                    "label": 0
                },
                {
                    "sent": "So that depends only on B&S.",
                    "label": 0
                },
                {
                    "sent": "OK, so who can tell me why that's useful?",
                    "label": 0
                },
                {
                    "sent": "So what we sort of done is we've just taken our factorization.",
                    "label": 0
                },
                {
                    "sent": "We've collapsed it into three terms, one for a, one for B and one for the overlap the cut set.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we remember what we're trying to prove right, what we're trying to prove is this guy right here.",
                    "label": 0
                },
                {
                    "sent": "We're trying to prove that when we condition on X of S that it becomes a product of two terms, so I won't write out the algebra.",
                    "label": 0
                },
                {
                    "sent": "I'll leave that for you guys, but essentially the proof is done because what happens now is if you take this and you condition, you're going to fix this.",
                    "label": 0
                },
                {
                    "sent": "This just becomes some constant that you don't care about.",
                    "label": 0
                },
                {
                    "sent": "You fix some of these variables and fix these variables here, but after conditioning what you have is just a separate product of A and terms indexed by B, so you can see exactly the splitting of.",
                    "label": 0
                },
                {
                    "sent": "After conditioning you get exactly the splitting that you wanted.",
                    "label": 0
                },
                {
                    "sent": "So somehow that decomposition that was the key in this particular argument.",
                    "label": 0
                },
                {
                    "sent": "And of course that decomposition that was using this claim that you should verify that we did in fact have a disjoint union that was using the fact that we had a vertex cut set.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that sort of shows us One Direction that shows us the factorization property.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Implies Markov, so just summarizing.",
                    "label": 0
                },
                {
                    "sent": "If we condition.",
                    "label": 0
                },
                {
                    "sent": "Then what you'll see is that PXAXB if we condition on X of S. If I write proportional 2 and I'm just going to drop things that depend only on S because those conditioning those are fixed, I'm going to get something that's basically proportional to feess.",
                    "label": 0
                },
                {
                    "sent": "XA Let's put bar because we're conditioning on it and I guess.",
                    "label": 0
                },
                {
                    "sent": "And VXB access right?",
                    "label": 0
                },
                {
                    "sent": "They'll be some constants floating around, but we don't care about those.",
                    "label": 0
                },
                {
                    "sent": "That's enough to show us that this thing breaks into the product that we wanted.",
                    "label": 0
                },
                {
                    "sent": "OK, so any questions about that?",
                    "label": 0
                },
                {
                    "sent": "OK, so that actually was the easier direction as you might have thought.",
                    "label": 0
                },
                {
                    "sent": "Probably just in the interest of time.",
                    "label": 0
                },
                {
                    "sent": "I won't prove the other direction 'cause it would take a fair bit of work, but I can sort of tell you how it works.",
                    "label": 0
                },
                {
                    "sent": "The other direction is a bit harder, so if you look at the assumption, the assumption of the theorem include that you have a strictly positive distribution.",
                    "label": 0
                },
                {
                    "sent": "But this argument, I mean, you might worry a little bit about dividing by zero when I condition, but if you look carefully at this argument, it actually holds for any distribution it holds for distributions that might assign zero probability to some configurations.",
                    "label": 0
                },
                {
                    "sent": "But the reverse step, if you have a Markov, if you have the Markov property, the factorization property doesn't always hold, and less the distribution is strictly positive.",
                    "label": 0
                },
                {
                    "sent": "So if you look in the book by Lauritsen, it's a book called Graphical Models.",
                    "label": 0
                },
                {
                    "sent": "You can find all these sort of weird kind of counterexamples.",
                    "label": 0
                },
                {
                    "sent": "What the proof uses is it uses something called the Mobius inversion formula.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is what I said.",
                    "label": 0
                },
                {
                    "sent": "It certainly needs this assumption that P of X is bigger than 0.",
                    "label": 0
                },
                {
                    "sent": "It's it's a nice proof.",
                    "label": 0
                },
                {
                    "sent": "It's kind of combinatorial.",
                    "label": 0
                },
                {
                    "sent": "It uses something called Mobius inversion.",
                    "label": 0
                },
                {
                    "sent": "And, um, I won't go into details on it here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You can, you can read it.",
                    "label": 0
                },
                {
                    "sent": "The proof of this.",
                    "label": 0
                },
                {
                    "sent": "So in the directory that I set up for this course on the first page of your slides, there's a web address.",
                    "label": 0
                },
                {
                    "sent": "I left a set of introductory lectures that actually will go will review this proof that I just did, but also state the Mobius inversion and go for the backwards direction too.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested, you can have a look at that if you haven't ever seen it, I think it's worth looking at just once, because well, historically it's interesting, and it's a nice argument.",
                    "label": 0
                },
                {
                    "sent": "It's nice to understand.",
                    "label": 0
                },
                {
                    "sent": "Why these two things are equivalent?",
                    "label": 0
                },
                {
                    "sent": "OK, so any questions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I'd like to start talking about now and what we'll spend a lot of tomorrow morning talking about is.",
                    "label": 0
                },
                {
                    "sent": "More the kinds of computational questions that arise when you apply a graphical model.",
                    "label": 0
                },
                {
                    "sent": "Right, and we've already seen some instances of the kinds of computational problems that you'd like to solve.",
                    "label": 0
                },
                {
                    "sent": "Um, one of them that you'd like to solve is basically a.",
                    "label": 0
                },
                {
                    "sent": "It's sort of simply stated.",
                    "label": 0
                },
                {
                    "sent": "It's basically just a giant summation or a giant integral.",
                    "label": 0
                },
                {
                    "sent": "Alright, I haven't said much so far about what this mysterious Z quantity IS0Z, right, but if you sort of, I said that it's the thing that makes the distribution normalize.",
                    "label": 0
                },
                {
                    "sent": "And so if you had a discrete set of variables, that would be a sum.",
                    "label": 0
                },
                {
                    "sent": "If you had continuous ones, it would be an integral, it's just a constant that you get by summing or integrating over all configurations.",
                    "label": 0
                },
                {
                    "sent": "This product of all the terms on your cliques.",
                    "label": 0
                },
                {
                    "sent": "So it seems kind of boring, but in any learning problem that thing will be something like the log likelihood of your data.",
                    "label": 0
                },
                {
                    "sent": "If you take the log of it, it would be the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "So things like maximum likelihood or expectation maximization.",
                    "label": 0
                },
                {
                    "sent": "All of these things care very much.",
                    "label": 0
                },
                {
                    "sent": "They are only efficient if you are able to compute this quantity quickly.",
                    "label": 0
                },
                {
                    "sent": "Alright, so it looks kind of innocuous, but um, it's actually very important.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we think about the complexity even in the discrete case, you can see it's exponential, right?",
                    "label": 0
                },
                {
                    "sent": "Because even if you just had two states, a binary variable at every node, you have two to the end summation.",
                    "label": 0
                },
                {
                    "sent": "So that thing if you think about it naively, is very hard.",
                    "label": 0
                },
                {
                    "sent": "What we're going to see is that if you're clever about it, if you exploit the graph structure when you have trees, then you can do this exactly.",
                    "label": 0
                },
                {
                    "sent": "You can do it by essentially.",
                    "label": 0
                },
                {
                    "sent": "It's essentially a form of dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "It's known as the Max product or the sum product algorithm, and that's what we'll see when you have cycles.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite hard.",
                    "label": 0
                },
                {
                    "sent": "There's many complexity theoretic results in the literature that tell you that it's very unlikely that people are going to be able to compute these sums in general exactly for general problems.",
                    "label": 0
                },
                {
                    "sent": "OK, the other problem that you might be interested in is also a summation or an integration problem.",
                    "label": 0
                },
                {
                    "sent": "Let me just go back to the examples that we had.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Right, so when we were talking about speech recognition or we were talking about predicting typewriter sequences on your on your laptop or your iPad, what you'd like to do is you maybe have observed all the keystrokes up to some time T and you'd like to predict the next time T + 1.",
                    "label": 0
                },
                {
                    "sent": "So what you'd like to do is marginalized efficiently.",
                    "label": 0
                },
                {
                    "sent": "You'd like to sum or integrate over all these variables to try and predict what the marginal distribution is at that node, right?",
                    "label": 0
                },
                {
                    "sent": "So it's again a kind of summation problem.",
                    "label": 0
                },
                {
                    "sent": "It's a summation or an integration problem.",
                    "label": 0
                },
                {
                    "sent": "What's another problem that we might want to solve in this case?",
                    "label": 0
                },
                {
                    "sent": "How many people know about sequence alignment problems in computational biology?",
                    "label": 0
                },
                {
                    "sent": "OK, So what kind of problem might you want to solve in that area?",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "You could do that, but maybe let's let's be more concrete, you might like to do a sequence alignment problem.",
                    "label": 0
                },
                {
                    "sent": "You might like to search efficiently overall alignments and match alignments you might like to find the alignment that maximizes the probability of being matched under some probability model, so problems like sequence alignment.",
                    "label": 0
                },
                {
                    "sent": "Many of them you can formulate them as you're essentially building a graphical model over sequences.",
                    "label": 0
                },
                {
                    "sent": "Ann, you're penalizing certain transitions.",
                    "label": 0
                },
                {
                    "sent": "You're saying some transitions are more likely than others, and what you'd like to do.",
                    "label": 0
                },
                {
                    "sent": "It's kind of like a batch computation.",
                    "label": 0
                },
                {
                    "sent": "It's not an online computation you'd like to search overall sequences over the entire length of the sequence you'd like to search all sequences and maximize, find the sequence that has the highest probability of match of matching the observed data.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's like this problem.",
                    "label": 0
                },
                {
                    "sent": "I defined it's instead.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The summation that's a problem down here right?",
                    "label": 0
                },
                {
                    "sent": "In this problem, we wanted to sum over all the variables.",
                    "label": 0
                },
                {
                    "sent": "In this problem, we want to maximize over all of the variables, so there's sort of two different types of problems.",
                    "label": 0
                },
                {
                    "sent": "Those that involve summations and those that involve maximization's.",
                    "label": 0
                },
                {
                    "sent": "And it sort of depends on your application.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you might want to compute the likelihood of data, or you might want to compute a marginal distribution to make predictions.",
                    "label": 0
                },
                {
                    "sent": "In other times, you might want to search the whole space to find the best alignment, or for computer vision you might want to search over all pairs of images to find the one that best matches the depth between the two.",
                    "label": 0
                },
                {
                    "sent": "So those are more search problems when you have a maximization.",
                    "label": 0
                },
                {
                    "sent": "OK, so both sets of problems are hard in general.",
                    "label": 0
                },
                {
                    "sent": "This one is hard because you have again an exponential number of summations, or you have a very high dimensional integral.",
                    "label": 0
                },
                {
                    "sent": "Right integrals are very easy if they're in one dimension or two, but they rapidly become very hard if you have higher dimensional ones.",
                    "label": 0
                },
                {
                    "sent": "Right, so these are generalizations of the problems that I think our node would have spoken about.",
                    "label": 0
                },
                {
                    "sent": "He's probably spoke about the problem of filtering that you have a sequence, something like a timeseries evolving overtime and you want to compute or approximate the marginal distribution given all the data up to time T. That would be this problem here.",
                    "label": 0
                },
                {
                    "sent": "This marginalization problem if you were on a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "We're going to be interested in it for more general graphs.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I go back to this example so that that demo that I showed you what was I doing or what was the algorithm approximately doing?",
                    "label": 0
                },
                {
                    "sent": "Really, there were extra nodes here.",
                    "label": 0
                },
                {
                    "sent": "You were observing noisy versions, right?",
                    "label": 0
                },
                {
                    "sent": "You had the clean image that was sitting there.",
                    "label": 0
                },
                {
                    "sent": "You didn't see that.",
                    "label": 0
                },
                {
                    "sent": "You saw a noisy image, so there'd be some shaded nodes there that show what you observe and what that algorithm was doing was it was approximately summing very quickly.",
                    "label": 0
                },
                {
                    "sent": "Overall, code words that are possible, and for every node it was finding out is it more likely to be a one, or to be a 0.",
                    "label": 0
                },
                {
                    "sent": "So it was computing the marginal at every node, and if it was more likely to be a one then it declared A1 and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So that's what that algorithm was doing.",
                    "label": 0
                },
                {
                    "sent": "It wasn't doing it exactly because as somebody mentioned, this graph actually has cycles, but if it had been a tree.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that will present will do it exactly and do it very efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so those are the problems we'd like to solve.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me come back to this tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About variational methods.",
                    "label": 0
                },
                {
                    "sent": "Let me just start talking about the Max product algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK so the Max product algorithm.",
                    "label": 0
                },
                {
                    "sent": "How many people know about the Viterbi algorithm?",
                    "label": 0
                },
                {
                    "sent": "OK, so the verb algorithm is a special case of the Max product algorithm.",
                    "label": 0
                },
                {
                    "sent": "One reason graphical models are useful is because they essentially generalize many classical algorithms.",
                    "label": 0
                },
                {
                    "sent": "There's a list of at least 10 to 15 algorithms that are special cases of what we're going to do now, so if you've heard of atterby, that's a special case.",
                    "label": 0
                },
                {
                    "sent": "Forward.",
                    "label": 0
                },
                {
                    "sent": "Backward is a special case.",
                    "label": 0
                },
                {
                    "sent": "Alpha Beta is a special case.",
                    "label": 0
                },
                {
                    "sent": "Common filtering is a special case.",
                    "label": 0
                },
                {
                    "sent": "Fast Fourier Transform is a special case.",
                    "label": 0
                },
                {
                    "sent": "You can essentially go down and name many influential algorithms there.",
                    "label": 0
                },
                {
                    "sent": "All this a special case of what we're going to do now that's part of the utility of graphical models.",
                    "label": 0
                },
                {
                    "sent": "They somehow allowed you see what the key ideas in all of these prob.",
                    "label": 0
                },
                {
                    "sent": "Zara OK so Max product.",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 0
                },
                {
                    "sent": "It's an algorithm for trying to solve.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of problem #3 right?",
                    "label": 0
                },
                {
                    "sent": "It's trying to solve this maximization problems we have.",
                    "label": 0
                },
                {
                    "sent": "What we'd like to do is maximize over all the nodes of the graph.",
                    "label": 0
                },
                {
                    "sent": "This product of terms.",
                    "label": 0
                },
                {
                    "sent": "That's where it gets its name.",
                    "label": 0
                },
                {
                    "sent": "You'll see in a minute because it has a Max Anna product and it the algorithm.",
                    "label": 0
                },
                {
                    "sent": "All it does is play games with those two.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Arms.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let me do this concrete example.",
                    "label": 0
                },
                {
                    "sent": "OK so Max product.",
                    "label": 0
                },
                {
                    "sent": "Right, so the simple example I'm showing you, we've got three nodes, 1, two and three, so we got.",
                    "label": 0
                },
                {
                    "sent": "Three variables, and so I know that I have a factorization over these three variables.",
                    "label": 0
                },
                {
                    "sent": "I'm going to write it in a certain way because it's useful to me.",
                    "label": 0
                },
                {
                    "sent": "Later, I'm going to write it as.",
                    "label": 0
                },
                {
                    "sent": "Proportional to a product of three terms, one for every node.",
                    "label": 0
                },
                {
                    "sent": "It should be an S sorry.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to have.",
                    "label": 0
                },
                {
                    "sent": "In my case, I'm just going to have two edges.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I've done here is you can either write these things as compatibility functions as size, or if they're strictly positive, you could always write them as the exponential of some other function, which I'll call Theta.",
                    "label": 0
                },
                {
                    "sent": "The reason I'm going to do that is it when we talk about approximate methods tomorrow it's more convenient, often to work with these thetas.",
                    "label": 0
                },
                {
                    "sent": "If you know what an exponential family is, then it shows you that this is just an instance of an exponential family.",
                    "label": 0
                },
                {
                    "sent": "A physicist would call this sort of the Gibbs representation of the distribution.",
                    "label": 0
                },
                {
                    "sent": "For those of you who are have some physics background.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we want to do is we want to compute a maximum we want.",
                    "label": 0
                },
                {
                    "sent": "We'd like to maximize over these three terms.",
                    "label": 0
                },
                {
                    "sent": "Right, so if we think about it.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So I've got X one X2 and X3.",
                    "label": 0
                },
                {
                    "sent": "We can sort of do this successively, maybe first all Max over X2 or first do it over X3.",
                    "label": 0
                },
                {
                    "sent": "If I maximize over X3.",
                    "label": 0
                },
                {
                    "sent": "What do we know about Max and product?",
                    "label": 0
                },
                {
                    "sent": "Right so Max and products are interchangeable.",
                    "label": 0
                },
                {
                    "sent": "You can sort of distribute Maxima over products and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So in this case, all I'm saying is you have a maximum over X3, But you can basically ignore any term, for instance PSI one of X1 has no dependence on X3, so you can push the maximization inside right?",
                    "label": 0
                },
                {
                    "sent": "So all we're doing is we're using the fact that maximization distributes.",
                    "label": 0
                },
                {
                    "sent": "So what I can do is I can just extract the terms that depend on PSI three.",
                    "label": 0
                },
                {
                    "sent": "In this case they are.",
                    "label": 0
                },
                {
                    "sent": "Or X3 so I would get a size 3X3 and also Sai 23X2X3 those terms depend on X3.",
                    "label": 0
                },
                {
                    "sent": "And now the other terms that I have.",
                    "label": 0
                },
                {
                    "sent": "This is proportional to.",
                    "label": 0
                },
                {
                    "sent": "Other terms I have left or side 1X1 side 2X2.",
                    "label": 0
                },
                {
                    "sent": "Anssi 12X1X2.",
                    "label": 0
                },
                {
                    "sent": "Right, so that the key here, though, is these curly braces mean the Max is now only over a limited set of terms, right?",
                    "label": 0
                },
                {
                    "sent": "It's only over 2 terms, one that sits at node 3 right there, and one that sits on the edge between 2:00 and 3:00.",
                    "label": 0
                },
                {
                    "sent": "OK, so after I do this operation.",
                    "label": 0
                },
                {
                    "sent": "What do I get?",
                    "label": 0
                },
                {
                    "sent": "I get if you said a function of X1 and X2 it's.",
                    "label": 0
                },
                {
                    "sent": "It is a function.",
                    "label": 0
                },
                {
                    "sent": "It is a function of X1 and X2, but actually even simpler.",
                    "label": 0
                },
                {
                    "sent": "It's only a function of X2 in this case.",
                    "label": 0
                },
                {
                    "sent": "Right, because if there had been an edge between three and one, there would have been an extra term and it would have been a function of X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "But because there's no edge, this is only a function of X2.",
                    "label": 0
                },
                {
                    "sent": "So the way you should think about this, you sort of think I put a number in there and I solve this maximization problem.",
                    "label": 0
                },
                {
                    "sent": "I get a number I can do that for every value of X2 and I get a function of X2.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this function has a name.",
                    "label": 0
                },
                {
                    "sent": "It's called a message and.",
                    "label": 0
                },
                {
                    "sent": "You might call it the message that's passed from three to two.",
                    "label": 0
                },
                {
                    "sent": "Right, because it's a function of X2.",
                    "label": 0
                },
                {
                    "sent": "Right, So what we're going to think about when the algorithm works here?",
                    "label": 0
                },
                {
                    "sent": "I'm just writing algebra, but when the algorithm is working the way it's going to be working is that X3 is going to be doing the that maximization that I just said.",
                    "label": 0
                },
                {
                    "sent": "And it's going to be passing the result of its computation as a message from three to two.",
                    "label": 0
                },
                {
                    "sent": "So compute and pass.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called message.",
                    "label": 0
                },
                {
                    "sent": "Passing.",
                    "label": 0
                },
                {
                    "sent": "Messages are just functions of random variables that are summarizing what X2 needs to know.",
                    "label": 0
                },
                {
                    "sent": "OK, what should I maximize over next if I want to make my life easy?",
                    "label": 0
                },
                {
                    "sent": "Someone said X one, so why is X1 better somehow?",
                    "label": 0
                },
                {
                    "sent": "Well, it's better because of symmetry in this case, but in general it's there's a general algorithm that I'll talk about tomorrow elimination algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's sort of proceeds from the outer boundary of the graph, and it eliminates nodes by maximizing their, summing them, and it strips the graph down to a single node.",
                    "label": 0
                },
                {
                    "sent": "So I could just as easily do this same operation.",
                    "label": 0
                },
                {
                    "sent": "Let's use a different color.",
                    "label": 0
                },
                {
                    "sent": "I could maximize this guy over X1.",
                    "label": 0
                },
                {
                    "sent": "And what's nice now is that maximum passes into here.",
                    "label": 0
                },
                {
                    "sent": "And I haven't ordered my terms properly.",
                    "label": 0
                },
                {
                    "sent": "Let's erase this 'cause it doesn't depend on.",
                    "label": 0
                },
                {
                    "sent": "X1 and put it outside.",
                    "label": 0
                },
                {
                    "sent": "Right, so I can now do another maximization over X1.",
                    "label": 0
                },
                {
                    "sent": "And now I have a message.",
                    "label": 0
                },
                {
                    "sent": "This message is a function of X2 as well.",
                    "label": 0
                },
                {
                    "sent": "And this guy also flows in two X2.",
                    "label": 0
                },
                {
                    "sent": "Right, so the way you can think about it now is that no two basically now has all the information that he or she needs to solve the problem because node two can just do a maximum over X2 of these two quantities of the message from three to two X2.",
                    "label": 0
                },
                {
                    "sent": "The other guy.",
                    "label": 0
                },
                {
                    "sent": "One to two X2 and then the last thing is this piece of information his own local information or local evidence.",
                    "label": 0
                },
                {
                    "sent": "He needs to maximize over that.",
                    "label": 0
                },
                {
                    "sent": "And the problem is solved.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "For this example, if you summed out over X2 first, if you think about the key thing in this argument, was the messages that I create.",
                    "label": 0
                },
                {
                    "sent": "How big are they?",
                    "label": 0
                },
                {
                    "sent": "If I somehow over X2 first, so I sum out X2, I end up with a function of X1 and X3.",
                    "label": 0
                },
                {
                    "sent": "So instead of just being a vector, it would now be a matrix.",
                    "label": 0
                },
                {
                    "sent": "Now, that doesn't seem bad, because this is a toy example, but I could make a very bad graph such that if you here is perhaps the worst graph that you could imagine.",
                    "label": 0
                },
                {
                    "sent": "This is also a tree.",
                    "label": 0
                },
                {
                    "sent": "But it's like a star.",
                    "label": 0
                },
                {
                    "sent": "Right, so if you wanted to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "The right thing to do would be to pick one of the leaves.",
                    "label": 0
                },
                {
                    "sent": "These are the leaves of the tree.",
                    "label": 0
                },
                {
                    "sent": "For obvious reasons.",
                    "label": 0
                },
                {
                    "sent": "You would sum that out and you get a message that filters up a message that filters up a message that filters up, and those would all be vectors.",
                    "label": 0
                },
                {
                    "sent": "They'd be very compact.",
                    "label": 0
                },
                {
                    "sent": "If I summed this Sky out, it would have a disastrous effect.",
                    "label": 0
                },
                {
                    "sent": "It would couple all of these guys together.",
                    "label": 0
                },
                {
                    "sent": "You would get a giant message, in this case on five variables I could have made it a star with 100 variables, But you would get a very large dimensional message and you eliminate all the sort of savings that we're after.",
                    "label": 0
                },
                {
                    "sent": "What we're after, and we're going to drive tomorrow is for any tree we're going to have an algorithm that all it does at every round.",
                    "label": 0
                },
                {
                    "sent": "Every node does a little bit of summation and passes a single vector along every edge, right?",
                    "label": 0
                },
                {
                    "sent": "So that's important because the things being passed or very easy to store and very easy to pass.",
                    "label": 0
                },
                {
                    "sent": "You don't want to pass messages that are huge tables, right?",
                    "label": 0
                },
                {
                    "sent": "Huge table.",
                    "label": 0
                },
                {
                    "sent": "If you had 100 variables in the leaves and you sum that guy out, you have to pass something that was maybe 2 to the 100 dimensional that's.",
                    "label": 0
                },
                {
                    "sent": "That's definitely not what we want.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Right, So what we've done here?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What we've done here is basically just organize the structure of our computation right?",
                    "label": 0
                },
                {
                    "sent": "We've just played simple games with maximum that we can move.",
                    "label": 0
                },
                {
                    "sent": "For instance, this maximum can get moved in effect only those terms.",
                    "label": 0
                },
                {
                    "sent": "The way to think about it is what we're doing is we're structuring the computation in the right way that we have to do the minimal number of maximization's and products.",
                    "label": 0
                },
                {
                    "sent": "If we don't structured in this way, right?",
                    "label": 0
                },
                {
                    "sent": "If you're naive about it and you didn't structure, then you have to do 8 maximization's.",
                    "label": 0
                },
                {
                    "sent": "If you had binary variables and every time you have to do a product if you do a little bit of algebra discounting operations, you will see that this is already starting to save us.",
                    "label": 0
                },
                {
                    "sent": "Maybe not so much because it was an example with three nodes, but.",
                    "label": 0
                },
                {
                    "sent": "Example, if you imagine a much longer chain like you would have if you were doing a parsing problem, you can see that the computer the savings are going to get exponential we're going to drop the complexity from being exponential in the graph size to being linear in the graph size.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So wrap up now.",
                    "label": 0
                },
                {
                    "sent": "'cause probably everyone is hungry, but what you might want to think about is.",
                    "label": 0
                },
                {
                    "sent": "I've sort of given you the first building block.",
                    "label": 0
                },
                {
                    "sent": "I did three nodes for you.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple graph.",
                    "label": 0
                },
                {
                    "sent": "Now you want to sort of think if I had a somewhat larger graph with five nodes and I had additional people beyond three, I need to play the same game.",
                    "label": 0
                },
                {
                    "sent": "I need to structure my Max and my products in the right way and I need to pass messages.",
                    "label": 0
                },
                {
                    "sent": "And if you play games, you'll find out that you can derive.",
                    "label": 0
                },
                {
                    "sent": "Rule that tells you how the messages should be updated.",
                    "label": 0
                },
                {
                    "sent": "And the way they should be updated is you should take messages that are incoming to like these red guys.",
                    "label": 0
                },
                {
                    "sent": "You should multiply them.",
                    "label": 0
                },
                {
                    "sent": "That's a product and then you should do a maximization that involves your local evidence and the evidence on the edges.",
                    "label": 0
                },
                {
                    "sent": "So that update right there for this simple graph, that's called the Max product update, right?",
                    "label": 0
                },
                {
                    "sent": "It's sort of clear why it's a maximum and you have a product that if you did it on a chain, that's exactly what the Viterbi algorithm is doing, except the Viterbi algorithm is doing it.",
                    "label": 0
                },
                {
                    "sent": "Usually they take the log of the problem, and they do something like the Max sum algorithm or them in some algorithm.",
                    "label": 0
                },
                {
                    "sent": "But they're all the same.",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter whether you maximize a product or if you maximize the log of the product, that's.",
                    "label": 0
                },
                {
                    "sent": "Course the same thing.",
                    "label": 0
                },
                {
                    "sent": "OK so tomorrow will finish driving that this sort of simple update rule and we'll see that it's exact on trees.",
                    "label": 0
                },
                {
                    "sent": "But then we're going to want to understand.",
                    "label": 0
                },
                {
                    "sent": "It's also used on graphs with cycles where it need not be exact, but it can often behave well, so we'd like to try and understand that a little bit tomorrow.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                }
            ]
        }
    }
}