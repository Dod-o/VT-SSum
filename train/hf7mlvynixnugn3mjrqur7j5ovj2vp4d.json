{
    "id": "hf7mlvynixnugn3mjrqur7j5ovj2vp4d",
    "title": "A PTAS for Agnostically Learning Halfspaces",
    "info": {
        "author": [
            "Amit Daniely, Einstein Institute of Mathematics, The Hebrew University of Jerusalem"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_daniely_agnostically_learning/",
    "segmentation": [
        [
            "So thank you.",
            "So I said the title suggests the main result of this paper is polynomial time approximation scheme for agnostically learning half process with respect to the uniform distribution."
        ],
        [
            "So we consider the standard problem of agnostically learning halfspaces.",
            "We have an access to labeled examples in our Anna and our goal is to output a classifier whose 01 error is at most there of the optimal Hospice classifier.",
            "Plus some access error epsilon and you want to do that in time that is polynomial in the dimension in N and in one over epsilon.",
            "So as you all know, this is a truly basic problem in machine learning, but unfortunately there is an extensive evidence that this problem is hard."
        ],
        [
            "So one natural thing we can do is to consider approximation algorithms.",
            "We can fix some Alpha that is strictly greater than one an require our algorithm to return a classifier with error is at most Alpha times opt.",
            "Unfortunately, even for approximation algorithm agnostic learning spaces is hard.",
            "The best known approximation algorithm has quite terrible approximation ratio of N. The dimension, and there is a certain evidence that doing much better than that is impossible.",
            "So fat man in Thailand or it'll show that for proper algorithm in a very recent results also showed it for improper algorithm and will say few words about that in the impromptu."
        ],
        [
            "So we need to assume, oh, and one mathematically natural such assumption is the distribution is uniform on the N dimensional sphere.",
            "So this is admittedly a not very realistic assumption, but I think that it is a natural and clean assumption.",
            "So it might seem like a basic thing that might be useful to understand, and it also might derive new algorithms, improve techniques, and so on.",
            "So let's see what is known for agnostically learning halfspaces with respect to the uniform distribution.",
            "So if you insist on exact algorithm that is allowed teams with approximation ratio of 1, then the problem is probably still hard.",
            "It was recently shown by clients and Kothari.",
            "But now we have nontrivial approximation algorithms, so the first one was due to.",
            "Clients Coloumn Answer Answer video and they show an algorithm with an approximation ratio of square root of log of one over opt.",
            "And the way they did it, by the way, is using regression.",
            "Last year it was stable can and long show an algorithm with a constant approximation ratio.",
            "They did it by introducing a new techniques called localization.",
            "But as you can see, unfortunately the cost the constant was a relatively large with 64 and a natural question is whether we can do better than that.",
            "And the main result of this work is to show that indeed we can, so we get an optimal, probably optimal approximation ratio.",
            "We show that for every Alpha that is great and the one there is an efficient learning algorithm with approximation ratio of Alpha, which is by Clarence inquiry, probably optimal and one word about the proof and the algorithm.",
            "The way we do that is in some sense to combine the algorithm of.",
            "Yes, with the algorithm of able and for more details you're welcome to talk to me or come to the poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you.",
                    "label": 0
                },
                {
                    "sent": "So I said the title suggests the main result of this paper is polynomial time approximation scheme for agnostically learning half process with respect to the uniform distribution.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we consider the standard problem of agnostically learning halfspaces.",
                    "label": 1
                },
                {
                    "sent": "We have an access to labeled examples in our Anna and our goal is to output a classifier whose 01 error is at most there of the optimal Hospice classifier.",
                    "label": 1
                },
                {
                    "sent": "Plus some access error epsilon and you want to do that in time that is polynomial in the dimension in N and in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "So as you all know, this is a truly basic problem in machine learning, but unfortunately there is an extensive evidence that this problem is hard.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one natural thing we can do is to consider approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "We can fix some Alpha that is strictly greater than one an require our algorithm to return a classifier with error is at most Alpha times opt.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, even for approximation algorithm agnostic learning spaces is hard.",
                    "label": 1
                },
                {
                    "sent": "The best known approximation algorithm has quite terrible approximation ratio of N. The dimension, and there is a certain evidence that doing much better than that is impossible.",
                    "label": 1
                },
                {
                    "sent": "So fat man in Thailand or it'll show that for proper algorithm in a very recent results also showed it for improper algorithm and will say few words about that in the impromptu.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we need to assume, oh, and one mathematically natural such assumption is the distribution is uniform on the N dimensional sphere.",
                    "label": 0
                },
                {
                    "sent": "So this is admittedly a not very realistic assumption, but I think that it is a natural and clean assumption.",
                    "label": 1
                },
                {
                    "sent": "So it might seem like a basic thing that might be useful to understand, and it also might derive new algorithms, improve techniques, and so on.",
                    "label": 0
                },
                {
                    "sent": "So let's see what is known for agnostically learning halfspaces with respect to the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "So if you insist on exact algorithm that is allowed teams with approximation ratio of 1, then the problem is probably still hard.",
                    "label": 0
                },
                {
                    "sent": "It was recently shown by clients and Kothari.",
                    "label": 0
                },
                {
                    "sent": "But now we have nontrivial approximation algorithms, so the first one was due to.",
                    "label": 0
                },
                {
                    "sent": "Clients Coloumn Answer Answer video and they show an algorithm with an approximation ratio of square root of log of one over opt.",
                    "label": 1
                },
                {
                    "sent": "And the way they did it, by the way, is using regression.",
                    "label": 0
                },
                {
                    "sent": "Last year it was stable can and long show an algorithm with a constant approximation ratio.",
                    "label": 0
                },
                {
                    "sent": "They did it by introducing a new techniques called localization.",
                    "label": 0
                },
                {
                    "sent": "But as you can see, unfortunately the cost the constant was a relatively large with 64 and a natural question is whether we can do better than that.",
                    "label": 0
                },
                {
                    "sent": "And the main result of this work is to show that indeed we can, so we get an optimal, probably optimal approximation ratio.",
                    "label": 0
                },
                {
                    "sent": "We show that for every Alpha that is great and the one there is an efficient learning algorithm with approximation ratio of Alpha, which is by Clarence inquiry, probably optimal and one word about the proof and the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The way we do that is in some sense to combine the algorithm of.",
                    "label": 0
                },
                {
                    "sent": "Yes, with the algorithm of able and for more details you're welcome to talk to me or come to the poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}