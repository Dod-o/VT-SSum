{
    "id": "rvp3wo67ei3y6hryj5asikd22vuayqhx",
    "title": "Optimally Combining Classifiers Using Unlabeled Data",
    "info": {
        "author": [
            "Akshay Balsubramani, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_balsubramani_unlabeled_data/",
    "segmentation": [
        [
            "Long last name this is this is work with with your front."
        ],
        [
            "So first the setting we're looking at binary classification.",
            "Given an ensemble of hypothesis classifiers, and in a transductive setting where the test set is.",
            "Also given to us on which we are judged by the classification error.",
            "01 classification error and there are sort of two themes that we'll be looking at.",
            "One is trying to perform as well as the best single classifier in the ensemble and the other is maybe more interesting.",
            "Sometimes actually beat the majority vote over the ensemble and."
        ],
        [
            "Flea, that'll be clear with a couple of examples, will start off."
        ],
        [
            "So here's one.",
            "The this is the unlabeled information, the unlabeled data, that that we sort of assume we're given.",
            "It's the predictions of the ensemble, in this case, 3.",
            "Classifiers on the test test set, which is 4 test points, and."
        ],
        [
            "Now this we just see the predictions, but of course there's some true unknown labeling on the test set that's going to induce some of the predictions to be correct, and some of them incorrect.",
            "We don't know which boxes are red and which boxes are green here, but we do."
        ],
        [
            "Assume we know the sort of overall aggregate error rates of the individual classifiers in the in the ensemble.",
            "So through, say, uniform convergence bounds in a statistical learning setting.",
            "We have that each classifier is suppose makes makes one mistake on the test set.",
            "So we have this aggregate information and through this we can start some combinatorial reasoning.",
            "Regardless of the the true labeling on the 1st three points X, One X2 and X3, the there is at least one minority classifier in each and therefore the at least one classifier that makes a mistake regardless of the true labeling.",
            "And so that's."
        ],
        [
            "Three mistakes we are allowed three mistakes, and so that is the true labeling, and so we can conclude that the true labeling must be this the majority vote, not coincidentally.",
            "And to kind of come up with a maybe more."
        ],
        [
            "Non trivial example in which we will show that it's possible to beat ERM and the majority vote through this reasoning, let's say we have 8 allowed mistakes among five classifiers.",
            "And here's here's the picture.",
            "Now you know each you can see that on X2 through X6.",
            "The last five examples you have at least one mistake being made on each, so at least five mistakes total, and then three more on X1.",
            "If the minority is in fact correct.",
            "Nothing."
        ],
        [
            "So that would result in this labeling, and in fact this can be the only possible labeling here as well, because if any of the let's say X2, if any of those last five examples have the minority correct, then four mistakes would arise.",
            "At least one mistake on the other on each of the other four.",
            "That's the eight mistake budget and no more left for X1.",
            "So by contradiction this can be the only possible labeling a couple of things to note about this labeling.",
            "It is the sort of generated by this majority vote over the hypothesis, which are the worst hypothesis?",
            "Erm, would would prescribe one of the first 2 hypothesis here, and indeed the majority vote the flat majority vote over the ensemble would give us the same labeling which would err on X1.",
            "So."
        ],
        [
            "To sort of try to encode this in generalize this at little, let's say that look at the goal for now that we're trying to predict the labeling on an animal and element test set and our predicted labeling is G, we're trying to match these true labels Z, and they are randomized in binary, so you know closed in the hypercube.",
            "And."
        ],
        [
            "Yeah, the one thing we know about the true labels is that they are linearly constrained with one constraint per hypothesis.",
            "That is, the hypothesis errors that we talked about before.",
            "And under these constraints, if we want to predict to, say, hedge against the worst case.",
            "We would end up with this, which is a nice linear game and we're trying to match the we're trying to minimize their one classification error match.",
            "The OG&Z is being constrained played by the Adversary.",
            "Now the solution to this game.",
            "See the poster for more, but I'll go over a couple of things one."
        ],
        [
            "That the learning process is just finding this this optimum waiting Sigma starts optimizing in the dual space, which if there P classifiers in the ensemble much less than N, the number of test data, it would be just a small optimization.",
            "Here.",
            "Everything sort of depends on each test point is only entering in through this average prediction.",
            "This waiting is not a distribution, it's just a non negative weight vector of LaGrange multipliers and the.",
            "What you do at Test time is once you have found this this waiting through a convex optimization of the function above, then you would you would apply that red transformation to it, which looks very much like a like a link function or sigmoid function, but indeed just arises again from the minimax structure.",
            "No more assumptions required."
        ],
        [
            "So you end up with something that is very much like a weighted majority.",
            "Vote over with this signal star and more details, please ask us or come to our poster, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Long last name this is this is work with with your front.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first the setting we're looking at binary classification.",
                    "label": 1
                },
                {
                    "sent": "Given an ensemble of hypothesis classifiers, and in a transductive setting where the test set is.",
                    "label": 1
                },
                {
                    "sent": "Also given to us on which we are judged by the classification error.",
                    "label": 1
                },
                {
                    "sent": "01 classification error and there are sort of two themes that we'll be looking at.",
                    "label": 0
                },
                {
                    "sent": "One is trying to perform as well as the best single classifier in the ensemble and the other is maybe more interesting.",
                    "label": 1
                },
                {
                    "sent": "Sometimes actually beat the majority vote over the ensemble and.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flea, that'll be clear with a couple of examples, will start off.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's one.",
                    "label": 0
                },
                {
                    "sent": "The this is the unlabeled information, the unlabeled data, that that we sort of assume we're given.",
                    "label": 0
                },
                {
                    "sent": "It's the predictions of the ensemble, in this case, 3.",
                    "label": 0
                },
                {
                    "sent": "Classifiers on the test test set, which is 4 test points, and.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now this we just see the predictions, but of course there's some true unknown labeling on the test set that's going to induce some of the predictions to be correct, and some of them incorrect.",
                    "label": 0
                },
                {
                    "sent": "We don't know which boxes are red and which boxes are green here, but we do.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assume we know the sort of overall aggregate error rates of the individual classifiers in the in the ensemble.",
                    "label": 0
                },
                {
                    "sent": "So through, say, uniform convergence bounds in a statistical learning setting.",
                    "label": 1
                },
                {
                    "sent": "We have that each classifier is suppose makes makes one mistake on the test set.",
                    "label": 0
                },
                {
                    "sent": "So we have this aggregate information and through this we can start some combinatorial reasoning.",
                    "label": 0
                },
                {
                    "sent": "Regardless of the the true labeling on the 1st three points X, One X2 and X3, the there is at least one minority classifier in each and therefore the at least one classifier that makes a mistake regardless of the true labeling.",
                    "label": 0
                },
                {
                    "sent": "And so that's.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three mistakes we are allowed three mistakes, and so that is the true labeling, and so we can conclude that the true labeling must be this the majority vote, not coincidentally.",
                    "label": 0
                },
                {
                    "sent": "And to kind of come up with a maybe more.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Non trivial example in which we will show that it's possible to beat ERM and the majority vote through this reasoning, let's say we have 8 allowed mistakes among five classifiers.",
                    "label": 1
                },
                {
                    "sent": "And here's here's the picture.",
                    "label": 0
                },
                {
                    "sent": "Now you know each you can see that on X2 through X6.",
                    "label": 0
                },
                {
                    "sent": "The last five examples you have at least one mistake being made on each, so at least five mistakes total, and then three more on X1.",
                    "label": 0
                },
                {
                    "sent": "If the minority is in fact correct.",
                    "label": 0
                },
                {
                    "sent": "Nothing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that would result in this labeling, and in fact this can be the only possible labeling here as well, because if any of the let's say X2, if any of those last five examples have the minority correct, then four mistakes would arise.",
                    "label": 0
                },
                {
                    "sent": "At least one mistake on the other on each of the other four.",
                    "label": 0
                },
                {
                    "sent": "That's the eight mistake budget and no more left for X1.",
                    "label": 0
                },
                {
                    "sent": "So by contradiction this can be the only possible labeling a couple of things to note about this labeling.",
                    "label": 0
                },
                {
                    "sent": "It is the sort of generated by this majority vote over the hypothesis, which are the worst hypothesis?",
                    "label": 1
                },
                {
                    "sent": "Erm, would would prescribe one of the first 2 hypothesis here, and indeed the majority vote the flat majority vote over the ensemble would give us the same labeling which would err on X1.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To sort of try to encode this in generalize this at little, let's say that look at the goal for now that we're trying to predict the labeling on an animal and element test set and our predicted labeling is G, we're trying to match these true labels Z, and they are randomized in binary, so you know closed in the hypercube.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, the one thing we know about the true labels is that they are linearly constrained with one constraint per hypothesis.",
                    "label": 0
                },
                {
                    "sent": "That is, the hypothesis errors that we talked about before.",
                    "label": 0
                },
                {
                    "sent": "And under these constraints, if we want to predict to, say, hedge against the worst case.",
                    "label": 0
                },
                {
                    "sent": "We would end up with this, which is a nice linear game and we're trying to match the we're trying to minimize their one classification error match.",
                    "label": 0
                },
                {
                    "sent": "The OG&Z is being constrained played by the Adversary.",
                    "label": 0
                },
                {
                    "sent": "Now the solution to this game.",
                    "label": 0
                },
                {
                    "sent": "See the poster for more, but I'll go over a couple of things one.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That the learning process is just finding this this optimum waiting Sigma starts optimizing in the dual space, which if there P classifiers in the ensemble much less than N, the number of test data, it would be just a small optimization.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Everything sort of depends on each test point is only entering in through this average prediction.",
                    "label": 1
                },
                {
                    "sent": "This waiting is not a distribution, it's just a non negative weight vector of LaGrange multipliers and the.",
                    "label": 0
                },
                {
                    "sent": "What you do at Test time is once you have found this this waiting through a convex optimization of the function above, then you would you would apply that red transformation to it, which looks very much like a like a link function or sigmoid function, but indeed just arises again from the minimax structure.",
                    "label": 0
                },
                {
                    "sent": "No more assumptions required.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you end up with something that is very much like a weighted majority.",
                    "label": 0
                },
                {
                    "sent": "Vote over with this signal star and more details, please ask us or come to our poster, thanks.",
                    "label": 0
                }
            ]
        }
    }
}