{
    "id": "6rpdjy5a7kzseba737hjjdvhwccwqtif",
    "title": "A mathematical description of musical signals",
    "info": {
        "author": [
            "Monika D\u00f6rfler, Faculty for Mathematics, University of Vienna"
        ],
        "published": "April 5, 2019",
        "recorded": "March 2019",
        "category": [
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/fmf_dorfler_musical_signals/",
    "segmentation": [
        [
            "Of what I'm going to touch on, I will not expect too much or any actual technical knowledge, since obviously you are expected to more general audience.",
            "However, if you are interested in any details we can.",
            "Don't hesitate to ask, or we can also talk later.",
            "OK, so first of all I give you a short introduction on what could I mean or what could be meant by the structure that we find in audio signals, right?",
            "And how can we make what could be a mathematical meaning of that?",
            "Yeah, I mean, structure is always something that we have to imply in some sense.",
            "Yeah, I'll give you a short example on what we do because this is the application example example that I'm going to come back on back to many times during my talk which is singing voice detection.",
            "An yeah then I go more into detail on how to mathematically extract structure from audio signals, like talking about some particular representations.",
            "And let you know a little bit about what a feature extractor is.",
            "And then in the third part we will dig a little bit more into the actual theory.",
            "Let's say of convolutional neural networks.",
            "And yeah, there we will see how we can expect neural networks to learn structure that is present in a signal."
        ],
        [
            "OK, so we start with introduction.",
            "Now.",
            "The first slide is a little bit not so illustrative.",
            "Maybe, but I think for a mathematician it can be easy to see and that's why I put this because I really want you to see immediately what's the central idea of what I mean by signal representation.",
            "So if we have an audio signal, so a signal as a mathematician, you can just think of it as a function.",
            "Yeah, so in principle you map points in time either.",
            "I mean physically you would speak about the time as a continuum continuous variable, but in practice it's very often discrete, but anyway and then to these points in time you always, you you assign certain amplitude values.",
            "So in that sense it's just a function and then the idea of harmonic analysis you could say is to find some basic building blocks.",
            "And here I called MGK, which help you to understand better what's going on in this signal, right so?",
            "And what does it mean?",
            "Help you to understand it means that you take your signal and you project your signal onto.",
            "Onto the building blocks.",
            "Chique, yeah, so you just take inner product.",
            "I'm not saying that this is necessarily an orthogonal projection, but.",
            "It's it might be an oblique projection, and then you are.",
            "Then you consider these projections as as the values that give you the information about your signal, right?",
            "Examples Fourier transform.",
            "I mean, if you interpret this inner product just as an integral, and you interpret the GK as pure sine pure frequencies, yes, then what you see here is just a Fourier transform of F. Short term for transform.",
            "This is the most important probably signal transform in audio processing, and in this case the building blocks are.",
            "Small windows, so windows with short or other short support which are modulated and shifted.",
            "You're going to see later more detailed in more detail what this means.",
            "OK, so now."
        ],
        [
            "After giving you these two examples of building blocks I want to, I want you to see what the resulting coefficients look like.",
            "So the coefficients again when I talk about coefficient of coefficients, I usually mean the inner product of your signal with the building blocks.",
            "So sorry this is tight means time.",
            "For some reason this is a German that doesn't matter.",
            "So here you've got the signal.",
            "The function yes.",
            "So basically this is really just the evaluation of the amplitude overtime, so this is a music signal and on the right side on the top right plug you see the Fourier coefficients.",
            "Absolute values of free coefficients, yes?",
            "So you consider this whole signal, you just want you just interested in which frequencies are present in the signal with what energy or what weight.",
            "And as you can see, maybe this is not very insightful.",
            "What you see in the lower part is now now the coefficients FGK with respect to this modulated and shifted windows.",
            "So this is the short time for a transform.",
            "What happens here is you can see that now for every.",
            "Point or interval in time you get the information in frequency, so this is what we call time frequency analysis.",
            "So now we've got simultaneous information about time and about frequency and there you can see this looks almost like I mean for everyone who's familiar with how music is notated, this looks almost like what we would expect.",
            "I mean, if we have a melody we go dumb bum, bum, bum, bum and here you can see something like bom, bom, bom, bom.",
            "So it's almost like how you would notate music.",
            "This is very intuitive."
        ],
        [
            "Yeah, here now you can see a zoom of the signal that we saw before.",
            "And here I want you to somehow understand that.",
            "The coefficients that we extract they depend heavily.",
            "On the characteristics of the representation system that we use, so they depend on how we design these GGK yeah, what you see here are basically just different.",
            "Shorten for transforms with respect to Windows which are in one case very narrow up there.",
            "So that means that if they are very narrow so they have a very short support, that means that we have we get a good resolution in time.",
            "Yeah, I can tell you exactly when the pianist this is a piano sound.",
            "Hit the hammer.",
            "On the other hand, as you can see, the frequency is a bit smeared, so I might not be able to tell you exactly which tone.",
            "She or he hit, on the other hand here, if I use a wide window in time, then that means and this is related to Heisenberg uncertainty principle for the physicists here in the audience.",
            "Then I can tell you much more precisely which frequency the musician played.",
            "Because if I have a window that's wide in time, then its Fourier transform is very narrow, yes, but on the other hand, as you can see, the time is smeared.",
            "So unfortunately I will not be able to tell you exactly when this happened, so this is.",
            "Like you can't measure those two entities arbitrarily precisely if you wish.",
            "Now in the lower part you see we will talk about this a bit more later, and nonstationary Gabor system.",
            "So here we basically adapt the windows to the content of the signal and here you can see that we get a good localization in time and in frequency.",
            "OK, so this is just to give you an idea of what kind of questions we ask, right?",
            "So we may ask for various problems that we are facing or that we want to.",
            "That we want to solve that we want to tackle.",
            "We may ask well which representation is a good representation and you will see later how this what we turned out to learn in the context of machine learning, which is just one possible application."
        ],
        [
            "OK, so this is a summary of what I actually just said, so in fact we are looking for alternative sets or dictionaries of building blocks.",
            "So beyond the classical standard, just used for short term for transform hand window and so on.",
            "For people who are more experts.",
            "We think OK, maybe we can do something better, which gives us more precise information or more useful information about the windows.",
            "And this is basically more or less a side remark, but mathematically of course a very important one.",
            "You see here, so this equality here should remind everyone of an orthonormal basis expansion, I guess right?",
            "However, we don't usually use orthonormal basis because the concept of an orthonormal basis is in many ways very restrictive.",
            "So we relax the the.",
            "Property of author, orthogonality and and even of author normality and orthogonality.",
            "So we don't even use basis, but we use frames.",
            "So that means frames means that we don't even require the representation to be unique.",
            "But we have many many possible different representations with respect to 1 frame.",
            "But if we if we require the frame property which I will define, then we always have an expansion that looks like this.",
            "Usually it will not be using the same.",
            "Family of signal blocks of building blocks for the reconstruction is for analysis, but we can prove it where it's an easy proof to show that so-called dual frame is always exists and actually an infinite number of your frames exists in order to reconstruct your signal.",
            "So why is this important?",
            "If we know if we if we did not know that we can reconstruct our signal, then potentially we could lose information, which I mean we don't know which information we might.",
            "Want to discard or not so This is why A-frame is important."
        ],
        [
            "OK, so in which context are these kind of structured representations potentially useful?",
            "While we have worked on denoising, yes, source separation.",
            "And this is what I'm going to talk about today.",
            "Deep learning.",
            "So deep learning for audio.",
            "So what is the?"
        ],
        [
            "Learning.",
            "Deep learning is, I mean the word deep meaning.",
            "Many people have asked me why does it?",
            "Why is it called deep?",
            "This is just it comes actually from the theory of neural networks, because if you have neural networks then you have several layers.",
            "You will see this in a minute and they can be wide in the sense of there can be many coefficients in one layer or they can be deep in the sense of you just concatenate or you iterate a lot of layers one after the other.",
            "And deep means you've got many layers.",
            "That means deep, I mean by formal definition.",
            "I think it means that you've got more than one hidden layer.",
            "Yes, OK. And then what I mean, in my opinion is the most was or seems to have been the most critical idea in deep learning to be so successful.",
            "As we have seen, it is the introduction of convolutional neural networks and they have been introduced in image processing and actually typical tasks.",
            "Just to give you a flavor or an idea of what's going on well.",
            "Recognition of handwritten digits.",
            "That's obvious.",
            "That's the showcase application and this is solved.",
            "I mean, we have the good neural networks they recognize even better than humans.",
            "OK, this is a solved problem.",
            "Another much more.",
            "As you can imagine.",
            "Complex problem is to recognize photographed objects and this is interesting because this is interesting to me because here we can see that we have much more semantic content content, yes, so we could ask questions like and I guess all of you have seen these Internet.",
            "Sometimes when you want to use some pages you have to prove that you are not a machine.",
            "You have to prove that you're human.",
            "So you have to find all the pictures where there is a cat.",
            "And I always laugh to myself because I know that there are neural networks that can do this pretty well.",
            "But anyway, so you could also ask find find the pictures where you where there is a person who is happy.",
            "This would be much more complicated or find a picture with the sunset.",
            "These kind of things.",
            "So that's semantic context, content and now convolutional neural networks have increasingly become able to solve semantic problems.",
            "And also in audio.",
            "As you will see, OK."
        ],
        [
            "Again, this is a quick summary.",
            "Neural networks.",
            "Deep neural networks and why has it improved so much in the past decade?",
            "I would actually say well, because now we have got we not I personally, but there's such.",
            "There are these huge databases.",
            "I mean, there's this big data.",
            "And of course, if we've got a lot of data, we can learn a lot.",
            "Yes, so this is probably the main reason, and learning is usually performed using some gradient descent based interference of the weights which are involved in the network.",
            "And I also just wanted to mention this because I think it's very important that there's a lot of tricks in the network community, so and these tricks were very important for boosting the success.",
            "So one of the most important ones is dropout.",
            "I will not explain this because it would go beyond.",
            "Stochastic gradient descent, that's a very interesting approach, because what it means is that you.",
            "Randomly choose parts of your data in order to learn, and this is also an idea which Interestingly improves performance in many.",
            "In many contexts.",
            "Good."
        ],
        [
            "So quick.",
            "Just quick illustration of basic building blocks in newer networks I think.",
            "Or I guess everyone has heard of perception mean this is also this looks like a graph, yes and.",
            "Yeah, so the perceptron is the you could say the seed.",
            "So the most original idea of neural networks.",
            "What happens?",
            "You have an input.",
            "Yeah, it's here.",
            "You on the top you can see it.",
            "You multiply the input by some weights.",
            "You sum up and then you pass it through a non linearity.",
            "So this is somehow the idea why it's called neural networks because that's more or less how our brain works, right?",
            "I mean you have some neurons and they are depending on the level of activity they are on zero or not, so they fire.",
            "They don't fire, so that was the original idea to somehow imitate the brain.",
            "So that's that's the node.",
            "So each now in the in the lower in the lower plots.",
            "Each node has this kind of character and here you can see what what is the input layer.",
            "That's the basically the input, then hidden layer.",
            "So you've got the as many of those nodes.",
            "If you want to.",
            "Usually you want to three, but many hundreds, and then you've got the output layer.",
            "And in deep neural networks, you've got many hidden layers.",
            "And of course the non linearity.",
            "What I call transformation here not, it does not necessarily have to be such a step function as in the original perceptron, but usually it will be some more smooth functions, simply because it's easier for gradient based learning."
        ],
        [
            "And another nice picture.",
            "So now what I showed you before is a general neural network.",
            "What you can see here is not a network, but it's in kind of illustration of a convolutional neural network.",
            "And yeah, I found this and I find I found this picture to find it quite insightful.",
            "Maybe if you have never heard about convolutional neural networks, because here you can see if you input an image.",
            "And then by using convolutional kernels, so you extract certain certain features of the image, so the convolutional kernels really what they do is they really act like a filter on the image.",
            "So you can imagine that you have a big image like this, sign here and then you apply.",
            "You compare locally to some kernels.",
            "Yes, for example you would like to in the first in one kernel, for example, you only want to find.",
            "Lines which go in a certain direction, yes, and whenever you have a good.",
            "If those if the points in the image and your kernel correspond sufficiently well, then you are going to keep that pixel, and if not, you're going to set it to zero basically so you're looking for particular features in your image, you're doing this in the whole image with the same weights, so that's what's called weight sharing.",
            "And this is very important because that way local structure which obviously is something that we use when we recognize images, we were going to compare locally different features of a human being.",
            "For example, will see does this is their nose.",
            "Are their eyes?",
            "Do we find those images?",
            "Then we will conclude or high it's a human being, yes."
        ],
        [
            "So let's now that's because this for me is very important now more schematically or more reduced than before difference between general neural network and convolution.",
            "I mean sorry general dense layer in a neural network and the convolution layer.",
            "So the difference is really the amount of connections that you allow for and the weights that you allow for the connections.",
            "So here I didn't put the.",
            "The labeling for all the weights because here for every connection you are basically allowed to have a different weight.",
            "So what that means?",
            "I guess I may use the what that means.",
            "If you have the matrix, the linear mapping that connects the nodes, then this basically just means that here you could.",
            "Basically all the so let's see Omega 112, Omega M1 and MN.",
            "And they can all be different.",
            "And none of them has to be 0, right?",
            "So we've got everywhere.",
            "Yeah, totally arbitrary matrix in the second case you've got a matrix that has very very restrictive conditions, so you only allow for.",
            "I mean in this image in every row four only for two weights which are non zero and.",
            "Each row.",
            "His only the same weights?",
            "Yes, so this is a it's just a toplitz matrix, yes?",
            "Like this?",
            "And so there are two two things that we can immediately observe.",
            "One thing is that we have this locality.",
            "Right, we're looking for local structures.",
            "And we're looking for the same structures everywhere in the picture.",
            "Two things plus.",
            "We have by far fewer waits to learn, right?",
            "So that's also good because learning."
        ],
        [
            "Is expensive as we will see in a minute.",
            "So here just some applications.",
            "I think I will not read this to you.",
            "Because it's just at least, and it's absolutely not comprehensive either.",
            "So there are many more applications.",
            "It seems like now in our community, if you look for papers or deep learning, you will.",
            "Sometimes you're surprised because it seems that all the people try to solve all the problems with neural networks, which I find quite amusing sometimes.",
            "But yeah, finance prediction of you know, stock market values and so on everywhere people earn money with it, in fact.",
            "It seems that OK, these convolutional neural networks can.",
            "They can do everything or anything that."
        ],
        [
            "English.",
            "Even beating a human being in goal, which up to a few years ago, people said that this will never happen.",
            "OK, the computers are better now than humans in chess.",
            "That's OK, but some people said in God will never happen because this is such a complex game, it will never happen.",
            "And then just, I think two years ago this happened, but I was quite sad to be honest because I thought it was pretty button.",
            "Now there's the big butt.",
            "This is a colleague of mine actually research that so the electricity of one game of Alphago is 3000 US dollars.",
            "This is an approximation, but it's expensive.",
            "It used more than thousand CPUs, one Red, 76 GPS and many, many scientists who programmed all this and then OK the best Go Player of the world.",
            "Where does he use one brain, one coffee?",
            "I mean, now you can talk about the value of a human brain.",
            "Of course, maybe this is more valuable than $3000, but still.",
            "It's what this should actually illustrate is OK.",
            "It's not that you just use your little smart phone, and you're going to solve all the problems now.",
            "You use huge databases and you use huge amount of GPUs and enormous amount of power somehow.",
            "So why are we telling this?",
            "Well, because those interesting problems which have semantic flavor, which are the ones we're interested in, they really need a lot.",
            "So what we believe still is that the human brain can think about, for example, the nature characteristic of data and can do something smart before putting the data into the network.",
            "So this is what we're trying to do.",
            "And then I try to.",
            "I would like to try to convey some of the ideas.",
            "So instead of just saying we've got so many data, why not just plug them into computer and hope that the computer will learn something we said no before we plug it into the computer?",
            "We think a little bit about our problem and we do something.",
            "Smart, something mathematical.",
            "Back to the beginning, we choose a nice representation and then we.",
            "You say that we use the transformed data as an input.",
            "Yeah."
        ],
        [
            "And this is particular mean.",
            "I think that this could be done for image processing as well, but in audio this is almost compulsory because in audio.",
            "Among all papers from the applied people who write about audio machine learning, 99.9% will use some FFT based preprocessing.",
            "And only the rest working work on the raw data.",
            "This is something that people hardly ever do, and we will say see why."
        ],
        [
            "Good and this.",
            "This is some.",
            "Basically also just the recalculation error for you.",
            "A reminder of what I said already before.",
            "So what we do we design the frames and here this is another example.",
            "Now I showed you before nonstationary Gabor frame that adept somehow in time the width of the window?",
            "What you see here on the right hand side is a is a representation that adapts the resolution in frequency and this is something that we've been doing a lot because what you see here these two.",
            "Representations, both of them up time frequency representations, so we have an evaluation overtime and an evaluation over frequency, but the left hand side it's just a standard spectrogram.",
            "So what everyone does on the right and said you have got a constant Q spectrogram.",
            "That's something that has been proposed along time ago, but we developed an implementation based on nonstationary Gabor frames and what you can see is that in some sense it looks like on the right hand side the information that's in the signal is.",
            "Better distributed, yes.",
            "Here we've got so many zeros up there and here it looks like OK.",
            "The information in the lower frequencies is somehow clearer.",
            "And then in the top frequencies where there seems to be not so much going on, we don't use so many coefficients.",
            "So that's the idea of consecutive transform and yeah.",
            "And they are here.",
            "This is the definition of frames that are promised before.",
            "So we say now that a sequence of basic building blocks, GJ is the frame.",
            "If this so called relaxed possible relation holds so you can see I mean, if you were asking for an author orthogonal basis of normal basis, then this would have to be equal to F norm of F squared.",
            "Now we only relax it in the sense that it has to be bounded above and below by some multiple of the constant of F. Of the normal faith.",
            "Good."
        ],
        [
            "This is this is for you to see what such a gobble frame vertical frame looks like, so the gobble frame is at the basis of the short term for transform that I showed you.",
            "So I mentioned before.",
            "At the very beginning, actually shortening for transform analyzes the signal using localized windows.",
            "And these windows are then shifted in time, so they are shifted to all the time points you're interested in and then modulate it.",
            "So this is like a local Fourier transform, I mean, because Fourier transform is also just a modulation.",
            "So this is an illustration of that.",
            "This is what the building blocks look like."
        ],
        [
            "OK. Good, now we.",
            "I hope I've given enough information for you to see to you to see why these three questions are interesting.",
            "Interesting for us.",
            "So very interesting for us, so we now ask since there are so many possible ways to represent a signal, we ask?",
            "Well, can we say something about what representation in your network would learn if the network is given certain certain data set?",
            "Because if a network learns particular representation, you can assume that this is somehow represents the characteristics of the data set.",
            "Yeah, the second question which was answered to an extent not by us, but by some colleagues, is if we.",
            "If we did not use a representation in the 1st place but worked directly on the on the raw signal.",
            "Candace improve performance if we really have a lot of data, or will it always stay approximately the same?",
            "And this is kind of the dual question to the second question.",
            "This is the one that I'm more interested in.",
            "If I if I design these smart representations that encode symmetries in the in the signal in the signals, then can we reduce the network size and learn faster?",
            "And the answer is to some extent yes.",
            "So we have been able to propose some representations which really improve performance with smaller networks."
        ],
        [
            "And this is one of them.",
            "So this is a this is a problem that PhD student of mine is mainly working on.",
            "This is a transformation which I will define a little later.",
            "It's garbage scattering.",
            "It's called Garber scattering and this is also based on Gabor frames which I just introduced before with this picture and hear what what we do or what was Vito does.",
            "Is she iteratively applies Garber phrase so in several layers and why I put this image here is because I want you to understand what I mean by in.",
            "But invariants or symmetry in some sense.",
            "So, because here the signal, which is a synthetic signal, consists of two synthetic tones.",
            "So you can see it's a tone with a lot of overtones and onset.",
            "And the frequency content is the same in both, while in the second term.",
            "So here you've got an amplitude modulation, so the tone gets louder and softer, so it goes, Oh yeah.",
            "And here we now we created two layers and you can see that the first layer contains all the frequency content.",
            "But it seems to be invariant to the amplitude information.",
            "So to the changes in loudness, yes this is smooth out.",
            "On the other hand, the second layer has lost frequency information.",
            "So in this second layer does not tell you anything about the frequency that's going on in the in the signal, but on the other hand you can see that it stores the amplitude amplitude information.",
            "So here this is the frequency of the modulation of the amplitude."
        ],
        [
            "OK, and now since we have already had so much information, I want to give you a short.",
            "This is the only music unfortunately that's going to that you're going to hear today during my talk, but at least and in order to give you a quick glimpse of what can be done and the problem is singing voice detection and we'll come back to this later.",
            "This is an important problem.",
            "Also commercially.",
            "Obviously.",
            "What does it mean?",
            "Well, as the name hints already, singing voice detection means that in a signal in a music piece, you want to say exactly when a person thinks or human things or not, or when no one thinks that only instruments play.",
            "And somehow for some reason, my colleagues from the this is my colleagues from the Institute of.",
            "Artificial intelligence in Vienna.",
            "They work a lot on this problem and it seems to be of commercial interests, because how much the musicians get paid depends on whether someone thinks or not.",
            "Don't ask me why, but this seems to be somehow the reason why people are so interested in this.",
            "And yeah, and please keep this.",
            "Number in mind.",
            "So the my colleagues they have reached the state of the art for this problem, so they have a very good amount of recognition and they use a network with 1.4 million weights and you will see later that we were able to reduce this number significantly by using smart input.",
            "But anyway, sorry now.",
            "I see that I'm too slow, so I will probably.",
            "OK, so this is this.",
            "Here.",
            "What you see is a spectrogram again, and what you see down there is the question is the so to say the.",
            "Ground truth on where do I find?",
            "Where is someone singing into where not and what's funny?",
            "And you will see I mean, for me this is very obvious if you're familiar with spectrograms, but you can hopefully also see you can be will be able to tell from in spectrogram you can see the singer because this is whenever you have got this lurde in frequency components.",
            "So let's listen.",
            "Oh, it's here, Sir.",
            "Outside snows fall.",
            "Cancel.",
            "OK, so this is my colleague Jan Schlueter working.",
            "We were working together in a project.",
            "He's now in France.",
            "And he and his colleagues were very successful.",
            "But you can what you can see here is a standard problem.",
            "You get.",
            "You get the spectrum input, you get the annotations and then you have to learn to deduce from the spectrogram whether or not there is a human voice present.",
            "And of course the hope is that this is going to work not only in the data for which you have the information, but on new data.",
            "This is what we call generalization.",
            "So oops, try to get rid of this.",
            "Yeah good.",
            "OK."
        ],
        [
            "Now I."
        ],
        [
            "I think I will try to be very fast here because here I basically just tried to give more details of the mathematical description, but I see that I'm already a bit late.",
            "And I will go through this quickly.",
            "In principle this is really just a formalization, or more details on how short."
        ],
        [
            "Free transform work."
        ],
        [
            "Here you can see the effect of again wider and smaller windows and now this is the.",
            "This may be the only point where I want to be a bit more precise.",
            "Again re collection of we saw what we saw before.",
            "So here's the spectrogram.",
            "So this is these matrices that you always see.",
            "So this is basically just defined as you multiply your signal F with shifted window G and then you take Fourier transform and this can therefore you can write the spectrogram also as the absolute value squared.",
            "Of the short term for transform, and obviously, since Fourier transform again is just an integral, this can be written like this, and if we take alternative.",
            "So what you see here?",
            "Maybe that's the important point.",
            "The GKL adjust this shift.",
            "Didn't modulated windows that are illustrated.",
            "Minutes ago, and now it."
        ],
        [
            "Yeah, in the case of nonstationary frames we have Windows which are not just shifted in frequency, but you can stretch them and that way adapt them somehow to the frequency and you can split in some sense the frequency axis in different ways as you wish so.",
            "In the this is time.",
            "And this is frequency.",
            "Then in the short inference from what you do, you split this.",
            "Exist so the frequency axis in completely equal intervals in adaptive transforms you can say.",
            "Well, no, I don't want this.",
            "I want it differently.",
            "So for example I want to have a better resolution in the lower frequencies.",
            "And then I'm happy with.",
            "Not so good resolution in the higher frequencies or you could have it differently, so our systems might learn something that has good resolution up to some point and then not so good resolution and maybe something better again.",
            "And so we are completely free to do this the way we want.",
            "And yeah, that's what we call nonstationary Gabor framing, mathematically speaking.",
            "Now, we have still shifted versions of a filter Bank of Windows in the filter bank.",
            "But the filter bank is adaptive, so it's not.",
            "We're not restricted to just have shifted Windows in freak."
        ],
        [
            "Good and yes.",
            "So now what we've seen is that a spectrogram essentially expresses signal properties clear much more clearly than what we see in raw audio data, and this is leads us to an ocean that's important in all the machine learning community which is feature extractor.",
            "So in general we can say that a feature extractors is mapping that Maps our signal or given data.",
            "Into some space that might be much higher dimensional but is more informative.",
            "So in particular, as you saw in the garbage scattering, we might split it in an array of images and each of the images in this array might give us different information, yes, so that's why we've got this RR to the M12 times etc.",
            "MD.",
            "Good.",
            "And.",
            "So one example again now that is deduced from spectrogram derived from the spectrogram by taking weighted averages over frequency.",
            "This is the Mail spectrogram that's the second most used representation in audio processing, which is why I mentioned here, and also because we might come back to this later.",
            "So what happens here?",
            "In principle you take first a regular spectrogram, but then you average over frequency.",
            "And what does averaging mean?",
            "Everything always means that you get rid of some.",
            "Some information that's in the signal and getting rid of some information that you don't need means that you create a certain.",
            "You always create certain invariants.",
            "So for example, if you average over some frequency interval, that means that all signals that have the same information within these intervals are going to belong to the same class later, and this is something that is actually at the core of learning, because that's what we all do.",
            "I mean, if the little children learn what a dog is first, you think how would they ever learn that?",
            "So I don't know the English name.",
            "Small tiny dog and the big, you know, German Shepherd is the same species, but they learn, and that's because they're able to basically discard or get rid of all the information that's not important.",
            "And this is the average over all the information that they get from their parents and teachers about what the dog actually is.",
            "And we do the same.",
            "Yes, in some sense we average along certain dimensions in order to extract the frequency exactly that we need.",
            "And of course in neural network learning you would like to see.",
            "You would like.",
            "You can even ask the network how much everything do we need, and this is something that we did.",
            "And let's see."
        ],
        [
            "I will skip the exact precise definition of garbage."
        ],
        [
            "Bring because this is not important.",
            "And."
        ],
        [
            "Yeah, this is so the hope.",
            "The hope if we.",
            "If we if we design these features.",
            "Which encodes exactly the symmetries that we need.",
            "Is that instead of learning a very complicated mapping which we would need for example for if we learn directly on raw data, we hope that we can do something very simple.",
            "And of course the most simple.",
            "Thing that we can do is basically just what we've seen in this perception.",
            "So just apply a linear map.",
            "So like here.",
            "So we say that feature extractor separates F. So the function that we want to learn linearly if F is at least sufficiently closely approximated by just a linear projection.",
            "So projection onto a set vector of weights.",
            "As you can see here, yes.",
            "So this is, this would be the simplest case, and in fact a network and neural network does an iteration of this kind of.",
            "Projection."
        ],
        [
            "Yeah, and now we did and I did exactly into what I promised at the very beginning at the beginning.",
            "Sometime though, we there's a recent very recent paper.",
            "You can see this was Autumn 2018, where a colleague so real NATO and some other colleagues they showed that if you have.",
            "Enormous amounts of data then.",
            "In fact you can learn from raw audio and they had a very interesting problem because their problem is we discussed it a while ago in Vienna.",
            "The problem is to learn semantic tags that users put on.",
            "Songs so text like this is good music for chugging this music is relaxing, so they had a set of properties and then they so this is he works for Pandora in California and they have an T. That's what they write.",
            "They also described in this article.",
            "They have enormous amounts of data and I think it's 500,000 hours of music or something.",
            "I don't recall exactly the I think I mentioned it here."
        ],
        [
            "So Pandora owns 1.5 millions of manually annotated music tracks, yes.",
            "And they will not give you the data.",
            "They will not give the data to US academics, of course, but what they showed, and I think it's very nice.",
            "They really showed exactly to which point it's better to use spectrograms or some structured input and from which point onward you can get better results from learning on raw data.",
            "But I mean, it's really huge, huge and we will never possess this, and trading time is around 4 weeks.",
            "So.",
            "So this is."
        ],
        [
            "I hope I convinced you that it's for us.",
            "It's better since we always have restricted amount of data.",
            "It's always better to encode the info."
        ],
        [
            "Are interested we are interested in.",
            "So this is the maybe if you're interested to read more about the scattering transform that I mentioned, which directly encodes invariances, then this would be the links.",
            "I can put.",
            "The can give the I can put the slides on lines and.",
            "This is a work that we have now submitted.",
            "And where we were able to show that the scattering transform for some problems, so we use this on synthetic data and also on instrument recognition.",
            "So the problem to recognize instruments from a huge database and there we were able to show that GABA scattering actually for very small networks gives significantly better results than classical representations.",
            "So this is, I think this is really interesting."
        ],
        [
            "And yeah, I think I will.",
            "Do you have 10 more minutes or yes?",
            "OK, so then I will still talk about this.",
            "Um?",
            "So this is now the basically last idea.",
            "But it's totally in line of what I have already explained, because now I would like to formalize this idea that we have to look at the feature that we use the feature extractor and the network and I have to look at those two.",
            "Ingredients somehow in connection with each other.",
            "And I have to think about them with respect to the data set with I'm interested in because usually the data that we get from they are highly structured.",
            "The probably come from some low dimensional manifold or some approximation of it.",
            "They're not, they never filled high dimensional Euclidean space.",
            "So when we ask whether a pair of feature extractor and network is better than another one, then we have to consider these pairs and the data set, which is why we gave this.",
            "Definition so we have.",
            "We are considered 2 feature network pairs.",
            "Yes so.",
            "Big Fire J&J to the network means is really the architecture.",
            "Then we say that the first one is subordinate to the other one to the second one with respect to a given data set.",
            "If for all parameter vectors which actually realize the network.",
            "So all the weights in the network.",
            "In the first setting, there exists a parameter set.",
            "In the second setting such that every function that can be realized by the first network can also be realized by the second one, yes.",
            "And then if so, we call the two pairs equivalent with respect to D if they are supporting it to each other.",
            "Chris"
        ],
        [
            "And now this is the formulas formalization somehow to off this symmetry, because if we have if we have a data set D. And we augment it.",
            "By applying certain operators, so for example, small frequency for small shifts in time.",
            "So you can imagine that if we have if you have music or and you shift it a little bit in time, then it will still should still belong to the same class or much more, maybe obvious or much more intuitive.",
            "In images.",
            "I always use this example if you look for.",
            "For a cloud or dog image then you would probably want this to be invariant to rotation, right?",
            "Because if the cat is laying on the back, it's still a cat or a dog.",
            "I think I said the dog yet so you would know what people do in the machine learning community.",
            "They really use augmentation so they use that data and they apply certain operators, for example rotation and they tell they tell the network well if I rotate the image, it still is the same class.",
            "Right, of course we mathematicians think, well, why do I have to teach the network in such a rather?",
            "Rahway, if we can.",
            "Maybe design A feature extractor that does this for me, yes, and that's the idea of.",
            "Of this proposition that if I have an augmented data set, and if I design A, where is it?",
            "Yeah, if I decide to feature is extracted big file 2 which is invariant to this augmentation, right?",
            "So which means that the feature extractor ignores this, AUGMENT, ignores the operators here, then I can say if the network is.",
            "If I want to subordinate to file two and two with respect to this data set.",
            "And use the augmentation and then invitee is invariant to this augmentation.",
            "Then five one is will also be subordinate to 5, two and two with respect to the augmented data set.",
            "Yes, which means that I will not.",
            "I will be able to use again the smaller network.",
            "This is just an example which is very clear for people who know that of course if you use.",
            "The absolute value of the short term for transformer, even the Fourier transform.",
            "This is face in this invariant to phase shift.",
            "And therefore augmenting data set by face.",
            "By just multiplication with the face factor is an augmentation that will be ignored by short inference about spectrogram, sorry.",
            "Good."
        ],
        [
            "Yeah, and this may be just a mention.",
            "The formal proof of this will appear, hopefully in a paper soon.",
            "This refers to a very interesting article by Urasoe College and colleagues that.",
            "Where he where they showed that they were talking about generalization, but I cannot go into details on this anymore, but I think I mentioned it briefly.",
            "What does it mean?",
            "Generalization?",
            "Well, generalization is the property of a network to not only function on the data set data set that it learns on, but also new data.",
            "I mean, this is quite obvious, yes, and this is it's obvious that we're interested in that situation because, for example, if you think about automatic driving, if the if the automatic driving system is only able to.",
            "Deal with the situation is that it has seen exactly before.",
            "Like for example, if there is a dog on the street, I should stop, but it has not learned that if there is whatever else on the street, I should also stop, then it will be useless.",
            "Yes, so we need this kind of smoothing in order to be able to deal with new situations.",
            "That's what we called generalization and what they what they showed in this paper and which I put in this proposition is.",
            "So if we introduce invariants to augmentation in a stable learning algorithm.",
            "Then we can.",
            "Then we can reduce the generalization error exactly by the fraction of the covering of the.",
            "Of the day of the original data set divided by the covering, I mean the covering index of the augmented data set.",
            "So covering index you can imagine is describes the complexity of the data set.",
            "Again, I will not now give you the exact.",
            "Definition anymore, but what message of the?"
        ],
        [
            "Is if we use again if we use feature extractors that are have this useful invariances then generalization error is smaller.",
            "OK, good.",
            "And now the very last thing.",
            "Was is now."
        ],
        [
            "This comes back to really the question, can I learn which representation is the best represent which representation is the best representation for my problem in the data set?",
            "And here we looked at singing voice detection again and we were trying to reduce this huge network.",
            "You remember 1.4 million.",
            "Wait by allowing certain activity in the frequency axis.",
            "So instead of using a strict predis, predesigned resolution of frequency axis, we allowed the network to learn.",
            "Which is the best resolution of the frequency axis?",
            "And yeah, I think for the moment I will skip.",
            "Maybe the mathematical details.",
            "We can discuss them later if someone is interested in this."
        ],
        [
            "But yeah, here's here's a huge proposition which gives you the technical details why.",
            "Our statement is true, and here's the statement.",
            "So if we have now one convolutional neural network with DC convolutional layers and then a second one, which is analog but has one additional convolutional layer which has only a finite so very short number of weights which allows for averaging in time.",
            "Then we know that if we use.",
            "Miss Spectrogram and the first network then this is subordinate to using an adaptive.",
            "Spectrogram this is the essay you may have defined it before, so using this adaptive resolution of the frequency axis and the same network but with one additional convolutional layers if.",
            "The windows are chosen in an inappropriate way.",
            "And so this is something that we proved."
        ],
        [
            "Mathematically, and then, we could show that understanding voice detection.",
            "In fact, relaxing this resolution gives us a better performance.",
            "While decreasing the size of the network dramatically, and what does it mean?",
            "This is here, maybe it's not so important to understand all the details, but what you might now recall.",
            "So the network architecture is similar to before, but much much smaller.",
            "So instead of 1.4 million weights in the first case we have 94,000 something in the second 53,000.",
            "Something weights, so this is much smaller.",
            "And here the."
        ],
        [
            "No, this is not the results.",
            "These are the different inputs."
        ],
        [
            "And here the results.",
            "So what you can see?",
            "This is, the measure is area above area.",
            "Um?",
            "You see?",
            "I can't recall at the moment, but this is what it is.",
            "Maybe someone can.",
            "Usually it's a you see and then.",
            "Higher number is better, but in our case lower number means it's better and what you can see here I don't have pointer but here you see that the adaptive version.",
            "So here STT adaptive and hear filterbank with variable width is better than the comparison which is here.",
            "So small one is the smallest one with really the.",
            "So it seems area over the Roc curve, so now my brain works again.",
            "I'm also a bit tired already, so here you can see this is the.",
            "This is the reference implementation and this is the implementation allowing for adaptivity, and you can see that adaptivity gives us a small but significant improvement, and the same is true when we have filterbank with variable with time averaging.",
            "OK. Good."
        ],
        [
            "So."
        ],
        [
            "Quick summary.",
            "What I wanted to convey today is that.",
            "Appropriate representation systems give us.",
            "Good, more concise representations of structured signals, in particular audio signals.",
            "We saw that adaptivity can further enhance readability of representation coefficients and.",
            "Then the two questions.",
            "The answers to the two questions deep learning can help us to fine tune time frequency resolutions.",
            "If we, as we've just seen and on the other hand, if we impose known structure, that's what we saw in the garbage scattering and symmetry.",
            "Then we can improve learning and generalization while using way smaller architectures.",
            "Yeah, and this is ongoing work, of course is everything in deep learning and we are now looking at new problems and new representations.",
            "All the time and sometimes we are able to improve state of the art, sometimes not, and I thank you very much for your attention and will be happy to answer questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of what I'm going to touch on, I will not expect too much or any actual technical knowledge, since obviously you are expected to more general audience.",
                    "label": 0
                },
                {
                    "sent": "However, if you are interested in any details we can.",
                    "label": 0
                },
                {
                    "sent": "Don't hesitate to ask, or we can also talk later.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all I give you a short introduction on what could I mean or what could be meant by the structure that we find in audio signals, right?",
                    "label": 0
                },
                {
                    "sent": "And how can we make what could be a mathematical meaning of that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, structure is always something that we have to imply in some sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll give you a short example on what we do because this is the application example example that I'm going to come back on back to many times during my talk which is singing voice detection.",
                    "label": 0
                },
                {
                    "sent": "An yeah then I go more into detail on how to mathematically extract structure from audio signals, like talking about some particular representations.",
                    "label": 1
                },
                {
                    "sent": "And let you know a little bit about what a feature extractor is.",
                    "label": 0
                },
                {
                    "sent": "And then in the third part we will dig a little bit more into the actual theory.",
                    "label": 0
                },
                {
                    "sent": "Let's say of convolutional neural networks.",
                    "label": 1
                },
                {
                    "sent": "And yeah, there we will see how we can expect neural networks to learn structure that is present in a signal.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we start with introduction.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The first slide is a little bit not so illustrative.",
                    "label": 0
                },
                {
                    "sent": "Maybe, but I think for a mathematician it can be easy to see and that's why I put this because I really want you to see immediately what's the central idea of what I mean by signal representation.",
                    "label": 0
                },
                {
                    "sent": "So if we have an audio signal, so a signal as a mathematician, you can just think of it as a function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in principle you map points in time either.",
                    "label": 0
                },
                {
                    "sent": "I mean physically you would speak about the time as a continuum continuous variable, but in practice it's very often discrete, but anyway and then to these points in time you always, you you assign certain amplitude values.",
                    "label": 0
                },
                {
                    "sent": "So in that sense it's just a function and then the idea of harmonic analysis you could say is to find some basic building blocks.",
                    "label": 0
                },
                {
                    "sent": "And here I called MGK, which help you to understand better what's going on in this signal, right so?",
                    "label": 0
                },
                {
                    "sent": "And what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Help you to understand it means that you take your signal and you project your signal onto.",
                    "label": 0
                },
                {
                    "sent": "Onto the building blocks.",
                    "label": 0
                },
                {
                    "sent": "Chique, yeah, so you just take inner product.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that this is necessarily an orthogonal projection, but.",
                    "label": 0
                },
                {
                    "sent": "It's it might be an oblique projection, and then you are.",
                    "label": 0
                },
                {
                    "sent": "Then you consider these projections as as the values that give you the information about your signal, right?",
                    "label": 0
                },
                {
                    "sent": "Examples Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you interpret this inner product just as an integral, and you interpret the GK as pure sine pure frequencies, yes, then what you see here is just a Fourier transform of F. Short term for transform.",
                    "label": 0
                },
                {
                    "sent": "This is the most important probably signal transform in audio processing, and in this case the building blocks are.",
                    "label": 0
                },
                {
                    "sent": "Small windows, so windows with short or other short support which are modulated and shifted.",
                    "label": 0
                },
                {
                    "sent": "You're going to see later more detailed in more detail what this means.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After giving you these two examples of building blocks I want to, I want you to see what the resulting coefficients look like.",
                    "label": 1
                },
                {
                    "sent": "So the coefficients again when I talk about coefficient of coefficients, I usually mean the inner product of your signal with the building blocks.",
                    "label": 0
                },
                {
                    "sent": "So sorry this is tight means time.",
                    "label": 0
                },
                {
                    "sent": "For some reason this is a German that doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So here you've got the signal.",
                    "label": 0
                },
                {
                    "sent": "The function yes.",
                    "label": 0
                },
                {
                    "sent": "So basically this is really just the evaluation of the amplitude overtime, so this is a music signal and on the right side on the top right plug you see the Fourier coefficients.",
                    "label": 0
                },
                {
                    "sent": "Absolute values of free coefficients, yes?",
                    "label": 0
                },
                {
                    "sent": "So you consider this whole signal, you just want you just interested in which frequencies are present in the signal with what energy or what weight.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, maybe this is not very insightful.",
                    "label": 0
                },
                {
                    "sent": "What you see in the lower part is now now the coefficients FGK with respect to this modulated and shifted windows.",
                    "label": 0
                },
                {
                    "sent": "So this is the short time for a transform.",
                    "label": 0
                },
                {
                    "sent": "What happens here is you can see that now for every.",
                    "label": 0
                },
                {
                    "sent": "Point or interval in time you get the information in frequency, so this is what we call time frequency analysis.",
                    "label": 0
                },
                {
                    "sent": "So now we've got simultaneous information about time and about frequency and there you can see this looks almost like I mean for everyone who's familiar with how music is notated, this looks almost like what we would expect.",
                    "label": 0
                },
                {
                    "sent": "I mean, if we have a melody we go dumb bum, bum, bum, bum and here you can see something like bom, bom, bom, bom.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like how you would notate music.",
                    "label": 0
                },
                {
                    "sent": "This is very intuitive.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, here now you can see a zoom of the signal that we saw before.",
                    "label": 0
                },
                {
                    "sent": "And here I want you to somehow understand that.",
                    "label": 0
                },
                {
                    "sent": "The coefficients that we extract they depend heavily.",
                    "label": 0
                },
                {
                    "sent": "On the characteristics of the representation system that we use, so they depend on how we design these GGK yeah, what you see here are basically just different.",
                    "label": 0
                },
                {
                    "sent": "Shorten for transforms with respect to Windows which are in one case very narrow up there.",
                    "label": 0
                },
                {
                    "sent": "So that means that if they are very narrow so they have a very short support, that means that we have we get a good resolution in time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I can tell you exactly when the pianist this is a piano sound.",
                    "label": 0
                },
                {
                    "sent": "Hit the hammer.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, as you can see, the frequency is a bit smeared, so I might not be able to tell you exactly which tone.",
                    "label": 0
                },
                {
                    "sent": "She or he hit, on the other hand here, if I use a wide window in time, then that means and this is related to Heisenberg uncertainty principle for the physicists here in the audience.",
                    "label": 0
                },
                {
                    "sent": "Then I can tell you much more precisely which frequency the musician played.",
                    "label": 0
                },
                {
                    "sent": "Because if I have a window that's wide in time, then its Fourier transform is very narrow, yes, but on the other hand, as you can see, the time is smeared.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately I will not be able to tell you exactly when this happened, so this is.",
                    "label": 0
                },
                {
                    "sent": "Like you can't measure those two entities arbitrarily precisely if you wish.",
                    "label": 0
                },
                {
                    "sent": "Now in the lower part you see we will talk about this a bit more later, and nonstationary Gabor system.",
                    "label": 0
                },
                {
                    "sent": "So here we basically adapt the windows to the content of the signal and here you can see that we get a good localization in time and in frequency.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is just to give you an idea of what kind of questions we ask, right?",
                    "label": 0
                },
                {
                    "sent": "So we may ask for various problems that we are facing or that we want to.",
                    "label": 0
                },
                {
                    "sent": "That we want to solve that we want to tackle.",
                    "label": 0
                },
                {
                    "sent": "We may ask well which representation is a good representation and you will see later how this what we turned out to learn in the context of machine learning, which is just one possible application.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is a summary of what I actually just said, so in fact we are looking for alternative sets or dictionaries of building blocks.",
                    "label": 0
                },
                {
                    "sent": "So beyond the classical standard, just used for short term for transform hand window and so on.",
                    "label": 0
                },
                {
                    "sent": "For people who are more experts.",
                    "label": 0
                },
                {
                    "sent": "We think OK, maybe we can do something better, which gives us more precise information or more useful information about the windows.",
                    "label": 0
                },
                {
                    "sent": "And this is basically more or less a side remark, but mathematically of course a very important one.",
                    "label": 0
                },
                {
                    "sent": "You see here, so this equality here should remind everyone of an orthonormal basis expansion, I guess right?",
                    "label": 0
                },
                {
                    "sent": "However, we don't usually use orthonormal basis because the concept of an orthonormal basis is in many ways very restrictive.",
                    "label": 0
                },
                {
                    "sent": "So we relax the the.",
                    "label": 0
                },
                {
                    "sent": "Property of author, orthogonality and and even of author normality and orthogonality.",
                    "label": 0
                },
                {
                    "sent": "So we don't even use basis, but we use frames.",
                    "label": 0
                },
                {
                    "sent": "So that means frames means that we don't even require the representation to be unique.",
                    "label": 0
                },
                {
                    "sent": "But we have many many possible different representations with respect to 1 frame.",
                    "label": 0
                },
                {
                    "sent": "But if we if we require the frame property which I will define, then we always have an expansion that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Usually it will not be using the same.",
                    "label": 0
                },
                {
                    "sent": "Family of signal blocks of building blocks for the reconstruction is for analysis, but we can prove it where it's an easy proof to show that so-called dual frame is always exists and actually an infinite number of your frames exists in order to reconstruct your signal.",
                    "label": 0
                },
                {
                    "sent": "So why is this important?",
                    "label": 0
                },
                {
                    "sent": "If we know if we if we did not know that we can reconstruct our signal, then potentially we could lose information, which I mean we don't know which information we might.",
                    "label": 0
                },
                {
                    "sent": "Want to discard or not so This is why A-frame is important.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so in which context are these kind of structured representations potentially useful?",
                    "label": 1
                },
                {
                    "sent": "While we have worked on denoising, yes, source separation.",
                    "label": 0
                },
                {
                    "sent": "And this is what I'm going to talk about today.",
                    "label": 0
                },
                {
                    "sent": "Deep learning.",
                    "label": 0
                },
                {
                    "sent": "So deep learning for audio.",
                    "label": 0
                },
                {
                    "sent": "So what is the?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Deep learning is, I mean the word deep meaning.",
                    "label": 0
                },
                {
                    "sent": "Many people have asked me why does it?",
                    "label": 0
                },
                {
                    "sent": "Why is it called deep?",
                    "label": 0
                },
                {
                    "sent": "This is just it comes actually from the theory of neural networks, because if you have neural networks then you have several layers.",
                    "label": 0
                },
                {
                    "sent": "You will see this in a minute and they can be wide in the sense of there can be many coefficients in one layer or they can be deep in the sense of you just concatenate or you iterate a lot of layers one after the other.",
                    "label": 0
                },
                {
                    "sent": "And deep means you've got many layers.",
                    "label": 0
                },
                {
                    "sent": "That means deep, I mean by formal definition.",
                    "label": 0
                },
                {
                    "sent": "I think it means that you've got more than one hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK. And then what I mean, in my opinion is the most was or seems to have been the most critical idea in deep learning to be so successful.",
                    "label": 0
                },
                {
                    "sent": "As we have seen, it is the introduction of convolutional neural networks and they have been introduced in image processing and actually typical tasks.",
                    "label": 0
                },
                {
                    "sent": "Just to give you a flavor or an idea of what's going on well.",
                    "label": 0
                },
                {
                    "sent": "Recognition of handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "That's obvious.",
                    "label": 0
                },
                {
                    "sent": "That's the showcase application and this is solved.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have the good neural networks they recognize even better than humans.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a solved problem.",
                    "label": 0
                },
                {
                    "sent": "Another much more.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine.",
                    "label": 0
                },
                {
                    "sent": "Complex problem is to recognize photographed objects and this is interesting because this is interesting to me because here we can see that we have much more semantic content content, yes, so we could ask questions like and I guess all of you have seen these Internet.",
                    "label": 0
                },
                {
                    "sent": "Sometimes when you want to use some pages you have to prove that you are not a machine.",
                    "label": 0
                },
                {
                    "sent": "You have to prove that you're human.",
                    "label": 0
                },
                {
                    "sent": "So you have to find all the pictures where there is a cat.",
                    "label": 0
                },
                {
                    "sent": "And I always laugh to myself because I know that there are neural networks that can do this pretty well.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so you could also ask find find the pictures where you where there is a person who is happy.",
                    "label": 0
                },
                {
                    "sent": "This would be much more complicated or find a picture with the sunset.",
                    "label": 0
                },
                {
                    "sent": "These kind of things.",
                    "label": 0
                },
                {
                    "sent": "So that's semantic context, content and now convolutional neural networks have increasingly become able to solve semantic problems.",
                    "label": 0
                },
                {
                    "sent": "And also in audio.",
                    "label": 0
                },
                {
                    "sent": "As you will see, OK.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, this is a quick summary.",
                    "label": 0
                },
                {
                    "sent": "Neural networks.",
                    "label": 0
                },
                {
                    "sent": "Deep neural networks and why has it improved so much in the past decade?",
                    "label": 1
                },
                {
                    "sent": "I would actually say well, because now we have got we not I personally, but there's such.",
                    "label": 0
                },
                {
                    "sent": "There are these huge databases.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's this big data.",
                    "label": 0
                },
                {
                    "sent": "And of course, if we've got a lot of data, we can learn a lot.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is probably the main reason, and learning is usually performed using some gradient descent based interference of the weights which are involved in the network.",
                    "label": 0
                },
                {
                    "sent": "And I also just wanted to mention this because I think it's very important that there's a lot of tricks in the network community, so and these tricks were very important for boosting the success.",
                    "label": 0
                },
                {
                    "sent": "So one of the most important ones is dropout.",
                    "label": 0
                },
                {
                    "sent": "I will not explain this because it would go beyond.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient descent, that's a very interesting approach, because what it means is that you.",
                    "label": 0
                },
                {
                    "sent": "Randomly choose parts of your data in order to learn, and this is also an idea which Interestingly improves performance in many.",
                    "label": 0
                },
                {
                    "sent": "In many contexts.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So quick.",
                    "label": 0
                },
                {
                    "sent": "Just quick illustration of basic building blocks in newer networks I think.",
                    "label": 0
                },
                {
                    "sent": "Or I guess everyone has heard of perception mean this is also this looks like a graph, yes and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the perceptron is the you could say the seed.",
                    "label": 0
                },
                {
                    "sent": "So the most original idea of neural networks.",
                    "label": 0
                },
                {
                    "sent": "What happens?",
                    "label": 0
                },
                {
                    "sent": "You have an input.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's here.",
                    "label": 0
                },
                {
                    "sent": "You on the top you can see it.",
                    "label": 0
                },
                {
                    "sent": "You multiply the input by some weights.",
                    "label": 0
                },
                {
                    "sent": "You sum up and then you pass it through a non linearity.",
                    "label": 0
                },
                {
                    "sent": "So this is somehow the idea why it's called neural networks because that's more or less how our brain works, right?",
                    "label": 0
                },
                {
                    "sent": "I mean you have some neurons and they are depending on the level of activity they are on zero or not, so they fire.",
                    "label": 0
                },
                {
                    "sent": "They don't fire, so that was the original idea to somehow imitate the brain.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the node.",
                    "label": 0
                },
                {
                    "sent": "So each now in the in the lower in the lower plots.",
                    "label": 1
                },
                {
                    "sent": "Each node has this kind of character and here you can see what what is the input layer.",
                    "label": 0
                },
                {
                    "sent": "That's the basically the input, then hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So you've got the as many of those nodes.",
                    "label": 1
                },
                {
                    "sent": "If you want to.",
                    "label": 0
                },
                {
                    "sent": "Usually you want to three, but many hundreds, and then you've got the output layer.",
                    "label": 0
                },
                {
                    "sent": "And in deep neural networks, you've got many hidden layers.",
                    "label": 1
                },
                {
                    "sent": "And of course the non linearity.",
                    "label": 0
                },
                {
                    "sent": "What I call transformation here not, it does not necessarily have to be such a step function as in the original perceptron, but usually it will be some more smooth functions, simply because it's easier for gradient based learning.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And another nice picture.",
                    "label": 0
                },
                {
                    "sent": "So now what I showed you before is a general neural network.",
                    "label": 0
                },
                {
                    "sent": "What you can see here is not a network, but it's in kind of illustration of a convolutional neural network.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I found this and I find I found this picture to find it quite insightful.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you have never heard about convolutional neural networks, because here you can see if you input an image.",
                    "label": 0
                },
                {
                    "sent": "And then by using convolutional kernels, so you extract certain certain features of the image, so the convolutional kernels really what they do is they really act like a filter on the image.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine that you have a big image like this, sign here and then you apply.",
                    "label": 0
                },
                {
                    "sent": "You compare locally to some kernels.",
                    "label": 0
                },
                {
                    "sent": "Yes, for example you would like to in the first in one kernel, for example, you only want to find.",
                    "label": 0
                },
                {
                    "sent": "Lines which go in a certain direction, yes, and whenever you have a good.",
                    "label": 0
                },
                {
                    "sent": "If those if the points in the image and your kernel correspond sufficiently well, then you are going to keep that pixel, and if not, you're going to set it to zero basically so you're looking for particular features in your image, you're doing this in the whole image with the same weights, so that's what's called weight sharing.",
                    "label": 0
                },
                {
                    "sent": "And this is very important because that way local structure which obviously is something that we use when we recognize images, we were going to compare locally different features of a human being.",
                    "label": 0
                },
                {
                    "sent": "For example, will see does this is their nose.",
                    "label": 0
                },
                {
                    "sent": "Are their eyes?",
                    "label": 0
                },
                {
                    "sent": "Do we find those images?",
                    "label": 0
                },
                {
                    "sent": "Then we will conclude or high it's a human being, yes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's now that's because this for me is very important now more schematically or more reduced than before difference between general neural network and convolution.",
                    "label": 0
                },
                {
                    "sent": "I mean sorry general dense layer in a neural network and the convolution layer.",
                    "label": 0
                },
                {
                    "sent": "So the difference is really the amount of connections that you allow for and the weights that you allow for the connections.",
                    "label": 0
                },
                {
                    "sent": "So here I didn't put the.",
                    "label": 0
                },
                {
                    "sent": "The labeling for all the weights because here for every connection you are basically allowed to have a different weight.",
                    "label": 0
                },
                {
                    "sent": "So what that means?",
                    "label": 0
                },
                {
                    "sent": "I guess I may use the what that means.",
                    "label": 0
                },
                {
                    "sent": "If you have the matrix, the linear mapping that connects the nodes, then this basically just means that here you could.",
                    "label": 0
                },
                {
                    "sent": "Basically all the so let's see Omega 112, Omega M1 and MN.",
                    "label": 0
                },
                {
                    "sent": "And they can all be different.",
                    "label": 0
                },
                {
                    "sent": "And none of them has to be 0, right?",
                    "label": 0
                },
                {
                    "sent": "So we've got everywhere.",
                    "label": 0
                },
                {
                    "sent": "Yeah, totally arbitrary matrix in the second case you've got a matrix that has very very restrictive conditions, so you only allow for.",
                    "label": 0
                },
                {
                    "sent": "I mean in this image in every row four only for two weights which are non zero and.",
                    "label": 0
                },
                {
                    "sent": "Each row.",
                    "label": 0
                },
                {
                    "sent": "His only the same weights?",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is a it's just a toplitz matrix, yes?",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "And so there are two two things that we can immediately observe.",
                    "label": 0
                },
                {
                    "sent": "One thing is that we have this locality.",
                    "label": 0
                },
                {
                    "sent": "Right, we're looking for local structures.",
                    "label": 0
                },
                {
                    "sent": "And we're looking for the same structures everywhere in the picture.",
                    "label": 0
                },
                {
                    "sent": "Two things plus.",
                    "label": 0
                },
                {
                    "sent": "We have by far fewer waits to learn, right?",
                    "label": 0
                },
                {
                    "sent": "So that's also good because learning.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is expensive as we will see in a minute.",
                    "label": 0
                },
                {
                    "sent": "So here just some applications.",
                    "label": 0
                },
                {
                    "sent": "I think I will not read this to you.",
                    "label": 0
                },
                {
                    "sent": "Because it's just at least, and it's absolutely not comprehensive either.",
                    "label": 0
                },
                {
                    "sent": "So there are many more applications.",
                    "label": 0
                },
                {
                    "sent": "It seems like now in our community, if you look for papers or deep learning, you will.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you're surprised because it seems that all the people try to solve all the problems with neural networks, which I find quite amusing sometimes.",
                    "label": 0
                },
                {
                    "sent": "But yeah, finance prediction of you know, stock market values and so on everywhere people earn money with it, in fact.",
                    "label": 0
                },
                {
                    "sent": "It seems that OK, these convolutional neural networks can.",
                    "label": 0
                },
                {
                    "sent": "They can do everything or anything that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "Even beating a human being in goal, which up to a few years ago, people said that this will never happen.",
                    "label": 0
                },
                {
                    "sent": "OK, the computers are better now than humans in chess.",
                    "label": 0
                },
                {
                    "sent": "That's OK, but some people said in God will never happen because this is such a complex game, it will never happen.",
                    "label": 0
                },
                {
                    "sent": "And then just, I think two years ago this happened, but I was quite sad to be honest because I thought it was pretty button.",
                    "label": 0
                },
                {
                    "sent": "Now there's the big butt.",
                    "label": 0
                },
                {
                    "sent": "This is a colleague of mine actually research that so the electricity of one game of Alphago is 3000 US dollars.",
                    "label": 0
                },
                {
                    "sent": "This is an approximation, but it's expensive.",
                    "label": 0
                },
                {
                    "sent": "It used more than thousand CPUs, one Red, 76 GPS and many, many scientists who programmed all this and then OK the best Go Player of the world.",
                    "label": 0
                },
                {
                    "sent": "Where does he use one brain, one coffee?",
                    "label": 0
                },
                {
                    "sent": "I mean, now you can talk about the value of a human brain.",
                    "label": 0
                },
                {
                    "sent": "Of course, maybe this is more valuable than $3000, but still.",
                    "label": 0
                },
                {
                    "sent": "It's what this should actually illustrate is OK.",
                    "label": 0
                },
                {
                    "sent": "It's not that you just use your little smart phone, and you're going to solve all the problems now.",
                    "label": 0
                },
                {
                    "sent": "You use huge databases and you use huge amount of GPUs and enormous amount of power somehow.",
                    "label": 0
                },
                {
                    "sent": "So why are we telling this?",
                    "label": 0
                },
                {
                    "sent": "Well, because those interesting problems which have semantic flavor, which are the ones we're interested in, they really need a lot.",
                    "label": 0
                },
                {
                    "sent": "So what we believe still is that the human brain can think about, for example, the nature characteristic of data and can do something smart before putting the data into the network.",
                    "label": 0
                },
                {
                    "sent": "So this is what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "And then I try to.",
                    "label": 0
                },
                {
                    "sent": "I would like to try to convey some of the ideas.",
                    "label": 0
                },
                {
                    "sent": "So instead of just saying we've got so many data, why not just plug them into computer and hope that the computer will learn something we said no before we plug it into the computer?",
                    "label": 0
                },
                {
                    "sent": "We think a little bit about our problem and we do something.",
                    "label": 0
                },
                {
                    "sent": "Smart, something mathematical.",
                    "label": 0
                },
                {
                    "sent": "Back to the beginning, we choose a nice representation and then we.",
                    "label": 0
                },
                {
                    "sent": "You say that we use the transformed data as an input.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is particular mean.",
                    "label": 0
                },
                {
                    "sent": "I think that this could be done for image processing as well, but in audio this is almost compulsory because in audio.",
                    "label": 0
                },
                {
                    "sent": "Among all papers from the applied people who write about audio machine learning, 99.9% will use some FFT based preprocessing.",
                    "label": 0
                },
                {
                    "sent": "And only the rest working work on the raw data.",
                    "label": 0
                },
                {
                    "sent": "This is something that people hardly ever do, and we will say see why.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good and this.",
                    "label": 0
                },
                {
                    "sent": "This is some.",
                    "label": 0
                },
                {
                    "sent": "Basically also just the recalculation error for you.",
                    "label": 0
                },
                {
                    "sent": "A reminder of what I said already before.",
                    "label": 0
                },
                {
                    "sent": "So what we do we design the frames and here this is another example.",
                    "label": 0
                },
                {
                    "sent": "Now I showed you before nonstationary Gabor frame that adept somehow in time the width of the window?",
                    "label": 0
                },
                {
                    "sent": "What you see here on the right hand side is a is a representation that adapts the resolution in frequency and this is something that we've been doing a lot because what you see here these two.",
                    "label": 0
                },
                {
                    "sent": "Representations, both of them up time frequency representations, so we have an evaluation overtime and an evaluation over frequency, but the left hand side it's just a standard spectrogram.",
                    "label": 0
                },
                {
                    "sent": "So what everyone does on the right and said you have got a constant Q spectrogram.",
                    "label": 0
                },
                {
                    "sent": "That's something that has been proposed along time ago, but we developed an implementation based on nonstationary Gabor frames and what you can see is that in some sense it looks like on the right hand side the information that's in the signal is.",
                    "label": 0
                },
                {
                    "sent": "Better distributed, yes.",
                    "label": 0
                },
                {
                    "sent": "Here we've got so many zeros up there and here it looks like OK.",
                    "label": 0
                },
                {
                    "sent": "The information in the lower frequencies is somehow clearer.",
                    "label": 0
                },
                {
                    "sent": "And then in the top frequencies where there seems to be not so much going on, we don't use so many coefficients.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea of consecutive transform and yeah.",
                    "label": 0
                },
                {
                    "sent": "And they are here.",
                    "label": 0
                },
                {
                    "sent": "This is the definition of frames that are promised before.",
                    "label": 0
                },
                {
                    "sent": "So we say now that a sequence of basic building blocks, GJ is the frame.",
                    "label": 0
                },
                {
                    "sent": "If this so called relaxed possible relation holds so you can see I mean, if you were asking for an author orthogonal basis of normal basis, then this would have to be equal to F norm of F squared.",
                    "label": 0
                },
                {
                    "sent": "Now we only relax it in the sense that it has to be bounded above and below by some multiple of the constant of F. Of the normal faith.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is this is for you to see what such a gobble frame vertical frame looks like, so the gobble frame is at the basis of the short term for transform that I showed you.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned before.",
                    "label": 0
                },
                {
                    "sent": "At the very beginning, actually shortening for transform analyzes the signal using localized windows.",
                    "label": 0
                },
                {
                    "sent": "And these windows are then shifted in time, so they are shifted to all the time points you're interested in and then modulate it.",
                    "label": 0
                },
                {
                    "sent": "So this is like a local Fourier transform, I mean, because Fourier transform is also just a modulation.",
                    "label": 0
                },
                {
                    "sent": "So this is an illustration of that.",
                    "label": 0
                },
                {
                    "sent": "This is what the building blocks look like.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Good, now we.",
                    "label": 0
                },
                {
                    "sent": "I hope I've given enough information for you to see to you to see why these three questions are interesting.",
                    "label": 0
                },
                {
                    "sent": "Interesting for us.",
                    "label": 0
                },
                {
                    "sent": "So very interesting for us, so we now ask since there are so many possible ways to represent a signal, we ask?",
                    "label": 0
                },
                {
                    "sent": "Well, can we say something about what representation in your network would learn if the network is given certain certain data set?",
                    "label": 0
                },
                {
                    "sent": "Because if a network learns particular representation, you can assume that this is somehow represents the characteristics of the data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the second question which was answered to an extent not by us, but by some colleagues, is if we.",
                    "label": 0
                },
                {
                    "sent": "If we did not use a representation in the 1st place but worked directly on the on the raw signal.",
                    "label": 0
                },
                {
                    "sent": "Candace improve performance if we really have a lot of data, or will it always stay approximately the same?",
                    "label": 0
                },
                {
                    "sent": "And this is kind of the dual question to the second question.",
                    "label": 0
                },
                {
                    "sent": "This is the one that I'm more interested in.",
                    "label": 0
                },
                {
                    "sent": "If I if I design these smart representations that encode symmetries in the in the signal in the signals, then can we reduce the network size and learn faster?",
                    "label": 0
                },
                {
                    "sent": "And the answer is to some extent yes.",
                    "label": 0
                },
                {
                    "sent": "So we have been able to propose some representations which really improve performance with smaller networks.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is one of them.",
                    "label": 0
                },
                {
                    "sent": "So this is a this is a problem that PhD student of mine is mainly working on.",
                    "label": 0
                },
                {
                    "sent": "This is a transformation which I will define a little later.",
                    "label": 0
                },
                {
                    "sent": "It's garbage scattering.",
                    "label": 0
                },
                {
                    "sent": "It's called Garber scattering and this is also based on Gabor frames which I just introduced before with this picture and hear what what we do or what was Vito does.",
                    "label": 0
                },
                {
                    "sent": "Is she iteratively applies Garber phrase so in several layers and why I put this image here is because I want you to understand what I mean by in.",
                    "label": 0
                },
                {
                    "sent": "But invariants or symmetry in some sense.",
                    "label": 0
                },
                {
                    "sent": "So, because here the signal, which is a synthetic signal, consists of two synthetic tones.",
                    "label": 0
                },
                {
                    "sent": "So you can see it's a tone with a lot of overtones and onset.",
                    "label": 0
                },
                {
                    "sent": "And the frequency content is the same in both, while in the second term.",
                    "label": 0
                },
                {
                    "sent": "So here you've got an amplitude modulation, so the tone gets louder and softer, so it goes, Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "And here we now we created two layers and you can see that the first layer contains all the frequency content.",
                    "label": 0
                },
                {
                    "sent": "But it seems to be invariant to the amplitude information.",
                    "label": 0
                },
                {
                    "sent": "So to the changes in loudness, yes this is smooth out.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the second layer has lost frequency information.",
                    "label": 0
                },
                {
                    "sent": "So in this second layer does not tell you anything about the frequency that's going on in the in the signal, but on the other hand you can see that it stores the amplitude amplitude information.",
                    "label": 0
                },
                {
                    "sent": "So here this is the frequency of the modulation of the amplitude.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and now since we have already had so much information, I want to give you a short.",
                    "label": 0
                },
                {
                    "sent": "This is the only music unfortunately that's going to that you're going to hear today during my talk, but at least and in order to give you a quick glimpse of what can be done and the problem is singing voice detection and we'll come back to this later.",
                    "label": 0
                },
                {
                    "sent": "This is an important problem.",
                    "label": 0
                },
                {
                    "sent": "Also commercially.",
                    "label": 0
                },
                {
                    "sent": "Obviously.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "Well, as the name hints already, singing voice detection means that in a signal in a music piece, you want to say exactly when a person thinks or human things or not, or when no one thinks that only instruments play.",
                    "label": 0
                },
                {
                    "sent": "And somehow for some reason, my colleagues from the this is my colleagues from the Institute of.",
                    "label": 0
                },
                {
                    "sent": "Artificial intelligence in Vienna.",
                    "label": 0
                },
                {
                    "sent": "They work a lot on this problem and it seems to be of commercial interests, because how much the musicians get paid depends on whether someone thinks or not.",
                    "label": 0
                },
                {
                    "sent": "Don't ask me why, but this seems to be somehow the reason why people are so interested in this.",
                    "label": 0
                },
                {
                    "sent": "And yeah, and please keep this.",
                    "label": 0
                },
                {
                    "sent": "Number in mind.",
                    "label": 0
                },
                {
                    "sent": "So the my colleagues they have reached the state of the art for this problem, so they have a very good amount of recognition and they use a network with 1.4 million weights and you will see later that we were able to reduce this number significantly by using smart input.",
                    "label": 0
                },
                {
                    "sent": "But anyway, sorry now.",
                    "label": 0
                },
                {
                    "sent": "I see that I'm too slow, so I will probably.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is this.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "What you see is a spectrogram again, and what you see down there is the question is the so to say the.",
                    "label": 0
                },
                {
                    "sent": "Ground truth on where do I find?",
                    "label": 0
                },
                {
                    "sent": "Where is someone singing into where not and what's funny?",
                    "label": 0
                },
                {
                    "sent": "And you will see I mean, for me this is very obvious if you're familiar with spectrograms, but you can hopefully also see you can be will be able to tell from in spectrogram you can see the singer because this is whenever you have got this lurde in frequency components.",
                    "label": 0
                },
                {
                    "sent": "So let's listen.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's here, Sir.",
                    "label": 0
                },
                {
                    "sent": "Outside snows fall.",
                    "label": 0
                },
                {
                    "sent": "Cancel.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is my colleague Jan Schlueter working.",
                    "label": 0
                },
                {
                    "sent": "We were working together in a project.",
                    "label": 0
                },
                {
                    "sent": "He's now in France.",
                    "label": 0
                },
                {
                    "sent": "And he and his colleagues were very successful.",
                    "label": 0
                },
                {
                    "sent": "But you can what you can see here is a standard problem.",
                    "label": 0
                },
                {
                    "sent": "You get.",
                    "label": 0
                },
                {
                    "sent": "You get the spectrum input, you get the annotations and then you have to learn to deduce from the spectrogram whether or not there is a human voice present.",
                    "label": 0
                },
                {
                    "sent": "And of course the hope is that this is going to work not only in the data for which you have the information, but on new data.",
                    "label": 0
                },
                {
                    "sent": "This is what we call generalization.",
                    "label": 0
                },
                {
                    "sent": "So oops, try to get rid of this.",
                    "label": 0
                },
                {
                    "sent": "Yeah good.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I will try to be very fast here because here I basically just tried to give more details of the mathematical description, but I see that I'm already a bit late.",
                    "label": 0
                },
                {
                    "sent": "And I will go through this quickly.",
                    "label": 0
                },
                {
                    "sent": "In principle this is really just a formalization, or more details on how short.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Free transform work.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you can see the effect of again wider and smaller windows and now this is the.",
                    "label": 0
                },
                {
                    "sent": "This may be the only point where I want to be a bit more precise.",
                    "label": 0
                },
                {
                    "sent": "Again re collection of we saw what we saw before.",
                    "label": 0
                },
                {
                    "sent": "So here's the spectrogram.",
                    "label": 0
                },
                {
                    "sent": "So this is these matrices that you always see.",
                    "label": 0
                },
                {
                    "sent": "So this is basically just defined as you multiply your signal F with shifted window G and then you take Fourier transform and this can therefore you can write the spectrogram also as the absolute value squared.",
                    "label": 0
                },
                {
                    "sent": "Of the short term for transform, and obviously, since Fourier transform again is just an integral, this can be written like this, and if we take alternative.",
                    "label": 0
                },
                {
                    "sent": "So what you see here?",
                    "label": 0
                },
                {
                    "sent": "Maybe that's the important point.",
                    "label": 0
                },
                {
                    "sent": "The GKL adjust this shift.",
                    "label": 0
                },
                {
                    "sent": "Didn't modulated windows that are illustrated.",
                    "label": 0
                },
                {
                    "sent": "Minutes ago, and now it.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, in the case of nonstationary frames we have Windows which are not just shifted in frequency, but you can stretch them and that way adapt them somehow to the frequency and you can split in some sense the frequency axis in different ways as you wish so.",
                    "label": 0
                },
                {
                    "sent": "In the this is time.",
                    "label": 0
                },
                {
                    "sent": "And this is frequency.",
                    "label": 0
                },
                {
                    "sent": "Then in the short inference from what you do, you split this.",
                    "label": 0
                },
                {
                    "sent": "Exist so the frequency axis in completely equal intervals in adaptive transforms you can say.",
                    "label": 0
                },
                {
                    "sent": "Well, no, I don't want this.",
                    "label": 0
                },
                {
                    "sent": "I want it differently.",
                    "label": 0
                },
                {
                    "sent": "So for example I want to have a better resolution in the lower frequencies.",
                    "label": 0
                },
                {
                    "sent": "And then I'm happy with.",
                    "label": 0
                },
                {
                    "sent": "Not so good resolution in the higher frequencies or you could have it differently, so our systems might learn something that has good resolution up to some point and then not so good resolution and maybe something better again.",
                    "label": 0
                },
                {
                    "sent": "And so we are completely free to do this the way we want.",
                    "label": 0
                },
                {
                    "sent": "And yeah, that's what we call nonstationary Gabor framing, mathematically speaking.",
                    "label": 0
                },
                {
                    "sent": "Now, we have still shifted versions of a filter Bank of Windows in the filter bank.",
                    "label": 0
                },
                {
                    "sent": "But the filter bank is adaptive, so it's not.",
                    "label": 0
                },
                {
                    "sent": "We're not restricted to just have shifted Windows in freak.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good and yes.",
                    "label": 0
                },
                {
                    "sent": "So now what we've seen is that a spectrogram essentially expresses signal properties clear much more clearly than what we see in raw audio data, and this is leads us to an ocean that's important in all the machine learning community which is feature extractor.",
                    "label": 0
                },
                {
                    "sent": "So in general we can say that a feature extractors is mapping that Maps our signal or given data.",
                    "label": 0
                },
                {
                    "sent": "Into some space that might be much higher dimensional but is more informative.",
                    "label": 0
                },
                {
                    "sent": "So in particular, as you saw in the garbage scattering, we might split it in an array of images and each of the images in this array might give us different information, yes, so that's why we've got this RR to the M12 times etc.",
                    "label": 0
                },
                {
                    "sent": "MD.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So one example again now that is deduced from spectrogram derived from the spectrogram by taking weighted averages over frequency.",
                    "label": 0
                },
                {
                    "sent": "This is the Mail spectrogram that's the second most used representation in audio processing, which is why I mentioned here, and also because we might come back to this later.",
                    "label": 0
                },
                {
                    "sent": "So what happens here?",
                    "label": 0
                },
                {
                    "sent": "In principle you take first a regular spectrogram, but then you average over frequency.",
                    "label": 0
                },
                {
                    "sent": "And what does averaging mean?",
                    "label": 0
                },
                {
                    "sent": "Everything always means that you get rid of some.",
                    "label": 0
                },
                {
                    "sent": "Some information that's in the signal and getting rid of some information that you don't need means that you create a certain.",
                    "label": 0
                },
                {
                    "sent": "You always create certain invariants.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you average over some frequency interval, that means that all signals that have the same information within these intervals are going to belong to the same class later, and this is something that is actually at the core of learning, because that's what we all do.",
                    "label": 0
                },
                {
                    "sent": "I mean, if the little children learn what a dog is first, you think how would they ever learn that?",
                    "label": 0
                },
                {
                    "sent": "So I don't know the English name.",
                    "label": 0
                },
                {
                    "sent": "Small tiny dog and the big, you know, German Shepherd is the same species, but they learn, and that's because they're able to basically discard or get rid of all the information that's not important.",
                    "label": 0
                },
                {
                    "sent": "And this is the average over all the information that they get from their parents and teachers about what the dog actually is.",
                    "label": 0
                },
                {
                    "sent": "And we do the same.",
                    "label": 0
                },
                {
                    "sent": "Yes, in some sense we average along certain dimensions in order to extract the frequency exactly that we need.",
                    "label": 0
                },
                {
                    "sent": "And of course in neural network learning you would like to see.",
                    "label": 0
                },
                {
                    "sent": "You would like.",
                    "label": 0
                },
                {
                    "sent": "You can even ask the network how much everything do we need, and this is something that we did.",
                    "label": 0
                },
                {
                    "sent": "And let's see.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will skip the exact precise definition of garbage.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring because this is not important.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, this is so the hope.",
                    "label": 0
                },
                {
                    "sent": "The hope if we.",
                    "label": 0
                },
                {
                    "sent": "If we if we design these features.",
                    "label": 0
                },
                {
                    "sent": "Which encodes exactly the symmetries that we need.",
                    "label": 0
                },
                {
                    "sent": "Is that instead of learning a very complicated mapping which we would need for example for if we learn directly on raw data, we hope that we can do something very simple.",
                    "label": 0
                },
                {
                    "sent": "And of course the most simple.",
                    "label": 0
                },
                {
                    "sent": "Thing that we can do is basically just what we've seen in this perception.",
                    "label": 0
                },
                {
                    "sent": "So just apply a linear map.",
                    "label": 0
                },
                {
                    "sent": "So like here.",
                    "label": 0
                },
                {
                    "sent": "So we say that feature extractor separates F. So the function that we want to learn linearly if F is at least sufficiently closely approximated by just a linear projection.",
                    "label": 1
                },
                {
                    "sent": "So projection onto a set vector of weights.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, yes.",
                    "label": 0
                },
                {
                    "sent": "So this is, this would be the simplest case, and in fact a network and neural network does an iteration of this kind of.",
                    "label": 0
                },
                {
                    "sent": "Projection.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and now we did and I did exactly into what I promised at the very beginning at the beginning.",
                    "label": 0
                },
                {
                    "sent": "Sometime though, we there's a recent very recent paper.",
                    "label": 0
                },
                {
                    "sent": "You can see this was Autumn 2018, where a colleague so real NATO and some other colleagues they showed that if you have.",
                    "label": 0
                },
                {
                    "sent": "Enormous amounts of data then.",
                    "label": 0
                },
                {
                    "sent": "In fact you can learn from raw audio and they had a very interesting problem because their problem is we discussed it a while ago in Vienna.",
                    "label": 0
                },
                {
                    "sent": "The problem is to learn semantic tags that users put on.",
                    "label": 0
                },
                {
                    "sent": "Songs so text like this is good music for chugging this music is relaxing, so they had a set of properties and then they so this is he works for Pandora in California and they have an T. That's what they write.",
                    "label": 0
                },
                {
                    "sent": "They also described in this article.",
                    "label": 0
                },
                {
                    "sent": "They have enormous amounts of data and I think it's 500,000 hours of music or something.",
                    "label": 0
                },
                {
                    "sent": "I don't recall exactly the I think I mentioned it here.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Pandora owns 1.5 millions of manually annotated music tracks, yes.",
                    "label": 0
                },
                {
                    "sent": "And they will not give you the data.",
                    "label": 0
                },
                {
                    "sent": "They will not give the data to US academics, of course, but what they showed, and I think it's very nice.",
                    "label": 0
                },
                {
                    "sent": "They really showed exactly to which point it's better to use spectrograms or some structured input and from which point onward you can get better results from learning on raw data.",
                    "label": 0
                },
                {
                    "sent": "But I mean, it's really huge, huge and we will never possess this, and trading time is around 4 weeks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hope I convinced you that it's for us.",
                    "label": 0
                },
                {
                    "sent": "It's better since we always have restricted amount of data.",
                    "label": 0
                },
                {
                    "sent": "It's always better to encode the info.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are interested we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So this is the maybe if you're interested to read more about the scattering transform that I mentioned, which directly encodes invariances, then this would be the links.",
                    "label": 0
                },
                {
                    "sent": "I can put.",
                    "label": 0
                },
                {
                    "sent": "The can give the I can put the slides on lines and.",
                    "label": 0
                },
                {
                    "sent": "This is a work that we have now submitted.",
                    "label": 0
                },
                {
                    "sent": "And where we were able to show that the scattering transform for some problems, so we use this on synthetic data and also on instrument recognition.",
                    "label": 0
                },
                {
                    "sent": "So the problem to recognize instruments from a huge database and there we were able to show that GABA scattering actually for very small networks gives significantly better results than classical representations.",
                    "label": 0
                },
                {
                    "sent": "So this is, I think this is really interesting.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, I think I will.",
                    "label": 0
                },
                {
                    "sent": "Do you have 10 more minutes or yes?",
                    "label": 0
                },
                {
                    "sent": "OK, so then I will still talk about this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is now the basically last idea.",
                    "label": 0
                },
                {
                    "sent": "But it's totally in line of what I have already explained, because now I would like to formalize this idea that we have to look at the feature that we use the feature extractor and the network and I have to look at those two.",
                    "label": 0
                },
                {
                    "sent": "Ingredients somehow in connection with each other.",
                    "label": 0
                },
                {
                    "sent": "And I have to think about them with respect to the data set with I'm interested in because usually the data that we get from they are highly structured.",
                    "label": 0
                },
                {
                    "sent": "The probably come from some low dimensional manifold or some approximation of it.",
                    "label": 0
                },
                {
                    "sent": "They're not, they never filled high dimensional Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So when we ask whether a pair of feature extractor and network is better than another one, then we have to consider these pairs and the data set, which is why we gave this.",
                    "label": 0
                },
                {
                    "sent": "Definition so we have.",
                    "label": 0
                },
                {
                    "sent": "We are considered 2 feature network pairs.",
                    "label": 0
                },
                {
                    "sent": "Yes so.",
                    "label": 0
                },
                {
                    "sent": "Big Fire J&J to the network means is really the architecture.",
                    "label": 0
                },
                {
                    "sent": "Then we say that the first one is subordinate to the other one to the second one with respect to a given data set.",
                    "label": 0
                },
                {
                    "sent": "If for all parameter vectors which actually realize the network.",
                    "label": 0
                },
                {
                    "sent": "So all the weights in the network.",
                    "label": 0
                },
                {
                    "sent": "In the first setting, there exists a parameter set.",
                    "label": 0
                },
                {
                    "sent": "In the second setting such that every function that can be realized by the first network can also be realized by the second one, yes.",
                    "label": 0
                },
                {
                    "sent": "And then if so, we call the two pairs equivalent with respect to D if they are supporting it to each other.",
                    "label": 0
                },
                {
                    "sent": "Chris",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now this is the formulas formalization somehow to off this symmetry, because if we have if we have a data set D. And we augment it.",
                    "label": 1
                },
                {
                    "sent": "By applying certain operators, so for example, small frequency for small shifts in time.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine that if we have if you have music or and you shift it a little bit in time, then it will still should still belong to the same class or much more, maybe obvious or much more intuitive.",
                    "label": 0
                },
                {
                    "sent": "In images.",
                    "label": 0
                },
                {
                    "sent": "I always use this example if you look for.",
                    "label": 0
                },
                {
                    "sent": "For a cloud or dog image then you would probably want this to be invariant to rotation, right?",
                    "label": 0
                },
                {
                    "sent": "Because if the cat is laying on the back, it's still a cat or a dog.",
                    "label": 0
                },
                {
                    "sent": "I think I said the dog yet so you would know what people do in the machine learning community.",
                    "label": 0
                },
                {
                    "sent": "They really use augmentation so they use that data and they apply certain operators, for example rotation and they tell they tell the network well if I rotate the image, it still is the same class.",
                    "label": 0
                },
                {
                    "sent": "Right, of course we mathematicians think, well, why do I have to teach the network in such a rather?",
                    "label": 0
                },
                {
                    "sent": "Rahway, if we can.",
                    "label": 0
                },
                {
                    "sent": "Maybe design A feature extractor that does this for me, yes, and that's the idea of.",
                    "label": 0
                },
                {
                    "sent": "Of this proposition that if I have an augmented data set, and if I design A, where is it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, if I decide to feature is extracted big file 2 which is invariant to this augmentation, right?",
                    "label": 0
                },
                {
                    "sent": "So which means that the feature extractor ignores this, AUGMENT, ignores the operators here, then I can say if the network is.",
                    "label": 0
                },
                {
                    "sent": "If I want to subordinate to file two and two with respect to this data set.",
                    "label": 1
                },
                {
                    "sent": "And use the augmentation and then invitee is invariant to this augmentation.",
                    "label": 1
                },
                {
                    "sent": "Then five one is will also be subordinate to 5, two and two with respect to the augmented data set.",
                    "label": 0
                },
                {
                    "sent": "Yes, which means that I will not.",
                    "label": 0
                },
                {
                    "sent": "I will be able to use again the smaller network.",
                    "label": 0
                },
                {
                    "sent": "This is just an example which is very clear for people who know that of course if you use.",
                    "label": 0
                },
                {
                    "sent": "The absolute value of the short term for transformer, even the Fourier transform.",
                    "label": 0
                },
                {
                    "sent": "This is face in this invariant to phase shift.",
                    "label": 0
                },
                {
                    "sent": "And therefore augmenting data set by face.",
                    "label": 0
                },
                {
                    "sent": "By just multiplication with the face factor is an augmentation that will be ignored by short inference about spectrogram, sorry.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and this may be just a mention.",
                    "label": 0
                },
                {
                    "sent": "The formal proof of this will appear, hopefully in a paper soon.",
                    "label": 0
                },
                {
                    "sent": "This refers to a very interesting article by Urasoe College and colleagues that.",
                    "label": 0
                },
                {
                    "sent": "Where he where they showed that they were talking about generalization, but I cannot go into details on this anymore, but I think I mentioned it briefly.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "Generalization?",
                    "label": 0
                },
                {
                    "sent": "Well, generalization is the property of a network to not only function on the data set data set that it learns on, but also new data.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is quite obvious, yes, and this is it's obvious that we're interested in that situation because, for example, if you think about automatic driving, if the if the automatic driving system is only able to.",
                    "label": 0
                },
                {
                    "sent": "Deal with the situation is that it has seen exactly before.",
                    "label": 0
                },
                {
                    "sent": "Like for example, if there is a dog on the street, I should stop, but it has not learned that if there is whatever else on the street, I should also stop, then it will be useless.",
                    "label": 0
                },
                {
                    "sent": "Yes, so we need this kind of smoothing in order to be able to deal with new situations.",
                    "label": 0
                },
                {
                    "sent": "That's what we called generalization and what they what they showed in this paper and which I put in this proposition is.",
                    "label": 0
                },
                {
                    "sent": "So if we introduce invariants to augmentation in a stable learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "Then we can reduce the generalization error exactly by the fraction of the covering of the.",
                    "label": 0
                },
                {
                    "sent": "Of the day of the original data set divided by the covering, I mean the covering index of the augmented data set.",
                    "label": 0
                },
                {
                    "sent": "So covering index you can imagine is describes the complexity of the data set.",
                    "label": 0
                },
                {
                    "sent": "Again, I will not now give you the exact.",
                    "label": 0
                },
                {
                    "sent": "Definition anymore, but what message of the?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is if we use again if we use feature extractors that are have this useful invariances then generalization error is smaller.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "And now the very last thing.",
                    "label": 0
                },
                {
                    "sent": "Was is now.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This comes back to really the question, can I learn which representation is the best represent which representation is the best representation for my problem in the data set?",
                    "label": 0
                },
                {
                    "sent": "And here we looked at singing voice detection again and we were trying to reduce this huge network.",
                    "label": 0
                },
                {
                    "sent": "You remember 1.4 million.",
                    "label": 0
                },
                {
                    "sent": "Wait by allowing certain activity in the frequency axis.",
                    "label": 0
                },
                {
                    "sent": "So instead of using a strict predis, predesigned resolution of frequency axis, we allowed the network to learn.",
                    "label": 0
                },
                {
                    "sent": "Which is the best resolution of the frequency axis?",
                    "label": 1
                },
                {
                    "sent": "And yeah, I think for the moment I will skip.",
                    "label": 0
                },
                {
                    "sent": "Maybe the mathematical details.",
                    "label": 0
                },
                {
                    "sent": "We can discuss them later if someone is interested in this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But yeah, here's here's a huge proposition which gives you the technical details why.",
                    "label": 0
                },
                {
                    "sent": "Our statement is true, and here's the statement.",
                    "label": 0
                },
                {
                    "sent": "So if we have now one convolutional neural network with DC convolutional layers and then a second one, which is analog but has one additional convolutional layer which has only a finite so very short number of weights which allows for averaging in time.",
                    "label": 0
                },
                {
                    "sent": "Then we know that if we use.",
                    "label": 0
                },
                {
                    "sent": "Miss Spectrogram and the first network then this is subordinate to using an adaptive.",
                    "label": 0
                },
                {
                    "sent": "Spectrogram this is the essay you may have defined it before, so using this adaptive resolution of the frequency axis and the same network but with one additional convolutional layers if.",
                    "label": 0
                },
                {
                    "sent": "The windows are chosen in an inappropriate way.",
                    "label": 0
                },
                {
                    "sent": "And so this is something that we proved.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mathematically, and then, we could show that understanding voice detection.",
                    "label": 0
                },
                {
                    "sent": "In fact, relaxing this resolution gives us a better performance.",
                    "label": 0
                },
                {
                    "sent": "While decreasing the size of the network dramatically, and what does it mean?",
                    "label": 0
                },
                {
                    "sent": "This is here, maybe it's not so important to understand all the details, but what you might now recall.",
                    "label": 0
                },
                {
                    "sent": "So the network architecture is similar to before, but much much smaller.",
                    "label": 0
                },
                {
                    "sent": "So instead of 1.4 million weights in the first case we have 94,000 something in the second 53,000.",
                    "label": 0
                },
                {
                    "sent": "Something weights, so this is much smaller.",
                    "label": 0
                },
                {
                    "sent": "And here the.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, this is not the results.",
                    "label": 0
                },
                {
                    "sent": "These are the different inputs.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here the results.",
                    "label": 0
                },
                {
                    "sent": "So what you can see?",
                    "label": 0
                },
                {
                    "sent": "This is, the measure is area above area.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You see?",
                    "label": 0
                },
                {
                    "sent": "I can't recall at the moment, but this is what it is.",
                    "label": 0
                },
                {
                    "sent": "Maybe someone can.",
                    "label": 0
                },
                {
                    "sent": "Usually it's a you see and then.",
                    "label": 0
                },
                {
                    "sent": "Higher number is better, but in our case lower number means it's better and what you can see here I don't have pointer but here you see that the adaptive version.",
                    "label": 0
                },
                {
                    "sent": "So here STT adaptive and hear filterbank with variable width is better than the comparison which is here.",
                    "label": 0
                },
                {
                    "sent": "So small one is the smallest one with really the.",
                    "label": 0
                },
                {
                    "sent": "So it seems area over the Roc curve, so now my brain works again.",
                    "label": 0
                },
                {
                    "sent": "I'm also a bit tired already, so here you can see this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the reference implementation and this is the implementation allowing for adaptivity, and you can see that adaptivity gives us a small but significant improvement, and the same is true when we have filterbank with variable with time averaging.",
                    "label": 0
                },
                {
                    "sent": "OK. Good.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quick summary.",
                    "label": 0
                },
                {
                    "sent": "What I wanted to convey today is that.",
                    "label": 0
                },
                {
                    "sent": "Appropriate representation systems give us.",
                    "label": 0
                },
                {
                    "sent": "Good, more concise representations of structured signals, in particular audio signals.",
                    "label": 0
                },
                {
                    "sent": "We saw that adaptivity can further enhance readability of representation coefficients and.",
                    "label": 0
                },
                {
                    "sent": "Then the two questions.",
                    "label": 0
                },
                {
                    "sent": "The answers to the two questions deep learning can help us to fine tune time frequency resolutions.",
                    "label": 0
                },
                {
                    "sent": "If we, as we've just seen and on the other hand, if we impose known structure, that's what we saw in the garbage scattering and symmetry.",
                    "label": 0
                },
                {
                    "sent": "Then we can improve learning and generalization while using way smaller architectures.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and this is ongoing work, of course is everything in deep learning and we are now looking at new problems and new representations.",
                    "label": 0
                },
                {
                    "sent": "All the time and sometimes we are able to improve state of the art, sometimes not, and I thank you very much for your attention and will be happy to answer questions.",
                    "label": 0
                }
            ]
        }
    }
}