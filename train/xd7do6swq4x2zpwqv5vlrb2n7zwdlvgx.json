{
    "id": "xd7do6swq4x2zpwqv5vlrb2n7zwdlvgx",
    "title": "Abstraction Augmented Markov Models",
    "info": {
        "author": [
            "Adrian Silvescu, Iowa State University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Computational Biology"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_silvescu_aamm/",
    "segmentation": [
        [
            "So in bioinformatics, sometimes we can set up our problem as sequence classification problem and this is so for example a protein localization or protein function prediction can be set up like this where the sequence is the sequence of amino acids and my my talk is going to be basically about.",
            "Sequence."
        ],
        [
            "Classification.",
            "So.",
            "I'll start with the standard way of addressing this problem, which is to use Markov models to model the sequences."
        ],
        [
            "And then I'll present.",
            "Our improved version of Markov models, called abstraction augmented Markov models, and then I'll move to X."
        ],
        [
            "Elements and results.",
            "So.",
            "The standard way too, so the standard way to set up sequence classification problem is we have data set where we have a bunch of data points who are sequenced sequences and their labels.",
            "And we would like to learn a classifier that correctly predicts the label and.",
            "One way to do this is to have a way to come up with the probability of the class given the example and the standard.",
            "Generative way of addressing this problem is to use the Bayes rule and to express the probability of Y given XX.",
            "To use the fact that the probability of Y given X is proportional to the probability of X given Y times the class probability and now all we have to do is say how we're going to model the probability of X."
        ],
        [
            "So the standard way.",
            "To model the probability of X where X is a sequence is to use the Markov model.",
            "So in a Markov model, we assume that each element in the sequence is dependent on the previous K elements.",
            "And it satisfies this.",
            "The Bayesian net that describe this thing satisfied this Markov property, and as a consequence the probability overall probability distribution over the sequence decomposes.",
            "I like this one of the problems with Markov models is that the complexity of KE order Markov model is exponential in K an hopefully."
        ],
        [
            "Our.",
            "I abstraction augmented Markov models will address this problem."
        ],
        [
            "So this is what I'm going to describe next.",
            "So again, in a Markov model.",
            "Each element is dependent on the previous K elements and what we want to do is to take this previous K elements somehow produce an abstraction hierarchy over this over this elements and then pick higher point in this abstraction hierarchy, which is going to be smaller than.",
            "The size of the vocabulary to the K and we use this thing to model the.",
            "Future."
        ],
        [
            "So an abstraction hierarchy in our is nothing but a tree where at the bottom will have in our case the all the K grams.",
            "For based on the sequence and.",
            "The tree is such that in each element in each year, not in the tree, each node in the tree represents the union of the leaf nodes in that subtree.",
            "And the cutting.",
            "This tree is basically a partition of the of the set of elements here.",
            "So basically a cut is a set of nodes that has the property that none of the node is.",
            "Descendant over in our node."
        ],
        [
            "So now given such a.",
            "Given such such a partition, we can.",
            "Assuming that we already have constructed abstraction hierarchy and we already have chosen a partition, what we can do is we can.",
            "Model the probability of XID Cup basically decouples the probability of the XI from the past using only the abstraction.",
            "Multiplied with the probability of the abstraction.",
            "Given the past kilogram, but this thing is going to be a functional dependency is not really probabilistic dependencies, so it's going to be either zero or one depending on whether the abstraction belongs to whether the kilogram belongs to the abstraction cluster or not."
        ],
        [
            "So in order to learn.",
            "The abstraction hierarchy.",
            "We use a hierarchical agglomerative clustering, so.",
            "We start with the bottom with all the all the K grams and then we recursively merge 22 abstraction.",
            "Note based on their similarity until we get a tree.",
            "So this is standard hierarchical."
        ],
        [
            "Clustering and the formula for merging is.",
            "Based on the Jensen Shannon, divergent, Jensen, Shannon distance, and.",
            "It can be shown that if you are to take two abstractions which are sets of elements and.",
            "Put them together into a bigger set, denoted by by this operation.",
            "Here the information loss do that is incurred due to.",
            "Doing such a merging is given by this formula and this is what we are used to pick.",
            "The two things to merge together based on the lowest information loss."
        ],
        [
            "So now back to the model for for 2 grams, the model looks like this, But this part is a functional dependency in the Bayesian network, so it is basically only.",
            "Saying whether the previous kilogram belongs to the cluster or not, and this can be.",
            "We don't have to store the whole matrix.",
            "All we need to store is for each kilogram to which clustering belongs.",
            "So in order to so, there is no estimation of this parameter, and in order to estimate this parameter we just count the number of times the.",
            "Each element in the in the future occurs together with the abstraction and divide by the by the number of times that abstraction occurs."
        ],
        [
            "So now do you to put all of these things together?",
            "First we learn an abstraction hierarchy using hierarchical agglomerative clustering.",
            "Then we estimate the for each class, wasting made the parameters for probability of X given the class for each class, and then in the end we used the Bayes rule to get.",
            "Posterior and we take the argmax."
        ],
        [
            "So.",
            "In terms of experiments, we experimented with three protein cell subcellular localization datasets."
        ],
        [
            "And so the first, the first type of experiment is where we learn one abstraction hierarchy for each.",
            "Separately for each class, so we take.",
            "All the all the K grams in each class and learn 11 abstraction hierarchy for based on those datasets and.",
            "Then for each possible cut in the tree.",
            "We train on.",
            "We trained for that M cut an abstraction augmented Markov model, and because we learn different abstraction hierarchy for each class, we can you know for Class 1 have an abstraction hierarchy tree that looks like this, and for the class to something like that looks like that, but picking up tree cut would involve speaking.",
            "These values.",
            "As the values for the abstraction.",
            "4 + 1 and involves picking these other tree values here."
        ],
        [
            "So.",
            "Here we compare.",
            "We have two baselines.",
            "One of them is the performance of the Markov models using all the kilograms.",
            "These other agree that the grid line the green line is the naive base, which is equivalent with Markov models of zero and.",
            "The blue line is the one where we look at the performance of the.",
            "Obstruction augmented Markov models, so we plot classification accuracy versus how many abstractions we have used, which is in a log scale.",
            "So one would mean 10 abstractions, so we can see that we.",
            "Much fewer.",
            "Abstractions we can get the same.",
            "Performance in terms of accuracy as using the total number of abstractions.",
            "So in this case we use 3 grams and the total number and number of three grams for protein sequences is about 8000 and.",
            "This is about 100 abstractions.",
            "So in some cases we we managed to to get the same performance or even exceed the performance of the full model with much fewer."
        ],
        [
            "Winters.",
            "We've also tried this thing in a semi supervised setting, so the way we set up our semi supervised setting is we assume that we don't have all the data are labeled.",
            "We assume, for example that we have only 1% of the data points labeled.",
            "So for example for.",
            "For one of the datasets, which is 1000 examples, we assume that only 10 of them we are only given for 10 of them the classes.",
            "And then we train one abstraction hierarchy for both from both the labeled and unlabeled examples, without looking at the class.",
            "And in this case, we train only one one abstraction hierarchy from the whole data set, because it's not.",
            "Very feasible to train it from only the labeled, especially in the 1% case."
        ],
        [
            "So then we compare what the performance is.",
            "If we so the first row of graphs over here is are the graphs where we have.",
            "Um?",
            "Well, only 1% of the samples labeled and this one is the.",
            "This one are the graphs where we have 10 percent 25% of the graph, so of course the performance of the Markov model is much much lower.",
            "But it turns out that we've used the abstractions and if we have much fewer examples labeled we are able to generalize better due to the fact that we have fewer parameters to fit.",
            "And the same thing.",
            "It's also present in the 25% case, but here the differences are much are somewhat smaller."
        ],
        [
            "Um?",
            "So there are.",
            "Three types of there are two so far we looked at.",
            "Using.",
            "Inferring the abstractions from the whole data set and the other case which was inferring the abstraction from.",
            "From inferring one abstraction hierarchy per class, so these are.",
            "Represented over here.",
            "The other possibilities too.",
            "Um, infer the abstraction hierarchy.",
            "Instead of.",
            "Do instead of trying to predict the next element and try to find the best best.",
            "Hierarchy that predicts the next element is to try to best predict the class, and there is some prior work.",
            "On on this thing.",
            "So.",
            "In this case, for the abstraction hierarchy, the target is Y and the generation on divergences computed between the conditional distributions of Y."
        ],
        [
            "So.",
            "In that case so.",
            "For this type of results we have, we have shown that so we would read we have the class conditional abstractions.",
            "And with the green we have the.",
            "Abstraction augmented model Markov models where we use only one.",
            "Abstraction hierarchy, which is derived from the whole data and the blue is the one with.",
            "As many abstraction hierarchies as there are classes, so it turns out that using.",
            "Using one abstraction hierarchy per class improves.",
            "Improve the performance but is not is not so necessarily so easy to use incentives to provide."
        ],
        [
            "The setups.",
            "So to summarize, we have described an algorithm for learning abstraction based Markov models on sequence classification task, namely protein cellular localization, and the results showed that using abstraction of metric Markov models.",
            "Outperform we can outperform the standard Markov models in terms of model size and in some cases also in terms of.",
            "Accuracy and organizing the.",
            "Kilograms in an abstraction hierarchy based on the conditional distribution of the next letter produced more suitable results than using conditionals on class classes.",
            "And.",
            "Um?",
            "Obstruction, hot air and obstruction augmenting Markov model is trained using class specific abstraction.",
            "Hierarchy provide better performing Mount models than the ones where we only train one hierarchy from the whole data set.",
            "Yes."
        ],
        [
            "Broke although the lens market models to compare to these approaches, that would be a.",
            "Do we?",
            "We haven't, but that would be an interesting thing for the future.",
            "Work for us to even also try to do abstractions for that case.",
            "For this."
        ],
        [
            "Text last for conditional distributions.",
            "In this case you you estimate the parameters of the model still in a generic way or.",
            "To use then also estimate the parameters of your model in the disconnect in all the cases the the models are estimated generatively.",
            "The only difference is.",
            "What are we gonna?",
            "What are the conditional distributions that we are going to try to compute?",
            "Jensen, Shannon Divergent when we try to produce the tree?",
            "But in principle you could.",
            "Recast this whole thing in, you know discriminative setup, but it's going to be.",
            "Is not going to run as fast as this is generating.",
            "This mode compared to complete my love in terms of time.",
            "So the main domain part.",
            "Is generating the hierarchy rather than training the model.",
            "So once you have the hierarchy.",
            "Once you have the hierarchy.",
            "Training the model.",
            "The older abstract or abstraction augmented models can be done in one pass through the data, but.",
            "Coming up with the abstractions is as hard as the complexity of hierarchical agglomerative clustering, which is order N squared in terms of the number of kilograms.",
            "But in principle, if you have a better clustering algorithm, you can substitute it there and generate your hierarchy differently."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in bioinformatics, sometimes we can set up our problem as sequence classification problem and this is so for example a protein localization or protein function prediction can be set up like this where the sequence is the sequence of amino acids and my my talk is going to be basically about.",
                    "label": 0
                },
                {
                    "sent": "Sequence.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classification.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'll start with the standard way of addressing this problem, which is to use Markov models to model the sequences.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I'll present.",
                    "label": 0
                },
                {
                    "sent": "Our improved version of Markov models, called abstraction augmented Markov models, and then I'll move to X.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elements and results.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The standard way too, so the standard way to set up sequence classification problem is we have data set where we have a bunch of data points who are sequenced sequences and their labels.",
                    "label": 0
                },
                {
                    "sent": "And we would like to learn a classifier that correctly predicts the label and.",
                    "label": 0
                },
                {
                    "sent": "One way to do this is to have a way to come up with the probability of the class given the example and the standard.",
                    "label": 0
                },
                {
                    "sent": "Generative way of addressing this problem is to use the Bayes rule and to express the probability of Y given XX.",
                    "label": 0
                },
                {
                    "sent": "To use the fact that the probability of Y given X is proportional to the probability of X given Y times the class probability and now all we have to do is say how we're going to model the probability of X.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the standard way.",
                    "label": 0
                },
                {
                    "sent": "To model the probability of X where X is a sequence is to use the Markov model.",
                    "label": 0
                },
                {
                    "sent": "So in a Markov model, we assume that each element in the sequence is dependent on the previous K elements.",
                    "label": 0
                },
                {
                    "sent": "And it satisfies this.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian net that describe this thing satisfied this Markov property, and as a consequence the probability overall probability distribution over the sequence decomposes.",
                    "label": 0
                },
                {
                    "sent": "I like this one of the problems with Markov models is that the complexity of KE order Markov model is exponential in K an hopefully.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our.",
                    "label": 0
                },
                {
                    "sent": "I abstraction augmented Markov models will address this problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is what I'm going to describe next.",
                    "label": 0
                },
                {
                    "sent": "So again, in a Markov model.",
                    "label": 0
                },
                {
                    "sent": "Each element is dependent on the previous K elements and what we want to do is to take this previous K elements somehow produce an abstraction hierarchy over this over this elements and then pick higher point in this abstraction hierarchy, which is going to be smaller than.",
                    "label": 0
                },
                {
                    "sent": "The size of the vocabulary to the K and we use this thing to model the.",
                    "label": 0
                },
                {
                    "sent": "Future.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an abstraction hierarchy in our is nothing but a tree where at the bottom will have in our case the all the K grams.",
                    "label": 0
                },
                {
                    "sent": "For based on the sequence and.",
                    "label": 0
                },
                {
                    "sent": "The tree is such that in each element in each year, not in the tree, each node in the tree represents the union of the leaf nodes in that subtree.",
                    "label": 0
                },
                {
                    "sent": "And the cutting.",
                    "label": 0
                },
                {
                    "sent": "This tree is basically a partition of the of the set of elements here.",
                    "label": 0
                },
                {
                    "sent": "So basically a cut is a set of nodes that has the property that none of the node is.",
                    "label": 0
                },
                {
                    "sent": "Descendant over in our node.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now given such a.",
                    "label": 0
                },
                {
                    "sent": "Given such such a partition, we can.",
                    "label": 0
                },
                {
                    "sent": "Assuming that we already have constructed abstraction hierarchy and we already have chosen a partition, what we can do is we can.",
                    "label": 1
                },
                {
                    "sent": "Model the probability of XID Cup basically decouples the probability of the XI from the past using only the abstraction.",
                    "label": 1
                },
                {
                    "sent": "Multiplied with the probability of the abstraction.",
                    "label": 0
                },
                {
                    "sent": "Given the past kilogram, but this thing is going to be a functional dependency is not really probabilistic dependencies, so it's going to be either zero or one depending on whether the abstraction belongs to whether the kilogram belongs to the abstraction cluster or not.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to learn.",
                    "label": 0
                },
                {
                    "sent": "The abstraction hierarchy.",
                    "label": 0
                },
                {
                    "sent": "We use a hierarchical agglomerative clustering, so.",
                    "label": 0
                },
                {
                    "sent": "We start with the bottom with all the all the K grams and then we recursively merge 22 abstraction.",
                    "label": 0
                },
                {
                    "sent": "Note based on their similarity until we get a tree.",
                    "label": 0
                },
                {
                    "sent": "So this is standard hierarchical.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clustering and the formula for merging is.",
                    "label": 0
                },
                {
                    "sent": "Based on the Jensen Shannon, divergent, Jensen, Shannon distance, and.",
                    "label": 0
                },
                {
                    "sent": "It can be shown that if you are to take two abstractions which are sets of elements and.",
                    "label": 0
                },
                {
                    "sent": "Put them together into a bigger set, denoted by by this operation.",
                    "label": 0
                },
                {
                    "sent": "Here the information loss do that is incurred due to.",
                    "label": 0
                },
                {
                    "sent": "Doing such a merging is given by this formula and this is what we are used to pick.",
                    "label": 0
                },
                {
                    "sent": "The two things to merge together based on the lowest information loss.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now back to the model for for 2 grams, the model looks like this, But this part is a functional dependency in the Bayesian network, so it is basically only.",
                    "label": 0
                },
                {
                    "sent": "Saying whether the previous kilogram belongs to the cluster or not, and this can be.",
                    "label": 0
                },
                {
                    "sent": "We don't have to store the whole matrix.",
                    "label": 0
                },
                {
                    "sent": "All we need to store is for each kilogram to which clustering belongs.",
                    "label": 0
                },
                {
                    "sent": "So in order to so, there is no estimation of this parameter, and in order to estimate this parameter we just count the number of times the.",
                    "label": 0
                },
                {
                    "sent": "Each element in the in the future occurs together with the abstraction and divide by the by the number of times that abstraction occurs.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now do you to put all of these things together?",
                    "label": 0
                },
                {
                    "sent": "First we learn an abstraction hierarchy using hierarchical agglomerative clustering.",
                    "label": 0
                },
                {
                    "sent": "Then we estimate the for each class, wasting made the parameters for probability of X given the class for each class, and then in the end we used the Bayes rule to get.",
                    "label": 0
                },
                {
                    "sent": "Posterior and we take the argmax.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In terms of experiments, we experimented with three protein cell subcellular localization datasets.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the first, the first type of experiment is where we learn one abstraction hierarchy for each.",
                    "label": 0
                },
                {
                    "sent": "Separately for each class, so we take.",
                    "label": 0
                },
                {
                    "sent": "All the all the K grams in each class and learn 11 abstraction hierarchy for based on those datasets and.",
                    "label": 0
                },
                {
                    "sent": "Then for each possible cut in the tree.",
                    "label": 0
                },
                {
                    "sent": "We train on.",
                    "label": 0
                },
                {
                    "sent": "We trained for that M cut an abstraction augmented Markov model, and because we learn different abstraction hierarchy for each class, we can you know for Class 1 have an abstraction hierarchy tree that looks like this, and for the class to something like that looks like that, but picking up tree cut would involve speaking.",
                    "label": 0
                },
                {
                    "sent": "These values.",
                    "label": 0
                },
                {
                    "sent": "As the values for the abstraction.",
                    "label": 0
                },
                {
                    "sent": "4 + 1 and involves picking these other tree values here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we compare.",
                    "label": 0
                },
                {
                    "sent": "We have two baselines.",
                    "label": 0
                },
                {
                    "sent": "One of them is the performance of the Markov models using all the kilograms.",
                    "label": 0
                },
                {
                    "sent": "These other agree that the grid line the green line is the naive base, which is equivalent with Markov models of zero and.",
                    "label": 0
                },
                {
                    "sent": "The blue line is the one where we look at the performance of the.",
                    "label": 0
                },
                {
                    "sent": "Obstruction augmented Markov models, so we plot classification accuracy versus how many abstractions we have used, which is in a log scale.",
                    "label": 0
                },
                {
                    "sent": "So one would mean 10 abstractions, so we can see that we.",
                    "label": 0
                },
                {
                    "sent": "Much fewer.",
                    "label": 0
                },
                {
                    "sent": "Abstractions we can get the same.",
                    "label": 0
                },
                {
                    "sent": "Performance in terms of accuracy as using the total number of abstractions.",
                    "label": 0
                },
                {
                    "sent": "So in this case we use 3 grams and the total number and number of three grams for protein sequences is about 8000 and.",
                    "label": 0
                },
                {
                    "sent": "This is about 100 abstractions.",
                    "label": 0
                },
                {
                    "sent": "So in some cases we we managed to to get the same performance or even exceed the performance of the full model with much fewer.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Winters.",
                    "label": 0
                },
                {
                    "sent": "We've also tried this thing in a semi supervised setting, so the way we set up our semi supervised setting is we assume that we don't have all the data are labeled.",
                    "label": 0
                },
                {
                    "sent": "We assume, for example that we have only 1% of the data points labeled.",
                    "label": 0
                },
                {
                    "sent": "So for example for.",
                    "label": 0
                },
                {
                    "sent": "For one of the datasets, which is 1000 examples, we assume that only 10 of them we are only given for 10 of them the classes.",
                    "label": 0
                },
                {
                    "sent": "And then we train one abstraction hierarchy for both from both the labeled and unlabeled examples, without looking at the class.",
                    "label": 0
                },
                {
                    "sent": "And in this case, we train only one one abstraction hierarchy from the whole data set, because it's not.",
                    "label": 0
                },
                {
                    "sent": "Very feasible to train it from only the labeled, especially in the 1% case.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we compare what the performance is.",
                    "label": 0
                },
                {
                    "sent": "If we so the first row of graphs over here is are the graphs where we have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, only 1% of the samples labeled and this one is the.",
                    "label": 0
                },
                {
                    "sent": "This one are the graphs where we have 10 percent 25% of the graph, so of course the performance of the Markov model is much much lower.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that we've used the abstractions and if we have much fewer examples labeled we are able to generalize better due to the fact that we have fewer parameters to fit.",
                    "label": 0
                },
                {
                    "sent": "And the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's also present in the 25% case, but here the differences are much are somewhat smaller.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there are.",
                    "label": 0
                },
                {
                    "sent": "Three types of there are two so far we looked at.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Inferring the abstractions from the whole data set and the other case which was inferring the abstraction from.",
                    "label": 0
                },
                {
                    "sent": "From inferring one abstraction hierarchy per class, so these are.",
                    "label": 0
                },
                {
                    "sent": "Represented over here.",
                    "label": 0
                },
                {
                    "sent": "The other possibilities too.",
                    "label": 0
                },
                {
                    "sent": "Um, infer the abstraction hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Instead of.",
                    "label": 0
                },
                {
                    "sent": "Do instead of trying to predict the next element and try to find the best best.",
                    "label": 0
                },
                {
                    "sent": "Hierarchy that predicts the next element is to try to best predict the class, and there is some prior work.",
                    "label": 0
                },
                {
                    "sent": "On on this thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In this case, for the abstraction hierarchy, the target is Y and the generation on divergences computed between the conditional distributions of Y.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In that case so.",
                    "label": 0
                },
                {
                    "sent": "For this type of results we have, we have shown that so we would read we have the class conditional abstractions.",
                    "label": 0
                },
                {
                    "sent": "And with the green we have the.",
                    "label": 0
                },
                {
                    "sent": "Abstraction augmented model Markov models where we use only one.",
                    "label": 0
                },
                {
                    "sent": "Abstraction hierarchy, which is derived from the whole data and the blue is the one with.",
                    "label": 0
                },
                {
                    "sent": "As many abstraction hierarchies as there are classes, so it turns out that using.",
                    "label": 0
                },
                {
                    "sent": "Using one abstraction hierarchy per class improves.",
                    "label": 0
                },
                {
                    "sent": "Improve the performance but is not is not so necessarily so easy to use incentives to provide.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The setups.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, we have described an algorithm for learning abstraction based Markov models on sequence classification task, namely protein cellular localization, and the results showed that using abstraction of metric Markov models.",
                    "label": 0
                },
                {
                    "sent": "Outperform we can outperform the standard Markov models in terms of model size and in some cases also in terms of.",
                    "label": 0
                },
                {
                    "sent": "Accuracy and organizing the.",
                    "label": 0
                },
                {
                    "sent": "Kilograms in an abstraction hierarchy based on the conditional distribution of the next letter produced more suitable results than using conditionals on class classes.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Obstruction, hot air and obstruction augmenting Markov model is trained using class specific abstraction.",
                    "label": 0
                },
                {
                    "sent": "Hierarchy provide better performing Mount models than the ones where we only train one hierarchy from the whole data set.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Broke although the lens market models to compare to these approaches, that would be a.",
                    "label": 0
                },
                {
                    "sent": "Do we?",
                    "label": 0
                },
                {
                    "sent": "We haven't, but that would be an interesting thing for the future.",
                    "label": 0
                },
                {
                    "sent": "Work for us to even also try to do abstractions for that case.",
                    "label": 0
                },
                {
                    "sent": "For this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text last for conditional distributions.",
                    "label": 0
                },
                {
                    "sent": "In this case you you estimate the parameters of the model still in a generic way or.",
                    "label": 0
                },
                {
                    "sent": "To use then also estimate the parameters of your model in the disconnect in all the cases the the models are estimated generatively.",
                    "label": 0
                },
                {
                    "sent": "The only difference is.",
                    "label": 0
                },
                {
                    "sent": "What are we gonna?",
                    "label": 0
                },
                {
                    "sent": "What are the conditional distributions that we are going to try to compute?",
                    "label": 0
                },
                {
                    "sent": "Jensen, Shannon Divergent when we try to produce the tree?",
                    "label": 0
                },
                {
                    "sent": "But in principle you could.",
                    "label": 0
                },
                {
                    "sent": "Recast this whole thing in, you know discriminative setup, but it's going to be.",
                    "label": 0
                },
                {
                    "sent": "Is not going to run as fast as this is generating.",
                    "label": 0
                },
                {
                    "sent": "This mode compared to complete my love in terms of time.",
                    "label": 0
                },
                {
                    "sent": "So the main domain part.",
                    "label": 0
                },
                {
                    "sent": "Is generating the hierarchy rather than training the model.",
                    "label": 0
                },
                {
                    "sent": "So once you have the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Once you have the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "Training the model.",
                    "label": 0
                },
                {
                    "sent": "The older abstract or abstraction augmented models can be done in one pass through the data, but.",
                    "label": 0
                },
                {
                    "sent": "Coming up with the abstractions is as hard as the complexity of hierarchical agglomerative clustering, which is order N squared in terms of the number of kilograms.",
                    "label": 0
                },
                {
                    "sent": "But in principle, if you have a better clustering algorithm, you can substitute it there and generate your hierarchy differently.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}