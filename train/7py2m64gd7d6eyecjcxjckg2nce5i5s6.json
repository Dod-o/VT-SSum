{
    "id": "7py2m64gd7d6eyecjcxjckg2nce5i5s6",
    "title": "Incorporating Literals into Knowledge Graph Embeddings",
    "info": {
        "author": [
            "Denis Lukovnikov, University of Bonn"
        ],
        "published": "Nov. 27, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_lukovnikov_knowledge_graph_embeddings/",
    "segmentation": [
        [
            "My name is Dennis.",
            "The first soldiers couldn't be here, so I'll be presenting our work on incorporating neutrals into knowledge graphs.",
            "First quickly illustrate how."
        ],
        [
            "Materials could be useful for link prediction with knowledge graphs and.",
            "Then introduce latent feature models and discuss our approach and present some experimental results."
        ],
        [
            "So in in production we basically want to predict whether it's in fact is true based given all the other effects in the Knowledge Graph.",
            "So here for example, we have three entities.",
            "John Jane and the high school, and we have two facts that John and Jane study the same high school.",
            "And now we want to predict how likely it is that John and Jane know each other, so only using the facts between entities, we can assign a higher likelihood.",
            "But to get a more accurate prediction, it will be nice to also take into account the years that they were born.",
            "So it says John was born in 2001 and 2000, so."
        ],
        [
            "If two people are born.",
            "Around the same time and attend the same high school, they are much more likely to know each other through high school compared to people who are born far apart.",
            "So.",
            "This is an example.",
            "WHI literals could be useful for the prediction between two entities.",
            "So."
        ],
        [
            "In the Knowledge Graph we can consider 2 facts of two sets of facts.",
            "So the first set of facts describes links between entities, so it connects entities with another entity with a given relation, and the second set of facts describe some attributes about entities.",
            "So like a birth, birth year or height of a person or the number of floors of a building.",
            "And then in prediction we're interested in predicting the truth value of the first kind of fact so."
        ],
        [
            "We want to predict whether a certain entity should be connected to another entity given with a certain relation.",
            "So and we can solve this task if we can define the function.",
            "That takes the entities and relationship and produces a scalar score that is higher if the triple is true.",
            "So we can implement this function.",
            "The scoring function by."
        ],
        [
            "They are using a function that takes like feature vectors or features for the entities and relationships and this function.",
            "And for example, take three vectors.",
            "So features for the first entity features or tail entity and the features for the relation and produces a single scalar score that should be higher if the facts are more likely to be true if the trip is more likely to be true."
        ],
        [
            "And in recent years, many methods have been proposed to automatically learn these features.",
            "So the feature is not observed anywhere, so they're kind of invented by algorithms and other Golden features, and they're also called usually called embeddings.",
            "So every entity gets.",
            "Vector of our meters which describe its features or logarithm."
        ],
        [
            "So there are many such methods and in this work we experiment with three of them.",
            "So the first one is this mold, which is just a very simple function.",
            "It just takes the dot product with three vectors to compute the score for the triple.",
            "But the problem with this notice that this malt cannot handle a symmetrical relationships.",
            "So if you exchange the hat and detail entity in the function, you will get the same score."
        ],
        [
            "The second thing we experiment with this complex which is.",
            "Very similar to this malt except that it uses it learns complex values.",
            "Embeddings for entities.",
            "And.",
            "But the scoring function that they use is very similar to this mode.",
            "But it allows to handle the symmetric relation."
        ],
        [
            "And then this hurts."
        ],
        [
            "Better to experiment with his coffee.",
            "Which is conditional craters."
        ],
        [
            "Cites the scoring function.",
            "So our actual contribution is about our contribution is Universal approach.",
            "To incorporate that little information into the knowledge into the embeddings of entities.",
            "And we evaluate our approach."
        ],
        [
            "And three standard data sets.",
            "Maybe 15 K would be reduced to 15 K and the other data set.",
            "Let me provide the extended versions of the AP15K data sets.",
            "And in our experiments we can show that using literally extension improves the prediction performance and also might lead to better higher quality embeddings.",
            "So in literally we have two inputs.",
            "The first input is the vector E which is the venue embedding for for some entity."
        ],
        [
            "And this vector is this is learned by the algorithm, so it's their parameters of the model.",
            "And the second input, this vector L the vector features for some entity.",
            "And this vector describes all the attributes of some entity.",
            "The number of entries in this vector is equal to the number of data relations in the Knowledge graph, so.",
            "2 equal to the total number of all the relations in the Knowledge Graph.",
            "So it will have a lot of zeros because many of the relations are not specified folder entities.",
            "So for example, if we in our knowledge graph we have in total for.",
            "Dot relationships Heights.",
            "Birth year surface area and number of floors.",
            "Four person we will have only entries for the first 2 for the height and the birth year.",
            "And we set other entries to 0.",
            "And for a country you will have a surface area but not very severe.",
            "Yeah, so given this so."
        ],
        [
            "Basically, literally the central idea is to update the vanilla and determining factor using the literal vector, and this is done using a function G. Which takes the vanilla and embedding.",
            "And digital embedding and produces an updated and embedding vector of the same size as the original entity embedding vector.",
            "And this other detector can be used in this original scoring function instead of the vanilla entity embedding vector.",
            "So it's compatible with many different scoring functions from the literature."
        ],
        [
            "So.",
            "One simple implementation for tablet function would be to just do a linear projection."
        ],
        [
            "The two vectors.",
            "But in our experiments we use gate implementation.",
            "Which consists of two parts.",
            "The first part computes.",
            "Proposed updates to the manual and embedding vector and the second part.",
            "Computes an interpolation vector which will mix together the.",
            "Proposed update and when you land in vector so in this way it can decide to completely ignore literal information or completely ignored original original embedding."
        ],
        [
            "Yeah, we sure this time."
        ],
        [
            "So in our experiments.",
            "Fuck."
        ],
        [
            "In those experiments, we experimented with the three standard datasets, some statistics.",
            "We used this note complex and comfy as the base methods for literally so we can extend most methods with literature literally and use the literals from the knowledge graphs.",
            "We use it."
        ],
        [
            "Loss one of the centerpieces for link prediction that."
        ],
        [
            "Using literature, so here we have a new cross entropy loss in the 1:10 scoring scheme.",
            "But basically it's just trying to maximize the probability of the model for facts that exist in the Knowledge graph already and minimize the probability of the model for effects that do not exist in knowledge graph already.",
            "And the probability of the model is obtained just by applying a sigmoid function on the score of a triple.",
            "To get the value between zero and one so you can interpret this probabilities.",
            "And you say?"
        ],
        [
            "Time to optimize this."
        ],
        [
            "For evaluation, we use the standard evaluation metrics, meaning Community, reciprocal rank and hits.",
            "And."
        ],
        [
            "In our experiments, we first consider the base methods without any literals in there.",
            "We also reimplement.",
            "A couple 2."
        ],
        [
            "That's that also incorporated literals for introduction.",
            "And we compare them with the base methods.",
            "The same based methods but extended with her literally extension.",
            "Sorry.",
            "So for every 15 K we can see that.",
            "Extending the base methods with literally."
        ],
        [
            "Increase performance on prediction.",
            "And we also outperform and perform better than the.",
            "There are literal baselines here.",
            "For free base for the reduced previously seen K, we also observe improvements.",
            "From the base methods and also performed baseline.",
            "And in yogurts, less conclusive, but we can still see that or one of our result is better.",
            "The usual baselines.",
            "So we also."
        ],
        [
            "So try to analyze the quality of embeddings by."
        ],
        [
            "Looking up.",
            "The closest entities in the embedding space for some entity.",
            "So for example.",
            "If you take the Entity Roman Republic and get its vector and don't get the closest vectors to this in the different spaces, so."
        ],
        [
            "In the space of this mode.",
            "One of the closest entities, Israel Defense Force, which maybe it doesn't make much sense.",
            "Maybe some noise for one of the baselines that incorporate literals.",
            "One of the entities is retinal, which is vitamin A1 I think.",
            "Traditionally we get some.",
            "More similar entities.",
            "In the embedding space eventually.",
            "So this illustrates it's open.",
            "Literally could be more useful for other problems than just in prediction.",
            "So."
        ],
        [
            "That's our work, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Dennis.",
                    "label": 0
                },
                {
                    "sent": "The first soldiers couldn't be here, so I'll be presenting our work on incorporating neutrals into knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "First quickly illustrate how.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Materials could be useful for link prediction with knowledge graphs and.",
                    "label": 0
                },
                {
                    "sent": "Then introduce latent feature models and discuss our approach and present some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in in production we basically want to predict whether it's in fact is true based given all the other effects in the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "So here for example, we have three entities.",
                    "label": 0
                },
                {
                    "sent": "John Jane and the high school, and we have two facts that John and Jane study the same high school.",
                    "label": 1
                },
                {
                    "sent": "And now we want to predict how likely it is that John and Jane know each other, so only using the facts between entities, we can assign a higher likelihood.",
                    "label": 0
                },
                {
                    "sent": "But to get a more accurate prediction, it will be nice to also take into account the years that they were born.",
                    "label": 0
                },
                {
                    "sent": "So it says John was born in 2001 and 2000, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If two people are born.",
                    "label": 0
                },
                {
                    "sent": "Around the same time and attend the same high school, they are much more likely to know each other through high school compared to people who are born far apart.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is an example.",
                    "label": 0
                },
                {
                    "sent": "WHI literals could be useful for the prediction between two entities.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the Knowledge Graph we can consider 2 facts of two sets of facts.",
                    "label": 0
                },
                {
                    "sent": "So the first set of facts describes links between entities, so it connects entities with another entity with a given relation, and the second set of facts describe some attributes about entities.",
                    "label": 0
                },
                {
                    "sent": "So like a birth, birth year or height of a person or the number of floors of a building.",
                    "label": 0
                },
                {
                    "sent": "And then in prediction we're interested in predicting the truth value of the first kind of fact so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to predict whether a certain entity should be connected to another entity given with a certain relation.",
                    "label": 0
                },
                {
                    "sent": "So and we can solve this task if we can define the function.",
                    "label": 0
                },
                {
                    "sent": "That takes the entities and relationship and produces a scalar score that is higher if the triple is true.",
                    "label": 0
                },
                {
                    "sent": "So we can implement this function.",
                    "label": 0
                },
                {
                    "sent": "The scoring function by.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are using a function that takes like feature vectors or features for the entities and relationships and this function.",
                    "label": 0
                },
                {
                    "sent": "And for example, take three vectors.",
                    "label": 0
                },
                {
                    "sent": "So features for the first entity features or tail entity and the features for the relation and produces a single scalar score that should be higher if the facts are more likely to be true if the trip is more likely to be true.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in recent years, many methods have been proposed to automatically learn these features.",
                    "label": 0
                },
                {
                    "sent": "So the feature is not observed anywhere, so they're kind of invented by algorithms and other Golden features, and they're also called usually called embeddings.",
                    "label": 0
                },
                {
                    "sent": "So every entity gets.",
                    "label": 0
                },
                {
                    "sent": "Vector of our meters which describe its features or logarithm.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are many such methods and in this work we experiment with three of them.",
                    "label": 0
                },
                {
                    "sent": "So the first one is this mold, which is just a very simple function.",
                    "label": 0
                },
                {
                    "sent": "It just takes the dot product with three vectors to compute the score for the triple.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this notice that this malt cannot handle a symmetrical relationships.",
                    "label": 0
                },
                {
                    "sent": "So if you exchange the hat and detail entity in the function, you will get the same score.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second thing we experiment with this complex which is.",
                    "label": 0
                },
                {
                    "sent": "Very similar to this malt except that it uses it learns complex values.",
                    "label": 0
                },
                {
                    "sent": "Embeddings for entities.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But the scoring function that they use is very similar to this mode.",
                    "label": 0
                },
                {
                    "sent": "But it allows to handle the symmetric relation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this hurts.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better to experiment with his coffee.",
                    "label": 0
                },
                {
                    "sent": "Which is conditional craters.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cites the scoring function.",
                    "label": 0
                },
                {
                    "sent": "So our actual contribution is about our contribution is Universal approach.",
                    "label": 1
                },
                {
                    "sent": "To incorporate that little information into the knowledge into the embeddings of entities.",
                    "label": 0
                },
                {
                    "sent": "And we evaluate our approach.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And three standard data sets.",
                    "label": 0
                },
                {
                    "sent": "Maybe 15 K would be reduced to 15 K and the other data set.",
                    "label": 0
                },
                {
                    "sent": "Let me provide the extended versions of the AP15K data sets.",
                    "label": 0
                },
                {
                    "sent": "And in our experiments we can show that using literally extension improves the prediction performance and also might lead to better higher quality embeddings.",
                    "label": 0
                },
                {
                    "sent": "So in literally we have two inputs.",
                    "label": 0
                },
                {
                    "sent": "The first input is the vector E which is the venue embedding for for some entity.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this vector is this is learned by the algorithm, so it's their parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "And the second input, this vector L the vector features for some entity.",
                    "label": 0
                },
                {
                    "sent": "And this vector describes all the attributes of some entity.",
                    "label": 0
                },
                {
                    "sent": "The number of entries in this vector is equal to the number of data relations in the Knowledge graph, so.",
                    "label": 1
                },
                {
                    "sent": "2 equal to the total number of all the relations in the Knowledge Graph.",
                    "label": 1
                },
                {
                    "sent": "So it will have a lot of zeros because many of the relations are not specified folder entities.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we in our knowledge graph we have in total for.",
                    "label": 1
                },
                {
                    "sent": "Dot relationships Heights.",
                    "label": 1
                },
                {
                    "sent": "Birth year surface area and number of floors.",
                    "label": 1
                },
                {
                    "sent": "Four person we will have only entries for the first 2 for the height and the birth year.",
                    "label": 0
                },
                {
                    "sent": "And we set other entries to 0.",
                    "label": 0
                },
                {
                    "sent": "And for a country you will have a surface area but not very severe.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so given this so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, literally the central idea is to update the vanilla and determining factor using the literal vector, and this is done using a function G. Which takes the vanilla and embedding.",
                    "label": 0
                },
                {
                    "sent": "And digital embedding and produces an updated and embedding vector of the same size as the original entity embedding vector.",
                    "label": 0
                },
                {
                    "sent": "And this other detector can be used in this original scoring function instead of the vanilla entity embedding vector.",
                    "label": 0
                },
                {
                    "sent": "So it's compatible with many different scoring functions from the literature.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One simple implementation for tablet function would be to just do a linear projection.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The two vectors.",
                    "label": 0
                },
                {
                    "sent": "But in our experiments we use gate implementation.",
                    "label": 0
                },
                {
                    "sent": "Which consists of two parts.",
                    "label": 0
                },
                {
                    "sent": "The first part computes.",
                    "label": 0
                },
                {
                    "sent": "Proposed updates to the manual and embedding vector and the second part.",
                    "label": 0
                },
                {
                    "sent": "Computes an interpolation vector which will mix together the.",
                    "label": 0
                },
                {
                    "sent": "Proposed update and when you land in vector so in this way it can decide to completely ignore literal information or completely ignored original original embedding.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, we sure this time.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our experiments.",
                    "label": 0
                },
                {
                    "sent": "Fuck.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In those experiments, we experimented with the three standard datasets, some statistics.",
                    "label": 0
                },
                {
                    "sent": "We used this note complex and comfy as the base methods for literally so we can extend most methods with literature literally and use the literals from the knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "We use it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loss one of the centerpieces for link prediction that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using literature, so here we have a new cross entropy loss in the 1:10 scoring scheme.",
                    "label": 0
                },
                {
                    "sent": "But basically it's just trying to maximize the probability of the model for facts that exist in the Knowledge graph already and minimize the probability of the model for effects that do not exist in knowledge graph already.",
                    "label": 0
                },
                {
                    "sent": "And the probability of the model is obtained just by applying a sigmoid function on the score of a triple.",
                    "label": 0
                },
                {
                    "sent": "To get the value between zero and one so you can interpret this probabilities.",
                    "label": 0
                },
                {
                    "sent": "And you say?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time to optimize this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For evaluation, we use the standard evaluation metrics, meaning Community, reciprocal rank and hits.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our experiments, we first consider the base methods without any literals in there.",
                    "label": 0
                },
                {
                    "sent": "We also reimplement.",
                    "label": 0
                },
                {
                    "sent": "A couple 2.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's that also incorporated literals for introduction.",
                    "label": 0
                },
                {
                    "sent": "And we compare them with the base methods.",
                    "label": 0
                },
                {
                    "sent": "The same based methods but extended with her literally extension.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So for every 15 K we can see that.",
                    "label": 0
                },
                {
                    "sent": "Extending the base methods with literally.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Increase performance on prediction.",
                    "label": 0
                },
                {
                    "sent": "And we also outperform and perform better than the.",
                    "label": 0
                },
                {
                    "sent": "There are literal baselines here.",
                    "label": 0
                },
                {
                    "sent": "For free base for the reduced previously seen K, we also observe improvements.",
                    "label": 0
                },
                {
                    "sent": "From the base methods and also performed baseline.",
                    "label": 0
                },
                {
                    "sent": "And in yogurts, less conclusive, but we can still see that or one of our result is better.",
                    "label": 0
                },
                {
                    "sent": "The usual baselines.",
                    "label": 0
                },
                {
                    "sent": "So we also.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So try to analyze the quality of embeddings by.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking up.",
                    "label": 0
                },
                {
                    "sent": "The closest entities in the embedding space for some entity.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "If you take the Entity Roman Republic and get its vector and don't get the closest vectors to this in the different spaces, so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the space of this mode.",
                    "label": 0
                },
                {
                    "sent": "One of the closest entities, Israel Defense Force, which maybe it doesn't make much sense.",
                    "label": 0
                },
                {
                    "sent": "Maybe some noise for one of the baselines that incorporate literals.",
                    "label": 0
                },
                {
                    "sent": "One of the entities is retinal, which is vitamin A1 I think.",
                    "label": 0
                },
                {
                    "sent": "Traditionally we get some.",
                    "label": 0
                },
                {
                    "sent": "More similar entities.",
                    "label": 0
                },
                {
                    "sent": "In the embedding space eventually.",
                    "label": 0
                },
                {
                    "sent": "So this illustrates it's open.",
                    "label": 0
                },
                {
                    "sent": "Literally could be more useful for other problems than just in prediction.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's our work, thank you.",
                    "label": 0
                }
            ]
        }
    }
}