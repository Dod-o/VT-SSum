{
    "id": "nqdhfi7r7stctsv6wl6sgiopzngyj2as",
    "title": "Feature Selection by Transfer Learning with Linear Regularized Models",
    "info": {
        "author": [
            "Thibault Helleputte, Computing Science Engineering Department, Universit\u00e9 catholique de Louvain"
        ],
        "published": "Oct. 20, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd09_helleputte_fstllrm/",
    "segmentation": [
        [
            "OK, so hello, I'm glad because many things in my talk will be related to other things we heard already today, and in fact this Turkey is a kind of sequel of another talk we made that I CML where we introduced the feature selection technique, able to incorporate prior knowledge which we get from field experts.",
            "And here we will apply this technique in another setting and namely the transfer.",
            "Learning setting.",
            "And the the technique will be based on linear regularised models and we are particularly interested in future in datasets where we have a huge number of dimensions and really few really low number of samples."
        ],
        [
            "And one example of such data is a micro radiator, and if you don't know what are micro array you can think of it as a way to measure the genes expression, which is a kind of gene activity, and so the data looks like that where you have one column by gene and one row by sample.",
            "A simple Mary correspond to a patient and generally we add external rotation, which are the class label.",
            "For example, is the patient status."
        ],
        [
            "And with recent technologies it is frequent that we have several 10s of thousands of dimension, but since it is expensive, we only have a few samples and if you get something like 2 or 300 samples.",
            "Your."
        ],
        [
            "Ricky.",
            "Those data are particularly useful to perform automated diagnosis or prognosis, which can be used in a clinical or pharmaceutical setting.",
            "And."
        ],
        [
            "If we for them or perform feature selection, that is, select a small subset of genes able to perform a good classification.",
            "This is called signature discovery, and it is convenient for explanatory concern to give an indication to biologists which dimensions with jeans are related to the output, we try to predict.",
            "I need to also useful to make cheaper diagnosis or prognosis.",
            "Kids and in some task feature selection has been shown to improve the classification."
        ],
        [
            "Conferences.",
            "And then one frequently used algorithm to perform classification is the Super vector machine, which should good performances in many applications.",
            "And and I think is that many extensions for feature selections have been proposed which are based on."
        ],
        [
            "Support vector machine and one example is the recursive feature elimination, which is an iterative algorithm based on linear support vector machine and here we can see an example in two dimensions.",
            "Suppose we want to reduce the space.",
            "He is a toy example because we only have two features there.",
            "But you see that the feature two corresponds to a small component of the W vector which defines the separating hyperplane, and so you can drop that feature and projects on the subspace.",
            "The composer here just feature one without decreasing much the margin of your classifier.",
            "This is an embedded technique because it relies on the very structure of the classifier, which is an elegant way of performing feature selection.",
            "In a context."
        ],
        [
            "Classification and other approaches have been proposed to test the minimisation of the zero norm of linear model.",
            "The zero normal as another speaker reminded, this is just the number of manual element of a vector and this is a direct way to express feature selection in the context of classification.",
            "But that problem is not tractable."
        ],
        [
            "As such so relaxations have been proposed.",
            "For example, the error methods in which we replace the zero norm by the sum of the logarithm of the absolute value of each component.",
            "And you have this little epsilon over there with just to prevent problems with when one dimension becomes exactly 0 and you want to minimize that objective function.",
            "And you can see here that it is a good approximation to the zero norm objective function because it is nearly flat everywhere except.",
            "On the exist when.",
            "One dimension becomes exactly 0, or tends toward zero.",
            "There is a huge gain in the value of the objective, and we add a margin constraint here to guarantee good classification.",
            "And the authors of the."
        ],
        [
            "This message so that they could derive an algorithm also based on linear SVM, just as the one we showed before where you in the first step of the algorithm you train a linear SVM but with rescaled inputs of the margin constraints.",
            "And here W case just initialize with ones and the second step consists of updating that vector by multiplying component wise with the results of the SVM estimation.",
            "And you do that iteratively and progressively.",
            "Less important features will tend to decrease and then fall below the machine precision.",
            "An are just then eliminated.",
            "So it is a kind of smoother way to perform feature selection than the recursive feature elimination."
        ],
        [
            "The problem is that even with simple linear model, when you have so many dimensions and so few points, you have overfitting problem and lack of robustness.",
            "Even if you regularize with maximal margin or other devices like that."
        ],
        [
            "And so we want to.",
            "Add extra information into our learning system.",
            "The problem is that we do not have more symptoms."
        ],
        [
            "So we will consider here a setting of transfer learning in traditional learning, setting you each time you have a new data set with a classification task you want to solve, you will design a new learning system.",
            "In a transfer learning setting you will reuse similar data sets that you already get from elsewhere or for previous tasks.",
            "We will try to extract knowledge from them and transfer that knowledge into your new learning system.",
            "You have to design on your target task so we will have source datasets and target datasets."
        ],
        [
            "And as I said in my short introduction, we will use here are technically introduced some months ago, which allowed us to incorporate expert knowledge on the feature relevance.",
            "For example, biologists may know or guess that some of the dimensions which are genes in this case are related to the output we want to predict.",
            "And that knowledge may not be sufficient to build a model just on those dimension, but it is extra information we want to use.",
            "Unfortunately, this knowledge is not always available, and that's why here we will simulate that knowledge in the transfer learning setting where we will extract knowledge from.",
            "The source datasets and bias the feature selection on the target that I said accordingly to that knowledge and here you don't have to be confused.",
            "This technique is called partially supervised feature selection.",
            "But it is nothing.",
            "It has nothing to do with some supervised learning, because here we assume we have a full supervision on the class."
        ],
        [
            "Labels."
        ],
        [
            "OK, and then this is the technique.",
            "In fact, we just extended around methods to incorporate prior knowledge and we define a relevance vector beta of the same size as the number of dimensions of our problem.",
            "And each better Jay will include in fact, the prior relevance of feature J and the Morra.",
            "Feature is relevant, the higher its corresponding letter J and if you don't know anything about the prior relevance of a feature, you just put it to one.",
            "And so you recognize here the nearly the same problem as the PARAM method.",
            "Except that here you have a multiplicative factor which will penalize less the dimensions which are assumed to be more relevant.",
            "And the same the rest of the."
        ],
        [
            "The problem is the same with the margin constraints an.",
            "We also derive an iterative algorithm still based on a linear SVM with also this rescaling inputs of the inputs of the margin constraint.",
            "But instead of initializing WK212 vector of 1, so we initialize it to beat them.",
            "And in the second step we re multiply the value K by the solution at the first step.",
            "And also by or prior elements."
        ],
        [
            "And we really here use 3 micro rated.",
            "I said which are both related to prostate cancer.",
            "The task is to discriminate between normal tissue and tumor tissue.",
            "You see that there is a large number of feature.",
            "And those data have been produced with similar but different technologies that explains the small differences in the number of features, but also in that number.",
            "Some features are not the same, so we restrict those datasets to the common.",
            "Dimension the share, which is a 12,600 and you see also that the ratio between the two classes are not the same man.",
            "The way be logical material has been extracted is not the same, so although they are related, this is not.",
            "We cannot just concatenate."
        ],
        [
            "Those datasets.",
            "And we will be interested in two performances.",
            "Measures stability.",
            "We talked about it already this morning here.",
            "We're interested in stability with respect to the sampling variation.",
            "We assume that if we add two patient or remove two patients from the experiment experiments, the feature selection should lead to the same result because.",
            "The biological process behind the data is not expected to change much.",
            "And here we will use the Culture Index, which is, roughly speaking, the ratio between the size of the intersection of two signatures divided by the size of the signature, and there is a two extra terms which simply takes into account the fact that if you select really large signature, you will have you're more likely to have similar sing.",
            "It should just because there is no other choice.",
            "So This is why we choose that index, but you cannot look at stability alone, because if you do so, you could have one method that always return an arbitrary set of dimensions, and indeed it will have a maximal stability.",
            "But the predictive power of that signature is likely to be poor.",
            "So along with stability we will also monitor classification.",
            "Performance is an here.",
            "We don't use accuracy because you, so the datasets are unbalanced.",
            "And so, for example, a classifier simply classifying every samples in the majority class.",
            "It would look as if he it were really good, and in fact it's not really the case.",
            "So here we take the balanced classification rate, which is the average of the specificity and sensitivity."
        ],
        [
            "And this is our first experimental setting.",
            "Where we will have one of the three datasets I showed you a source domain and one other data set as target domain.",
            "And 1st we will extract 50 features with a simple T test rules rule where we just test if features are have a high discriminative power or not based on the T test.",
            "And this will be or transferred Signature S. And then here we will perform 200 resampling of 90% of the data.",
            "On which we will normally perform normalization, feature selection with or PSL two around technique.",
            "With the beta vector defined as follows, we put ones everywhere except on the features transferred from the source domain where we put a value of 10.",
            "And then we build a linear regime on the selected dimension and we test it on the 10% data remaining.",
            "And then we will average the BCR computed on that test in computer.",
            "So the average stability between the two 100 signatures."
        ],
        [
            "But we can also do something else since we have more than one data set as source.",
            "Available, we can combine several source domains and here we we will select features with at Test on one source domain on the other one 2 and then we will compute the intersection between both signatures and we will select signature with the size leading to an intersection of 50 features just to compare with the single transfer setting where we also transfer 50 features.",
            "And this part is exactly the same.",
            "We perform 200 re samplings.",
            "90% of 10% and perform feature selection with PSL, two room with the beta vector set accordingly to the transfer."
        ],
        [
            "True.",
            "And this is what we what we have.",
            "You have the stability on your left and the classification performance is on your right.",
            "And here you have the decreasing number of selected features on the target domain.",
            "The curve below here is just when we perform no transfer.",
            "So every bit Ajay is put to 1:00, every feature is assumed to be.",
            "Likely relevant.",
            "And you see that if we perform single transfer for one or the other data set or multiple transfer with the intersection of the two source domain, we have a huge gain of stability.",
            "And particularly for small feature size I said below 100 features, which is particularly convenient because that's the kind of size biologists are interested in.",
            "And here now if you if we look at classification performances you see that in that Journal.",
            "So there is a huge gain in BCR, and most of those differences are significant.",
            "And we performed the same experiment by replacing one data set by the author and making just the same experiment of the tree data set."
        ],
        [
            "And we also perform a sensitivity analysis because as I said, we put the value corresponding to the transferred feature to 10.",
            "This is quite arbitrary indeed, but we tested the sensitivity of the method with respect to that parameter and you see that of course, when you put it to one, there is like no transfer when you put it to two.",
            "There is only a slight variation.",
            "But as soon as you get to 5 and this is keep being true until the more than thousand there is that remarkable gain instability, an in classification performance soda method is quite insensitive to that parameter, which is convenient because you don't have to add another cross validation loop to tune that."
        ],
        [
            "So my take home messages first when you perform feature selection, it is a good thing to monitor also the stability, but not alone.",
            "You have to do that with a measure of the classification performance is.",
            "And or technique PSS room can also be used to perform inductive transfer learning at the feature level and.",
            "In that case, you can get my credit access from gene expression Omnibus with which is a micro experiment database and it's free.",
            "And the transfer learning improves mainly classification performances and stability with respect.",
            "Two sampling variation, and it has a particularly strong effect on small signatures.",
            "You can perform single or multiple transfer, and the method is quite insensitive to the choice of beta and."
        ],
        [
            "Thank you for your attention.",
            "Question.",
            "Transfer case.",
            "I did reach out aviation.",
            "No, not really.",
            "It's the L2 around which is the the second one I presented.",
            "So it's.",
            "That's one.",
            "That's it, OK. And then use it.",
            "Way to pander to the 50.",
            "Feature select oh no, we didn't fight but we.",
            "Extra source.",
            "Exit sound yes.",
            "Yeah, I understand.",
            "Yes, it's interesting, but we think that taking advantage of multiple datasets and transferring all this information will bring more information to the system and just performing the test on the same data set but.",
            "The one that is there.",
            "Yeah, but yeah, yeah we should try that.",
            "What happens if you?",
            "Have backfired out, so suppose, yeah, that's that's an interesting point.",
            "Yeah.",
            "10 would be a very bad idea that biases your stability.",
            "OK, but that's an interesting question and I thank you for asking it, because in the previous paper we tested that with by feeding the model with appropriate irrelevant knowledge and we see that since it is an interactive approach, the technique is still able to depart from that knowledge and come to the situation just as if you provide nothing, so it doesn't degrade the reasons to provide wrong knowledge.",
            "I assume it takes longer to move away from it.",
            "If your beta is higher, yeah, probably.",
            "One more question here.",
            "Extend your method to nonlinear SVM.",
            "Yes, but the problem, but the point is to keep a linear regime to keep the interpretation of the model where you can still interpret your feature as each gene separately, because that is what interests biologist to to have the genes kept as jeans and not mixed in a kernel.",
            "Yeah, yeah, sure.",
            "Yeah.",
            "Thanks, Pete."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so hello, I'm glad because many things in my talk will be related to other things we heard already today, and in fact this Turkey is a kind of sequel of another talk we made that I CML where we introduced the feature selection technique, able to incorporate prior knowledge which we get from field experts.",
                    "label": 0
                },
                {
                    "sent": "And here we will apply this technique in another setting and namely the transfer.",
                    "label": 0
                },
                {
                    "sent": "Learning setting.",
                    "label": 0
                },
                {
                    "sent": "And the the technique will be based on linear regularised models and we are particularly interested in future in datasets where we have a huge number of dimensions and really few really low number of samples.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And one example of such data is a micro radiator, and if you don't know what are micro array you can think of it as a way to measure the genes expression, which is a kind of gene activity, and so the data looks like that where you have one column by gene and one row by sample.",
                    "label": 0
                },
                {
                    "sent": "A simple Mary correspond to a patient and generally we add external rotation, which are the class label.",
                    "label": 0
                },
                {
                    "sent": "For example, is the patient status.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with recent technologies it is frequent that we have several 10s of thousands of dimension, but since it is expensive, we only have a few samples and if you get something like 2 or 300 samples.",
                    "label": 0
                },
                {
                    "sent": "Your.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ricky.",
                    "label": 0
                },
                {
                    "sent": "Those data are particularly useful to perform automated diagnosis or prognosis, which can be used in a clinical or pharmaceutical setting.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we for them or perform feature selection, that is, select a small subset of genes able to perform a good classification.",
                    "label": 1
                },
                {
                    "sent": "This is called signature discovery, and it is convenient for explanatory concern to give an indication to biologists which dimensions with jeans are related to the output, we try to predict.",
                    "label": 0
                },
                {
                    "sent": "I need to also useful to make cheaper diagnosis or prognosis.",
                    "label": 0
                },
                {
                    "sent": "Kids and in some task feature selection has been shown to improve the classification.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conferences.",
                    "label": 0
                },
                {
                    "sent": "And then one frequently used algorithm to perform classification is the Super vector machine, which should good performances in many applications.",
                    "label": 0
                },
                {
                    "sent": "And and I think is that many extensions for feature selections have been proposed which are based on.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Support vector machine and one example is the recursive feature elimination, which is an iterative algorithm based on linear support vector machine and here we can see an example in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Suppose we want to reduce the space.",
                    "label": 0
                },
                {
                    "sent": "He is a toy example because we only have two features there.",
                    "label": 0
                },
                {
                    "sent": "But you see that the feature two corresponds to a small component of the W vector which defines the separating hyperplane, and so you can drop that feature and projects on the subspace.",
                    "label": 0
                },
                {
                    "sent": "The composer here just feature one without decreasing much the margin of your classifier.",
                    "label": 1
                },
                {
                    "sent": "This is an embedded technique because it relies on the very structure of the classifier, which is an elegant way of performing feature selection.",
                    "label": 1
                },
                {
                    "sent": "In a context.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification and other approaches have been proposed to test the minimisation of the zero norm of linear model.",
                    "label": 1
                },
                {
                    "sent": "The zero normal as another speaker reminded, this is just the number of manual element of a vector and this is a direct way to express feature selection in the context of classification.",
                    "label": 0
                },
                {
                    "sent": "But that problem is not tractable.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As such so relaxations have been proposed.",
                    "label": 0
                },
                {
                    "sent": "For example, the error methods in which we replace the zero norm by the sum of the logarithm of the absolute value of each component.",
                    "label": 0
                },
                {
                    "sent": "And you have this little epsilon over there with just to prevent problems with when one dimension becomes exactly 0 and you want to minimize that objective function.",
                    "label": 0
                },
                {
                    "sent": "And you can see here that it is a good approximation to the zero norm objective function because it is nearly flat everywhere except.",
                    "label": 0
                },
                {
                    "sent": "On the exist when.",
                    "label": 0
                },
                {
                    "sent": "One dimension becomes exactly 0, or tends toward zero.",
                    "label": 0
                },
                {
                    "sent": "There is a huge gain in the value of the objective, and we add a margin constraint here to guarantee good classification.",
                    "label": 0
                },
                {
                    "sent": "And the authors of the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This message so that they could derive an algorithm also based on linear SVM, just as the one we showed before where you in the first step of the algorithm you train a linear SVM but with rescaled inputs of the margin constraints.",
                    "label": 0
                },
                {
                    "sent": "And here W case just initialize with ones and the second step consists of updating that vector by multiplying component wise with the results of the SVM estimation.",
                    "label": 0
                },
                {
                    "sent": "And you do that iteratively and progressively.",
                    "label": 0
                },
                {
                    "sent": "Less important features will tend to decrease and then fall below the machine precision.",
                    "label": 0
                },
                {
                    "sent": "An are just then eliminated.",
                    "label": 0
                },
                {
                    "sent": "So it is a kind of smoother way to perform feature selection than the recursive feature elimination.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is that even with simple linear model, when you have so many dimensions and so few points, you have overfitting problem and lack of robustness.",
                    "label": 0
                },
                {
                    "sent": "Even if you regularize with maximal margin or other devices like that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we want to.",
                    "label": 0
                },
                {
                    "sent": "Add extra information into our learning system.",
                    "label": 0
                },
                {
                    "sent": "The problem is that we do not have more symptoms.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we will consider here a setting of transfer learning in traditional learning, setting you each time you have a new data set with a classification task you want to solve, you will design a new learning system.",
                    "label": 1
                },
                {
                    "sent": "In a transfer learning setting you will reuse similar data sets that you already get from elsewhere or for previous tasks.",
                    "label": 1
                },
                {
                    "sent": "We will try to extract knowledge from them and transfer that knowledge into your new learning system.",
                    "label": 0
                },
                {
                    "sent": "You have to design on your target task so we will have source datasets and target datasets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as I said in my short introduction, we will use here are technically introduced some months ago, which allowed us to incorporate expert knowledge on the feature relevance.",
                    "label": 1
                },
                {
                    "sent": "For example, biologists may know or guess that some of the dimensions which are genes in this case are related to the output we want to predict.",
                    "label": 0
                },
                {
                    "sent": "And that knowledge may not be sufficient to build a model just on those dimension, but it is extra information we want to use.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, this knowledge is not always available, and that's why here we will simulate that knowledge in the transfer learning setting where we will extract knowledge from.",
                    "label": 0
                },
                {
                    "sent": "The source datasets and bias the feature selection on the target that I said accordingly to that knowledge and here you don't have to be confused.",
                    "label": 0
                },
                {
                    "sent": "This technique is called partially supervised feature selection.",
                    "label": 1
                },
                {
                    "sent": "But it is nothing.",
                    "label": 0
                },
                {
                    "sent": "It has nothing to do with some supervised learning, because here we assume we have a full supervision on the class.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Labels.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then this is the technique.",
                    "label": 0
                },
                {
                    "sent": "In fact, we just extended around methods to incorporate prior knowledge and we define a relevance vector beta of the same size as the number of dimensions of our problem.",
                    "label": 0
                },
                {
                    "sent": "And each better Jay will include in fact, the prior relevance of feature J and the Morra.",
                    "label": 1
                },
                {
                    "sent": "Feature is relevant, the higher its corresponding letter J and if you don't know anything about the prior relevance of a feature, you just put it to one.",
                    "label": 0
                },
                {
                    "sent": "And so you recognize here the nearly the same problem as the PARAM method.",
                    "label": 0
                },
                {
                    "sent": "Except that here you have a multiplicative factor which will penalize less the dimensions which are assumed to be more relevant.",
                    "label": 0
                },
                {
                    "sent": "And the same the rest of the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem is the same with the margin constraints an.",
                    "label": 0
                },
                {
                    "sent": "We also derive an iterative algorithm still based on a linear SVM with also this rescaling inputs of the inputs of the margin constraint.",
                    "label": 0
                },
                {
                    "sent": "But instead of initializing WK212 vector of 1, so we initialize it to beat them.",
                    "label": 0
                },
                {
                    "sent": "And in the second step we re multiply the value K by the solution at the first step.",
                    "label": 0
                },
                {
                    "sent": "And also by or prior elements.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we really here use 3 micro rated.",
                    "label": 0
                },
                {
                    "sent": "I said which are both related to prostate cancer.",
                    "label": 0
                },
                {
                    "sent": "The task is to discriminate between normal tissue and tumor tissue.",
                    "label": 0
                },
                {
                    "sent": "You see that there is a large number of feature.",
                    "label": 0
                },
                {
                    "sent": "And those data have been produced with similar but different technologies that explains the small differences in the number of features, but also in that number.",
                    "label": 0
                },
                {
                    "sent": "Some features are not the same, so we restrict those datasets to the common.",
                    "label": 0
                },
                {
                    "sent": "Dimension the share, which is a 12,600 and you see also that the ratio between the two classes are not the same man.",
                    "label": 0
                },
                {
                    "sent": "The way be logical material has been extracted is not the same, so although they are related, this is not.",
                    "label": 0
                },
                {
                    "sent": "We cannot just concatenate.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Those datasets.",
                    "label": 0
                },
                {
                    "sent": "And we will be interested in two performances.",
                    "label": 0
                },
                {
                    "sent": "Measures stability.",
                    "label": 0
                },
                {
                    "sent": "We talked about it already this morning here.",
                    "label": 0
                },
                {
                    "sent": "We're interested in stability with respect to the sampling variation.",
                    "label": 0
                },
                {
                    "sent": "We assume that if we add two patient or remove two patients from the experiment experiments, the feature selection should lead to the same result because.",
                    "label": 0
                },
                {
                    "sent": "The biological process behind the data is not expected to change much.",
                    "label": 0
                },
                {
                    "sent": "And here we will use the Culture Index, which is, roughly speaking, the ratio between the size of the intersection of two signatures divided by the size of the signature, and there is a two extra terms which simply takes into account the fact that if you select really large signature, you will have you're more likely to have similar sing.",
                    "label": 0
                },
                {
                    "sent": "It should just because there is no other choice.",
                    "label": 0
                },
                {
                    "sent": "So This is why we choose that index, but you cannot look at stability alone, because if you do so, you could have one method that always return an arbitrary set of dimensions, and indeed it will have a maximal stability.",
                    "label": 0
                },
                {
                    "sent": "But the predictive power of that signature is likely to be poor.",
                    "label": 0
                },
                {
                    "sent": "So along with stability we will also monitor classification.",
                    "label": 0
                },
                {
                    "sent": "Performance is an here.",
                    "label": 0
                },
                {
                    "sent": "We don't use accuracy because you, so the datasets are unbalanced.",
                    "label": 0
                },
                {
                    "sent": "And so, for example, a classifier simply classifying every samples in the majority class.",
                    "label": 0
                },
                {
                    "sent": "It would look as if he it were really good, and in fact it's not really the case.",
                    "label": 0
                },
                {
                    "sent": "So here we take the balanced classification rate, which is the average of the specificity and sensitivity.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is our first experimental setting.",
                    "label": 0
                },
                {
                    "sent": "Where we will have one of the three datasets I showed you a source domain and one other data set as target domain.",
                    "label": 1
                },
                {
                    "sent": "And 1st we will extract 50 features with a simple T test rules rule where we just test if features are have a high discriminative power or not based on the T test.",
                    "label": 0
                },
                {
                    "sent": "And this will be or transferred Signature S. And then here we will perform 200 resampling of 90% of the data.",
                    "label": 1
                },
                {
                    "sent": "On which we will normally perform normalization, feature selection with or PSL two around technique.",
                    "label": 0
                },
                {
                    "sent": "With the beta vector defined as follows, we put ones everywhere except on the features transferred from the source domain where we put a value of 10.",
                    "label": 0
                },
                {
                    "sent": "And then we build a linear regime on the selected dimension and we test it on the 10% data remaining.",
                    "label": 0
                },
                {
                    "sent": "And then we will average the BCR computed on that test in computer.",
                    "label": 0
                },
                {
                    "sent": "So the average stability between the two 100 signatures.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we can also do something else since we have more than one data set as source.",
                    "label": 0
                },
                {
                    "sent": "Available, we can combine several source domains and here we we will select features with at Test on one source domain on the other one 2 and then we will compute the intersection between both signatures and we will select signature with the size leading to an intersection of 50 features just to compare with the single transfer setting where we also transfer 50 features.",
                    "label": 1
                },
                {
                    "sent": "And this part is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "We perform 200 re samplings.",
                    "label": 0
                },
                {
                    "sent": "90% of 10% and perform feature selection with PSL, two room with the beta vector set accordingly to the transfer.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "And this is what we what we have.",
                    "label": 0
                },
                {
                    "sent": "You have the stability on your left and the classification performance is on your right.",
                    "label": 0
                },
                {
                    "sent": "And here you have the decreasing number of selected features on the target domain.",
                    "label": 1
                },
                {
                    "sent": "The curve below here is just when we perform no transfer.",
                    "label": 0
                },
                {
                    "sent": "So every bit Ajay is put to 1:00, every feature is assumed to be.",
                    "label": 0
                },
                {
                    "sent": "Likely relevant.",
                    "label": 0
                },
                {
                    "sent": "And you see that if we perform single transfer for one or the other data set or multiple transfer with the intersection of the two source domain, we have a huge gain of stability.",
                    "label": 1
                },
                {
                    "sent": "And particularly for small feature size I said below 100 features, which is particularly convenient because that's the kind of size biologists are interested in.",
                    "label": 0
                },
                {
                    "sent": "And here now if you if we look at classification performances you see that in that Journal.",
                    "label": 0
                },
                {
                    "sent": "So there is a huge gain in BCR, and most of those differences are significant.",
                    "label": 0
                },
                {
                    "sent": "And we performed the same experiment by replacing one data set by the author and making just the same experiment of the tree data set.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also perform a sensitivity analysis because as I said, we put the value corresponding to the transferred feature to 10.",
                    "label": 0
                },
                {
                    "sent": "This is quite arbitrary indeed, but we tested the sensitivity of the method with respect to that parameter and you see that of course, when you put it to one, there is like no transfer when you put it to two.",
                    "label": 0
                },
                {
                    "sent": "There is only a slight variation.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you get to 5 and this is keep being true until the more than thousand there is that remarkable gain instability, an in classification performance soda method is quite insensitive to that parameter, which is convenient because you don't have to add another cross validation loop to tune that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my take home messages first when you perform feature selection, it is a good thing to monitor also the stability, but not alone.",
                    "label": 1
                },
                {
                    "sent": "You have to do that with a measure of the classification performance is.",
                    "label": 0
                },
                {
                    "sent": "And or technique PSS room can also be used to perform inductive transfer learning at the feature level and.",
                    "label": 1
                },
                {
                    "sent": "In that case, you can get my credit access from gene expression Omnibus with which is a micro experiment database and it's free.",
                    "label": 1
                },
                {
                    "sent": "And the transfer learning improves mainly classification performances and stability with respect.",
                    "label": 0
                },
                {
                    "sent": "Two sampling variation, and it has a particularly strong effect on small signatures.",
                    "label": 1
                },
                {
                    "sent": "You can perform single or multiple transfer, and the method is quite insensitive to the choice of beta and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention.",
                    "label": 1
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Transfer case.",
                    "label": 0
                },
                {
                    "sent": "I did reach out aviation.",
                    "label": 0
                },
                {
                    "sent": "No, not really.",
                    "label": 0
                },
                {
                    "sent": "It's the L2 around which is the the second one I presented.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "That's one.",
                    "label": 0
                },
                {
                    "sent": "That's it, OK. And then use it.",
                    "label": 0
                },
                {
                    "sent": "Way to pander to the 50.",
                    "label": 0
                },
                {
                    "sent": "Feature select oh no, we didn't fight but we.",
                    "label": 0
                },
                {
                    "sent": "Extra source.",
                    "label": 0
                },
                {
                    "sent": "Exit sound yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I understand.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's interesting, but we think that taking advantage of multiple datasets and transferring all this information will bring more information to the system and just performing the test on the same data set but.",
                    "label": 0
                },
                {
                    "sent": "The one that is there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but yeah, yeah we should try that.",
                    "label": 0
                },
                {
                    "sent": "What happens if you?",
                    "label": 0
                },
                {
                    "sent": "Have backfired out, so suppose, yeah, that's that's an interesting point.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "10 would be a very bad idea that biases your stability.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's an interesting question and I thank you for asking it, because in the previous paper we tested that with by feeding the model with appropriate irrelevant knowledge and we see that since it is an interactive approach, the technique is still able to depart from that knowledge and come to the situation just as if you provide nothing, so it doesn't degrade the reasons to provide wrong knowledge.",
                    "label": 0
                },
                {
                    "sent": "I assume it takes longer to move away from it.",
                    "label": 0
                },
                {
                    "sent": "If your beta is higher, yeah, probably.",
                    "label": 0
                },
                {
                    "sent": "One more question here.",
                    "label": 0
                },
                {
                    "sent": "Extend your method to nonlinear SVM.",
                    "label": 0
                },
                {
                    "sent": "Yes, but the problem, but the point is to keep a linear regime to keep the interpretation of the model where you can still interpret your feature as each gene separately, because that is what interests biologist to to have the genes kept as jeans and not mixed in a kernel.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Thanks, Pete.",
                    "label": 0
                }
            ]
        }
    }
}