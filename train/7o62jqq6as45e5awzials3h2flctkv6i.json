{
    "id": "7o62jqq6as45e5awzials3h2flctkv6i",
    "title": "Rule Learning with Monotonicity Constraints",
    "info": {
        "author": [
            "Wojciech Kotlowski, Institute of Computing Science, Poznan University of Technology"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_kotlowski_rlwmc/",
    "segmentation": [
        [
            "So."
        ],
        [
            "Let's start with where describing what is classification with an automatic constraints, and this is a classification problem with additional assumptions, so we assume that there is some kind of an order between class labels and.",
            "Moreover, we assume that every attribute, so every input variable is also ordered, so the domain of that reduces at least ordinal, and Moreover we assume that there is a monotone relationship between input and output in the sense that an increase in values of attributes should not decrease the.",
            "Class label So what we are going to do you have trying to influence to do inference with monotone functions."
        ],
        [
            "Here's a simple example.",
            "This is a house pricing problem.",
            "A little data we can which are available and they contain.",
            "That house is salting in Windsor and the class here is the selling price of the House.",
            "As soon as we are doing classification rather than regression, we discretize it into 4 levels into four classes, and I presented several attributes here.",
            "Then one can easily show that, for instance, the price of the House should not decrease when the size of the lot increases as long as other attributes are are being fixed and also the other attributes, like the number of bedrooms.",
            "Number of stories and so on.",
            "If it could increases and we don't expect that the price should decrease.",
            "So this is the general idea."
        ],
        [
            "The question is why we used knowledge about the monotonicity and there are at least two important reasons to use it to do it.",
            "First, monotonicity imposes some constraints on the production function, so we we decrease the hypothesis space, which means that we have less complex models, so it means that we expect an increase in accuracy of predictions, and Moreover, there is another reason that sometimes only the model which is consistent with the expert knowledge can be accepted by experts.",
            "So Express will not accept the model which.",
            "Violates data common knowledge about the problem."
        ],
        [
            "So the outline of the talk is following.",
            "First, we start with probabilistic model based on the starting dominant.",
            "So I will show how to how to.",
            "How to use knowledge about 132 by constraining the probability distribution.",
            "Then I will present the nonparametric method of classification.",
            "And finally I show how to learn rule and samples in the presence of monotonicity constraints.",
            "So let's start with."
        ],
        [
            "Theory, so we assume as usual, that objects are generated IID according to some probability distribution, which is unknown, and we propose some loss function which is a penalty for predicting Y hat when the observed value is Y.",
            "Observe class labels Y and the classifier is just a function from X to Y, and we measure the quality of the classifier as expected loss according to the probability distribution, which is also called the risk and the minimizer of the risk is the bias classifier.",
            "This is the classifier which we want to achieve.",
            "This is our goal.",
            "This is the best possible classifier based classifier."
        ],
        [
            "And the problem is that how can we incorporate in knowledge about the monotonicity?",
            "In terms of the probability distribution, so in this formalism we try somehow to incorporate the monotonicity constraints, and to do that."
        ],
        [
            "We will define to Domino's relation and monotone functions.",
            "So demonstration is defined as a partial order between points on input space.",
            "So we assume that one point X dominates expire is if it has higher or equal values on all attributes.",
            "So it's just a simple partial order between vectors and it's called dominance from historical results and then it will define this relation.",
            "We can define the function the monotone function.",
            "We say that the function is monotone if for any two points XX prime it holds that whatever.",
            "X dominates explained, then the value of function is not greater than value function that expire."
        ],
        [
            "So here's an example of dominoes relations.",
            "So this we have two attributes and objects from 3, three classes, three order process, and for example, this object is dominates those objects.",
            "Here as being dominated by those objects there.",
            "So we should assume that this object should probably have class label greater than those and then not smaller than those not greater than those.",
            "So intuitively."
        ],
        [
            "I assume that if X double X pregnant ex probably has a higher or equal class available labeled an ex prime, and we formalize it in the following."
        ],
        [
            "I we assume that the objects are generated according to some probability distribution which will call monotonically constraint and this is expressed in the following way that we assume that whenever X dominates X prime, then the probability of the event that class labels is at least K conditioner X is greater equal in probability of the event condition X prime.",
            "So in other word, for each K the function.",
            "Probability of the event class at least K is a monotone function.",
            "Or one can also look at this from the other point of view.",
            "And I mean this is something which is known as stochastic dominance relation between probability distributions, so in.",
            "So we can also say about that that if object as dominant explained, then the probability distribution condition on X dominates probability distribution condition at X."
        ],
        [
            "Right, so if we assume that we have such problems distribution, we ask if the bias classifier is a monotone function, and it appears that is not always the case, and then you can we show that if we restrict our considerations only to the function loss functions, which depends on the difference between class labels like 01, those absolute error loss, quieter levels and so on, then device classifiers monotone if and only if this function is convex.",
            "So we restrict our considerations only to convex functions.",
            "We do because this result suggests that probably nonconvex functions may not be appropriate for those kinds of problems."
        ],
        [
            "OK, So what we what I was talking about till now was about.",
            "Let's say the population level when the probability distribution is known, but it's never the case in general.",
            "Distribution is unknown and so is the bias classifier.",
            "So we train a classifier using a sample of data points and usually it's performed by mutation of the empirical risk, which is the training error and this is done in a restricted class of function to avoid overfitting.",
            "So we use let's say linear class or function trees rules.",
            "We impose some kind of regularization and so on.",
            "And in classification with the monotonicity constraints, we are able to minimize the risk in the class of all monotone function, although the class of model monotone function is not nonparametric in the sense that it can be described by a finite number of parameters."
        ],
        [
            "I know it is doable because we only need to estimate the values of monotone functions at the points where we have training data, so we can be stated as an integer in our program.",
            "So we minimize the value of the loss in all points in all training points, with the assumption that effects dominate XI XJ, then the value of the function at that point must be not smaller than the value of the function at point J.",
            "So Dr values of the monotone function which minimizes the risk.",
            "And here is the integer constraint.",
            "But after transforming this program and using the fact that the constraint matrix is uni modular.",
            "We can drop the integer constraints and we can solve it as a linear program.",
            "We still get optimal solution and the optimal solution can be solved can be obtained efficiently using by solving a normal linear program without any integer constraints, so it's probably not a problem and it works quite fast and the interpretation of this is the following that we can treat values of the values of function which minimizes the risk as new labels.",
            "So the interpretation is to label the objects to remove inconsistencies with respect to democracy.",
            "Monotonicity constraints and those new labels are always monotone, so we can think of this as some kind of a data data minimization, or nonparametric error correction, and one can show that as the sample size increases, those values of this function 10 they converge to the base classifier."
        ],
        [
            "So here's an example.",
            "Here we have original data and we have some violation of monotonicity constraints.",
            "For instance, this object is dominating that one, but it has smaller label.",
            "Similar with Object 610, and the result of the."
        ],
        [
            "Modernization is like that, so we change those two labels and now we have a data set which is consistent with our monotonicity constraints and we."
        ],
        [
            "Do prediction with this.",
            "For instance, we have a test object."
        ],
        [
            "And we look.",
            "What are the objects dominated by that one and what object dominated this?",
            "And we see that since this object is dominated by object from class one and dominates object from Class 1."
        ],
        [
            "We get assigned this object Class 1."
        ],
        [
            "But sometimes the prediction is ambiguous.",
            "For instance in this case."
        ],
        [
            "If you look at what objects are dominating are being terminated, we can assign to this object either Class 2 or cluster."
        ],
        [
            "So both results are possible and they do not violate."
        ],
        [
            "Tony City so."
        ],
        [
            "But the problem of nonparametric classification is that it can give an in process prediction and it requires memorization of a large part of the training data to classify later.",
            "So our solution is to 1st apply this parametric on property classification to obtain this modernized data which we called the prime and then we try to somehow compress them alternate data using a set of rules.",
            "And we combined the set of rules into the ensemble."
        ],
        [
            "So what's the rule or decision rule?",
            "It's a logical expression of the form if condition, decision and the other case decision condition is condition part is a conjunction of constraints before XI is greater, equal something or X is smaller or equal.",
            "Something where XI is the value of object on.",
            "I attribute and decision is a vote for a given combination of classes.",
            "So class, at least K or at most K. Here's an example from the House pricing data.",
            "If lot size is greater equal than 80,000 and storage number of stories is at least two, then price level.",
            "So they do class size classes at least three.",
            "And of course a single rule is too weak to classify, so a set of rules you needed, and we combine those rules in the interlinear combinat."
        ],
        [
            "So here is an example of how rules might compress the data.",
            "So this is again the modified date."
        ],
        [
            "Asset and now the first roll which indicates class.",
            "At least two.",
            "This is a trivial because every object satisfies this rule."
        ],
        [
            "Then we added the final rule, which.",
            "Which has a condition one condition, and it indicates class at."
        ],
        [
            "Most one and finally federal, which has additional conditions, and it indicates a class at least three.",
            "And as we see here now we have.",
            "We have universal prediction in every point and the data where was compressed using the three rules."
        ],
        [
            "So formally rule and some was set of K -- 1 convex combinations of rules, so each.",
            "Convex combination is.",
            "As a function is the sum of using a combination of roles such that the coefficient sums to one another loan limit non negative and for each KFK aims at separating class at least K from class at most K -- 1.",
            "So we have kind minus one binary problems.",
            "So we transform the problem today minus one diner problems.",
            "So rules are Katie is a set of rules voting for class at least K and then.",
            "It's value is 1 or at most K -- 1 and then it's value is minus one.",
            "It's like binary classifiers in a typical boosting problem and the final response of the classifier is just the sum of the science of the functions F and one can show that if we combine the classifiers in the following way, then the absolute error of the classifier H does not exceed the sum of 01 errors of the functions F, which means that if we are able to train a function on the binary.",
            "Problem then we are also able to train a classifier which works well with absolute error of us."
        ],
        [
            "So generating rules is done using the immunization of maximum margin.",
            "So we introduce auxiliary variables for The Cave of anger problem which equals to one if the original variable is that this K and minus Y 1 otherwise an, we minimize this something which is called the minimum margin.",
            "So we maximize something which for minimal margin, and this is well known problem and can be solved efficiently as a linear program.",
            "But using a column generation algorithm.",
            "And it is exactly how linear programming boosting works, and we can prove that the solution with a positive margin exists if and only if the data set is monotone, which somehow justifies why we did modernization of the data before.",
            "So we also have a dinner."
        ],
        [
            "Position bound with.",
            "When we assume that the distribution is monitoring the constraints and H is the final classifier and FK are the rules.",
            "Sample training on the monotonic data set.",
            "The prime.",
            "And they achieve minimal margin gamma K again with probability at least gamma.",
            "We have that the expected expected accuracy of the risk of our classifier is not not greater than the risk of the best possible classifier bias classifier.",
            "So the difference between them.",
            "The regret is not greater than this complexity term.",
            "This is the same standard.",
            "A way of proving visualization bounds using the margins.",
            "But here we show that we can bound the difference between our classifier and the optimal classifier.",
            "While untypical bounds, we bound the difference between the classifier and some value obtained on the training set.",
            "So there."
        ],
        [
            "Experimental results.",
            "We did an experiment.",
            "We used 10 datasets for which the monotone relationships are known to exist, and we used five classifiers.",
            "Those two maybe are not well known, but they were used before in dealing with problems with monotonicity constraints.",
            "While those two are well known, but they do not use knowledge about monotonicity constraint.",
            "Anyway, we run them in ordinal setting just to ensure that they minimize absolute error loss instead of 01 loss because the absolute error was the measure.",
            "Our accuracy we used and our method which we call linear programming rules.",
            "This is the matter that describe before and we did 10 fold cross validation repeated it 10 times to improve the replica ability of the experiment.",
            "Here are the results.",
            "So those."
        ],
        [
            "The datasets, those are the algorithms our algorithms in middle and each time we mark the best results and every result within one standard error from the best result we marked with bold.",
            "So we see that in most cases our algorithm behaves better than the other algorithms."
        ],
        [
            "So summarizing, we propose a statistical theory for classification.",
            "With monotonicity constraints, we show how to classify in a nonparametric way without any additional assumption about the model.",
            "So classifying using the class of all monotone functions, and finally we propose a method for compressing the training data with rule and sample, an show that achieves a nice accuracy on the real data set.",
            "Thank you very much."
        ],
        [
            "So looking at your example, it seems like what you're kind of trying to do.",
            "Something like K nearest neighbors.",
            "House your approach parenting standard instance based learning.",
            "Do you mean nonparametric classification, right?",
            "Yeah, I mean I know about you mean that this is similar to kind of maybe probably mean this parametric classification.",
            "So here we don't use any distance measure, we just observe the dominance relations.",
            "So we only look at which objects are in relation with other objects that this has nothing to do with any kind of distance.",
            "So we only take into account the order in this space.",
            "This partial order in this space.",
            "This is the only thing we exploit here, so we don't use any distance metric.",
            "Can you lose the 1st order like you have an almost partial order where you may be penalized if something breaks it, but it's really good, so can you repeat.",
            "Can you change the thing so penalized for breaking the partial order, but you don't disallow it?",
            "Yeah, I mean.",
            "Yeah, you could do that, but anyway, the even the solution which which does not bring the partial orders are at all is easy to obtain, but indeed you can do that.",
            "You can leave some violations.",
            "Unchanged.",
            "Can you generalize?",
            "Yeah, I think this is not going to be hard to generalize it to regression, right?",
            "But you need to.",
            "I mean, there exists an approach to regression like that, it's called isotonic regression.",
            "And we can minimize the squared error loss in the same way.",
            "So we have also only assumption about the monotonicity, and this is only thing we should we use and we try to.",
            "To minimize the squared error loss and the output is continuous.",
            "So in this case such an approach already in existing statistics.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start with where describing what is classification with an automatic constraints, and this is a classification problem with additional assumptions, so we assume that there is some kind of an order between class labels and.",
                    "label": 1
                },
                {
                    "sent": "Moreover, we assume that every attribute, so every input variable is also ordered, so the domain of that reduces at least ordinal, and Moreover we assume that there is a monotone relationship between input and output in the sense that an increase in values of attributes should not decrease the.",
                    "label": 1
                },
                {
                    "sent": "Class label So what we are going to do you have trying to influence to do inference with monotone functions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a simple example.",
                    "label": 0
                },
                {
                    "sent": "This is a house pricing problem.",
                    "label": 0
                },
                {
                    "sent": "A little data we can which are available and they contain.",
                    "label": 0
                },
                {
                    "sent": "That house is salting in Windsor and the class here is the selling price of the House.",
                    "label": 1
                },
                {
                    "sent": "As soon as we are doing classification rather than regression, we discretize it into 4 levels into four classes, and I presented several attributes here.",
                    "label": 0
                },
                {
                    "sent": "Then one can easily show that, for instance, the price of the House should not decrease when the size of the lot increases as long as other attributes are are being fixed and also the other attributes, like the number of bedrooms.",
                    "label": 1
                },
                {
                    "sent": "Number of stories and so on.",
                    "label": 0
                },
                {
                    "sent": "If it could increases and we don't expect that the price should decrease.",
                    "label": 0
                },
                {
                    "sent": "So this is the general idea.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question is why we used knowledge about the monotonicity and there are at least two important reasons to use it to do it.",
                    "label": 0
                },
                {
                    "sent": "First, monotonicity imposes some constraints on the production function, so we we decrease the hypothesis space, which means that we have less complex models, so it means that we expect an increase in accuracy of predictions, and Moreover, there is another reason that sometimes only the model which is consistent with the expert knowledge can be accepted by experts.",
                    "label": 1
                },
                {
                    "sent": "So Express will not accept the model which.",
                    "label": 1
                },
                {
                    "sent": "Violates data common knowledge about the problem.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of the talk is following.",
                    "label": 0
                },
                {
                    "sent": "First, we start with probabilistic model based on the starting dominant.",
                    "label": 1
                },
                {
                    "sent": "So I will show how to how to.",
                    "label": 0
                },
                {
                    "sent": "How to use knowledge about 132 by constraining the probability distribution.",
                    "label": 1
                },
                {
                    "sent": "Then I will present the nonparametric method of classification.",
                    "label": 0
                },
                {
                    "sent": "And finally I show how to learn rule and samples in the presence of monotonicity constraints.",
                    "label": 0
                },
                {
                    "sent": "So let's start with.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theory, so we assume as usual, that objects are generated IID according to some probability distribution, which is unknown, and we propose some loss function which is a penalty for predicting Y hat when the observed value is Y.",
                    "label": 1
                },
                {
                    "sent": "Observe class labels Y and the classifier is just a function from X to Y, and we measure the quality of the classifier as expected loss according to the probability distribution, which is also called the risk and the minimizer of the risk is the bias classifier.",
                    "label": 0
                },
                {
                    "sent": "This is the classifier which we want to achieve.",
                    "label": 0
                },
                {
                    "sent": "This is our goal.",
                    "label": 0
                },
                {
                    "sent": "This is the best possible classifier based classifier.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the problem is that how can we incorporate in knowledge about the monotonicity?",
                    "label": 0
                },
                {
                    "sent": "In terms of the probability distribution, so in this formalism we try somehow to incorporate the monotonicity constraints, and to do that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We will define to Domino's relation and monotone functions.",
                    "label": 0
                },
                {
                    "sent": "So demonstration is defined as a partial order between points on input space.",
                    "label": 0
                },
                {
                    "sent": "So we assume that one point X dominates expire is if it has higher or equal values on all attributes.",
                    "label": 1
                },
                {
                    "sent": "So it's just a simple partial order between vectors and it's called dominance from historical results and then it will define this relation.",
                    "label": 0
                },
                {
                    "sent": "We can define the function the monotone function.",
                    "label": 1
                },
                {
                    "sent": "We say that the function is monotone if for any two points XX prime it holds that whatever.",
                    "label": 0
                },
                {
                    "sent": "X dominates explained, then the value of function is not greater than value function that expire.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of dominoes relations.",
                    "label": 0
                },
                {
                    "sent": "So this we have two attributes and objects from 3, three classes, three order process, and for example, this object is dominates those objects.",
                    "label": 0
                },
                {
                    "sent": "Here as being dominated by those objects there.",
                    "label": 0
                },
                {
                    "sent": "So we should assume that this object should probably have class label greater than those and then not smaller than those not greater than those.",
                    "label": 0
                },
                {
                    "sent": "So intuitively.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I assume that if X double X pregnant ex probably has a higher or equal class available labeled an ex prime, and we formalize it in the following.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I we assume that the objects are generated according to some probability distribution which will call monotonically constraint and this is expressed in the following way that we assume that whenever X dominates X prime, then the probability of the event that class labels is at least K conditioner X is greater equal in probability of the event condition X prime.",
                    "label": 1
                },
                {
                    "sent": "So in other word, for each K the function.",
                    "label": 1
                },
                {
                    "sent": "Probability of the event class at least K is a monotone function.",
                    "label": 1
                },
                {
                    "sent": "Or one can also look at this from the other point of view.",
                    "label": 0
                },
                {
                    "sent": "And I mean this is something which is known as stochastic dominance relation between probability distributions, so in.",
                    "label": 0
                },
                {
                    "sent": "So we can also say about that that if object as dominant explained, then the probability distribution condition on X dominates probability distribution condition at X.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so if we assume that we have such problems distribution, we ask if the bias classifier is a monotone function, and it appears that is not always the case, and then you can we show that if we restrict our considerations only to the function loss functions, which depends on the difference between class labels like 01, those absolute error loss, quieter levels and so on, then device classifiers monotone if and only if this function is convex.",
                    "label": 1
                },
                {
                    "sent": "So we restrict our considerations only to convex functions.",
                    "label": 0
                },
                {
                    "sent": "We do because this result suggests that probably nonconvex functions may not be appropriate for those kinds of problems.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what we what I was talking about till now was about.",
                    "label": 0
                },
                {
                    "sent": "Let's say the population level when the probability distribution is known, but it's never the case in general.",
                    "label": 0
                },
                {
                    "sent": "Distribution is unknown and so is the bias classifier.",
                    "label": 1
                },
                {
                    "sent": "So we train a classifier using a sample of data points and usually it's performed by mutation of the empirical risk, which is the training error and this is done in a restricted class of function to avoid overfitting.",
                    "label": 1
                },
                {
                    "sent": "So we use let's say linear class or function trees rules.",
                    "label": 1
                },
                {
                    "sent": "We impose some kind of regularization and so on.",
                    "label": 0
                },
                {
                    "sent": "And in classification with the monotonicity constraints, we are able to minimize the risk in the class of all monotone function, although the class of model monotone function is not nonparametric in the sense that it can be described by a finite number of parameters.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know it is doable because we only need to estimate the values of monotone functions at the points where we have training data, so we can be stated as an integer in our program.",
                    "label": 1
                },
                {
                    "sent": "So we minimize the value of the loss in all points in all training points, with the assumption that effects dominate XI XJ, then the value of the function at that point must be not smaller than the value of the function at point J.",
                    "label": 0
                },
                {
                    "sent": "So Dr values of the monotone function which minimizes the risk.",
                    "label": 0
                },
                {
                    "sent": "And here is the integer constraint.",
                    "label": 0
                },
                {
                    "sent": "But after transforming this program and using the fact that the constraint matrix is uni modular.",
                    "label": 0
                },
                {
                    "sent": "We can drop the integer constraints and we can solve it as a linear program.",
                    "label": 0
                },
                {
                    "sent": "We still get optimal solution and the optimal solution can be solved can be obtained efficiently using by solving a normal linear program without any integer constraints, so it's probably not a problem and it works quite fast and the interpretation of this is the following that we can treat values of the values of function which minimizes the risk as new labels.",
                    "label": 0
                },
                {
                    "sent": "So the interpretation is to label the objects to remove inconsistencies with respect to democracy.",
                    "label": 1
                },
                {
                    "sent": "Monotonicity constraints and those new labels are always monotone, so we can think of this as some kind of a data data minimization, or nonparametric error correction, and one can show that as the sample size increases, those values of this function 10 they converge to the base classifier.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Here we have original data and we have some violation of monotonicity constraints.",
                    "label": 0
                },
                {
                    "sent": "For instance, this object is dominating that one, but it has smaller label.",
                    "label": 0
                },
                {
                    "sent": "Similar with Object 610, and the result of the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Modernization is like that, so we change those two labels and now we have a data set which is consistent with our monotonicity constraints and we.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do prediction with this.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have a test object.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we look.",
                    "label": 0
                },
                {
                    "sent": "What are the objects dominated by that one and what object dominated this?",
                    "label": 0
                },
                {
                    "sent": "And we see that since this object is dominated by object from class one and dominates object from Class 1.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get assigned this object Class 1.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But sometimes the prediction is ambiguous.",
                    "label": 0
                },
                {
                    "sent": "For instance in this case.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you look at what objects are dominating are being terminated, we can assign to this object either Class 2 or cluster.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So both results are possible and they do not violate.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tony City so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the problem of nonparametric classification is that it can give an in process prediction and it requires memorization of a large part of the training data to classify later.",
                    "label": 1
                },
                {
                    "sent": "So our solution is to 1st apply this parametric on property classification to obtain this modernized data which we called the prime and then we try to somehow compress them alternate data using a set of rules.",
                    "label": 0
                },
                {
                    "sent": "And we combined the set of rules into the ensemble.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the rule or decision rule?",
                    "label": 0
                },
                {
                    "sent": "It's a logical expression of the form if condition, decision and the other case decision condition is condition part is a conjunction of constraints before XI is greater, equal something or X is smaller or equal.",
                    "label": 1
                },
                {
                    "sent": "Something where XI is the value of object on.",
                    "label": 1
                },
                {
                    "sent": "I attribute and decision is a vote for a given combination of classes.",
                    "label": 0
                },
                {
                    "sent": "So class, at least K or at most K. Here's an example from the House pricing data.",
                    "label": 0
                },
                {
                    "sent": "If lot size is greater equal than 80,000 and storage number of stories is at least two, then price level.",
                    "label": 0
                },
                {
                    "sent": "So they do class size classes at least three.",
                    "label": 0
                },
                {
                    "sent": "And of course a single rule is too weak to classify, so a set of rules you needed, and we combine those rules in the interlinear combinat.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is an example of how rules might compress the data.",
                    "label": 0
                },
                {
                    "sent": "So this is again the modified date.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asset and now the first roll which indicates class.",
                    "label": 0
                },
                {
                    "sent": "At least two.",
                    "label": 0
                },
                {
                    "sent": "This is a trivial because every object satisfies this rule.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we added the final rule, which.",
                    "label": 0
                },
                {
                    "sent": "Which has a condition one condition, and it indicates class at.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most one and finally federal, which has additional conditions, and it indicates a class at least three.",
                    "label": 0
                },
                {
                    "sent": "And as we see here now we have.",
                    "label": 0
                },
                {
                    "sent": "We have universal prediction in every point and the data where was compressed using the three rules.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So formally rule and some was set of K -- 1 convex combinations of rules, so each.",
                    "label": 1
                },
                {
                    "sent": "Convex combination is.",
                    "label": 0
                },
                {
                    "sent": "As a function is the sum of using a combination of roles such that the coefficient sums to one another loan limit non negative and for each KFK aims at separating class at least K from class at most K -- 1.",
                    "label": 1
                },
                {
                    "sent": "So we have kind minus one binary problems.",
                    "label": 1
                },
                {
                    "sent": "So we transform the problem today minus one diner problems.",
                    "label": 1
                },
                {
                    "sent": "So rules are Katie is a set of rules voting for class at least K and then.",
                    "label": 0
                },
                {
                    "sent": "It's value is 1 or at most K -- 1 and then it's value is minus one.",
                    "label": 0
                },
                {
                    "sent": "It's like binary classifiers in a typical boosting problem and the final response of the classifier is just the sum of the science of the functions F and one can show that if we combine the classifiers in the following way, then the absolute error of the classifier H does not exceed the sum of 01 errors of the functions F, which means that if we are able to train a function on the binary.",
                    "label": 0
                },
                {
                    "sent": "Problem then we are also able to train a classifier which works well with absolute error of us.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So generating rules is done using the immunization of maximum margin.",
                    "label": 0
                },
                {
                    "sent": "So we introduce auxiliary variables for The Cave of anger problem which equals to one if the original variable is that this K and minus Y 1 otherwise an, we minimize this something which is called the minimum margin.",
                    "label": 0
                },
                {
                    "sent": "So we maximize something which for minimal margin, and this is well known problem and can be solved efficiently as a linear program.",
                    "label": 1
                },
                {
                    "sent": "But using a column generation algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it is exactly how linear programming boosting works, and we can prove that the solution with a positive margin exists if and only if the data set is monotone, which somehow justifies why we did modernization of the data before.",
                    "label": 1
                },
                {
                    "sent": "So we also have a dinner.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Position bound with.",
                    "label": 0
                },
                {
                    "sent": "When we assume that the distribution is monitoring the constraints and H is the final classifier and FK are the rules.",
                    "label": 1
                },
                {
                    "sent": "Sample training on the monotonic data set.",
                    "label": 0
                },
                {
                    "sent": "The prime.",
                    "label": 0
                },
                {
                    "sent": "And they achieve minimal margin gamma K again with probability at least gamma.",
                    "label": 1
                },
                {
                    "sent": "We have that the expected expected accuracy of the risk of our classifier is not not greater than the risk of the best possible classifier bias classifier.",
                    "label": 0
                },
                {
                    "sent": "So the difference between them.",
                    "label": 0
                },
                {
                    "sent": "The regret is not greater than this complexity term.",
                    "label": 0
                },
                {
                    "sent": "This is the same standard.",
                    "label": 0
                },
                {
                    "sent": "A way of proving visualization bounds using the margins.",
                    "label": 0
                },
                {
                    "sent": "But here we show that we can bound the difference between our classifier and the optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "While untypical bounds, we bound the difference between the classifier and some value obtained on the training set.",
                    "label": 0
                },
                {
                    "sent": "So there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental results.",
                    "label": 0
                },
                {
                    "sent": "We did an experiment.",
                    "label": 0
                },
                {
                    "sent": "We used 10 datasets for which the monotone relationships are known to exist, and we used five classifiers.",
                    "label": 1
                },
                {
                    "sent": "Those two maybe are not well known, but they were used before in dealing with problems with monotonicity constraints.",
                    "label": 0
                },
                {
                    "sent": "While those two are well known, but they do not use knowledge about monotonicity constraint.",
                    "label": 0
                },
                {
                    "sent": "Anyway, we run them in ordinal setting just to ensure that they minimize absolute error loss instead of 01 loss because the absolute error was the measure.",
                    "label": 1
                },
                {
                    "sent": "Our accuracy we used and our method which we call linear programming rules.",
                    "label": 1
                },
                {
                    "sent": "This is the matter that describe before and we did 10 fold cross validation repeated it 10 times to improve the replica ability of the experiment.",
                    "label": 0
                },
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "So those.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The datasets, those are the algorithms our algorithms in middle and each time we mark the best results and every result within one standard error from the best result we marked with bold.",
                    "label": 0
                },
                {
                    "sent": "So we see that in most cases our algorithm behaves better than the other algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So summarizing, we propose a statistical theory for classification.",
                    "label": 0
                },
                {
                    "sent": "With monotonicity constraints, we show how to classify in a nonparametric way without any additional assumption about the model.",
                    "label": 0
                },
                {
                    "sent": "So classifying using the class of all monotone functions, and finally we propose a method for compressing the training data with rule and sample, an show that achieves a nice accuracy on the real data set.",
                    "label": 1
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So looking at your example, it seems like what you're kind of trying to do.",
                    "label": 0
                },
                {
                    "sent": "Something like K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "House your approach parenting standard instance based learning.",
                    "label": 0
                },
                {
                    "sent": "Do you mean nonparametric classification, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean I know about you mean that this is similar to kind of maybe probably mean this parametric classification.",
                    "label": 0
                },
                {
                    "sent": "So here we don't use any distance measure, we just observe the dominance relations.",
                    "label": 0
                },
                {
                    "sent": "So we only look at which objects are in relation with other objects that this has nothing to do with any kind of distance.",
                    "label": 0
                },
                {
                    "sent": "So we only take into account the order in this space.",
                    "label": 0
                },
                {
                    "sent": "This partial order in this space.",
                    "label": 0
                },
                {
                    "sent": "This is the only thing we exploit here, so we don't use any distance metric.",
                    "label": 0
                },
                {
                    "sent": "Can you lose the 1st order like you have an almost partial order where you may be penalized if something breaks it, but it's really good, so can you repeat.",
                    "label": 0
                },
                {
                    "sent": "Can you change the thing so penalized for breaking the partial order, but you don't disallow it?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could do that, but anyway, the even the solution which which does not bring the partial orders are at all is easy to obtain, but indeed you can do that.",
                    "label": 0
                },
                {
                    "sent": "You can leave some violations.",
                    "label": 0
                },
                {
                    "sent": "Unchanged.",
                    "label": 0
                },
                {
                    "sent": "Can you generalize?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think this is not going to be hard to generalize it to regression, right?",
                    "label": 0
                },
                {
                    "sent": "But you need to.",
                    "label": 0
                },
                {
                    "sent": "I mean, there exists an approach to regression like that, it's called isotonic regression.",
                    "label": 0
                },
                {
                    "sent": "And we can minimize the squared error loss in the same way.",
                    "label": 0
                },
                {
                    "sent": "So we have also only assumption about the monotonicity, and this is only thing we should we use and we try to.",
                    "label": 0
                },
                {
                    "sent": "To minimize the squared error loss and the output is continuous.",
                    "label": 0
                },
                {
                    "sent": "So in this case such an approach already in existing statistics.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}