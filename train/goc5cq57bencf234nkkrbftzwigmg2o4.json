{
    "id": "goc5cq57bencf234nkkrbftzwigmg2o4",
    "title": "DWRank: Learning Concept Ranking for Ontology Search",
    "info": {
        "author": [
            "Armin Haller, Research School of Information Sciences and Engineering, Australian National University"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_haller_DWRank/",
    "segmentation": [
        [
            "So in in order to start with the talk, I start with the foundations of this algorithms and then at the end I will talk about the learning part of this of this work.",
            "So I want to start with the."
        ],
        [
            "Concept search and ranking.",
            "That she has implemented and what is actually the problem?",
            "What one do we want to address here?",
            "The the problem is you could image."
        ],
        [
            "Do you have an ontology search engine?",
            "We all know love the linked open vocabularies search engine and you want to find concept in an ontology that would match your query term.",
            "And the problem is of course that these ontologies have different levels of hierarchies.",
            "They they have a different focus state.",
            "They use different ways of modeling, restrick, shun's and all kinds of things, and we're looking at the repository which only consists on ontology.",
            "So we're not looking at the data.",
            "We're not looking to instance data.",
            "We want to make those decisions based on the ontologies themselves.",
            "So of course the the problem is how do we rank these ontologies based on this search term and."
        ],
        [
            "So what we came up with is model which is called DW rank and we also implemented some top K filters on top of it to improve the results at the end."
        ],
        [
            "So I want to start with the DW rank, which means the dual walk and you will hear in a second why we call it dual walk and it consists of hop score which is the centrality of the concept and the authority score which represent the authorative stiffness of the ontology itself and both of those are computed at design time.",
            "So we build an index and that's based on your ontology repository.",
            "And so at runtime, of course, when you when you want to match, we take a linear model of the the two scores.",
            "The top score on the authority score and the text relevancy of the of the search term."
        ],
        [
            "What's the top score, the centrality of concept?",
            "It's pretty much let me just go to the back here.",
            "We don't have much time.",
            "It is pretty much pretty much of reverse page rank.",
            "So my student tried a lot of different graph algorithms to determine the centrality of concept in ontology and she kind of figured out after trying a lot of different algorithms that reverse page rank performs the best.",
            "And why is that the case?",
            "Because in an ontology, of course, a concept that is the subject of many relationships.",
            "Seems to be the the most relevant.",
            "The most central concept in that ontology, so we used a reverse page rank for the for the top score."
        ],
        [
            "For the authoritativeness of the ontology, that's pretty straightforward.",
            "We can use a page rank within the ontology repository, because obviously the ontologies that are imported by many other ontologies seem to be authorative ontologies in this corpus.",
            "So if you think about the photology, of course that is imported in many other ontologies, so it seems to be quite authoritative in specific ontology corpus.",
            "So then we took a linear model to combine those scores and the text relevancy and we end up with a score for the for the concept and in this case for this person would be 0.47.",
            "Out of this calculation, here is very simple.",
            "There's a weighting factor and the text relevancy, and so what's the difference of this of this initial?"
        ],
        [
            "Do a walk ranking model.",
            "We look at Inter ontology links, which is the the reverse page rank, so that's within an ontology and we look at Inter ontology links.",
            "How this ontology is connected to ontologies and we calculate those in dependently on the ontology and in the case of the hub scores.",
            "So that's within the ontology and we do the authority score based on the corpus that you're currently looking at."
        ],
        [
            "So we also implemented some top K filters to improve the results.",
            "One of them was to get the intended type of the of the of the ontology description.",
            "So if you think about a label which is called name of the person, the intended type is the name and the context resource, which would be the person.",
            "So if someone is searching for person that probably that result wouldn't be the one you want us as a concept.",
            "We also implemented a distinct resource filter which pretty much removes duplicate classes.",
            "So if you have a Class A concept importing, extending another concept.",
            "Often the descriptions is this is pretty much the same, so we took a.",
            "We took the one which is.",
            "Is there more?",
            "I mean with which has less of those additional, so we just took the additional descriptions and separate those two descriptions.",
            "So there's obviously the top class.",
            "And then there could be a subclass which just changes a couple of words in the description."
        ],
        [
            "So we did an evaluation of these two algorithms and it's coming here.",
            "So we evaluated against the benchmark which my student developed earlier.",
            "A couple of machine learning approaches, and some of the state of the art in ontology ranking, and you can see on the very top right corner that this model outperformed the other models.",
            "And we also."
        ],
        [
            "See there's on based on the queries we got this.",
            "We got those queries from the the linked open vocabulary.",
            "If we had a search log and we took the 10 most popular search queries on on the search log.",
            "And the the gold standard is coming from this ontology benchmark that must've been also developed earlier and presented at our assembly, actually.",
            "And you can see that the WDW rank also.",
            "Outperformed all the best algorithms of any of those queries, so we took the previous ones and the best one for each query.",
            "And then you saw that in most cases it even outperforms the best one of the other ones.",
            "And you can see also that there the filter the top K field on top.",
            "Also edit some precision."
        ],
        [
            "So why do we need some learning to rank some of the sum?"
        ],
        [
            "The queries you saw some variety in the in the results depending on the query, so none of the algorithms outperformed algorithms all the time and including DVD rank here.",
            "There were some queries where we could improve the model, of course.",
            "So we we try to change the weights of this linear model and you pretty much have to change it all the time for every query.",
            "So what we try to do is to learn those weights based on ontology corpus.",
            "So we use their learning to rank model to learn those those weights and what are the feature sets is obviously the hop score and the authority score and then the Max and min because the other one is 1 minus the top score and the text relevancy and the target features.",
            "Obviously the development score.",
            "And we looked at the the training Taylor was the CBR bench that my student developed earlier, which is which is an apology benchmark of like 1000."
        ],
        [
            "And 100 ontologies.",
            "And the human annotated rankings of those search terms.",
            "And that was our training data, including obviously the test set and the validation set we're trying."
        ],
        [
            "To solve and the metrics were obviously precision at 10 and average precision attendant normalized discounted gain.",
            "And we use the Lambda Mart learning to rank model so that is available as open source and we perform the leave one out cross validation and we optimize for the normalized discounted gain.",
            "And if you look at the frame."
        ],
        [
            "Overview imagist OK my son was busy so the idea is that you start with the ontology benchmark the CBR bench which is our training data set our gold standard and we generate the training data set.",
            "From that we use the ontology collection that you're currently using for your.",
            "For your query matching of course and built the authorities and the score and the centrality score, and we then build the ranking model.",
            "At runtime we use the ranking model.",
            "We take the candidate result sets from the query based on the text relevancy and feeding the learning model and obviously get.",
            "Then that's OK results.",
            "And we looked at the performance again of this of this improvement too."
        ],
        [
            "DW rank model and you can see here the comparison of the one with the fixed weight model and the one with the learning to rank model and you can see with few exceptions it outperforms significantly our old model and our old model already outperformed the state of the art, so that was pretty much where my student got too.",
            "She wasn't meant to actually look then at instance data as well, but there was beyond her PhD, so you obviously could improve the results if you include instance data, because then you have more information of what concepts and potentially also properties people use, but that was purely on the ontology data, so it's purely what you would for example have in the linked open vocabulary.",
            "That's pretty much it."
        ],
        [
            "It's time for questions."
        ],
        [
            "I was curious in the learning to rank model what exactly you're learning.",
            "You're learning the weights on what exactly you were learning the weight in the linear model.",
            "Learning.",
            "We're learning this weights.",
            "We set them at 0.5 so 50.",
            "At the bottom there and learning those those weights based on the on the query.",
            "And if you were going to do instances, what would you?",
            "What would you do here with using the instances so we're not using instances, but if you would use instances?",
            "You could use that.",
            "I mean, you could probably use the popularity of a concept in terms of how many instances there are in terms of informing these these weights, I would assume more questions and so where would you use this?",
            "I mean, so can you give the bigger picture in terms of what kinds of applications you reduce this work well?",
            "The main application I guess would be the linked open Cavaleri we were in contact with Pierre.",
            "I don't know if he's in the audience, but I guess they could implement this.",
            "I guess maybe they implemented some of these aspects.",
            "They obviously have other.",
            "Factors that come into their ranking because they can see also the search log and they can see how that you know what people clicked on.",
            "I mean, the kinds of things that Google does, but that would be the the implementation of that would be in the linked up with calories where people are searching for a domain expert who wants to build an ontology and finding what's out there in the world.",
            "Of course I guess in terms of the longer term that could inform search engine that also uses the instant state of course and then.",
            "We're back to smuggle, for example, where you could implement that as well.",
            "You said when you compute this Hub scored at somebody, something having outgoing link is much more important than something I've been going.",
            "I don't quite get it.",
            "In some cases it's arbitrary.",
            "If I use equivalent class, what is the subject?",
            "What is the object is just a random decision in other cases of ability ontology with scores?",
            "For example, if I use narrower or broader as a predicate, it also changes the game, right?",
            "So why is it that the subject is more important than the object?",
            "Well, I guess I miss my student.",
            "Tried this in terms of the equivalence relationship.",
            "She would have materialized both both directions in terms of costs.",
            "Yeah, that's a design designers choice.",
            "I guess sometimes in ontologies when you decide the direction of the of the link.",
            "He also sometimes have the users in mind, so you might not.",
            "You know sometimes in an apology don't have the inverse off because you just decide that in this specific use case that should be the subject in most cases, because that's the one you first have.",
            "So I guess there's an intention by the ontology engineer sometimes as well that.",
            "The direction is from, you know, the more central thing which is the subject to the less central one, which is the object.",
            "And this is what the observations we had.",
            "So I'm just curious, did you test this or just heuristic that you replied?",
            "We tested that.",
            "Yeah, so we use the other one.",
            "So obviously the page rank and that one outperformed on the corpus quite significantly.",
            "Maybe a small question that was triggered by a previous one you used your algorithm to rank concepts.",
            "Would it also be possible to use the algorithm to rank ontologies as a whole and use it to evaluate the quality of the ontology for a certain sector while the forgiveness is basically a ranking of the ontology in that specific corpus?",
            "The ones we used was the CBR bench, which was just a crawl of the ontologies on the web from.",
            "2 three years ago, which was inside pretty similar to the Manchester one.",
            "So that would be there for Discord would be ranking for the ontologies pretty much so you could just rank deontologist based on that.",
            "And I guess we have that somewhere.",
            "We if you're interested we can give it to you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in in order to start with the talk, I start with the foundations of this algorithms and then at the end I will talk about the learning part of this of this work.",
                    "label": 0
                },
                {
                    "sent": "So I want to start with the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concept search and ranking.",
                    "label": 0
                },
                {
                    "sent": "That she has implemented and what is actually the problem?",
                    "label": 0
                },
                {
                    "sent": "What one do we want to address here?",
                    "label": 0
                },
                {
                    "sent": "The the problem is you could image.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you have an ontology search engine?",
                    "label": 0
                },
                {
                    "sent": "We all know love the linked open vocabularies search engine and you want to find concept in an ontology that would match your query term.",
                    "label": 0
                },
                {
                    "sent": "And the problem is of course that these ontologies have different levels of hierarchies.",
                    "label": 1
                },
                {
                    "sent": "They they have a different focus state.",
                    "label": 0
                },
                {
                    "sent": "They use different ways of modeling, restrick, shun's and all kinds of things, and we're looking at the repository which only consists on ontology.",
                    "label": 0
                },
                {
                    "sent": "So we're not looking at the data.",
                    "label": 0
                },
                {
                    "sent": "We're not looking to instance data.",
                    "label": 0
                },
                {
                    "sent": "We want to make those decisions based on the ontologies themselves.",
                    "label": 1
                },
                {
                    "sent": "So of course the the problem is how do we rank these ontologies based on this search term and.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we came up with is model which is called DW rank and we also implemented some top K filters on top of it to improve the results at the end.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to start with the DW rank, which means the dual walk and you will hear in a second why we call it dual walk and it consists of hop score which is the centrality of the concept and the authority score which represent the authorative stiffness of the ontology itself and both of those are computed at design time.",
                    "label": 1
                },
                {
                    "sent": "So we build an index and that's based on your ontology repository.",
                    "label": 0
                },
                {
                    "sent": "And so at runtime, of course, when you when you want to match, we take a linear model of the the two scores.",
                    "label": 0
                },
                {
                    "sent": "The top score on the authority score and the text relevancy of the of the search term.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's the top score, the centrality of concept?",
                    "label": 1
                },
                {
                    "sent": "It's pretty much let me just go to the back here.",
                    "label": 0
                },
                {
                    "sent": "We don't have much time.",
                    "label": 0
                },
                {
                    "sent": "It is pretty much pretty much of reverse page rank.",
                    "label": 0
                },
                {
                    "sent": "So my student tried a lot of different graph algorithms to determine the centrality of concept in ontology and she kind of figured out after trying a lot of different algorithms that reverse page rank performs the best.",
                    "label": 0
                },
                {
                    "sent": "And why is that the case?",
                    "label": 1
                },
                {
                    "sent": "Because in an ontology, of course, a concept that is the subject of many relationships.",
                    "label": 1
                },
                {
                    "sent": "Seems to be the the most relevant.",
                    "label": 0
                },
                {
                    "sent": "The most central concept in that ontology, so we used a reverse page rank for the for the top score.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the authoritativeness of the ontology, that's pretty straightforward.",
                    "label": 1
                },
                {
                    "sent": "We can use a page rank within the ontology repository, because obviously the ontologies that are imported by many other ontologies seem to be authorative ontologies in this corpus.",
                    "label": 0
                },
                {
                    "sent": "So if you think about the photology, of course that is imported in many other ontologies, so it seems to be quite authoritative in specific ontology corpus.",
                    "label": 0
                },
                {
                    "sent": "So then we took a linear model to combine those scores and the text relevancy and we end up with a score for the for the concept and in this case for this person would be 0.47.",
                    "label": 0
                },
                {
                    "sent": "Out of this calculation, here is very simple.",
                    "label": 0
                },
                {
                    "sent": "There's a weighting factor and the text relevancy, and so what's the difference of this of this initial?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do a walk ranking model.",
                    "label": 0
                },
                {
                    "sent": "We look at Inter ontology links, which is the the reverse page rank, so that's within an ontology and we look at Inter ontology links.",
                    "label": 0
                },
                {
                    "sent": "How this ontology is connected to ontologies and we calculate those in dependently on the ontology and in the case of the hub scores.",
                    "label": 1
                },
                {
                    "sent": "So that's within the ontology and we do the authority score based on the corpus that you're currently looking at.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we also implemented some top K filters to improve the results.",
                    "label": 0
                },
                {
                    "sent": "One of them was to get the intended type of the of the of the ontology description.",
                    "label": 0
                },
                {
                    "sent": "So if you think about a label which is called name of the person, the intended type is the name and the context resource, which would be the person.",
                    "label": 1
                },
                {
                    "sent": "So if someone is searching for person that probably that result wouldn't be the one you want us as a concept.",
                    "label": 1
                },
                {
                    "sent": "We also implemented a distinct resource filter which pretty much removes duplicate classes.",
                    "label": 0
                },
                {
                    "sent": "So if you have a Class A concept importing, extending another concept.",
                    "label": 0
                },
                {
                    "sent": "Often the descriptions is this is pretty much the same, so we took a.",
                    "label": 0
                },
                {
                    "sent": "We took the one which is.",
                    "label": 0
                },
                {
                    "sent": "Is there more?",
                    "label": 0
                },
                {
                    "sent": "I mean with which has less of those additional, so we just took the additional descriptions and separate those two descriptions.",
                    "label": 0
                },
                {
                    "sent": "So there's obviously the top class.",
                    "label": 0
                },
                {
                    "sent": "And then there could be a subclass which just changes a couple of words in the description.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did an evaluation of these two algorithms and it's coming here.",
                    "label": 0
                },
                {
                    "sent": "So we evaluated against the benchmark which my student developed earlier.",
                    "label": 0
                },
                {
                    "sent": "A couple of machine learning approaches, and some of the state of the art in ontology ranking, and you can see on the very top right corner that this model outperformed the other models.",
                    "label": 0
                },
                {
                    "sent": "And we also.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See there's on based on the queries we got this.",
                    "label": 0
                },
                {
                    "sent": "We got those queries from the the linked open vocabulary.",
                    "label": 0
                },
                {
                    "sent": "If we had a search log and we took the 10 most popular search queries on on the search log.",
                    "label": 0
                },
                {
                    "sent": "And the the gold standard is coming from this ontology benchmark that must've been also developed earlier and presented at our assembly, actually.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the WDW rank also.",
                    "label": 0
                },
                {
                    "sent": "Outperformed all the best algorithms of any of those queries, so we took the previous ones and the best one for each query.",
                    "label": 0
                },
                {
                    "sent": "And then you saw that in most cases it even outperforms the best one of the other ones.",
                    "label": 0
                },
                {
                    "sent": "And you can see also that there the filter the top K field on top.",
                    "label": 0
                },
                {
                    "sent": "Also edit some precision.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So why do we need some learning to rank some of the sum?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The queries you saw some variety in the in the results depending on the query, so none of the algorithms outperformed algorithms all the time and including DVD rank here.",
                    "label": 1
                },
                {
                    "sent": "There were some queries where we could improve the model, of course.",
                    "label": 1
                },
                {
                    "sent": "So we we try to change the weights of this linear model and you pretty much have to change it all the time for every query.",
                    "label": 0
                },
                {
                    "sent": "So what we try to do is to learn those weights based on ontology corpus.",
                    "label": 1
                },
                {
                    "sent": "So we use their learning to rank model to learn those those weights and what are the feature sets is obviously the hop score and the authority score and then the Max and min because the other one is 1 minus the top score and the text relevancy and the target features.",
                    "label": 0
                },
                {
                    "sent": "Obviously the development score.",
                    "label": 0
                },
                {
                    "sent": "And we looked at the the training Taylor was the CBR bench that my student developed earlier, which is which is an apology benchmark of like 1000.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And 100 ontologies.",
                    "label": 0
                },
                {
                    "sent": "And the human annotated rankings of those search terms.",
                    "label": 0
                },
                {
                    "sent": "And that was our training data, including obviously the test set and the validation set we're trying.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve and the metrics were obviously precision at 10 and average precision attendant normalized discounted gain.",
                    "label": 0
                },
                {
                    "sent": "And we use the Lambda Mart learning to rank model so that is available as open source and we perform the leave one out cross validation and we optimize for the normalized discounted gain.",
                    "label": 1
                },
                {
                    "sent": "And if you look at the frame.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overview imagist OK my son was busy so the idea is that you start with the ontology benchmark the CBR bench which is our training data set our gold standard and we generate the training data set.",
                    "label": 0
                },
                {
                    "sent": "From that we use the ontology collection that you're currently using for your.",
                    "label": 1
                },
                {
                    "sent": "For your query matching of course and built the authorities and the score and the centrality score, and we then build the ranking model.",
                    "label": 1
                },
                {
                    "sent": "At runtime we use the ranking model.",
                    "label": 1
                },
                {
                    "sent": "We take the candidate result sets from the query based on the text relevancy and feeding the learning model and obviously get.",
                    "label": 0
                },
                {
                    "sent": "Then that's OK results.",
                    "label": 0
                },
                {
                    "sent": "And we looked at the performance again of this of this improvement too.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "DW rank model and you can see here the comparison of the one with the fixed weight model and the one with the learning to rank model and you can see with few exceptions it outperforms significantly our old model and our old model already outperformed the state of the art, so that was pretty much where my student got too.",
                    "label": 1
                },
                {
                    "sent": "She wasn't meant to actually look then at instance data as well, but there was beyond her PhD, so you obviously could improve the results if you include instance data, because then you have more information of what concepts and potentially also properties people use, but that was purely on the ontology data, so it's purely what you would for example have in the linked open vocabulary.",
                    "label": 0
                },
                {
                    "sent": "That's pretty much it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's time for questions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was curious in the learning to rank model what exactly you're learning.",
                    "label": 0
                },
                {
                    "sent": "You're learning the weights on what exactly you were learning the weight in the linear model.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "We're learning this weights.",
                    "label": 0
                },
                {
                    "sent": "We set them at 0.5 so 50.",
                    "label": 0
                },
                {
                    "sent": "At the bottom there and learning those those weights based on the on the query.",
                    "label": 0
                },
                {
                    "sent": "And if you were going to do instances, what would you?",
                    "label": 0
                },
                {
                    "sent": "What would you do here with using the instances so we're not using instances, but if you would use instances?",
                    "label": 0
                },
                {
                    "sent": "You could use that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could probably use the popularity of a concept in terms of how many instances there are in terms of informing these these weights, I would assume more questions and so where would you use this?",
                    "label": 0
                },
                {
                    "sent": "I mean, so can you give the bigger picture in terms of what kinds of applications you reduce this work well?",
                    "label": 0
                },
                {
                    "sent": "The main application I guess would be the linked open Cavaleri we were in contact with Pierre.",
                    "label": 0
                },
                {
                    "sent": "I don't know if he's in the audience, but I guess they could implement this.",
                    "label": 0
                },
                {
                    "sent": "I guess maybe they implemented some of these aspects.",
                    "label": 0
                },
                {
                    "sent": "They obviously have other.",
                    "label": 0
                },
                {
                    "sent": "Factors that come into their ranking because they can see also the search log and they can see how that you know what people clicked on.",
                    "label": 0
                },
                {
                    "sent": "I mean, the kinds of things that Google does, but that would be the the implementation of that would be in the linked up with calories where people are searching for a domain expert who wants to build an ontology and finding what's out there in the world.",
                    "label": 0
                },
                {
                    "sent": "Of course I guess in terms of the longer term that could inform search engine that also uses the instant state of course and then.",
                    "label": 0
                },
                {
                    "sent": "We're back to smuggle, for example, where you could implement that as well.",
                    "label": 0
                },
                {
                    "sent": "You said when you compute this Hub scored at somebody, something having outgoing link is much more important than something I've been going.",
                    "label": 0
                },
                {
                    "sent": "I don't quite get it.",
                    "label": 0
                },
                {
                    "sent": "In some cases it's arbitrary.",
                    "label": 0
                },
                {
                    "sent": "If I use equivalent class, what is the subject?",
                    "label": 0
                },
                {
                    "sent": "What is the object is just a random decision in other cases of ability ontology with scores?",
                    "label": 0
                },
                {
                    "sent": "For example, if I use narrower or broader as a predicate, it also changes the game, right?",
                    "label": 0
                },
                {
                    "sent": "So why is it that the subject is more important than the object?",
                    "label": 0
                },
                {
                    "sent": "Well, I guess I miss my student.",
                    "label": 0
                },
                {
                    "sent": "Tried this in terms of the equivalence relationship.",
                    "label": 0
                },
                {
                    "sent": "She would have materialized both both directions in terms of costs.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a design designers choice.",
                    "label": 0
                },
                {
                    "sent": "I guess sometimes in ontologies when you decide the direction of the of the link.",
                    "label": 0
                },
                {
                    "sent": "He also sometimes have the users in mind, so you might not.",
                    "label": 0
                },
                {
                    "sent": "You know sometimes in an apology don't have the inverse off because you just decide that in this specific use case that should be the subject in most cases, because that's the one you first have.",
                    "label": 0
                },
                {
                    "sent": "So I guess there's an intention by the ontology engineer sometimes as well that.",
                    "label": 0
                },
                {
                    "sent": "The direction is from, you know, the more central thing which is the subject to the less central one, which is the object.",
                    "label": 0
                },
                {
                    "sent": "And this is what the observations we had.",
                    "label": 0
                },
                {
                    "sent": "So I'm just curious, did you test this or just heuristic that you replied?",
                    "label": 0
                },
                {
                    "sent": "We tested that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we use the other one.",
                    "label": 0
                },
                {
                    "sent": "So obviously the page rank and that one outperformed on the corpus quite significantly.",
                    "label": 0
                },
                {
                    "sent": "Maybe a small question that was triggered by a previous one you used your algorithm to rank concepts.",
                    "label": 0
                },
                {
                    "sent": "Would it also be possible to use the algorithm to rank ontologies as a whole and use it to evaluate the quality of the ontology for a certain sector while the forgiveness is basically a ranking of the ontology in that specific corpus?",
                    "label": 0
                },
                {
                    "sent": "The ones we used was the CBR bench, which was just a crawl of the ontologies on the web from.",
                    "label": 0
                },
                {
                    "sent": "2 three years ago, which was inside pretty similar to the Manchester one.",
                    "label": 0
                },
                {
                    "sent": "So that would be there for Discord would be ranking for the ontologies pretty much so you could just rank deontologist based on that.",
                    "label": 0
                },
                {
                    "sent": "And I guess we have that somewhere.",
                    "label": 0
                },
                {
                    "sent": "We if you're interested we can give it to you.",
                    "label": 0
                }
            ]
        }
    }
}