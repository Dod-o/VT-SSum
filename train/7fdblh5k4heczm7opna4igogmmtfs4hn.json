{
    "id": "7fdblh5k4heczm7opna4igogmmtfs4hn",
    "title": "Deep Reinforcement Learning",
    "info": {
        "author": [
            "Hado van Hasselt, Google, Inc."
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_van_hasselt_deep_reinforcement/",
    "segmentation": [
        [
            "OK.",
            "I'm glad you're made it in in the morning.",
            "Thanks for coming and thank you for inviting me to talk here.",
            "So yes, I'll be talking about deep reinforcement learning.",
            "And.",
            "To talk about that first wanted to step back and talk about the big picture.",
            "Why do I care about deep reinforcement learning?",
            "Maybe?",
            "Why you want to care about the reinforcement learning.",
            "That's of course up to you to decide."
        ],
        [
            "But I wanted to step way back and start.",
            "At the Industrial Revolution and what they sometimes called the machine age and think about what that was, what happened there and essentially one way to think about this is that this was a time in which we took manual solutions that we used to do with manual labor and then replace that with machines.",
            "This is where factories and such came into play.",
            "Steam engines and then later in the digital revolution more recently or the information age.",
            "Essentially we did something very similar where we now too.",
            "Repetitive mental solutions and we replace those with machines.",
            "So one way, one example of this is a Calculator where we know how to calculate things by hand.",
            "But you could also automate that process if it's formal enough, but it's an implementation of a solution.",
            "In both cases we do have to come up with the solution 1st and then we implement that solution.",
            "Now what I wanted and posit, which might seem obvious now, is that.",
            "The next step should perhaps be that we only specify a goal, and we leave the discovery of the solutions to the system.",
            "This is what learning is, and this is what you might call the AI revolution.",
            "This is very broad."
        ],
        [
            "Pause.",
            "Broad sense, so let's dive a little bit deeper into what Aion might mean.",
            "So back in the day when people started talking about artificial intelligence, a lot of the first approaches were symbolic, where we get them reason formally about things that might might happen, or things that might imply other things.",
            "And this is already an automated form of reason.",
            "You could come up with new conclusions based on the information that you put into the system.",
            "But the rules themselves were programmed in advance and static, and often there was a high level of hand pick, knowledge, formalism or level of abstraction.",
            "If you want to think about it that way, with which I mean you had to basically pick what the symbols were, what they meant, how they would interact with each other, and then you could drive things.",
            "But there was a lot of specialist knowledge in some sense involved.",
            "And one thing that happened with these systems is that in many of the cases they basically didn't scale that well.",
            "Too messy data as you would have in the real world and to uncertainty.",
            "From a different perspective, there is classical statistics.",
            "An in classical statistics we are able to deal with insurgency with messy data, but typically the classical goal in statistics is already in the name.",
            "It's to basically get statistics about your data.",
            "It's about analyzing and then this decisions were still made after the fact.",
            "So you would analyze something and then based on this analysis you would learn something about your system or whatever it is you're interested in, and then you could make decisions and even picking what to analyze in the 1st place is a decision you do.",
            "Up front.",
            "So there's something in here which you could already call learning.",
            "This is the automatic analysis of the data, but it's not completely end to end from observing something to making decisions in the world.",
            "So again, here are positive something and this is just to make you think if you agree with that phrase through a, I should learn to make decisions autonomously.",
            "This kind of makes sense in reinforcement learning context, of course, so this might not be a very surprising thing for me to say."
        ],
        [
            "So on that note, we go into reinforcement learning and I first just very briefly wanted to recap some things that you were covered yesterday, so that I'm sure that we're on the same page.",
            "Whenever you have any questions, just feel free to raise your hand and I'll try to look around sufficiently to spot you.",
            "Yeah, good.",
            "Just like we would you put on a control, both classes and stuff.",
            "Networks.",
            "So it depends.",
            "Depends a little bit on what you use for installation networks for an, But in some sense, maybe in district like if you consider it in a narrow sense, you could view the Bayesian approaches as the analysis, but then you still might come up with things.",
            "Visions yourself based on this analysis, there's a little bit of Gray area there, of course, because you also have Beijing decision theory, in which then decisions are kind of automatically determines and there's of course a lot of other Gray areas.",
            "For instance, in statistics, well known area is active learning, which is all about sampling, which you could say our decisions.",
            "The control problem is an interesting one as well, because sometimes when we say control.",
            "Especially in the more classic approaches, we basically assume we have full access to a system and we basically inspect the system and then learn what a good policy of controllers for the system.",
            "So there is not maybe a lot of learning happening there, it's more inference.",
            "In that sense, reinforcement learning is a little bit different because we definitely I'm learning in the name for a reason.",
            "We're definitely focused on the learning part.",
            "Thanks very good question."
        ],
        [
            "OK, so this is just to recap, what is reinforcement learning about?",
            "It's a framework for making decisions and this is maybe slightly slightly subtly different points than others have made before, because I really want to focus on it as a framework.",
            "In addition, there is a lot of algorithms that fall under the header of reinforcement learning, but to me reinforced learning is more than that.",
            "It's about the reasoning framework in which you can reason about what it even means to make decisions and why you would even want to make certain decisions.",
            "The general setup that we're thinking about is that there is an agent or an actor that interacts with an environment and sends actions out into the world.",
            "As you could see.",
            "And then observations come back in either passively.",
            "It just comes in at a certain decorate maybe, or maybe actively.",
            "You pull in observations whenever you have the resources to do so.",
            "And then in this talk I want to focus mostly on learning to act, which might seem obvious, especially given what I said before, but I wanted to contrast that to what Rich Sutton was saying yesterday, where you could also think about reinforcement learning as being the study of learning to predict.",
            "And maybe it's both.",
            "But in this talk I'm mostly going to focus on learning to act, so there's a certain goal here, which is to find a policy of behavior that is good for some certain goal that you want to specify.",
            "So each of these actions that the agent sends out can change the state of the world.",
            "And also can result in the reward.",
            "I deliberately did not put the reward in this little diagram.",
            "You've probably seen many versions of this diagram.",
            "I know there were like at least three different ones yesterday.",
            "And oftentimes you see the reward coming from the environment into the agents.",
            "And that's definitely a valid way to think about these things.",
            "If you have a certain task, it's easy to think about these things as generating the rewards that your agents shoot and optimize, but sometimes it's actually more natural to think of these rewards internal to the agents that the agent, like certain observations, observations, might in some sense carry the information that you need to infer the reward, but you know the world out there might not care about which things you prefer, so there might be many, many signals.",
            "In your observation that all might be taken to be rewards, maybe also different rewards by different agents, and maybe in that case reward lives within the Asians.",
            "Most of the algorithms that we talk about in reinforcement learning don't particularly care where the reward comes from.",
            "They consider it to be external to the learning algorithm, but it might still be internal to your agent.",
            "And then the goal is to pick one to optimize future rewards for some reward signal."
        ],
        [
            "So there's many examples we saw several yesterday.",
            "We'll see more today.",
            "As a reminder, things like video games have been done.",
            "Board games.",
            "Robotics is an application area of reinforcement learning recommender systems.",
            "There's many, many, many more, and there's more.",
            "Each day, more or less, and essentially this is because you could think about all problems that involve making decisions and or predictions about the future as falling under the head of the general header of reinforcement learning.",
            "So that makes it a very general field, and many of the algorithms are also intended to be very general purpose.",
            "It also means that you might actually be doing.",
            "Be doing what I would call reinforcement learning without necessarily realizing that you're doing reinforcement learning.",
            "But as long as you're making decisions, and these decisions might impact the world in some way, and especially if there's time dependencies and decision making in the same problem, then I would say this falls under the header of reinforcement learning."
        ],
        [
            "So now I want to talk about what approaches you could take to reinforcement learning.",
            "So the goal is to reiterate this.",
            "We've specified the goal as to learn a policy of behavior that optimizes something that we'll talk about more.",
            "That means that there's at least three possibilities and potentially more, but these are the main areas that you could think about.",
            "One is, you could learn a policy directly.",
            "An Peterbilt talked about this yesterday at quite some length.",
            "I'll touch panel briefly again today, just to remind you how you could do these things.",
            "Pretty, and this is also something that was talked about at length yesterday for instance, for instance by rich, but also by Joelle earlier.",
            "Excuse me, you could also learn values and these values could capture what you predict the reward will be.",
            "And then if you have these values, it actually becomes fairly easy to also infer policy.",
            "Essentially, if you learn action values, you can just infer a policy by inspection.",
            "You pick the highest valued action in each state that you're in.",
            "A different approach which we haven't talked about much yet as much yet at least, is to learn a model of your environment.",
            "Maybe it doesn't have to be precise model, but maybe it's some abstraction of the environments of capacity.",
            "Want to solve?",
            "And then maybe you can infer a policy by planning.",
            "This is actually the more classical approach where maybe the most used way to solve such systems is by actually not learning the model, but constructing the model and then doing planning on top of that and planning and search are very rich research fields.",
            "And still very active, and there's many interesting questions there.",
            "Even if you have a model, of course it becomes extra.",
            "Maybe interesting, but also complex if you have to learn a model as well, and maybe you cannot trust the model fully to be exactly what you want the task to be.",
            "So that means concretely, that you have to have one of these components in your agents.",
            "You either have to have a policy explicitly like a function that Maps your states, your current state of your system to an action you have to have a value function, or you have to have a model.",
            "Maybe there's alternatives, but these are the main main components that we typically consider.",
            "You could also have combinations of these, so it says at least one of these components, just to give you an example, if you have both a policy and value function, this is what we often call in.",
            "Actor critic system.",
            "The actor is on the policy.",
            "The critic is knowing your value function and we've seen examples of this yesterday.",
            "Also briefly give you more today."
        ],
        [
            "So just recapping some notation, this is commonly used notation for some of these things.",
            "Actually for models, models are more out there.",
            "Sometimes people just use F as a function for the model Rich talked about Q value.",
            "Sometimes you see a capital, QD says more.",
            "More more often you see lower case Q.",
            "But this is the notation that are mostly stick to.",
            "In these slides there was a question, yes.",
            "How?",
            "American Medical.",
            "Yes, it's very good question if I let me let me phrase it so that you can check whether items to correctly question is how do you pick which of these things that you learn?",
            "And that's essentially an open question.",
            "An one thing that we've noticed, let me just give you a little bit of a teaser as well to the deep part.",
            "Until recently, a lot of reinforcement learning was done, not in the context of deep neural networks, and then it was kind of common knowledge that the model based approach was definitely the most data efficient one.",
            "It takes maybe a lot of computes to learn a model, but it's very data efficient because you can put all the information that you get from the world into your model, and then you can just plan with it.",
            "This is where the computer comes in.",
            "The planning might take you some computer.",
            "Somehow we haven't really been able to replicate that yet with the deep neural networks and I'll talk about reasons for that as well.",
            "Both the policy to direct policy learning approaches and Peterbilt talks about this quite a bit yesterday are very successful, but also the value based approaches are very successful.",
            "It might be a little bit problem dependence.",
            "There might even be these things might be more related than we realized in the past, and there's some recent work that showed that these things maybe maybe are quite closely related, depending on how specifically you implement them.",
            "But it's a very good question, and it's basically one of the main topics that we are researching to figure out when to use which method.",
            "Fortunately, there all the methods that I'll talk about are typically quite general purpose, so even if you pick the wrong one, it doesn't mean that it won't work.",
            "It might just be maybe slightly better if you use a different approach.",
            "So all of these components are essentially functions an we need to learn represents and learn these functions.",
            "Whichever components you choose to put in there."
        ],
        [
            "And this is where deep learning then comes in.",
            "So now to switch to deep reinforcement learning.",
            "This is essentially the study of using deep learning to learn policies, values and or models to use in the reinforcement learning domain.",
            "And again, this is actually fairly broad area, and as I said before, you might actually be doing reinforcement, or at least what I would call reinforcement learning without necessarily thinking about reinforcement learning or using the Canonical algorithms that we think of.",
            "When we say reinforcement thing.",
            "As long as you're using deep learning and somehow you're making predictions about the future and making decisions that might optimize something, you could say this falls under the general framework of reinforcement learning, and then it might be useful to also figure out what has been done.",
            "In this field.",
            "Or something?",
            "Something beeping.",
            "So to recap, reinforcement learning provides a framework for making decisions and predictions which would get upset if I didn't say that and then deep learning provides maybe a toolbox.",
            "This is one way to think about it.",
            "To learn these components.",
            "And together this is sometimes positive as something to think about.",
            "I'm not going to commit.",
            "I put a question Mark.",
            "Is this sufficient?",
            "Maybe, maybe not.",
            "I haven't talked about a lot of important details right?",
            "I haven't talked about, say, memory.",
            "I haven't talked about knowledge, maybe more generally, but these can fit within the under these headers.",
            "And then the question is, is this enough for AI?",
            "I'm not going to commit, but I encourage you to think about that, especially after all of these summer schools have concluded.",
            "But concretely, we're just going to do our L, where the components are now deep neural networks.",
            "So now basically my my talk is done and you can all go do something else.",
            "Well it's slightly more slightly more settlement, slightly harder."
        ],
        [
            "And I'm going to dive a little bit in some into depth into some of the methods that have been done and partially to make you aware of what has been done.",
            "But also partially, to make you aware of some of these problems and issues and interesting research questions that pop up if you when you combine deep learning with reinforcement learning to start off, I'm going to talk about deep."
        ],
        [
            "Networks.",
            "And to do that first, I'll quickly recap what Q learning is.",
            "So this is the bellman.",
            "Optimality equation.",
            "Which was raised by Richard Bellman in the 50s last century.",
            "And this is an equality.",
            "So this is.",
            "This is, essentially you could say definition.",
            "Q star here represents your optimal value function.",
            "What is optimal mean?",
            "This is the value of the policy that will get you the most rewards in this system.",
            "There is an expectation here.",
            "Any expectation basically spends any randomness that might occur in your reward function and in the transition to the next state, which is called S T + 1 there.",
            "This assumes that this is a Markov decision process.",
            "It assumes that all the information that you need is contained in the states.",
            "Joel talks a little bit about what that means in practice, and also like formally what that means.",
            "But if this is Mark decision process, this is an equality and you could also use this.",
            "You could do dynamic programming to solve for this optimal value function if you don't know it's in advance, and essentially when your quality starts to hold, then you know that these must be the optimal values.",
            "This only works.",
            "You can only do that if you have access to the model.",
            "This is exactly what dynamic programming does, but we can turn this into an algorithm.",
            "A temporal difference algorithm where essentially we just sample this thing within the expectation and then we make a small step.",
            "So the formulation I've written down here is the tabular formulation, so you can just think of this as there is an entry in a table for each state and action separately, and we just take a small step whenever we enter this state and take that action towards this target.",
            "The target here being the reward plus the discounted value of the next state.",
            "But the value of the next state here is defined as taking the maximum value over all of the Action values index state.",
            "This follows from the optimality equation up there.",
            "It also makes a little bit intuitive sense if you think about it.",
            "This way, we're trying to approximate the value of what would happen if I take this action, and then I take greedy actions with respect to my current values.",
            "These taking the greedy actions with respect to my current values is sometimes called policy improvements, because this should give you a better policy than if you, for instance, take random actions.",
            "And then the learning of the values here is sometimes called policy evaluation in Q learning algorithm, these things are wrapped into one updates where you're saying she doing evaluation and improvement more or less in the in the same step.",
            "This is very similar to value iteration in dynamic programming.",
            "So now we have a sample based algorithm that can learn these values and these will give you approximations to the optimal values and you might not want to wait all the way until you get the optimal values.",
            "It might be good enough to be good enough if you have an estimate of these optimal values.",
            "This might already give you a good policy.",
            "This is an off policy algorithm, with which we mean that it actually learns about a policy that might differ from the policy that you're following.",
            "And this is what I meant when I said you're learning about the greedy policy with respect to your current values.",
            "The greedy policy is just a policy that picks the highest value action in every state that you bump into.",
            "So the greedy policy with respect to your current value just uses your current value estimates to do that.",
            "What's nice about this is that it doesn't really matter what data you throw at it.",
            "That's not completely true, but it's it can learn about this even if you explore, and that's very valuable, because it means if you, for instance have a lot of data, you might be able just to go through that with this algorithm and still learn about the optimal values, even if you didn't follow the optimal policy while you were collecting the data.",
            "An off policy reinforcement is also very rich topic with a lot of application areas, yeah?",
            "Reply yes.",
            "Sure.",
            "You just like to find things random variables because we're sampling, so this is no longer for any, says an action.",
            "This is an update specifically for the state in action that you happened to be in.",
            "So this is this is maybe I shouldn't have used inequality there actually, but it does still hold because now it's no longer.",
            "It's no longer the same function because you can see all the way at the left it says Q T + 1.",
            "So this is essentially the values after the update.",
            "So in that sense it's well defined, but it might be a little bit confusing the way it's written down.",
            "Good question, thanks, yes.",
            "Is there a reason that there's a discount factor in the second equation but not the first one?",
            "Yes, it's because I forgot to put the disk on factor in the first equation.",
            "Sorry, very good question.",
            "Actually deliberately first didn't have discount factors anywhere because I just wanted to basically defer talking about those, but then because they were already covered yesterday, I start plugging them in.",
            "I missed one, sorry.",
            "So there's no reason that there's no discount factor in the first one.",
            "Good question, thanks.",
            "Yeah.",
            "Kind of.",
            "Yeah, yeah.",
            "Like if I collect random data, can I do like efficient cooling right?",
            "Yeah.",
            "So I have a.",
            "This is very good question.",
            "So the question is.",
            "Essentially, do I need to know anything about the data time collecting in order to be able to do Q learning?",
            "Do I need the order?",
            "Maybe some constraints on the data?",
            "Can you just do anything or?",
            "Going to give 2 answers for that first answer is the theoretical one which holds for the tabular case, and in this case it's actually sufficient that you cover each station action pair infinitely often in the limit.",
            "And then you will learn these optimal values.",
            "That's a pretty strong result because it means you basically don't care at all what your data distribution is, as long as it covers everything.",
            "This only works in the tabular case, though, because if you're going to do function approximations, you're going to generalize.",
            "This is the whole point of function approximation, and that means that if you're going to sample certain states actions more than others, you're essentially going to weigh the approximation error that you're going to end up with with your data distribution.",
            "So if we're going to do deep learning with reinforcement learning, it's actually very important.",
            "The sample data in specific ways and I'll have some concrete examples of that later on.",
            "Very good question."
        ],
        [
            "OK, so now this is just to reiterate, we now have.",
            "A way to approximate the optimal values and just to point out what might be obvious but just good to be explicit about this we can now ascentia we get an optimal policy if we have these optimal values by simply selecting the highest valued action in each and every state.",
            "Actually, this might not be completely obvious immediately, but it is through an.",
            "It's good, good too.",
            "If you're interested in this and learning more about this is always good to refer to the classics, so this doesn't embarrass or textbook gives a very good introduction on this."
        ],
        [
            "OK, so we were talking already about deep reinforcement learning, so whereas the deep full here here it is deep here just means you have a multi layer neural network that represents this Q function.",
            "So my notation here now is that there is a Q value which takes space and an action an some parameters W. These parameters are just all of the weights in your neural network.",
            "I've just condensed them into a single bullet variable here vector which contains all your parameters and then one way to do the updates.",
            "the Q Learning updates is essentially to change these parameters, so I could Delta W there.",
            "This means I'm going to change my parameters.",
            "And I put inequality, but maybe it's better just to think of this as kind of proportional to the quantity here at the right hand side.",
            "El faces stepsize here, so this is like a small number.",
            "We're going to take a small step.",
            "This is important.",
            "Because the function that we're using is nonlinear, and then a big step might actually ruin your function.",
            "It's also important because the data coming in might have noise, so you want to average out this noise and therefore you want to not use too big a step size.",
            "The.",
            "Quantity within the brackets is exactly the same as before, but now using my.",
            "New parametric Q function as my approximation.",
            "And what we do to update the parameters is simply to multiply this with the gradients of my queue function with respect to the parameters.",
            "This is a fairly simple update rule.",
            "There's something a little bit all about it, and I don't know if anybody notice if you notice something a little bit all, maybe you could raise your hand.",
            "Edit 80 Another the next month.",
            "Why's it evaluated at 80?",
            "This is because.",
            "This is the update to the weights after taking action 80 in state SD.",
            "And this is essentially the value that we're going to update, so we're not interested in updating only the value of the maximum action it states.",
            "In fact, we can't.",
            "Even because we haven't perhaps taken that action, so we might not know anything meaningful about this.",
            "So if we go back to the Bellman equation.",
            "This is conditional on the States and the action.",
            "So the target here is also considered a target for this state and action pair.",
            "This is why we're updating that value, but did not make it clear.",
            "A little bit.",
            "Yes.",
            "The maximum related with while they collect current rates and activates that that that you will have like why they make double next step inside like you.",
            "So why not take the next W on the next step is the question.",
            "Is that right?",
            "Why do we take the parents here?",
            "Well, this is an update for the W, so why don't we take the next W?",
            "None of the current one.",
            "So first thing to just just be very clear there's one set of weights here right now, so there's just a neural network with some weights, so the weights aren't dependent on states or anything like that.",
            "Just to be clear on that and then to be clear on what this update is about, this is about updating the weights, so in order to update the weights, the next weights aren't available yet.",
            "This is exactly what we're going to use to get the next weights, but having said that, there are actually algorithms that kind of do this where they first do kind of a partial update, then revisit the updates.",
            "And redo it.",
            "And sometimes you can get some gain from that.",
            "They're not very common in deep reinforcement learning because essentially evaluating one of these functions is not completely free.",
            "It's not very expensive.",
            "These networks aren't huge, but you don't want to do too many forwards if you can help it.",
            "Yeah.",
            "Why?",
            "Aren't the weights power parameterising the state action pair?",
            "So what's the state?",
            "Outside of.",
            "So yes, very good.",
            "The question is whether how are the states different from the weights in some sense.",
            "And so one way to think about this, I hope this clarifies, is that the States that I put in here as an input to the function is essentially your observation of the world.",
            "It could be a little bit more than that.",
            "It could also be the memory of your agents in addition to the observation of the world.",
            "If you will.",
            "And then the weights are just the parameters of your function.",
            "Yes.",
            "Are all the parameters of only all the layers into the same vector?",
            "In this notation, W here contains all the parameters lower layers just to simplify the notation, yes.",
            "Well, the way to update this, you still do backpropagation.",
            "This is the standard updates and actually.",
            "Let me."
        ],
        [
            "Maybe just flash this up.",
            "You could also just interpret this as a certain loss where basically we basically have squared loss where we have a certain target Y and we're updating our parents or Q function which are now denote it with a small Q.",
            "And I apologize, but I changed weights here to thetus.",
            "I should have kept that should've made at the W in literature is actually very common to use data for the ways you want to use, But these are just the parameters of the network, so this is the same queue function that we had before and then one way to interpret the exact same update that we had on the last slide is basically we're doing stochastic gradient descent with this loss, where you basically plug in a certain targets, which happens to depend on your estimated Q values as well, but otherwise it's just a target and we're just updating.",
            "We're regressing towards set targets.",
            "It was a question.",
            "Why aren't we considering the grading for respect to the parameters in the state of our bootstrapping on?",
            "That's an excellent question, and this is.",
            "This is actually what I was referring to when I was saying there's something slightly odd about this updates.",
            "Ann, there is a good reason for this.",
            "There's actually multiple good reasons for this one.",
            "One good reason for this is if we.",
            "If we get rid of the Max for a moment and we consider the actual action that you take in the next 8, then this becomes the source algorithm or basically just the state action version of TD.",
            "I say that just to get rid of the Max is just forget about the Max because it's nonlinear.",
            "For now.",
            "It's a very similar algorithm.",
            "Otherwise you could think about also taking the gradient with respect to the value of the next phase.",
            "But what would that mean?",
            "That would essentially mean that we're updating the value of the next states.",
            "To be a better target for my update of the value of of this state and action.",
            "That kind of violate causality because in some sense we want to know about the future and not necessarily we want not to make our predictions in the future to be better so that our predictions in the past were somehow better.",
            "Better already for these.",
            "There's also practical reason the algorithm that does take the greater respect to these problems in the next days actually tends to work less well in practice.",
            "So it seems that there are some something some truth to this causality argument at least.",
            "Yes.",
            "Can you explain why minimize this loss?",
            "Connect to the.",
            "Automatic.",
            "Connect to the original killer impression.",
            "Yes.",
            "So why does minimizing this loss connect into the Q learning equation?",
            "Well, we could think about.",
            "Just think about sampling this loss, which basically means you just have this one square term.",
            "You have y -- Q ^2 and then half of that.",
            "So if you want to minimize this, one way to minimize this is just to take the gradient with respect to this sort of gradient with respect to parameters of this loss and then take one step to minimize.",
            "Then you get exactly this update.",
            "But also, if you just think about what that means, it means that we're going to essentially move our action value function RQ function.",
            "Towards a situation where my Q function at the current state action pair in expectation equal the next reward and then next state value.",
            "If you manage to minimize this all the way to 0.",
            "Of course, if you're doing function approximation, this is typically not possible, But in this application might actually be able to reduce it all the way to 0."
        ],
        [
            "And if you would reduce it a little bit later, zero, essentially you get exactly that Bellman equation back.",
            "One way to see that if you could move the Q value there on the left hand side of the equality to the right hand side, it gets a minus term.",
            "Then of course and then you could square that term.",
            "And if that if that becomes zero, that means that these things must be equal.",
            "So if you're able to minimize the loss fully, you kind of get development automatically equation.",
            "I say kind of because of course if you're doing function proximation, you're not going to be able to reuse it all the way to 0, and you're not going to get the equality exactly, but you will still get something related.",
            "And this is this is the goal.",
            "Yes.",
            "You're asking about the specific in the nature paper.",
            "There's a slightly different update.",
            "So I'll talk about that in the ask again if it's not clear after a few slides, but I'll I'll go into that.",
            "So.",
            "Just one more thing I wanted to say about this is that the targets are now basically considered a constant as we discussed it just now, so we're not taking the greater with respect to the parameters into targets.",
            "This is very easy to implement in current a framework.",
            "For instance, in Tensorflow you might just put a stop gradients on the targets and then your gradients don't flow into there.",
            "And it's very easy to implement.",
            "So to continue indeed.",
            "So in the nature paper on the on these systems, there were two slight changes to this.",
            "Important changes, one is if you compare this first equation here to the second equation on the slide, there is a difference and the difference is in the parameters at the next state value there is a new quantity.",
            "Here this W which I made red with a minus in superscript.",
            "And what this W is, it's essentially a copy of your parameters, which is then kept fixed for awhile.",
            "So in practice, what happens every few 1000 steps we would just take the copy.",
            "We take the parameters from the network, we just copy that and then we can keep keeping fixed for a few 1000 steps and then after a few 1000 steps we repeat that again.",
            "So why would we do this?",
            "Well, if you don't do this.",
            "And I'll talk about this a little bit more.",
            "Your targets essentially become nonstationary and this.",
            "Turns out not to be a lot of work that well with these deep networks.",
            "The deep deep learning doesn't like it.",
            "A different thing that was also done.",
            "This relates to a question that was asked earlier about the data distribution was experience replay.",
            "So what we normally consider when we first explain reinforcement learning and what was also very commonly done and still is in some cases is to learn online.",
            "So you basically act you get your data and then you use that data to immediately update your Q values or your policy.",
            "And this is very appealing I think.",
            "But you could also of course store the data comes in and then just replay this to learn more efficiently from the data.",
            "Essentially you just collect some data as if you're building a datasets and then you're just learning more and more from this data set and in the queue and algorithm.",
            "This was done by creating a buffer of transitions.",
            "And the number of transitions in there is not maybe vitally important, but there were like a million transitions in this buffer and then just sample from this uniformly.",
            "This does mean that we're sampling from a policy that is a little bit outdated on average, but that's OK because we're doing Q learning, so we're learning off policy anyway.",
            "And what it does is it makes your data distribution more diverse, so it's a little bit closer to the typical IID assumption that is made when you do standard supervised learning.",
            "And again, it turns out that the deep learning algorithms.",
            "Maybe work better in this case and this is a general theme that you have to realize that many of these algorithms, especially in deep learning, were developed with certain in a certain context of classification or regression, and many of the things that work really well there might not necessarily transfer.",
            "If you're going to violate some of these assumptions.",
            "For instance, if your data distribution is no longer ID.",
            "So it's good to be aware of these things and maybe to also adapt things a little bit to make it more similar to the regime where we know these algorithms work well, yes.",
            "Sorry, how do we?",
            "How we?",
            "How we fill the replay buffer ascensi?",
            "What happens in the standard EQ?",
            "An algorithm is just.",
            "There's a.",
            "There's an agent taking actions in these Atari games.",
            "For instance, an every transition which is then a state and action a reward.",
            "Anna next state gets stored in this replay buffer.",
            "Of course, you could do this efficiently by just storing the sequence so you don't have to duplicate states both at the beginning at the end of a tuple.",
            "And then the learning was actually completely from the replay buffer, so the online data was put into the buffer, but the learning algorithm would read from the buff, wouldn't see, see the online data immediately, and on average each sample would be looked at more than once.",
            "So on average you would see sample four times.",
            "But there might be a few samples you never see.",
            "There might be a few samples which get replayed more often."
        ],
        [
            "And I'll talk about that a little bit more in a moment.",
            "But first I want to talk a little bit more about this target network.",
            "This is how we call that network, which depends on these copied over as weights.",
            "I copied this slide over from flat and I see he's using the Theta so don't get confused by sometimes it's a W, some places a theater.",
            "These are just the parameters of your neural network.",
            "It's the same thing.",
            "And you can also ignore the subscript I here for simplicity.",
            "So this is essentially just the same thing as we had before.",
            "Phrased here as a loss so it's like a square loss.",
            "We're taking the gradient with respect to current trends.",
            "So what's the intuition here?",
            "If you would update the parameters and you would have them both at the next state an at their current state, the same parameters, then even though you only want to update the value at this state, you kind of automatically also update the value at the next state, and this is especially true if you consider something like this and Atari game as depicted here at the bottom right.",
            "The frames might actually look very similar from one set index, because were acting at a fairly fast frequency, taking about 15 or 15.",
            "Actions per second.",
            "And that means that actually these values might be generalizing.",
            "It might be flowing over into each other, which means that if you update the value at this state almost automatically devalues the next day.",
            "It also changes, and this might make your learning quite unstable.",
            "So this is how intuition behind this target network where we keep these weights fix a little bit is that we update the value of Dicks this states.",
            "But in the values and exit isn't automatically changed and this just makes everything a little bit more similar to regression to things that we know it works.",
            "And it does really seem to help performance quite a bit.",
            "There was some point someone surprising later finding is that you have.",
            "If you have a bigger network with which we just mean you have more parameters in your network more hidden nodes.",
            "This is less of a problem and this kind of makes sense if you think about it in terms of generalization, because now the network has more capacity so it can also maybe see the differences between these very similar states more easily, and therefore if you change the value at this stage, it doesn't automatically change the values in next states as much if your network is smaller, it needs to generalize more across all of these States and actions, and therefore you have more of this effect.",
            "Yes.",
            "So typically what we do when we question was when we update the States.",
            "There's two answers to that.",
            "First, there's the online, the acting of the agents.",
            "Here, the state automatically changes whenever you pick an action, so the state is just whatever the system provides you.",
            "In this case, it's a frame of the Atari game.",
            "These are put into the replay buffer and what we typically do in the queue and set up is we sample sample mini batch from the replay buffer.",
            "We do an update with that mini batch.",
            "We toss all of them away and we sample a completely new mini batch.",
            "You could also keep the same States and do like a couple of updates and this might work well.",
            "I don't know that anybody is try.",
            "Yeah.",
            "Use of the target.",
            "Necessary in later place.",
            "So the question is, is the target network more important at the beginning when all your values are basically crap?",
            "Or maybe less important later when your values have improved?",
            "I don't know.",
            "It's an interesting question.",
            "Maybe you were able to get rid of it more and more later in learning, and there's some benefit if you could because it could speed up the process of propagating the information, because if you're going to keep these weights fixed for a little while, this doesn't mean that like newer updates aren't yet in those weights, so it's a good good idea to maybe investigate.",
            "Yeah.",
            "Bootstrapping related to aliasing.",
            "If you, I mean if you're if you're system aliases anyway.",
            "Or or is it was a listing only issue because of the bootstrap?",
            "That's a very good question.",
            "The question is ascension.",
            "How do bootstrapping and aliasing or generalization?",
            "How do these things interact with each other?",
            "And why is there?",
            "Why am I focusing on it?",
            "In some sense there's definitely an interaction here because considerable Carlo case where we would do full rollouts.",
            "Let's just consider prediction.",
            "So forget the off policy bits here.",
            "You could still consider doing bootstrapping or getting a full Monte Carlo Rolos, and you could still consider doing generalizations, so doing function approximation.",
            "Aliasing the generalization here is important.",
            "How much you alias?",
            "Because the learning the guests from the guests actually changes, not just your bias variance tradeoff that rich rich doesn't talked about yesterday.",
            "It's in some sense, but it also changes.",
            "Exactly what the capacity is of that architecture.",
            "Bootstrapping on so in some sense, there's actually there's a more formal way to talk about this as well.",
            "An one thing that's been investigated quite well is temporal difference methods with linear function approximation, and there you can actually show that the fixed point, so the eventual solution that the system converges to depends on how much you bootstrap.",
            "If you bootstrap more, you're going to actually end up in a different ends.",
            "Answer All the way at the end.",
            "Even then, if you don't bootstrap, it'll.",
            "There's a tradeoff there where learning is still faster with bootstrapping, but in the end the actual solution you're gets might be eventually better if you would just do the Monte Carlo thing we're doing, pure Monte Carlo is very data inefficient, so we wouldn't want to do that.",
            "Of course, here is a different thing that's going on, which is the off policy learning for which we kind of maybe need to bootstrap a little bit more than if you would just be doing predictions.",
            "So there is another confounding factor there.",
            "Yes.",
            "What?",
            "Fire.",
            "That's a good question.",
            "So the question is, how do you decide what to keep your replay buffer?",
            "And this is a little bit of another investigator to topic.",
            "I think what was done was just the most recent million frames.",
            "And this does by issue.",
            "It does by issue towards what your policy has done in the somewhat recent past, and it might not be optimal way, it's related to something else that I want to talk about."
        ],
        [
            "Now, which is also related to the experience we pay, because in addition to deciding what is in the replay, even if there's lots of stuff in the replay, let's go to the extreme.",
            "And let's say we never thought anything away.",
            "We just put everything in the replayed, then there's still this question, which is very related to the questions you were asking.",
            "Which things do you pick to even learn from?",
            "So the tossing wave data is similar because if you throw it away, you can never learn from it again.",
            "But even if these days is there, it doesn't mean you actually have to learn about each sample equally much.",
            "Anne.",
            "There's benefits to doing experience replay, which were quite clear from the from the results.",
            "So first to recap that it's more data efficient because the data is used more than once on average four times in the standard equipment set up, and it resembles supervised learning.",
            "More so for this reason the deep learning property works better."
        ],
        [
            "This is just a list of improvement that related on the decline, which I won't go through step by step, but I'll highlight a few and this is going back again to the question about the data distribution and the experience replay."
        ],
        [
            "And one thing I wanted to highlight first is that we can basically think of this replay differently.",
            "We can think of it as a nonparametric model of the environment and there are some benefits to thinking about it this way.",
            "Because now if we consider it a model, we can think about querying the model.",
            "It's a nonparametric model, so you don't actually get from a state in an action.",
            "What the expectation is over next reward States and so on.",
            "But you get a sample from this.",
            "The sample that you actually saw in the past.",
            "And.",
            "There is multiple ways you can use this and I wanted to highlight 2 examples of this.",
            "One is to sample Nonuniformly I was already.",
            "Are pointing towards that and this is school prioritized replay and this really helps a lot in terms of performance and one way to do that is, well.",
            "First, let me mention the second one.",
            "You could even use this model to kind of plan.",
            "So in some sense you could, for instance, look at a state in action.",
            "You could look at what happens then and you could take this as an indication of what might happen again if you're in a similar or in the same state in action and maybe you can even even compose these things together.",
            "This is not yet a fully fleshed.",
            "I don't think the last word has been said about this research.",
            "There's a lot more that can be done also, especially when you want to.",
            "Maybe then distill this down into a parametric model, which is a very interesting Avenue, I think."
        ],
        [
            "But to talk a little bit more about this prioritized replay.",
            "So what, for instance, could you do?",
            "You could do something which is fairly simple.",
            "You could sort of temporal difference error of each of these transitions, and this is basically a snapshot of how much would you learn from this transition, and then when you sample that you might learn maybe this amount, but it also gives you an indication of maybe which things you want to sample, because just consider the situation where this temporal difference error is zero, which means that just by chance the state action value that you had here.",
            "Accurately predicts that the one step reward and the resulting state value.",
            "That you end up in if you're going to sample this from your replay, you're going to learn exactly nothing, because your error will be 0 and your update will therefore be 0.",
            "That's obviously a waste.",
            "If instead you pick something right information areas high, you're going to do a comparatively bigger update to your weights.",
            "In addition, this is kind of the stuff that you want to learn about, because these are the transitions that were.",
            "These were inaccurate, and thereby if you learn about this, you might learn more about the world's than you would otherwise.",
            "This is just one way to prioritize an you have to be careful a little bit hard to pick this because the Atari games are actually.",
            "Deterministic, so if you do assertions in a certain state, do certain action you deterministically end up in end up in the next state.",
            "And this is more or less irrelevant to most of the learning algorithms, but its relevance, for instance for this part, because if you consider a situation where there is actually very noisy environments, if you would use exactly this prioritization, it might just replayed annoys a lot, especially if the noise is nonuniform.",
            "There's some very noisy transitions and others aren't.",
            "So then maybe you want to tweak the prioritization like you might still want to prioritize your replay.",
            "If you're doing experience replay.",
            "A different thing that I wanted to highlight the question first.",
            "Can yeah so could you learn maybe which things to sample?",
            "Which things to look at.",
            "This is definitely.",
            "Yes, I would say.",
            "How less clear would be an interesting area of research.",
            "I don't know that I don't know that as been done there yet.",
            "It's a good question.",
            "Yeah.",
            "For example, these two graphs on the right.",
            "They probably represent like.",
            "I'm ready to break.",
            "Independent games are samples, but there is no error bars and that's something that throws me off guard within some of the like your paper will be great.",
            "This maybe they all within one standard deviation 20 Saturday or not.",
            "Yes yes sure.",
            "So you often see many of such curves an indeed often there aren't error bars.",
            "In practice, this is often the case because there is only one seed.",
            "So you might run.",
            "So what we typically do, we take like 57 Atari games or these days at least in the nature paper was 49, but we added a few and then we run these algorithms.",
            "All of these games and one way to think about it is that we're kind of doing an analysis on the whole set of games, so we're still doing 57 different things, but we're not doing, say, 30 samples per game.",
            "Ideally we would write, but it's just so much more compute that's.",
            "Let's say you do 5 for you.",
            "Only take one seat and that's it.",
            "Or you take the best before.",
            "Oh no.",
            "So yeah, no we don't take the best seed.",
            "We just take one seed and then we take whatever that gives you, taking the best seat I would say is much more dodgy.",
            "There are different approaches, so in some cases some people have, for instance done random hyperparameter search and then it becomes a little bit more Gray area.",
            "Sometimes you just want to pick the best hyperparameters we have to be a little bit careful because you might be maximizing over things.",
            "So then what's often done is you pick basically best couple of hyper parameters and then maybe you pick like what are the third best is like the three best and it gives you some sense of how robust the algorithm is, but the details are often.",
            "Phrased explicitly in the papers, but as I said, we would love to do these things, maybe with a lot more samples, but it's prohibitively.",
            "Compute intensive.",
            "Throw you off 'cause there's a lot of variance when you're doing enforcement.",
            "Yes, yeah.",
            "Extra credit why, but yeah.",
            "So there are some variants definitely, so you shouldn't.",
            "You shouldn't pay too much attention to like the specific numbers.",
            "Too far down to the right hand side of the number.",
            "In my experience daughters.",
            "Many games such as PAC Man is a well known one or Miss Pacman where these learning curves are actually very consistent.",
            "An if you improve your algorithm, you gotta learning curve that's basically consistently above the other one.",
            "And if you have a bug in your algorithm is basically consistently not learning, so these games are apparently a little bit more robust to these things than I would have expected, maybe beforehand before working with them, but it's definitely something to keep in mind, because this doesn't hold for all domains and for other domains it's going to be even Wilder.",
            "It also depends on your algorithm alot.",
            "Some algorithms are more susceptible to these things to noise.",
            "In some sense, yes.",
            "So you mentioned this.",
            "Are you talking about the environment?",
            "So the question is, are we talking about the seed for learning or seeds for the environments?",
            "I typically mean there's just like, for instance, you might randomly pick a seed and then this is used for both the environment and the algorithm.",
            "However, like I said before, Atari is actually deterministic.",
            "So what we often do in Atari to make it less.",
            "We want robust algorithms that can work in general domain, so we don't want to exploit the determinism of the Atari games essentially.",
            "So one thing that's often done this we you might take a couple of random no OP actions at the beginning so that you start off slightly differently.",
            "For instance, you might take 30 actions or up to 30 actions that don't do anything before you actually start playing, and this kind of breaks the determinism in newer version of the newer versions of the arcades learning environment, which.",
            "Has these Atari games actually features a possibility to also add noise?",
            "Which makes it, which completely breaks is determinism.",
            "Just to make it a better testbed for algorithms that don't want to exploit that.",
            "I'm not changing the model parameter.",
            "Learning.",
            "Yeah, so typically we would we do if we would start anew afresh.",
            "New algorithm you would pick a different random seeds for your learning and this would automatically change everything because your policy would be different.",
            "There are few data distribution is different and therefore for instance anything any randomness that may or may not suit environment size maybe already matters less because you're already doing different things in this environment.",
            "Yeah, is this spread to overfitting on special cases that shouldn't be fit like overfit?",
            "I guess maybe like obscured cases that you only see a few times in the data, but since you're so.",
            "This is prone, more prone to overfitting.",
            "Well, I think there's more generally.",
            "If your system, if your task allows you to find a certain policy which.",
            "Exploits certain things it will.",
            "It will latch onto that.",
            "It will try to find that I don't think overfitting is so much of a problem in reinforcement learning.",
            "Because essentially typically what we do is we have a certain task, especially in this case we have a simulator, right?",
            "We have these computer games that you can play overfitting.",
            "This game is essentially impossible because this just means you're better at it, right?",
            "So you can't really say that you're it's less common to have this very clean train set test set divide in reinforcement learning, because just doing better at the task is just doing better at the task and that can never be overfitting in some sense.",
            "It depends a little bit on your specific setup, so there are cases in which it is a problem.",
            "In this case it's not so much of a problem.",
            "This for very many good questions, but I'm going to."
        ],
        [
            "Continue to see if we can cover a little bit more ground.",
            "This is another example of something that helps improve things.",
            "Anne.",
            "And for this, I first wanted to just rewrite the update that is done in the queue and.",
            "This is making it slightly more explicit, something that I said already, which is that we're actually evaluating a certain policy of policy and what we're evaluating is the greedy policy.",
            "But since we're using this target network with this W minus as parameters will actually evaluating the greedy policy with respect to our current parameters that we're actively updating.",
            "But we're evaluating the greedy policy with respect to the target network.",
            "This is what this arc Max means in the middle there were picking the highest valued action.",
            "According to the parameters of my target network, this W minus and then we're evaluating those actions with that same network.",
            "This is a little bit dangerous, because now we're essentially we have these random action values which depend on your learning process and potential noise in your environments, but also just some things like the initialization of your network.",
            "And then we're saying we're going to pick something that maximizes that, and then we're going to use the exact same values that we used to pick to evaluate that.",
            "This means that it's actually more likely to pick something that is highly valued and therefore get a higher value.",
            "It turns out this is a real bias.",
            "And then there's a way to prevent it very easily in the case of TQM, because we already have two sets of parameters.",
            "What I did here at the bottom equation I just colored the black W as we had before.",
            "These are the things that we're actively updating.",
            "Each updates, I color them blue an all the way inside.",
            "In this arc Max.",
            "I'm now using instead of.",
            "This slowly moving target network parameters.",
            "I'm using the online parameters.",
            "What this means is I'm now evaluating the greedy policy with respect to my current.",
            "Parameters.",
            "With the parameters that are slowly more slowly moving, the idea is to break the correlation a little bit between the selection and evaluation.",
            "And this should then reduce this overestimation bias that you might otherwise get."
        ],
        [
            "And this turns out to actually really work.",
            "This is 2 Atari games and what you see here on the X axis is the estimated value according to your current action values and Y axis is on the log scale.",
            "And what you see here is that in red DQ and at some point the values could go quite hugely up.",
            "And you might think if you're just looking at the values you might think oh, this is good preparing me.",
            "I'm learning the values go up.",
            "So apparently my policy is improving."
        ],
        [
            "But turns out almost exactly at the point where these values start to increase greatly.",
            "My actual performance goes down, and there there is now a very big mismatch between the actual rewards that you get and the rewards that your value function says it's going to get.",
            "This is something you could notice if you're tracking these statistics, you could basically see hey, my value estimates are now way over estimating the actual values that I'm guessing.",
            "If you get rid of this overestimation bias, the performance drop also goes away.",
            "So apparently the performance drop was completely due to this overestimation bias.",
            "In this case, which is kind of interesting."
        ],
        [
            "And then if we apply this, all these Atari games, this is basically performance, graphic full of all of those 57 Atari games.",
            "The details are not too important, but if you apply this trick."
        ],
        [
            "Turns out it works pretty much across the board, so it wasn't just like random game where this happened.",
            "Apparently this overestimation bias bias was quite prevalent.",
            "This was a little bit surprising to me.",
            "I hadn't expected to be so severe.",
            "In the take home message, I wanted to give in this is that it's good to be aware of the properties of your learning algorithms, in this case in Q learning, where taking the Max over something that is good just to stop and think for a moment while we're taking the Max offer.",
            "What does that mean?",
            "What could then happen?",
            "In addition, is very good to track and to analyze statistics.",
            "In this case we could be tracking these action values, and in addition perhaps the performance and maybe see how is my prediction error behaving?",
            "Is it going up?",
            "Is it going down?",
            "It's perfectly OK if your prediction error goes up during learning.",
            "This is a little bit weird if you're coming from a supervised learning background, because then you kind of expect your loss function continues to go down, but in reinforcement learning because you're changing your policy is perfectly acceptable that when your policy improves, your predictions are a little bit outdated and your prediction error might actually go up.",
            "This is fine, but at some point we should at least stabilize, go down again or your policy should continue to improve, both of which you could track.",
            "If at some point your values go way up in your prediction error goes way up.",
            "But at the same time, your policy is not improving.",
            "Then there might be something wrong and then you could basically analyze your online statistics to find that out, yeah?",
            "That spectacle yeah, it's a very good question.",
            "So what are we breaking the target network?",
            "I'm breaking the benefit of the target network in some sense by now making the targets more nonstationary again.",
            "So there's actually two ways you could do this.",
            "If you look at the equation here, what I mentioned is I wanted to break the correlation.",
            "I wanted to break the selection from the evaluation and there's two ways to do that.",
            "If you do not have to networks, we could either pick the greedy policy with respect to my online parameters and then evaluate that with the slowly moving parameters.",
            "It's our growth parameters or the other way around.",
            "And it might not be immediately clear which one works better.",
            "Turns out this one works much better, and the reason is probably twofold.",
            "One is that we're evaluating the policy, which is greedy with respect to my current online parameters, and these are also the parameters that we actually used to act with.",
            "So it kind of makes intuitive sense that this is also the policy that we actually care about, so it makes sense to evaluate that one in addition, and perhaps more importantly, for the learning process, maybe it's OK if my target change a little bit overtime in terms of how the policy changes.",
            "And maybe it's less OK if the value is also change and this is what we're doing here, we're keeping the value function fixed.",
            "We're just changing the policy that the value function is evaluating, and this is apparently more stable.",
            "But this was indeed I actually did.",
            "Also, try the other version, because I wasn't immediately convinced that this is the only way that would work.",
            "Different.",
            "The gradients are going to be different.",
            "The updates are going to going to actually be different.",
            "But it's still OK.",
            "It's still.",
            "It's still.",
            "It's still a reasonable thing to do.",
            "And more generally, you could think of this update with the target network more or less is a fine thing to do, regardless of the policy that you're trying to estimate.",
            "Even if the policies itself might be fairly quickly changing, this is a harder estimation problem.",
            "Your prediction errors will be harder if your policy changes very frequently, but in this case, even if we updated parameters, the policy might not change that much.",
            "An one way to see that we're taking this argmax if your action values are like if one action is true.",
            "Truly clearly a lot better than the other ones.",
            "We might improve the prediction errors on all of these actions, but it wouldn't actually change my policy, so it's not actually clear that my policy is changing that quickly anyway.",
            "It will depend on the game will depend on the specifics, but it might already be slow enough to be stable.",
            "There might definitely be cases in which is not true, and then it's useful to do those things that I said later to track statistics, just to keep track of what's working, what is not working and so on.",
            "Yes.",
            "And freezing.",
            "But the general.",
            "So the general intuition is just to get rid of this overestimation bias.",
            "That's basically it.",
            "An double DQ and I would say this specific implementation is just one way to do that.",
            "If you want to get rid of your estimation bias in a different way.",
            "What these plots basically tell me is that it's about the overestimation bias.",
            "If you can get rid of it in a different way, that might be perfectly fine and you might not want to use double DQ and, But that's the main intuition and this is just what I wanted to do here with.",
            "The update is basically find the very simplest way, the smallest change to the algorithm to get rid of that overestimation bias, not necessarily find the best way.",
            "But also to keep most of the other stuff fixed so that we understand that it's truly this thing that matters.",
            "That was the intuition.",
            "Thanks.",
            "Yes.",
            "There are convergence analysis for many of these algorithms, especially for the tabular cases.",
            "Before double DQ and I keep proposed double Q learning, which is essentially the same thing, and there's also convergence analysis for that particular case.",
            "The off policy algorithms.",
            "Have less guarantees in the function approximation case then the on policy algorithms.",
            "So for the TD algorithm that rich talks about yesterday, the pure prediction one.",
            "There's also a bunch of convergence results also with function approximation.",
            "There are some convergence results with non linear function approximation but already less so.",
            "Most of these results not all of them, but most of these results only concern the asymptotics which may or may not be of interest to you.",
            "It basically tells you whether or not the update is stable.",
            "These things are a little bit important, especially to understand the cases when they're not stable, when they might actually blow up and diverge.",
            "That said, there are known cases in which combining Q learning with non linear function approximation is unstable and the parameters might diverge.",
            "But as we've seen many times in practice by now, if you're careful enough, this doesn't need to happen.",
            "The fact that there already is counterexamples doesn't mean that it's inherently unstable, and another will never work.",
            "For instance.",
            "There's there's a very good book from 1996 by vertical Synthetic Lease.",
            "They call it Neurodynamic programming, and this is essentially about reinforcement learning with function approximation.",
            "They have a lot of results.",
            "There's a new book about particles from 2007 in which basically has an updated version of many of these results as well.",
            "An much more if you're interested in that kind of thing.",
            "Those are good pointers.",
            "In addition, there is a shorter book by Chavez Obituary, who also will be speaking here later today, and he has.",
            "Many has done much much work in this area as well."
        ],
        [
            "OK. Oh yeah, the last point I still wanted to mention.",
            "If you understand what the problem is in this case the overestimation.",
            "Sometimes you can do a very simple thing to fix it.",
            "An for instance, in WCU, and one of the reasons why specifically did this one.",
            "It's a one line change of code, and that's very nice if you can just OK, it's just a hypothesis.",
            "If I change this, I'm going to change this property that you can expect, inspect, and then you know what you've done and you know what the impact is.",
            "Changing too many things at the same time.",
            "It might work, but it's much harder to understand what happens and why these things work.",
            "And then it's hard in the next problem to know exactly what you need to do.",
            "This fits into a more general theme that I would just want to highlight.",
            "There's two of these things that are a little bit related, but different views on the same thing.",
            "The target networks and the experience replay you could view them as deep learning aware reinforcement learning where basically changing the updates a little bit to be more similar to what we know.",
            "That works with deep learning experience.",
            "Replay in some sense makes it more IID, and we know that that's something that these deep networks in nonlinear functions in regression like the target networks, make the targets more stationary.",
            "You know that that's something that these these systems work well with.",
            "Similarly, but in the opposite view is reinforcement.",
            "Learning aware deep learning where maybe you want to change, say the optimizer that you're using.",
            "Maybe you want to change your network structure to be more conducive to what you're trying to do with this.",
            "Anne."
        ],
        [
            "On that Latin on that second note, here is an example of that.",
            "Which is during the Q and the actual title paper.",
            "Title is a bit longer.",
            "Dueling network architectures for deep reinforcement learning from last year's ICL.",
            "The idea is very simple.",
            "You can think of the action value function as a combination of the value of this state under a certain policy.",
            "All of this is under the same policy.",
            "This is the first equation over there, plus the advantage of picking a certain action.",
            "And for simplicity, let's just say that these advantages are zero mean.",
            "That means that their value function is now basically learning the offsets.",
            "How important is it to be in this state and the advantages then learn how much of an advantage do you get?",
            "In terms of value for picking this specific action in that state.",
            "Now you could write it down slightly differently, which is the 2nd equation there and what we what's being done.",
            "There is essentially taking away 1 degree of freedom.",
            "This is essentially making sure that these values the value function indeed is trying to estimate the baseline.",
            "Assuming that the advantages are roughly 0 mean, and then if you want to have accurate Q values, these advantages will almost automatically then navigate towards being 0 mean to to fit nicely within the within the loss that we're specifying now.",
            "What's kind of cool about this is that we have action values back right?",
            "We just basically decompose the action.",
            "Using the specific way and one way to implement that is just to change the network structure and this is what's depicted there at the bottom.",
            "The first thing that you see there is the typical deep network where there is a common flare or a couple of players, and then there's a fully connected layer in the dueling network architecture.",
            "It's exactly the same, but we split the fully connected layer.",
            "And then we go into a value function and into these advantage functions we just add these together to get my act together action value functions.",
            "And that's all.",
            "And then we do the same WN update with prioritized repair without reproach replay.",
            "On top of that.",
            "And this works, this helps, which is kind of interesting, because it always doesn't change the network architecture.",
            "We didn't change the update in any other way, but by changing the network architecture where basically we give a bias to the system to represent things in a certain way, and this way happens to be a good nice way to represent the value function and what I'll show you now with this video.",
            "I hope it's a little visible.",
            "What you see on the left hand side here?",
            "This is a game of enduro.",
            "It's a racing game from this from Guitar suite of games and what you see on the left here is basically the gradient with respect to the inputs of your value function.",
            "This is sometimes called saliency map.",
            "One way to think about this this is what my algorithm is paying attention to to get the value and what you see is that the vision.",
            "In some sense, the saliency is all the way at the end of the road where the road is heading towards the right hand side is the advantage functions is basically what is my.",
            "The advantages of each of every action paying attention to in the current screen.",
            "You don't see any red.",
            "There's nothing there, but if you watch what I press play, you'll see it flash sometimes.",
            "And maybe I can actually stop it.",
            "Well, slightly too late there maybe I'll come grab another one.",
            "So sometimes you see flash when there's cars nearby while grabbing it.",
            "Interestingly, it seems that the value function is paying attention to things that happen way.",
            "At the end, whereas the advantages are mostly paying attention, things that happen very close by, this makes sense because the action differences between different actions are much more pronounced when there's cars nearby when it actually matters to your value, which actually take most of the time, it doesn't really matter that much which actually take if there's no cars nearby.",
            "It doesn't really matter, you could pick any of them.",
            "The differences are very small.",
            "And what we see here is essentially that the system has now learned to use different parts to pay attention to different parts of the of the problem.",
            "And this then helps performance.",
            "Turns out again, this is very simple change to the algorithm, so even to the learning updates.",
            "But just to the network architecture and this helps performance because we know that it has a certain semantics to it.",
            "We know what it's trying to do, yes.",
            "The target for the value in the advantage.",
            "Yes, it's the latter.",
            "The question is, do we have separate targets for the value and the advantage?",
            "Or do we just have one target for the queue functions?",
            "It's the latter, but actually you could write this down because it's just going in the sense so you could ride this out and squared loss on your Q values and you could decompose that into a basically a loss on your value and a loss in your advantage, but these are not related to these two losses.",
            "They depend on each other because the actual losses on the action values there is a very good question.",
            "Yeah.",
            "So why do we?",
            "Why do we get rid of the average advantage in the Q value?",
            "So the idea there is.",
            "We want the value function.",
            "We basically want to force the value function to really be the value of the States and the advantage is just to be the offsets.",
            "So we kind of want the advantage to be 0 mean and this is one way to do that because in order for this value function to be accurate because we're deducting the mean advantage, it needs to be that value.",
            "It's conscious being arbitrary thing.",
            "If you don't do this, you have an additional degree of freedom.",
            "What this means in practice is that, for instance, you could increase all your advantages and decrease the value and get exactly the same Q values.",
            "Turns out if you run that in practice, it doesn't seem that harmful, but it turns out it works less well.",
            "It's not maybe completely clear or understood why, but it makes a little bit intuitive sense, and then in practice it does turn out to be important.",
            "It's a good question.",
            "Yeah.",
            "Sorry.",
            "Oh yeah, so many of these algorithms actually pay attention to the score that's on the screen.",
            "It's not completely clear why, but in some games it might actually be a very good indication of how much more rewards you can accrue.",
            "For instance, how far along in the left leg are sometimes these scores can almost be seen as kind of like a Clock, but it depends on the game and I honestly don't quite know why it's paying attention to it here enduro, but it's quite a common thing.",
            "We see that preceded a lot.",
            "Yeah, yes.",
            "So soothing, like if you were to put some partial pooling over, yeah?",
            "I have a similar factor.",
            "So this is a form of smoothing I don't know.",
            "I hadn't thought about it like that.",
            "I'm more than just see it like a semantic decomposition of the value into two different parts.",
            "Maybe there's something there, maybe there's like a small.",
            "So what I would encourage, especially if people start playing with these things, is for instance to take hypothesis like this.",
            "Or maybe maybe this is good because it's doing some form of smoothing and maybe then there's a way to do just the smoothing, like very very constrained way and see if that that also helps.",
            "Whether this has a similar benefit.",
            "I don't immediately see it here.",
            "OK, so.",
            "Anne.",
            "Mostly running out of sign.",
            "So let's see what's on.",
            "Let's"
        ],
        [
            "Yep, that.",
            "There's one more thing that I wanted to.",
            "Mentioned this is mostly stuff the piece reveals talks about yesterday with the policy gradients.",
            "This is the same derivation that he had on the slide, so I'm not going to go through that again.",
            "But I just wanted to highlight again that there's different ways you could.",
            "You could learn a policy, and one way is to do that directly and let me just jump in basically here.",
            "The algorithm here is calls reinforce.",
            "It has different names, but."
        ],
        [
            "Common name is reinforce."
        ],
        [
            "Which is from a paper from 1992.",
            "And it gives you sample based way to update your policy.",
            "This is different from what we talked about before because we're not learning a value function anymore.",
            "We're directly updating policy.",
            "And the important equations here.",
            "So the first one here.",
            "The first equation here basically says J.",
            "Here is maybe standard notation in some parts, but you can just think of this as the value of.",
            "Your policy under those parameters.",
            "Basically across states, so Jay doesn't depend on state, but just think of it as the expected value across all states.",
            "Anne then turns out this."
        ],
        [
            "Derivation that we had before that's Peter talks about yesterday.",
            "Basically shows us that we can sample the."
        ],
        [
            "Thing, which is why you see on the right hand side there is an expectation around something and this expectation has the true Q value in there and the greater with respect to the logarithm of your policy.",
            "What this essentially means one way to think about this is that what happens to your policy parameters is if your value function is high, you should increase the probability of selecting that action.",
            "If it's low, you should either decrease the probability of selecting the action or just increase it less and then by normalization it turns out to decrease.",
            "And this is the whole idea behind it."
        ],
        [
            "Force algorithm.",
            "You could, as a baseline, which typically is just a value function.",
            "This reduces the variance, but it doesn't change the direction of your gradient, so this is a fine thing to do.",
            "Peter also mentioned this in his talk.",
            "And the intuition is still exactly the same.",
            "We're still increasing the probability of actions that were better now, better than expected with respect to the baseline, and then we're decreasing the probability of actions that were worse than expected with respect to our baseline.",
            "It's not unclear.",
            "Database line needs to be very accurate because otherwise we would use increasing certain actions and others less, and then in the net effect is maybe still the same.",
            "But picking a good baseline is important for the variance of the system, yes.",
            "Oh, it's very good question.",
            "So how does the baseline decrease the variance?",
            "The idea is actually quite similar to the Jeweling architecture that I just showed you.",
            "The baseline here basically keeps the variance due to the states out of the system.",
            "The returns will have variance because your your actions will give you like a random return and maybe the system is also some noise.",
            "Is there will be variance in that.",
            "But in addition the states might all have different different values and this just gives you additional variance if you don't take this out.",
            "So this is basically a way to think about it.",
            "The randomness due to which state you're in is taken care of by the baseline and therefore the variance is lower.",
            "OK.",
            "So I'm skipping over this.",
            "This is about can you do replay in that?"
        ],
        [
            "I think you can, but let's first talk about when we don't do replay.",
            "So what is an alternative to doing replay?",
            "So think about why we were doing replay.",
            "This is again about talking about whether the properties of these algorithms when do we expect them to behave well, why we were doing replay in the 1st place.",
            "So there's two reasons.",
            "One is data efficiency, but the other one was we want the data to be a little bit more diverse, more like what we know that works well with deep learning.",
            "But there's more than one way to do that, and this is a different approach.",
            "You could think also about just storing many simulate.",
            "Sorry, having many simulates relations happening at the same time and having more or less your same agents acting in all of these simulators at the same time.",
            "In parallel.",
            "This would also give you diverse data.",
            "In a completely different way, but it's still diverse day, so maybe this still works.",
            "And then this was investigated by Vlad and others an they found that this does really work even if you just then.",
            "Pretty wildly updated parameters from all of these different threads which are happening asynchronously on a single machine or many CPUs perhaps.",
            "And you can basically combine this with any reinforcement learning algorithm.",
            "Just this is like a separate thing.",
            "Whether you do replay or whether you do this parallel environment thing.",
            "This is separate from whether you do, say, reinforce or Q learning, but they both are.",
            "Combinable with each other and this turns out to work really well.",
            "Skip through this, but."
        ],
        [
            "I'll show you a slide.",
            "This is the slide I wanted to show you where now at the bottom left you basically see these networks which now have two heads.",
            "One is a policy head and one is value head.",
            "This is similar to this dueling network architecture, but instead of having advantages and the value we now have this policy and a value and then we can use the.",
            "The reinforce updates, or an approximation derolf which is the equation there at the top.",
            "The top equation on the right where we don't take the full return but reduce the variance even a little bit further by bootstrapping a little bit short you could do the one step version, but you could also do the multi step version which was used here and then you could basically tweak how many steps you will get a good tradeoff between the variance in the bias.",
            "And I just wanted to highlight this for its similarity and also that it's a different approach to do something similar to get the similar benefits from the replay.",
            "I understand we need to stop here for coffee.",
            "But feel free to come up to me for questions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I'm glad you're made it in in the morning.",
                    "label": 0
                },
                {
                    "sent": "Thanks for coming and thank you for inviting me to talk here.",
                    "label": 0
                },
                {
                    "sent": "So yes, I'll be talking about deep reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To talk about that first wanted to step back and talk about the big picture.",
                    "label": 0
                },
                {
                    "sent": "Why do I care about deep reinforcement learning?",
                    "label": 1
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Why you want to care about the reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That's of course up to you to decide.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I wanted to step way back and start.",
                    "label": 0
                },
                {
                    "sent": "At the Industrial Revolution and what they sometimes called the machine age and think about what that was, what happened there and essentially one way to think about this is that this was a time in which we took manual solutions that we used to do with manual labor and then replace that with machines.",
                    "label": 0
                },
                {
                    "sent": "This is where factories and such came into play.",
                    "label": 0
                },
                {
                    "sent": "Steam engines and then later in the digital revolution more recently or the information age.",
                    "label": 0
                },
                {
                    "sent": "Essentially we did something very similar where we now too.",
                    "label": 0
                },
                {
                    "sent": "Repetitive mental solutions and we replace those with machines.",
                    "label": 1
                },
                {
                    "sent": "So one way, one example of this is a Calculator where we know how to calculate things by hand.",
                    "label": 0
                },
                {
                    "sent": "But you could also automate that process if it's formal enough, but it's an implementation of a solution.",
                    "label": 0
                },
                {
                    "sent": "In both cases we do have to come up with the solution 1st and then we implement that solution.",
                    "label": 1
                },
                {
                    "sent": "Now what I wanted and posit, which might seem obvious now, is that.",
                    "label": 0
                },
                {
                    "sent": "The next step should perhaps be that we only specify a goal, and we leave the discovery of the solutions to the system.",
                    "label": 0
                },
                {
                    "sent": "This is what learning is, and this is what you might call the AI revolution.",
                    "label": 0
                },
                {
                    "sent": "This is very broad.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pause.",
                    "label": 0
                },
                {
                    "sent": "Broad sense, so let's dive a little bit deeper into what Aion might mean.",
                    "label": 0
                },
                {
                    "sent": "So back in the day when people started talking about artificial intelligence, a lot of the first approaches were symbolic, where we get them reason formally about things that might might happen, or things that might imply other things.",
                    "label": 0
                },
                {
                    "sent": "And this is already an automated form of reason.",
                    "label": 0
                },
                {
                    "sent": "You could come up with new conclusions based on the information that you put into the system.",
                    "label": 0
                },
                {
                    "sent": "But the rules themselves were programmed in advance and static, and often there was a high level of hand pick, knowledge, formalism or level of abstraction.",
                    "label": 1
                },
                {
                    "sent": "If you want to think about it that way, with which I mean you had to basically pick what the symbols were, what they meant, how they would interact with each other, and then you could drive things.",
                    "label": 0
                },
                {
                    "sent": "But there was a lot of specialist knowledge in some sense involved.",
                    "label": 0
                },
                {
                    "sent": "And one thing that happened with these systems is that in many of the cases they basically didn't scale that well.",
                    "label": 0
                },
                {
                    "sent": "Too messy data as you would have in the real world and to uncertainty.",
                    "label": 0
                },
                {
                    "sent": "From a different perspective, there is classical statistics.",
                    "label": 1
                },
                {
                    "sent": "An in classical statistics we are able to deal with insurgency with messy data, but typically the classical goal in statistics is already in the name.",
                    "label": 0
                },
                {
                    "sent": "It's to basically get statistics about your data.",
                    "label": 0
                },
                {
                    "sent": "It's about analyzing and then this decisions were still made after the fact.",
                    "label": 0
                },
                {
                    "sent": "So you would analyze something and then based on this analysis you would learn something about your system or whatever it is you're interested in, and then you could make decisions and even picking what to analyze in the 1st place is a decision you do.",
                    "label": 0
                },
                {
                    "sent": "Up front.",
                    "label": 0
                },
                {
                    "sent": "So there's something in here which you could already call learning.",
                    "label": 0
                },
                {
                    "sent": "This is the automatic analysis of the data, but it's not completely end to end from observing something to making decisions in the world.",
                    "label": 0
                },
                {
                    "sent": "So again, here are positive something and this is just to make you think if you agree with that phrase through a, I should learn to make decisions autonomously.",
                    "label": 1
                },
                {
                    "sent": "This kind of makes sense in reinforcement learning context, of course, so this might not be a very surprising thing for me to say.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on that note, we go into reinforcement learning and I first just very briefly wanted to recap some things that you were covered yesterday, so that I'm sure that we're on the same page.",
                    "label": 0
                },
                {
                    "sent": "Whenever you have any questions, just feel free to raise your hand and I'll try to look around sufficiently to spot you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, good.",
                    "label": 0
                },
                {
                    "sent": "Just like we would you put on a control, both classes and stuff.",
                    "label": 0
                },
                {
                    "sent": "Networks.",
                    "label": 0
                },
                {
                    "sent": "So it depends.",
                    "label": 0
                },
                {
                    "sent": "Depends a little bit on what you use for installation networks for an, But in some sense, maybe in district like if you consider it in a narrow sense, you could view the Bayesian approaches as the analysis, but then you still might come up with things.",
                    "label": 0
                },
                {
                    "sent": "Visions yourself based on this analysis, there's a little bit of Gray area there, of course, because you also have Beijing decision theory, in which then decisions are kind of automatically determines and there's of course a lot of other Gray areas.",
                    "label": 0
                },
                {
                    "sent": "For instance, in statistics, well known area is active learning, which is all about sampling, which you could say our decisions.",
                    "label": 0
                },
                {
                    "sent": "The control problem is an interesting one as well, because sometimes when we say control.",
                    "label": 0
                },
                {
                    "sent": "Especially in the more classic approaches, we basically assume we have full access to a system and we basically inspect the system and then learn what a good policy of controllers for the system.",
                    "label": 0
                },
                {
                    "sent": "So there is not maybe a lot of learning happening there, it's more inference.",
                    "label": 0
                },
                {
                    "sent": "In that sense, reinforcement learning is a little bit different because we definitely I'm learning in the name for a reason.",
                    "label": 1
                },
                {
                    "sent": "We're definitely focused on the learning part.",
                    "label": 0
                },
                {
                    "sent": "Thanks very good question.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is just to recap, what is reinforcement learning about?",
                    "label": 0
                },
                {
                    "sent": "It's a framework for making decisions and this is maybe slightly slightly subtly different points than others have made before, because I really want to focus on it as a framework.",
                    "label": 1
                },
                {
                    "sent": "In addition, there is a lot of algorithms that fall under the header of reinforcement learning, but to me reinforced learning is more than that.",
                    "label": 0
                },
                {
                    "sent": "It's about the reasoning framework in which you can reason about what it even means to make decisions and why you would even want to make certain decisions.",
                    "label": 0
                },
                {
                    "sent": "The general setup that we're thinking about is that there is an agent or an actor that interacts with an environment and sends actions out into the world.",
                    "label": 0
                },
                {
                    "sent": "As you could see.",
                    "label": 0
                },
                {
                    "sent": "And then observations come back in either passively.",
                    "label": 0
                },
                {
                    "sent": "It just comes in at a certain decorate maybe, or maybe actively.",
                    "label": 0
                },
                {
                    "sent": "You pull in observations whenever you have the resources to do so.",
                    "label": 0
                },
                {
                    "sent": "And then in this talk I want to focus mostly on learning to act, which might seem obvious, especially given what I said before, but I wanted to contrast that to what Rich Sutton was saying yesterday, where you could also think about reinforcement learning as being the study of learning to predict.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's both.",
                    "label": 0
                },
                {
                    "sent": "But in this talk I'm mostly going to focus on learning to act, so there's a certain goal here, which is to find a policy of behavior that is good for some certain goal that you want to specify.",
                    "label": 0
                },
                {
                    "sent": "So each of these actions that the agent sends out can change the state of the world.",
                    "label": 1
                },
                {
                    "sent": "And also can result in the reward.",
                    "label": 0
                },
                {
                    "sent": "I deliberately did not put the reward in this little diagram.",
                    "label": 0
                },
                {
                    "sent": "You've probably seen many versions of this diagram.",
                    "label": 0
                },
                {
                    "sent": "I know there were like at least three different ones yesterday.",
                    "label": 0
                },
                {
                    "sent": "And oftentimes you see the reward coming from the environment into the agents.",
                    "label": 0
                },
                {
                    "sent": "And that's definitely a valid way to think about these things.",
                    "label": 0
                },
                {
                    "sent": "If you have a certain task, it's easy to think about these things as generating the rewards that your agents shoot and optimize, but sometimes it's actually more natural to think of these rewards internal to the agents that the agent, like certain observations, observations, might in some sense carry the information that you need to infer the reward, but you know the world out there might not care about which things you prefer, so there might be many, many signals.",
                    "label": 1
                },
                {
                    "sent": "In your observation that all might be taken to be rewards, maybe also different rewards by different agents, and maybe in that case reward lives within the Asians.",
                    "label": 1
                },
                {
                    "sent": "Most of the algorithms that we talk about in reinforcement learning don't particularly care where the reward comes from.",
                    "label": 0
                },
                {
                    "sent": "They consider it to be external to the learning algorithm, but it might still be internal to your agent.",
                    "label": 0
                },
                {
                    "sent": "And then the goal is to pick one to optimize future rewards for some reward signal.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's many examples we saw several yesterday.",
                    "label": 0
                },
                {
                    "sent": "We'll see more today.",
                    "label": 0
                },
                {
                    "sent": "As a reminder, things like video games have been done.",
                    "label": 0
                },
                {
                    "sent": "Board games.",
                    "label": 0
                },
                {
                    "sent": "Robotics is an application area of reinforcement learning recommender systems.",
                    "label": 1
                },
                {
                    "sent": "There's many, many, many more, and there's more.",
                    "label": 0
                },
                {
                    "sent": "Each day, more or less, and essentially this is because you could think about all problems that involve making decisions and or predictions about the future as falling under the head of the general header of reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So that makes it a very general field, and many of the algorithms are also intended to be very general purpose.",
                    "label": 0
                },
                {
                    "sent": "It also means that you might actually be doing.",
                    "label": 0
                },
                {
                    "sent": "Be doing what I would call reinforcement learning without necessarily realizing that you're doing reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "But as long as you're making decisions, and these decisions might impact the world in some way, and especially if there's time dependencies and decision making in the same problem, then I would say this falls under the header of reinforcement learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I want to talk about what approaches you could take to reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So the goal is to reiterate this.",
                    "label": 1
                },
                {
                    "sent": "We've specified the goal as to learn a policy of behavior that optimizes something that we'll talk about more.",
                    "label": 0
                },
                {
                    "sent": "That means that there's at least three possibilities and potentially more, but these are the main areas that you could think about.",
                    "label": 1
                },
                {
                    "sent": "One is, you could learn a policy directly.",
                    "label": 0
                },
                {
                    "sent": "An Peterbilt talked about this yesterday at quite some length.",
                    "label": 0
                },
                {
                    "sent": "I'll touch panel briefly again today, just to remind you how you could do these things.",
                    "label": 0
                },
                {
                    "sent": "Pretty, and this is also something that was talked about at length yesterday for instance, for instance by rich, but also by Joelle earlier.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, you could also learn values and these values could capture what you predict the reward will be.",
                    "label": 1
                },
                {
                    "sent": "And then if you have these values, it actually becomes fairly easy to also infer policy.",
                    "label": 0
                },
                {
                    "sent": "Essentially, if you learn action values, you can just infer a policy by inspection.",
                    "label": 0
                },
                {
                    "sent": "You pick the highest valued action in each state that you're in.",
                    "label": 0
                },
                {
                    "sent": "A different approach which we haven't talked about much yet as much yet at least, is to learn a model of your environment.",
                    "label": 1
                },
                {
                    "sent": "Maybe it doesn't have to be precise model, but maybe it's some abstraction of the environments of capacity.",
                    "label": 0
                },
                {
                    "sent": "Want to solve?",
                    "label": 0
                },
                {
                    "sent": "And then maybe you can infer a policy by planning.",
                    "label": 0
                },
                {
                    "sent": "This is actually the more classical approach where maybe the most used way to solve such systems is by actually not learning the model, but constructing the model and then doing planning on top of that and planning and search are very rich research fields.",
                    "label": 0
                },
                {
                    "sent": "And still very active, and there's many interesting questions there.",
                    "label": 0
                },
                {
                    "sent": "Even if you have a model, of course it becomes extra.",
                    "label": 0
                },
                {
                    "sent": "Maybe interesting, but also complex if you have to learn a model as well, and maybe you cannot trust the model fully to be exactly what you want the task to be.",
                    "label": 0
                },
                {
                    "sent": "So that means concretely, that you have to have one of these components in your agents.",
                    "label": 0
                },
                {
                    "sent": "You either have to have a policy explicitly like a function that Maps your states, your current state of your system to an action you have to have a value function, or you have to have a model.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's alternatives, but these are the main main components that we typically consider.",
                    "label": 0
                },
                {
                    "sent": "You could also have combinations of these, so it says at least one of these components, just to give you an example, if you have both a policy and value function, this is what we often call in.",
                    "label": 1
                },
                {
                    "sent": "Actor critic system.",
                    "label": 0
                },
                {
                    "sent": "The actor is on the policy.",
                    "label": 0
                },
                {
                    "sent": "The critic is knowing your value function and we've seen examples of this yesterday.",
                    "label": 0
                },
                {
                    "sent": "Also briefly give you more today.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just recapping some notation, this is commonly used notation for some of these things.",
                    "label": 0
                },
                {
                    "sent": "Actually for models, models are more out there.",
                    "label": 0
                },
                {
                    "sent": "Sometimes people just use F as a function for the model Rich talked about Q value.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you see a capital, QD says more.",
                    "label": 0
                },
                {
                    "sent": "More more often you see lower case Q.",
                    "label": 0
                },
                {
                    "sent": "But this is the notation that are mostly stick to.",
                    "label": 0
                },
                {
                    "sent": "In these slides there was a question, yes.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "American Medical.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's very good question if I let me let me phrase it so that you can check whether items to correctly question is how do you pick which of these things that you learn?",
                    "label": 0
                },
                {
                    "sent": "And that's essentially an open question.",
                    "label": 0
                },
                {
                    "sent": "An one thing that we've noticed, let me just give you a little bit of a teaser as well to the deep part.",
                    "label": 0
                },
                {
                    "sent": "Until recently, a lot of reinforcement learning was done, not in the context of deep neural networks, and then it was kind of common knowledge that the model based approach was definitely the most data efficient one.",
                    "label": 0
                },
                {
                    "sent": "It takes maybe a lot of computes to learn a model, but it's very data efficient because you can put all the information that you get from the world into your model, and then you can just plan with it.",
                    "label": 0
                },
                {
                    "sent": "This is where the computer comes in.",
                    "label": 0
                },
                {
                    "sent": "The planning might take you some computer.",
                    "label": 0
                },
                {
                    "sent": "Somehow we haven't really been able to replicate that yet with the deep neural networks and I'll talk about reasons for that as well.",
                    "label": 0
                },
                {
                    "sent": "Both the policy to direct policy learning approaches and Peterbilt talks about this quite a bit yesterday are very successful, but also the value based approaches are very successful.",
                    "label": 0
                },
                {
                    "sent": "It might be a little bit problem dependence.",
                    "label": 0
                },
                {
                    "sent": "There might even be these things might be more related than we realized in the past, and there's some recent work that showed that these things maybe maybe are quite closely related, depending on how specifically you implement them.",
                    "label": 0
                },
                {
                    "sent": "But it's a very good question, and it's basically one of the main topics that we are researching to figure out when to use which method.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, there all the methods that I'll talk about are typically quite general purpose, so even if you pick the wrong one, it doesn't mean that it won't work.",
                    "label": 0
                },
                {
                    "sent": "It might just be maybe slightly better if you use a different approach.",
                    "label": 0
                },
                {
                    "sent": "So all of these components are essentially functions an we need to learn represents and learn these functions.",
                    "label": 0
                },
                {
                    "sent": "Whichever components you choose to put in there.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is where deep learning then comes in.",
                    "label": 0
                },
                {
                    "sent": "So now to switch to deep reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "This is essentially the study of using deep learning to learn policies, values and or models to use in the reinforcement learning domain.",
                    "label": 0
                },
                {
                    "sent": "And again, this is actually fairly broad area, and as I said before, you might actually be doing reinforcement, or at least what I would call reinforcement learning without necessarily thinking about reinforcement learning or using the Canonical algorithms that we think of.",
                    "label": 0
                },
                {
                    "sent": "When we say reinforcement thing.",
                    "label": 0
                },
                {
                    "sent": "As long as you're using deep learning and somehow you're making predictions about the future and making decisions that might optimize something, you could say this falls under the general framework of reinforcement learning, and then it might be useful to also figure out what has been done.",
                    "label": 0
                },
                {
                    "sent": "In this field.",
                    "label": 0
                },
                {
                    "sent": "Or something?",
                    "label": 0
                },
                {
                    "sent": "Something beeping.",
                    "label": 0
                },
                {
                    "sent": "So to recap, reinforcement learning provides a framework for making decisions and predictions which would get upset if I didn't say that and then deep learning provides maybe a toolbox.",
                    "label": 1
                },
                {
                    "sent": "This is one way to think about it.",
                    "label": 0
                },
                {
                    "sent": "To learn these components.",
                    "label": 0
                },
                {
                    "sent": "And together this is sometimes positive as something to think about.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to commit.",
                    "label": 0
                },
                {
                    "sent": "I put a question Mark.",
                    "label": 0
                },
                {
                    "sent": "Is this sufficient?",
                    "label": 0
                },
                {
                    "sent": "Maybe, maybe not.",
                    "label": 0
                },
                {
                    "sent": "I haven't talked about a lot of important details right?",
                    "label": 0
                },
                {
                    "sent": "I haven't talked about, say, memory.",
                    "label": 0
                },
                {
                    "sent": "I haven't talked about knowledge, maybe more generally, but these can fit within the under these headers.",
                    "label": 0
                },
                {
                    "sent": "And then the question is, is this enough for AI?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to commit, but I encourage you to think about that, especially after all of these summer schools have concluded.",
                    "label": 0
                },
                {
                    "sent": "But concretely, we're just going to do our L, where the components are now deep neural networks.",
                    "label": 0
                },
                {
                    "sent": "So now basically my my talk is done and you can all go do something else.",
                    "label": 0
                },
                {
                    "sent": "Well it's slightly more slightly more settlement, slightly harder.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to dive a little bit in some into depth into some of the methods that have been done and partially to make you aware of what has been done.",
                    "label": 0
                },
                {
                    "sent": "But also partially, to make you aware of some of these problems and issues and interesting research questions that pop up if you when you combine deep learning with reinforcement learning to start off, I'm going to talk about deep.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Networks.",
                    "label": 0
                },
                {
                    "sent": "And to do that first, I'll quickly recap what Q learning is.",
                    "label": 0
                },
                {
                    "sent": "So this is the bellman.",
                    "label": 0
                },
                {
                    "sent": "Optimality equation.",
                    "label": 0
                },
                {
                    "sent": "Which was raised by Richard Bellman in the 50s last century.",
                    "label": 0
                },
                {
                    "sent": "And this is an equality.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is, essentially you could say definition.",
                    "label": 0
                },
                {
                    "sent": "Q star here represents your optimal value function.",
                    "label": 1
                },
                {
                    "sent": "What is optimal mean?",
                    "label": 0
                },
                {
                    "sent": "This is the value of the policy that will get you the most rewards in this system.",
                    "label": 0
                },
                {
                    "sent": "There is an expectation here.",
                    "label": 0
                },
                {
                    "sent": "Any expectation basically spends any randomness that might occur in your reward function and in the transition to the next state, which is called S T + 1 there.",
                    "label": 0
                },
                {
                    "sent": "This assumes that this is a Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "It assumes that all the information that you need is contained in the states.",
                    "label": 0
                },
                {
                    "sent": "Joel talks a little bit about what that means in practice, and also like formally what that means.",
                    "label": 0
                },
                {
                    "sent": "But if this is Mark decision process, this is an equality and you could also use this.",
                    "label": 0
                },
                {
                    "sent": "You could do dynamic programming to solve for this optimal value function if you don't know it's in advance, and essentially when your quality starts to hold, then you know that these must be the optimal values.",
                    "label": 0
                },
                {
                    "sent": "This only works.",
                    "label": 0
                },
                {
                    "sent": "You can only do that if you have access to the model.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what dynamic programming does, but we can turn this into an algorithm.",
                    "label": 1
                },
                {
                    "sent": "A temporal difference algorithm where essentially we just sample this thing within the expectation and then we make a small step.",
                    "label": 0
                },
                {
                    "sent": "So the formulation I've written down here is the tabular formulation, so you can just think of this as there is an entry in a table for each state and action separately, and we just take a small step whenever we enter this state and take that action towards this target.",
                    "label": 0
                },
                {
                    "sent": "The target here being the reward plus the discounted value of the next state.",
                    "label": 0
                },
                {
                    "sent": "But the value of the next state here is defined as taking the maximum value over all of the Action values index state.",
                    "label": 0
                },
                {
                    "sent": "This follows from the optimality equation up there.",
                    "label": 0
                },
                {
                    "sent": "It also makes a little bit intuitive sense if you think about it.",
                    "label": 0
                },
                {
                    "sent": "This way, we're trying to approximate the value of what would happen if I take this action, and then I take greedy actions with respect to my current values.",
                    "label": 0
                },
                {
                    "sent": "These taking the greedy actions with respect to my current values is sometimes called policy improvements, because this should give you a better policy than if you, for instance, take random actions.",
                    "label": 0
                },
                {
                    "sent": "And then the learning of the values here is sometimes called policy evaluation in Q learning algorithm, these things are wrapped into one updates where you're saying she doing evaluation and improvement more or less in the in the same step.",
                    "label": 0
                },
                {
                    "sent": "This is very similar to value iteration in dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "So now we have a sample based algorithm that can learn these values and these will give you approximations to the optimal values and you might not want to wait all the way until you get the optimal values.",
                    "label": 0
                },
                {
                    "sent": "It might be good enough to be good enough if you have an estimate of these optimal values.",
                    "label": 0
                },
                {
                    "sent": "This might already give you a good policy.",
                    "label": 0
                },
                {
                    "sent": "This is an off policy algorithm, with which we mean that it actually learns about a policy that might differ from the policy that you're following.",
                    "label": 0
                },
                {
                    "sent": "And this is what I meant when I said you're learning about the greedy policy with respect to your current values.",
                    "label": 0
                },
                {
                    "sent": "The greedy policy is just a policy that picks the highest value action in every state that you bump into.",
                    "label": 0
                },
                {
                    "sent": "So the greedy policy with respect to your current value just uses your current value estimates to do that.",
                    "label": 0
                },
                {
                    "sent": "What's nice about this is that it doesn't really matter what data you throw at it.",
                    "label": 0
                },
                {
                    "sent": "That's not completely true, but it's it can learn about this even if you explore, and that's very valuable, because it means if you, for instance have a lot of data, you might be able just to go through that with this algorithm and still learn about the optimal values, even if you didn't follow the optimal policy while you were collecting the data.",
                    "label": 0
                },
                {
                    "sent": "An off policy reinforcement is also very rich topic with a lot of application areas, yeah?",
                    "label": 0
                },
                {
                    "sent": "Reply yes.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "You just like to find things random variables because we're sampling, so this is no longer for any, says an action.",
                    "label": 0
                },
                {
                    "sent": "This is an update specifically for the state in action that you happened to be in.",
                    "label": 0
                },
                {
                    "sent": "So this is this is maybe I shouldn't have used inequality there actually, but it does still hold because now it's no longer.",
                    "label": 0
                },
                {
                    "sent": "It's no longer the same function because you can see all the way at the left it says Q T + 1.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially the values after the update.",
                    "label": 0
                },
                {
                    "sent": "So in that sense it's well defined, but it might be a little bit confusing the way it's written down.",
                    "label": 0
                },
                {
                    "sent": "Good question, thanks, yes.",
                    "label": 0
                },
                {
                    "sent": "Is there a reason that there's a discount factor in the second equation but not the first one?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's because I forgot to put the disk on factor in the first equation.",
                    "label": 0
                },
                {
                    "sent": "Sorry, very good question.",
                    "label": 0
                },
                {
                    "sent": "Actually deliberately first didn't have discount factors anywhere because I just wanted to basically defer talking about those, but then because they were already covered yesterday, I start plugging them in.",
                    "label": 0
                },
                {
                    "sent": "I missed one, sorry.",
                    "label": 0
                },
                {
                    "sent": "So there's no reason that there's no discount factor in the first one.",
                    "label": 0
                },
                {
                    "sent": "Good question, thanks.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Like if I collect random data, can I do like efficient cooling right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I have a.",
                    "label": 0
                },
                {
                    "sent": "This is very good question.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "Essentially, do I need to know anything about the data time collecting in order to be able to do Q learning?",
                    "label": 0
                },
                {
                    "sent": "Do I need the order?",
                    "label": 0
                },
                {
                    "sent": "Maybe some constraints on the data?",
                    "label": 0
                },
                {
                    "sent": "Can you just do anything or?",
                    "label": 0
                },
                {
                    "sent": "Going to give 2 answers for that first answer is the theoretical one which holds for the tabular case, and in this case it's actually sufficient that you cover each station action pair infinitely often in the limit.",
                    "label": 0
                },
                {
                    "sent": "And then you will learn these optimal values.",
                    "label": 0
                },
                {
                    "sent": "That's a pretty strong result because it means you basically don't care at all what your data distribution is, as long as it covers everything.",
                    "label": 0
                },
                {
                    "sent": "This only works in the tabular case, though, because if you're going to do function approximations, you're going to generalize.",
                    "label": 0
                },
                {
                    "sent": "This is the whole point of function approximation, and that means that if you're going to sample certain states actions more than others, you're essentially going to weigh the approximation error that you're going to end up with with your data distribution.",
                    "label": 0
                },
                {
                    "sent": "So if we're going to do deep learning with reinforcement learning, it's actually very important.",
                    "label": 0
                },
                {
                    "sent": "The sample data in specific ways and I'll have some concrete examples of that later on.",
                    "label": 0
                },
                {
                    "sent": "Very good question.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now this is just to reiterate, we now have.",
                    "label": 0
                },
                {
                    "sent": "A way to approximate the optimal values and just to point out what might be obvious but just good to be explicit about this we can now ascentia we get an optimal policy if we have these optimal values by simply selecting the highest valued action in each and every state.",
                    "label": 1
                },
                {
                    "sent": "Actually, this might not be completely obvious immediately, but it is through an.",
                    "label": 0
                },
                {
                    "sent": "It's good, good too.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in this and learning more about this is always good to refer to the classics, so this doesn't embarrass or textbook gives a very good introduction on this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we were talking already about deep reinforcement learning, so whereas the deep full here here it is deep here just means you have a multi layer neural network that represents this Q function.",
                    "label": 1
                },
                {
                    "sent": "So my notation here now is that there is a Q value which takes space and an action an some parameters W. These parameters are just all of the weights in your neural network.",
                    "label": 0
                },
                {
                    "sent": "I've just condensed them into a single bullet variable here vector which contains all your parameters and then one way to do the updates.",
                    "label": 0
                },
                {
                    "sent": "the Q Learning updates is essentially to change these parameters, so I could Delta W there.",
                    "label": 0
                },
                {
                    "sent": "This means I'm going to change my parameters.",
                    "label": 0
                },
                {
                    "sent": "And I put inequality, but maybe it's better just to think of this as kind of proportional to the quantity here at the right hand side.",
                    "label": 0
                },
                {
                    "sent": "El faces stepsize here, so this is like a small number.",
                    "label": 0
                },
                {
                    "sent": "We're going to take a small step.",
                    "label": 0
                },
                {
                    "sent": "This is important.",
                    "label": 0
                },
                {
                    "sent": "Because the function that we're using is nonlinear, and then a big step might actually ruin your function.",
                    "label": 0
                },
                {
                    "sent": "It's also important because the data coming in might have noise, so you want to average out this noise and therefore you want to not use too big a step size.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Quantity within the brackets is exactly the same as before, but now using my.",
                    "label": 0
                },
                {
                    "sent": "New parametric Q function as my approximation.",
                    "label": 0
                },
                {
                    "sent": "And what we do to update the parameters is simply to multiply this with the gradients of my queue function with respect to the parameters.",
                    "label": 0
                },
                {
                    "sent": "This is a fairly simple update rule.",
                    "label": 0
                },
                {
                    "sent": "There's something a little bit all about it, and I don't know if anybody notice if you notice something a little bit all, maybe you could raise your hand.",
                    "label": 0
                },
                {
                    "sent": "Edit 80 Another the next month.",
                    "label": 0
                },
                {
                    "sent": "Why's it evaluated at 80?",
                    "label": 0
                },
                {
                    "sent": "This is because.",
                    "label": 0
                },
                {
                    "sent": "This is the update to the weights after taking action 80 in state SD.",
                    "label": 0
                },
                {
                    "sent": "And this is essentially the value that we're going to update, so we're not interested in updating only the value of the maximum action it states.",
                    "label": 0
                },
                {
                    "sent": "In fact, we can't.",
                    "label": 0
                },
                {
                    "sent": "Even because we haven't perhaps taken that action, so we might not know anything meaningful about this.",
                    "label": 0
                },
                {
                    "sent": "So if we go back to the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "This is conditional on the States and the action.",
                    "label": 0
                },
                {
                    "sent": "So the target here is also considered a target for this state and action pair.",
                    "label": 0
                },
                {
                    "sent": "This is why we're updating that value, but did not make it clear.",
                    "label": 0
                },
                {
                    "sent": "A little bit.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The maximum related with while they collect current rates and activates that that that you will have like why they make double next step inside like you.",
                    "label": 0
                },
                {
                    "sent": "So why not take the next W on the next step is the question.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "Why do we take the parents here?",
                    "label": 0
                },
                {
                    "sent": "Well, this is an update for the W, so why don't we take the next W?",
                    "label": 0
                },
                {
                    "sent": "None of the current one.",
                    "label": 0
                },
                {
                    "sent": "So first thing to just just be very clear there's one set of weights here right now, so there's just a neural network with some weights, so the weights aren't dependent on states or anything like that.",
                    "label": 0
                },
                {
                    "sent": "Just to be clear on that and then to be clear on what this update is about, this is about updating the weights, so in order to update the weights, the next weights aren't available yet.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what we're going to use to get the next weights, but having said that, there are actually algorithms that kind of do this where they first do kind of a partial update, then revisit the updates.",
                    "label": 0
                },
                {
                    "sent": "And redo it.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you can get some gain from that.",
                    "label": 0
                },
                {
                    "sent": "They're not very common in deep reinforcement learning because essentially evaluating one of these functions is not completely free.",
                    "label": 0
                },
                {
                    "sent": "It's not very expensive.",
                    "label": 0
                },
                {
                    "sent": "These networks aren't huge, but you don't want to do too many forwards if you can help it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Aren't the weights power parameterising the state action pair?",
                    "label": 0
                },
                {
                    "sent": "So what's the state?",
                    "label": 0
                },
                {
                    "sent": "Outside of.",
                    "label": 0
                },
                {
                    "sent": "So yes, very good.",
                    "label": 0
                },
                {
                    "sent": "The question is whether how are the states different from the weights in some sense.",
                    "label": 0
                },
                {
                    "sent": "And so one way to think about this, I hope this clarifies, is that the States that I put in here as an input to the function is essentially your observation of the world.",
                    "label": 0
                },
                {
                    "sent": "It could be a little bit more than that.",
                    "label": 0
                },
                {
                    "sent": "It could also be the memory of your agents in addition to the observation of the world.",
                    "label": 0
                },
                {
                    "sent": "If you will.",
                    "label": 0
                },
                {
                    "sent": "And then the weights are just the parameters of your function.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Are all the parameters of only all the layers into the same vector?",
                    "label": 0
                },
                {
                    "sent": "In this notation, W here contains all the parameters lower layers just to simplify the notation, yes.",
                    "label": 0
                },
                {
                    "sent": "Well, the way to update this, you still do backpropagation.",
                    "label": 0
                },
                {
                    "sent": "This is the standard updates and actually.",
                    "label": 0
                },
                {
                    "sent": "Let me.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe just flash this up.",
                    "label": 0
                },
                {
                    "sent": "You could also just interpret this as a certain loss where basically we basically have squared loss where we have a certain target Y and we're updating our parents or Q function which are now denote it with a small Q.",
                    "label": 1
                },
                {
                    "sent": "And I apologize, but I changed weights here to thetus.",
                    "label": 0
                },
                {
                    "sent": "I should have kept that should've made at the W in literature is actually very common to use data for the ways you want to use, But these are just the parameters of the network, so this is the same queue function that we had before and then one way to interpret the exact same update that we had on the last slide is basically we're doing stochastic gradient descent with this loss, where you basically plug in a certain targets, which happens to depend on your estimated Q values as well, but otherwise it's just a target and we're just updating.",
                    "label": 0
                },
                {
                    "sent": "We're regressing towards set targets.",
                    "label": 0
                },
                {
                    "sent": "It was a question.",
                    "label": 0
                },
                {
                    "sent": "Why aren't we considering the grading for respect to the parameters in the state of our bootstrapping on?",
                    "label": 0
                },
                {
                    "sent": "That's an excellent question, and this is.",
                    "label": 0
                },
                {
                    "sent": "This is actually what I was referring to when I was saying there's something slightly odd about this updates.",
                    "label": 0
                },
                {
                    "sent": "Ann, there is a good reason for this.",
                    "label": 0
                },
                {
                    "sent": "There's actually multiple good reasons for this one.",
                    "label": 0
                },
                {
                    "sent": "One good reason for this is if we.",
                    "label": 0
                },
                {
                    "sent": "If we get rid of the Max for a moment and we consider the actual action that you take in the next 8, then this becomes the source algorithm or basically just the state action version of TD.",
                    "label": 1
                },
                {
                    "sent": "I say that just to get rid of the Max is just forget about the Max because it's nonlinear.",
                    "label": 0
                },
                {
                    "sent": "For now.",
                    "label": 0
                },
                {
                    "sent": "It's a very similar algorithm.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you could think about also taking the gradient with respect to the value of the next phase.",
                    "label": 0
                },
                {
                    "sent": "But what would that mean?",
                    "label": 0
                },
                {
                    "sent": "That would essentially mean that we're updating the value of the next states.",
                    "label": 0
                },
                {
                    "sent": "To be a better target for my update of the value of of this state and action.",
                    "label": 0
                },
                {
                    "sent": "That kind of violate causality because in some sense we want to know about the future and not necessarily we want not to make our predictions in the future to be better so that our predictions in the past were somehow better.",
                    "label": 0
                },
                {
                    "sent": "Better already for these.",
                    "label": 0
                },
                {
                    "sent": "There's also practical reason the algorithm that does take the greater respect to these problems in the next days actually tends to work less well in practice.",
                    "label": 0
                },
                {
                    "sent": "So it seems that there are some something some truth to this causality argument at least.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Can you explain why minimize this loss?",
                    "label": 0
                },
                {
                    "sent": "Connect to the.",
                    "label": 0
                },
                {
                    "sent": "Automatic.",
                    "label": 0
                },
                {
                    "sent": "Connect to the original killer impression.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So why does minimizing this loss connect into the Q learning equation?",
                    "label": 0
                },
                {
                    "sent": "Well, we could think about.",
                    "label": 0
                },
                {
                    "sent": "Just think about sampling this loss, which basically means you just have this one square term.",
                    "label": 0
                },
                {
                    "sent": "You have y -- Q ^2 and then half of that.",
                    "label": 0
                },
                {
                    "sent": "So if you want to minimize this, one way to minimize this is just to take the gradient with respect to this sort of gradient with respect to parameters of this loss and then take one step to minimize.",
                    "label": 0
                },
                {
                    "sent": "Then you get exactly this update.",
                    "label": 0
                },
                {
                    "sent": "But also, if you just think about what that means, it means that we're going to essentially move our action value function RQ function.",
                    "label": 0
                },
                {
                    "sent": "Towards a situation where my Q function at the current state action pair in expectation equal the next reward and then next state value.",
                    "label": 0
                },
                {
                    "sent": "If you manage to minimize this all the way to 0.",
                    "label": 0
                },
                {
                    "sent": "Of course, if you're doing function approximation, this is typically not possible, But in this application might actually be able to reduce it all the way to 0.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you would reduce it a little bit later, zero, essentially you get exactly that Bellman equation back.",
                    "label": 0
                },
                {
                    "sent": "One way to see that if you could move the Q value there on the left hand side of the equality to the right hand side, it gets a minus term.",
                    "label": 0
                },
                {
                    "sent": "Then of course and then you could square that term.",
                    "label": 0
                },
                {
                    "sent": "And if that if that becomes zero, that means that these things must be equal.",
                    "label": 0
                },
                {
                    "sent": "So if you're able to minimize the loss fully, you kind of get development automatically equation.",
                    "label": 0
                },
                {
                    "sent": "I say kind of because of course if you're doing function proximation, you're not going to be able to reuse it all the way to 0, and you're not going to get the equality exactly, but you will still get something related.",
                    "label": 0
                },
                {
                    "sent": "And this is this is the goal.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "You're asking about the specific in the nature paper.",
                    "label": 0
                },
                {
                    "sent": "There's a slightly different update.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about that in the ask again if it's not clear after a few slides, but I'll I'll go into that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just one more thing I wanted to say about this is that the targets are now basically considered a constant as we discussed it just now, so we're not taking the greater with respect to the parameters into targets.",
                    "label": 0
                },
                {
                    "sent": "This is very easy to implement in current a framework.",
                    "label": 0
                },
                {
                    "sent": "For instance, in Tensorflow you might just put a stop gradients on the targets and then your gradients don't flow into there.",
                    "label": 0
                },
                {
                    "sent": "And it's very easy to implement.",
                    "label": 0
                },
                {
                    "sent": "So to continue indeed.",
                    "label": 0
                },
                {
                    "sent": "So in the nature paper on the on these systems, there were two slight changes to this.",
                    "label": 0
                },
                {
                    "sent": "Important changes, one is if you compare this first equation here to the second equation on the slide, there is a difference and the difference is in the parameters at the next state value there is a new quantity.",
                    "label": 0
                },
                {
                    "sent": "Here this W which I made red with a minus in superscript.",
                    "label": 0
                },
                {
                    "sent": "And what this W is, it's essentially a copy of your parameters, which is then kept fixed for awhile.",
                    "label": 0
                },
                {
                    "sent": "So in practice, what happens every few 1000 steps we would just take the copy.",
                    "label": 0
                },
                {
                    "sent": "We take the parameters from the network, we just copy that and then we can keep keeping fixed for a few 1000 steps and then after a few 1000 steps we repeat that again.",
                    "label": 0
                },
                {
                    "sent": "So why would we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, if you don't do this.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about this a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Your targets essentially become nonstationary and this.",
                    "label": 0
                },
                {
                    "sent": "Turns out not to be a lot of work that well with these deep networks.",
                    "label": 0
                },
                {
                    "sent": "The deep deep learning doesn't like it.",
                    "label": 0
                },
                {
                    "sent": "A different thing that was also done.",
                    "label": 0
                },
                {
                    "sent": "This relates to a question that was asked earlier about the data distribution was experience replay.",
                    "label": 0
                },
                {
                    "sent": "So what we normally consider when we first explain reinforcement learning and what was also very commonly done and still is in some cases is to learn online.",
                    "label": 0
                },
                {
                    "sent": "So you basically act you get your data and then you use that data to immediately update your Q values or your policy.",
                    "label": 0
                },
                {
                    "sent": "And this is very appealing I think.",
                    "label": 0
                },
                {
                    "sent": "But you could also of course store the data comes in and then just replay this to learn more efficiently from the data.",
                    "label": 0
                },
                {
                    "sent": "Essentially you just collect some data as if you're building a datasets and then you're just learning more and more from this data set and in the queue and algorithm.",
                    "label": 0
                },
                {
                    "sent": "This was done by creating a buffer of transitions.",
                    "label": 0
                },
                {
                    "sent": "And the number of transitions in there is not maybe vitally important, but there were like a million transitions in this buffer and then just sample from this uniformly.",
                    "label": 0
                },
                {
                    "sent": "This does mean that we're sampling from a policy that is a little bit outdated on average, but that's OK because we're doing Q learning, so we're learning off policy anyway.",
                    "label": 0
                },
                {
                    "sent": "And what it does is it makes your data distribution more diverse, so it's a little bit closer to the typical IID assumption that is made when you do standard supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And again, it turns out that the deep learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Maybe work better in this case and this is a general theme that you have to realize that many of these algorithms, especially in deep learning, were developed with certain in a certain context of classification or regression, and many of the things that work really well there might not necessarily transfer.",
                    "label": 0
                },
                {
                    "sent": "If you're going to violate some of these assumptions.",
                    "label": 0
                },
                {
                    "sent": "For instance, if your data distribution is no longer ID.",
                    "label": 0
                },
                {
                    "sent": "So it's good to be aware of these things and maybe to also adapt things a little bit to make it more similar to the regime where we know these algorithms work well, yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry, how do we?",
                    "label": 0
                },
                {
                    "sent": "How we?",
                    "label": 0
                },
                {
                    "sent": "How we fill the replay buffer ascensi?",
                    "label": 0
                },
                {
                    "sent": "What happens in the standard EQ?",
                    "label": 0
                },
                {
                    "sent": "An algorithm is just.",
                    "label": 0
                },
                {
                    "sent": "There's a.",
                    "label": 0
                },
                {
                    "sent": "There's an agent taking actions in these Atari games.",
                    "label": 0
                },
                {
                    "sent": "For instance, an every transition which is then a state and action a reward.",
                    "label": 0
                },
                {
                    "sent": "Anna next state gets stored in this replay buffer.",
                    "label": 0
                },
                {
                    "sent": "Of course, you could do this efficiently by just storing the sequence so you don't have to duplicate states both at the beginning at the end of a tuple.",
                    "label": 0
                },
                {
                    "sent": "And then the learning was actually completely from the replay buffer, so the online data was put into the buffer, but the learning algorithm would read from the buff, wouldn't see, see the online data immediately, and on average each sample would be looked at more than once.",
                    "label": 0
                },
                {
                    "sent": "So on average you would see sample four times.",
                    "label": 0
                },
                {
                    "sent": "But there might be a few samples you never see.",
                    "label": 0
                },
                {
                    "sent": "There might be a few samples which get replayed more often.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll talk about that a little bit more in a moment.",
                    "label": 0
                },
                {
                    "sent": "But first I want to talk a little bit more about this target network.",
                    "label": 0
                },
                {
                    "sent": "This is how we call that network, which depends on these copied over as weights.",
                    "label": 0
                },
                {
                    "sent": "I copied this slide over from flat and I see he's using the Theta so don't get confused by sometimes it's a W, some places a theater.",
                    "label": 0
                },
                {
                    "sent": "These are just the parameters of your neural network.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing.",
                    "label": 0
                },
                {
                    "sent": "And you can also ignore the subscript I here for simplicity.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially just the same thing as we had before.",
                    "label": 0
                },
                {
                    "sent": "Phrased here as a loss so it's like a square loss.",
                    "label": 0
                },
                {
                    "sent": "We're taking the gradient with respect to current trends.",
                    "label": 0
                },
                {
                    "sent": "So what's the intuition here?",
                    "label": 0
                },
                {
                    "sent": "If you would update the parameters and you would have them both at the next state an at their current state, the same parameters, then even though you only want to update the value at this state, you kind of automatically also update the value at the next state, and this is especially true if you consider something like this and Atari game as depicted here at the bottom right.",
                    "label": 0
                },
                {
                    "sent": "The frames might actually look very similar from one set index, because were acting at a fairly fast frequency, taking about 15 or 15.",
                    "label": 0
                },
                {
                    "sent": "Actions per second.",
                    "label": 0
                },
                {
                    "sent": "And that means that actually these values might be generalizing.",
                    "label": 0
                },
                {
                    "sent": "It might be flowing over into each other, which means that if you update the value at this state almost automatically devalues the next day.",
                    "label": 0
                },
                {
                    "sent": "It also changes, and this might make your learning quite unstable.",
                    "label": 0
                },
                {
                    "sent": "So this is how intuition behind this target network where we keep these weights fix a little bit is that we update the value of Dicks this states.",
                    "label": 1
                },
                {
                    "sent": "But in the values and exit isn't automatically changed and this just makes everything a little bit more similar to regression to things that we know it works.",
                    "label": 0
                },
                {
                    "sent": "And it does really seem to help performance quite a bit.",
                    "label": 0
                },
                {
                    "sent": "There was some point someone surprising later finding is that you have.",
                    "label": 0
                },
                {
                    "sent": "If you have a bigger network with which we just mean you have more parameters in your network more hidden nodes.",
                    "label": 1
                },
                {
                    "sent": "This is less of a problem and this kind of makes sense if you think about it in terms of generalization, because now the network has more capacity so it can also maybe see the differences between these very similar states more easily, and therefore if you change the value at this stage, it doesn't automatically change the values in next states as much if your network is smaller, it needs to generalize more across all of these States and actions, and therefore you have more of this effect.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So typically what we do when we question was when we update the States.",
                    "label": 0
                },
                {
                    "sent": "There's two answers to that.",
                    "label": 0
                },
                {
                    "sent": "First, there's the online, the acting of the agents.",
                    "label": 0
                },
                {
                    "sent": "Here, the state automatically changes whenever you pick an action, so the state is just whatever the system provides you.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's a frame of the Atari game.",
                    "label": 0
                },
                {
                    "sent": "These are put into the replay buffer and what we typically do in the queue and set up is we sample sample mini batch from the replay buffer.",
                    "label": 0
                },
                {
                    "sent": "We do an update with that mini batch.",
                    "label": 0
                },
                {
                    "sent": "We toss all of them away and we sample a completely new mini batch.",
                    "label": 0
                },
                {
                    "sent": "You could also keep the same States and do like a couple of updates and this might work well.",
                    "label": 0
                },
                {
                    "sent": "I don't know that anybody is try.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Use of the target.",
                    "label": 0
                },
                {
                    "sent": "Necessary in later place.",
                    "label": 0
                },
                {
                    "sent": "So the question is, is the target network more important at the beginning when all your values are basically crap?",
                    "label": 0
                },
                {
                    "sent": "Or maybe less important later when your values have improved?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "Maybe you were able to get rid of it more and more later in learning, and there's some benefit if you could because it could speed up the process of propagating the information, because if you're going to keep these weights fixed for a little while, this doesn't mean that like newer updates aren't yet in those weights, so it's a good good idea to maybe investigate.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Bootstrapping related to aliasing.",
                    "label": 1
                },
                {
                    "sent": "If you, I mean if you're if you're system aliases anyway.",
                    "label": 0
                },
                {
                    "sent": "Or or is it was a listing only issue because of the bootstrap?",
                    "label": 0
                },
                {
                    "sent": "That's a very good question.",
                    "label": 0
                },
                {
                    "sent": "The question is ascension.",
                    "label": 0
                },
                {
                    "sent": "How do bootstrapping and aliasing or generalization?",
                    "label": 0
                },
                {
                    "sent": "How do these things interact with each other?",
                    "label": 0
                },
                {
                    "sent": "And why is there?",
                    "label": 0
                },
                {
                    "sent": "Why am I focusing on it?",
                    "label": 0
                },
                {
                    "sent": "In some sense there's definitely an interaction here because considerable Carlo case where we would do full rollouts.",
                    "label": 0
                },
                {
                    "sent": "Let's just consider prediction.",
                    "label": 0
                },
                {
                    "sent": "So forget the off policy bits here.",
                    "label": 0
                },
                {
                    "sent": "You could still consider doing bootstrapping or getting a full Monte Carlo Rolos, and you could still consider doing generalizations, so doing function approximation.",
                    "label": 0
                },
                {
                    "sent": "Aliasing the generalization here is important.",
                    "label": 0
                },
                {
                    "sent": "How much you alias?",
                    "label": 0
                },
                {
                    "sent": "Because the learning the guests from the guests actually changes, not just your bias variance tradeoff that rich rich doesn't talked about yesterday.",
                    "label": 0
                },
                {
                    "sent": "It's in some sense, but it also changes.",
                    "label": 0
                },
                {
                    "sent": "Exactly what the capacity is of that architecture.",
                    "label": 1
                },
                {
                    "sent": "Bootstrapping on so in some sense, there's actually there's a more formal way to talk about this as well.",
                    "label": 0
                },
                {
                    "sent": "An one thing that's been investigated quite well is temporal difference methods with linear function approximation, and there you can actually show that the fixed point, so the eventual solution that the system converges to depends on how much you bootstrap.",
                    "label": 0
                },
                {
                    "sent": "If you bootstrap more, you're going to actually end up in a different ends.",
                    "label": 0
                },
                {
                    "sent": "Answer All the way at the end.",
                    "label": 0
                },
                {
                    "sent": "Even then, if you don't bootstrap, it'll.",
                    "label": 0
                },
                {
                    "sent": "There's a tradeoff there where learning is still faster with bootstrapping, but in the end the actual solution you're gets might be eventually better if you would just do the Monte Carlo thing we're doing, pure Monte Carlo is very data inefficient, so we wouldn't want to do that.",
                    "label": 0
                },
                {
                    "sent": "Of course, here is a different thing that's going on, which is the off policy learning for which we kind of maybe need to bootstrap a little bit more than if you would just be doing predictions.",
                    "label": 0
                },
                {
                    "sent": "So there is another confounding factor there.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "That's a good question.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do you decide what to keep your replay buffer?",
                    "label": 0
                },
                {
                    "sent": "And this is a little bit of another investigator to topic.",
                    "label": 0
                },
                {
                    "sent": "I think what was done was just the most recent million frames.",
                    "label": 0
                },
                {
                    "sent": "And this does by issue.",
                    "label": 0
                },
                {
                    "sent": "It does by issue towards what your policy has done in the somewhat recent past, and it might not be optimal way, it's related to something else that I want to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, which is also related to the experience we pay, because in addition to deciding what is in the replay, even if there's lots of stuff in the replay, let's go to the extreme.",
                    "label": 0
                },
                {
                    "sent": "And let's say we never thought anything away.",
                    "label": 0
                },
                {
                    "sent": "We just put everything in the replayed, then there's still this question, which is very related to the questions you were asking.",
                    "label": 0
                },
                {
                    "sent": "Which things do you pick to even learn from?",
                    "label": 0
                },
                {
                    "sent": "So the tossing wave data is similar because if you throw it away, you can never learn from it again.",
                    "label": 0
                },
                {
                    "sent": "But even if these days is there, it doesn't mean you actually have to learn about each sample equally much.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "There's benefits to doing experience replay, which were quite clear from the from the results.",
                    "label": 0
                },
                {
                    "sent": "So first to recap that it's more data efficient because the data is used more than once on average four times in the standard equipment set up, and it resembles supervised learning.",
                    "label": 1
                },
                {
                    "sent": "More so for this reason the deep learning property works better.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just a list of improvement that related on the decline, which I won't go through step by step, but I'll highlight a few and this is going back again to the question about the data distribution and the experience replay.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one thing I wanted to highlight first is that we can basically think of this replay differently.",
                    "label": 0
                },
                {
                    "sent": "We can think of it as a nonparametric model of the environment and there are some benefits to thinking about it this way.",
                    "label": 1
                },
                {
                    "sent": "Because now if we consider it a model, we can think about querying the model.",
                    "label": 0
                },
                {
                    "sent": "It's a nonparametric model, so you don't actually get from a state in an action.",
                    "label": 0
                },
                {
                    "sent": "What the expectation is over next reward States and so on.",
                    "label": 0
                },
                {
                    "sent": "But you get a sample from this.",
                    "label": 0
                },
                {
                    "sent": "The sample that you actually saw in the past.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "There is multiple ways you can use this and I wanted to highlight 2 examples of this.",
                    "label": 1
                },
                {
                    "sent": "One is to sample Nonuniformly I was already.",
                    "label": 1
                },
                {
                    "sent": "Are pointing towards that and this is school prioritized replay and this really helps a lot in terms of performance and one way to do that is, well.",
                    "label": 0
                },
                {
                    "sent": "First, let me mention the second one.",
                    "label": 0
                },
                {
                    "sent": "You could even use this model to kind of plan.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you could, for instance, look at a state in action.",
                    "label": 0
                },
                {
                    "sent": "You could look at what happens then and you could take this as an indication of what might happen again if you're in a similar or in the same state in action and maybe you can even even compose these things together.",
                    "label": 0
                },
                {
                    "sent": "This is not yet a fully fleshed.",
                    "label": 0
                },
                {
                    "sent": "I don't think the last word has been said about this research.",
                    "label": 0
                },
                {
                    "sent": "There's a lot more that can be done also, especially when you want to.",
                    "label": 0
                },
                {
                    "sent": "Maybe then distill this down into a parametric model, which is a very interesting Avenue, I think.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But to talk a little bit more about this prioritized replay.",
                    "label": 0
                },
                {
                    "sent": "So what, for instance, could you do?",
                    "label": 0
                },
                {
                    "sent": "You could do something which is fairly simple.",
                    "label": 0
                },
                {
                    "sent": "You could sort of temporal difference error of each of these transitions, and this is basically a snapshot of how much would you learn from this transition, and then when you sample that you might learn maybe this amount, but it also gives you an indication of maybe which things you want to sample, because just consider the situation where this temporal difference error is zero, which means that just by chance the state action value that you had here.",
                    "label": 0
                },
                {
                    "sent": "Accurately predicts that the one step reward and the resulting state value.",
                    "label": 0
                },
                {
                    "sent": "That you end up in if you're going to sample this from your replay, you're going to learn exactly nothing, because your error will be 0 and your update will therefore be 0.",
                    "label": 0
                },
                {
                    "sent": "That's obviously a waste.",
                    "label": 0
                },
                {
                    "sent": "If instead you pick something right information areas high, you're going to do a comparatively bigger update to your weights.",
                    "label": 0
                },
                {
                    "sent": "In addition, this is kind of the stuff that you want to learn about, because these are the transitions that were.",
                    "label": 0
                },
                {
                    "sent": "These were inaccurate, and thereby if you learn about this, you might learn more about the world's than you would otherwise.",
                    "label": 0
                },
                {
                    "sent": "This is just one way to prioritize an you have to be careful a little bit hard to pick this because the Atari games are actually.",
                    "label": 0
                },
                {
                    "sent": "Deterministic, so if you do assertions in a certain state, do certain action you deterministically end up in end up in the next state.",
                    "label": 0
                },
                {
                    "sent": "And this is more or less irrelevant to most of the learning algorithms, but its relevance, for instance for this part, because if you consider a situation where there is actually very noisy environments, if you would use exactly this prioritization, it might just replayed annoys a lot, especially if the noise is nonuniform.",
                    "label": 0
                },
                {
                    "sent": "There's some very noisy transitions and others aren't.",
                    "label": 0
                },
                {
                    "sent": "So then maybe you want to tweak the prioritization like you might still want to prioritize your replay.",
                    "label": 0
                },
                {
                    "sent": "If you're doing experience replay.",
                    "label": 1
                },
                {
                    "sent": "A different thing that I wanted to highlight the question first.",
                    "label": 0
                },
                {
                    "sent": "Can yeah so could you learn maybe which things to sample?",
                    "label": 0
                },
                {
                    "sent": "Which things to look at.",
                    "label": 0
                },
                {
                    "sent": "This is definitely.",
                    "label": 0
                },
                {
                    "sent": "Yes, I would say.",
                    "label": 0
                },
                {
                    "sent": "How less clear would be an interesting area of research.",
                    "label": 0
                },
                {
                    "sent": "I don't know that I don't know that as been done there yet.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "For example, these two graphs on the right.",
                    "label": 0
                },
                {
                    "sent": "They probably represent like.",
                    "label": 0
                },
                {
                    "sent": "I'm ready to break.",
                    "label": 0
                },
                {
                    "sent": "Independent games are samples, but there is no error bars and that's something that throws me off guard within some of the like your paper will be great.",
                    "label": 0
                },
                {
                    "sent": "This maybe they all within one standard deviation 20 Saturday or not.",
                    "label": 0
                },
                {
                    "sent": "Yes yes sure.",
                    "label": 0
                },
                {
                    "sent": "So you often see many of such curves an indeed often there aren't error bars.",
                    "label": 0
                },
                {
                    "sent": "In practice, this is often the case because there is only one seed.",
                    "label": 0
                },
                {
                    "sent": "So you might run.",
                    "label": 0
                },
                {
                    "sent": "So what we typically do, we take like 57 Atari games or these days at least in the nature paper was 49, but we added a few and then we run these algorithms.",
                    "label": 0
                },
                {
                    "sent": "All of these games and one way to think about it is that we're kind of doing an analysis on the whole set of games, so we're still doing 57 different things, but we're not doing, say, 30 samples per game.",
                    "label": 0
                },
                {
                    "sent": "Ideally we would write, but it's just so much more compute that's.",
                    "label": 0
                },
                {
                    "sent": "Let's say you do 5 for you.",
                    "label": 0
                },
                {
                    "sent": "Only take one seat and that's it.",
                    "label": 0
                },
                {
                    "sent": "Or you take the best before.",
                    "label": 0
                },
                {
                    "sent": "Oh no.",
                    "label": 0
                },
                {
                    "sent": "So yeah, no we don't take the best seed.",
                    "label": 0
                },
                {
                    "sent": "We just take one seed and then we take whatever that gives you, taking the best seat I would say is much more dodgy.",
                    "label": 0
                },
                {
                    "sent": "There are different approaches, so in some cases some people have, for instance done random hyperparameter search and then it becomes a little bit more Gray area.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you just want to pick the best hyperparameters we have to be a little bit careful because you might be maximizing over things.",
                    "label": 0
                },
                {
                    "sent": "So then what's often done is you pick basically best couple of hyper parameters and then maybe you pick like what are the third best is like the three best and it gives you some sense of how robust the algorithm is, but the details are often.",
                    "label": 0
                },
                {
                    "sent": "Phrased explicitly in the papers, but as I said, we would love to do these things, maybe with a lot more samples, but it's prohibitively.",
                    "label": 0
                },
                {
                    "sent": "Compute intensive.",
                    "label": 0
                },
                {
                    "sent": "Throw you off 'cause there's a lot of variance when you're doing enforcement.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "Extra credit why, but yeah.",
                    "label": 0
                },
                {
                    "sent": "So there are some variants definitely, so you shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't pay too much attention to like the specific numbers.",
                    "label": 0
                },
                {
                    "sent": "Too far down to the right hand side of the number.",
                    "label": 0
                },
                {
                    "sent": "In my experience daughters.",
                    "label": 0
                },
                {
                    "sent": "Many games such as PAC Man is a well known one or Miss Pacman where these learning curves are actually very consistent.",
                    "label": 0
                },
                {
                    "sent": "An if you improve your algorithm, you gotta learning curve that's basically consistently above the other one.",
                    "label": 0
                },
                {
                    "sent": "And if you have a bug in your algorithm is basically consistently not learning, so these games are apparently a little bit more robust to these things than I would have expected, maybe beforehand before working with them, but it's definitely something to keep in mind, because this doesn't hold for all domains and for other domains it's going to be even Wilder.",
                    "label": 0
                },
                {
                    "sent": "It also depends on your algorithm alot.",
                    "label": 0
                },
                {
                    "sent": "Some algorithms are more susceptible to these things to noise.",
                    "label": 0
                },
                {
                    "sent": "In some sense, yes.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned this.",
                    "label": 0
                },
                {
                    "sent": "Are you talking about the environment?",
                    "label": 0
                },
                {
                    "sent": "So the question is, are we talking about the seed for learning or seeds for the environments?",
                    "label": 0
                },
                {
                    "sent": "I typically mean there's just like, for instance, you might randomly pick a seed and then this is used for both the environment and the algorithm.",
                    "label": 0
                },
                {
                    "sent": "However, like I said before, Atari is actually deterministic.",
                    "label": 0
                },
                {
                    "sent": "So what we often do in Atari to make it less.",
                    "label": 0
                },
                {
                    "sent": "We want robust algorithms that can work in general domain, so we don't want to exploit the determinism of the Atari games essentially.",
                    "label": 0
                },
                {
                    "sent": "So one thing that's often done this we you might take a couple of random no OP actions at the beginning so that you start off slightly differently.",
                    "label": 0
                },
                {
                    "sent": "For instance, you might take 30 actions or up to 30 actions that don't do anything before you actually start playing, and this kind of breaks the determinism in newer version of the newer versions of the arcades learning environment, which.",
                    "label": 0
                },
                {
                    "sent": "Has these Atari games actually features a possibility to also add noise?",
                    "label": 0
                },
                {
                    "sent": "Which makes it, which completely breaks is determinism.",
                    "label": 0
                },
                {
                    "sent": "Just to make it a better testbed for algorithms that don't want to exploit that.",
                    "label": 0
                },
                {
                    "sent": "I'm not changing the model parameter.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so typically we would we do if we would start anew afresh.",
                    "label": 0
                },
                {
                    "sent": "New algorithm you would pick a different random seeds for your learning and this would automatically change everything because your policy would be different.",
                    "label": 0
                },
                {
                    "sent": "There are few data distribution is different and therefore for instance anything any randomness that may or may not suit environment size maybe already matters less because you're already doing different things in this environment.",
                    "label": 0
                },
                {
                    "sent": "Yeah, is this spread to overfitting on special cases that shouldn't be fit like overfit?",
                    "label": 0
                },
                {
                    "sent": "I guess maybe like obscured cases that you only see a few times in the data, but since you're so.",
                    "label": 0
                },
                {
                    "sent": "This is prone, more prone to overfitting.",
                    "label": 0
                },
                {
                    "sent": "Well, I think there's more generally.",
                    "label": 0
                },
                {
                    "sent": "If your system, if your task allows you to find a certain policy which.",
                    "label": 0
                },
                {
                    "sent": "Exploits certain things it will.",
                    "label": 0
                },
                {
                    "sent": "It will latch onto that.",
                    "label": 0
                },
                {
                    "sent": "It will try to find that I don't think overfitting is so much of a problem in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Because essentially typically what we do is we have a certain task, especially in this case we have a simulator, right?",
                    "label": 0
                },
                {
                    "sent": "We have these computer games that you can play overfitting.",
                    "label": 0
                },
                {
                    "sent": "This game is essentially impossible because this just means you're better at it, right?",
                    "label": 0
                },
                {
                    "sent": "So you can't really say that you're it's less common to have this very clean train set test set divide in reinforcement learning, because just doing better at the task is just doing better at the task and that can never be overfitting in some sense.",
                    "label": 0
                },
                {
                    "sent": "It depends a little bit on your specific setup, so there are cases in which it is a problem.",
                    "label": 0
                },
                {
                    "sent": "In this case it's not so much of a problem.",
                    "label": 0
                },
                {
                    "sent": "This for very many good questions, but I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Continue to see if we can cover a little bit more ground.",
                    "label": 0
                },
                {
                    "sent": "This is another example of something that helps improve things.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And for this, I first wanted to just rewrite the update that is done in the queue and.",
                    "label": 0
                },
                {
                    "sent": "This is making it slightly more explicit, something that I said already, which is that we're actually evaluating a certain policy of policy and what we're evaluating is the greedy policy.",
                    "label": 0
                },
                {
                    "sent": "But since we're using this target network with this W minus as parameters will actually evaluating the greedy policy with respect to our current parameters that we're actively updating.",
                    "label": 0
                },
                {
                    "sent": "But we're evaluating the greedy policy with respect to the target network.",
                    "label": 0
                },
                {
                    "sent": "This is what this arc Max means in the middle there were picking the highest valued action.",
                    "label": 0
                },
                {
                    "sent": "According to the parameters of my target network, this W minus and then we're evaluating those actions with that same network.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit dangerous, because now we're essentially we have these random action values which depend on your learning process and potential noise in your environments, but also just some things like the initialization of your network.",
                    "label": 0
                },
                {
                    "sent": "And then we're saying we're going to pick something that maximizes that, and then we're going to use the exact same values that we used to pick to evaluate that.",
                    "label": 0
                },
                {
                    "sent": "This means that it's actually more likely to pick something that is highly valued and therefore get a higher value.",
                    "label": 0
                },
                {
                    "sent": "It turns out this is a real bias.",
                    "label": 0
                },
                {
                    "sent": "And then there's a way to prevent it very easily in the case of TQM, because we already have two sets of parameters.",
                    "label": 0
                },
                {
                    "sent": "What I did here at the bottom equation I just colored the black W as we had before.",
                    "label": 0
                },
                {
                    "sent": "These are the things that we're actively updating.",
                    "label": 0
                },
                {
                    "sent": "Each updates, I color them blue an all the way inside.",
                    "label": 0
                },
                {
                    "sent": "In this arc Max.",
                    "label": 0
                },
                {
                    "sent": "I'm now using instead of.",
                    "label": 0
                },
                {
                    "sent": "This slowly moving target network parameters.",
                    "label": 0
                },
                {
                    "sent": "I'm using the online parameters.",
                    "label": 0
                },
                {
                    "sent": "What this means is I'm now evaluating the greedy policy with respect to my current.",
                    "label": 0
                },
                {
                    "sent": "Parameters.",
                    "label": 0
                },
                {
                    "sent": "With the parameters that are slowly more slowly moving, the idea is to break the correlation a little bit between the selection and evaluation.",
                    "label": 1
                },
                {
                    "sent": "And this should then reduce this overestimation bias that you might otherwise get.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this turns out to actually really work.",
                    "label": 0
                },
                {
                    "sent": "This is 2 Atari games and what you see here on the X axis is the estimated value according to your current action values and Y axis is on the log scale.",
                    "label": 0
                },
                {
                    "sent": "And what you see here is that in red DQ and at some point the values could go quite hugely up.",
                    "label": 0
                },
                {
                    "sent": "And you might think if you're just looking at the values you might think oh, this is good preparing me.",
                    "label": 0
                },
                {
                    "sent": "I'm learning the values go up.",
                    "label": 0
                },
                {
                    "sent": "So apparently my policy is improving.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But turns out almost exactly at the point where these values start to increase greatly.",
                    "label": 0
                },
                {
                    "sent": "My actual performance goes down, and there there is now a very big mismatch between the actual rewards that you get and the rewards that your value function says it's going to get.",
                    "label": 0
                },
                {
                    "sent": "This is something you could notice if you're tracking these statistics, you could basically see hey, my value estimates are now way over estimating the actual values that I'm guessing.",
                    "label": 0
                },
                {
                    "sent": "If you get rid of this overestimation bias, the performance drop also goes away.",
                    "label": 0
                },
                {
                    "sent": "So apparently the performance drop was completely due to this overestimation bias.",
                    "label": 0
                },
                {
                    "sent": "In this case, which is kind of interesting.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if we apply this, all these Atari games, this is basically performance, graphic full of all of those 57 Atari games.",
                    "label": 0
                },
                {
                    "sent": "The details are not too important, but if you apply this trick.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turns out it works pretty much across the board, so it wasn't just like random game where this happened.",
                    "label": 0
                },
                {
                    "sent": "Apparently this overestimation bias bias was quite prevalent.",
                    "label": 0
                },
                {
                    "sent": "This was a little bit surprising to me.",
                    "label": 0
                },
                {
                    "sent": "I hadn't expected to be so severe.",
                    "label": 0
                },
                {
                    "sent": "In the take home message, I wanted to give in this is that it's good to be aware of the properties of your learning algorithms, in this case in Q learning, where taking the Max over something that is good just to stop and think for a moment while we're taking the Max offer.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "What could then happen?",
                    "label": 0
                },
                {
                    "sent": "In addition, is very good to track and to analyze statistics.",
                    "label": 0
                },
                {
                    "sent": "In this case we could be tracking these action values, and in addition perhaps the performance and maybe see how is my prediction error behaving?",
                    "label": 0
                },
                {
                    "sent": "Is it going up?",
                    "label": 0
                },
                {
                    "sent": "Is it going down?",
                    "label": 0
                },
                {
                    "sent": "It's perfectly OK if your prediction error goes up during learning.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit weird if you're coming from a supervised learning background, because then you kind of expect your loss function continues to go down, but in reinforcement learning because you're changing your policy is perfectly acceptable that when your policy improves, your predictions are a little bit outdated and your prediction error might actually go up.",
                    "label": 0
                },
                {
                    "sent": "This is fine, but at some point we should at least stabilize, go down again or your policy should continue to improve, both of which you could track.",
                    "label": 0
                },
                {
                    "sent": "If at some point your values go way up in your prediction error goes way up.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, your policy is not improving.",
                    "label": 0
                },
                {
                    "sent": "Then there might be something wrong and then you could basically analyze your online statistics to find that out, yeah?",
                    "label": 0
                },
                {
                    "sent": "That spectacle yeah, it's a very good question.",
                    "label": 0
                },
                {
                    "sent": "So what are we breaking the target network?",
                    "label": 0
                },
                {
                    "sent": "I'm breaking the benefit of the target network in some sense by now making the targets more nonstationary again.",
                    "label": 0
                },
                {
                    "sent": "So there's actually two ways you could do this.",
                    "label": 0
                },
                {
                    "sent": "If you look at the equation here, what I mentioned is I wanted to break the correlation.",
                    "label": 0
                },
                {
                    "sent": "I wanted to break the selection from the evaluation and there's two ways to do that.",
                    "label": 0
                },
                {
                    "sent": "If you do not have to networks, we could either pick the greedy policy with respect to my online parameters and then evaluate that with the slowly moving parameters.",
                    "label": 0
                },
                {
                    "sent": "It's our growth parameters or the other way around.",
                    "label": 0
                },
                {
                    "sent": "And it might not be immediately clear which one works better.",
                    "label": 0
                },
                {
                    "sent": "Turns out this one works much better, and the reason is probably twofold.",
                    "label": 0
                },
                {
                    "sent": "One is that we're evaluating the policy, which is greedy with respect to my current online parameters, and these are also the parameters that we actually used to act with.",
                    "label": 0
                },
                {
                    "sent": "So it kind of makes intuitive sense that this is also the policy that we actually care about, so it makes sense to evaluate that one in addition, and perhaps more importantly, for the learning process, maybe it's OK if my target change a little bit overtime in terms of how the policy changes.",
                    "label": 0
                },
                {
                    "sent": "And maybe it's less OK if the value is also change and this is what we're doing here, we're keeping the value function fixed.",
                    "label": 0
                },
                {
                    "sent": "We're just changing the policy that the value function is evaluating, and this is apparently more stable.",
                    "label": 0
                },
                {
                    "sent": "But this was indeed I actually did.",
                    "label": 0
                },
                {
                    "sent": "Also, try the other version, because I wasn't immediately convinced that this is the only way that would work.",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "The gradients are going to be different.",
                    "label": 0
                },
                {
                    "sent": "The updates are going to going to actually be different.",
                    "label": 0
                },
                {
                    "sent": "But it's still OK.",
                    "label": 0
                },
                {
                    "sent": "It's still.",
                    "label": 0
                },
                {
                    "sent": "It's still.",
                    "label": 0
                },
                {
                    "sent": "It's still a reasonable thing to do.",
                    "label": 0
                },
                {
                    "sent": "And more generally, you could think of this update with the target network more or less is a fine thing to do, regardless of the policy that you're trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "Even if the policies itself might be fairly quickly changing, this is a harder estimation problem.",
                    "label": 0
                },
                {
                    "sent": "Your prediction errors will be harder if your policy changes very frequently, but in this case, even if we updated parameters, the policy might not change that much.",
                    "label": 0
                },
                {
                    "sent": "An one way to see that we're taking this argmax if your action values are like if one action is true.",
                    "label": 0
                },
                {
                    "sent": "Truly clearly a lot better than the other ones.",
                    "label": 0
                },
                {
                    "sent": "We might improve the prediction errors on all of these actions, but it wouldn't actually change my policy, so it's not actually clear that my policy is changing that quickly anyway.",
                    "label": 0
                },
                {
                    "sent": "It will depend on the game will depend on the specifics, but it might already be slow enough to be stable.",
                    "label": 0
                },
                {
                    "sent": "There might definitely be cases in which is not true, and then it's useful to do those things that I said later to track statistics, just to keep track of what's working, what is not working and so on.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And freezing.",
                    "label": 0
                },
                {
                    "sent": "But the general.",
                    "label": 0
                },
                {
                    "sent": "So the general intuition is just to get rid of this overestimation bias.",
                    "label": 0
                },
                {
                    "sent": "That's basically it.",
                    "label": 0
                },
                {
                    "sent": "An double DQ and I would say this specific implementation is just one way to do that.",
                    "label": 0
                },
                {
                    "sent": "If you want to get rid of your estimation bias in a different way.",
                    "label": 0
                },
                {
                    "sent": "What these plots basically tell me is that it's about the overestimation bias.",
                    "label": 0
                },
                {
                    "sent": "If you can get rid of it in a different way, that might be perfectly fine and you might not want to use double DQ and, But that's the main intuition and this is just what I wanted to do here with.",
                    "label": 0
                },
                {
                    "sent": "The update is basically find the very simplest way, the smallest change to the algorithm to get rid of that overestimation bias, not necessarily find the best way.",
                    "label": 0
                },
                {
                    "sent": "But also to keep most of the other stuff fixed so that we understand that it's truly this thing that matters.",
                    "label": 0
                },
                {
                    "sent": "That was the intuition.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "There are convergence analysis for many of these algorithms, especially for the tabular cases.",
                    "label": 0
                },
                {
                    "sent": "Before double DQ and I keep proposed double Q learning, which is essentially the same thing, and there's also convergence analysis for that particular case.",
                    "label": 0
                },
                {
                    "sent": "The off policy algorithms.",
                    "label": 0
                },
                {
                    "sent": "Have less guarantees in the function approximation case then the on policy algorithms.",
                    "label": 0
                },
                {
                    "sent": "So for the TD algorithm that rich talks about yesterday, the pure prediction one.",
                    "label": 0
                },
                {
                    "sent": "There's also a bunch of convergence results also with function approximation.",
                    "label": 0
                },
                {
                    "sent": "There are some convergence results with non linear function approximation but already less so.",
                    "label": 0
                },
                {
                    "sent": "Most of these results not all of them, but most of these results only concern the asymptotics which may or may not be of interest to you.",
                    "label": 0
                },
                {
                    "sent": "It basically tells you whether or not the update is stable.",
                    "label": 0
                },
                {
                    "sent": "These things are a little bit important, especially to understand the cases when they're not stable, when they might actually blow up and diverge.",
                    "label": 0
                },
                {
                    "sent": "That said, there are known cases in which combining Q learning with non linear function approximation is unstable and the parameters might diverge.",
                    "label": 0
                },
                {
                    "sent": "But as we've seen many times in practice by now, if you're careful enough, this doesn't need to happen.",
                    "label": 0
                },
                {
                    "sent": "The fact that there already is counterexamples doesn't mean that it's inherently unstable, and another will never work.",
                    "label": 0
                },
                {
                    "sent": "For instance.",
                    "label": 0
                },
                {
                    "sent": "There's there's a very good book from 1996 by vertical Synthetic Lease.",
                    "label": 0
                },
                {
                    "sent": "They call it Neurodynamic programming, and this is essentially about reinforcement learning with function approximation.",
                    "label": 1
                },
                {
                    "sent": "They have a lot of results.",
                    "label": 0
                },
                {
                    "sent": "There's a new book about particles from 2007 in which basically has an updated version of many of these results as well.",
                    "label": 0
                },
                {
                    "sent": "An much more if you're interested in that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Those are good pointers.",
                    "label": 0
                },
                {
                    "sent": "In addition, there is a shorter book by Chavez Obituary, who also will be speaking here later today, and he has.",
                    "label": 0
                },
                {
                    "sent": "Many has done much much work in this area as well.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Oh yeah, the last point I still wanted to mention.",
                    "label": 0
                },
                {
                    "sent": "If you understand what the problem is in this case the overestimation.",
                    "label": 1
                },
                {
                    "sent": "Sometimes you can do a very simple thing to fix it.",
                    "label": 0
                },
                {
                    "sent": "An for instance, in WCU, and one of the reasons why specifically did this one.",
                    "label": 0
                },
                {
                    "sent": "It's a one line change of code, and that's very nice if you can just OK, it's just a hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If I change this, I'm going to change this property that you can expect, inspect, and then you know what you've done and you know what the impact is.",
                    "label": 0
                },
                {
                    "sent": "Changing too many things at the same time.",
                    "label": 0
                },
                {
                    "sent": "It might work, but it's much harder to understand what happens and why these things work.",
                    "label": 0
                },
                {
                    "sent": "And then it's hard in the next problem to know exactly what you need to do.",
                    "label": 0
                },
                {
                    "sent": "This fits into a more general theme that I would just want to highlight.",
                    "label": 0
                },
                {
                    "sent": "There's two of these things that are a little bit related, but different views on the same thing.",
                    "label": 0
                },
                {
                    "sent": "The target networks and the experience replay you could view them as deep learning aware reinforcement learning where basically changing the updates a little bit to be more similar to what we know.",
                    "label": 0
                },
                {
                    "sent": "That works with deep learning experience.",
                    "label": 0
                },
                {
                    "sent": "Replay in some sense makes it more IID, and we know that that's something that these deep networks in nonlinear functions in regression like the target networks, make the targets more stationary.",
                    "label": 0
                },
                {
                    "sent": "You know that that's something that these these systems work well with.",
                    "label": 0
                },
                {
                    "sent": "Similarly, but in the opposite view is reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning aware deep learning where maybe you want to change, say the optimizer that you're using.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to change your network structure to be more conducive to what you're trying to do with this.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On that Latin on that second note, here is an example of that.",
                    "label": 0
                },
                {
                    "sent": "Which is during the Q and the actual title paper.",
                    "label": 0
                },
                {
                    "sent": "Title is a bit longer.",
                    "label": 0
                },
                {
                    "sent": "Dueling network architectures for deep reinforcement learning from last year's ICL.",
                    "label": 1
                },
                {
                    "sent": "The idea is very simple.",
                    "label": 0
                },
                {
                    "sent": "You can think of the action value function as a combination of the value of this state under a certain policy.",
                    "label": 0
                },
                {
                    "sent": "All of this is under the same policy.",
                    "label": 0
                },
                {
                    "sent": "This is the first equation over there, plus the advantage of picking a certain action.",
                    "label": 0
                },
                {
                    "sent": "And for simplicity, let's just say that these advantages are zero mean.",
                    "label": 0
                },
                {
                    "sent": "That means that their value function is now basically learning the offsets.",
                    "label": 0
                },
                {
                    "sent": "How important is it to be in this state and the advantages then learn how much of an advantage do you get?",
                    "label": 0
                },
                {
                    "sent": "In terms of value for picking this specific action in that state.",
                    "label": 0
                },
                {
                    "sent": "Now you could write it down slightly differently, which is the 2nd equation there and what we what's being done.",
                    "label": 0
                },
                {
                    "sent": "There is essentially taking away 1 degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "This is essentially making sure that these values the value function indeed is trying to estimate the baseline.",
                    "label": 0
                },
                {
                    "sent": "Assuming that the advantages are roughly 0 mean, and then if you want to have accurate Q values, these advantages will almost automatically then navigate towards being 0 mean to to fit nicely within the within the loss that we're specifying now.",
                    "label": 0
                },
                {
                    "sent": "What's kind of cool about this is that we have action values back right?",
                    "label": 0
                },
                {
                    "sent": "We just basically decompose the action.",
                    "label": 0
                },
                {
                    "sent": "Using the specific way and one way to implement that is just to change the network structure and this is what's depicted there at the bottom.",
                    "label": 0
                },
                {
                    "sent": "The first thing that you see there is the typical deep network where there is a common flare or a couple of players, and then there's a fully connected layer in the dueling network architecture.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same, but we split the fully connected layer.",
                    "label": 0
                },
                {
                    "sent": "And then we go into a value function and into these advantage functions we just add these together to get my act together action value functions.",
                    "label": 0
                },
                {
                    "sent": "And that's all.",
                    "label": 0
                },
                {
                    "sent": "And then we do the same WN update with prioritized repair without reproach replay.",
                    "label": 0
                },
                {
                    "sent": "On top of that.",
                    "label": 0
                },
                {
                    "sent": "And this works, this helps, which is kind of interesting, because it always doesn't change the network architecture.",
                    "label": 0
                },
                {
                    "sent": "We didn't change the update in any other way, but by changing the network architecture where basically we give a bias to the system to represent things in a certain way, and this way happens to be a good nice way to represent the value function and what I'll show you now with this video.",
                    "label": 0
                },
                {
                    "sent": "I hope it's a little visible.",
                    "label": 0
                },
                {
                    "sent": "What you see on the left hand side here?",
                    "label": 0
                },
                {
                    "sent": "This is a game of enduro.",
                    "label": 0
                },
                {
                    "sent": "It's a racing game from this from Guitar suite of games and what you see on the left here is basically the gradient with respect to the inputs of your value function.",
                    "label": 0
                },
                {
                    "sent": "This is sometimes called saliency map.",
                    "label": 0
                },
                {
                    "sent": "One way to think about this this is what my algorithm is paying attention to to get the value and what you see is that the vision.",
                    "label": 0
                },
                {
                    "sent": "In some sense, the saliency is all the way at the end of the road where the road is heading towards the right hand side is the advantage functions is basically what is my.",
                    "label": 0
                },
                {
                    "sent": "The advantages of each of every action paying attention to in the current screen.",
                    "label": 0
                },
                {
                    "sent": "You don't see any red.",
                    "label": 0
                },
                {
                    "sent": "There's nothing there, but if you watch what I press play, you'll see it flash sometimes.",
                    "label": 0
                },
                {
                    "sent": "And maybe I can actually stop it.",
                    "label": 0
                },
                {
                    "sent": "Well, slightly too late there maybe I'll come grab another one.",
                    "label": 0
                },
                {
                    "sent": "So sometimes you see flash when there's cars nearby while grabbing it.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, it seems that the value function is paying attention to things that happen way.",
                    "label": 0
                },
                {
                    "sent": "At the end, whereas the advantages are mostly paying attention, things that happen very close by, this makes sense because the action differences between different actions are much more pronounced when there's cars nearby when it actually matters to your value, which actually take most of the time, it doesn't really matter that much which actually take if there's no cars nearby.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter, you could pick any of them.",
                    "label": 0
                },
                {
                    "sent": "The differences are very small.",
                    "label": 0
                },
                {
                    "sent": "And what we see here is essentially that the system has now learned to use different parts to pay attention to different parts of the of the problem.",
                    "label": 0
                },
                {
                    "sent": "And this then helps performance.",
                    "label": 0
                },
                {
                    "sent": "Turns out again, this is very simple change to the algorithm, so even to the learning updates.",
                    "label": 0
                },
                {
                    "sent": "But just to the network architecture and this helps performance because we know that it has a certain semantics to it.",
                    "label": 0
                },
                {
                    "sent": "We know what it's trying to do, yes.",
                    "label": 0
                },
                {
                    "sent": "The target for the value in the advantage.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's the latter.",
                    "label": 0
                },
                {
                    "sent": "The question is, do we have separate targets for the value and the advantage?",
                    "label": 0
                },
                {
                    "sent": "Or do we just have one target for the queue functions?",
                    "label": 0
                },
                {
                    "sent": "It's the latter, but actually you could write this down because it's just going in the sense so you could ride this out and squared loss on your Q values and you could decompose that into a basically a loss on your value and a loss in your advantage, but these are not related to these two losses.",
                    "label": 0
                },
                {
                    "sent": "They depend on each other because the actual losses on the action values there is a very good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So why do we?",
                    "label": 0
                },
                {
                    "sent": "Why do we get rid of the average advantage in the Q value?",
                    "label": 0
                },
                {
                    "sent": "So the idea there is.",
                    "label": 0
                },
                {
                    "sent": "We want the value function.",
                    "label": 0
                },
                {
                    "sent": "We basically want to force the value function to really be the value of the States and the advantage is just to be the offsets.",
                    "label": 0
                },
                {
                    "sent": "So we kind of want the advantage to be 0 mean and this is one way to do that because in order for this value function to be accurate because we're deducting the mean advantage, it needs to be that value.",
                    "label": 0
                },
                {
                    "sent": "It's conscious being arbitrary thing.",
                    "label": 0
                },
                {
                    "sent": "If you don't do this, you have an additional degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "What this means in practice is that, for instance, you could increase all your advantages and decrease the value and get exactly the same Q values.",
                    "label": 0
                },
                {
                    "sent": "Turns out if you run that in practice, it doesn't seem that harmful, but it turns out it works less well.",
                    "label": 0
                },
                {
                    "sent": "It's not maybe completely clear or understood why, but it makes a little bit intuitive sense, and then in practice it does turn out to be important.",
                    "label": 0
                },
                {
                    "sent": "It's a good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so many of these algorithms actually pay attention to the score that's on the screen.",
                    "label": 0
                },
                {
                    "sent": "It's not completely clear why, but in some games it might actually be a very good indication of how much more rewards you can accrue.",
                    "label": 0
                },
                {
                    "sent": "For instance, how far along in the left leg are sometimes these scores can almost be seen as kind of like a Clock, but it depends on the game and I honestly don't quite know why it's paying attention to it here enduro, but it's quite a common thing.",
                    "label": 0
                },
                {
                    "sent": "We see that preceded a lot.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yes.",
                    "label": 0
                },
                {
                    "sent": "So soothing, like if you were to put some partial pooling over, yeah?",
                    "label": 0
                },
                {
                    "sent": "I have a similar factor.",
                    "label": 0
                },
                {
                    "sent": "So this is a form of smoothing I don't know.",
                    "label": 0
                },
                {
                    "sent": "I hadn't thought about it like that.",
                    "label": 0
                },
                {
                    "sent": "I'm more than just see it like a semantic decomposition of the value into two different parts.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's something there, maybe there's like a small.",
                    "label": 0
                },
                {
                    "sent": "So what I would encourage, especially if people start playing with these things, is for instance to take hypothesis like this.",
                    "label": 0
                },
                {
                    "sent": "Or maybe maybe this is good because it's doing some form of smoothing and maybe then there's a way to do just the smoothing, like very very constrained way and see if that that also helps.",
                    "label": 0
                },
                {
                    "sent": "Whether this has a similar benefit.",
                    "label": 0
                },
                {
                    "sent": "I don't immediately see it here.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Mostly running out of sign.",
                    "label": 0
                },
                {
                    "sent": "So let's see what's on.",
                    "label": 0
                },
                {
                    "sent": "Let's",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep, that.",
                    "label": 0
                },
                {
                    "sent": "There's one more thing that I wanted to.",
                    "label": 0
                },
                {
                    "sent": "Mentioned this is mostly stuff the piece reveals talks about yesterday with the policy gradients.",
                    "label": 0
                },
                {
                    "sent": "This is the same derivation that he had on the slide, so I'm not going to go through that again.",
                    "label": 0
                },
                {
                    "sent": "But I just wanted to highlight again that there's different ways you could.",
                    "label": 0
                },
                {
                    "sent": "You could learn a policy, and one way is to do that directly and let me just jump in basically here.",
                    "label": 0
                },
                {
                    "sent": "The algorithm here is calls reinforce.",
                    "label": 0
                },
                {
                    "sent": "It has different names, but.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Common name is reinforce.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is from a paper from 1992.",
                    "label": 0
                },
                {
                    "sent": "And it gives you sample based way to update your policy.",
                    "label": 0
                },
                {
                    "sent": "This is different from what we talked about before because we're not learning a value function anymore.",
                    "label": 0
                },
                {
                    "sent": "We're directly updating policy.",
                    "label": 0
                },
                {
                    "sent": "And the important equations here.",
                    "label": 0
                },
                {
                    "sent": "So the first one here.",
                    "label": 0
                },
                {
                    "sent": "The first equation here basically says J.",
                    "label": 0
                },
                {
                    "sent": "Here is maybe standard notation in some parts, but you can just think of this as the value of.",
                    "label": 0
                },
                {
                    "sent": "Your policy under those parameters.",
                    "label": 0
                },
                {
                    "sent": "Basically across states, so Jay doesn't depend on state, but just think of it as the expected value across all states.",
                    "label": 0
                },
                {
                    "sent": "Anne then turns out this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Derivation that we had before that's Peter talks about yesterday.",
                    "label": 0
                },
                {
                    "sent": "Basically shows us that we can sample the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing, which is why you see on the right hand side there is an expectation around something and this expectation has the true Q value in there and the greater with respect to the logarithm of your policy.",
                    "label": 0
                },
                {
                    "sent": "What this essentially means one way to think about this is that what happens to your policy parameters is if your value function is high, you should increase the probability of selecting that action.",
                    "label": 0
                },
                {
                    "sent": "If it's low, you should either decrease the probability of selecting the action or just increase it less and then by normalization it turns out to decrease.",
                    "label": 0
                },
                {
                    "sent": "And this is the whole idea behind it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Force algorithm.",
                    "label": 0
                },
                {
                    "sent": "You could, as a baseline, which typically is just a value function.",
                    "label": 1
                },
                {
                    "sent": "This reduces the variance, but it doesn't change the direction of your gradient, so this is a fine thing to do.",
                    "label": 1
                },
                {
                    "sent": "Peter also mentioned this in his talk.",
                    "label": 0
                },
                {
                    "sent": "And the intuition is still exactly the same.",
                    "label": 0
                },
                {
                    "sent": "We're still increasing the probability of actions that were better now, better than expected with respect to the baseline, and then we're decreasing the probability of actions that were worse than expected with respect to our baseline.",
                    "label": 0
                },
                {
                    "sent": "It's not unclear.",
                    "label": 1
                },
                {
                    "sent": "Database line needs to be very accurate because otherwise we would use increasing certain actions and others less, and then in the net effect is maybe still the same.",
                    "label": 0
                },
                {
                    "sent": "But picking a good baseline is important for the variance of the system, yes.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's very good question.",
                    "label": 1
                },
                {
                    "sent": "So how does the baseline decrease the variance?",
                    "label": 0
                },
                {
                    "sent": "The idea is actually quite similar to the Jeweling architecture that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "The baseline here basically keeps the variance due to the states out of the system.",
                    "label": 0
                },
                {
                    "sent": "The returns will have variance because your your actions will give you like a random return and maybe the system is also some noise.",
                    "label": 0
                },
                {
                    "sent": "Is there will be variance in that.",
                    "label": 0
                },
                {
                    "sent": "But in addition the states might all have different different values and this just gives you additional variance if you don't take this out.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a way to think about it.",
                    "label": 0
                },
                {
                    "sent": "The randomness due to which state you're in is taken care of by the baseline and therefore the variance is lower.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm skipping over this.",
                    "label": 0
                },
                {
                    "sent": "This is about can you do replay in that?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think you can, but let's first talk about when we don't do replay.",
                    "label": 0
                },
                {
                    "sent": "So what is an alternative to doing replay?",
                    "label": 0
                },
                {
                    "sent": "So think about why we were doing replay.",
                    "label": 0
                },
                {
                    "sent": "This is again about talking about whether the properties of these algorithms when do we expect them to behave well, why we were doing replay in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So there's two reasons.",
                    "label": 0
                },
                {
                    "sent": "One is data efficiency, but the other one was we want the data to be a little bit more diverse, more like what we know that works well with deep learning.",
                    "label": 0
                },
                {
                    "sent": "But there's more than one way to do that, and this is a different approach.",
                    "label": 0
                },
                {
                    "sent": "You could think also about just storing many simulate.",
                    "label": 0
                },
                {
                    "sent": "Sorry, having many simulates relations happening at the same time and having more or less your same agents acting in all of these simulators at the same time.",
                    "label": 0
                },
                {
                    "sent": "In parallel.",
                    "label": 0
                },
                {
                    "sent": "This would also give you diverse data.",
                    "label": 0
                },
                {
                    "sent": "In a completely different way, but it's still diverse day, so maybe this still works.",
                    "label": 0
                },
                {
                    "sent": "And then this was investigated by Vlad and others an they found that this does really work even if you just then.",
                    "label": 0
                },
                {
                    "sent": "Pretty wildly updated parameters from all of these different threads which are happening asynchronously on a single machine or many CPUs perhaps.",
                    "label": 0
                },
                {
                    "sent": "And you can basically combine this with any reinforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Just this is like a separate thing.",
                    "label": 0
                },
                {
                    "sent": "Whether you do replay or whether you do this parallel environment thing.",
                    "label": 0
                },
                {
                    "sent": "This is separate from whether you do, say, reinforce or Q learning, but they both are.",
                    "label": 0
                },
                {
                    "sent": "Combinable with each other and this turns out to work really well.",
                    "label": 0
                },
                {
                    "sent": "Skip through this, but.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll show you a slide.",
                    "label": 0
                },
                {
                    "sent": "This is the slide I wanted to show you where now at the bottom left you basically see these networks which now have two heads.",
                    "label": 0
                },
                {
                    "sent": "One is a policy head and one is value head.",
                    "label": 1
                },
                {
                    "sent": "This is similar to this dueling network architecture, but instead of having advantages and the value we now have this policy and a value and then we can use the.",
                    "label": 1
                },
                {
                    "sent": "The reinforce updates, or an approximation derolf which is the equation there at the top.",
                    "label": 0
                },
                {
                    "sent": "The top equation on the right where we don't take the full return but reduce the variance even a little bit further by bootstrapping a little bit short you could do the one step version, but you could also do the multi step version which was used here and then you could basically tweak how many steps you will get a good tradeoff between the variance in the bias.",
                    "label": 0
                },
                {
                    "sent": "And I just wanted to highlight this for its similarity and also that it's a different approach to do something similar to get the similar benefits from the replay.",
                    "label": 0
                },
                {
                    "sent": "I understand we need to stop here for coffee.",
                    "label": 0
                },
                {
                    "sent": "But feel free to come up to me for questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}