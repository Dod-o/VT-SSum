{
    "id": "chmkbgwoinskax6ww6vlfvevbphitnox",
    "title": "Imitation Learning by Model-based Probabilistic Trajectory Matching",
    "info": {
        "author": [
            "Marc Peter Deisenroth, Department of Computing, Imperial College London"
        ],
        "published": "Aug. 6, 2013",
        "recorded": "April 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/machine_deisenroth_imitation_learning/",
    "segmentation": [
        [
            "Hello, my name is Mark Dice.",
            "Note I'm going to present some work jointly with Peter Englert, Alexanders, Parashas and Jan Peters on imitation learning by probabilistic trajectory matching.",
            "It's an imitation learning we are interested in."
        ],
        [
            "Kind of.",
            "Recreating demonstrated behavior by.",
            "So for example, you can imagine a human demonstrates a behavior we want to have the robot to do something that looks very similar to the demonstrated behavior.",
            "There are several classical methods of dealing with this.",
            "One is inverse reinforcement learning and one is behavioral cloning, but they kind of do a detour of solving this problem.",
            "Take a detour.",
            "So investment for some learning tries to learn or infer the cost function of the human, and then afterwards tries to find or make the robot minimize this cost function and then basically hope by minimizing this cost function to generate a trajectory that looks similar to what the human did.",
            "Behavior cloning solves a supervised learning problem, but it's also an indirect way of solving this problem.",
            "This this similarity problem that we have in there by minimizing some sort of distance between demonstrated trajectory and the robots trajectory, and we can learn robot specific controller by doing this OK."
        ],
        [
            "The problem with it doesn't show the problem with real systems is that if we demonstrate things a few times, we have a lot of noise in the demonstrations we need to account for this, and also if we have a robot and we have a model of the robot, we have uncertainties in this system as well.",
            "So there are two types of uncertainties in the in the model.",
            "So model uncertainties because we are uncertain about the model itself and also stochastic city in the in the model.",
            "So that's a reason why we need probabilistic trajectory matching here."
        ],
        [
            "We approximate the demonstrated trajectory probability distribution over trajectory SPY.",
            "Some sort of Gaussian distributions that we do maximum likelihood estimation of the of the Gaussian parameters mean and variances.",
            "And for the robot trajectory we use Gaussian processes and predict them long term ahead.",
            "So we do approximate inference for creating these predicted trajectory's.",
            "And then the objective in our imitation learning setup is to minimize the KL divergent between the probability distribution over predicted trajectories and the probability distribution over demonstrated trajectories."
        ],
        [
            "The nice thing with this and also with these Gaussian approximations in there, is that you can that you can write down this imitation learning cost function of minimizing the KL divergent between trajectories.",
            "As the sum of individual KL divergences at each time step between probability distributions at each time step, and if you want, you can consider this a reinforcement learning cost function with a naturally induced cost function.",
            "Usually, if we do reinforcement learning, we write down some, for example time squared distance between a set point or so, but in this case if we just take this reward function to be the KL divergent between two state distributions, then we can phrase this imitation learning problem as a reinforcement learning problem and we can use reinforcement learning algorithms to solve this imitation learning problem.",
            "In this case, we use a model based policy search method to solve this imitation learning problem by indirectly by by reinforcement learning, and we applied this to table tennis hitting movements.",
            "We have a tenant driven robot, so we have a robot arm that is driven by tenants, has a lot of Springs, is really nasty to model, but we have we have a demonstrator who demonstrates how to hit a table tennis ball in this setup, and the robot can by using this.",
            "Imitation learning with trajectory matching approach can very closely imitate this behavior, so the demonstration is shown on the top and the imitative behavior is shown at the bottom, and the only thing you should see is that they are very similar to each other.",
            "We have also other applications, but I think this is the most most interesting one.",
            "OK, if you want to know more about this, please come to our poster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, my name is Mark Dice.",
                    "label": 0
                },
                {
                    "sent": "Note I'm going to present some work jointly with Peter Englert, Alexanders, Parashas and Jan Peters on imitation learning by probabilistic trajectory matching.",
                    "label": 1
                },
                {
                    "sent": "It's an imitation learning we are interested in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of.",
                    "label": 0
                },
                {
                    "sent": "Recreating demonstrated behavior by.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can imagine a human demonstrates a behavior we want to have the robot to do something that looks very similar to the demonstrated behavior.",
                    "label": 0
                },
                {
                    "sent": "There are several classical methods of dealing with this.",
                    "label": 0
                },
                {
                    "sent": "One is inverse reinforcement learning and one is behavioral cloning, but they kind of do a detour of solving this problem.",
                    "label": 1
                },
                {
                    "sent": "Take a detour.",
                    "label": 0
                },
                {
                    "sent": "So investment for some learning tries to learn or infer the cost function of the human, and then afterwards tries to find or make the robot minimize this cost function and then basically hope by minimizing this cost function to generate a trajectory that looks similar to what the human did.",
                    "label": 0
                },
                {
                    "sent": "Behavior cloning solves a supervised learning problem, but it's also an indirect way of solving this problem.",
                    "label": 1
                },
                {
                    "sent": "This this similarity problem that we have in there by minimizing some sort of distance between demonstrated trajectory and the robots trajectory, and we can learn robot specific controller by doing this OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem with it doesn't show the problem with real systems is that if we demonstrate things a few times, we have a lot of noise in the demonstrations we need to account for this, and also if we have a robot and we have a model of the robot, we have uncertainties in this system as well.",
                    "label": 0
                },
                {
                    "sent": "So there are two types of uncertainties in the in the model.",
                    "label": 0
                },
                {
                    "sent": "So model uncertainties because we are uncertain about the model itself and also stochastic city in the in the model.",
                    "label": 0
                },
                {
                    "sent": "So that's a reason why we need probabilistic trajectory matching here.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We approximate the demonstrated trajectory probability distribution over trajectory SPY.",
                    "label": 0
                },
                {
                    "sent": "Some sort of Gaussian distributions that we do maximum likelihood estimation of the of the Gaussian parameters mean and variances.",
                    "label": 0
                },
                {
                    "sent": "And for the robot trajectory we use Gaussian processes and predict them long term ahead.",
                    "label": 0
                },
                {
                    "sent": "So we do approximate inference for creating these predicted trajectory's.",
                    "label": 0
                },
                {
                    "sent": "And then the objective in our imitation learning setup is to minimize the KL divergent between the probability distribution over predicted trajectories and the probability distribution over demonstrated trajectories.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The nice thing with this and also with these Gaussian approximations in there, is that you can that you can write down this imitation learning cost function of minimizing the KL divergent between trajectories.",
                    "label": 0
                },
                {
                    "sent": "As the sum of individual KL divergences at each time step between probability distributions at each time step, and if you want, you can consider this a reinforcement learning cost function with a naturally induced cost function.",
                    "label": 0
                },
                {
                    "sent": "Usually, if we do reinforcement learning, we write down some, for example time squared distance between a set point or so, but in this case if we just take this reward function to be the KL divergent between two state distributions, then we can phrase this imitation learning problem as a reinforcement learning problem and we can use reinforcement learning algorithms to solve this imitation learning problem.",
                    "label": 0
                },
                {
                    "sent": "In this case, we use a model based policy search method to solve this imitation learning problem by indirectly by by reinforcement learning, and we applied this to table tennis hitting movements.",
                    "label": 0
                },
                {
                    "sent": "We have a tenant driven robot, so we have a robot arm that is driven by tenants, has a lot of Springs, is really nasty to model, but we have we have a demonstrator who demonstrates how to hit a table tennis ball in this setup, and the robot can by using this.",
                    "label": 0
                },
                {
                    "sent": "Imitation learning with trajectory matching approach can very closely imitate this behavior, so the demonstration is shown on the top and the imitative behavior is shown at the bottom, and the only thing you should see is that they are very similar to each other.",
                    "label": 0
                },
                {
                    "sent": "We have also other applications, but I think this is the most most interesting one.",
                    "label": 0
                },
                {
                    "sent": "OK, if you want to know more about this, please come to our poster.",
                    "label": 0
                }
            ]
        }
    }
}