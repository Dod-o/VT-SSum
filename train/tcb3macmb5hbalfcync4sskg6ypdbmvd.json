{
    "id": "tcb3macmb5hbalfcync4sskg6ypdbmvd",
    "title": "Generating Images from Captions with Attention",
    "info": {
        "author": [
            "Elman Mansimov, Department of Computer Science, University of Toronto"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_mansimov_generating_images/",
    "segmentation": [
        [
            "Thank you so a Melman and I'm going to be presenting our joint work with my colleagues at University of Toronto, Emilio, Jimmy and Russ on generating images from captions so."
        ],
        [
            "Why are we interested in this problem?",
            "So I guess apart from being intellectually interesting.",
            "Captions offer several advantages over unsupervised or models conditioned on labels and mainly captions contain more information about it and therefore.",
            "It's easier to model the image when you have more information about it.",
            "And that thing is.",
            "You can better understand model generalization by constructing some weird captions which aren't in the training set, something like."
        ],
        [
            "A stop sign is flying in the blue skies and the yellow bus is flying in the blue skies and you can see that the model generates.",
            "Something were relevant to whatever was asked in the caption and can generalize to those out of box examples."
        ],
        [
            "So the general idea kind of takes inspiration from recent advances in neural machine translation, and it's kind of a part of a sequence to sequence framework.",
            "So effectively we have a sentence which is represented as a sequence of words and we have an image which is represented as a sequence of patches which are slowly accumulated on canvas.",
            "And the challenge with images is that you also need to figure out where you're going to put those patches in the canvas overtime.",
            "So going."
        ],
        [
            "A bit more into detail, so the language model is a standard bidirectional STM where we have like 1 forward Dallas TM reading sentence from left to right and one back for Dallas team reading sentence from right to left and then basically virtual representation is just concatenation of both forward and backward LCMS.",
            "And for simplicity you can consider for now the sentence representation to be just an average of each of those vert representations."
        ],
        [
            "Going a bit into like the image model is a bit more sophisticated, so effectively it kind of consists of two parts.",
            "One is an inference network which takes an image and in which infers the latent representations of it, which is kind of conventional encoder an you have generative network, which kind of takes those latent representations and tries to generate an image.",
            "And I guess the difference here is that instead of.",
            "Kind of reading the whole image in one time step you read kind of small P by P patches and then.",
            "You generate when you generate.",
            "You also generate those P by P patches an you slowly accumulate them.",
            "So effectively this is draw model with other tricks that was presented at last year's eye CMO.",
            "Anne."
        ],
        [
            "You can have a very simple version of the model by just concatenating both this language psan model P and the image piece together where you just conditioned generative units by the sentence representation.",
            "And the model is trained by backpropagation, and it's trained to maximize the lower bound of likelihood which you heard a lot about yesterday.",
            "So effectively it kind of consists of one piece is a reconstruction, the likelihood piece, which is a standard reconstruction in autoencoders, Ann.",
            "You have a second KL divergences term, which is kind of encourages the latent representations to follow specific predefined prior distribution and.",
            "Since at the test time we won't have a ground truth image, we're going to discard our inference network and sample from our prior distribution, so it's better if the posterior is as close as possible to the prior."
        ],
        [
            "So we can add more tricks to it that can potentially improve performance.",
            "One is basically alignment which kind of instead of having a fixed sentence representation, what we can do, we can dynamically update it at each time step, and so basically the motivation is to encourage the model to look at certain more relevant verts when it generates some Patch and the way to do it is we're going to have another multilayer perceptron between the.",
            "Generative units kind of like an hour vert representations.",
            "That's going to pretty much output the probabilities for each of the vert.",
            "An instead of kind of just averaging all the verts, we're going to just take a weighted sum of those vert representations.",
            "And the other cheek."
        ],
        [
            "You can do to improve the quality of images is to kind of have another post processing step to kind of do super resolution over them, so it's kind of well known that mean squared error has its own drawbacks where.",
            "It whenever it misses the position in the image, it kind of starts averaging and outputs the blurry image.",
            "So in the in our case where we did, we kind of trained another kind of network which is like adversarial Network which was explicitly trained to take a blurry image and generate The Sharper Image.",
            "Yeah.",
            "So you can tell."
        ],
        [
            "Kind of all the pieces together and combine them into one kind of complete model."
        ],
        [
            "So going a bit more into experiments, majority of them were done on the Microsoft Coco data set, which is kind of a fairly decently sized.",
            "Data set which has around 85,000 images and each image had around 5 captions and it was pretty much the standard benchmark of all the recent.",
            "Image captioning systems that you've seen.",
            "So after kind of training this model and just looking after the samples we try to do something more interesting and what we did is we kind of took a sentence.",
            "And fixed all the words except for one and try changing it and seeing whether the model makes relevant changes.",
            "So effectively what it means, you just flip to bits in the sentence so it can have."
        ],
        [
            "Like you can take, I don't know a caption a school bus parked in the parking lot and change the color from yellow to red to green to blue and it changes the color so it generates a blob, which kind of course has a bus shape.",
            "But when changing the color it makes the relevant changes in the image too.",
            "So."
        ],
        [
            "We do more kind of similar experiments where you can take something like a commercial airplane flying in the clear skies and you can change clear skies to rainy skies, or you can take a herd of elephants walking in the dry grass field and change like dry grass field to green grass field and just by changing one vert it makes relevant changes in the image."
        ],
        [
            "So you can do more experiments like that, and that thing is where you can look like.",
            "Look at is which verts does the model.",
            "Pay attention to while generating that image and unfortunately when generating those kind of patches.",
            "There is not really connection between the vert, it looks like but on."
        ],
        [
            "Average you can see that something like in the top left case, a writer on the Blue motorcycle in a desert.",
            "Most of the time it looks at the relevant words, such as like motorcycle and desert.",
            "An ignores other more or less irrelevant stuff."
        ],
        [
            "They are the kind of fun experiments you can do is you can have some kind of a loop interaction between kind of text to image model an image to text models where you can generate an image, something very large airplane in the clear skies on the top and you can throw that image into the captioning system and see what it generates.",
            "And sometimes it generates the relevant thing that I asked at the 1st place.",
            "So like in the first case it also generates that the airplane in the blue skies, but often it just generates something irrelevant, and if you run this loop for like a long time, it will just be completely off.",
            "So."
        ],
        [
            "Going a bit more into quantitative side to get a better understanding which model performs better, we kind of considered image retrieval task where.",
            "Kind of, we tried different sentence representations.",
            "So instead of kind of we had tried sentence representation without alignment or some pre trained skip thought representation and we also tried different generative models such as variational encoder.",
            "So instead of recurrent neural network kind of generate and read the whole image in one time step an you can see that on the medium rank.",
            "So it's the column before the last one.",
            "You can see that the model.",
            "The draw model, which is the recurrent model, performs better than just a simple feedforward model, while at the same time sentence represent like.",
            "When you consider different sentence representation, the medium rank stays the same.",
            "But and if you compare that those sentence representations, those medium rank results with other image to text or image to text models, the results are much worse.",
            "So I think like I want showed that usually the rank in medium rank is top five, so don't don't really use those models for image retrieval.",
            "And you can do kind of other kind of experiments where we can consider that each caption in the test set is independent and you can kind of generate image and compare the generated image with the true image.",
            "What using more fence he ran better metric rather than MSE, which is called structural similarity index and this way you can compare with other models which can compute likelihoods such of such as generative adversarial networks, and you can see that in the last column.",
            "Also, like our model performs, has a little bit better scores.",
            "And again, you can do more."
        ],
        [
            "Quantitative experiments where you also calculate the log likelihood and compare the log likelihood of the models and you can see that again, sentence representation doesn't really make such a huge difference in terms of your scores, which is kind of understandable because Microsoft Coco sentences are usually very short and not very complicated so.",
            "Technically there is not not really need for very complicated sentence representations.",
            "Ann you."
        ],
        [
            "And do kind of a more qualitative experiments where you compare different models where you can just say comparing them with variational autoencoders.",
            "We just generate noise.",
            "I really bad and you can compare them with the current lab gun models, which is generative.",
            "Adversarial networks would generate better images than VIA is, but still there quite noisy with sharper and this is kind of our model and this is kind of the images corresponding to captions, like a group of people walking on the beach."
        ],
        [
            "And I guess to conclude that kind of the generated images are still small 32 by 32 bulbs and it's still far from what we want in ideal.",
            "Having like maybe 1024 by 1024 very clear images.",
            "So there's still a lot of work to be done in that direction, but.",
            "At least the good thing is that the model generalizes to out of box examples and examples that were never seen in the training set.",
            "And we personally believe that it's probably due to underfitting that we're not really pushing the generative model to it limits.",
            "So the."
        ],
        [
            "The code is available on GitHub, so you can just download it and play with it, so thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so a Melman and I'm going to be presenting our joint work with my colleagues at University of Toronto, Emilio, Jimmy and Russ on generating images from captions so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why are we interested in this problem?",
                    "label": 0
                },
                {
                    "sent": "So I guess apart from being intellectually interesting.",
                    "label": 0
                },
                {
                    "sent": "Captions offer several advantages over unsupervised or models conditioned on labels and mainly captions contain more information about it and therefore.",
                    "label": 1
                },
                {
                    "sent": "It's easier to model the image when you have more information about it.",
                    "label": 0
                },
                {
                    "sent": "And that thing is.",
                    "label": 1
                },
                {
                    "sent": "You can better understand model generalization by constructing some weird captions which aren't in the training set, something like.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A stop sign is flying in the blue skies and the yellow bus is flying in the blue skies and you can see that the model generates.",
                    "label": 0
                },
                {
                    "sent": "Something were relevant to whatever was asked in the caption and can generalize to those out of box examples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general idea kind of takes inspiration from recent advances in neural machine translation, and it's kind of a part of a sequence to sequence framework.",
                    "label": 0
                },
                {
                    "sent": "So effectively we have a sentence which is represented as a sequence of words and we have an image which is represented as a sequence of patches which are slowly accumulated on canvas.",
                    "label": 1
                },
                {
                    "sent": "And the challenge with images is that you also need to figure out where you're going to put those patches in the canvas overtime.",
                    "label": 0
                },
                {
                    "sent": "So going.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit more into detail, so the language model is a standard bidirectional STM where we have like 1 forward Dallas TM reading sentence from left to right and one back for Dallas team reading sentence from right to left and then basically virtual representation is just concatenation of both forward and backward LCMS.",
                    "label": 0
                },
                {
                    "sent": "And for simplicity you can consider for now the sentence representation to be just an average of each of those vert representations.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going a bit into like the image model is a bit more sophisticated, so effectively it kind of consists of two parts.",
                    "label": 0
                },
                {
                    "sent": "One is an inference network which takes an image and in which infers the latent representations of it, which is kind of conventional encoder an you have generative network, which kind of takes those latent representations and tries to generate an image.",
                    "label": 0
                },
                {
                    "sent": "And I guess the difference here is that instead of.",
                    "label": 0
                },
                {
                    "sent": "Kind of reading the whole image in one time step you read kind of small P by P patches and then.",
                    "label": 0
                },
                {
                    "sent": "You generate when you generate.",
                    "label": 0
                },
                {
                    "sent": "You also generate those P by P patches an you slowly accumulate them.",
                    "label": 0
                },
                {
                    "sent": "So effectively this is draw model with other tricks that was presented at last year's eye CMO.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can have a very simple version of the model by just concatenating both this language psan model P and the image piece together where you just conditioned generative units by the sentence representation.",
                    "label": 0
                },
                {
                    "sent": "And the model is trained by backpropagation, and it's trained to maximize the lower bound of likelihood which you heard a lot about yesterday.",
                    "label": 1
                },
                {
                    "sent": "So effectively it kind of consists of one piece is a reconstruction, the likelihood piece, which is a standard reconstruction in autoencoders, Ann.",
                    "label": 0
                },
                {
                    "sent": "You have a second KL divergences term, which is kind of encourages the latent representations to follow specific predefined prior distribution and.",
                    "label": 0
                },
                {
                    "sent": "Since at the test time we won't have a ground truth image, we're going to discard our inference network and sample from our prior distribution, so it's better if the posterior is as close as possible to the prior.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can add more tricks to it that can potentially improve performance.",
                    "label": 0
                },
                {
                    "sent": "One is basically alignment which kind of instead of having a fixed sentence representation, what we can do, we can dynamically update it at each time step, and so basically the motivation is to encourage the model to look at certain more relevant verts when it generates some Patch and the way to do it is we're going to have another multilayer perceptron between the.",
                    "label": 0
                },
                {
                    "sent": "Generative units kind of like an hour vert representations.",
                    "label": 0
                },
                {
                    "sent": "That's going to pretty much output the probabilities for each of the vert.",
                    "label": 0
                },
                {
                    "sent": "An instead of kind of just averaging all the verts, we're going to just take a weighted sum of those vert representations.",
                    "label": 0
                },
                {
                    "sent": "And the other cheek.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do to improve the quality of images is to kind of have another post processing step to kind of do super resolution over them, so it's kind of well known that mean squared error has its own drawbacks where.",
                    "label": 0
                },
                {
                    "sent": "It whenever it misses the position in the image, it kind of starts averaging and outputs the blurry image.",
                    "label": 0
                },
                {
                    "sent": "So in the in our case where we did, we kind of trained another kind of network which is like adversarial Network which was explicitly trained to take a blurry image and generate The Sharper Image.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So you can tell.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of all the pieces together and combine them into one kind of complete model.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So going a bit more into experiments, majority of them were done on the Microsoft Coco data set, which is kind of a fairly decently sized.",
                    "label": 0
                },
                {
                    "sent": "Data set which has around 85,000 images and each image had around 5 captions and it was pretty much the standard benchmark of all the recent.",
                    "label": 1
                },
                {
                    "sent": "Image captioning systems that you've seen.",
                    "label": 1
                },
                {
                    "sent": "So after kind of training this model and just looking after the samples we try to do something more interesting and what we did is we kind of took a sentence.",
                    "label": 0
                },
                {
                    "sent": "And fixed all the words except for one and try changing it and seeing whether the model makes relevant changes.",
                    "label": 0
                },
                {
                    "sent": "So effectively what it means, you just flip to bits in the sentence so it can have.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like you can take, I don't know a caption a school bus parked in the parking lot and change the color from yellow to red to green to blue and it changes the color so it generates a blob, which kind of course has a bus shape.",
                    "label": 1
                },
                {
                    "sent": "But when changing the color it makes the relevant changes in the image too.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We do more kind of similar experiments where you can take something like a commercial airplane flying in the clear skies and you can change clear skies to rainy skies, or you can take a herd of elephants walking in the dry grass field and change like dry grass field to green grass field and just by changing one vert it makes relevant changes in the image.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can do more experiments like that, and that thing is where you can look like.",
                    "label": 0
                },
                {
                    "sent": "Look at is which verts does the model.",
                    "label": 0
                },
                {
                    "sent": "Pay attention to while generating that image and unfortunately when generating those kind of patches.",
                    "label": 0
                },
                {
                    "sent": "There is not really connection between the vert, it looks like but on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Average you can see that something like in the top left case, a writer on the Blue motorcycle in a desert.",
                    "label": 1
                },
                {
                    "sent": "Most of the time it looks at the relevant words, such as like motorcycle and desert.",
                    "label": 0
                },
                {
                    "sent": "An ignores other more or less irrelevant stuff.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They are the kind of fun experiments you can do is you can have some kind of a loop interaction between kind of text to image model an image to text models where you can generate an image, something very large airplane in the clear skies on the top and you can throw that image into the captioning system and see what it generates.",
                    "label": 1
                },
                {
                    "sent": "And sometimes it generates the relevant thing that I asked at the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So like in the first case it also generates that the airplane in the blue skies, but often it just generates something irrelevant, and if you run this loop for like a long time, it will just be completely off.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going a bit more into quantitative side to get a better understanding which model performs better, we kind of considered image retrieval task where.",
                    "label": 0
                },
                {
                    "sent": "Kind of, we tried different sentence representations.",
                    "label": 0
                },
                {
                    "sent": "So instead of kind of we had tried sentence representation without alignment or some pre trained skip thought representation and we also tried different generative models such as variational encoder.",
                    "label": 0
                },
                {
                    "sent": "So instead of recurrent neural network kind of generate and read the whole image in one time step an you can see that on the medium rank.",
                    "label": 0
                },
                {
                    "sent": "So it's the column before the last one.",
                    "label": 0
                },
                {
                    "sent": "You can see that the model.",
                    "label": 0
                },
                {
                    "sent": "The draw model, which is the recurrent model, performs better than just a simple feedforward model, while at the same time sentence represent like.",
                    "label": 0
                },
                {
                    "sent": "When you consider different sentence representation, the medium rank stays the same.",
                    "label": 0
                },
                {
                    "sent": "But and if you compare that those sentence representations, those medium rank results with other image to text or image to text models, the results are much worse.",
                    "label": 0
                },
                {
                    "sent": "So I think like I want showed that usually the rank in medium rank is top five, so don't don't really use those models for image retrieval.",
                    "label": 0
                },
                {
                    "sent": "And you can do kind of other kind of experiments where we can consider that each caption in the test set is independent and you can kind of generate image and compare the generated image with the true image.",
                    "label": 0
                },
                {
                    "sent": "What using more fence he ran better metric rather than MSE, which is called structural similarity index and this way you can compare with other models which can compute likelihoods such of such as generative adversarial networks, and you can see that in the last column.",
                    "label": 0
                },
                {
                    "sent": "Also, like our model performs, has a little bit better scores.",
                    "label": 0
                },
                {
                    "sent": "And again, you can do more.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quantitative experiments where you also calculate the log likelihood and compare the log likelihood of the models and you can see that again, sentence representation doesn't really make such a huge difference in terms of your scores, which is kind of understandable because Microsoft Coco sentences are usually very short and not very complicated so.",
                    "label": 0
                },
                {
                    "sent": "Technically there is not not really need for very complicated sentence representations.",
                    "label": 0
                },
                {
                    "sent": "Ann you.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And do kind of a more qualitative experiments where you compare different models where you can just say comparing them with variational autoencoders.",
                    "label": 0
                },
                {
                    "sent": "We just generate noise.",
                    "label": 0
                },
                {
                    "sent": "I really bad and you can compare them with the current lab gun models, which is generative.",
                    "label": 0
                },
                {
                    "sent": "Adversarial networks would generate better images than VIA is, but still there quite noisy with sharper and this is kind of our model and this is kind of the images corresponding to captions, like a group of people walking on the beach.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I guess to conclude that kind of the generated images are still small 32 by 32 bulbs and it's still far from what we want in ideal.",
                    "label": 0
                },
                {
                    "sent": "Having like maybe 1024 by 1024 very clear images.",
                    "label": 0
                },
                {
                    "sent": "So there's still a lot of work to be done in that direction, but.",
                    "label": 0
                },
                {
                    "sent": "At least the good thing is that the model generalizes to out of box examples and examples that were never seen in the training set.",
                    "label": 1
                },
                {
                    "sent": "And we personally believe that it's probably due to underfitting that we're not really pushing the generative model to it limits.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The code is available on GitHub, so you can just download it and play with it, so thank you.",
                    "label": 0
                }
            ]
        }
    }
}