{
    "id": "apwwgscyxtesdhwr4pe7shdk4xtzyyvc",
    "title": "Recent Progress in Combinatorial Statistics",
    "info": {
        "author": [
            "Elchanan Mossel, Department of Statistics, UC Berkeley"
        ],
        "published": "July 30, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning",
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/mlss09us_mossel_rpcs/",
    "segmentation": [
        [
            "And I see that there were some very interesting toxin in the last couple of weeks.",
            "So in order to keep you alive, I'm going to beat some items during the talk.",
            "So the first item I'm going to bid is a boarding pass.",
            "Unfortunately, it has a name Emmanuel Candace.",
            "Anybody wants this boarding pass?",
            "OK, so that's the first item that goes down here in 20 minutes.",
            "I'm going to have another item which is actually more interesting and may interest one of the talks.",
            "Elf this."
        ],
        [
            "OK, so I'm going to talk about something completely different and this is what I called combinatorial statistics and it's related to many of the talks that you've seen so far, but with a slight twist.",
            "So what is combinatorial statistics?",
            "Combinatorics?",
            "Statistics is an area which deals with influence of discrete parameters such as graph partitions, trees and so on and so forth.",
            "So many of the objects that were discussed previously, but we have more rigorous constraints.",
            "We want algorithms with guarantees on sampling complexity, computational complexity and success probability, or Alternatively, when this is cannot be done sampling complexity or lower bounds.",
            "So of course many of the examples that you've seen this workshop before fall into this setup.",
            "Questions or objections?",
            "So I'm just going to give you full examples of little problems in combinatorial statistics."
        ],
        [
            "You know you can see what you think about it.",
            "So one problem I don't know.",
            "I don't know if this was discussed in this forum or not.",
            "Is the problem of graphical model reconstruction.",
            "So here's a cartoon of what graphical model reconstruction.",
            "So somebody is doing an experiment in the lab, they measure some stuff and then they want you to draw some graphical model out of it.",
            "And perhaps the reason they want you to do that is then they want to do more experiments, right?",
            "So that's.",
            "Sounds like a very natural statistical problem, so let's formalize it a bit."
        ],
        [
            "And what I'm going to use is I'm going to use the standard graphical model or Gibbs measure set up so we're going to look at the graph G with vertex vertex set V and agency, and they're going to be potentials for each clicks of the graph.",
            "So potential is from some finite set Sigma.",
            "To the size of the click into the positive numbers and then we're going to look at the gifts they gave us."
        ],
        [
            "Distribution where they give distribution, assigns to every.",
            "Vector of colors or every vectors of letter in Sigma to the size of the graph of probability, which is just the product of all the clicks in the graph.",
            "The potential of the click of the element of that clip.",
            "OK, so just this is just the standard formulation of factorized distribution or gives distribution.",
            "And what is the problem?",
            "Of reconstruction of random alcove.",
            "Random field.",
            "Well this problem is the problem where we don't know this graph structure.",
            "We don't know the potential cyan we want to recover the graph."
        ],
        [
            "So what?",
            "More specifically, what we will?",
            "What will we assume?",
            "We assume that we have independent samples from the Markov random field.",
            "We assume that we have observed data at all the nodes of this graph that I've."
        ],
        [
            "Describe before right.",
            "So whenever we have a sample, we see when node one was blue.",
            "Note two was red notes, it was yellow and not forward blue, but of course we don't know what the graph structure is."
        ],
        [
            "And later, I'm going to talk a little bit about what happens when we don't see some of the nodes."
        ],
        [
            "So let me formalize it even further.",
            "We're going to look at K independent samples of Vector Sigma.",
            "Sigma is a vector of N elements, Sigma one to Sigma and corresponding to the end vertexes of the graph.",
            "And given this cane dependent samples, our goal is to recover the graph G. And for reasons which I'm going to talk about soon, which will be apparent where I'm going to restrict to graph where the maximal degree, the maximum number of neighbors for every vertex of the graph is D. What we want we wanted estimator.",
            "What is an estimator?",
            "An estimator is the map that given K vector.",
            "K vectors of less length N of labelings of the graphs.",
            "This should be Sigma returns one of the elements.",
            "One of the graphs of degree at most D. Is the setup clear?",
            "OK, so of course the obvious questions are the following.",
            "How many samples K are required in order to reconstruct the mark of random field?",
            "With N nodes, Max degree D with probability approaching one.",
            "So what we want is that the probability that the estimator applied to the K samples that we see actually equals the generating graph.",
            "This probability is close to 1, right?",
            "So this probability include the randomness over the generation of the case samples.",
            "And of course, we want an efficient algorithm to do that.",
            "If you have questions about this problem, you know raise your hand because I'm going to discuss it for the next 10 or 15 minutes, yes.",
            "I am going to talk about it.",
            "This is going to come.",
            "OK."
        ],
        [
            "So there is a lot of work and I'm actually going to talk a little bit about some of the work so some cases are very easy.",
            "Something I worked a lot about a lot is trees, so for trees you can do it very very easily and you can do it very very easily.",
            "Even in cases where you see only some of the nodes, you don't have to see all of the nodes enough to see a subset of the nodes, and you can still do it.",
            "The two most related work to what I'm going to talk about is very nice work of a billion dollar: and from Stanford from zero 6 and work using regularization of Wainwright, Ravikumar Lafferty, and let me tell you first what other result?"
        ],
        [
            "Thanks talk in detail about.",
            "There is not, but I'm going to go back to the slide I just skipped over.",
            "So we do two things.",
            "We prove an upper bound and we prove a lower bound.",
            "So let me start with the lower bound.",
            "So the lower bound is the following.",
            "In order to recover G of Max degree D, we need at least some absolute constant which does not depend on the graph or anything.",
            "Time D time Logan samples.",
            "In order to be able to reconstruct the graph with probability, say more than two to the minus N exponentially small in the number of vertices.",
            "They do not this constant.",
            "The note does not depend on the potential function.",
            "The next constant is going to depend on the potential function, and I'll say something about it.",
            "OK, this is just a lower bound.",
            "OK so.",
            "But you have to understand the second.",
            "The setup is a setup.",
            "It's a game theoretic setup, right?",
            "I can choose any graph that you propose an algorithm.",
            "OK, I can choose any graph or distribution on graph that I want, and I claim that against my distribution of graph your success probability is going to be small.",
            "That's what this lower bound tells you.",
            "OK, so now algorithm you propose that uses all only C * D Time Logan samples as good probability of working against my distributions of graphical models of this form, and we also have a positive result if the distribution is known to generate and the nondegeneracy level will come into this constancy.",
            "But if the distribution is not generate, then using C * D times Logan samples so fast.",
            "It's like the model is probability which is greater than say 1 -- 1 over the number of nodes to the power 100.",
            "Moreover, the running time of the algorithm is polynomial in N, but is of the form and to power two DSA.",
            "OK so the result says.",
            "Well, the sampling complexity of the problem we understand completely.",
            "If some constants times times D times log in and Moreover we have a running time, which is good when the maximal degree is small.",
            "But it's very bad when the maximal degree is large.",
            "So let me compare compare this to the related works that I."
        ],
        [
            "I just talked about briefly and this.",
            "I think I can find.",
            "You know exactly what do I mean by combinatorial statistics.",
            "So there are three works.",
            "You know, this column, this column in this column and they differ in different different aspect.",
            "And I'll explain all of them.",
            "So the first question is what do you assume about the generative model and what's common to both the attack walk into our work is that we assume a general Markov random field.",
            "We put some assumptions about the maximal degree, but we assume a general Markov random field.",
            "The work using regularization of sparse sensing just because the way of the algorithm works, you need to assume that you are not allowing clicks.",
            "You're only allowing edge potentials.",
            "So if you think about the theory of groups, measures or gives distribution, you have to ask yourself how natural is this constraint.",
            "It's not clear how natural it is.",
            "Oops.",
            "I guess I'm not connected to the electricity.",
            "Again, what is what is the game that we're trying to play?",
            "The game that we're trying to play?",
            "Is we want to reconstruct this graph and we want to do it with a small number of samples, but there are various ways where you can do where you want to reconstruct the graph.",
            "One important difference between 2 settings that I'm going to talk about.",
            "So one important aspect that makes a difference between the work and Abilit Allen our work is that in the pack setting you don't actually care about reconstruction of the construction of the correct graph.",
            "What you care about is reconstructing some distribution that has small statistical distance to the distribution that you will see while what's coming to our work to the Wainwright it'll work is that we actually interested in reconstructing the correct graph that generated the samples.",
            "Now.",
            "What's nice about our work in the bill?",
            "It'll work is that there are no additional constraint.",
            "While the L1 regularization techniques actually use some very hard to verify technical condition that is needed in order for the algorithm to work.",
            "So when I ask Martin if there are cases where you can verify this condition essentially tell me, well, we cannot verify them, but it seems to work well in practice and for the combinatorial statistic point of view, this is not a satisfying answer.",
            "You really want to say for this kind of models we can reconstruct the graph.",
            "For other models, we cannot reconstruct the graph.",
            "Of course, the great advantage of regularization is that the running time is much better, so their running time for this restricted model with this restricted condition does not depend on D, while both of these works hours in their bill, it'll work are the running time does depend on the Ann.",
            "You know the sampling complexity in both of these work is optimal.",
            "OK, this is for the experts.",
            "If you didn't follow all of the comparisons, don't worry about it."
        ],
        [
            "So I'm not planning to tell you too much about, you know proves I'm more interested in telling you about the kind of results that we proved recently, but what's not specifically about this result, which is very easy, at least in principle, is that you do the obvious thing.",
            "And what is the obvious thing is, you know it's something that's been used before and also analyzed in the PAC learning setting by ability.",
            "What you do, and this is where the end to the day comes from.",
            "The well, I want to learn the study of this graph, so let me do it as follows.",
            "I'm going to choose a vertex and I'm going to enumerate.",
            "Overall, it's D possible neighbors, so this is the enumeration of costs tend to.",
            "The DI will say this is the neighbor.",
            "This is the neighbor and this is the neighbor and I'll check now that I enumerated over this three neighbors.",
            "I can check if this is indeed a neighborhood by checking conditional independent statements saying that what I see here, given the green neighbors is independent from what I'm seeing outside.",
            "So this is a very trivial algorithm.",
            "What's actually hard to do, and actually you need and a variant of this algorithm in order for the statement to work is to show that whenever the distribution is known to generate whenever the some of the weights or the potentials on these edges are nontrivial.",
            "This algorithm is actually going to work, so this is where the proof is.",
            "This is where the mathematics is.",
            "The algorithm is very, very simple, but the mathematics lies in proving that if this potentials are nonconstant, they actually depend on the interaction between these two guys.",
            "This algorithm is actually going to output the right graph.",
            "And if you think about it, it's not at all trivial that you can do that.",
            "Questions about this first example, I'm going to give you examples until you're going to fall asleep so.",
            "Yes.",
            "So essentially told you very quickly, it just says that the potential on the edges are functions that are far from epsilon, far from constant functions.",
            "They actually depend on the two endpoints of the edges, or more generally, the potentials on the clicks have.",
            "If you change one of the guys, it would actually put the value of the potential will change by something noticeable.",
            "It has to work for all of the potentials of the clicks and you can view it Alternatively.",
            "Otherwise, essentially this edge doesn't matter if you look at the graphical model where you remove these edges, remove then you can generate a distribution that's still pretty close to the distribution that you're looking at.",
            "OK."
        ],
        [
            "OK.",
            "So the next question, so of course, once you study this problem, you say.",
            "But what can I do?",
            "We see the nodes, so in many of the application especially you know biology, real life applications.",
            "You can only see some of the nodes and not all of them.",
            "So let me do this cartoon right?",
            "So we assume that the generative model is given by this complicated graph.",
            "But as a biologist you hired 5 postdocs and they worked very hard and they only succeeded to measure these two guys.",
            "And then the question is, can they reconstruct the big rough or not?",
            "So this is a trickier philosophical question, right?",
            "Because there is this big graph.",
            "Oops.",
            "There is this big graph down here and you only observe that.",
            "So what does it even mean to reconstruct that?",
            "And of course, as people with background in statistics, you know that the first objection is that maybe this problem is."
        ],
        [
            "Is an identifiable right?",
            "Maybe there can be many different graphs that are being that give me the same distribution on the two nodes or the five nodes that I'm seeing, right?",
            "So that's an obvious objection and somehow you have to live with it, right?",
            "It could be that there are many, many graphical consistent with what you're saying, but I'm just saying, suppose this is not what this wasn't a problem.",
            "Can we still solve this?",
            "Can we still discuss this problem somehow and what I want to do now is discussed.",
            "This discussed this from the computational point of view.",
            "In particular, I'm going to ask a different question, which is how hard is it to distinguish?",
            "Statistically, between two models."
        ],
        [
            "So this is the.",
            "This is the story that I like because it makes me very powerful, so I'm going to be an editor at in nature in Nature magazine and they are going to be two groups of biologists are going to give mark of random fields that explain the cell.",
            "And now there are two natural questions that I can ask myself.",
            "One natural question is the Group A gave me a very complicated model would be gave me a very complicated model and you know there are some experiments that measure the data at some nodes of the graph and the first question that I can ask myself are the two models actually producing something that is statistically different or not?",
            "And it could be that the two models that I'm looking at, you know one is very, very complicated in one way.",
            "One is very, very complicated.",
            "Another way, both of them have five nodes that I observe.",
            "And actually if you look at the distribution restricted to these five nodes, it's you know it's different distributions, but I don't know how to see that it's different.",
            "A second question is even more annoying as an editor of Nature.",
            "So supposing this editor of Nature and these two groups are sending me this paper and I wake up in the middle of the night with a dream and somebody comes to me in my dream.",
            "Tells me the two models are statistically different.",
            "Still, I have to figure out which one is the correct one.",
            "OK, so there are some experiments.",
            "Group A gave me Model M1 Group together Model M2 and I should ask which one is consistent with the experiment that I'm seeing."
        ],
        [
            "So of course, all of these problems are computationally hard, so if you had the motivation that hidden nodes make these problems hard, then you write, he denotes do make these problems hard and problems one and two are intractable, unless MP equal P or more technically NP equal randomized polynomial time.",
            "OK, so even if somebody gives Me 2 models and they promised me that they are different on the five nodes that I'm seeing, it's NP hard to say.",
            "If in real experiment, which I promise you either comes from this model or the other model.",
            "It's coming from this one or the other one."
        ],
        [
            "So now I actually, I'll tell you a little philosophical story just to entertain you.",
            "So.",
            "One of the objections that during the discussion of this problem we realize is that.",
            "There is a philosophical question.",
            "Can you sample from nature or not?",
            "So in particular I think about this complicated Malcolm Randall Field, which is supposed to represent the cell and think of your five measurements.",
            "And the question is.",
            "And this is something that we needed for the result that I just told you about.",
            "Is that sampling these five guys or sampling from the big model of the cell is hard, so this is something that's non interactive computer for a lot of time.",
            "If you want to sample from a high dimensional distribution it's computationally hard, but then you may have the objection.",
            "It couldn't be that nature does things that are more complicated than NP.",
            "And then you can ask more elaborate elaborated question.",
            "Suppose nature only does things that are efficient, computationally efficient.",
            "Can we still say something about the problem that?"
        ],
        [
            "Told you about before.",
            "And of course the answer is yes so.",
            "What I'm going to assume now is I'm going to assume that M1 and M2 are statistically far apart.",
            "I have samples to access of one of them.",
            "Can you tell if it's coming from one or four M2 and I'm going to assume that M1 and M2 or efficiently sampleable?",
            "What does it mean?",
            "It means anything of nature.",
            "It's not just enough that you give me this pictures of the cell, you have to give me an algorithm that samples from this configurations and the theorem is that this is also hard.",
            "So I won't tell you what zero knowledge is.",
            "This would be a zero knowledge statement, but zero knowledge is something that people believe is nontrivial, and this problem is intractable and there's some computational complexity issues here.",
            "Questions about the second example.",
            "OK, so this talk is built positive, negative, positive negative, so this was negative.",
            "Now I'm going to move to a positive thing.",
            "And."
        ],
        [
            "This has to do with consensus ranking.",
            "And what's called the mallows model?",
            "And here what you want to do is you want to do something pretty strange.",
            "You take it home with.",
            "You have a collection of permutations and you want to find the average.",
            "So what is an average of a set of permutation well or set of rankings?",
            "This actually doesn't seem to make so much sense, but what you want to find is we want to find the permutation \u03c0 zero that minimizes the sum of the distances from the permutation Pi that you were given to this permutation \u03c0 zero.",
            "So all I have to tell you in order to tell you what this problem is, I have to tell you what is this distance and the classical distance for that is what's called the came in distance, which is defined as follows.",
            "The distance between two permutations.",
            "Is the number of pairs where I less than J in the first permutation, but I bigger than J in the second permutation.",
            "So there is one permutation in a different permutation.",
            "I said that there are far apart if there are many pairs that if you look at this, there's only the reversed in one permutation with respect to the other.",
            "So now this problem is well defined, there's no statistics feel so far the statistics is going to come in the second or probability is going to come in the second.",
            "But right now this problem is defined as a combinatorial problem of finding the average of a set of permutations."
        ],
        [
            "Where is the probability coming in the probability coming in what's called the mallows model, which was introduced by models in the 50s.",
            "This is exponential model so it's exponential model in a parameter beta and the probability distribution of.",
            "A permutation Pi given the parameter \u03c0 zero is proportional to something exponential in minus beta.",
            "The distance between the permutation that we're generating and the kernel permutation \u03c0 zero.",
            "Crystal is exponential family or exponentially distribution overall of permutations.",
            "Of course, if beta is equal to 0, this expression does not actually depend on the permutation.",
            "This is just the uniform distribution.",
            "However, if beta is greater than zero, then it's easy to see that \u03c0 zero is the unique maximizer of this expression.",
            "So if you look at \u03c0 zero, this is the maximal probability things that are different only by 1 flip from \u03c0 zero is a little smaller probability, and so on and so forth.",
            "So where did I hear about this model?",
            "I heard about this model in the talk of Marina Maya from UW and what they showed is he showed that this problem of consensus ranking which OK one thing I should tell you the problem that I told you before, finding the average of this permutation.",
            "Is exactly the same as the problem of finding the maximum likelihood estimate of \u03c0 zero given samples by one 2\u03c0 K. So I should repeat it because I did it a little confused before I told about the about computer tutorial problem, where you're given K permutation when you want to find the average.",
            "Now I'm telling you about a generative statistical model which tells you what's the probability of Pi given \u03c0 zero, and I'm going to ask a similar question, which is there's going to be pie one that is going to be generated from this model with the secret \u03c0 zero Pi 2 that is going to be generated according this model with the secret by zero and so on and so forth.",
            "And given this by one 2\u03c0 K. You want to reconstruct the original \u03c0 zero.",
            "Now it's easy to write the maximum likelihood equation.",
            "The maximum likelihood equation tells you that you have to solve exactly the same optimization problem that we've seen before.",
            "And what this work showed is that there is a branch and bound algorithm that can solve this problem exactly.",
            "However, it can take exponential or even more than exponential time in some cases.",
            "On the other hand, it seems to perform well on simulated data.",
            "So this got me curious.",
            "And before I tell you what we can prove, let me ask you any questions about the model.",
            "OK."
        ],
        [
            "So I'm going to skip all of the related work, forsake of time.",
            "Actually not used any of this."
        ],
        [
            "Once we can prove the following, so this is a recent joint work with Mark Braverman.",
            "So given our independent samples from this mallows model we can find the maximum likelihood efine maximum likelihood solution exactly in polynomial time times, then to the B.",
            "And with high probability probability greater than say 1 -- 10 to the minus 100.",
            "Moreover, if you ask a polynomial time can be very large.",
            "So this polynomial time is actually not very large.",
            "It's one plus order.",
            "One of the beta, which is bad and one over the number of examples which is good.",
            "OK, So what does it mean?",
            "Think let me tell you the story about this.",
            "I suppose I asked, there is a two ordering about the quality of the talks in this workshop and I'm probably way down there.",
            "But you know, there is the true ranking in each of you have a noisy version of these two ranking.",
            "So you tell me your noisy version, you tell me your noisy version.",
            "You told me in your noisy version and so on and so forth and so on and so forth.",
            "And at the end what I want to do is I want to find the true maximum likelihood order and the answer is that I can do it in polynomial time, right?",
            "So if my flight tonight is delayed?",
            "As expected, I will be able to tell you what's the actual \u03c0 zero.",
            "That maximizes, sorry, I wouldn't be able to tell you what actually I would be able to tell you what the maximum likelihood pie.",
            "So let me tell you again, just in once."
        ],
        [
            "Side what's the ID in the proof?",
            "So the main idea, and it's something that's recurrent in many of the things that you observe in combinatorial statistics, is that you really have to understand the statistical properties of the problem well before you analyze it.",
            "And here ingredient one is the following.",
            "Is that if you look at the permutation Pi I and you look at it in terms of the original permutation \u03c0 zero, then it's pretty close to \u03c0 zero.",
            "In particular, with high probability the overall.",
            "This locations between pie, pie pie and PIE 0 this should be \u03c0 zero is of order N. And the maximum distance between \u03c0 and \u03c0 zero is of order log and Moreover.",
            "This is not written on the slide.",
            "Moreover, the same trees to the same thing is true for the maximum likelihood permutation.",
            "OK, so the picture is a little complicated, so let me.",
            "So what are we doing?",
            "The isiro that we don't know?",
            "We generate by 1 by 2.",
            "Spicy.",
            "And so on and so forth.",
            "Now, given this by 1 by 2\u03c0 three and so on, there is a tie, which is the maximum likelihood estimator.",
            "What's easy to see and I haven't told you, is that actually the maximum likelihood estimator is not equal to \u03c0 zero.",
            "If I give you a small number of examples, you know you just look at the top two element one and two.",
            "There is a constant probability that they will be flipped, right?",
            "SO50 and Pi are not the same permutation.",
            "But what is true is that both for the permutation \u03c0 zero and for the permutation Pi, which is the maximum likely permutation, and each of the generated permutations there closed Boston average and the maximal dislocation is also small.",
            "And an additional ingredient that algorithmic ingredient, once you understand the statistical property, is a dynamic programming algorithm to find \u03c0 given a starting point where each element is a distance at most K away, and the running time is exponential in this K, which would correspond to this log North, but only times a linear factor in the number of elements.",
            "Questions about this example?",
            "OK, I should really sell you something, but I sort of want to say so.",
            "This belongs to somebody, but apparently it does this belong to somebody?",
            "No, OK, I cannot tell you."
        ],
        [
            "Cannot be too big for this item.",
            "OK, so since I'm overall a negative negative outlook of life, I'm a pessimist.",
            "I have to end up with a pessimist POV and I'm going to try to annoy all of you.",
            "Run Markov chain Monte Carlo's, who has more market share Monte Carlo.",
            "OK, here you know, I'll try to do my best so you know, many of you know what Markov chain Monte Carlo is.",
            "You know you do it when you want to sample form a chain and it's really used in huge collection of areas.",
            "It.",
            "One thing that I should say in a note because even though most of you probably know it, I should still repeat it again.",
            "Sampling distribution is a problem that's computationally hard, so if somebody comes to me and tell me I have a sampling, I have a sampling algorithm which can sample any distribution that you want, even from a very restricted family.",
            "Then I know that is lying and it's actually happened to me like two or three times somebody comes to you and say have a Markov chain Monte Carlo where I heat up, cool down, moving paths, do this, do that.",
            "And it works for any kind of problem in a wide enough family.",
            "Then I know that you lying, because then you know how to solve NP hard problems.",
            "And most of you do not know how to solve NP hard problems.",
            "OK, so now I'm going to even think you further, because usually when I raise this objection in the Markov chain Monte Carlo Community people."
        ],
        [
            "We are.",
            "You know what you're saying is correct, it's true.",
            "We don't know how to samples, but we have diagnostics and we run the diagnostics and if the diagnostics told us that we you know we have converged, then we know that we converge.",
            "OK, so the aim of the last part of the talk is to annoy you in this aspect so the Markov chain we're imagining this person walking and walking and walking, and the diagnostic is supposed to tell him to cell phone.",
            "We arrive to the right spot on."
        ],
        [
            "So there is a vast literature on diagnostics.",
            "I'm not going to talk about that.",
            "Let me check."
        ],
        [
            "You just.",
            "The kind of result that you get, so these are the kind of computational results that you get, so I given a Markov chain P on say.",
            "It doesn't matter which space, but let's make the space it has to be exponentially large 012 then at time T, such that either you in total variation distance.",
            "Either the time it takes you to get the total version distance 1/4 is less than T or it's at least 100 T, then it is P space complete to decide if this is true or the system.",
            "So in particular, what does it mean?",
            "You give me a Markov chain.",
            "We agreed before that.",
            "You know sampling is very hard in general, but now we're going to ask a more elaborate question.",
            "You were going to say ah sometimes I'm not going to know if it's converging or not converging, but every diagnostic that's going to tell me and makes me happy.",
            "But what I'm telling you is that even if I give you the mixing time up to a huge factor, it is PSPACE complete to decide between these two options.",
            "So what is P?"
        ],
        [
            "Space complete, I shouldn't do this example because this example is actually confusing.",
            "Piece based complete means that it's harder than all the problems that you know, but actually, the more the nicest example of that is if you play on an infinite go board and you want to know if the black wins or the wide with this is a P space complete.",
            "So you cannot solve this problem, but now you say oh.",
            "You silly people are so annoying.",
            "You keep giving us this negative result, but I actually know because somebody told me or I have a good intuition that my Markov chain actually mixes not in exponential time, not in very big time in sort of now I have a good feeling it makes us in a reasonable amount of time.",
            "So what is this story that you're telling me?"
        ],
        [
            "And then I would like to know you again, I will tell you, even if you have a promise that the mixing time is smaller than say, Ncube North Square and you want to know if it's mixing times TO N * 100 T, then it's what's called Co NP hard.",
            "So even this you cannot know.",
            "Not even if I promise you that the mixing times at most N squared and you just need to know if it's you know you're on your chain and you want to know as it mixed or not, as it mixed up.",
            "So of course after Times Square it would have mixed, but in order to know when when is the point in time where it makes this is Co NP hard?"
        ],
        [
            "And you can ask."
        ],
        [
            "More and more annoying questions, but I'll stop here.",
            "Let me just wrap up, but by saying a little bit about what I think about some features of combinatorial statistics versus some of the more applied statistic approaches so.",
            "This is if you happen to see my reviews on your papers in applied statistics.",
            "This is what guides me.",
            "So what I like is I like using explicit well defined models.",
            "I like explicit, known as symptomatic running time in sample complexity bounds.",
            "What often comes with these two constraint is that often you don't get statistical efficiency and asymptotic sense.",
            "And also all of the results that we know how to prove inputs have to be generated according either to the correct distributions or close to it.",
            "But you know, we know that often if you're given a general input, it's NP hard problems.",
            "OK, I hope I amuse you and you're still awake and."
        ],
        [
            "You have any questions left?"
        ],
        [
            "No.",
            "Ah.",
            "Don't let me know yet.",
            "These are my collaborators, so the Berkeley Group is an entire Bhatnagar guy.",
            "Bressler, Allan, Sly, and the book, though is now in Hong Kong.",
            "Mark brother, money, Microsoft."
        ],
        [
            "Silver done in Harvard.",
            "Thank you.",
            "Yes, yes.",
            "It's not just the problems.",
            "Have the same philosophical spirit.",
            "It's not the same.",
            "It's not that one problem implies the other, it was really different examples, but both have to do is reconstructing discrete object in one of them.",
            "It was a graph, it was the other one.",
            "It was a permutation.",
            "Both of them have the assumptions that you see something that's generated according to the model, and you know in one of them you actually want to reconstruct the correct graph.",
            "In another one you want to reconstruct the maximum likelihood.",
            "Station, just the same spirit.",
            "It's the technology and the pools and so on so forth are completely different.",
            "Series yes, sure.",
            "More annoying."
        ],
        [
            "Yes.",
            "Alright.",
            "Yeah.",
            "Yeah, but how do you know that if these two distributions are the same or not?",
            "This is a very hard problem, right?",
            "So it is to once I guarantee you that you're running times that most tend to the 100 you can run it for him to the 100 and you'll get good samples.",
            "But then you say I, I'm going to cheat, you run until time and check if these two distributions are the same, but it's very hard to check if two distributions are the same.",
            "Because.",
            "Purpose.",
            "Maybe also.",
            "I don't know why this is true, right?",
            "So it depends.",
            "It depends what.",
            "Right no, but but this is a good point, and there's actually.",
            "There's actually also.",
            "That actually also more annoying results, even if you make assumptions about some probability and so on and so forth.",
            "It is true that if you look just at a specific statistics, say you only care about the average of the bits and you know you want until enter the 100, you see what's the distribution here?",
            "Run until time and you see if you get the same distribution you done, you don't have to do anything else.",
            "Right no no no no, but this here you don't even hear the result doesn't hold right.",
            "In order to know if you need in order to know if you mixed or not.",
            "This is a high dimensional question.",
            "If you just project yourself to a low dimensional question, does a specific statistics mixed or not?",
            "Then once I promise to you that in enter the other steps you mixed, you can sample the distribution runtime.",
            "What?",
            "No, it's hard.",
            "It's hard.",
            "This is hard, yeah.",
            "Indiana Local in hours.",
            "I don't think it really.",
            "I don't think this really comes into two heavily into the right, so there is not is very impressive computationally.",
            "Definitely in.",
            "In particular the number of edges that does not come in any significant way, and our result in our results definitely does not use the number of edges.",
            "And similarly ability is or does not use the number of edges.",
            "No, there is an issue which is an interesting issue that the two problems are actually different.",
            "In one of them, look at graphical models where there are clicks and the clicks make it a much higher dimensional problem.",
            "In the United all kind of problem that only edges and this makes the dimension must much smaller.",
            "So this is where somehow these two setups are different.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I see that there were some very interesting toxin in the last couple of weeks.",
                    "label": 0
                },
                {
                    "sent": "So in order to keep you alive, I'm going to beat some items during the talk.",
                    "label": 0
                },
                {
                    "sent": "So the first item I'm going to bid is a boarding pass.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it has a name Emmanuel Candace.",
                    "label": 0
                },
                {
                    "sent": "Anybody wants this boarding pass?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the first item that goes down here in 20 minutes.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have another item which is actually more interesting and may interest one of the talks.",
                    "label": 0
                },
                {
                    "sent": "Elf this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to talk about something completely different and this is what I called combinatorial statistics and it's related to many of the talks that you've seen so far, but with a slight twist.",
                    "label": 0
                },
                {
                    "sent": "So what is combinatorial statistics?",
                    "label": 1
                },
                {
                    "sent": "Combinatorics?",
                    "label": 1
                },
                {
                    "sent": "Statistics is an area which deals with influence of discrete parameters such as graph partitions, trees and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So many of the objects that were discussed previously, but we have more rigorous constraints.",
                    "label": 0
                },
                {
                    "sent": "We want algorithms with guarantees on sampling complexity, computational complexity and success probability, or Alternatively, when this is cannot be done sampling complexity or lower bounds.",
                    "label": 1
                },
                {
                    "sent": "So of course many of the examples that you've seen this workshop before fall into this setup.",
                    "label": 0
                },
                {
                    "sent": "Questions or objections?",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to give you full examples of little problems in combinatorial statistics.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know you can see what you think about it.",
                    "label": 0
                },
                {
                    "sent": "So one problem I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know if this was discussed in this forum or not.",
                    "label": 0
                },
                {
                    "sent": "Is the problem of graphical model reconstruction.",
                    "label": 1
                },
                {
                    "sent": "So here's a cartoon of what graphical model reconstruction.",
                    "label": 0
                },
                {
                    "sent": "So somebody is doing an experiment in the lab, they measure some stuff and then they want you to draw some graphical model out of it.",
                    "label": 0
                },
                {
                    "sent": "And perhaps the reason they want you to do that is then they want to do more experiments, right?",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Sounds like a very natural statistical problem, so let's formalize it a bit.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'm going to use is I'm going to use the standard graphical model or Gibbs measure set up so we're going to look at the graph G with vertex vertex set V and agency, and they're going to be potentials for each clicks of the graph.",
                    "label": 0
                },
                {
                    "sent": "So potential is from some finite set Sigma.",
                    "label": 0
                },
                {
                    "sent": "To the size of the click into the positive numbers and then we're going to look at the gifts they gave us.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distribution where they give distribution, assigns to every.",
                    "label": 0
                },
                {
                    "sent": "Vector of colors or every vectors of letter in Sigma to the size of the graph of probability, which is just the product of all the clicks in the graph.",
                    "label": 0
                },
                {
                    "sent": "The potential of the click of the element of that clip.",
                    "label": 0
                },
                {
                    "sent": "OK, so just this is just the standard formulation of factorized distribution or gives distribution.",
                    "label": 0
                },
                {
                    "sent": "And what is the problem?",
                    "label": 0
                },
                {
                    "sent": "Of reconstruction of random alcove.",
                    "label": 0
                },
                {
                    "sent": "Random field.",
                    "label": 0
                },
                {
                    "sent": "Well this problem is the problem where we don't know this graph structure.",
                    "label": 0
                },
                {
                    "sent": "We don't know the potential cyan we want to recover the graph.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "More specifically, what we will?",
                    "label": 0
                },
                {
                    "sent": "What will we assume?",
                    "label": 0
                },
                {
                    "sent": "We assume that we have independent samples from the Markov random field.",
                    "label": 1
                },
                {
                    "sent": "We assume that we have observed data at all the nodes of this graph that I've.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Describe before right.",
                    "label": 0
                },
                {
                    "sent": "So whenever we have a sample, we see when node one was blue.",
                    "label": 0
                },
                {
                    "sent": "Note two was red notes, it was yellow and not forward blue, but of course we don't know what the graph structure is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And later, I'm going to talk a little bit about what happens when we don't see some of the nodes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me formalize it even further.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at K independent samples of Vector Sigma.",
                    "label": 0
                },
                {
                    "sent": "Sigma is a vector of N elements, Sigma one to Sigma and corresponding to the end vertexes of the graph.",
                    "label": 0
                },
                {
                    "sent": "And given this cane dependent samples, our goal is to recover the graph G. And for reasons which I'm going to talk about soon, which will be apparent where I'm going to restrict to graph where the maximal degree, the maximum number of neighbors for every vertex of the graph is D. What we want we wanted estimator.",
                    "label": 0
                },
                {
                    "sent": "What is an estimator?",
                    "label": 0
                },
                {
                    "sent": "An estimator is the map that given K vector.",
                    "label": 0
                },
                {
                    "sent": "K vectors of less length N of labelings of the graphs.",
                    "label": 0
                },
                {
                    "sent": "This should be Sigma returns one of the elements.",
                    "label": 0
                },
                {
                    "sent": "One of the graphs of degree at most D. Is the setup clear?",
                    "label": 0
                },
                {
                    "sent": "OK, so of course the obvious questions are the following.",
                    "label": 0
                },
                {
                    "sent": "How many samples K are required in order to reconstruct the mark of random field?",
                    "label": 1
                },
                {
                    "sent": "With N nodes, Max degree D with probability approaching one.",
                    "label": 0
                },
                {
                    "sent": "So what we want is that the probability that the estimator applied to the K samples that we see actually equals the generating graph.",
                    "label": 0
                },
                {
                    "sent": "This probability is close to 1, right?",
                    "label": 0
                },
                {
                    "sent": "So this probability include the randomness over the generation of the case samples.",
                    "label": 0
                },
                {
                    "sent": "And of course, we want an efficient algorithm to do that.",
                    "label": 0
                },
                {
                    "sent": "If you have questions about this problem, you know raise your hand because I'm going to discuss it for the next 10 or 15 minutes, yes.",
                    "label": 0
                },
                {
                    "sent": "I am going to talk about it.",
                    "label": 0
                },
                {
                    "sent": "This is going to come.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is a lot of work and I'm actually going to talk a little bit about some of the work so some cases are very easy.",
                    "label": 0
                },
                {
                    "sent": "Something I worked a lot about a lot is trees, so for trees you can do it very very easily and you can do it very very easily.",
                    "label": 0
                },
                {
                    "sent": "Even in cases where you see only some of the nodes, you don't have to see all of the nodes enough to see a subset of the nodes, and you can still do it.",
                    "label": 0
                },
                {
                    "sent": "The two most related work to what I'm going to talk about is very nice work of a billion dollar: and from Stanford from zero 6 and work using regularization of Wainwright, Ravikumar Lafferty, and let me tell you first what other result?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks talk in detail about.",
                    "label": 0
                },
                {
                    "sent": "There is not, but I'm going to go back to the slide I just skipped over.",
                    "label": 0
                },
                {
                    "sent": "So we do two things.",
                    "label": 0
                },
                {
                    "sent": "We prove an upper bound and we prove a lower bound.",
                    "label": 0
                },
                {
                    "sent": "So let me start with the lower bound.",
                    "label": 0
                },
                {
                    "sent": "So the lower bound is the following.",
                    "label": 0
                },
                {
                    "sent": "In order to recover G of Max degree D, we need at least some absolute constant which does not depend on the graph or anything.",
                    "label": 1
                },
                {
                    "sent": "Time D time Logan samples.",
                    "label": 1
                },
                {
                    "sent": "In order to be able to reconstruct the graph with probability, say more than two to the minus N exponentially small in the number of vertices.",
                    "label": 0
                },
                {
                    "sent": "They do not this constant.",
                    "label": 0
                },
                {
                    "sent": "The note does not depend on the potential function.",
                    "label": 0
                },
                {
                    "sent": "The next constant is going to depend on the potential function, and I'll say something about it.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just a lower bound.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "But you have to understand the second.",
                    "label": 0
                },
                {
                    "sent": "The setup is a setup.",
                    "label": 0
                },
                {
                    "sent": "It's a game theoretic setup, right?",
                    "label": 0
                },
                {
                    "sent": "I can choose any graph that you propose an algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, I can choose any graph or distribution on graph that I want, and I claim that against my distribution of graph your success probability is going to be small.",
                    "label": 0
                },
                {
                    "sent": "That's what this lower bound tells you.",
                    "label": 0
                },
                {
                    "sent": "OK, so now algorithm you propose that uses all only C * D Time Logan samples as good probability of working against my distributions of graphical models of this form, and we also have a positive result if the distribution is known to generate and the nondegeneracy level will come into this constancy.",
                    "label": 0
                },
                {
                    "sent": "But if the distribution is not generate, then using C * D times Logan samples so fast.",
                    "label": 0
                },
                {
                    "sent": "It's like the model is probability which is greater than say 1 -- 1 over the number of nodes to the power 100.",
                    "label": 0
                },
                {
                    "sent": "Moreover, the running time of the algorithm is polynomial in N, but is of the form and to power two DSA.",
                    "label": 0
                },
                {
                    "sent": "OK so the result says.",
                    "label": 0
                },
                {
                    "sent": "Well, the sampling complexity of the problem we understand completely.",
                    "label": 0
                },
                {
                    "sent": "If some constants times times D times log in and Moreover we have a running time, which is good when the maximal degree is small.",
                    "label": 0
                },
                {
                    "sent": "But it's very bad when the maximal degree is large.",
                    "label": 0
                },
                {
                    "sent": "So let me compare compare this to the related works that I.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I just talked about briefly and this.",
                    "label": 0
                },
                {
                    "sent": "I think I can find.",
                    "label": 0
                },
                {
                    "sent": "You know exactly what do I mean by combinatorial statistics.",
                    "label": 0
                },
                {
                    "sent": "So there are three works.",
                    "label": 0
                },
                {
                    "sent": "You know, this column, this column in this column and they differ in different different aspect.",
                    "label": 0
                },
                {
                    "sent": "And I'll explain all of them.",
                    "label": 0
                },
                {
                    "sent": "So the first question is what do you assume about the generative model and what's common to both the attack walk into our work is that we assume a general Markov random field.",
                    "label": 0
                },
                {
                    "sent": "We put some assumptions about the maximal degree, but we assume a general Markov random field.",
                    "label": 0
                },
                {
                    "sent": "The work using regularization of sparse sensing just because the way of the algorithm works, you need to assume that you are not allowing clicks.",
                    "label": 0
                },
                {
                    "sent": "You're only allowing edge potentials.",
                    "label": 0
                },
                {
                    "sent": "So if you think about the theory of groups, measures or gives distribution, you have to ask yourself how natural is this constraint.",
                    "label": 0
                },
                {
                    "sent": "It's not clear how natural it is.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "I guess I'm not connected to the electricity.",
                    "label": 0
                },
                {
                    "sent": "Again, what is what is the game that we're trying to play?",
                    "label": 1
                },
                {
                    "sent": "The game that we're trying to play?",
                    "label": 0
                },
                {
                    "sent": "Is we want to reconstruct this graph and we want to do it with a small number of samples, but there are various ways where you can do where you want to reconstruct the graph.",
                    "label": 1
                },
                {
                    "sent": "One important difference between 2 settings that I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "So one important aspect that makes a difference between the work and Abilit Allen our work is that in the pack setting you don't actually care about reconstruction of the construction of the correct graph.",
                    "label": 0
                },
                {
                    "sent": "What you care about is reconstructing some distribution that has small statistical distance to the distribution that you will see while what's coming to our work to the Wainwright it'll work is that we actually interested in reconstructing the correct graph that generated the samples.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What's nice about our work in the bill?",
                    "label": 0
                },
                {
                    "sent": "It'll work is that there are no additional constraint.",
                    "label": 0
                },
                {
                    "sent": "While the L1 regularization techniques actually use some very hard to verify technical condition that is needed in order for the algorithm to work.",
                    "label": 0
                },
                {
                    "sent": "So when I ask Martin if there are cases where you can verify this condition essentially tell me, well, we cannot verify them, but it seems to work well in practice and for the combinatorial statistic point of view, this is not a satisfying answer.",
                    "label": 0
                },
                {
                    "sent": "You really want to say for this kind of models we can reconstruct the graph.",
                    "label": 0
                },
                {
                    "sent": "For other models, we cannot reconstruct the graph.",
                    "label": 0
                },
                {
                    "sent": "Of course, the great advantage of regularization is that the running time is much better, so their running time for this restricted model with this restricted condition does not depend on D, while both of these works hours in their bill, it'll work are the running time does depend on the Ann.",
                    "label": 0
                },
                {
                    "sent": "You know the sampling complexity in both of these work is optimal.",
                    "label": 1
                },
                {
                    "sent": "OK, this is for the experts.",
                    "label": 0
                },
                {
                    "sent": "If you didn't follow all of the comparisons, don't worry about it.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm not planning to tell you too much about, you know proves I'm more interested in telling you about the kind of results that we proved recently, but what's not specifically about this result, which is very easy, at least in principle, is that you do the obvious thing.",
                    "label": 0
                },
                {
                    "sent": "And what is the obvious thing is, you know it's something that's been used before and also analyzed in the PAC learning setting by ability.",
                    "label": 0
                },
                {
                    "sent": "What you do, and this is where the end to the day comes from.",
                    "label": 0
                },
                {
                    "sent": "The well, I want to learn the study of this graph, so let me do it as follows.",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose a vertex and I'm going to enumerate.",
                    "label": 0
                },
                {
                    "sent": "Overall, it's D possible neighbors, so this is the enumeration of costs tend to.",
                    "label": 0
                },
                {
                    "sent": "The DI will say this is the neighbor.",
                    "label": 0
                },
                {
                    "sent": "This is the neighbor and this is the neighbor and I'll check now that I enumerated over this three neighbors.",
                    "label": 0
                },
                {
                    "sent": "I can check if this is indeed a neighborhood by checking conditional independent statements saying that what I see here, given the green neighbors is independent from what I'm seeing outside.",
                    "label": 0
                },
                {
                    "sent": "So this is a very trivial algorithm.",
                    "label": 0
                },
                {
                    "sent": "What's actually hard to do, and actually you need and a variant of this algorithm in order for the statement to work is to show that whenever the distribution is known to generate whenever the some of the weights or the potentials on these edges are nontrivial.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is actually going to work, so this is where the proof is.",
                    "label": 0
                },
                {
                    "sent": "This is where the mathematics is.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is very, very simple, but the mathematics lies in proving that if this potentials are nonconstant, they actually depend on the interaction between these two guys.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is actually going to output the right graph.",
                    "label": 0
                },
                {
                    "sent": "And if you think about it, it's not at all trivial that you can do that.",
                    "label": 0
                },
                {
                    "sent": "Questions about this first example, I'm going to give you examples until you're going to fall asleep so.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So essentially told you very quickly, it just says that the potential on the edges are functions that are far from epsilon, far from constant functions.",
                    "label": 0
                },
                {
                    "sent": "They actually depend on the two endpoints of the edges, or more generally, the potentials on the clicks have.",
                    "label": 0
                },
                {
                    "sent": "If you change one of the guys, it would actually put the value of the potential will change by something noticeable.",
                    "label": 0
                },
                {
                    "sent": "It has to work for all of the potentials of the clicks and you can view it Alternatively.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, essentially this edge doesn't matter if you look at the graphical model where you remove these edges, remove then you can generate a distribution that's still pretty close to the distribution that you're looking at.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the next question, so of course, once you study this problem, you say.",
                    "label": 0
                },
                {
                    "sent": "But what can I do?",
                    "label": 0
                },
                {
                    "sent": "We see the nodes, so in many of the application especially you know biology, real life applications.",
                    "label": 0
                },
                {
                    "sent": "You can only see some of the nodes and not all of them.",
                    "label": 1
                },
                {
                    "sent": "So let me do this cartoon right?",
                    "label": 0
                },
                {
                    "sent": "So we assume that the generative model is given by this complicated graph.",
                    "label": 0
                },
                {
                    "sent": "But as a biologist you hired 5 postdocs and they worked very hard and they only succeeded to measure these two guys.",
                    "label": 0
                },
                {
                    "sent": "And then the question is, can they reconstruct the big rough or not?",
                    "label": 0
                },
                {
                    "sent": "So this is a trickier philosophical question, right?",
                    "label": 0
                },
                {
                    "sent": "Because there is this big graph.",
                    "label": 0
                },
                {
                    "sent": "Oops.",
                    "label": 0
                },
                {
                    "sent": "There is this big graph down here and you only observe that.",
                    "label": 1
                },
                {
                    "sent": "So what does it even mean to reconstruct that?",
                    "label": 0
                },
                {
                    "sent": "And of course, as people with background in statistics, you know that the first objection is that maybe this problem is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is an identifiable right?",
                    "label": 0
                },
                {
                    "sent": "Maybe there can be many different graphs that are being that give me the same distribution on the two nodes or the five nodes that I'm seeing, right?",
                    "label": 0
                },
                {
                    "sent": "So that's an obvious objection and somehow you have to live with it, right?",
                    "label": 0
                },
                {
                    "sent": "It could be that there are many, many graphical consistent with what you're saying, but I'm just saying, suppose this is not what this wasn't a problem.",
                    "label": 0
                },
                {
                    "sent": "Can we still solve this?",
                    "label": 0
                },
                {
                    "sent": "Can we still discuss this problem somehow and what I want to do now is discussed.",
                    "label": 0
                },
                {
                    "sent": "This discussed this from the computational point of view.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm going to ask a different question, which is how hard is it to distinguish?",
                    "label": 1
                },
                {
                    "sent": "Statistically, between two models.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "This is the story that I like because it makes me very powerful, so I'm going to be an editor at in nature in Nature magazine and they are going to be two groups of biologists are going to give mark of random fields that explain the cell.",
                    "label": 0
                },
                {
                    "sent": "And now there are two natural questions that I can ask myself.",
                    "label": 0
                },
                {
                    "sent": "One natural question is the Group A gave me a very complicated model would be gave me a very complicated model and you know there are some experiments that measure the data at some nodes of the graph and the first question that I can ask myself are the two models actually producing something that is statistically different or not?",
                    "label": 0
                },
                {
                    "sent": "And it could be that the two models that I'm looking at, you know one is very, very complicated in one way.",
                    "label": 0
                },
                {
                    "sent": "One is very, very complicated.",
                    "label": 0
                },
                {
                    "sent": "Another way, both of them have five nodes that I observe.",
                    "label": 1
                },
                {
                    "sent": "And actually if you look at the distribution restricted to these five nodes, it's you know it's different distributions, but I don't know how to see that it's different.",
                    "label": 0
                },
                {
                    "sent": "A second question is even more annoying as an editor of Nature.",
                    "label": 0
                },
                {
                    "sent": "So supposing this editor of Nature and these two groups are sending me this paper and I wake up in the middle of the night with a dream and somebody comes to me in my dream.",
                    "label": 0
                },
                {
                    "sent": "Tells me the two models are statistically different.",
                    "label": 1
                },
                {
                    "sent": "Still, I have to figure out which one is the correct one.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are some experiments.",
                    "label": 0
                },
                {
                    "sent": "Group A gave me Model M1 Group together Model M2 and I should ask which one is consistent with the experiment that I'm seeing.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course, all of these problems are computationally hard, so if you had the motivation that hidden nodes make these problems hard, then you write, he denotes do make these problems hard and problems one and two are intractable, unless MP equal P or more technically NP equal randomized polynomial time.",
                    "label": 0
                },
                {
                    "sent": "OK, so even if somebody gives Me 2 models and they promised me that they are different on the five nodes that I'm seeing, it's NP hard to say.",
                    "label": 0
                },
                {
                    "sent": "If in real experiment, which I promise you either comes from this model or the other model.",
                    "label": 0
                },
                {
                    "sent": "It's coming from this one or the other one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I actually, I'll tell you a little philosophical story just to entertain you.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One of the objections that during the discussion of this problem we realize is that.",
                    "label": 0
                },
                {
                    "sent": "There is a philosophical question.",
                    "label": 0
                },
                {
                    "sent": "Can you sample from nature or not?",
                    "label": 0
                },
                {
                    "sent": "So in particular I think about this complicated Malcolm Randall Field, which is supposed to represent the cell and think of your five measurements.",
                    "label": 0
                },
                {
                    "sent": "And the question is.",
                    "label": 0
                },
                {
                    "sent": "And this is something that we needed for the result that I just told you about.",
                    "label": 0
                },
                {
                    "sent": "Is that sampling these five guys or sampling from the big model of the cell is hard, so this is something that's non interactive computer for a lot of time.",
                    "label": 0
                },
                {
                    "sent": "If you want to sample from a high dimensional distribution it's computationally hard, but then you may have the objection.",
                    "label": 0
                },
                {
                    "sent": "It couldn't be that nature does things that are more complicated than NP.",
                    "label": 0
                },
                {
                    "sent": "And then you can ask more elaborate elaborated question.",
                    "label": 0
                },
                {
                    "sent": "Suppose nature only does things that are efficient, computationally efficient.",
                    "label": 1
                },
                {
                    "sent": "Can we still say something about the problem that?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Told you about before.",
                    "label": 0
                },
                {
                    "sent": "And of course the answer is yes so.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to assume now is I'm going to assume that M1 and M2 are statistically far apart.",
                    "label": 1
                },
                {
                    "sent": "I have samples to access of one of them.",
                    "label": 1
                },
                {
                    "sent": "Can you tell if it's coming from one or four M2 and I'm going to assume that M1 and M2 or efficiently sampleable?",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means anything of nature.",
                    "label": 0
                },
                {
                    "sent": "It's not just enough that you give me this pictures of the cell, you have to give me an algorithm that samples from this configurations and the theorem is that this is also hard.",
                    "label": 0
                },
                {
                    "sent": "So I won't tell you what zero knowledge is.",
                    "label": 0
                },
                {
                    "sent": "This would be a zero knowledge statement, but zero knowledge is something that people believe is nontrivial, and this problem is intractable and there's some computational complexity issues here.",
                    "label": 0
                },
                {
                    "sent": "Questions about the second example.",
                    "label": 0
                },
                {
                    "sent": "OK, so this talk is built positive, negative, positive negative, so this was negative.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to move to a positive thing.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This has to do with consensus ranking.",
                    "label": 1
                },
                {
                    "sent": "And what's called the mallows model?",
                    "label": 1
                },
                {
                    "sent": "And here what you want to do is you want to do something pretty strange.",
                    "label": 0
                },
                {
                    "sent": "You take it home with.",
                    "label": 1
                },
                {
                    "sent": "You have a collection of permutations and you want to find the average.",
                    "label": 0
                },
                {
                    "sent": "So what is an average of a set of permutation well or set of rankings?",
                    "label": 1
                },
                {
                    "sent": "This actually doesn't seem to make so much sense, but what you want to find is we want to find the permutation \u03c0 zero that minimizes the sum of the distances from the permutation Pi that you were given to this permutation \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "So all I have to tell you in order to tell you what this problem is, I have to tell you what is this distance and the classical distance for that is what's called the came in distance, which is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "The distance between two permutations.",
                    "label": 0
                },
                {
                    "sent": "Is the number of pairs where I less than J in the first permutation, but I bigger than J in the second permutation.",
                    "label": 0
                },
                {
                    "sent": "So there is one permutation in a different permutation.",
                    "label": 0
                },
                {
                    "sent": "I said that there are far apart if there are many pairs that if you look at this, there's only the reversed in one permutation with respect to the other.",
                    "label": 0
                },
                {
                    "sent": "So now this problem is well defined, there's no statistics feel so far the statistics is going to come in the second or probability is going to come in the second.",
                    "label": 0
                },
                {
                    "sent": "But right now this problem is defined as a combinatorial problem of finding the average of a set of permutations.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where is the probability coming in the probability coming in what's called the mallows model, which was introduced by models in the 50s.",
                    "label": 1
                },
                {
                    "sent": "This is exponential model so it's exponential model in a parameter beta and the probability distribution of.",
                    "label": 0
                },
                {
                    "sent": "A permutation Pi given the parameter \u03c0 zero is proportional to something exponential in minus beta.",
                    "label": 0
                },
                {
                    "sent": "The distance between the permutation that we're generating and the kernel permutation \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "Crystal is exponential family or exponentially distribution overall of permutations.",
                    "label": 0
                },
                {
                    "sent": "Of course, if beta is equal to 0, this expression does not actually depend on the permutation.",
                    "label": 0
                },
                {
                    "sent": "This is just the uniform distribution.",
                    "label": 1
                },
                {
                    "sent": "However, if beta is greater than zero, then it's easy to see that \u03c0 zero is the unique maximizer of this expression.",
                    "label": 0
                },
                {
                    "sent": "So if you look at \u03c0 zero, this is the maximal probability things that are different only by 1 flip from \u03c0 zero is a little smaller probability, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So where did I hear about this model?",
                    "label": 0
                },
                {
                    "sent": "I heard about this model in the talk of Marina Maya from UW and what they showed is he showed that this problem of consensus ranking which OK one thing I should tell you the problem that I told you before, finding the average of this permutation.",
                    "label": 0
                },
                {
                    "sent": "Is exactly the same as the problem of finding the maximum likelihood estimate of \u03c0 zero given samples by one 2\u03c0 K. So I should repeat it because I did it a little confused before I told about the about computer tutorial problem, where you're given K permutation when you want to find the average.",
                    "label": 0
                },
                {
                    "sent": "Now I'm telling you about a generative statistical model which tells you what's the probability of Pi given \u03c0 zero, and I'm going to ask a similar question, which is there's going to be pie one that is going to be generated from this model with the secret \u03c0 zero Pi 2 that is going to be generated according this model with the secret by zero and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And given this by one 2\u03c0 K. You want to reconstruct the original \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "Now it's easy to write the maximum likelihood equation.",
                    "label": 0
                },
                {
                    "sent": "The maximum likelihood equation tells you that you have to solve exactly the same optimization problem that we've seen before.",
                    "label": 0
                },
                {
                    "sent": "And what this work showed is that there is a branch and bound algorithm that can solve this problem exactly.",
                    "label": 1
                },
                {
                    "sent": "However, it can take exponential or even more than exponential time in some cases.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, it seems to perform well on simulated data.",
                    "label": 1
                },
                {
                    "sent": "So this got me curious.",
                    "label": 0
                },
                {
                    "sent": "And before I tell you what we can prove, let me ask you any questions about the model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to skip all of the related work, forsake of time.",
                    "label": 0
                },
                {
                    "sent": "Actually not used any of this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once we can prove the following, so this is a recent joint work with Mark Braverman.",
                    "label": 0
                },
                {
                    "sent": "So given our independent samples from this mallows model we can find the maximum likelihood efine maximum likelihood solution exactly in polynomial time times, then to the B.",
                    "label": 1
                },
                {
                    "sent": "And with high probability probability greater than say 1 -- 10 to the minus 100.",
                    "label": 0
                },
                {
                    "sent": "Moreover, if you ask a polynomial time can be very large.",
                    "label": 0
                },
                {
                    "sent": "So this polynomial time is actually not very large.",
                    "label": 0
                },
                {
                    "sent": "It's one plus order.",
                    "label": 1
                },
                {
                    "sent": "One of the beta, which is bad and one over the number of examples which is good.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Think let me tell you the story about this.",
                    "label": 0
                },
                {
                    "sent": "I suppose I asked, there is a two ordering about the quality of the talks in this workshop and I'm probably way down there.",
                    "label": 0
                },
                {
                    "sent": "But you know, there is the true ranking in each of you have a noisy version of these two ranking.",
                    "label": 0
                },
                {
                    "sent": "So you tell me your noisy version, you tell me your noisy version.",
                    "label": 0
                },
                {
                    "sent": "You told me in your noisy version and so on and so forth and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And at the end what I want to do is I want to find the true maximum likelihood order and the answer is that I can do it in polynomial time, right?",
                    "label": 0
                },
                {
                    "sent": "So if my flight tonight is delayed?",
                    "label": 0
                },
                {
                    "sent": "As expected, I will be able to tell you what's the actual \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "That maximizes, sorry, I wouldn't be able to tell you what actually I would be able to tell you what the maximum likelihood pie.",
                    "label": 0
                },
                {
                    "sent": "So let me tell you again, just in once.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Side what's the ID in the proof?",
                    "label": 0
                },
                {
                    "sent": "So the main idea, and it's something that's recurrent in many of the things that you observe in combinatorial statistics, is that you really have to understand the statistical properties of the problem well before you analyze it.",
                    "label": 0
                },
                {
                    "sent": "And here ingredient one is the following.",
                    "label": 0
                },
                {
                    "sent": "Is that if you look at the permutation Pi I and you look at it in terms of the original permutation \u03c0 zero, then it's pretty close to \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "In particular, with high probability the overall.",
                    "label": 0
                },
                {
                    "sent": "This locations between pie, pie pie and PIE 0 this should be \u03c0 zero is of order N. And the maximum distance between \u03c0 and \u03c0 zero is of order log and Moreover.",
                    "label": 0
                },
                {
                    "sent": "This is not written on the slide.",
                    "label": 0
                },
                {
                    "sent": "Moreover, the same trees to the same thing is true for the maximum likelihood permutation.",
                    "label": 0
                },
                {
                    "sent": "OK, so the picture is a little complicated, so let me.",
                    "label": 0
                },
                {
                    "sent": "So what are we doing?",
                    "label": 0
                },
                {
                    "sent": "The isiro that we don't know?",
                    "label": 0
                },
                {
                    "sent": "We generate by 1 by 2.",
                    "label": 0
                },
                {
                    "sent": "Spicy.",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "Now, given this by 1 by 2\u03c0 three and so on, there is a tie, which is the maximum likelihood estimator.",
                    "label": 0
                },
                {
                    "sent": "What's easy to see and I haven't told you, is that actually the maximum likelihood estimator is not equal to \u03c0 zero.",
                    "label": 0
                },
                {
                    "sent": "If I give you a small number of examples, you know you just look at the top two element one and two.",
                    "label": 0
                },
                {
                    "sent": "There is a constant probability that they will be flipped, right?",
                    "label": 0
                },
                {
                    "sent": "SO50 and Pi are not the same permutation.",
                    "label": 0
                },
                {
                    "sent": "But what is true is that both for the permutation \u03c0 zero and for the permutation Pi, which is the maximum likely permutation, and each of the generated permutations there closed Boston average and the maximal dislocation is also small.",
                    "label": 0
                },
                {
                    "sent": "And an additional ingredient that algorithmic ingredient, once you understand the statistical property, is a dynamic programming algorithm to find \u03c0 given a starting point where each element is a distance at most K away, and the running time is exponential in this K, which would correspond to this log North, but only times a linear factor in the number of elements.",
                    "label": 1
                },
                {
                    "sent": "Questions about this example?",
                    "label": 0
                },
                {
                    "sent": "OK, I should really sell you something, but I sort of want to say so.",
                    "label": 0
                },
                {
                    "sent": "This belongs to somebody, but apparently it does this belong to somebody?",
                    "label": 0
                },
                {
                    "sent": "No, OK, I cannot tell you.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cannot be too big for this item.",
                    "label": 0
                },
                {
                    "sent": "OK, so since I'm overall a negative negative outlook of life, I'm a pessimist.",
                    "label": 0
                },
                {
                    "sent": "I have to end up with a pessimist POV and I'm going to try to annoy all of you.",
                    "label": 0
                },
                {
                    "sent": "Run Markov chain Monte Carlo's, who has more market share Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "OK, here you know, I'll try to do my best so you know, many of you know what Markov chain Monte Carlo is.",
                    "label": 0
                },
                {
                    "sent": "You know you do it when you want to sample form a chain and it's really used in huge collection of areas.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "One thing that I should say in a note because even though most of you probably know it, I should still repeat it again.",
                    "label": 0
                },
                {
                    "sent": "Sampling distribution is a problem that's computationally hard, so if somebody comes to me and tell me I have a sampling, I have a sampling algorithm which can sample any distribution that you want, even from a very restricted family.",
                    "label": 0
                },
                {
                    "sent": "Then I know that is lying and it's actually happened to me like two or three times somebody comes to you and say have a Markov chain Monte Carlo where I heat up, cool down, moving paths, do this, do that.",
                    "label": 0
                },
                {
                    "sent": "And it works for any kind of problem in a wide enough family.",
                    "label": 0
                },
                {
                    "sent": "Then I know that you lying, because then you know how to solve NP hard problems.",
                    "label": 0
                },
                {
                    "sent": "And most of you do not know how to solve NP hard problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to even think you further, because usually when I raise this objection in the Markov chain Monte Carlo Community people.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "You know what you're saying is correct, it's true.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to samples, but we have diagnostics and we run the diagnostics and if the diagnostics told us that we you know we have converged, then we know that we converge.",
                    "label": 0
                },
                {
                    "sent": "OK, so the aim of the last part of the talk is to annoy you in this aspect so the Markov chain we're imagining this person walking and walking and walking, and the diagnostic is supposed to tell him to cell phone.",
                    "label": 0
                },
                {
                    "sent": "We arrive to the right spot on.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is a vast literature on diagnostics.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about that.",
                    "label": 0
                },
                {
                    "sent": "Let me check.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You just.",
                    "label": 0
                },
                {
                    "sent": "The kind of result that you get, so these are the kind of computational results that you get, so I given a Markov chain P on say.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter which space, but let's make the space it has to be exponentially large 012 then at time T, such that either you in total variation distance.",
                    "label": 1
                },
                {
                    "sent": "Either the time it takes you to get the total version distance 1/4 is less than T or it's at least 100 T, then it is P space complete to decide if this is true or the system.",
                    "label": 0
                },
                {
                    "sent": "So in particular, what does it mean?",
                    "label": 0
                },
                {
                    "sent": "You give me a Markov chain.",
                    "label": 0
                },
                {
                    "sent": "We agreed before that.",
                    "label": 0
                },
                {
                    "sent": "You know sampling is very hard in general, but now we're going to ask a more elaborate question.",
                    "label": 0
                },
                {
                    "sent": "You were going to say ah sometimes I'm not going to know if it's converging or not converging, but every diagnostic that's going to tell me and makes me happy.",
                    "label": 0
                },
                {
                    "sent": "But what I'm telling you is that even if I give you the mixing time up to a huge factor, it is PSPACE complete to decide between these two options.",
                    "label": 0
                },
                {
                    "sent": "So what is P?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space complete, I shouldn't do this example because this example is actually confusing.",
                    "label": 0
                },
                {
                    "sent": "Piece based complete means that it's harder than all the problems that you know, but actually, the more the nicest example of that is if you play on an infinite go board and you want to know if the black wins or the wide with this is a P space complete.",
                    "label": 0
                },
                {
                    "sent": "So you cannot solve this problem, but now you say oh.",
                    "label": 0
                },
                {
                    "sent": "You silly people are so annoying.",
                    "label": 0
                },
                {
                    "sent": "You keep giving us this negative result, but I actually know because somebody told me or I have a good intuition that my Markov chain actually mixes not in exponential time, not in very big time in sort of now I have a good feeling it makes us in a reasonable amount of time.",
                    "label": 0
                },
                {
                    "sent": "So what is this story that you're telling me?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I would like to know you again, I will tell you, even if you have a promise that the mixing time is smaller than say, Ncube North Square and you want to know if it's mixing times TO N * 100 T, then it's what's called Co NP hard.",
                    "label": 0
                },
                {
                    "sent": "So even this you cannot know.",
                    "label": 0
                },
                {
                    "sent": "Not even if I promise you that the mixing times at most N squared and you just need to know if it's you know you're on your chain and you want to know as it mixed or not, as it mixed up.",
                    "label": 0
                },
                {
                    "sent": "So of course after Times Square it would have mixed, but in order to know when when is the point in time where it makes this is Co NP hard?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can ask.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More and more annoying questions, but I'll stop here.",
                    "label": 0
                },
                {
                    "sent": "Let me just wrap up, but by saying a little bit about what I think about some features of combinatorial statistics versus some of the more applied statistic approaches so.",
                    "label": 1
                },
                {
                    "sent": "This is if you happen to see my reviews on your papers in applied statistics.",
                    "label": 0
                },
                {
                    "sent": "This is what guides me.",
                    "label": 0
                },
                {
                    "sent": "So what I like is I like using explicit well defined models.",
                    "label": 1
                },
                {
                    "sent": "I like explicit, known as symptomatic running time in sample complexity bounds.",
                    "label": 0
                },
                {
                    "sent": "What often comes with these two constraint is that often you don't get statistical efficiency and asymptotic sense.",
                    "label": 1
                },
                {
                    "sent": "And also all of the results that we know how to prove inputs have to be generated according either to the correct distributions or close to it.",
                    "label": 0
                },
                {
                    "sent": "But you know, we know that often if you're given a general input, it's NP hard problems.",
                    "label": 0
                },
                {
                    "sent": "OK, I hope I amuse you and you're still awake and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have any questions left?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Don't let me know yet.",
                    "label": 0
                },
                {
                    "sent": "These are my collaborators, so the Berkeley Group is an entire Bhatnagar guy.",
                    "label": 0
                },
                {
                    "sent": "Bressler, Allan, Sly, and the book, though is now in Hong Kong.",
                    "label": 1
                },
                {
                    "sent": "Mark brother, money, Microsoft.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Silver done in Harvard.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "It's not just the problems.",
                    "label": 0
                },
                {
                    "sent": "Have the same philosophical spirit.",
                    "label": 0
                },
                {
                    "sent": "It's not the same.",
                    "label": 0
                },
                {
                    "sent": "It's not that one problem implies the other, it was really different examples, but both have to do is reconstructing discrete object in one of them.",
                    "label": 0
                },
                {
                    "sent": "It was a graph, it was the other one.",
                    "label": 0
                },
                {
                    "sent": "It was a permutation.",
                    "label": 0
                },
                {
                    "sent": "Both of them have the assumptions that you see something that's generated according to the model, and you know in one of them you actually want to reconstruct the correct graph.",
                    "label": 0
                },
                {
                    "sent": "In another one you want to reconstruct the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Station, just the same spirit.",
                    "label": 0
                },
                {
                    "sent": "It's the technology and the pools and so on so forth are completely different.",
                    "label": 0
                },
                {
                    "sent": "Series yes, sure.",
                    "label": 0
                },
                {
                    "sent": "More annoying.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but how do you know that if these two distributions are the same or not?",
                    "label": 0
                },
                {
                    "sent": "This is a very hard problem, right?",
                    "label": 0
                },
                {
                    "sent": "So it is to once I guarantee you that you're running times that most tend to the 100 you can run it for him to the 100 and you'll get good samples.",
                    "label": 0
                },
                {
                    "sent": "But then you say I, I'm going to cheat, you run until time and check if these two distributions are the same, but it's very hard to check if two distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Purpose.",
                    "label": 0
                },
                {
                    "sent": "Maybe also.",
                    "label": 0
                },
                {
                    "sent": "I don't know why this is true, right?",
                    "label": 0
                },
                {
                    "sent": "So it depends.",
                    "label": 0
                },
                {
                    "sent": "It depends what.",
                    "label": 0
                },
                {
                    "sent": "Right no, but but this is a good point, and there's actually.",
                    "label": 0
                },
                {
                    "sent": "There's actually also.",
                    "label": 0
                },
                {
                    "sent": "That actually also more annoying results, even if you make assumptions about some probability and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "It is true that if you look just at a specific statistics, say you only care about the average of the bits and you know you want until enter the 100, you see what's the distribution here?",
                    "label": 0
                },
                {
                    "sent": "Run until time and you see if you get the same distribution you done, you don't have to do anything else.",
                    "label": 0
                },
                {
                    "sent": "Right no no no no, but this here you don't even hear the result doesn't hold right.",
                    "label": 0
                },
                {
                    "sent": "In order to know if you need in order to know if you mixed or not.",
                    "label": 0
                },
                {
                    "sent": "This is a high dimensional question.",
                    "label": 0
                },
                {
                    "sent": "If you just project yourself to a low dimensional question, does a specific statistics mixed or not?",
                    "label": 0
                },
                {
                    "sent": "Then once I promise to you that in enter the other steps you mixed, you can sample the distribution runtime.",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "No, it's hard.",
                    "label": 0
                },
                {
                    "sent": "It's hard.",
                    "label": 0
                },
                {
                    "sent": "This is hard, yeah.",
                    "label": 0
                },
                {
                    "sent": "Indiana Local in hours.",
                    "label": 0
                },
                {
                    "sent": "I don't think it really.",
                    "label": 0
                },
                {
                    "sent": "I don't think this really comes into two heavily into the right, so there is not is very impressive computationally.",
                    "label": 0
                },
                {
                    "sent": "Definitely in.",
                    "label": 0
                },
                {
                    "sent": "In particular the number of edges that does not come in any significant way, and our result in our results definitely does not use the number of edges.",
                    "label": 0
                },
                {
                    "sent": "And similarly ability is or does not use the number of edges.",
                    "label": 0
                },
                {
                    "sent": "No, there is an issue which is an interesting issue that the two problems are actually different.",
                    "label": 0
                },
                {
                    "sent": "In one of them, look at graphical models where there are clicks and the clicks make it a much higher dimensional problem.",
                    "label": 0
                },
                {
                    "sent": "In the United all kind of problem that only edges and this makes the dimension must much smaller.",
                    "label": 0
                },
                {
                    "sent": "So this is where somehow these two setups are different.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}