{
    "id": "gqhdssnedks4gtbjcety5vt7yufdgzgc",
    "title": "SINCO - An Efficient Greedy Method for Learning Sparse INverse COvariance Matrix",
    "info": {
        "author": [
            "Katya Scheinberg, Lehigh University"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_scheinberg_egm/",
    "segmentation": [
        [
            "OK, so this morning I had a little bit of an eye injury, that's why I'm wearing this glass is not a fashion statement, but it insisted that I, you know, justified somehow in my talk and it inspired me to one of my favorite quotes, which is actually very relevant here in this."
        ],
        [
            "So let you read it for yourself.",
            "So you probably probably most of you had seen the matrix, so you understand the dark glasses relevance to the matrix, but I really like this.",
            "This matrix is around us.",
            "It's now in this very room basically describes our life and it's by the way it's the world.",
            "The world that has been pulled over your eyes to blind us from the truth.",
            "That's very important.",
            "OK, So what are the matrices we're going to be talking about?",
            "Yes, we're going to be talking about the inverse covariance matrices the same as dumb law was talking about this morning.",
            "And she gave the justification much better than I will, so I'll just, you know, very quickly.",
            "Go over this.",
            "Basically, we assume that we have random variables, P of them an we."
        ],
        [
            "There's a multivariate Gaussian distribution associated with those, and the fact is that if some of the variables are conditionally independent, it will manifest itself in zeros in the appropriate entries of the inverse of the covariance matrix.",
            "So given some observations, we have some observations.",
            "Here we're going to try to match these observations by maximizing log likelihood."
        ],
        [
            "But we also would like to have the sparsity of the inverse covariance, because if indeed some variables are conditionally independent, that is important for interpretability and sometimes prediction as well.",
            "And of course the problem is that if you just take the maximum log log, likelihood and invert.",
            "And sorry, and solve and optimize that.",
            "What you will get is the inverse of the covariance will be just the inverse of your empirical covariance matrix, which is from now on will be denoted as a. OK, and basically that's a is just basically.",
            "You know that that particular thing was, you know, computers from the data, right?",
            "And the inverse of that is almost never sparse, so you cannot recover sparsity from just the observations.",
            "You have to enforce them, and you know the classical approach people have been discussing here.",
            "A lot is instead of minimizing or some."
        ],
        [
            "How constraining the cardinality of the matrix so from now on we are going to denote the variable matrix as C. The when we optimize, we get the empirical covariance size inverse, but we'll just call it matrix C. And so she is the thing we're optimizing over an we're optimizing this part, which is the log likelihood, and this part, which is just the number of non zero elements in C. But of course instead we will replace it with the L1 norm, right?",
            "There's a proxy for the Olympics in normal.",
            "OK, so we have a convex problem is exactly the same as Amla had discussed this morning.",
            "So a convex problem and you can have instead of the one norm here.",
            "You can have a weighted one norm.",
            "And this is by the way, not the matrix normal, right?",
            "This is just the sum of the elements, absolute absolute values of elements."
        ],
        [
            "Matrix.",
            "So OK, so we're not going to be talking about how to choose Lambda, but this is a big problem, right?",
            "So why?",
            "You know how?",
            "How sparse do we want our problem to be?",
            "So basically, the sparsity is not very well.",
            "Oh I mean this is not known in advance, probably, or maybe some level of sparsity is desired by the user, but you have no idea how to achieve that particular level of sparsity.",
            "So what people do is play with the parameter Lambda, so clearly."
        ],
        [
            "Lambda let me go back for a second.",
            "If Lambda is very large then this matrix is going to be basically diagonal.",
            "Now something is going to be 0 because of the log that term it cannot possibly get zeros on the diagonal, so you might actually decide not even penalize the tag."
        ],
        [
            "Elements here, which is fine too, but you can.",
            "You will get very sparse matrix with large Lambda.",
            "You will get basically just the inverse of the of a when you get a very small Lambda approximately inversely.",
            "So varying Lambda you will get kind of all levels of sparsity.",
            "So if you start from very large Lambda and slowly decrease it you will get more and more non zero elements in your matrix."
        ],
        [
            "OK, and then you plot.",
            "You can plot that thing so you what you plot is if you know the true structure you're trying to recover, you can plot how many positives you guessed correctly and how many positives you guessed incorrectly.",
            "How many nonzero elements of a you have?",
            "That should be there and how many non zero elements of a that shouldn't be there an what you want is.",
            "You want a lot of 090 that should be there and very little.",
            "The ones that shouldn't, right?",
            "And if you plot one against the other, this is what's called RC curve.",
            "You probably all of you know it way better than I do.",
            "OK, so.",
            "This is this is a side.",
            "This is what we're just looking at from work for a moment.",
            "Let's just concentrate on the problem itself.",
            "So we have this primal problem.",
            "Uh and OK.",
            "So this is the.",
            "Problem we had in the dual problem has this particular nice formulation where you're looking for.",
            "Again, you're minimizing log that term of another matrix which happens to be an inverse of C. And the optimal solution and this matrix has to be basically not very different from your empirical covariance, right?",
            "So that kind of makes sense.",
            "You're trying to find a matrix which would be the inverse of C. So the covariance matrix.",
            "So the dual problem looks for a covariance matrix and that covariance matrix would not be very different from the.",
            "You know from the empirical covariance and then how different is actually defined by this Lambda parameters.",
            "It's a robust interpretation of this problem.",
            "OK, so many methods, including what damala was talking about.",
            "Look at the.",
            "Dual problem, dual formulation, but not necessarily so."
        ],
        [
            "As a matter of fact, more recently, so these are the methods I will not be comparing to, and I'll explain to you for what reason.",
            "So there is several gradient based methods.",
            "Well, this one is not was not implemented as far as I know, but we've discussed in this application of nesters smoothing technique.",
            "Well actually not even selectable 1st order method to this problem.",
            "There is projected gradient.",
            "There is variant of smoothing method by Lou from I think last year and also we are working on this method.",
            "So right now, which is, I think, a little bit faster than these and also has a convergence guarantees.",
            "So all these methods?",
            "Well, I don't know about this one, but these methods have convergence guarantees in terms or.",
            "Number of iterations they would take.",
            "They are all quite fast for a first order method, so this basically as fast as it gets.",
            "The If you really need to solve this problem very fast.",
            "In general I would go for one of these.",
            "I would not go for what I will be talking about or geloso which we were.",
            "You know we are comparing against, so why am I talking about what I'm talking about?",
            "So these are methods that are gradient methods, which means that they take the matrix and they changed the whole thing.",
            "And they make a step and then they make another step, and so on until they converge somewhere, in which case, in the limit they recover the zero structure.",
            "OK, it's sometimes not trivial to do so because you have very little elements.",
            "Would they be 0?",
            "Should be not zero.",
            "Depends on the accuracy and so on, but the point is that the zero structure is typically recovered in the limit, so nowhere in the process you have a sparse matrix.",
            "OK, even though you're looking for a sparse matrix.",
            "OK.",
            "So that's what we that's why we leave these methods behind each iteration of these methods is Ncube P cube.",
            "Sorry, P is the number of variables PQ OK?",
            "You know it's not not horrible, but it's not dirt cheap either.",
            "But again, these are good methods."
        ],
        [
            "Just that they don't have the property that we want.",
            "So, and we really want to look at the methods that generate.",
            "Sparsity structure or non sparsity structure slowly starts with a very sparse matrix and slowly builds it up OK.",
            "So, uh.",
            "So far there has been block coordinate descent.",
            "What does block coordinate descent do?",
            "It basically looks at the dual matrix.",
            "OK, so we look at the dual matrix.",
            "We fix a whole matrix except for one row and one column.",
            "Let's say last row in the last point.",
            "Without loss of generality.",
            "And then we basically look at the what will become of the dual problem.",
            "So we can reformulate the dual problem and do some tricks on it, and then basically we'll get this.",
            "OK, and this is a quadratic problem.",
            "You can see.",
            "So this is a matrix that's fixed because we fixed the whole thing here, except for this we're looking for the row and a column.",
            "And if we rewrite things here, we get some kind of constraints.",
            "And we get a quadratic convex quadratic objective, right?",
            "This is a convex.",
            "This is a positive semidefinite positive definite matrix.",
            "As a matter of fact.",
            "And this turns out the dual of that problem is actually a lasso problem, and this is exactly what.",
            "You know Gypsy, Ronnie and Company used in the Geloso algorithm.",
            "They use the fact that this is the last problem and then therefore they can use it.",
            "They can just.",
            "Plug in their last soldier for every iteration here, OK?",
            "OK, So what does the law social would do?"
        ],
        [
            "Well, it's actually so there are several ones, and I at first I thought they were using large, but actually they're using their also coordinated descent, so they're doing block coordinate descent within which they are doing coordinate descent.",
            "So in some sense they're doing the same thing that will be doing, but in a.",
            "In a way, in a strange order.",
            "And perhaps it's better from the computational perspective in terms of speed, but I'm not sure.",
            "You know?",
            "Yeah, we have to really do a proper comparison.",
            "So what do they do?",
            "They have this problem.",
            "This is the last problem, right?",
            "Everybody knows that they fix all the variables except for one.",
            "They get a small quadratic function in one variable.",
            "OK, and they basically well.",
            "It's a quadratic function plus the absolute value, and we know how to solve that very quickly.",
            "There is this thresholding operator and basically that's it, so they they do.",
            "Updates for a variable by solving this quadratic function so they go and optimize the quadratic every time for one variable, and then they go to the next variable and then they go to the next variable, and so on.",
            "An if you go back here, what they do is they solved."
        ],
        [
            "This problem, like that for one row in one column and then they move to the next row in next column and then the next one and the next one OK. Um so.",
            "Basically the question is why such order of choosing you know the variables to optimize over, so you could.",
            "I mean there are many ways to choose a coordinate descent have been around for a long time and there are many different.",
            "Variants of them, all of them, starting with Gauss, something so Gauss Seidel goes South well whatever, and they differ.",
            "By the way, the variables that chosen and updated so you can for example, cycle through all the coordinates, right?",
            "Which I don't know.",
            "I am not sure why it's not slow.",
            "I mean it may be slow, it may be so.",
            "This is somewhat seems they're doing.",
            "They say it works fast, but we don't really know what exactly they're doing.",
            "It's not even in their paper, let alone we don't know what's happening in the implementation, so you can update all the variables at once, but that doesn't work for this problem, because you actually will violate positive semidefinite list.",
            "You can also choose the variable with the largest gradient component, but at least you know in what we observed with this problem.",
            "It doesn't work very well.",
            "Or you can be sort of extreme and basically choose.",
            "The coordinates which give you the largest improvement, so you're actually doing the exact line search along that coordinate, right?",
            "And you will be able to find the exact improvement this variable gives you in the objective function.",
            "So what we're suggesting is choose the one that gives you the largest improvement.",
            "OK, so it sounds like it's expensive step, but in the second will understand that it's not.",
            "OK, so that's what we want to do and why do we want to do that?",
            "Well, when you think about it, you have a sparse matrix, and if you want to choose the next element that should be non 0 right?",
            "If you kind of iteratively iteratively want to build up your non zero structure.",
            "It seems reasonable to choose the variable that gives you the best improvement of the objective function you're trying to optimize, right?",
            "But I can't prove any statement so far about that, but intuitively it seems like a reasonable thing.",
            "OK, So what does it mean to choose one variable in a matrix?",
            "There's no, we're not going to look at.",
            "You know, particularly columns and rows.",
            "We're going to look at the whole matrix, so that's why we step back from their method, and we're saying, OK, we have a matrix.",
            "We are going to look for and I am updating and I JS element of it inj right.",
            "It's the same as saying I'm going to Malta symmetric matrix so I have to update the ijaaf element and J."
        ],
        [
            "If element right?",
            "So it's the same as basically adding some Alpha.",
            "To the IJ's element, and I can write it like this.",
            "This is a sparse rank to update to the matrix, OK?",
            "And then the determinant of the matrix has this following expression.",
            "What is important here is the determinant of C remains fixed.",
            "We're only really changing this part, and what are these guys?",
            "These guys are the elements of the gradient.",
            "The W the inverse.",
            "It's the same as basically the gradient in some sense, so the inverse of if the inverse of C is given, we can compute the change to the objective function in these terms.",
            "OK, if we if we change the element of C by Alpha, we can compute the change in the objective function.",
            "And we can find the minimum of that change so it becomes a 1 dimensional function which we can minimize by solving quadratic equation.",
            "So it's a.",
            "It's a few steps, it's not dependent on dimension, it's just I don't know 10 arithmetic operations or something right?",
            "To do the exact line search for every specific element IJ.",
            "OK so I can do now.",
            "So how many elements do I have?",
            "I have P squared elements, right?",
            "That's how many variables I have in my problem?",
            "I don't have to look at all of them if I don't want to.",
            "But the point is, I can look at all of them if I look at all of them, then choosing the best step so I can compute the step for all the elements and choosing the best one will be linear in the number of variables of P ^2.",
            "Which is the same as it takes to update the gradient after I've done one step.",
            "So after I do a step after I've chosen the coordinates to change, it takes me the same amount of time to update the W OK.",
            "So in a way, it's it's a balance.",
            "If it takes me that much to update the gradient, I better do a good job choosing the next coordinate to change.",
            "And basically these things are balanced because they take about the same time.",
            "There is a lot of improvement that can be done there in implementation, but that's basically the idea.",
            "OK, So what this this idea buys us is basically this greedy approach as I was saying we are choosing coordinates one at a time.",
            "OK, so again we are 4."
        ],
        [
            "Each pair IG we do the exact line search and then in that many operations we choose the best and then in that many operation we have data matrix and also it's very easy.",
            "I mean you can see that it's extremely easy to parallelize basically because everything is just done, kind of independently and even."
        ],
        [
            "Data the matrix can be.",
            "It can be stored separately and updated separately.",
            "OK, alright, so now about a little bit about the."
        ],
        [
            "Testing environment.",
            "So OK, let me just cut to the chase.",
            "So what?"
        ],
        [
            "What is it that we've observed?",
            "What we observe this?",
            "If we run this algorithm OK, and we intermittently observe the solutions and then we plot how many nonzeros we guessed correctly and how many we did not guess correctly.",
            "So basically we will get."
        ],
        [
            "The same orosi curves is pretty much as we would get by changing Lambda itself.",
            "Now here we're not changing Lambda, we just choose some Lambda, small enough, some.",
            "I mean, there is a good idea to have some regularization, and then we're slowly applying this algorithm, and in blue we have the steps of the algorithm here and in red we have the RC curve obtained by changing Lambda and you see they are basically the same.",
            "OK with you.",
            "I mean, this just shows that with geloso you cannot do that.",
            "It doesn't have this, you know nature of growth."
        ],
        [
            "Things slowly OK. OK, let me not explain that because it's going to take awhile.",
            "So this this also shows basically that even in a bad case these are good orosi curves.",
            "And this is a bad one.",
            "And still they are very close to each other very quickly.",
            "What this slide shows is basically the bottom line of this slide is that.",
            "I mean, are they really the same paths these two are they producing the same non zeros?",
            "This is just the number of true positive versus false positives, right?",
            "This is not this.",
            "We don't know if the same.",
            "Elements that are produced and it turns out that they basically produce these two paths, produce more or less the same now."
        ],
        [
            "Zero elements and until the basically garbage starts appearing OK.",
            "So if it's a good good."
        ],
        [
            "Blim and really can generate the good."
        ],
        [
            "Amount of non zeros, they're correct, non zeros 1st and then you start generating the non necessary non zeros because if you learn this small you eventually will generate all the non zeros that you don't want.",
            "But you will."
        ],
        [
            "Generate so you will basically start producing garbage when.",
            "When it's you know when, when it basically has to appear in some sense.",
            "OK, so this is.",
            "This slide shows basically that if you increase the amount of data, you will actually end up with sparsest solution with this problem.",
            "And so on.",
            "I'm not going to even try to go over these things."
        ],
        [
            "'cause I don't have the time.",
            "So let me just do the bottom line.",
            "So basically this algorithm can be used.",
            "In some sense an an.",
            "Again we can improve it probably substantially, so it's more of an idea rather than the specific implementation that I'm trying to advertise here, but.",
            "You know you can basically run it until you get the desired level of sparsity, which was not possible before with the Lambda, because with Lambda you have to play.",
            "You don't know what level of sparsity is going to give you.",
            "With this algorithm, you basically produce the solution slowly until you're satisfied with the amount of sparsity you got.",
            "Or you can cross validate what solutions.",
            "In the process and you know.",
            "So basically it produces a path of solutions by itself.",
            "This algorithm, not varying Lambda.",
            "And yeah, and then basically it's very very simple algorithm.",
            "I mean, it's really dumb.",
            "Every step is computed by solving quadratic equation.",
            "It can paralyze it.",
            "You can probably improve it dramatically and you can maybe even include different.",
            "Instead of L1 norm, you can maybe do some other separable functions, basically which we may explore at some point.",
            "I'll stop here.",
            "I'm sorry.",
            "Yeah.",
            "No, no, no.",
            "That's the whole point.",
            "It's not P, it's it's a.",
            "It's a finite number of operations.",
            "I mean, not finite, but it's of course find it.",
            "But it independent of.",
            "It's just solving a quadratic equation in one variable.",
            "Yeah.",
            "Do what?",
            "No no no no no I don't I I when I when I do need it to compute I don't need it because I mean I'm maintaining the inverse and that inverse gives me all the information I need to update the universe.",
            "I need to do a rank to update to it which is of P squared.",
            "But it's again all P squared not of the cube.",
            "Yeah, but I it's old P squared but I.",
            "No, no, but right for each point I'm doing with the square.",
            "That's correct, yes.",
            "So if I need to update all elements in my matrix eventually if I need if they're all not going to be non zeros.",
            "Of course I will be very expensive.",
            "This is not a good method for that for sure.",
            "This is only good when you have a fairly sparse."
        ],
        [
            "Solution that you're looking for, and so most of the elements is hopefully never going to even look at.",
            "OK, no no.",
            "No look at it.",
            "Might look at them.",
            "It might not change them.",
            "So yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this morning I had a little bit of an eye injury, that's why I'm wearing this glass is not a fashion statement, but it insisted that I, you know, justified somehow in my talk and it inspired me to one of my favorite quotes, which is actually very relevant here in this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let you read it for yourself.",
                    "label": 0
                },
                {
                    "sent": "So you probably probably most of you had seen the matrix, so you understand the dark glasses relevance to the matrix, but I really like this.",
                    "label": 0
                },
                {
                    "sent": "This matrix is around us.",
                    "label": 1
                },
                {
                    "sent": "It's now in this very room basically describes our life and it's by the way it's the world.",
                    "label": 1
                },
                {
                    "sent": "The world that has been pulled over your eyes to blind us from the truth.",
                    "label": 1
                },
                {
                    "sent": "That's very important.",
                    "label": 0
                },
                {
                    "sent": "OK, So what are the matrices we're going to be talking about?",
                    "label": 0
                },
                {
                    "sent": "Yes, we're going to be talking about the inverse covariance matrices the same as dumb law was talking about this morning.",
                    "label": 0
                },
                {
                    "sent": "And she gave the justification much better than I will, so I'll just, you know, very quickly.",
                    "label": 0
                },
                {
                    "sent": "Go over this.",
                    "label": 0
                },
                {
                    "sent": "Basically, we assume that we have random variables, P of them an we.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a multivariate Gaussian distribution associated with those, and the fact is that if some of the variables are conditionally independent, it will manifest itself in zeros in the appropriate entries of the inverse of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So given some observations, we have some observations.",
                    "label": 0
                },
                {
                    "sent": "Here we're going to try to match these observations by maximizing log likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we also would like to have the sparsity of the inverse covariance, because if indeed some variables are conditionally independent, that is important for interpretability and sometimes prediction as well.",
                    "label": 1
                },
                {
                    "sent": "And of course the problem is that if you just take the maximum log log, likelihood and invert.",
                    "label": 1
                },
                {
                    "sent": "And sorry, and solve and optimize that.",
                    "label": 0
                },
                {
                    "sent": "What you will get is the inverse of the covariance will be just the inverse of your empirical covariance matrix, which is from now on will be denoted as a. OK, and basically that's a is just basically.",
                    "label": 0
                },
                {
                    "sent": "You know that that particular thing was, you know, computers from the data, right?",
                    "label": 0
                },
                {
                    "sent": "And the inverse of that is almost never sparse, so you cannot recover sparsity from just the observations.",
                    "label": 0
                },
                {
                    "sent": "You have to enforce them, and you know the classical approach people have been discussing here.",
                    "label": 1
                },
                {
                    "sent": "A lot is instead of minimizing or some.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How constraining the cardinality of the matrix so from now on we are going to denote the variable matrix as C. The when we optimize, we get the empirical covariance size inverse, but we'll just call it matrix C. And so she is the thing we're optimizing over an we're optimizing this part, which is the log likelihood, and this part, which is just the number of non zero elements in C. But of course instead we will replace it with the L1 norm, right?",
                    "label": 0
                },
                {
                    "sent": "There's a proxy for the Olympics in normal.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have a convex problem is exactly the same as Amla had discussed this morning.",
                    "label": 0
                },
                {
                    "sent": "So a convex problem and you can have instead of the one norm here.",
                    "label": 0
                },
                {
                    "sent": "You can have a weighted one norm.",
                    "label": 0
                },
                {
                    "sent": "And this is by the way, not the matrix normal, right?",
                    "label": 0
                },
                {
                    "sent": "This is just the sum of the elements, absolute absolute values of elements.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "So OK, so we're not going to be talking about how to choose Lambda, but this is a big problem, right?",
                    "label": 0
                },
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "You know how?",
                    "label": 0
                },
                {
                    "sent": "How sparse do we want our problem to be?",
                    "label": 0
                },
                {
                    "sent": "So basically, the sparsity is not very well.",
                    "label": 0
                },
                {
                    "sent": "Oh I mean this is not known in advance, probably, or maybe some level of sparsity is desired by the user, but you have no idea how to achieve that particular level of sparsity.",
                    "label": 0
                },
                {
                    "sent": "So what people do is play with the parameter Lambda, so clearly.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lambda let me go back for a second.",
                    "label": 0
                },
                {
                    "sent": "If Lambda is very large then this matrix is going to be basically diagonal.",
                    "label": 0
                },
                {
                    "sent": "Now something is going to be 0 because of the log that term it cannot possibly get zeros on the diagonal, so you might actually decide not even penalize the tag.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elements here, which is fine too, but you can.",
                    "label": 0
                },
                {
                    "sent": "You will get very sparse matrix with large Lambda.",
                    "label": 0
                },
                {
                    "sent": "You will get basically just the inverse of the of a when you get a very small Lambda approximately inversely.",
                    "label": 0
                },
                {
                    "sent": "So varying Lambda you will get kind of all levels of sparsity.",
                    "label": 0
                },
                {
                    "sent": "So if you start from very large Lambda and slowly decrease it you will get more and more non zero elements in your matrix.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and then you plot.",
                    "label": 0
                },
                {
                    "sent": "You can plot that thing so you what you plot is if you know the true structure you're trying to recover, you can plot how many positives you guessed correctly and how many positives you guessed incorrectly.",
                    "label": 0
                },
                {
                    "sent": "How many nonzero elements of a you have?",
                    "label": 0
                },
                {
                    "sent": "That should be there and how many non zero elements of a that shouldn't be there an what you want is.",
                    "label": 0
                },
                {
                    "sent": "You want a lot of 090 that should be there and very little.",
                    "label": 0
                },
                {
                    "sent": "The ones that shouldn't, right?",
                    "label": 0
                },
                {
                    "sent": "And if you plot one against the other, this is what's called RC curve.",
                    "label": 0
                },
                {
                    "sent": "You probably all of you know it way better than I do.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This is this is a side.",
                    "label": 0
                },
                {
                    "sent": "This is what we're just looking at from work for a moment.",
                    "label": 0
                },
                {
                    "sent": "Let's just concentrate on the problem itself.",
                    "label": 0
                },
                {
                    "sent": "So we have this primal problem.",
                    "label": 0
                },
                {
                    "sent": "Uh and OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the.",
                    "label": 0
                },
                {
                    "sent": "Problem we had in the dual problem has this particular nice formulation where you're looking for.",
                    "label": 0
                },
                {
                    "sent": "Again, you're minimizing log that term of another matrix which happens to be an inverse of C. And the optimal solution and this matrix has to be basically not very different from your empirical covariance, right?",
                    "label": 0
                },
                {
                    "sent": "So that kind of makes sense.",
                    "label": 0
                },
                {
                    "sent": "You're trying to find a matrix which would be the inverse of C. So the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So the dual problem looks for a covariance matrix and that covariance matrix would not be very different from the.",
                    "label": 0
                },
                {
                    "sent": "You know from the empirical covariance and then how different is actually defined by this Lambda parameters.",
                    "label": 0
                },
                {
                    "sent": "It's a robust interpretation of this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so many methods, including what damala was talking about.",
                    "label": 0
                },
                {
                    "sent": "Look at the.",
                    "label": 0
                },
                {
                    "sent": "Dual problem, dual formulation, but not necessarily so.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As a matter of fact, more recently, so these are the methods I will not be comparing to, and I'll explain to you for what reason.",
                    "label": 0
                },
                {
                    "sent": "So there is several gradient based methods.",
                    "label": 0
                },
                {
                    "sent": "Well, this one is not was not implemented as far as I know, but we've discussed in this application of nesters smoothing technique.",
                    "label": 0
                },
                {
                    "sent": "Well actually not even selectable 1st order method to this problem.",
                    "label": 1
                },
                {
                    "sent": "There is projected gradient.",
                    "label": 1
                },
                {
                    "sent": "There is variant of smoothing method by Lou from I think last year and also we are working on this method.",
                    "label": 0
                },
                {
                    "sent": "So right now, which is, I think, a little bit faster than these and also has a convergence guarantees.",
                    "label": 0
                },
                {
                    "sent": "So all these methods?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know about this one, but these methods have convergence guarantees in terms or.",
                    "label": 0
                },
                {
                    "sent": "Number of iterations they would take.",
                    "label": 0
                },
                {
                    "sent": "They are all quite fast for a first order method, so this basically as fast as it gets.",
                    "label": 1
                },
                {
                    "sent": "The If you really need to solve this problem very fast.",
                    "label": 0
                },
                {
                    "sent": "In general I would go for one of these.",
                    "label": 0
                },
                {
                    "sent": "I would not go for what I will be talking about or geloso which we were.",
                    "label": 0
                },
                {
                    "sent": "You know we are comparing against, so why am I talking about what I'm talking about?",
                    "label": 0
                },
                {
                    "sent": "So these are methods that are gradient methods, which means that they take the matrix and they changed the whole thing.",
                    "label": 0
                },
                {
                    "sent": "And they make a step and then they make another step, and so on until they converge somewhere, in which case, in the limit they recover the zero structure.",
                    "label": 0
                },
                {
                    "sent": "OK, it's sometimes not trivial to do so because you have very little elements.",
                    "label": 0
                },
                {
                    "sent": "Would they be 0?",
                    "label": 0
                },
                {
                    "sent": "Should be not zero.",
                    "label": 0
                },
                {
                    "sent": "Depends on the accuracy and so on, but the point is that the zero structure is typically recovered in the limit, so nowhere in the process you have a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, even though you're looking for a sparse matrix.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's what we that's why we leave these methods behind each iteration of these methods is Ncube P cube.",
                    "label": 0
                },
                {
                    "sent": "Sorry, P is the number of variables PQ OK?",
                    "label": 0
                },
                {
                    "sent": "You know it's not not horrible, but it's not dirt cheap either.",
                    "label": 0
                },
                {
                    "sent": "But again, these are good methods.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just that they don't have the property that we want.",
                    "label": 0
                },
                {
                    "sent": "So, and we really want to look at the methods that generate.",
                    "label": 0
                },
                {
                    "sent": "Sparsity structure or non sparsity structure slowly starts with a very sparse matrix and slowly builds it up OK.",
                    "label": 0
                },
                {
                    "sent": "So, uh.",
                    "label": 0
                },
                {
                    "sent": "So far there has been block coordinate descent.",
                    "label": 1
                },
                {
                    "sent": "What does block coordinate descent do?",
                    "label": 1
                },
                {
                    "sent": "It basically looks at the dual matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, so we look at the dual matrix.",
                    "label": 0
                },
                {
                    "sent": "We fix a whole matrix except for one row and one column.",
                    "label": 1
                },
                {
                    "sent": "Let's say last row in the last point.",
                    "label": 0
                },
                {
                    "sent": "Without loss of generality.",
                    "label": 0
                },
                {
                    "sent": "And then we basically look at the what will become of the dual problem.",
                    "label": 0
                },
                {
                    "sent": "So we can reformulate the dual problem and do some tricks on it, and then basically we'll get this.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is a quadratic problem.",
                    "label": 0
                },
                {
                    "sent": "You can see.",
                    "label": 0
                },
                {
                    "sent": "So this is a matrix that's fixed because we fixed the whole thing here, except for this we're looking for the row and a column.",
                    "label": 0
                },
                {
                    "sent": "And if we rewrite things here, we get some kind of constraints.",
                    "label": 0
                },
                {
                    "sent": "And we get a quadratic convex quadratic objective, right?",
                    "label": 0
                },
                {
                    "sent": "This is a convex.",
                    "label": 0
                },
                {
                    "sent": "This is a positive semidefinite positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "As a matter of fact.",
                    "label": 0
                },
                {
                    "sent": "And this turns out the dual of that problem is actually a lasso problem, and this is exactly what.",
                    "label": 0
                },
                {
                    "sent": "You know Gypsy, Ronnie and Company used in the Geloso algorithm.",
                    "label": 0
                },
                {
                    "sent": "They use the fact that this is the last problem and then therefore they can use it.",
                    "label": 0
                },
                {
                    "sent": "They can just.",
                    "label": 0
                },
                {
                    "sent": "Plug in their last soldier for every iteration here, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, So what does the law social would do?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it's actually so there are several ones, and I at first I thought they were using large, but actually they're using their also coordinated descent, so they're doing block coordinate descent within which they are doing coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "So in some sense they're doing the same thing that will be doing, but in a.",
                    "label": 0
                },
                {
                    "sent": "In a way, in a strange order.",
                    "label": 0
                },
                {
                    "sent": "And perhaps it's better from the computational perspective in terms of speed, but I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we have to really do a proper comparison.",
                    "label": 0
                },
                {
                    "sent": "So what do they do?",
                    "label": 0
                },
                {
                    "sent": "They have this problem.",
                    "label": 0
                },
                {
                    "sent": "This is the last problem, right?",
                    "label": 0
                },
                {
                    "sent": "Everybody knows that they fix all the variables except for one.",
                    "label": 0
                },
                {
                    "sent": "They get a small quadratic function in one variable.",
                    "label": 0
                },
                {
                    "sent": "OK, and they basically well.",
                    "label": 0
                },
                {
                    "sent": "It's a quadratic function plus the absolute value, and we know how to solve that very quickly.",
                    "label": 0
                },
                {
                    "sent": "There is this thresholding operator and basically that's it, so they they do.",
                    "label": 0
                },
                {
                    "sent": "Updates for a variable by solving this quadratic function so they go and optimize the quadratic every time for one variable, and then they go to the next variable and then they go to the next variable, and so on.",
                    "label": 0
                },
                {
                    "sent": "An if you go back here, what they do is they solved.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem, like that for one row in one column and then they move to the next row in next column and then the next one and the next one OK. Um so.",
                    "label": 0
                },
                {
                    "sent": "Basically the question is why such order of choosing you know the variables to optimize over, so you could.",
                    "label": 0
                },
                {
                    "sent": "I mean there are many ways to choose a coordinate descent have been around for a long time and there are many different.",
                    "label": 0
                },
                {
                    "sent": "Variants of them, all of them, starting with Gauss, something so Gauss Seidel goes South well whatever, and they differ.",
                    "label": 0
                },
                {
                    "sent": "By the way, the variables that chosen and updated so you can for example, cycle through all the coordinates, right?",
                    "label": 1
                },
                {
                    "sent": "Which I don't know.",
                    "label": 0
                },
                {
                    "sent": "I am not sure why it's not slow.",
                    "label": 0
                },
                {
                    "sent": "I mean it may be slow, it may be so.",
                    "label": 0
                },
                {
                    "sent": "This is somewhat seems they're doing.",
                    "label": 0
                },
                {
                    "sent": "They say it works fast, but we don't really know what exactly they're doing.",
                    "label": 0
                },
                {
                    "sent": "It's not even in their paper, let alone we don't know what's happening in the implementation, so you can update all the variables at once, but that doesn't work for this problem, because you actually will violate positive semidefinite list.",
                    "label": 0
                },
                {
                    "sent": "You can also choose the variable with the largest gradient component, but at least you know in what we observed with this problem.",
                    "label": 1
                },
                {
                    "sent": "It doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "Or you can be sort of extreme and basically choose.",
                    "label": 0
                },
                {
                    "sent": "The coordinates which give you the largest improvement, so you're actually doing the exact line search along that coordinate, right?",
                    "label": 0
                },
                {
                    "sent": "And you will be able to find the exact improvement this variable gives you in the objective function.",
                    "label": 1
                },
                {
                    "sent": "So what we're suggesting is choose the one that gives you the largest improvement.",
                    "label": 0
                },
                {
                    "sent": "OK, so it sounds like it's expensive step, but in the second will understand that it's not.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what we want to do and why do we want to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, when you think about it, you have a sparse matrix, and if you want to choose the next element that should be non 0 right?",
                    "label": 0
                },
                {
                    "sent": "If you kind of iteratively iteratively want to build up your non zero structure.",
                    "label": 0
                },
                {
                    "sent": "It seems reasonable to choose the variable that gives you the best improvement of the objective function you're trying to optimize, right?",
                    "label": 0
                },
                {
                    "sent": "But I can't prove any statement so far about that, but intuitively it seems like a reasonable thing.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does it mean to choose one variable in a matrix?",
                    "label": 0
                },
                {
                    "sent": "There's no, we're not going to look at.",
                    "label": 0
                },
                {
                    "sent": "You know, particularly columns and rows.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at the whole matrix, so that's why we step back from their method, and we're saying, OK, we have a matrix.",
                    "label": 0
                },
                {
                    "sent": "We are going to look for and I am updating and I JS element of it inj right.",
                    "label": 0
                },
                {
                    "sent": "It's the same as saying I'm going to Malta symmetric matrix so I have to update the ijaaf element and J.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If element right?",
                    "label": 0
                },
                {
                    "sent": "So it's the same as basically adding some Alpha.",
                    "label": 0
                },
                {
                    "sent": "To the IJ's element, and I can write it like this.",
                    "label": 0
                },
                {
                    "sent": "This is a sparse rank to update to the matrix, OK?",
                    "label": 0
                },
                {
                    "sent": "And then the determinant of the matrix has this following expression.",
                    "label": 0
                },
                {
                    "sent": "What is important here is the determinant of C remains fixed.",
                    "label": 0
                },
                {
                    "sent": "We're only really changing this part, and what are these guys?",
                    "label": 0
                },
                {
                    "sent": "These guys are the elements of the gradient.",
                    "label": 0
                },
                {
                    "sent": "The W the inverse.",
                    "label": 0
                },
                {
                    "sent": "It's the same as basically the gradient in some sense, so the inverse of if the inverse of C is given, we can compute the change to the objective function in these terms.",
                    "label": 0
                },
                {
                    "sent": "OK, if we if we change the element of C by Alpha, we can compute the change in the objective function.",
                    "label": 0
                },
                {
                    "sent": "And we can find the minimum of that change so it becomes a 1 dimensional function which we can minimize by solving quadratic equation.",
                    "label": 0
                },
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a few steps, it's not dependent on dimension, it's just I don't know 10 arithmetic operations or something right?",
                    "label": 0
                },
                {
                    "sent": "To do the exact line search for every specific element IJ.",
                    "label": 0
                },
                {
                    "sent": "OK so I can do now.",
                    "label": 0
                },
                {
                    "sent": "So how many elements do I have?",
                    "label": 0
                },
                {
                    "sent": "I have P squared elements, right?",
                    "label": 0
                },
                {
                    "sent": "That's how many variables I have in my problem?",
                    "label": 0
                },
                {
                    "sent": "I don't have to look at all of them if I don't want to.",
                    "label": 0
                },
                {
                    "sent": "But the point is, I can look at all of them if I look at all of them, then choosing the best step so I can compute the step for all the elements and choosing the best one will be linear in the number of variables of P ^2.",
                    "label": 0
                },
                {
                    "sent": "Which is the same as it takes to update the gradient after I've done one step.",
                    "label": 0
                },
                {
                    "sent": "So after I do a step after I've chosen the coordinates to change, it takes me the same amount of time to update the W OK.",
                    "label": 0
                },
                {
                    "sent": "So in a way, it's it's a balance.",
                    "label": 0
                },
                {
                    "sent": "If it takes me that much to update the gradient, I better do a good job choosing the next coordinate to change.",
                    "label": 0
                },
                {
                    "sent": "And basically these things are balanced because they take about the same time.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of improvement that can be done there in implementation, but that's basically the idea.",
                    "label": 0
                },
                {
                    "sent": "OK, So what this this idea buys us is basically this greedy approach as I was saying we are choosing coordinates one at a time.",
                    "label": 0
                },
                {
                    "sent": "OK, so again we are 4.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each pair IG we do the exact line search and then in that many operations we choose the best and then in that many operation we have data matrix and also it's very easy.",
                    "label": 0
                },
                {
                    "sent": "I mean you can see that it's extremely easy to parallelize basically because everything is just done, kind of independently and even.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data the matrix can be.",
                    "label": 0
                },
                {
                    "sent": "It can be stored separately and updated separately.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, so now about a little bit about the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Testing environment.",
                    "label": 0
                },
                {
                    "sent": "So OK, let me just cut to the chase.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is it that we've observed?",
                    "label": 0
                },
                {
                    "sent": "What we observe this?",
                    "label": 0
                },
                {
                    "sent": "If we run this algorithm OK, and we intermittently observe the solutions and then we plot how many nonzeros we guessed correctly and how many we did not guess correctly.",
                    "label": 0
                },
                {
                    "sent": "So basically we will get.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same orosi curves is pretty much as we would get by changing Lambda itself.",
                    "label": 0
                },
                {
                    "sent": "Now here we're not changing Lambda, we just choose some Lambda, small enough, some.",
                    "label": 0
                },
                {
                    "sent": "I mean, there is a good idea to have some regularization, and then we're slowly applying this algorithm, and in blue we have the steps of the algorithm here and in red we have the RC curve obtained by changing Lambda and you see they are basically the same.",
                    "label": 0
                },
                {
                    "sent": "OK with you.",
                    "label": 0
                },
                {
                    "sent": "I mean, this just shows that with geloso you cannot do that.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have this, you know nature of growth.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things slowly OK. OK, let me not explain that because it's going to take awhile.",
                    "label": 0
                },
                {
                    "sent": "So this this also shows basically that even in a bad case these are good orosi curves.",
                    "label": 0
                },
                {
                    "sent": "And this is a bad one.",
                    "label": 0
                },
                {
                    "sent": "And still they are very close to each other very quickly.",
                    "label": 0
                },
                {
                    "sent": "What this slide shows is basically the bottom line of this slide is that.",
                    "label": 0
                },
                {
                    "sent": "I mean, are they really the same paths these two are they producing the same non zeros?",
                    "label": 0
                },
                {
                    "sent": "This is just the number of true positive versus false positives, right?",
                    "label": 0
                },
                {
                    "sent": "This is not this.",
                    "label": 0
                },
                {
                    "sent": "We don't know if the same.",
                    "label": 0
                },
                {
                    "sent": "Elements that are produced and it turns out that they basically produce these two paths, produce more or less the same now.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zero elements and until the basically garbage starts appearing OK.",
                    "label": 0
                },
                {
                    "sent": "So if it's a good good.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Blim and really can generate the good.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amount of non zeros, they're correct, non zeros 1st and then you start generating the non necessary non zeros because if you learn this small you eventually will generate all the non zeros that you don't want.",
                    "label": 0
                },
                {
                    "sent": "But you will.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generate so you will basically start producing garbage when.",
                    "label": 0
                },
                {
                    "sent": "When it's you know when, when it basically has to appear in some sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This slide shows basically that if you increase the amount of data, you will actually end up with sparsest solution with this problem.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to even try to go over these things.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause I don't have the time.",
                    "label": 0
                },
                {
                    "sent": "So let me just do the bottom line.",
                    "label": 0
                },
                {
                    "sent": "So basically this algorithm can be used.",
                    "label": 1
                },
                {
                    "sent": "In some sense an an.",
                    "label": 0
                },
                {
                    "sent": "Again we can improve it probably substantially, so it's more of an idea rather than the specific implementation that I'm trying to advertise here, but.",
                    "label": 0
                },
                {
                    "sent": "You know you can basically run it until you get the desired level of sparsity, which was not possible before with the Lambda, because with Lambda you have to play.",
                    "label": 1
                },
                {
                    "sent": "You don't know what level of sparsity is going to give you.",
                    "label": 0
                },
                {
                    "sent": "With this algorithm, you basically produce the solution slowly until you're satisfied with the amount of sparsity you got.",
                    "label": 0
                },
                {
                    "sent": "Or you can cross validate what solutions.",
                    "label": 0
                },
                {
                    "sent": "In the process and you know.",
                    "label": 0
                },
                {
                    "sent": "So basically it produces a path of solutions by itself.",
                    "label": 0
                },
                {
                    "sent": "This algorithm, not varying Lambda.",
                    "label": 1
                },
                {
                    "sent": "And yeah, and then basically it's very very simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's really dumb.",
                    "label": 0
                },
                {
                    "sent": "Every step is computed by solving quadratic equation.",
                    "label": 0
                },
                {
                    "sent": "It can paralyze it.",
                    "label": 0
                },
                {
                    "sent": "You can probably improve it dramatically and you can maybe even include different.",
                    "label": 0
                },
                {
                    "sent": "Instead of L1 norm, you can maybe do some other separable functions, basically which we may explore at some point.",
                    "label": 0
                },
                {
                    "sent": "I'll stop here.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "That's the whole point.",
                    "label": 0
                },
                {
                    "sent": "It's not P, it's it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a finite number of operations.",
                    "label": 0
                },
                {
                    "sent": "I mean, not finite, but it's of course find it.",
                    "label": 0
                },
                {
                    "sent": "But it independent of.",
                    "label": 0
                },
                {
                    "sent": "It's just solving a quadratic equation in one variable.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Do what?",
                    "label": 0
                },
                {
                    "sent": "No no no no no I don't I I when I when I do need it to compute I don't need it because I mean I'm maintaining the inverse and that inverse gives me all the information I need to update the universe.",
                    "label": 0
                },
                {
                    "sent": "I need to do a rank to update to it which is of P squared.",
                    "label": 0
                },
                {
                    "sent": "But it's again all P squared not of the cube.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I it's old P squared but I.",
                    "label": 0
                },
                {
                    "sent": "No, no, but right for each point I'm doing with the square.",
                    "label": 0
                },
                {
                    "sent": "That's correct, yes.",
                    "label": 0
                },
                {
                    "sent": "So if I need to update all elements in my matrix eventually if I need if they're all not going to be non zeros.",
                    "label": 1
                },
                {
                    "sent": "Of course I will be very expensive.",
                    "label": 0
                },
                {
                    "sent": "This is not a good method for that for sure.",
                    "label": 0
                },
                {
                    "sent": "This is only good when you have a fairly sparse.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution that you're looking for, and so most of the elements is hopefully never going to even look at.",
                    "label": 0
                },
                {
                    "sent": "OK, no no.",
                    "label": 0
                },
                {
                    "sent": "No look at it.",
                    "label": 0
                },
                {
                    "sent": "Might look at them.",
                    "label": 0
                },
                {
                    "sent": "It might not change them.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        }
    }
}