{
    "id": "tdtuc6rp3ckgfyu6tjvdn7itojedrt35",
    "title": "A Modular Approach to Training Cascades of Boosted Ensembles",
    "info": {
        "author": [
            "Teo Susnjak, Institute of Information and Mathematical Sciences, Massey University"
        ],
        "published": "Sept. 13, 2010",
        "recorded": "August 2010",
        "category": [
            "Top->Computer Science->Pattern Recognition"
        ]
    },
    "url": "http://videolectures.net/ssspr2010_susnjak_mat/",
    "segmentation": [
        [
            "So good morning to you all.",
            "My name is tell Sasha come from Mass University in New Zealand.",
            "My presentation to you today will cover the work that my colleagues and I have been conducting into multiple classifier systems.",
            "And more specifically, research into finding new methods that add tracked ability to the training process of cascades of boosted ensembles."
        ],
        [
            "So I'll give you a very brief introduction into ensemble based learning as well as on some based learning with cascades.",
            "I'll talk about some of the problems that we encounter in that, and also I'll address or introduce you the PSL structure which is meant to address some of those shortcomings.",
            "The primary point of this presentation will be the positive sample bootstrapping method that we managed to.",
            "Incorporate into the PSL structure, so I'll talk about that.",
            "The experimental results will cover the work that we've done with face detection and we'll talk about the results we got from that, as well as the limitations.",
            "And then if I have some time left, I'll go into the area that we've sort of branched out into the last few months, and that is adaptive learning with this structure as well."
        ],
        [
            "So how does ensemble based learning work while to put it simply, ensemble based systems create a large number of classifiers in order to formulate a solution to a problem instead of justice one.",
            "We tend to call such and samples of classifiers as experts and they comprise a committee of experts.",
            "The underlying principle behind that is that a collective decision made by a committee of experts is going to be more robust.",
            "More accurate than a decision made by any single one classifier.",
            "Some of the key components of an SOM based learning and Sobel diversity, and with that we try and introduce divergences of opinions between experts.",
            "So we don't have all the experts misclassifying the same samples.",
            "We do that in this research using Adaboost machine learning algorithm, the second key component strategies that strategies that combined decisions of the classifiers and in our research we used weighted majority voting."
        ],
        [
            "So the problem is in most difficult an real world problems when we train on them, we tend to generate a whole lot of ensembles, very large number and the issue with that is that if a classifier is required to run in real time, that is obviously going to be an issue.",
            "So some years ago Viola and Jones came up with a clever solution and they came up with the structure cascading structure that basically decomposed ensembles.",
            "Into different layers.",
            "Each layer was designed to.",
            "Classify and reject as many samples as possible.",
            "So when an incoming sample would come, most of the samples will be rejected by earlier layers and therefore most of the classifiers did not need to be classified, so computational speed was realized.",
            "In that sense, one of the main components of a cascade structure in terms of training was the fact that it enabled the bootstrapping of negative samples, and because of this.",
            "We are able to attain very low false positive rates."
        ],
        [
            "Now, some of the disadvantages with training using this methodology, a training speed.",
            "Of course this depends on what sort of feature types you're using and data set sizes.",
            "But one of the issues slow convergence rates.",
            "Tulare targets later targets are false positive, false acceptance rates, and the hit rates.",
            "The other limitation is the size of the positive data set.",
            "With these sort of training methods, all the positive samples are required to be trained simultaneously, so there are no facilities available to bootstrap them, and this is what we're trying to address here.",
            "And the other issue are cascade optimization problems as well."
        ],
        [
            "So a couple of years ago, team from Massey University led by Andre Bachok, came up with the structure that they turned the PSL that stands for parallel strong classifier within the same layer, which is a little bit wordy, but anyway works and it was designed to address some of those issues and it did so by creating nested cascade within each one of the layers.",
            "So instead of just having individual ensembles.",
            "It broke them up into modular.",
            "Little system subgroups.",
            "So basically this ended up being jewel cascaded system in effect.",
            "It is successfully.",
            "Accelerated convergence to lead targets and also the training run times were quite substantially accelerated."
        ],
        [
            "Now, it's probably going to be easiest for me to explain how this structure works when we look at the the way that the positive samples are trained within the structure and the negative sets as well.",
            "So in the left hand side we see the way that the positive samples propagate through the training structure.",
            "We start off with an entire 100% of the positive data set were big in training and odora stage here and we train it until we.",
            "Reach a predetermined criterion, which in our case is the maximum number of ensembles.",
            "Once that is reached, all the misclassified positive samples are passed for training to the next stage, while the correctly positive are correctly classified, positives are removed, depresses continues until all the positive samples are correctly classified.",
            "From the perspective of the negative samples, it's simpler, or the negative samples are simply pasta every single stage.",
            "However, when it comes to detection time in order, for example to be rejected or in order for negative sample to be rejected, there has to be unanimous vote from all the nodes when it comes to the positive samples, a single positive vote from any one of the stages will classify a sample as a positive."
        ],
        [
            "Sorry, I'm in the last year we started looking into ways of extending this framework in order to enable positive sample bootstrapping and this is what we came up with.",
            "The basic strategy that we're users divide and conquer.",
            "We start off training a layer by first of all.",
            "Taking out randomly selecting a small subset of samples from the entire pool of positive samples that we have available to us, we begin training a single intra layer stage.",
            "Once that completes, we apply this classifier here onto the entire pool of positive samples.",
            "Here we then remove all the correctly classified positive samples from training, and then we can begin training the next stage.",
            "We take samples from the remaining pool of positive samples that are still correct, incorrectly classified, and we augment the next training set with those samples.",
            "So this prices continues until we've successfully illuminated all the positive samples.",
            "The beauty of this approach is that we don't necessarily have to train explicitly on every single positive sample.",
            "We only end up training on a small subset, but we're still able to use very large number of positive samples."
        ],
        [
            "OK, so these are the results that we got in terms of training runtimes and what we did was we created three separate training data sets.",
            "One was 5000 positive samples 10,000 and 15,000.",
            "They were all trained against 2000 negative samples.",
            "We have three sets of classifiers.",
            "These ones here represent the Viola and Jones training runtimes.",
            "The Middle 2 bars are the naive PSL classifiers without the bootstrapping facility and the first 2 represented the one with the.",
            "With the bootstrapping capability where we see clearly that as the positive data set sizes increase.",
            "The training runtimes for the BDC classifiers, those are the ones with the bootstrapping capability increased marginally, whereas the other training structures increase quite dramatically."
        ],
        [
            "We can probably see that a little bit clearer in this graph.",
            "Which shows down on the X axis the number of days it took the train classifiers.",
            "Here we see the number of classifiers being generated and Viola and Jones graph.",
            "Is this one here.",
            "So it took several months to actually train a classifier.",
            "On 15,000 positive samples, which is what this graph represents, the PSL structure the naive one without bootstrapping took a couple months, whereas our training structure with the bootstrapping capability took a couple of weeks, so there was a dramatic reduction in training runtimes."
        ],
        [
            "Later on, we realized that there was a degree of overfitting happening in our training.",
            "And we realized that this was partly due to the nature of the way that our framework trains positive samples.",
            "We found that the very difficult.",
            "Training samples tend to be delayed in training until the very last.",
            "Until last stages, so there tends to be a very large country concentration of them in the large in the last stages."
        ],
        [
            "And we see that in this graph.",
            "That is training as the intra last stages were created, the positive data sets not only decreased but the concentration of those difficult samples with with occlusions and strong illumination changes also increased what we did to counter that was was quite simple.",
            "We ended up augmenting the positive data sets in the trailing stages with redundant positive samples that we had already learned to correctly classified in earlier stages.",
            "And in addition to that, we also join boosting heads, increased the weights of the positives that had yet up until that point being misclassified, so there was a slight little modification that we had to make."
        ],
        [
            "So this was the result that we ended up getting in terms of accuracy.",
            "This blue graph here in the middle, represents the classifier.",
            "The accuracy of classifiers trained with the modification.",
            "And of the algorithm with the bootstrapping facility, the other ones are PSL and the naive ones, and we see that there has been a reasonable improvement on accuracy.",
            "However, the Viola and Jones classifiers still performed better, so there's plenty of room for improvement in our structure still, but we think that with the reduction of training run times that we have achieved that there's quite a fear, or quite a reasonable tradeoff."
        ],
        [
            "Now the training structures that I've talked about work very well and environments in which assumed to be static and non stationary.",
            "However, such classifiers tend to become insufficient when there is concept drift and concept drift tends to bring in unpredictable changes in the underlying distribution of the data.",
            "It can happen abruptly or gradually.",
            "As well as cyclically in an unpredictable manner.",
            "So the requirements for countering that and creating a structure that is able to cope with that is that it needs to be timely in its adaptation.",
            "It needs to have no access to provide training data sets.",
            "An most importantly needs to have a balance between plasticity, its ability to incorporate new information into the classifier as well as stability, its ability to not forget what it has already learned.",
            "And."
        ],
        [
            "This is kind of what we came up with, so this is work that we've been doing for the last couple of months, it's.",
            "I still pretty new and it still has a lot of room for improvement, but basically what we do is we take one of the PSL classifiers that we've trained statically and what we do is we apply each one of the intra layer stages that I talked about before we apply them to the training data set that they were originally trained on.",
            "So the static one and we get a performance value out of there.",
            "So we get a competence value that we call enough for value.",
            "We do that for every single one of the stages.",
            "Then we deploy the classifier onto the domain.",
            "Once concept drift is detected.",
            "This is when learning and adaptation actually begins and the way we do it is we don't explicitly change or alter the values of the ensembles or of the competence values of each one of the interlayer stages, but instead we will learn a confidence value for the layer, so we learn a threshold based.",
            "On the Alpha values here.",
            "So when a sample is being classified, if it falls below this threshold, it is rejected, otherwise it is accepted and we have fairly good results with this, at least in face detection, which is a relevant detection environment, but in its early stages seems to work pretty well."
        ],
        [
            "So to conclude.",
            "Cascades of ensembles can actually be successfully modulated and were shown how tractability can be introduced into the training process of that.",
            "We've shown how by extra modularisation of the process we can actually enable.",
            "Positive sample bootstrapping an we believe that there's a lot of scope there for extending the framework to adaptive learning as well as incremental learning paradigms as well.",
            "So."
        ],
        [
            "That's me, I think I'm gonna go to bed too quickly."
        ],
        [
            "Questions performance.",
            "OK, have some time for questions.",
            "When you talked about over fitting, it seemed that it was overfitting with with regards to positive.",
            "So yes, that right exactly when I think of boosting the criticisms, it's it's usually because of label noise or higher rate.",
            "How does your?",
            "System handle label noise is it very robust?",
            "How does it handle loss?",
            "Well, we found that because of the high concentration of samples that tend to be non representative of the whole, that happens to take place in the final stages.",
            "There tends to be a little bit of overtraining on those samples, so there's a little bit of.",
            "Lack of capability to handle noise I guess, and that's something that we've been trying to counter by ornamenting those data sets with samples that are perhaps a little bit more representative of the whole set.",
            "So would you try a different boosting variance to handle that?",
            "Or no?",
            "I mean, we've been quite happy to actually stick with the ADA boost as a as a learning algorithm.",
            "What we've been trying to focus on is finding a training structure rather that is able to overcome some of those overfitting issues instead of actually modifying the boosting algorithm itself.",
            "More questions.",
            "I have a question myself.",
            "Your approach is one of the distinctness of your approach is that you concentrate on positive samples.",
            "Yes.",
            "Instead on negative ones.",
            "Have you considered doing some sort of mixture between these two?",
            "Roughly speaking approaches?",
            "I mean concentrating, maybe Alternatively, on the other hand, I would suspect that this could depend on the kind of problem, because not in, perhaps on how much positive and negative priors you have in your program, which is not very convenient in the experiment you have considered.",
            "So if I've understood your question correctly, you asking have we consider approaches that combine the Viola Jones focus on the negative samples as well as our unnecessarily.",
            "They've gotten jobs, but taking into account positive on one hand and negative on the other, sure.",
            "This is kind of the best approach that we could find in order to accelerate the convergence to layer targets of each one of the layers.",
            "The problem is currently that.",
            "When we have all the positive samples in the layer and all the negative samples and allow the convergence to those required false acceptance rates which are normally .5% and to hit rates that tend to be under 100% is very very slow and so the only way that we could find so far the way of eliminating fiddling with the hit rate is by having this nested cascade so far.",
            "With the structure, we've actually successfully illuminated the hit rate is being a target for us.",
            "It's always going to be 100% irrelevant of how difficult the positive samples are, but we still managed to reject 50% of negatives at each layer, so we actually finding a balance between focusing on the positives as well as rejecting 50% of the negative spill layer.",
            "If that answers your questions.",
            "No questions.",
            "So."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good morning to you all.",
                    "label": 0
                },
                {
                    "sent": "My name is tell Sasha come from Mass University in New Zealand.",
                    "label": 1
                },
                {
                    "sent": "My presentation to you today will cover the work that my colleagues and I have been conducting into multiple classifier systems.",
                    "label": 0
                },
                {
                    "sent": "And more specifically, research into finding new methods that add tracked ability to the training process of cascades of boosted ensembles.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll give you a very brief introduction into ensemble based learning as well as on some based learning with cascades.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about some of the problems that we encounter in that, and also I'll address or introduce you the PSL structure which is meant to address some of those shortcomings.",
                    "label": 0
                },
                {
                    "sent": "The primary point of this presentation will be the positive sample bootstrapping method that we managed to.",
                    "label": 1
                },
                {
                    "sent": "Incorporate into the PSL structure, so I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "The experimental results will cover the work that we've done with face detection and we'll talk about the results we got from that, as well as the limitations.",
                    "label": 1
                },
                {
                    "sent": "And then if I have some time left, I'll go into the area that we've sort of branched out into the last few months, and that is adaptive learning with this structure as well.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does ensemble based learning work while to put it simply, ensemble based systems create a large number of classifiers in order to formulate a solution to a problem instead of justice one.",
                    "label": 0
                },
                {
                    "sent": "We tend to call such and samples of classifiers as experts and they comprise a committee of experts.",
                    "label": 0
                },
                {
                    "sent": "The underlying principle behind that is that a collective decision made by a committee of experts is going to be more robust.",
                    "label": 1
                },
                {
                    "sent": "More accurate than a decision made by any single one classifier.",
                    "label": 0
                },
                {
                    "sent": "Some of the key components of an SOM based learning and Sobel diversity, and with that we try and introduce divergences of opinions between experts.",
                    "label": 1
                },
                {
                    "sent": "So we don't have all the experts misclassifying the same samples.",
                    "label": 0
                },
                {
                    "sent": "We do that in this research using Adaboost machine learning algorithm, the second key component strategies that strategies that combined decisions of the classifiers and in our research we used weighted majority voting.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem is in most difficult an real world problems when we train on them, we tend to generate a whole lot of ensembles, very large number and the issue with that is that if a classifier is required to run in real time, that is obviously going to be an issue.",
                    "label": 0
                },
                {
                    "sent": "So some years ago Viola and Jones came up with a clever solution and they came up with the structure cascading structure that basically decomposed ensembles.",
                    "label": 0
                },
                {
                    "sent": "Into different layers.",
                    "label": 0
                },
                {
                    "sent": "Each layer was designed to.",
                    "label": 1
                },
                {
                    "sent": "Classify and reject as many samples as possible.",
                    "label": 0
                },
                {
                    "sent": "So when an incoming sample would come, most of the samples will be rejected by earlier layers and therefore most of the classifiers did not need to be classified, so computational speed was realized.",
                    "label": 0
                },
                {
                    "sent": "In that sense, one of the main components of a cascade structure in terms of training was the fact that it enabled the bootstrapping of negative samples, and because of this.",
                    "label": 0
                },
                {
                    "sent": "We are able to attain very low false positive rates.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, some of the disadvantages with training using this methodology, a training speed.",
                    "label": 1
                },
                {
                    "sent": "Of course this depends on what sort of feature types you're using and data set sizes.",
                    "label": 0
                },
                {
                    "sent": "But one of the issues slow convergence rates.",
                    "label": 1
                },
                {
                    "sent": "Tulare targets later targets are false positive, false acceptance rates, and the hit rates.",
                    "label": 0
                },
                {
                    "sent": "The other limitation is the size of the positive data set.",
                    "label": 0
                },
                {
                    "sent": "With these sort of training methods, all the positive samples are required to be trained simultaneously, so there are no facilities available to bootstrap them, and this is what we're trying to address here.",
                    "label": 1
                },
                {
                    "sent": "And the other issue are cascade optimization problems as well.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So a couple of years ago, team from Massey University led by Andre Bachok, came up with the structure that they turned the PSL that stands for parallel strong classifier within the same layer, which is a little bit wordy, but anyway works and it was designed to address some of those issues and it did so by creating nested cascade within each one of the layers.",
                    "label": 1
                },
                {
                    "sent": "So instead of just having individual ensembles.",
                    "label": 0
                },
                {
                    "sent": "It broke them up into modular.",
                    "label": 0
                },
                {
                    "sent": "Little system subgroups.",
                    "label": 0
                },
                {
                    "sent": "So basically this ended up being jewel cascaded system in effect.",
                    "label": 0
                },
                {
                    "sent": "It is successfully.",
                    "label": 1
                },
                {
                    "sent": "Accelerated convergence to lead targets and also the training run times were quite substantially accelerated.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, it's probably going to be easiest for me to explain how this structure works when we look at the the way that the positive samples are trained within the structure and the negative sets as well.",
                    "label": 0
                },
                {
                    "sent": "So in the left hand side we see the way that the positive samples propagate through the training structure.",
                    "label": 0
                },
                {
                    "sent": "We start off with an entire 100% of the positive data set were big in training and odora stage here and we train it until we.",
                    "label": 0
                },
                {
                    "sent": "Reach a predetermined criterion, which in our case is the maximum number of ensembles.",
                    "label": 0
                },
                {
                    "sent": "Once that is reached, all the misclassified positive samples are passed for training to the next stage, while the correctly positive are correctly classified, positives are removed, depresses continues until all the positive samples are correctly classified.",
                    "label": 0
                },
                {
                    "sent": "From the perspective of the negative samples, it's simpler, or the negative samples are simply pasta every single stage.",
                    "label": 0
                },
                {
                    "sent": "However, when it comes to detection time in order, for example to be rejected or in order for negative sample to be rejected, there has to be unanimous vote from all the nodes when it comes to the positive samples, a single positive vote from any one of the stages will classify a sample as a positive.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry, I'm in the last year we started looking into ways of extending this framework in order to enable positive sample bootstrapping and this is what we came up with.",
                    "label": 0
                },
                {
                    "sent": "The basic strategy that we're users divide and conquer.",
                    "label": 0
                },
                {
                    "sent": "We start off training a layer by first of all.",
                    "label": 0
                },
                {
                    "sent": "Taking out randomly selecting a small subset of samples from the entire pool of positive samples that we have available to us, we begin training a single intra layer stage.",
                    "label": 0
                },
                {
                    "sent": "Once that completes, we apply this classifier here onto the entire pool of positive samples.",
                    "label": 0
                },
                {
                    "sent": "Here we then remove all the correctly classified positive samples from training, and then we can begin training the next stage.",
                    "label": 0
                },
                {
                    "sent": "We take samples from the remaining pool of positive samples that are still correct, incorrectly classified, and we augment the next training set with those samples.",
                    "label": 0
                },
                {
                    "sent": "So this prices continues until we've successfully illuminated all the positive samples.",
                    "label": 0
                },
                {
                    "sent": "The beauty of this approach is that we don't necessarily have to train explicitly on every single positive sample.",
                    "label": 0
                },
                {
                    "sent": "We only end up training on a small subset, but we're still able to use very large number of positive samples.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these are the results that we got in terms of training runtimes and what we did was we created three separate training data sets.",
                    "label": 0
                },
                {
                    "sent": "One was 5000 positive samples 10,000 and 15,000.",
                    "label": 0
                },
                {
                    "sent": "They were all trained against 2000 negative samples.",
                    "label": 0
                },
                {
                    "sent": "We have three sets of classifiers.",
                    "label": 0
                },
                {
                    "sent": "These ones here represent the Viola and Jones training runtimes.",
                    "label": 0
                },
                {
                    "sent": "The Middle 2 bars are the naive PSL classifiers without the bootstrapping facility and the first 2 represented the one with the.",
                    "label": 0
                },
                {
                    "sent": "With the bootstrapping capability where we see clearly that as the positive data set sizes increase.",
                    "label": 0
                },
                {
                    "sent": "The training runtimes for the BDC classifiers, those are the ones with the bootstrapping capability increased marginally, whereas the other training structures increase quite dramatically.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can probably see that a little bit clearer in this graph.",
                    "label": 0
                },
                {
                    "sent": "Which shows down on the X axis the number of days it took the train classifiers.",
                    "label": 0
                },
                {
                    "sent": "Here we see the number of classifiers being generated and Viola and Jones graph.",
                    "label": 0
                },
                {
                    "sent": "Is this one here.",
                    "label": 0
                },
                {
                    "sent": "So it took several months to actually train a classifier.",
                    "label": 0
                },
                {
                    "sent": "On 15,000 positive samples, which is what this graph represents, the PSL structure the naive one without bootstrapping took a couple months, whereas our training structure with the bootstrapping capability took a couple of weeks, so there was a dramatic reduction in training runtimes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Later on, we realized that there was a degree of overfitting happening in our training.",
                    "label": 0
                },
                {
                    "sent": "And we realized that this was partly due to the nature of the way that our framework trains positive samples.",
                    "label": 0
                },
                {
                    "sent": "We found that the very difficult.",
                    "label": 0
                },
                {
                    "sent": "Training samples tend to be delayed in training until the very last.",
                    "label": 0
                },
                {
                    "sent": "Until last stages, so there tends to be a very large country concentration of them in the large in the last stages.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we see that in this graph.",
                    "label": 0
                },
                {
                    "sent": "That is training as the intra last stages were created, the positive data sets not only decreased but the concentration of those difficult samples with with occlusions and strong illumination changes also increased what we did to counter that was was quite simple.",
                    "label": 0
                },
                {
                    "sent": "We ended up augmenting the positive data sets in the trailing stages with redundant positive samples that we had already learned to correctly classified in earlier stages.",
                    "label": 0
                },
                {
                    "sent": "And in addition to that, we also join boosting heads, increased the weights of the positives that had yet up until that point being misclassified, so there was a slight little modification that we had to make.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was the result that we ended up getting in terms of accuracy.",
                    "label": 0
                },
                {
                    "sent": "This blue graph here in the middle, represents the classifier.",
                    "label": 0
                },
                {
                    "sent": "The accuracy of classifiers trained with the modification.",
                    "label": 0
                },
                {
                    "sent": "And of the algorithm with the bootstrapping facility, the other ones are PSL and the naive ones, and we see that there has been a reasonable improvement on accuracy.",
                    "label": 0
                },
                {
                    "sent": "However, the Viola and Jones classifiers still performed better, so there's plenty of room for improvement in our structure still, but we think that with the reduction of training run times that we have achieved that there's quite a fear, or quite a reasonable tradeoff.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the training structures that I've talked about work very well and environments in which assumed to be static and non stationary.",
                    "label": 0
                },
                {
                    "sent": "However, such classifiers tend to become insufficient when there is concept drift and concept drift tends to bring in unpredictable changes in the underlying distribution of the data.",
                    "label": 1
                },
                {
                    "sent": "It can happen abruptly or gradually.",
                    "label": 0
                },
                {
                    "sent": "As well as cyclically in an unpredictable manner.",
                    "label": 0
                },
                {
                    "sent": "So the requirements for countering that and creating a structure that is able to cope with that is that it needs to be timely in its adaptation.",
                    "label": 1
                },
                {
                    "sent": "It needs to have no access to provide training data sets.",
                    "label": 0
                },
                {
                    "sent": "An most importantly needs to have a balance between plasticity, its ability to incorporate new information into the classifier as well as stability, its ability to not forget what it has already learned.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is kind of what we came up with, so this is work that we've been doing for the last couple of months, it's.",
                    "label": 0
                },
                {
                    "sent": "I still pretty new and it still has a lot of room for improvement, but basically what we do is we take one of the PSL classifiers that we've trained statically and what we do is we apply each one of the intra layer stages that I talked about before we apply them to the training data set that they were originally trained on.",
                    "label": 0
                },
                {
                    "sent": "So the static one and we get a performance value out of there.",
                    "label": 0
                },
                {
                    "sent": "So we get a competence value that we call enough for value.",
                    "label": 0
                },
                {
                    "sent": "We do that for every single one of the stages.",
                    "label": 0
                },
                {
                    "sent": "Then we deploy the classifier onto the domain.",
                    "label": 0
                },
                {
                    "sent": "Once concept drift is detected.",
                    "label": 0
                },
                {
                    "sent": "This is when learning and adaptation actually begins and the way we do it is we don't explicitly change or alter the values of the ensembles or of the competence values of each one of the interlayer stages, but instead we will learn a confidence value for the layer, so we learn a threshold based.",
                    "label": 0
                },
                {
                    "sent": "On the Alpha values here.",
                    "label": 0
                },
                {
                    "sent": "So when a sample is being classified, if it falls below this threshold, it is rejected, otherwise it is accepted and we have fairly good results with this, at least in face detection, which is a relevant detection environment, but in its early stages seems to work pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "Cascades of ensembles can actually be successfully modulated and were shown how tractability can be introduced into the training process of that.",
                    "label": 1
                },
                {
                    "sent": "We've shown how by extra modularisation of the process we can actually enable.",
                    "label": 0
                },
                {
                    "sent": "Positive sample bootstrapping an we believe that there's a lot of scope there for extending the framework to adaptive learning as well as incremental learning paradigms as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's me, I think I'm gonna go to bed too quickly.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions performance.",
                    "label": 0
                },
                {
                    "sent": "OK, have some time for questions.",
                    "label": 0
                },
                {
                    "sent": "When you talked about over fitting, it seemed that it was overfitting with with regards to positive.",
                    "label": 0
                },
                {
                    "sent": "So yes, that right exactly when I think of boosting the criticisms, it's it's usually because of label noise or higher rate.",
                    "label": 0
                },
                {
                    "sent": "How does your?",
                    "label": 0
                },
                {
                    "sent": "System handle label noise is it very robust?",
                    "label": 0
                },
                {
                    "sent": "How does it handle loss?",
                    "label": 0
                },
                {
                    "sent": "Well, we found that because of the high concentration of samples that tend to be non representative of the whole, that happens to take place in the final stages.",
                    "label": 0
                },
                {
                    "sent": "There tends to be a little bit of overtraining on those samples, so there's a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Lack of capability to handle noise I guess, and that's something that we've been trying to counter by ornamenting those data sets with samples that are perhaps a little bit more representative of the whole set.",
                    "label": 0
                },
                {
                    "sent": "So would you try a different boosting variance to handle that?",
                    "label": 0
                },
                {
                    "sent": "Or no?",
                    "label": 0
                },
                {
                    "sent": "I mean, we've been quite happy to actually stick with the ADA boost as a as a learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "What we've been trying to focus on is finding a training structure rather that is able to overcome some of those overfitting issues instead of actually modifying the boosting algorithm itself.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "I have a question myself.",
                    "label": 0
                },
                {
                    "sent": "Your approach is one of the distinctness of your approach is that you concentrate on positive samples.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Instead on negative ones.",
                    "label": 0
                },
                {
                    "sent": "Have you considered doing some sort of mixture between these two?",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking approaches?",
                    "label": 0
                },
                {
                    "sent": "I mean concentrating, maybe Alternatively, on the other hand, I would suspect that this could depend on the kind of problem, because not in, perhaps on how much positive and negative priors you have in your program, which is not very convenient in the experiment you have considered.",
                    "label": 0
                },
                {
                    "sent": "So if I've understood your question correctly, you asking have we consider approaches that combine the Viola Jones focus on the negative samples as well as our unnecessarily.",
                    "label": 0
                },
                {
                    "sent": "They've gotten jobs, but taking into account positive on one hand and negative on the other, sure.",
                    "label": 0
                },
                {
                    "sent": "This is kind of the best approach that we could find in order to accelerate the convergence to layer targets of each one of the layers.",
                    "label": 0
                },
                {
                    "sent": "The problem is currently that.",
                    "label": 0
                },
                {
                    "sent": "When we have all the positive samples in the layer and all the negative samples and allow the convergence to those required false acceptance rates which are normally .5% and to hit rates that tend to be under 100% is very very slow and so the only way that we could find so far the way of eliminating fiddling with the hit rate is by having this nested cascade so far.",
                    "label": 0
                },
                {
                    "sent": "With the structure, we've actually successfully illuminated the hit rate is being a target for us.",
                    "label": 0
                },
                {
                    "sent": "It's always going to be 100% irrelevant of how difficult the positive samples are, but we still managed to reject 50% of negatives at each layer, so we actually finding a balance between focusing on the positives as well as rejecting 50% of the negative spill layer.",
                    "label": 0
                },
                {
                    "sent": "If that answers your questions.",
                    "label": 0
                },
                {
                    "sent": "No questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        }
    }
}