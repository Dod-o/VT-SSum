{
    "id": "3ek7illnje4jctuats63ddtlpwwscpxb",
    "title": "Compact indexing of versioned data",
    "info": {
        "author": [
            "Ronny Lempel, Yahoo! Research"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2006",
        "category": [
            "Top->Computer Science->Information Retrieval",
            "Top->Computer Science->Algorithms and Data Structures"
        ]
    },
    "url": "http://videolectures.net/fws06_lempel_civd/",
    "segmentation": [
        [
            "This is joint work with Mickey Herskowitz Ansi Vanya from the IBM Haifa Research Lab and."
        ],
        [
            "And basically what I'm going to talk about is how to index version data and the motivation is that clearly a lot of information systems contain documents that undergo and evolve through versions, so content management systems or version control systems for code like CVS or CCO.",
            "Clearcase in wikis you have the whole change history of the wiki in many kinds of backup and archiving solutions you keep versions of the documents.",
            "And in some sense which also show on the next slide.",
            "Also email threads are a type of versioned data system.",
            "Now basically of course it's very easy to search over version data if you take every version of the document and index it separately independently as its own document, but that somehow seems not very efficient because of the overlap and redundancy within the versions.",
            "So what we're trying to do here?",
            "Is to somehow generate a much more compact index of these versions, but at the same time not to lose any retrieval capabilities.",
            "So we want to be able to search with all the features of the more naive index that indexes each version separately.",
            "I'm going."
        ],
        [
            "Need one little fact from string Golla G and that that if you have two strings S1 and S2, finding their longest common subsequence is easily solvable in polynomial time.",
            "So for example, if the two strings are ABCD, EF and abxy FY the longest common subsequence is ABF, and notice that this is not a common substring, it's just a common subsequence.",
            "Now."
        ],
        [
            "Let's talk a little bit about email, so let's look at a thread of emails between person A and person B, in which anytime one of them replies to the other one, they simply reply with history and contain the so that the answer contains the entire text of the previous note.",
            "So a rights to be in this light blue text 1B, then replies to a adding a purple box 2 on top of that same light, blue text 1.",
            "A replies to be and now both the light blue and the purple boxes are replicated and as this thread continues you can see that it's size becomes the square of the number of interactions, whereas there are only linear number of new textboxes.",
            "And of course this can go also to trees where person one send email to person to person three.",
            "They each reply.",
            "Thus the thread is now like a tree and again this continues.",
            "Now just consider this model where indeed you always reply with the entire history of whatever you received without ever changing or deleting or adding or embedding new text in between what you are quoting."
        ],
        [
            "So in a recently published paper by Ibmers and now former ibmers, it was shown that you can index pretty much every new piece of text once without repeating without repetitively indexing all the quoted text.",
            "And the idea is that if we look back on the tree here."
        ],
        [
            "Is that when you index the text of a document, the posting elements are sort of shared between the descendants of that node in the tree, so I'm not going to go into too deep, too much deeper detail in that I just want to say that."
        ],
        [
            "This indexing method works as long as the quoted messages aren't modified, but only quoted as is and this is what this work is trying to extend.",
            "What we're going to do, inversions, you know, things change.",
            "Wherever, so we're going to be able to index versions no matter how the edits were done.",
            "OK.",
            "So."
        ],
        [
            "Here is the running example I'll talk about and consider that you have four documents over here and just for simplicity, each word is denoted by a letter and again just for ease of presentation.",
            "The letters are distinct in each document.",
            "Of course, words can repeat.",
            "Let's also say that an Oracle somehow tells me a super sequence of all four documents.",
            "Now, of course I won't have that Oracle.",
            "I won't need that Oracle, but for the time being assumed that somehow I know of a super sequence of a super sequence of these four documents."
        ],
        [
            "What I'm going to use is this representation of a matrix whose first row or as we call it throw 0, is the Super sequence, and then it's a binary matrix where each row is an encoding of the corresponding sequence.",
            "So for example, to reconstruct the second sequence from row #2 what I need to do is to take the letters on top of columns that have ones in the matrix.",
            "So it's going to be a BXEFY.",
            "OK, this is just a representation of the four original strings or documents using a super sequence.",
            "The next step."
        ],
        [
            "Hope is to look at the runs of ones and the columns and down here I'm writing for each column.",
            "What are the runs of ones in it.",
            "So for example in the first column there is a single run of 1 starting at row four Ending Info 4.",
            "Here we have a run of ones starting at row one, ending that row 2.",
            "And in the third column we actually have two runs of once the first run starts at row one, ends at row two, and then a second run.",
            "Starts at row four and ends at Row 4.",
            "Of course there are 10 possible runs of ones that can happen in this matrix, and I'm going to order those runs of ones according to their end point, and then according second, the secondary sort is according to their start points.",
            "So first I list all the runs that end at index one, then the two runs that end in index 2.",
            "The three runs that ended index three, and the four runs that end at Index four, and within each group that ends at a certain index.",
            "The start points are also ordered in increasing order.",
            "Now what is this at all good for?"
        ],
        [
            "I'm going to forget about these four original documents and actually I'm going to create 4 excuse me 10 virtual documents.",
            "Each virtual document is going to correspond to a specific run of ones.",
            "And I'm going to distribute tokens or words to these 10 virtual documents according to the columns.",
            "Corresponding to that one of ones.",
            "So for example.",
            "I have a virtual document corresponding to the run that starts at row two and ends at 204.",
            "And you see, I've assigned words X&Y to that document.",
            "Why's that?",
            "Because in the column corresponding to XI had to run starting at 2, ending at four, and also in the column corresponding to Yi, had a run starting at row two and ending intro four.",
            "OK, so I'm basically distinct distributing tokens.",
            "From real documents to virtual documents according to the runs of ones and this in the columns of this matrix.",
            "And basically what I'm going."
        ],
        [
            "You have a question.",
            "Of course, of course, because there are repeated words in documents.",
            "Um?",
            "I'm actually going to index the virtual documents, not the true documents, and basically this is where I'm going to save space because the number of posting elements in the posting lists is reduced from the total number of ones in the matrix, which are the total number of words in the original documents to the number of runs of ones.",
            "In the in the in this alignment matrix.",
            "In this case, instead of indexing 25 words in the four original documents, I'm going to index only 12 words in a few more virtual documents.",
            "That's going to be where the index space is going to be saved.",
            "So this is."
        ],
        [
            "Just another view of the inverted index.",
            "I hear the virtual documents ordered from one to 10 according to my special way of numbering them, and there are 12 words within these documents.",
            "And here you see the posting lists just like a normal inverted index.",
            "But on these new documents.",
            "In Prague."
        ],
        [
            "This of course any interesting repository will have multiple groups of versioned documents, so I do this same process over and over again for each group.",
            "So if I had like four groups of version documents with a total of 11 documents, I would have built.",
            "22 virtual documents 6 corresponding to the first group of three versions and then three corresponding to the second group of two versions.",
            "10 corresponding to the group of four versions and another three.",
            "OK, so now we have a large index.",
            "Divided into groups of virtual documents.",
            "And it turns out."
        ],
        [
            "That we can do search operations on this index and simulate what we would have done on the full index.",
            "So basically I'm going to explain a little bit how to do that for Boolean queries, But this can be extended to vector space model, TF IDF queries very easily.",
            "The way I'm going to explain it is that if you got a bunch of Boolean queries, just replace each negated term or minus term by a virtual cursor that simulates, you know that stops at wherever the minus Trump does not stop.",
            "And we're going to use a document at a time.",
            "A normal document at a time algorithm where you go by your document space and you find the next candidate the next kind of document that satisfies the Boolean requirements.",
            "OK, nothing really new about here."
        ],
        [
            "Um?",
            "How are we going to find the Boolean document we're going to use a zigzag join and I'll skip the details here because."
        ],
        [
            "I'm going to try to explain it with this small example.",
            "So let's say you have a term.",
            "These are the physical documents, and these are the virtual documents.",
            "There are many more virtual documents, and let's say you have a term whose cursor stopped on.",
            "This specific virtual document that represents a span or a runs of 1A run of ones.",
            "From this document to this document.",
            "So in the original documents, this term appeared in all four documents.",
            "Now you have this second term, and let's say it's cursor is somewhere behind and it's posting lists over the virtual documents and you want to ask yourself where should I advance it to?",
            "So if this is the virtual document corresponding to a run of ones from document one to document from.",
            "You can safely disregard any virtual document before this point, you can simply skip over them using your posting list mechanisms, so you can skip over them, and this is where the interesting area of virtual documents that may overlap in terms of words begins.",
            "And in fact, all virtual documents between this point and this point.",
            "Correspond to ranges of physical documents that will surely overlap with this, so using a little bit of interval algebra, if the second term falls within this range of virtual documents, we can identify a range of physical documents that surely contain both terms.",
            "OK, that's going to be what we're going to do here.",
            "We're going to do some interval algebra over the virtual documents.",
            "And you know there are rules of what happens if the second cursor falls beyond this point and so forth.",
            "I don't want to go into the details.",
            "You can read them in the paper, and even in the notes in the handout, they have a lot more details on this.",
            "But basically we can do Boolean queries over the virtual documents."
        ],
        [
            "So if you believe that, then let's now do a little bit of analysis on the index size.",
            "What are we saving and where are we perhaps losing so in terms of the lexicon of the inverted index, nothing has changed.",
            "The set of unique terms remains the same, so the lexicon size remains the same.",
            "In terms of the number of posting elements in the posting lists, as I already talked about, we are instead of using the number of ones in the matrix, we're only going to be using the number of runs of ones in the matrix, so that's where the biggest reduction is.",
            "We are losing a little bit on the compression of posting lists since the virtual document space is larger so the gaps are larger.",
            "They don't compress as well.",
            "We're going to lose a little bit there and then.",
            "We also need a little bit of extra overhead.",
            "I won't go into that.",
            "You'll see that in the results."
        ],
        [
            "OK, so we said that the biggest savings come from not indexing ones in the matrix, but only runs of ones in the matrix.",
            "So we now have an optimization problem given a set of strings.",
            "Or documents find an alignment matrix that has the minimal number of runs of once.",
            "Sounds easy, but then there are two NP hard problems hiding in there.",
            "First is to find the shortest common supersequence.",
            "So as I said in the beginning, we have this Oracle that gave that sort of.",
            "We started with the Super sequence.",
            "Nobody knows how to find a short common super sequence, and we're not going to try to do that because it's NP hard and then even if someone gave you a super sequence and also gave you the alignment of each string to the Super sequence.",
            "So essentially, if someone gives you the Rose of the binary matrix and just ask you to permute them so that the number of runs of ones will be minimal, that's also NP hard.",
            "So we can't do that either.",
            "So what can we do?",
            "We're going to make a reason."
        ],
        [
            "Assumption, you know in real life to say that if the versions are generated in a serial manner, like for example in some CVS or CNBC version repository, then probably the normal time evolution of the sequences is the good or will be a good order in the matrix.",
            "So this changes the optimization problem too.",
            "When given an ordered set of strings, find an alignment matrix whose sums of sum of run of ones is minimal.",
            "Now nobody is giving us the Super sequence.",
            "And so nobody is giving us the rows of the matrix, just the strings.",
            "The original strings themselves.",
            "But it turns out that.",
            "The order that was given is enough so that a simple greedy algorithm that uses.",
            "I'll talk about why using the second will produce the optimal alignment matrix.",
            "That has this order of rows and basically what you want to do is you want to write the first string.",
            "And then for each subsequent string, just do your best using the longest common subsequence is of two strings.",
            "To glue the next string to the previous one.",
            "OK, you only care about.",
            "It's sort of a Markovian model where you only care about gluing the next one as best you can to the previous one.",
            "And also the full paper contains a theorem that justifies this intuition in mathematical terms, meaning that if your strings satisfies certain mathematical conditions.",
            "The natural order is indeed the best order.",
            "Just an example of."
        ],
        [
            "How to do the alignment you start up from the first string.",
            "You then take the second string and just by shortest common subsequence, you glue it to the first one.",
            "And you continue with the third.",
            "Continue with the 4th."
        ],
        [
            "And this, for example, is what the algorithm would do to our original 4 strings.",
            "Now.",
            "Note that this is a much wider matrix than what I started with.",
            "Because we're not finding the shortest common supersequence right?",
            "That's NP hard, but we don't care, because this still has the same number of runs of ones in its columns, 12 as that much leaner and more compact matrix.",
            "I showed you at the beginning.",
            "OK, so basically this is the algorithm.",
            "A few."
        ],
        [
            "Experimental results, so we took a couple of different versioned repository's.",
            "We started out with the 222 Wikipedia entries corresponding to countries, and then the second data set was a bunch of Mediawiki PHP source files, and for each we took up to 20 versions of each document set.",
            "So here we took up to 20 versions of each of the 222 documents, and then we did this indexing using Lucene.",
            "OK, not changing anything in Lucene.",
            "Just feeding it virtual documents instead of actual documents, and we measured two ratios.",
            "One was the alignment ratio, which is the ratio between the number of ones in the matrix and the number of runs of ones in the alignment, and then the index ratio.",
            "How much did the Lucene index shrink by when comparing the naive index to the Virtual Document Index?",
            "And here are the results."
        ],
        [
            "So for example, for Wikipedia, the 222 version sets contained over 4000 documents which translated to about 4445 thousand virtual documents.",
            "The alignment ratio was 7%, so the number of runs of ones is 7%.",
            "The number of total runs, and the Lucene index for the compact version was 13% that of the full version.",
            "And for Mediawiki we did pretty much the same alignment ratio.",
            "There was about 8.5%, and the index ratio again, we saved over 80% of the index space by indexing.",
            "Virtual documents versus actual real version documents."
        ],
        [
            "So to conclude, basically what we're doing is we're tapping multiple sequence alignment.",
            "These matrices that we generate for efficiently indexing groups of versioned documents that have a lot of overlapping content.",
            "And basically we also know how to optimize this alignment for a linear version model of evolution of versions.",
            "For future work, what we first want to do is to extend this to version document trees like in clearcase or in general email threads.",
            "Because so far we only treated things as if it was a linear evolution.",
            "And then the second open issue here is that this method is really appropriate only for batch indexing, because you need to take all your documents.",
            "And you order them in a specific way, so if later a new version comes into one of the groups, you can't just add posting elements in between there.",
            "Now this is fine for a lot of uses for a lot of applications like archiving.",
            "But it would also be nice to do incremental version indexing over evolving data, that's it.",
            "Understanding that your index doesn't contain positions, I mean the actual condition of the terms in the document.",
            "OK, you could extend this, yeah, because what you need to do to extend for proximity is to do the alignment on sentences, because you typically don't do phrase search processes and you don't apply proximity considerations across sentences.",
            "So just align sentences using hashes.",
            "And you do the same thing over sensors and then you can include positions.",
            "For Christmas more questions thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with Mickey Herskowitz Ansi Vanya from the IBM Haifa Research Lab and.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And basically what I'm going to talk about is how to index version data and the motivation is that clearly a lot of information systems contain documents that undergo and evolve through versions, so content management systems or version control systems for code like CVS or CCO.",
                    "label": 1
                },
                {
                    "sent": "Clearcase in wikis you have the whole change history of the wiki in many kinds of backup and archiving solutions you keep versions of the documents.",
                    "label": 1
                },
                {
                    "sent": "And in some sense which also show on the next slide.",
                    "label": 0
                },
                {
                    "sent": "Also email threads are a type of versioned data system.",
                    "label": 0
                },
                {
                    "sent": "Now basically of course it's very easy to search over version data if you take every version of the document and index it separately independently as its own document, but that somehow seems not very efficient because of the overlap and redundancy within the versions.",
                    "label": 0
                },
                {
                    "sent": "So what we're trying to do here?",
                    "label": 0
                },
                {
                    "sent": "Is to somehow generate a much more compact index of these versions, but at the same time not to lose any retrieval capabilities.",
                    "label": 1
                },
                {
                    "sent": "So we want to be able to search with all the features of the more naive index that indexes each version separately.",
                    "label": 0
                },
                {
                    "sent": "I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need one little fact from string Golla G and that that if you have two strings S1 and S2, finding their longest common subsequence is easily solvable in polynomial time.",
                    "label": 1
                },
                {
                    "sent": "So for example, if the two strings are ABCD, EF and abxy FY the longest common subsequence is ABF, and notice that this is not a common substring, it's just a common subsequence.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's talk a little bit about email, so let's look at a thread of emails between person A and person B, in which anytime one of them replies to the other one, they simply reply with history and contain the so that the answer contains the entire text of the previous note.",
                    "label": 0
                },
                {
                    "sent": "So a rights to be in this light blue text 1B, then replies to a adding a purple box 2 on top of that same light, blue text 1.",
                    "label": 0
                },
                {
                    "sent": "A replies to be and now both the light blue and the purple boxes are replicated and as this thread continues you can see that it's size becomes the square of the number of interactions, whereas there are only linear number of new textboxes.",
                    "label": 0
                },
                {
                    "sent": "And of course this can go also to trees where person one send email to person to person three.",
                    "label": 0
                },
                {
                    "sent": "They each reply.",
                    "label": 0
                },
                {
                    "sent": "Thus the thread is now like a tree and again this continues.",
                    "label": 0
                },
                {
                    "sent": "Now just consider this model where indeed you always reply with the entire history of whatever you received without ever changing or deleting or adding or embedding new text in between what you are quoting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in a recently published paper by Ibmers and now former ibmers, it was shown that you can index pretty much every new piece of text once without repeating without repetitively indexing all the quoted text.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that if we look back on the tree here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that when you index the text of a document, the posting elements are sort of shared between the descendants of that node in the tree, so I'm not going to go into too deep, too much deeper detail in that I just want to say that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This indexing method works as long as the quoted messages aren't modified, but only quoted as is and this is what this work is trying to extend.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do, inversions, you know, things change.",
                    "label": 0
                },
                {
                    "sent": "Wherever, so we're going to be able to index versions no matter how the edits were done.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is the running example I'll talk about and consider that you have four documents over here and just for simplicity, each word is denoted by a letter and again just for ease of presentation.",
                    "label": 1
                },
                {
                    "sent": "The letters are distinct in each document.",
                    "label": 0
                },
                {
                    "sent": "Of course, words can repeat.",
                    "label": 0
                },
                {
                    "sent": "Let's also say that an Oracle somehow tells me a super sequence of all four documents.",
                    "label": 0
                },
                {
                    "sent": "Now, of course I won't have that Oracle.",
                    "label": 0
                },
                {
                    "sent": "I won't need that Oracle, but for the time being assumed that somehow I know of a super sequence of a super sequence of these four documents.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'm going to use is this representation of a matrix whose first row or as we call it throw 0, is the Super sequence, and then it's a binary matrix where each row is an encoding of the corresponding sequence.",
                    "label": 1
                },
                {
                    "sent": "So for example, to reconstruct the second sequence from row #2 what I need to do is to take the letters on top of columns that have ones in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So it's going to be a BXEFY.",
                    "label": 0
                },
                {
                    "sent": "OK, this is just a representation of the four original strings or documents using a super sequence.",
                    "label": 0
                },
                {
                    "sent": "The next step.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hope is to look at the runs of ones and the columns and down here I'm writing for each column.",
                    "label": 1
                },
                {
                    "sent": "What are the runs of ones in it.",
                    "label": 1
                },
                {
                    "sent": "So for example in the first column there is a single run of 1 starting at row four Ending Info 4.",
                    "label": 0
                },
                {
                    "sent": "Here we have a run of ones starting at row one, ending that row 2.",
                    "label": 0
                },
                {
                    "sent": "And in the third column we actually have two runs of once the first run starts at row one, ends at row two, and then a second run.",
                    "label": 0
                },
                {
                    "sent": "Starts at row four and ends at Row 4.",
                    "label": 0
                },
                {
                    "sent": "Of course there are 10 possible runs of ones that can happen in this matrix, and I'm going to order those runs of ones according to their end point, and then according second, the secondary sort is according to their start points.",
                    "label": 1
                },
                {
                    "sent": "So first I list all the runs that end at index one, then the two runs that end in index 2.",
                    "label": 0
                },
                {
                    "sent": "The three runs that ended index three, and the four runs that end at Index four, and within each group that ends at a certain index.",
                    "label": 0
                },
                {
                    "sent": "The start points are also ordered in increasing order.",
                    "label": 0
                },
                {
                    "sent": "Now what is this at all good for?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to forget about these four original documents and actually I'm going to create 4 excuse me 10 virtual documents.",
                    "label": 0
                },
                {
                    "sent": "Each virtual document is going to correspond to a specific run of ones.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to distribute tokens or words to these 10 virtual documents according to the columns.",
                    "label": 1
                },
                {
                    "sent": "Corresponding to that one of ones.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "I have a virtual document corresponding to the run that starts at row two and ends at 204.",
                    "label": 1
                },
                {
                    "sent": "And you see, I've assigned words X&Y to that document.",
                    "label": 0
                },
                {
                    "sent": "Why's that?",
                    "label": 0
                },
                {
                    "sent": "Because in the column corresponding to XI had to run starting at 2, ending at four, and also in the column corresponding to Yi, had a run starting at row two and ending intro four.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm basically distinct distributing tokens.",
                    "label": 1
                },
                {
                    "sent": "From real documents to virtual documents according to the runs of ones and this in the columns of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And basically what I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have a question.",
                    "label": 0
                },
                {
                    "sent": "Of course, of course, because there are repeated words in documents.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to index the virtual documents, not the true documents, and basically this is where I'm going to save space because the number of posting elements in the posting lists is reduced from the total number of ones in the matrix, which are the total number of words in the original documents to the number of runs of ones.",
                    "label": 0
                },
                {
                    "sent": "In the in the in this alignment matrix.",
                    "label": 0
                },
                {
                    "sent": "In this case, instead of indexing 25 words in the four original documents, I'm going to index only 12 words in a few more virtual documents.",
                    "label": 0
                },
                {
                    "sent": "That's going to be where the index space is going to be saved.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just another view of the inverted index.",
                    "label": 0
                },
                {
                    "sent": "I hear the virtual documents ordered from one to 10 according to my special way of numbering them, and there are 12 words within these documents.",
                    "label": 0
                },
                {
                    "sent": "And here you see the posting lists just like a normal inverted index.",
                    "label": 0
                },
                {
                    "sent": "But on these new documents.",
                    "label": 0
                },
                {
                    "sent": "In Prague.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This of course any interesting repository will have multiple groups of versioned documents, so I do this same process over and over again for each group.",
                    "label": 1
                },
                {
                    "sent": "So if I had like four groups of version documents with a total of 11 documents, I would have built.",
                    "label": 1
                },
                {
                    "sent": "22 virtual documents 6 corresponding to the first group of three versions and then three corresponding to the second group of two versions.",
                    "label": 0
                },
                {
                    "sent": "10 corresponding to the group of four versions and another three.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have a large index.",
                    "label": 0
                },
                {
                    "sent": "Divided into groups of virtual documents.",
                    "label": 0
                },
                {
                    "sent": "And it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we can do search operations on this index and simulate what we would have done on the full index.",
                    "label": 0
                },
                {
                    "sent": "So basically I'm going to explain a little bit how to do that for Boolean queries, But this can be extended to vector space model, TF IDF queries very easily.",
                    "label": 0
                },
                {
                    "sent": "The way I'm going to explain it is that if you got a bunch of Boolean queries, just replace each negated term or minus term by a virtual cursor that simulates, you know that stops at wherever the minus Trump does not stop.",
                    "label": 1
                },
                {
                    "sent": "And we're going to use a document at a time.",
                    "label": 1
                },
                {
                    "sent": "A normal document at a time algorithm where you go by your document space and you find the next candidate the next kind of document that satisfies the Boolean requirements.",
                    "label": 0
                },
                {
                    "sent": "OK, nothing really new about here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "How are we going to find the Boolean document we're going to use a zigzag join and I'll skip the details here because.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to try to explain it with this small example.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have a term.",
                    "label": 0
                },
                {
                    "sent": "These are the physical documents, and these are the virtual documents.",
                    "label": 1
                },
                {
                    "sent": "There are many more virtual documents, and let's say you have a term whose cursor stopped on.",
                    "label": 1
                },
                {
                    "sent": "This specific virtual document that represents a span or a runs of 1A run of ones.",
                    "label": 0
                },
                {
                    "sent": "From this document to this document.",
                    "label": 1
                },
                {
                    "sent": "So in the original documents, this term appeared in all four documents.",
                    "label": 0
                },
                {
                    "sent": "Now you have this second term, and let's say it's cursor is somewhere behind and it's posting lists over the virtual documents and you want to ask yourself where should I advance it to?",
                    "label": 1
                },
                {
                    "sent": "So if this is the virtual document corresponding to a run of ones from document one to document from.",
                    "label": 0
                },
                {
                    "sent": "You can safely disregard any virtual document before this point, you can simply skip over them using your posting list mechanisms, so you can skip over them, and this is where the interesting area of virtual documents that may overlap in terms of words begins.",
                    "label": 0
                },
                {
                    "sent": "And in fact, all virtual documents between this point and this point.",
                    "label": 1
                },
                {
                    "sent": "Correspond to ranges of physical documents that will surely overlap with this, so using a little bit of interval algebra, if the second term falls within this range of virtual documents, we can identify a range of physical documents that surely contain both terms.",
                    "label": 1
                },
                {
                    "sent": "OK, that's going to be what we're going to do here.",
                    "label": 0
                },
                {
                    "sent": "We're going to do some interval algebra over the virtual documents.",
                    "label": 0
                },
                {
                    "sent": "And you know there are rules of what happens if the second cursor falls beyond this point and so forth.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go into the details.",
                    "label": 0
                },
                {
                    "sent": "You can read them in the paper, and even in the notes in the handout, they have a lot more details on this.",
                    "label": 0
                },
                {
                    "sent": "But basically we can do Boolean queries over the virtual documents.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you believe that, then let's now do a little bit of analysis on the index size.",
                    "label": 0
                },
                {
                    "sent": "What are we saving and where are we perhaps losing so in terms of the lexicon of the inverted index, nothing has changed.",
                    "label": 0
                },
                {
                    "sent": "The set of unique terms remains the same, so the lexicon size remains the same.",
                    "label": 0
                },
                {
                    "sent": "In terms of the number of posting elements in the posting lists, as I already talked about, we are instead of using the number of ones in the matrix, we're only going to be using the number of runs of ones in the matrix, so that's where the biggest reduction is.",
                    "label": 1
                },
                {
                    "sent": "We are losing a little bit on the compression of posting lists since the virtual document space is larger so the gaps are larger.",
                    "label": 0
                },
                {
                    "sent": "They don't compress as well.",
                    "label": 0
                },
                {
                    "sent": "We're going to lose a little bit there and then.",
                    "label": 0
                },
                {
                    "sent": "We also need a little bit of extra overhead.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that.",
                    "label": 0
                },
                {
                    "sent": "You'll see that in the results.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we said that the biggest savings come from not indexing ones in the matrix, but only runs of ones in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So we now have an optimization problem given a set of strings.",
                    "label": 1
                },
                {
                    "sent": "Or documents find an alignment matrix that has the minimal number of runs of once.",
                    "label": 1
                },
                {
                    "sent": "Sounds easy, but then there are two NP hard problems hiding in there.",
                    "label": 1
                },
                {
                    "sent": "First is to find the shortest common supersequence.",
                    "label": 0
                },
                {
                    "sent": "So as I said in the beginning, we have this Oracle that gave that sort of.",
                    "label": 0
                },
                {
                    "sent": "We started with the Super sequence.",
                    "label": 1
                },
                {
                    "sent": "Nobody knows how to find a short common super sequence, and we're not going to try to do that because it's NP hard and then even if someone gave you a super sequence and also gave you the alignment of each string to the Super sequence.",
                    "label": 0
                },
                {
                    "sent": "So essentially, if someone gives you the Rose of the binary matrix and just ask you to permute them so that the number of runs of ones will be minimal, that's also NP hard.",
                    "label": 0
                },
                {
                    "sent": "So we can't do that either.",
                    "label": 0
                },
                {
                    "sent": "So what can we do?",
                    "label": 0
                },
                {
                    "sent": "We're going to make a reason.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assumption, you know in real life to say that if the versions are generated in a serial manner, like for example in some CVS or CNBC version repository, then probably the normal time evolution of the sequences is the good or will be a good order in the matrix.",
                    "label": 0
                },
                {
                    "sent": "So this changes the optimization problem too.",
                    "label": 1
                },
                {
                    "sent": "When given an ordered set of strings, find an alignment matrix whose sums of sum of run of ones is minimal.",
                    "label": 1
                },
                {
                    "sent": "Now nobody is giving us the Super sequence.",
                    "label": 1
                },
                {
                    "sent": "And so nobody is giving us the rows of the matrix, just the strings.",
                    "label": 0
                },
                {
                    "sent": "The original strings themselves.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that.",
                    "label": 1
                },
                {
                    "sent": "The order that was given is enough so that a simple greedy algorithm that uses.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about why using the second will produce the optimal alignment matrix.",
                    "label": 0
                },
                {
                    "sent": "That has this order of rows and basically what you want to do is you want to write the first string.",
                    "label": 0
                },
                {
                    "sent": "And then for each subsequent string, just do your best using the longest common subsequence is of two strings.",
                    "label": 0
                },
                {
                    "sent": "To glue the next string to the previous one.",
                    "label": 0
                },
                {
                    "sent": "OK, you only care about.",
                    "label": 0
                },
                {
                    "sent": "It's sort of a Markovian model where you only care about gluing the next one as best you can to the previous one.",
                    "label": 1
                },
                {
                    "sent": "And also the full paper contains a theorem that justifies this intuition in mathematical terms, meaning that if your strings satisfies certain mathematical conditions.",
                    "label": 0
                },
                {
                    "sent": "The natural order is indeed the best order.",
                    "label": 0
                },
                {
                    "sent": "Just an example of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How to do the alignment you start up from the first string.",
                    "label": 0
                },
                {
                    "sent": "You then take the second string and just by shortest common subsequence, you glue it to the first one.",
                    "label": 0
                },
                {
                    "sent": "And you continue with the third.",
                    "label": 0
                },
                {
                    "sent": "Continue with the 4th.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this, for example, is what the algorithm would do to our original 4 strings.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Note that this is a much wider matrix than what I started with.",
                    "label": 0
                },
                {
                    "sent": "Because we're not finding the shortest common supersequence right?",
                    "label": 0
                },
                {
                    "sent": "That's NP hard, but we don't care, because this still has the same number of runs of ones in its columns, 12 as that much leaner and more compact matrix.",
                    "label": 0
                },
                {
                    "sent": "I showed you at the beginning.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "A few.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Experimental results, so we took a couple of different versioned repository's.",
                    "label": 1
                },
                {
                    "sent": "We started out with the 222 Wikipedia entries corresponding to countries, and then the second data set was a bunch of Mediawiki PHP source files, and for each we took up to 20 versions of each document set.",
                    "label": 0
                },
                {
                    "sent": "So here we took up to 20 versions of each of the 222 documents, and then we did this indexing using Lucene.",
                    "label": 0
                },
                {
                    "sent": "OK, not changing anything in Lucene.",
                    "label": 0
                },
                {
                    "sent": "Just feeding it virtual documents instead of actual documents, and we measured two ratios.",
                    "label": 0
                },
                {
                    "sent": "One was the alignment ratio, which is the ratio between the number of ones in the matrix and the number of runs of ones in the alignment, and then the index ratio.",
                    "label": 1
                },
                {
                    "sent": "How much did the Lucene index shrink by when comparing the naive index to the Virtual Document Index?",
                    "label": 0
                },
                {
                    "sent": "And here are the results.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, for Wikipedia, the 222 version sets contained over 4000 documents which translated to about 4445 thousand virtual documents.",
                    "label": 0
                },
                {
                    "sent": "The alignment ratio was 7%, so the number of runs of ones is 7%.",
                    "label": 1
                },
                {
                    "sent": "The number of total runs, and the Lucene index for the compact version was 13% that of the full version.",
                    "label": 0
                },
                {
                    "sent": "And for Mediawiki we did pretty much the same alignment ratio.",
                    "label": 0
                },
                {
                    "sent": "There was about 8.5%, and the index ratio again, we saved over 80% of the index space by indexing.",
                    "label": 1
                },
                {
                    "sent": "Virtual documents versus actual real version documents.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, basically what we're doing is we're tapping multiple sequence alignment.",
                    "label": 1
                },
                {
                    "sent": "These matrices that we generate for efficiently indexing groups of versioned documents that have a lot of overlapping content.",
                    "label": 1
                },
                {
                    "sent": "And basically we also know how to optimize this alignment for a linear version model of evolution of versions.",
                    "label": 1
                },
                {
                    "sent": "For future work, what we first want to do is to extend this to version document trees like in clearcase or in general email threads.",
                    "label": 1
                },
                {
                    "sent": "Because so far we only treated things as if it was a linear evolution.",
                    "label": 0
                },
                {
                    "sent": "And then the second open issue here is that this method is really appropriate only for batch indexing, because you need to take all your documents.",
                    "label": 0
                },
                {
                    "sent": "And you order them in a specific way, so if later a new version comes into one of the groups, you can't just add posting elements in between there.",
                    "label": 0
                },
                {
                    "sent": "Now this is fine for a lot of uses for a lot of applications like archiving.",
                    "label": 0
                },
                {
                    "sent": "But it would also be nice to do incremental version indexing over evolving data, that's it.",
                    "label": 0
                },
                {
                    "sent": "Understanding that your index doesn't contain positions, I mean the actual condition of the terms in the document.",
                    "label": 0
                },
                {
                    "sent": "OK, you could extend this, yeah, because what you need to do to extend for proximity is to do the alignment on sentences, because you typically don't do phrase search processes and you don't apply proximity considerations across sentences.",
                    "label": 0
                },
                {
                    "sent": "So just align sentences using hashes.",
                    "label": 0
                },
                {
                    "sent": "And you do the same thing over sensors and then you can include positions.",
                    "label": 0
                },
                {
                    "sent": "For Christmas more questions thank you.",
                    "label": 0
                }
            ]
        }
    }
}