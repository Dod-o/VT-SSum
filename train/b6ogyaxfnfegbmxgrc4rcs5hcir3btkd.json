{
    "id": "b6ogyaxfnfegbmxgrc4rcs5hcir3btkd",
    "title": "Transductive Learning and Computer Vision",
    "info": {
        "author": [
            "Jean Yves Audibert, Center for Education and Research in Computer Science of the \u00c9cole des ponts, \u00c9cole des Ponts ParisTech, MINES ParisTech"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/wehys08_audibert_tlcv/",
    "segmentation": [
        [
            "Is anybody Bethany?",
            "Will talk about active learning and comfortable.",
            "OK, thank you very much.",
            "So work I will present joint works with several coffers.",
            "My chest island would even look spoke for the theoretical part on graph Laplacian or distribution hooker even and John Ponds for support on image segmentation.",
            "And for the pattern Interactive image retrieval, it has been at work with Patrick at Azure and OK then, and it seems to be.",
            "In a semi supervised."
        ],
        [
            "Ning and.",
            "And in constructive learning we have we observed many more input points than corresponding output points, and we want to.",
            "Use the information conveyed by the extra input points to define data dependent concept space that improve how a prediction accuracy.",
            "In this presentation I will focus on one way of using these extra input points.",
            "This way is based on the graph Laplacian.",
            "So in the first part I will mainly talk on the property of the graph repression and how can we use it.",
            "And then I will turn to two applications to computer vision problems."
        ],
        [
            "OK, so I believe you all know this.",
            "The Standard learning task in machine learning machine learning is well in supervised machine learning.",
            "Is inductive learning in window in which you observe an input, output pairs and the question is what is the output associated to a new input.",
            "Any new input?",
            "In terms of active learning, you also observed unlabeled data points.",
            "And the question is just what are the levels of these extra input points and semi supervised learning is something in between.",
            "Where is the observation are the same as in constructive learning, you have unlabeled data.",
            "But the question is what is there put associated to any new input?",
            "Of course, if you have.",
            "A semi supervised learner.",
            "You can use it to solve this program and if you have a transductive learner you can use it to solve this problem just by.",
            "If you are asked the prediction for a new input small X, you add it here and you use your transductive learner on all your points.",
            "All your observed points plus X.",
            "But usually the algorithm, usually you don't want to do this.",
            "Because it will cost you.",
            "Offensive transitive learning learner textual.",
            "A lot of time to compute, so you don't want to use this type of idea."
        ],
        [
            "OK, and so key assumption, which has been very popular lately.",
            "And so the reason of this assumption is, is not technical here.",
            "It's because it really correspond at what you what you observe on real data.",
            "So this key assumption is that the decision boundary in so I'm talking about constructive classification appears in the low density region of the input space.",
            "So.",
            "Let me illustrate this assumption.",
            "So consider this data set.",
            "You have label points by blue or red.",
            "An unlabeled data points which are Gray, so this is the input space.",
            "If you.",
            "According to this assumption, the boundary between the blue class and the red class should be something like this.",
            "Other assumption without help Windows is providing context like having talked about in the morning.",
            "That would bring assumption.",
            "Yeah, yeah yeah but.",
            "Yeah.",
            "But maybe this is.",
            "I would say this is the most prominent one as well.",
            "OK, you might disagree.",
            "And OK, if you use an inductive learner, you see that it won't work because you will drop for get about all your unlabeled points.",
            "You will consider only these points and then your inductive learner usually ends out with this kind of separator between the classes.",
            "And you see that it won't classify the points subject it satisfies this key assumption.",
            "This popular assumption.",
            "OK.",
            "So.",
            "An his Tokyo is about.",
            "One way of designing an algorithm using this key assumption.",
            "An the way is to use graph flip an approximation of the Laplace operator."
        ],
        [
            "And and this approximation will depend on density so.",
            "I consider throughout this presentation that the input points are in a some Euclidean space.",
            "And there is a density.",
            "Of the input points distribution with respect.",
            "2.",
            "To the lab measure, if you are considering that your input points is the support of the of the distribution of your input is also early in space, but in a more general setting, if you consider that your input points are supported on a manifold, you just.",
            "We're just assuming that there is a density of the distribution with respect to the natural measure on this manifold."
        ],
        [
            "OK, let me recall briefly what is the Laplace operator.",
            "Well, for a function defined on R and taking real valued, this is just the repression of a function, it's just a second derivative.",
            "When you consider a function from a Euclidean space to R, then it is just the sum of the second derivatives with respect to each of the component.",
            "And the generalization of this formula to a manifold case just writes by Laplacian of the function it just divergents of the gradient of the."
        ],
        [
            "Ocean.",
            "But here.",
            "We don't have only a manifold, we have probability supported by this manifold, so we really want we really use an operator which takes into account this this distribution.",
            "So the weighted lap clash Beltrami operator is just a generalization of the universe.",
            "So usually passive greater.",
            "That takes into account the density in the following ways, repression.",
            "So we have a parameter S which is real and celebration the estimated oppression.",
            "Make this density.",
            "Of the input appear in this way, 1 / P power S times the divergent of people were S gradient of F. And this thing that you should notice is that for S = 0, then the estimated Laplace operator, it's just the usual Laplace operator.",
            "An more importantly, it's this escalated Laplace praytor satisfy this key property, which.",
            "Which you can see as the dual well as an integration by parts formula.",
            "Or do jewelry quality.",
            "It says that for any function with compact support on the manifold integral of F times the application of a liver function with respect to people, SDV is equal to minus integral of the product between the gradient of the two function you are considering or still with respect to the measure people SDV.",
            "And the problem is the problem.",
            "We will talk in the first time is given a function.",
            "How do you approximate?",
            "So the question of this function.",
            "At some point, X by only using the values of F at at at points which has been sampled on the density an X.",
            "So if you don't like manifold, you can also.",
            "Think of this.",
            "You can take is a manifold as being all your occlusion space and it will not change anything and you can already and this problem can be also sought in this situation when the space has dimension D = 1, it's still not that trivial to find."
        ],
        [
            "Solution.",
            "OK Anne, sorry.",
            "So wasting this program it says it's because it has many application for clustering, because when you have an estimation of the operation operator you can buy.",
            "Looking at the first again, Victor, do clustering spectral clustering you can do will see also dimensionality reduction by looking at the 1st.",
            "Again, victors.",
            "And what we will see here is how to use it in transductive learning.",
            "And many people has worked with this setting.",
            "Starting with the.",
            "Belkin and Yogi."
        ],
        [
            "OK, so how do you build an approximation of solar plus operator you start?",
            "By building a neighborhood graph.",
            "So for building it, it's a weighted graph.",
            "For building it you use a kernel.",
            "That is just a function which looks like this.",
            "So it is essentially it's positive and it's zero outside the compact set and this gives you a way of.",
            "Measuring similarity between two input points.",
            "So you see that.",
            "This way.",
            "By this definition, the similarity will be 0 if the distance between the two input points is greater than H. Then you can.",
            "Then we will use the degree well this function, which is the sum of the similarity of XI with all other points to define a new similarity metric.",
            "So up to now, I think I've been saying quite standard things.",
            "Here is the non standard part.",
            "Instead of using your initial similarity, what almost everybody do?",
            "Do you well when using graphical oppression techniques, you normalize it by degree.",
            "At some power, so Lambda will be a parameter here.",
            "And this parameter will be linked to the S parameter of the estimated Laplacian.",
            "And OK, so what's this normalization is doing?",
            "It is quite well known that up to a constant, this sum approximates the density the density.",
            "At Point XI.",
            "So here we are just normalizing by like.",
            "To density.",
            "Times didn't.",
            "Is density X XI times density I, XJ power Lambda, and since XI and XJ will be closed.",
            "This is basically the density at XI power to Lambda, so this is one way of things this normalization.",
            "And the neighborhood graph is just the graph in which the vertices are the input points you observe.",
            "The edges.",
            "Also, a pair of points such as the similarity is is positive.",
            "And the weight on these edges are given by this normalized similarity measure."
        ],
        [
            "OK, so this is the graph you can end up with if you have this curved manifold.",
            "And you notice that for the Bandwidth H that has been taken, you have something some unwanted edges which has appeared which connect points which are far in the distance.",
            "So the choice of H is always crucial when you have this kind of.",
            "A formula over form of the manifold."
        ],
        [
            "OK, so now if you want to approximate suppression of a function at a given point.",
            "You need to add this point.",
            "Well we will consider an extended neighborhood graph in which this point has to be is added.",
            "And.",
            "We will use this degree function so this degree function takes into account the normalized kernel.",
            "And the averaging operator is defined as you are.",
            "Everything F. With the weights with this weight and I recall that the weight here is equal to 0 if actually is at a distance greater than H from X.",
            "And OK. Yeah.",
            "Yeah, yeah, as you will see I will do it, but I prefer to introduce this this operators and refers to normalized."
        ],
        [
            "So in this slide introduces first random work graph location that appears in the Lake Shelter, and indeed I am normalizing by 1 / D such that you recover.",
            "Something which looked like to the nether Ha Watson estimators at the Shabba has to in the previous talk.",
            "So.",
            "What you see is that the repetition of the function F at point X is up to a constant F of X minus.",
            "The average of F. In a neighborhood of X and this is really what.",
            "People are doing in dimension one.",
            "If you want to estimate this.",
            "So second derivative at point X.",
            "You you can do it by this usual formula and you can rewrite this formula so subject you make appear the value of the function at point X and the average of the function at nearby points.",
            "Of course, in my setting you cannot do this becausw well.",
            "You have no choice of the points at which you know the function.",
            "OK, and why is it called the random walk?",
            "Replication it because it is very much linked with the expectation of the function.",
            "Of one step walk.",
            "Using this transition matrix, so from the similarity you can build this transition matrix which gives you the IG element of these metrics, gives you the probability of going to J starting from I.",
            "And if you start your own book at Point X and do just one step, which is wrong.",
            "Just one step with this from the move and you do not value one the value.",
            "Where you are after one step also random work expectation of FW One.",
            "Is exactly this AF over the so this explain the name of this graph location so this is 1 graph location that has been used, but several other ones has appeared."
        ],
        [
            "And here we have put all of them, so this is the previous one.",
            "The unnormalized graph oppression differs by supposition of this degree function.",
            "And the normalized graph oppression.",
            "Is something which is, which seems unnatural at first view, but which has nice property as we will see.",
            "Instead of applying the average operator directly on F, you are playing.",
            "At on F / sqrt T so people who has used the graph oppression user.",
            "Usually states the definition by matrices, so you should see this as identity matrix minus D -- 1 * W. The matrix of similarity.",
            "Here it's just D minus the value, and here it is D -- 1/2 * D -- W * D -- 1/2.",
            "So there are free graph liberation and the the main reason."
        ],
        [
            "Is that this three graph Laplacian indeed converges, indeed converge with something which is related to graph oppression but.",
            "They are converging to different limits.",
            "So.",
            "Basically the result is provided that the bandwidth is small enough, so this is the usual thing.",
            "Small enough, but not too small E in order to have really an average.",
            "Typically H equal login is working then the the this.",
            "Graph oppressions converges.",
            "To something in which the flash, the escalated repression appear.",
            "And S is 2 * 1 minus Lambda.",
            "Lambda is the parameter.",
            "That appeared in the normalization of your similarity measure.",
            "And what you can see is that if Lambda is equal to 1/2, all the limits are the same.",
            "So I'm getting rid of the constant because for.",
            "For the machine learning techniques that are using graph repressions, these constants are not important.",
            "OK, so for Lambda equal 1 have.",
            "The limit is just this one an for uniform distribution.",
            "You note that all the limits are also the same, but in other cases these limits are different.",
            "So when you are using graph pression metrics, you should really wonder.",
            "Watch what you are really doing.",
            "And this is maybe a way of choosing the right one for your problem.",
            "OK."
        ],
        [
            "I think that I have already said this.",
            "OK."
        ],
        [
            "So now let us come back to the initial problem, transductive classification using the graph Laplacian.",
            "So.",
            "How do we incorporate this key assumption that the input density addition Asian bhandary is more well a simpler and I would say natural thing to do is well.",
            "You assume that the levels are plus 1 -- 1, for instance.",
            "You will find look for regular function smooth function taking real value subject on your input points.",
            "Your function match outputs zero observed outputs.",
            "And you ask also that the function is smooth and also more smooth as the density is high.",
            "And this is natural in view of this assumption, you want F. You want your function.",
            "2.",
            "To change well, it is to have a high gradient only when when the density is very small.",
            "And the nice thing is, this optimization problem is, well, it's not a very nice optimization program because you have to optimize over an infinite set of function defined on manifold and you don't know the manifold.",
            "You don't know the density, but Fortunately you can rewrite this term as by the integration by parts formula if.",
            "I mentioned before as the integral of F times elaboration of F with respect to people were SDV.",
            "And from the law of large numbers.",
            "If you consider this some.",
            "Which make appears a function F times lation times.",
            "People were S -- 1.",
            "Well, this this rage converge to the expectation of this function and this expectation of this function is exactly this one.",
            "So yes, minus one coming from the fact that distribution is PDV.",
            "OK, so and the nice part of this formulation."
        ],
        [
            "Is that?",
            "This problem makes appear only value of the function and of its repression at the points that you have observed."
        ],
        [
            "So in this algorithm there are two parameters.",
            "Essentially, C is a how much you believe that the output you have been given are correct.",
            "And S is how much you believe in the assumption."
        ],
        [
            "And now the last thing you have to do is.",
            "To approach to approximate this quantity and what we have seen be."
        ],
        [
            "4 is that.",
            "You know how to do this, it's exactly the limit of the unnormalized graph Laplacian.",
            "So you can just replace the Red Temple which was in the previous slide by this and normalized oppression.",
            "And your.",
            "Now you're a normalized preparation.",
            "You can when it is taken at the points of your of your training data, it is just D -- W * F, where W is a similarity matrix.",
            "D is the.",
            "Diagonal matrix of the degrees and F are the predicted output vectors.",
            "And it contains the training, while the label points of your observation and also the unlabeled points."
        ],
        [
            "So so."
        ],
        [
            "The previous problem, this one can be written in."
        ],
        [
            "Metrics form and it makes clearly appears.",
            "That you have a problem.",
            "Your optimization program, which was on a set of function, is now just a program on.",
            "And the vectors on our end plus T. And this is a quadratic problem.",
            "So by differentiation you have that the solution satisfies this linear equation.",
            "And therefore it's tractable.",
            "And it is also more tractable as this system is very sparse, because this similarity, this matrix is diagonal and CC is also a diagonal matrix and the value is the similarity metrics, so it's usually a sparse matrix.",
            "And phone for the unlabeled input XI.",
            "The algorithm.",
            "I'll put the sign of F of I.",
            "So the F code in it?",
            "If so, you end up with an algorithm which has three parameters that is.",
            "So Bandwidth H, the constancy represented representing your belief in the labeled points that the label are correct and South, which is the belief in your assumption."
        ],
        [
            "So let's turn now to 1st application of this algorithm.",
            "That is, image segmentation.",
            "Image segmentation is a problem of partitioning numeric image into meaningful regions and it is really a key task in computer vision.",
            "But also an important problem, because for any given image there are several ways to segment it, and it really depends on what your user wants.",
            "The result really depends on its desire.",
            "So one way of using it is to use super user supplied."
        ],
        [
            "Seeds, so for instance, if you want to segment.",
            "This image without the blue and red strokes.",
            "So you ask the user to put strokes on it, saying well the objective of interest is, well, this this pixels in blue are in the same region and the pixels in red should be in the same region.",
            "And what you want to recover is this kind of image and this is typically transductive learning problem.",
            "So the inputs here are also pixels, so you have labeled pixels.",
            "And you want to predict the label of any other pixel of the image.",
            "So this is a way of.",
            "Of solving this task that well in computer vision, there are many, many ways of solving this program.",
            "It can be based on level sets, method.",
            "Or shield, is it computations and there are also other graph based method like minecart watershed.",
            "And also one which comes from.",
            "From similarity with electric."
        ],
        [
            "Potential.",
            "OK, so how is tradition was to apply the algorithm that we have seen in the first part?",
            "So the parameters then.",
            "You have more parameters because you're in computer vision.",
            "You have always plenty of parameters.",
            "Like the well you have.",
            "So the Pixel is there is not obvious things.",
            "Here it is the representation of the pixel.",
            "You can represent it by just its color, but it's better to represent it by both the geometric position of the pixel in the image.",
            "And some kind of color vectors which represents the texture around your pixel.",
            "So.",
            "Here we have represented by the these colors on a Patch of squares Patch around the pixel an just these geometric position and the tradeoff between the color information and the geometric information is just set up by using Sigma J and Sigma Si so Sigma J. Sigma Chi and Sigma C Just bond with you so this this was my H in the first part of the talk."
        ],
        [
            "OK, and then then we follow the schemes that has been described previously.",
            "So difference here is that you are you assume.",
            "That's the user has leveled correctly.",
            "Also points that he has leveled so C in the.",
            "Previous slide was is now equal to Infinity because we've forced the output function.",
            "To correspond to the level that has been given.",
            "So you you still end up with a linear system and the prediction of the four pixel J is just the sign of.",
            "FG."
        ],
        [
            "OK.",
            "There was ever that there have been other approaches.",
            "OK, there are approaches which are quite close to ours.",
            "Even if there were stated in a different way.",
            "First, if you use you think of graph cut method which has, which is a very popular method in this field.",
            "Well, it's doing exactly the same minimization, except that instead of looking for real valued, you look for only minus 1 + 1 value.",
            "And you consider also the normalization Lambda equal to 0.",
            "So normalization or in the similarity Lambda equals zero correspond to equal 2.",
            "If you normalize, Kurt is a method which also use this normalized mattress graph repression metrics.",
            "But well, it's it's not clear when you look at the paper, but it correspond to ethical one.",
            "Gwen and Q.",
            "Stated the programming in a different way, but you can check that it corresponds to this problem.",
            "And if you use the.",
            "As a fair tickle analysis of the first part, you see that the additive regularization term is not at all this one.",
            "It's this one, well you can see it's not.",
            "Natural regularization term.",
            "Because the function F. Can be linear, so any function F which is linear has paid nothing for moving from class minus one to class plus one, so this is a really an unwanted effect.",
            "And if you look at the greedy algorithm, in fact it is the same as ours for S equal 2.",
            "So yeah, one nice thing about this convergence result.",
            "It is that it provides a way of.",
            "New interpretation of existing algorithm and maybe some.",
            "Yeah, when you see that.",
            "You are recognizing by this formula form it's.",
            "I would say it's more explicit.",
            "It tells me more about how they are."
        ],
        [
            "Doesn't work.",
            "Yep."
        ],
        [
            "Sorry.",
            "Either it's motivated by graph theoretical electrical potential.",
            "Well.",
            "Well, she write it like this and she said OK, the paper is called something like Honda MC Segmentation.",
            "But if you look at it, it's not really a random look.",
            "It just selling argument and he really says in his paper.",
            "I've been inspired by this theory and he really yeah do this like this.",
            "Yeah, I think it is connected with the posters which has been one of your poster and also the other clusters."
        ],
        [
            "OK, here's some experimental results, so unfortunately in this field there is a.",
            "There is not much the database which correspond to this situation when the user provides only small information like this strokes.",
            "So database on which."
        ],
        [
            "As we can compare each other, is this one?",
            "So here is the image you want to segment.",
            "Here is the information you get, so the information is that white pixels should be in the foreground region and this.",
            "Hard Gray pixels should be in the other class, So what?",
            "You just what you are just looking is to locate the boundary in this region so it's a much easier task.",
            "And people have come with different algorithm.",
            "But here we obtained this data of the art results which is four 5.4% of the pixel of this green region here misclassified.",
            "And in fact, to be honest.",
            "The algorithm of low gravity, which correspond to S equal 2, is also doing 5.4, and the improvement here is statistically significant but with a P value between 5% and 10%, so not that not that signif."
        ],
        [
            "OK, I will skip support on interactive image search, but just to describe the principle and to see why it can be an application for this constructive learning with graph passion.",
            "So principle of interactive image serves is that you have a database and that you were under user which looks for photo for images in this database."
        ],
        [
            "So you display some images.",
            "So each point here is an image or supposed.",
            "And you display some images."
        ],
        [
            "The user can label them, and from this point switch has been leveled."
        ],
        [
            "2.",
            "You do.",
            "You decide."
        ],
        [
            "What is the next display in order to rapidly focus on the class of interest?",
            "So I will skip some method icing."
        ],
        [
            "OK."
        ],
        [
            "I think to conclude.",
            "OK. What I've been presenting is a modification of the way of using graph preparation for transductive learning.",
            "So main modification is this normalization because when you normalize differently correspond to believing more or less in your assumption that the decision boundary between classes.",
            "Is in low low density region of your input space.",
            "And this can be applied to image segmentation.",
            "And the main point is really when if you use a graph Laplacian matrix, you should.",
            "Knows that there are several possibilities, so a normalized random walk or normalized version of it.",
            "And you should also be aware that we know to which.",
            "Continuous quantity converge and this can be useful to interpret your algorithm.",
            "OK, thank you very much.",
            "Yep.",
            "To make this assumption that the decision boundaries go through low decorations and I'm thinking about the image example, but it's really hard.",
            "Yes, I features like.",
            "Features like you're collecting colors in a Patch around the pizza, and then you're moving the pixel around that.",
            "Yeah, OK then.",
            "Alright, so yeah I know for.",
            "For the database on which."
        ],
        [
            "People were fighting to have good segmentation results.",
            "It's a very particular database in which the color is very important, so for this particular database, the size of the Patch around the pixel was just one, so that is 1 pixel.",
            "So we're considering just one color and the geometric position.",
            "So maybe in this case it's more at least it's more natural, but I don't understand it.",
            "Even if you use patches, there will be a discontinuity.",
            "Pretty like if the patches it's a bit messy in the transitions that OK, I agree.",
            "But yeah, OK, yeah we are not thinking of very big patches anyway yeah.",
            "Have you tried to like?",
            "Think about the matter like many people like you're representing it.",
            "We were in 5D, if you take.",
            "The size of the Patch equal to 1, so just one pixel.",
            "We are in 5D.",
            "Our input spaces in 5D.",
            "And it's a bit complicated to represent things, so we have tried at some points, but it was not.",
            "There not so clear.",
            "Yeah.",
            "Question so.",
            "It was sorry.",
            "My sense is that the graph methods are best suited when data is in a relatively low dimensional space or a low dimensional manifold.",
            "Yeah, is that your friends like friends?",
            "If I had say two Gaussian saying high dimensional space may be staring deviation one or something separated by some distance, like yeah there would be a low density region between yeah, but two points in the same Gaussian would be like Ruthie apartment.",
            "There's a discipline, two points in the same side.",
            "Would be comparable.",
            "The distance between points in different size if you could, 'cause the dimension is so high you know it's like everybody is far from everybody else.",
            "Is your sense that kind of a graph methods are better when it's low dimension or this thing things seem to?",
            "Work well.",
            "Well.",
            "I don't know, it's not clear to me.",
            "Yeah, I don't know the answer of the question.",
            "I mean if you ask it, it's because you don't believe in graph repression well in graph based methods for high dimensional purpose.",
            "The bounds occurrences were manifold, can exponentially with dimensions available.",
            "So if you had data that wasn't necessary.",
            "But that's the dimension of money for another.",
            "Yeah, like if you had like the two high dimensional Gaussian, well separated, you could still have a low density region between them.",
            "It's just hard for me to think how you would seem like if you can make you know that the parameter for the graph too small.",
            "Nobody's connected if you make it too large, everybody's connected, so it was just wondering if you had.",
            "Thoughts on either?",
            "No, I have nobody else.",
            "Maybe there was.",
            "So turns on the detailed study of normalization for raffle caution methods and hammer all the details, but one of the things that were we looked at was.",
            "Rather than normalizing in the primal law.",
            "And he.",
            "This category normalized currently.",
            "How do you spell Stanley normally?",
            "Did you forget?",
            "No, I'm not aware of this.",
            "This work it was.",
            "It's normalizing by the degree to some power.",
            "Sidered that, but he also he considered a number of methods, and so the other thing is so like.",
            "No matter which released in history was able to.",
            "I think for the best pounds is a technical question.",
            "You find the kernel little caution and then you divide the kernel then by let's say you just had had had a matrix by my column, right?",
            "Alex alive by the two columns.",
            "That's why so that the norm of connected through as one normal the column.",
            "Tomorrow's morning yeah.",
            "Well I should look at it definitely.",
            "Once the convergence of the graph Laplacian operated on a function to the graph to the Loch Ness and Loch Ness words from your plate or or printed some function, it's appointment plus an operator convergence.",
            "No, it's a pointwise.",
            "Convergence is the statement is for a given point X."
        ],
        [
            "So let the suppression converge.",
            "OK, and regarding these convergence, do we have in rates or it's?",
            "And now it's it's an aesthetic result.",
            "Quite see you, as you said, was controlling how much you believe in, yeah?",
            "Yeah.",
            "Came into the actual algorithm.",
            "OK.",
            "It yeah, OK.",
            "The."
        ],
        [
            "It yeah.",
            "I use a normalized kernels, so I'm not using the usual similarity.",
            "I use this similarity.",
            "And the Lambda here the relation between S and Lambda is like S equal 2 * 1 minus Lambda.",
            "But here is the relation.",
            "Yeah.",
            "Yeah, exactly.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is anybody Bethany?",
                    "label": 0
                },
                {
                    "sent": "Will talk about active learning and comfortable.",
                    "label": 1
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So work I will present joint works with several coffers.",
                    "label": 1
                },
                {
                    "sent": "My chest island would even look spoke for the theoretical part on graph Laplacian or distribution hooker even and John Ponds for support on image segmentation.",
                    "label": 0
                },
                {
                    "sent": "And for the pattern Interactive image retrieval, it has been at work with Patrick at Azure and OK then, and it seems to be.",
                    "label": 0
                },
                {
                    "sent": "In a semi supervised.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ning and.",
                    "label": 0
                },
                {
                    "sent": "And in constructive learning we have we observed many more input points than corresponding output points, and we want to.",
                    "label": 0
                },
                {
                    "sent": "Use the information conveyed by the extra input points to define data dependent concept space that improve how a prediction accuracy.",
                    "label": 0
                },
                {
                    "sent": "In this presentation I will focus on one way of using these extra input points.",
                    "label": 0
                },
                {
                    "sent": "This way is based on the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "So in the first part I will mainly talk on the property of the graph repression and how can we use it.",
                    "label": 0
                },
                {
                    "sent": "And then I will turn to two applications to computer vision problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I believe you all know this.",
                    "label": 0
                },
                {
                    "sent": "The Standard learning task in machine learning machine learning is well in supervised machine learning.",
                    "label": 0
                },
                {
                    "sent": "Is inductive learning in window in which you observe an input, output pairs and the question is what is the output associated to a new input.",
                    "label": 1
                },
                {
                    "sent": "Any new input?",
                    "label": 0
                },
                {
                    "sent": "In terms of active learning, you also observed unlabeled data points.",
                    "label": 0
                },
                {
                    "sent": "And the question is just what are the levels of these extra input points and semi supervised learning is something in between.",
                    "label": 1
                },
                {
                    "sent": "Where is the observation are the same as in constructive learning, you have unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "But the question is what is there put associated to any new input?",
                    "label": 0
                },
                {
                    "sent": "Of course, if you have.",
                    "label": 0
                },
                {
                    "sent": "A semi supervised learner.",
                    "label": 0
                },
                {
                    "sent": "You can use it to solve this program and if you have a transductive learner you can use it to solve this problem just by.",
                    "label": 0
                },
                {
                    "sent": "If you are asked the prediction for a new input small X, you add it here and you use your transductive learner on all your points.",
                    "label": 0
                },
                {
                    "sent": "All your observed points plus X.",
                    "label": 0
                },
                {
                    "sent": "But usually the algorithm, usually you don't want to do this.",
                    "label": 0
                },
                {
                    "sent": "Because it will cost you.",
                    "label": 0
                },
                {
                    "sent": "Offensive transitive learning learner textual.",
                    "label": 0
                },
                {
                    "sent": "A lot of time to compute, so you don't want to use this type of idea.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so key assumption, which has been very popular lately.",
                    "label": 0
                },
                {
                    "sent": "And so the reason of this assumption is, is not technical here.",
                    "label": 0
                },
                {
                    "sent": "It's because it really correspond at what you what you observe on real data.",
                    "label": 0
                },
                {
                    "sent": "So this key assumption is that the decision boundary in so I'm talking about constructive classification appears in the low density region of the input space.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let me illustrate this assumption.",
                    "label": 0
                },
                {
                    "sent": "So consider this data set.",
                    "label": 0
                },
                {
                    "sent": "You have label points by blue or red.",
                    "label": 0
                },
                {
                    "sent": "An unlabeled data points which are Gray, so this is the input space.",
                    "label": 0
                },
                {
                    "sent": "If you.",
                    "label": 0
                },
                {
                    "sent": "According to this assumption, the boundary between the blue class and the red class should be something like this.",
                    "label": 0
                },
                {
                    "sent": "Other assumption without help Windows is providing context like having talked about in the morning.",
                    "label": 0
                },
                {
                    "sent": "That would bring assumption.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah yeah but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But maybe this is.",
                    "label": 0
                },
                {
                    "sent": "I would say this is the most prominent one as well.",
                    "label": 0
                },
                {
                    "sent": "OK, you might disagree.",
                    "label": 0
                },
                {
                    "sent": "And OK, if you use an inductive learner, you see that it won't work because you will drop for get about all your unlabeled points.",
                    "label": 0
                },
                {
                    "sent": "You will consider only these points and then your inductive learner usually ends out with this kind of separator between the classes.",
                    "label": 0
                },
                {
                    "sent": "And you see that it won't classify the points subject it satisfies this key assumption.",
                    "label": 0
                },
                {
                    "sent": "This popular assumption.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An his Tokyo is about.",
                    "label": 0
                },
                {
                    "sent": "One way of designing an algorithm using this key assumption.",
                    "label": 0
                },
                {
                    "sent": "An the way is to use graph flip an approximation of the Laplace operator.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and this approximation will depend on density so.",
                    "label": 0
                },
                {
                    "sent": "I consider throughout this presentation that the input points are in a some Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "And there is a density.",
                    "label": 0
                },
                {
                    "sent": "Of the input points distribution with respect.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "To the lab measure, if you are considering that your input points is the support of the of the distribution of your input is also early in space, but in a more general setting, if you consider that your input points are supported on a manifold, you just.",
                    "label": 0
                },
                {
                    "sent": "We're just assuming that there is a density of the distribution with respect to the natural measure on this manifold.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me recall briefly what is the Laplace operator.",
                    "label": 0
                },
                {
                    "sent": "Well, for a function defined on R and taking real valued, this is just the repression of a function, it's just a second derivative.",
                    "label": 0
                },
                {
                    "sent": "When you consider a function from a Euclidean space to R, then it is just the sum of the second derivatives with respect to each of the component.",
                    "label": 0
                },
                {
                    "sent": "And the generalization of this formula to a manifold case just writes by Laplacian of the function it just divergents of the gradient of the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "But here.",
                    "label": 0
                },
                {
                    "sent": "We don't have only a manifold, we have probability supported by this manifold, so we really want we really use an operator which takes into account this this distribution.",
                    "label": 0
                },
                {
                    "sent": "So the weighted lap clash Beltrami operator is just a generalization of the universe.",
                    "label": 0
                },
                {
                    "sent": "So usually passive greater.",
                    "label": 0
                },
                {
                    "sent": "That takes into account the density in the following ways, repression.",
                    "label": 0
                },
                {
                    "sent": "So we have a parameter S which is real and celebration the estimated oppression.",
                    "label": 0
                },
                {
                    "sent": "Make this density.",
                    "label": 0
                },
                {
                    "sent": "Of the input appear in this way, 1 / P power S times the divergent of people were S gradient of F. And this thing that you should notice is that for S = 0, then the estimated Laplace operator, it's just the usual Laplace operator.",
                    "label": 0
                },
                {
                    "sent": "An more importantly, it's this escalated Laplace praytor satisfy this key property, which.",
                    "label": 0
                },
                {
                    "sent": "Which you can see as the dual well as an integration by parts formula.",
                    "label": 0
                },
                {
                    "sent": "Or do jewelry quality.",
                    "label": 0
                },
                {
                    "sent": "It says that for any function with compact support on the manifold integral of F times the application of a liver function with respect to people, SDV is equal to minus integral of the product between the gradient of the two function you are considering or still with respect to the measure people SDV.",
                    "label": 0
                },
                {
                    "sent": "And the problem is the problem.",
                    "label": 0
                },
                {
                    "sent": "We will talk in the first time is given a function.",
                    "label": 0
                },
                {
                    "sent": "How do you approximate?",
                    "label": 0
                },
                {
                    "sent": "So the question of this function.",
                    "label": 0
                },
                {
                    "sent": "At some point, X by only using the values of F at at at points which has been sampled on the density an X.",
                    "label": 1
                },
                {
                    "sent": "So if you don't like manifold, you can also.",
                    "label": 0
                },
                {
                    "sent": "Think of this.",
                    "label": 0
                },
                {
                    "sent": "You can take is a manifold as being all your occlusion space and it will not change anything and you can already and this problem can be also sought in this situation when the space has dimension D = 1, it's still not that trivial to find.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "OK Anne, sorry.",
                    "label": 0
                },
                {
                    "sent": "So wasting this program it says it's because it has many application for clustering, because when you have an estimation of the operation operator you can buy.",
                    "label": 1
                },
                {
                    "sent": "Looking at the first again, Victor, do clustering spectral clustering you can do will see also dimensionality reduction by looking at the 1st.",
                    "label": 1
                },
                {
                    "sent": "Again, victors.",
                    "label": 0
                },
                {
                    "sent": "And what we will see here is how to use it in transductive learning.",
                    "label": 1
                },
                {
                    "sent": "And many people has worked with this setting.",
                    "label": 0
                },
                {
                    "sent": "Starting with the.",
                    "label": 0
                },
                {
                    "sent": "Belkin and Yogi.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do you build an approximation of solar plus operator you start?",
                    "label": 0
                },
                {
                    "sent": "By building a neighborhood graph.",
                    "label": 0
                },
                {
                    "sent": "So for building it, it's a weighted graph.",
                    "label": 0
                },
                {
                    "sent": "For building it you use a kernel.",
                    "label": 0
                },
                {
                    "sent": "That is just a function which looks like this.",
                    "label": 0
                },
                {
                    "sent": "So it is essentially it's positive and it's zero outside the compact set and this gives you a way of.",
                    "label": 0
                },
                {
                    "sent": "Measuring similarity between two input points.",
                    "label": 0
                },
                {
                    "sent": "So you see that.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "By this definition, the similarity will be 0 if the distance between the two input points is greater than H. Then you can.",
                    "label": 0
                },
                {
                    "sent": "Then we will use the degree well this function, which is the sum of the similarity of XI with all other points to define a new similarity metric.",
                    "label": 0
                },
                {
                    "sent": "So up to now, I think I've been saying quite standard things.",
                    "label": 0
                },
                {
                    "sent": "Here is the non standard part.",
                    "label": 0
                },
                {
                    "sent": "Instead of using your initial similarity, what almost everybody do?",
                    "label": 0
                },
                {
                    "sent": "Do you well when using graphical oppression techniques, you normalize it by degree.",
                    "label": 0
                },
                {
                    "sent": "At some power, so Lambda will be a parameter here.",
                    "label": 0
                },
                {
                    "sent": "And this parameter will be linked to the S parameter of the estimated Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And OK, so what's this normalization is doing?",
                    "label": 0
                },
                {
                    "sent": "It is quite well known that up to a constant, this sum approximates the density the density.",
                    "label": 0
                },
                {
                    "sent": "At Point XI.",
                    "label": 0
                },
                {
                    "sent": "So here we are just normalizing by like.",
                    "label": 0
                },
                {
                    "sent": "To density.",
                    "label": 0
                },
                {
                    "sent": "Times didn't.",
                    "label": 0
                },
                {
                    "sent": "Is density X XI times density I, XJ power Lambda, and since XI and XJ will be closed.",
                    "label": 0
                },
                {
                    "sent": "This is basically the density at XI power to Lambda, so this is one way of things this normalization.",
                    "label": 0
                },
                {
                    "sent": "And the neighborhood graph is just the graph in which the vertices are the input points you observe.",
                    "label": 0
                },
                {
                    "sent": "The edges.",
                    "label": 0
                },
                {
                    "sent": "Also, a pair of points such as the similarity is is positive.",
                    "label": 0
                },
                {
                    "sent": "And the weight on these edges are given by this normalized similarity measure.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is the graph you can end up with if you have this curved manifold.",
                    "label": 1
                },
                {
                    "sent": "And you notice that for the Bandwidth H that has been taken, you have something some unwanted edges which has appeared which connect points which are far in the distance.",
                    "label": 0
                },
                {
                    "sent": "So the choice of H is always crucial when you have this kind of.",
                    "label": 0
                },
                {
                    "sent": "A formula over form of the manifold.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now if you want to approximate suppression of a function at a given point.",
                    "label": 0
                },
                {
                    "sent": "You need to add this point.",
                    "label": 0
                },
                {
                    "sent": "Well we will consider an extended neighborhood graph in which this point has to be is added.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We will use this degree function so this degree function takes into account the normalized kernel.",
                    "label": 1
                },
                {
                    "sent": "And the averaging operator is defined as you are.",
                    "label": 1
                },
                {
                    "sent": "Everything F. With the weights with this weight and I recall that the weight here is equal to 0 if actually is at a distance greater than H from X.",
                    "label": 0
                },
                {
                    "sent": "And OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, as you will see I will do it, but I prefer to introduce this this operators and refers to normalized.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this slide introduces first random work graph location that appears in the Lake Shelter, and indeed I am normalizing by 1 / D such that you recover.",
                    "label": 0
                },
                {
                    "sent": "Something which looked like to the nether Ha Watson estimators at the Shabba has to in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What you see is that the repetition of the function F at point X is up to a constant F of X minus.",
                    "label": 0
                },
                {
                    "sent": "The average of F. In a neighborhood of X and this is really what.",
                    "label": 1
                },
                {
                    "sent": "People are doing in dimension one.",
                    "label": 0
                },
                {
                    "sent": "If you want to estimate this.",
                    "label": 0
                },
                {
                    "sent": "So second derivative at point X.",
                    "label": 0
                },
                {
                    "sent": "You you can do it by this usual formula and you can rewrite this formula so subject you make appear the value of the function at point X and the average of the function at nearby points.",
                    "label": 1
                },
                {
                    "sent": "Of course, in my setting you cannot do this becausw well.",
                    "label": 0
                },
                {
                    "sent": "You have no choice of the points at which you know the function.",
                    "label": 1
                },
                {
                    "sent": "OK, and why is it called the random walk?",
                    "label": 0
                },
                {
                    "sent": "Replication it because it is very much linked with the expectation of the function.",
                    "label": 0
                },
                {
                    "sent": "Of one step walk.",
                    "label": 0
                },
                {
                    "sent": "Using this transition matrix, so from the similarity you can build this transition matrix which gives you the IG element of these metrics, gives you the probability of going to J starting from I.",
                    "label": 0
                },
                {
                    "sent": "And if you start your own book at Point X and do just one step, which is wrong.",
                    "label": 0
                },
                {
                    "sent": "Just one step with this from the move and you do not value one the value.",
                    "label": 0
                },
                {
                    "sent": "Where you are after one step also random work expectation of FW One.",
                    "label": 0
                },
                {
                    "sent": "Is exactly this AF over the so this explain the name of this graph location so this is 1 graph location that has been used, but several other ones has appeared.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we have put all of them, so this is the previous one.",
                    "label": 0
                },
                {
                    "sent": "The unnormalized graph oppression differs by supposition of this degree function.",
                    "label": 0
                },
                {
                    "sent": "And the normalized graph oppression.",
                    "label": 0
                },
                {
                    "sent": "Is something which is, which seems unnatural at first view, but which has nice property as we will see.",
                    "label": 0
                },
                {
                    "sent": "Instead of applying the average operator directly on F, you are playing.",
                    "label": 0
                },
                {
                    "sent": "At on F / sqrt T so people who has used the graph oppression user.",
                    "label": 0
                },
                {
                    "sent": "Usually states the definition by matrices, so you should see this as identity matrix minus D -- 1 * W. The matrix of similarity.",
                    "label": 0
                },
                {
                    "sent": "Here it's just D minus the value, and here it is D -- 1/2 * D -- W * D -- 1/2.",
                    "label": 0
                },
                {
                    "sent": "So there are free graph liberation and the the main reason.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that this three graph Laplacian indeed converges, indeed converge with something which is related to graph oppression but.",
                    "label": 0
                },
                {
                    "sent": "They are converging to different limits.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically the result is provided that the bandwidth is small enough, so this is the usual thing.",
                    "label": 0
                },
                {
                    "sent": "Small enough, but not too small E in order to have really an average.",
                    "label": 0
                },
                {
                    "sent": "Typically H equal login is working then the the this.",
                    "label": 0
                },
                {
                    "sent": "Graph oppressions converges.",
                    "label": 0
                },
                {
                    "sent": "To something in which the flash, the escalated repression appear.",
                    "label": 0
                },
                {
                    "sent": "And S is 2 * 1 minus Lambda.",
                    "label": 0
                },
                {
                    "sent": "Lambda is the parameter.",
                    "label": 0
                },
                {
                    "sent": "That appeared in the normalization of your similarity measure.",
                    "label": 0
                },
                {
                    "sent": "And what you can see is that if Lambda is equal to 1/2, all the limits are the same.",
                    "label": 0
                },
                {
                    "sent": "So I'm getting rid of the constant because for.",
                    "label": 0
                },
                {
                    "sent": "For the machine learning techniques that are using graph repressions, these constants are not important.",
                    "label": 0
                },
                {
                    "sent": "OK, so for Lambda equal 1 have.",
                    "label": 0
                },
                {
                    "sent": "The limit is just this one an for uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "You note that all the limits are also the same, but in other cases these limits are different.",
                    "label": 0
                },
                {
                    "sent": "So when you are using graph pression metrics, you should really wonder.",
                    "label": 0
                },
                {
                    "sent": "Watch what you are really doing.",
                    "label": 0
                },
                {
                    "sent": "And this is maybe a way of choosing the right one for your problem.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that I have already said this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let us come back to the initial problem, transductive classification using the graph Laplacian.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "How do we incorporate this key assumption that the input density addition Asian bhandary is more well a simpler and I would say natural thing to do is well.",
                    "label": 0
                },
                {
                    "sent": "You assume that the levels are plus 1 -- 1, for instance.",
                    "label": 0
                },
                {
                    "sent": "You will find look for regular function smooth function taking real value subject on your input points.",
                    "label": 0
                },
                {
                    "sent": "Your function match outputs zero observed outputs.",
                    "label": 0
                },
                {
                    "sent": "And you ask also that the function is smooth and also more smooth as the density is high.",
                    "label": 0
                },
                {
                    "sent": "And this is natural in view of this assumption, you want F. You want your function.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "To change well, it is to have a high gradient only when when the density is very small.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is, this optimization problem is, well, it's not a very nice optimization program because you have to optimize over an infinite set of function defined on manifold and you don't know the manifold.",
                    "label": 0
                },
                {
                    "sent": "You don't know the density, but Fortunately you can rewrite this term as by the integration by parts formula if.",
                    "label": 0
                },
                {
                    "sent": "I mentioned before as the integral of F times elaboration of F with respect to people were SDV.",
                    "label": 0
                },
                {
                    "sent": "And from the law of large numbers.",
                    "label": 0
                },
                {
                    "sent": "If you consider this some.",
                    "label": 0
                },
                {
                    "sent": "Which make appears a function F times lation times.",
                    "label": 0
                },
                {
                    "sent": "People were S -- 1.",
                    "label": 0
                },
                {
                    "sent": "Well, this this rage converge to the expectation of this function and this expectation of this function is exactly this one.",
                    "label": 0
                },
                {
                    "sent": "So yes, minus one coming from the fact that distribution is PDV.",
                    "label": 0
                },
                {
                    "sent": "OK, so and the nice part of this formulation.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "This problem makes appear only value of the function and of its repression at the points that you have observed.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this algorithm there are two parameters.",
                    "label": 0
                },
                {
                    "sent": "Essentially, C is a how much you believe that the output you have been given are correct.",
                    "label": 0
                },
                {
                    "sent": "And S is how much you believe in the assumption.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now the last thing you have to do is.",
                    "label": 0
                },
                {
                    "sent": "To approach to approximate this quantity and what we have seen be.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "4 is that.",
                    "label": 0
                },
                {
                    "sent": "You know how to do this, it's exactly the limit of the unnormalized graph Laplacian.",
                    "label": 0
                },
                {
                    "sent": "So you can just replace the Red Temple which was in the previous slide by this and normalized oppression.",
                    "label": 0
                },
                {
                    "sent": "And your.",
                    "label": 0
                },
                {
                    "sent": "Now you're a normalized preparation.",
                    "label": 0
                },
                {
                    "sent": "You can when it is taken at the points of your of your training data, it is just D -- W * F, where W is a similarity matrix.",
                    "label": 0
                },
                {
                    "sent": "D is the.",
                    "label": 0
                },
                {
                    "sent": "Diagonal matrix of the degrees and F are the predicted output vectors.",
                    "label": 1
                },
                {
                    "sent": "And it contains the training, while the label points of your observation and also the unlabeled points.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The previous problem, this one can be written in.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Metrics form and it makes clearly appears.",
                    "label": 0
                },
                {
                    "sent": "That you have a problem.",
                    "label": 0
                },
                {
                    "sent": "Your optimization program, which was on a set of function, is now just a program on.",
                    "label": 0
                },
                {
                    "sent": "And the vectors on our end plus T. And this is a quadratic problem.",
                    "label": 0
                },
                {
                    "sent": "So by differentiation you have that the solution satisfies this linear equation.",
                    "label": 0
                },
                {
                    "sent": "And therefore it's tractable.",
                    "label": 0
                },
                {
                    "sent": "And it is also more tractable as this system is very sparse, because this similarity, this matrix is diagonal and CC is also a diagonal matrix and the value is the similarity metrics, so it's usually a sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "And phone for the unlabeled input XI.",
                    "label": 1
                },
                {
                    "sent": "The algorithm.",
                    "label": 0
                },
                {
                    "sent": "I'll put the sign of F of I.",
                    "label": 0
                },
                {
                    "sent": "So the F code in it?",
                    "label": 0
                },
                {
                    "sent": "If so, you end up with an algorithm which has three parameters that is.",
                    "label": 0
                },
                {
                    "sent": "So Bandwidth H, the constancy represented representing your belief in the labeled points that the label are correct and South, which is the belief in your assumption.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's turn now to 1st application of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "That is, image segmentation.",
                    "label": 0
                },
                {
                    "sent": "Image segmentation is a problem of partitioning numeric image into meaningful regions and it is really a key task in computer vision.",
                    "label": 1
                },
                {
                    "sent": "But also an important problem, because for any given image there are several ways to segment it, and it really depends on what your user wants.",
                    "label": 0
                },
                {
                    "sent": "The result really depends on its desire.",
                    "label": 0
                },
                {
                    "sent": "So one way of using it is to use super user supplied.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seeds, so for instance, if you want to segment.",
                    "label": 0
                },
                {
                    "sent": "This image without the blue and red strokes.",
                    "label": 0
                },
                {
                    "sent": "So you ask the user to put strokes on it, saying well the objective of interest is, well, this this pixels in blue are in the same region and the pixels in red should be in the same region.",
                    "label": 0
                },
                {
                    "sent": "And what you want to recover is this kind of image and this is typically transductive learning problem.",
                    "label": 1
                },
                {
                    "sent": "So the inputs here are also pixels, so you have labeled pixels.",
                    "label": 0
                },
                {
                    "sent": "And you want to predict the label of any other pixel of the image.",
                    "label": 0
                },
                {
                    "sent": "So this is a way of.",
                    "label": 1
                },
                {
                    "sent": "Of solving this task that well in computer vision, there are many, many ways of solving this program.",
                    "label": 0
                },
                {
                    "sent": "It can be based on level sets, method.",
                    "label": 0
                },
                {
                    "sent": "Or shield, is it computations and there are also other graph based method like minecart watershed.",
                    "label": 0
                },
                {
                    "sent": "And also one which comes from.",
                    "label": 0
                },
                {
                    "sent": "From similarity with electric.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Potential.",
                    "label": 0
                },
                {
                    "sent": "OK, so how is tradition was to apply the algorithm that we have seen in the first part?",
                    "label": 0
                },
                {
                    "sent": "So the parameters then.",
                    "label": 0
                },
                {
                    "sent": "You have more parameters because you're in computer vision.",
                    "label": 1
                },
                {
                    "sent": "You have always plenty of parameters.",
                    "label": 0
                },
                {
                    "sent": "Like the well you have.",
                    "label": 0
                },
                {
                    "sent": "So the Pixel is there is not obvious things.",
                    "label": 0
                },
                {
                    "sent": "Here it is the representation of the pixel.",
                    "label": 1
                },
                {
                    "sent": "You can represent it by just its color, but it's better to represent it by both the geometric position of the pixel in the image.",
                    "label": 0
                },
                {
                    "sent": "And some kind of color vectors which represents the texture around your pixel.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here we have represented by the these colors on a Patch of squares Patch around the pixel an just these geometric position and the tradeoff between the color information and the geometric information is just set up by using Sigma J and Sigma Si so Sigma J. Sigma Chi and Sigma C Just bond with you so this this was my H in the first part of the talk.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and then then we follow the schemes that has been described previously.",
                    "label": 0
                },
                {
                    "sent": "So difference here is that you are you assume.",
                    "label": 0
                },
                {
                    "sent": "That's the user has leveled correctly.",
                    "label": 0
                },
                {
                    "sent": "Also points that he has leveled so C in the.",
                    "label": 0
                },
                {
                    "sent": "Previous slide was is now equal to Infinity because we've forced the output function.",
                    "label": 0
                },
                {
                    "sent": "To correspond to the level that has been given.",
                    "label": 0
                },
                {
                    "sent": "So you you still end up with a linear system and the prediction of the four pixel J is just the sign of.",
                    "label": 0
                },
                {
                    "sent": "FG.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There was ever that there have been other approaches.",
                    "label": 0
                },
                {
                    "sent": "OK, there are approaches which are quite close to ours.",
                    "label": 0
                },
                {
                    "sent": "Even if there were stated in a different way.",
                    "label": 0
                },
                {
                    "sent": "First, if you use you think of graph cut method which has, which is a very popular method in this field.",
                    "label": 0
                },
                {
                    "sent": "Well, it's doing exactly the same minimization, except that instead of looking for real valued, you look for only minus 1 + 1 value.",
                    "label": 0
                },
                {
                    "sent": "And you consider also the normalization Lambda equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So normalization or in the similarity Lambda equals zero correspond to equal 2.",
                    "label": 0
                },
                {
                    "sent": "If you normalize, Kurt is a method which also use this normalized mattress graph repression metrics.",
                    "label": 0
                },
                {
                    "sent": "But well, it's it's not clear when you look at the paper, but it correspond to ethical one.",
                    "label": 0
                },
                {
                    "sent": "Gwen and Q.",
                    "label": 0
                },
                {
                    "sent": "Stated the programming in a different way, but you can check that it corresponds to this problem.",
                    "label": 0
                },
                {
                    "sent": "And if you use the.",
                    "label": 0
                },
                {
                    "sent": "As a fair tickle analysis of the first part, you see that the additive regularization term is not at all this one.",
                    "label": 0
                },
                {
                    "sent": "It's this one, well you can see it's not.",
                    "label": 0
                },
                {
                    "sent": "Natural regularization term.",
                    "label": 0
                },
                {
                    "sent": "Because the function F. Can be linear, so any function F which is linear has paid nothing for moving from class minus one to class plus one, so this is a really an unwanted effect.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the greedy algorithm, in fact it is the same as ours for S equal 2.",
                    "label": 0
                },
                {
                    "sent": "So yeah, one nice thing about this convergence result.",
                    "label": 0
                },
                {
                    "sent": "It is that it provides a way of.",
                    "label": 0
                },
                {
                    "sent": "New interpretation of existing algorithm and maybe some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when you see that.",
                    "label": 0
                },
                {
                    "sent": "You are recognizing by this formula form it's.",
                    "label": 0
                },
                {
                    "sent": "I would say it's more explicit.",
                    "label": 0
                },
                {
                    "sent": "It tells me more about how they are.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Either it's motivated by graph theoretical electrical potential.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Well, she write it like this and she said OK, the paper is called something like Honda MC Segmentation.",
                    "label": 0
                },
                {
                    "sent": "But if you look at it, it's not really a random look.",
                    "label": 0
                },
                {
                    "sent": "It just selling argument and he really says in his paper.",
                    "label": 0
                },
                {
                    "sent": "I've been inspired by this theory and he really yeah do this like this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think it is connected with the posters which has been one of your poster and also the other clusters.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here's some experimental results, so unfortunately in this field there is a.",
                    "label": 1
                },
                {
                    "sent": "There is not much the database which correspond to this situation when the user provides only small information like this strokes.",
                    "label": 0
                },
                {
                    "sent": "So database on which.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we can compare each other, is this one?",
                    "label": 0
                },
                {
                    "sent": "So here is the image you want to segment.",
                    "label": 0
                },
                {
                    "sent": "Here is the information you get, so the information is that white pixels should be in the foreground region and this.",
                    "label": 0
                },
                {
                    "sent": "Hard Gray pixels should be in the other class, So what?",
                    "label": 0
                },
                {
                    "sent": "You just what you are just looking is to locate the boundary in this region so it's a much easier task.",
                    "label": 0
                },
                {
                    "sent": "And people have come with different algorithm.",
                    "label": 0
                },
                {
                    "sent": "But here we obtained this data of the art results which is four 5.4% of the pixel of this green region here misclassified.",
                    "label": 1
                },
                {
                    "sent": "And in fact, to be honest.",
                    "label": 0
                },
                {
                    "sent": "The algorithm of low gravity, which correspond to S equal 2, is also doing 5.4, and the improvement here is statistically significant but with a P value between 5% and 10%, so not that not that signif.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I will skip support on interactive image search, but just to describe the principle and to see why it can be an application for this constructive learning with graph passion.",
                    "label": 0
                },
                {
                    "sent": "So principle of interactive image serves is that you have a database and that you were under user which looks for photo for images in this database.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you display some images.",
                    "label": 0
                },
                {
                    "sent": "So each point here is an image or supposed.",
                    "label": 0
                },
                {
                    "sent": "And you display some images.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The user can label them, and from this point switch has been leveled.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "You do.",
                    "label": 0
                },
                {
                    "sent": "You decide.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the next display in order to rapidly focus on the class of interest?",
                    "label": 0
                },
                {
                    "sent": "So I will skip some method icing.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think to conclude.",
                    "label": 0
                },
                {
                    "sent": "OK. What I've been presenting is a modification of the way of using graph preparation for transductive learning.",
                    "label": 0
                },
                {
                    "sent": "So main modification is this normalization because when you normalize differently correspond to believing more or less in your assumption that the decision boundary between classes.",
                    "label": 0
                },
                {
                    "sent": "Is in low low density region of your input space.",
                    "label": 0
                },
                {
                    "sent": "And this can be applied to image segmentation.",
                    "label": 0
                },
                {
                    "sent": "And the main point is really when if you use a graph Laplacian matrix, you should.",
                    "label": 1
                },
                {
                    "sent": "Knows that there are several possibilities, so a normalized random walk or normalized version of it.",
                    "label": 0
                },
                {
                    "sent": "And you should also be aware that we know to which.",
                    "label": 0
                },
                {
                    "sent": "Continuous quantity converge and this can be useful to interpret your algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "To make this assumption that the decision boundaries go through low decorations and I'm thinking about the image example, but it's really hard.",
                    "label": 0
                },
                {
                    "sent": "Yes, I features like.",
                    "label": 0
                },
                {
                    "sent": "Features like you're collecting colors in a Patch around the pizza, and then you're moving the pixel around that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK then.",
                    "label": 0
                },
                {
                    "sent": "Alright, so yeah I know for.",
                    "label": 0
                },
                {
                    "sent": "For the database on which.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People were fighting to have good segmentation results.",
                    "label": 0
                },
                {
                    "sent": "It's a very particular database in which the color is very important, so for this particular database, the size of the Patch around the pixel was just one, so that is 1 pixel.",
                    "label": 0
                },
                {
                    "sent": "So we're considering just one color and the geometric position.",
                    "label": 0
                },
                {
                    "sent": "So maybe in this case it's more at least it's more natural, but I don't understand it.",
                    "label": 0
                },
                {
                    "sent": "Even if you use patches, there will be a discontinuity.",
                    "label": 0
                },
                {
                    "sent": "Pretty like if the patches it's a bit messy in the transitions that OK, I agree.",
                    "label": 0
                },
                {
                    "sent": "But yeah, OK, yeah we are not thinking of very big patches anyway yeah.",
                    "label": 0
                },
                {
                    "sent": "Have you tried to like?",
                    "label": 0
                },
                {
                    "sent": "Think about the matter like many people like you're representing it.",
                    "label": 0
                },
                {
                    "sent": "We were in 5D, if you take.",
                    "label": 0
                },
                {
                    "sent": "The size of the Patch equal to 1, so just one pixel.",
                    "label": 0
                },
                {
                    "sent": "We are in 5D.",
                    "label": 0
                },
                {
                    "sent": "Our input spaces in 5D.",
                    "label": 0
                },
                {
                    "sent": "And it's a bit complicated to represent things, so we have tried at some points, but it was not.",
                    "label": 0
                },
                {
                    "sent": "There not so clear.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Question so.",
                    "label": 0
                },
                {
                    "sent": "It was sorry.",
                    "label": 0
                },
                {
                    "sent": "My sense is that the graph methods are best suited when data is in a relatively low dimensional space or a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Yeah, is that your friends like friends?",
                    "label": 0
                },
                {
                    "sent": "If I had say two Gaussian saying high dimensional space may be staring deviation one or something separated by some distance, like yeah there would be a low density region between yeah, but two points in the same Gaussian would be like Ruthie apartment.",
                    "label": 0
                },
                {
                    "sent": "There's a discipline, two points in the same side.",
                    "label": 0
                },
                {
                    "sent": "Would be comparable.",
                    "label": 0
                },
                {
                    "sent": "The distance between points in different size if you could, 'cause the dimension is so high you know it's like everybody is far from everybody else.",
                    "label": 0
                },
                {
                    "sent": "Is your sense that kind of a graph methods are better when it's low dimension or this thing things seem to?",
                    "label": 0
                },
                {
                    "sent": "Work well.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I don't know, it's not clear to me.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know the answer of the question.",
                    "label": 0
                },
                {
                    "sent": "I mean if you ask it, it's because you don't believe in graph repression well in graph based methods for high dimensional purpose.",
                    "label": 0
                },
                {
                    "sent": "The bounds occurrences were manifold, can exponentially with dimensions available.",
                    "label": 0
                },
                {
                    "sent": "So if you had data that wasn't necessary.",
                    "label": 0
                },
                {
                    "sent": "But that's the dimension of money for another.",
                    "label": 0
                },
                {
                    "sent": "Yeah, like if you had like the two high dimensional Gaussian, well separated, you could still have a low density region between them.",
                    "label": 0
                },
                {
                    "sent": "It's just hard for me to think how you would seem like if you can make you know that the parameter for the graph too small.",
                    "label": 0
                },
                {
                    "sent": "Nobody's connected if you make it too large, everybody's connected, so it was just wondering if you had.",
                    "label": 0
                },
                {
                    "sent": "Thoughts on either?",
                    "label": 0
                },
                {
                    "sent": "No, I have nobody else.",
                    "label": 0
                },
                {
                    "sent": "Maybe there was.",
                    "label": 0
                },
                {
                    "sent": "So turns on the detailed study of normalization for raffle caution methods and hammer all the details, but one of the things that were we looked at was.",
                    "label": 0
                },
                {
                    "sent": "Rather than normalizing in the primal law.",
                    "label": 0
                },
                {
                    "sent": "And he.",
                    "label": 0
                },
                {
                    "sent": "This category normalized currently.",
                    "label": 0
                },
                {
                    "sent": "How do you spell Stanley normally?",
                    "label": 0
                },
                {
                    "sent": "Did you forget?",
                    "label": 0
                },
                {
                    "sent": "No, I'm not aware of this.",
                    "label": 0
                },
                {
                    "sent": "This work it was.",
                    "label": 0
                },
                {
                    "sent": "It's normalizing by the degree to some power.",
                    "label": 0
                },
                {
                    "sent": "Sidered that, but he also he considered a number of methods, and so the other thing is so like.",
                    "label": 0
                },
                {
                    "sent": "No matter which released in history was able to.",
                    "label": 0
                },
                {
                    "sent": "I think for the best pounds is a technical question.",
                    "label": 0
                },
                {
                    "sent": "You find the kernel little caution and then you divide the kernel then by let's say you just had had had a matrix by my column, right?",
                    "label": 0
                },
                {
                    "sent": "Alex alive by the two columns.",
                    "label": 0
                },
                {
                    "sent": "That's why so that the norm of connected through as one normal the column.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow's morning yeah.",
                    "label": 0
                },
                {
                    "sent": "Well I should look at it definitely.",
                    "label": 0
                },
                {
                    "sent": "Once the convergence of the graph Laplacian operated on a function to the graph to the Loch Ness and Loch Ness words from your plate or or printed some function, it's appointment plus an operator convergence.",
                    "label": 1
                },
                {
                    "sent": "No, it's a pointwise.",
                    "label": 0
                },
                {
                    "sent": "Convergence is the statement is for a given point X.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let the suppression converge.",
                    "label": 0
                },
                {
                    "sent": "OK, and regarding these convergence, do we have in rates or it's?",
                    "label": 0
                },
                {
                    "sent": "And now it's it's an aesthetic result.",
                    "label": 0
                },
                {
                    "sent": "Quite see you, as you said, was controlling how much you believe in, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Came into the actual algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It yeah.",
                    "label": 0
                },
                {
                    "sent": "I use a normalized kernels, so I'm not using the usual similarity.",
                    "label": 0
                },
                {
                    "sent": "I use this similarity.",
                    "label": 0
                },
                {
                    "sent": "And the Lambda here the relation between S and Lambda is like S equal 2 * 1 minus Lambda.",
                    "label": 0
                },
                {
                    "sent": "But here is the relation.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}