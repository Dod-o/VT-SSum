{
    "id": "6rnio5q7xf2mydtapqqqecjrfwug3mhs",
    "title": "On the Convexity of Latent Social Network Inference",
    "info": {
        "author": [
            "Seth A. Myers, Stanford University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Social Media",
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/nips2010_myers_cls/",
    "segmentation": [
        [
            "Hi, I'm Seth and so will be."
        ],
        [
            "Again, by talking about what motivated this work and that's that many real world social networks are difficult to observe directly.",
            "One such example, let's say we have population people interested in the sexual relationship network that connects them.",
            "We can expect that most people aren't going to be upfront about this network, and so determining the edges that connect these people can be quite difficult.",
            "However, in this situation, which there are series of diseases that propagate along these relationships, we can much more easily observed when people become infected by these diseases.",
            "And in doing so, gain insight into the underlying network that connects everybody."
        ],
        [
            "So to kind of articulate probable better, we have an unobservant unobservable social network of influence that connects nodes and we have a diffusive process which we can observe, such as information propagation or disease spread.",
            "And we can observe the infection times of these nodes, and in doing so, or we can observe infection times these nodes and doing so in further network."
        ],
        [
            "So to give an example, we have some simple social network here connected by the edge is shown here.",
            "We started the initial infection designated by red and we know the infection time as designated by towel."
        ],
        [
            "Slowly."
        ],
        [
            "This."
        ],
        [
            "Infection propagates."
        ],
        [
            "The network."
        ],
        [
            "And all we observe, however, is just the infection times of each node, and that's one cast."
        ],
        [
            "Dayton in summer we can."
        ],
        [
            "Have a different cascade?"
        ],
        [
            "Starting a different infection this."
        ],
        [
            "Propagates."
        ],
        [
            "And record those."
        ],
        [
            "Infection times"
        ],
        [
            "So kind of more general examples in which this is applicable for disease spread infection spread from person to person, and we can easily observe when people become infected but not who infected them.",
            "And suddenly in lines of information propagation of viral marketing, people recommend products to each other.",
            "People in turn by these products which we can observe.",
            "But we don't know who influences purchase."
        ],
        [
            "Is in our goal is to infer who infected."
        ],
        [
            "So we're given a set of cascades, and our goal is to infer this network, specifically the adjacency matrix, AR.",
            "This adjacency matrix were allowed to be both directed and weighted, and the weights of these edges and correspond to the probability of the source node affecting the destination node.",
            "So AIJ is the probability that I will infect J and so our approach this problem is going to be as follows.",
            "First, define a probabilistic model that describes how cascades propagate to the network.",
            "Then we're going to build a likelihood function based on observed cascades, and then we're going to maximize likelihood function by solving a series of convex subproblems, and then finally special attention will be needed, will be needed for the case where most social networks are sparse, and so we need to address that specifically.",
            "And just to point out, we kind of two goals here.",
            "One is to infer the actual network structure where the edges are, what nodes are connected, as well as the edge weights or other infection probabilities associated with each edge."
        ],
        [
            "So first we'll define the Cascade model.",
            "We have a simple social network here defined by an adjacency matrix A and that gives us all the edge weights and we start off the initial infection node.",
            "I an then so each one of node eyes outbound neighbors is going to be sampled as to whether not the infection propagate according to the probability corresponding that edge."
        ],
        [
            "So in this particular case, K&J will become infected, but just so happened that Ellen will not be according to sampling, and each of these nodes sampled independently.",
            "And then now that we determine which of the nodes will become infected, we need to associate a new infection time with these nodes.",
            "So we came up with this idea of."
        ],
        [
            "Occupation model, which means that incubation times are distributed by some known density function, W. So if node eyes affected atau I each of its new infections is going to be how I plus some interval sampled independently from this density function."
        ],
        [
            "And so with this model building likely functions are relatively straightforward, forgiving cascade.",
            "See, we know we're going to assume we know each of the nodes, infection times, towel, and then the likelihood of 1st that particular node infects another.",
            "So likelihood of Jay infecting I just likelihood that the infection propagates along edge AJ I times.",
            "The likelihood of that particular incubation time interval since we know when each node is affected, we know what the incubation period had to be in order for the infection, probably along those lines.",
            "And then from there we can take it up another level and say what's the likelihood of a particular node being affected by a given cascade at simply just the likelihood of any of its possible potential inbound neighbors infecting it.",
            "So we basically take the product.",
            "Overall, all nodes are infected previously, and it's probably at least one of those infected it.",
            "And similarly for the case where a note it was not infected or infection times Infinity, I would basically just likelihood that none of these potentially infected neighbors infected it."
        ],
        [
            "So that's basically a likelihood function that's for one cascade, and we put it all together for all observed cascades.",
            "So this first term we're just taking the product across all drive cascades.",
            "This first term is likelihood of all infection events.",
            "All nodes that were infected by cascade and this term is infection of events which there was no infection or just knows not infected by the cascade, and then searching further network, we basically just maximize this function, and so that's just taking the minimum.",
            "The log likelihood function, and then we finally apply the constraints of the edge weights between zero and one 'cause they do represent probabilities.",
            "Now solving this problem has two really big issues with it.",
            "The first one is scalability.",
            "The number of parameters in this problem grows quadratically with the number of nodes in network.",
            "So forgiven network of in nodes there's intense in minus one potential edges, so that makes scaling this problem completely undoable basically.",
            "Additionally, this is in general non convex function and it is indeed played with many local minima, so we can never be sure we have optimality, so we're going to solve this.",
            "These two problems."
        ],
        [
            "Kind of 1 fell swoop.",
            "I'm going to drive a series of convex subproblems that allow us to not only solve attractively guarantee an optimal solution, so we do this first, breakdown the subproblems, giving just a conceptual argument, and then we're going to add parameters and we're basically in turn are sub problems into a series of geometric programs, and then the method for turning geometric program into a convex program as well known.",
            "So follow those steps."
        ],
        [
            "So first for the convex vacation.",
            "Or sorry for breaking down the subproblems we know when all infections occur and we know they occur independently.",
            "So we're trying to maximize likelihood of inodes, infection, or lack of infection.",
            "All it really cares about is when it's inbound neighbors got infected, not how they got infected.",
            "So Furthermore it's infection time, or it's infection.",
            "Likelihood is certainly a product of only it's inbound edges, and it's independent of any other edge in the network.",
            "So that means each likelihood of each node can be solved independently.",
            "Where the parameters are just it's inbound edges, which gives us in sub problems within minus one parameters, so that immediately allows us to scale this problem to much higher or much larger networks."
        ],
        [
            "Alright, and so this new function here represents the modified likelihood function for particular subproblem.",
            "It's really no different, we just we're given node.",
            "I basically just the product overall, all cascades in which it was infected, and then we hear is likelihood of all cascades which it wasn't.",
            "And then the first step in convex ification is related to allow this likelihood of infection to become an independent parameter gamma."
        ],
        [
            "So we plug that in there and then, as you might expect, we need to constrain gamma.",
            "But instead of doing any an equality constraint, we're going to impose an inequality constraint that allows for convex fication.",
            "And Additionally it can be shown that this will always be a binding constraint at the solution."
        ],
        [
            "Alright, from there we imply a few more change of variables.",
            "I won't go into the details here, but basically basically come from the fact we converted into geometric program and then apply a well known variable transformation to get into a convex program.",
            "And this is what we're left with a couple of highlights.",
            "Here are that it's a strong convex, so we're guaranteed optimality and has a nice linear objective function, and then these nonlinear inequality constraints are basically what happens.",
            "This constraint after the transformation, and it can be shown this will always be.",
            "Strictly zero at the solution."
        ],
        [
            "Alright, so as I mentioned before special attention we need for sparsity as because almost all social networks, at least the ones we're inferring, are going to be sparse.",
            "Most people aren't friends with everybody in the network, only as a very small subset of people.",
            "And similarly, the maximum output estimation for any particular edge will almost never be 0.",
            "The only way it can be zero, as if the source node was never infected in the same cascade, at least previously, as the destination node.",
            "Otherwise it happen at least once.",
            "Then will be arbitrarily small, but it will never be 0.",
            "So the obvious thing to do in order to accurately infer now structure is imposed an L1 penalty function, but that doing so ruins the convexity.",
            "After do the variable transformation you get like actually a concave function?",
            "So what we did instead was basically we modify this new concave function only slightly to create a convex function, and it's been shown that like over and over again that it doesn't do sparsity quite well.",
            "So once in the original edge space, this is what the function looks like, so we can offer kind of a conceptual explanation."
        ],
        [
            "As to why this particular function does work well and it comes from basically intuition, all familiar with is where the L1 penalty comes from.",
            "It simply relaxation of the zero norm.",
            "We've actually seen this figure earlier this week, but we have the level curves of the L1 here and it more often times not intersect the linear constraints in the same places.",
            "Zero norm would.",
            "Making this a valid relaxation.",
            "Similarly, if the level curves of our function for solutions close to 0 or sufficiently sparse, we have the same type of behavior where.",
            "The zero norm will often intersect in the same place.",
            "Alright."
        ],
        [
            "So that's basically our method as stated.",
            "So now we want to test it and so we kind of set up a few evaluation metrics to determine how successful this algorithm truly is.",
            "As mentioned before, we're just in kind of two different things, and for a network structure and also inferring infection probabilities of each of each edge.",
            "So that means we basically need to the evaluation metrics, the first one is for determining actual network structure.",
            "We look at the precision recall of basically where we predict edges.",
            "So for any given node pair, we predict whether or not there's an edge there.",
            "Based on whether or not the inferred edge probability is 0 or not, we like the precision.",
            "Recall of that of that method.",
            "Then for the mean square error, we basically have all infer that we take the union of all the edges of which we in Ferd, and then all the actual edges, and we take the mean square over all that and that gives us our how accurate infection probabilities are.",
            "And then we also compared it to a baseline algorithm net if it's only other one, we could find addresses this problem.",
            "In particular, it's an approximation algorithm based on submodular optimization.",
            "And it makes the assumption that all affection probabilities are the same, or but they're all edge weights are identical.",
            "Winston assumption holds.",
            "It's a fantastic Alderman, speedy, tractable, and as well.",
            "But we're going to see how well we do when we relax this assumption and and see where it goes."
        ],
        [
            "So our basically our experiments restriction three tiers.",
            "The first one is we generate synthetic networks with synthetic cascades.",
            "We looked at wide range of network topologies, scale free, Eros, rainy, Chronicle graphs and then we also looked at.",
            "We generate cascade using a bunch of different types of incubation models.",
            "Here's power law, only been shown them any information propagation models follow power law.",
            "In real life.",
            "We also we also looked at exponential.",
            "We looked at why Bolton showing that.",
            "The SARS outbreak in Hong Kong for the Weibull distribution.",
            "Anyway, we basically just generated synthetic cascade and see how we can get closer.",
            "Get ground truth.",
            "So this is 1 particular example for scale free network of 500 nodes.",
            "Here we have the precision and recall like on the next wave recall and precision and you can see the break even point of about .85 which we are extremely pleased with.",
            "Similarly over here we have the mean square error as a function of the number of edges in which we infer.",
            "So obviously these are correlated.",
            "To know more precise graph has less edges and so so.",
            "These plots corresponding that way and in the green line actually represents the exact number of true edges in the network, and so are the blue is all our algorithm and the red is a baseline.",
            "So keep in mind the red is so assuming that all the infection.",
            "Probably just saying we have a uniform distribution.",
            "So."
        ],
        [
            "Next year of experience looked at as real social networks and we again generated artificial cascades across them.",
            "One session network, which have included here, is an email network between approximately 600 people and we assigned infection probabilities based on the volume of emails sent from one person to another.",
            "Again, we generation cascades using parallel propagation and we have a break, even precision, recall break, even point about .95 the mean square error of less around .03.",
            "So again we were quite pleased."
        ],
        [
            "And so the last tiered experiences we looked at real networks with real cascades.",
            "One such network was a product recommendation network of about 275 people, and basically what happens is we have the ground truth of who recommended product, who, who, who.",
            "Then in turn bought the product on that recommendation.",
            "So all we give to the Nepal, we give the algorithm is who bought what product win, not who influenced them and we try to see how well we can infer the actual recommendation network.",
            "We didn't know what the incubation model was, not the ground truth that we observed it to be.",
            "Power law and we had to adjust the parameter accordingly, but even with that inaccuracy we get a break even point of around .75 an which is significantly better than the net 10th and yeah, so we're again quite pleased with that."
        ],
        [
            "So and then the last thing I'll include is the number of cascades needed to do this type of analysis.",
            "It's usually on the order of about to generate in order to get 99% of all edges, probably at least one infection.",
            "Usually it's on the order of twice as many nodes in the network, and so this is again a scale at the same scale.",
            "Free network showed you earlier and we just looked at the accuracy of the break even point as a function of the number of cascades.",
            "So around 1000 we already get a break even point well above .8."
        ],
        [
            "So in summary, we this our algorithm scalable and robust.",
            "I should say that we can infer thousand networks inside of 10 minutes just running on laptop, solving each problem in sequence.",
            "Each subproblem sequence and has applications to Epidemiology, viral marketing, etc.",
            "And some interesting topics of further study could be the case in which we have missing nodes.",
            "Mean there's external influences the network which you can observe directly.",
            "More specialized cascade models that specific domains, methods of handling not know, not knowing, incubation model and also exploring.",
            "Connections to more general graphical models.",
            "Thank you.",
            "If you have time, you have time for a few questions.",
            "Hype.",
            "Hi, your dual problem is maximum entropy problem.",
            "Have you considered the dual problem?",
            "If the convex formulation we while we shopped around for solving convex solvers and anything that usually we found that it has actually create an explosion of variables and dual space.",
            "So we avoided dual solving the problem.",
            "The dual OK but I wanted to understand if you think do and do again you get a convex problem.",
            "So is that what you're similar to what you're doing?",
            "So?",
            "So yeah, it is quite possible.",
            "Strong duality holds, and so it is quite possible solve this integral, which is we found.",
            "It's more efficient to keep in the primal OK.",
            "Thank you very much.",
            "So your sparsity regularization is inspired by this sort of by this parsing the network.",
            "But in all the network that you presented, it's the power law.",
            "Is there a compatibility?",
            "So your question is a sparsely premiering so yeah, we we looked at very lot of different innovation models.",
            "We included power law here, but actually the reason we did this because the power law was some of our lesser performing metrics like for example Bible distribution.",
            "We usually had break even points well above .9.",
            "That's that's a nice gentle unimodal distribution as opposed to fat tail distribution power law.",
            "So.",
            "So couple of questions.",
            "First is is on the information in these cascades used in order to infer the network?",
            "I guess I could envision like in the STD example for instance, that there may be several nodes who are infected but don't know it.",
            "So how essential is the idea that you know every infection time, surely?",
            "Well, that's a good question.",
            "I have a backup slide here, so this basically we add a perturbation to each nodes to each.",
            "Infection event and so then we so we basically added like a iid Gaussian perturbations each infection event and so this shows applied here.",
            "Basically this is noise.",
            "The average noise to average signal and even with the noise simulation about .2 we can still get reasonable amount of accuracy.",
            "And so this also is kind of like a proxy to what happens when we don't know the incubation model exactly because this creates a different incubation model robustness to the engagement.",
            "The other question I had was a lot of these social networks tend to be highly time variant.",
            "Do you have any sense of how well this would do in the case where your adjacency matrix was varying overtime?",
            "That's a very interesting question.",
            "Yeah, that definitely interesting way to explore is like coming up with how time variance effects 'cause we didn't really consider network dynamics at all in this, so that would be very interesting to explore.",
            "Alright, let's thank the speaker one more time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, I'm Seth and so will be.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, by talking about what motivated this work and that's that many real world social networks are difficult to observe directly.",
                    "label": 1
                },
                {
                    "sent": "One such example, let's say we have population people interested in the sexual relationship network that connects them.",
                    "label": 1
                },
                {
                    "sent": "We can expect that most people aren't going to be upfront about this network, and so determining the edges that connect these people can be quite difficult.",
                    "label": 0
                },
                {
                    "sent": "However, in this situation, which there are series of diseases that propagate along these relationships, we can much more easily observed when people become infected by these diseases.",
                    "label": 0
                },
                {
                    "sent": "And in doing so, gain insight into the underlying network that connects everybody.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to kind of articulate probable better, we have an unobservant unobservable social network of influence that connects nodes and we have a diffusive process which we can observe, such as information propagation or disease spread.",
                    "label": 0
                },
                {
                    "sent": "And we can observe the infection times of these nodes, and in doing so, or we can observe infection times these nodes and doing so in further network.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to give an example, we have some simple social network here connected by the edge is shown here.",
                    "label": 0
                },
                {
                    "sent": "We started the initial infection designated by red and we know the infection time as designated by towel.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Slowly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Infection propagates.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The network.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all we observe, however, is just the infection times of each node, and that's one cast.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dayton in summer we can.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a different cascade?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Starting a different infection this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Propagates.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And record those.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Infection times",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So kind of more general examples in which this is applicable for disease spread infection spread from person to person, and we can easily observe when people become infected but not who infected them.",
                    "label": 0
                },
                {
                    "sent": "And suddenly in lines of information propagation of viral marketing, people recommend products to each other.",
                    "label": 0
                },
                {
                    "sent": "People in turn by these products which we can observe.",
                    "label": 0
                },
                {
                    "sent": "But we don't know who influences purchase.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is in our goal is to infer who infected.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're given a set of cascades, and our goal is to infer this network, specifically the adjacency matrix, AR.",
                    "label": 0
                },
                {
                    "sent": "This adjacency matrix were allowed to be both directed and weighted, and the weights of these edges and correspond to the probability of the source node affecting the destination node.",
                    "label": 0
                },
                {
                    "sent": "So AIJ is the probability that I will infect J and so our approach this problem is going to be as follows.",
                    "label": 0
                },
                {
                    "sent": "First, define a probabilistic model that describes how cascades propagate to the network.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to build a likelihood function based on observed cascades, and then we're going to maximize likelihood function by solving a series of convex subproblems, and then finally special attention will be needed, will be needed for the case where most social networks are sparse, and so we need to address that specifically.",
                    "label": 0
                },
                {
                    "sent": "And just to point out, we kind of two goals here.",
                    "label": 0
                },
                {
                    "sent": "One is to infer the actual network structure where the edges are, what nodes are connected, as well as the edge weights or other infection probabilities associated with each edge.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first we'll define the Cascade model.",
                    "label": 0
                },
                {
                    "sent": "We have a simple social network here defined by an adjacency matrix A and that gives us all the edge weights and we start off the initial infection node.",
                    "label": 0
                },
                {
                    "sent": "I an then so each one of node eyes outbound neighbors is going to be sampled as to whether not the infection propagate according to the probability corresponding that edge.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this particular case, K&J will become infected, but just so happened that Ellen will not be according to sampling, and each of these nodes sampled independently.",
                    "label": 0
                },
                {
                    "sent": "And then now that we determine which of the nodes will become infected, we need to associate a new infection time with these nodes.",
                    "label": 0
                },
                {
                    "sent": "So we came up with this idea of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Occupation model, which means that incubation times are distributed by some known density function, W. So if node eyes affected atau I each of its new infections is going to be how I plus some interval sampled independently from this density function.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so with this model building likely functions are relatively straightforward, forgiving cascade.",
                    "label": 0
                },
                {
                    "sent": "See, we know we're going to assume we know each of the nodes, infection times, towel, and then the likelihood of 1st that particular node infects another.",
                    "label": 0
                },
                {
                    "sent": "So likelihood of Jay infecting I just likelihood that the infection propagates along edge AJ I times.",
                    "label": 0
                },
                {
                    "sent": "The likelihood of that particular incubation time interval since we know when each node is affected, we know what the incubation period had to be in order for the infection, probably along those lines.",
                    "label": 0
                },
                {
                    "sent": "And then from there we can take it up another level and say what's the likelihood of a particular node being affected by a given cascade at simply just the likelihood of any of its possible potential inbound neighbors infecting it.",
                    "label": 0
                },
                {
                    "sent": "So we basically take the product.",
                    "label": 0
                },
                {
                    "sent": "Overall, all nodes are infected previously, and it's probably at least one of those infected it.",
                    "label": 0
                },
                {
                    "sent": "And similarly for the case where a note it was not infected or infection times Infinity, I would basically just likelihood that none of these potentially infected neighbors infected it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's basically a likelihood function that's for one cascade, and we put it all together for all observed cascades.",
                    "label": 0
                },
                {
                    "sent": "So this first term we're just taking the product across all drive cascades.",
                    "label": 0
                },
                {
                    "sent": "This first term is likelihood of all infection events.",
                    "label": 0
                },
                {
                    "sent": "All nodes that were infected by cascade and this term is infection of events which there was no infection or just knows not infected by the cascade, and then searching further network, we basically just maximize this function, and so that's just taking the minimum.",
                    "label": 0
                },
                {
                    "sent": "The log likelihood function, and then we finally apply the constraints of the edge weights between zero and one 'cause they do represent probabilities.",
                    "label": 0
                },
                {
                    "sent": "Now solving this problem has two really big issues with it.",
                    "label": 0
                },
                {
                    "sent": "The first one is scalability.",
                    "label": 0
                },
                {
                    "sent": "The number of parameters in this problem grows quadratically with the number of nodes in network.",
                    "label": 0
                },
                {
                    "sent": "So forgiven network of in nodes there's intense in minus one potential edges, so that makes scaling this problem completely undoable basically.",
                    "label": 0
                },
                {
                    "sent": "Additionally, this is in general non convex function and it is indeed played with many local minima, so we can never be sure we have optimality, so we're going to solve this.",
                    "label": 0
                },
                {
                    "sent": "These two problems.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of 1 fell swoop.",
                    "label": 0
                },
                {
                    "sent": "I'm going to drive a series of convex subproblems that allow us to not only solve attractively guarantee an optimal solution, so we do this first, breakdown the subproblems, giving just a conceptual argument, and then we're going to add parameters and we're basically in turn are sub problems into a series of geometric programs, and then the method for turning geometric program into a convex program as well known.",
                    "label": 0
                },
                {
                    "sent": "So follow those steps.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first for the convex vacation.",
                    "label": 0
                },
                {
                    "sent": "Or sorry for breaking down the subproblems we know when all infections occur and we know they occur independently.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to maximize likelihood of inodes, infection, or lack of infection.",
                    "label": 0
                },
                {
                    "sent": "All it really cares about is when it's inbound neighbors got infected, not how they got infected.",
                    "label": 0
                },
                {
                    "sent": "So Furthermore it's infection time, or it's infection.",
                    "label": 0
                },
                {
                    "sent": "Likelihood is certainly a product of only it's inbound edges, and it's independent of any other edge in the network.",
                    "label": 0
                },
                {
                    "sent": "So that means each likelihood of each node can be solved independently.",
                    "label": 0
                },
                {
                    "sent": "Where the parameters are just it's inbound edges, which gives us in sub problems within minus one parameters, so that immediately allows us to scale this problem to much higher or much larger networks.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and so this new function here represents the modified likelihood function for particular subproblem.",
                    "label": 0
                },
                {
                    "sent": "It's really no different, we just we're given node.",
                    "label": 0
                },
                {
                    "sent": "I basically just the product overall, all cascades in which it was infected, and then we hear is likelihood of all cascades which it wasn't.",
                    "label": 0
                },
                {
                    "sent": "And then the first step in convex ification is related to allow this likelihood of infection to become an independent parameter gamma.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we plug that in there and then, as you might expect, we need to constrain gamma.",
                    "label": 0
                },
                {
                    "sent": "But instead of doing any an equality constraint, we're going to impose an inequality constraint that allows for convex fication.",
                    "label": 0
                },
                {
                    "sent": "And Additionally it can be shown that this will always be a binding constraint at the solution.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, from there we imply a few more change of variables.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the details here, but basically basically come from the fact we converted into geometric program and then apply a well known variable transformation to get into a convex program.",
                    "label": 0
                },
                {
                    "sent": "And this is what we're left with a couple of highlights.",
                    "label": 0
                },
                {
                    "sent": "Here are that it's a strong convex, so we're guaranteed optimality and has a nice linear objective function, and then these nonlinear inequality constraints are basically what happens.",
                    "label": 0
                },
                {
                    "sent": "This constraint after the transformation, and it can be shown this will always be.",
                    "label": 0
                },
                {
                    "sent": "Strictly zero at the solution.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so as I mentioned before special attention we need for sparsity as because almost all social networks, at least the ones we're inferring, are going to be sparse.",
                    "label": 0
                },
                {
                    "sent": "Most people aren't friends with everybody in the network, only as a very small subset of people.",
                    "label": 0
                },
                {
                    "sent": "And similarly, the maximum output estimation for any particular edge will almost never be 0.",
                    "label": 0
                },
                {
                    "sent": "The only way it can be zero, as if the source node was never infected in the same cascade, at least previously, as the destination node.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it happen at least once.",
                    "label": 0
                },
                {
                    "sent": "Then will be arbitrarily small, but it will never be 0.",
                    "label": 0
                },
                {
                    "sent": "So the obvious thing to do in order to accurately infer now structure is imposed an L1 penalty function, but that doing so ruins the convexity.",
                    "label": 0
                },
                {
                    "sent": "After do the variable transformation you get like actually a concave function?",
                    "label": 0
                },
                {
                    "sent": "So what we did instead was basically we modify this new concave function only slightly to create a convex function, and it's been shown that like over and over again that it doesn't do sparsity quite well.",
                    "label": 0
                },
                {
                    "sent": "So once in the original edge space, this is what the function looks like, so we can offer kind of a conceptual explanation.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As to why this particular function does work well and it comes from basically intuition, all familiar with is where the L1 penalty comes from.",
                    "label": 0
                },
                {
                    "sent": "It simply relaxation of the zero norm.",
                    "label": 0
                },
                {
                    "sent": "We've actually seen this figure earlier this week, but we have the level curves of the L1 here and it more often times not intersect the linear constraints in the same places.",
                    "label": 0
                },
                {
                    "sent": "Zero norm would.",
                    "label": 0
                },
                {
                    "sent": "Making this a valid relaxation.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if the level curves of our function for solutions close to 0 or sufficiently sparse, we have the same type of behavior where.",
                    "label": 0
                },
                {
                    "sent": "The zero norm will often intersect in the same place.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's basically our method as stated.",
                    "label": 0
                },
                {
                    "sent": "So now we want to test it and so we kind of set up a few evaluation metrics to determine how successful this algorithm truly is.",
                    "label": 0
                },
                {
                    "sent": "As mentioned before, we're just in kind of two different things, and for a network structure and also inferring infection probabilities of each of each edge.",
                    "label": 0
                },
                {
                    "sent": "So that means we basically need to the evaluation metrics, the first one is for determining actual network structure.",
                    "label": 0
                },
                {
                    "sent": "We look at the precision recall of basically where we predict edges.",
                    "label": 0
                },
                {
                    "sent": "So for any given node pair, we predict whether or not there's an edge there.",
                    "label": 0
                },
                {
                    "sent": "Based on whether or not the inferred edge probability is 0 or not, we like the precision.",
                    "label": 0
                },
                {
                    "sent": "Recall of that of that method.",
                    "label": 0
                },
                {
                    "sent": "Then for the mean square error, we basically have all infer that we take the union of all the edges of which we in Ferd, and then all the actual edges, and we take the mean square over all that and that gives us our how accurate infection probabilities are.",
                    "label": 0
                },
                {
                    "sent": "And then we also compared it to a baseline algorithm net if it's only other one, we could find addresses this problem.",
                    "label": 0
                },
                {
                    "sent": "In particular, it's an approximation algorithm based on submodular optimization.",
                    "label": 0
                },
                {
                    "sent": "And it makes the assumption that all affection probabilities are the same, or but they're all edge weights are identical.",
                    "label": 0
                },
                {
                    "sent": "Winston assumption holds.",
                    "label": 0
                },
                {
                    "sent": "It's a fantastic Alderman, speedy, tractable, and as well.",
                    "label": 0
                },
                {
                    "sent": "But we're going to see how well we do when we relax this assumption and and see where it goes.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our basically our experiments restriction three tiers.",
                    "label": 0
                },
                {
                    "sent": "The first one is we generate synthetic networks with synthetic cascades.",
                    "label": 0
                },
                {
                    "sent": "We looked at wide range of network topologies, scale free, Eros, rainy, Chronicle graphs and then we also looked at.",
                    "label": 0
                },
                {
                    "sent": "We generate cascade using a bunch of different types of incubation models.",
                    "label": 0
                },
                {
                    "sent": "Here's power law, only been shown them any information propagation models follow power law.",
                    "label": 0
                },
                {
                    "sent": "In real life.",
                    "label": 0
                },
                {
                    "sent": "We also we also looked at exponential.",
                    "label": 0
                },
                {
                    "sent": "We looked at why Bolton showing that.",
                    "label": 0
                },
                {
                    "sent": "The SARS outbreak in Hong Kong for the Weibull distribution.",
                    "label": 0
                },
                {
                    "sent": "Anyway, we basically just generated synthetic cascade and see how we can get closer.",
                    "label": 0
                },
                {
                    "sent": "Get ground truth.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 particular example for scale free network of 500 nodes.",
                    "label": 0
                },
                {
                    "sent": "Here we have the precision and recall like on the next wave recall and precision and you can see the break even point of about .85 which we are extremely pleased with.",
                    "label": 0
                },
                {
                    "sent": "Similarly over here we have the mean square error as a function of the number of edges in which we infer.",
                    "label": 0
                },
                {
                    "sent": "So obviously these are correlated.",
                    "label": 0
                },
                {
                    "sent": "To know more precise graph has less edges and so so.",
                    "label": 0
                },
                {
                    "sent": "These plots corresponding that way and in the green line actually represents the exact number of true edges in the network, and so are the blue is all our algorithm and the red is a baseline.",
                    "label": 0
                },
                {
                    "sent": "So keep in mind the red is so assuming that all the infection.",
                    "label": 0
                },
                {
                    "sent": "Probably just saying we have a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next year of experience looked at as real social networks and we again generated artificial cascades across them.",
                    "label": 0
                },
                {
                    "sent": "One session network, which have included here, is an email network between approximately 600 people and we assigned infection probabilities based on the volume of emails sent from one person to another.",
                    "label": 1
                },
                {
                    "sent": "Again, we generation cascades using parallel propagation and we have a break, even precision, recall break, even point about .95 the mean square error of less around .03.",
                    "label": 1
                },
                {
                    "sent": "So again we were quite pleased.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the last tiered experiences we looked at real networks with real cascades.",
                    "label": 0
                },
                {
                    "sent": "One such network was a product recommendation network of about 275 people, and basically what happens is we have the ground truth of who recommended product, who, who, who.",
                    "label": 0
                },
                {
                    "sent": "Then in turn bought the product on that recommendation.",
                    "label": 0
                },
                {
                    "sent": "So all we give to the Nepal, we give the algorithm is who bought what product win, not who influenced them and we try to see how well we can infer the actual recommendation network.",
                    "label": 0
                },
                {
                    "sent": "We didn't know what the incubation model was, not the ground truth that we observed it to be.",
                    "label": 0
                },
                {
                    "sent": "Power law and we had to adjust the parameter accordingly, but even with that inaccuracy we get a break even point of around .75 an which is significantly better than the net 10th and yeah, so we're again quite pleased with that.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So and then the last thing I'll include is the number of cascades needed to do this type of analysis.",
                    "label": 0
                },
                {
                    "sent": "It's usually on the order of about to generate in order to get 99% of all edges, probably at least one infection.",
                    "label": 0
                },
                {
                    "sent": "Usually it's on the order of twice as many nodes in the network, and so this is again a scale at the same scale.",
                    "label": 0
                },
                {
                    "sent": "Free network showed you earlier and we just looked at the accuracy of the break even point as a function of the number of cascades.",
                    "label": 0
                },
                {
                    "sent": "So around 1000 we already get a break even point well above .8.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary, we this our algorithm scalable and robust.",
                    "label": 0
                },
                {
                    "sent": "I should say that we can infer thousand networks inside of 10 minutes just running on laptop, solving each problem in sequence.",
                    "label": 0
                },
                {
                    "sent": "Each subproblem sequence and has applications to Epidemiology, viral marketing, etc.",
                    "label": 0
                },
                {
                    "sent": "And some interesting topics of further study could be the case in which we have missing nodes.",
                    "label": 0
                },
                {
                    "sent": "Mean there's external influences the network which you can observe directly.",
                    "label": 0
                },
                {
                    "sent": "More specialized cascade models that specific domains, methods of handling not know, not knowing, incubation model and also exploring.",
                    "label": 0
                },
                {
                    "sent": "Connections to more general graphical models.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "If you have time, you have time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "Hype.",
                    "label": 0
                },
                {
                    "sent": "Hi, your dual problem is maximum entropy problem.",
                    "label": 0
                },
                {
                    "sent": "Have you considered the dual problem?",
                    "label": 0
                },
                {
                    "sent": "If the convex formulation we while we shopped around for solving convex solvers and anything that usually we found that it has actually create an explosion of variables and dual space.",
                    "label": 0
                },
                {
                    "sent": "So we avoided dual solving the problem.",
                    "label": 0
                },
                {
                    "sent": "The dual OK but I wanted to understand if you think do and do again you get a convex problem.",
                    "label": 0
                },
                {
                    "sent": "So is that what you're similar to what you're doing?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So yeah, it is quite possible.",
                    "label": 0
                },
                {
                    "sent": "Strong duality holds, and so it is quite possible solve this integral, which is we found.",
                    "label": 0
                },
                {
                    "sent": "It's more efficient to keep in the primal OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So your sparsity regularization is inspired by this sort of by this parsing the network.",
                    "label": 0
                },
                {
                    "sent": "But in all the network that you presented, it's the power law.",
                    "label": 0
                },
                {
                    "sent": "Is there a compatibility?",
                    "label": 0
                },
                {
                    "sent": "So your question is a sparsely premiering so yeah, we we looked at very lot of different innovation models.",
                    "label": 0
                },
                {
                    "sent": "We included power law here, but actually the reason we did this because the power law was some of our lesser performing metrics like for example Bible distribution.",
                    "label": 0
                },
                {
                    "sent": "We usually had break even points well above .9.",
                    "label": 0
                },
                {
                    "sent": "That's that's a nice gentle unimodal distribution as opposed to fat tail distribution power law.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So couple of questions.",
                    "label": 0
                },
                {
                    "sent": "First is is on the information in these cascades used in order to infer the network?",
                    "label": 0
                },
                {
                    "sent": "I guess I could envision like in the STD example for instance, that there may be several nodes who are infected but don't know it.",
                    "label": 0
                },
                {
                    "sent": "So how essential is the idea that you know every infection time, surely?",
                    "label": 0
                },
                {
                    "sent": "Well, that's a good question.",
                    "label": 0
                },
                {
                    "sent": "I have a backup slide here, so this basically we add a perturbation to each nodes to each.",
                    "label": 0
                },
                {
                    "sent": "Infection event and so then we so we basically added like a iid Gaussian perturbations each infection event and so this shows applied here.",
                    "label": 0
                },
                {
                    "sent": "Basically this is noise.",
                    "label": 0
                },
                {
                    "sent": "The average noise to average signal and even with the noise simulation about .2 we can still get reasonable amount of accuracy.",
                    "label": 0
                },
                {
                    "sent": "And so this also is kind of like a proxy to what happens when we don't know the incubation model exactly because this creates a different incubation model robustness to the engagement.",
                    "label": 0
                },
                {
                    "sent": "The other question I had was a lot of these social networks tend to be highly time variant.",
                    "label": 0
                },
                {
                    "sent": "Do you have any sense of how well this would do in the case where your adjacency matrix was varying overtime?",
                    "label": 0
                },
                {
                    "sent": "That's a very interesting question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that definitely interesting way to explore is like coming up with how time variance effects 'cause we didn't really consider network dynamics at all in this, so that would be very interesting to explore.",
                    "label": 0
                },
                {
                    "sent": "Alright, let's thank the speaker one more time.",
                    "label": 0
                }
            ]
        }
    }
}