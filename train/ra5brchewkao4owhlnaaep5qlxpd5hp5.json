{
    "id": "ra5brchewkao4owhlnaaep5qlxpd5hp5",
    "title": "BBM: Bayesian Browsing Model from Petabyte-scale Data",
    "info": {
        "author": [
            "Chao Liu, Microsoft Research"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/kdd09_liu_bbmbbmpsd/",
    "segmentation": [
        [
            "Once conditioned on the sea."
        ],
        [
            "Uh, no previous clicks outcome and it's current SI so using this independence property we get this quantity in the analytical form.",
            "And then we can just apply the chain rule to get a piece."
        ],
        [
            "Given as, just use the chain rule across this M position and get them further, we use the mapping from S2R to convert this pcgames into PC give R. And this is actually the likelihood of one such instance given all the document relevance.",
            "Then putting things together, we get the posterior in the following form.",
            "In comparison with the last formula, what?"
        ],
        [
            "Different is only here we just go over all day in such instances.",
            "Take a look at this formula.",
            "No matter, no matter what fun Ki is, it must be one of the one of the index from one to Big N. And this number is the number of times where DJ was not clicked.",
            "One is at position R + D and the preceding clip position is R."
        ],
        [
            "And what does this tell us?",
            "First, it tells us we actually got the joint posterior of all the documents for this query includes form through exact inference.",
            "There's no approximation, and no iterations needed.",
            "A second it shows that they join the posterior of all the documents actually factorize so that they posterior for all the documents are mutually independent.",
            "And notice this formula.",
            "This is a posterior for the document J.",
            "In order to fully characterize this posterior, we only need to remember these numbers.",
            "So if we can count these numbers and keep record of these numbers, we are done with the post.",
            "We're done with the posterior, so we can somehow arrange these numbers as a vector, which we call the counter vector.",
            "And in the actual algorithm, we only need to accumulate and count these kind of these numbers."
        ],
        [
            "Nothing else."
        ],
        [
            "So let's see what's what's the algorithm looks like.",
            "So here is the.",
            "Since we already have the close form, we don't need to count these numbers.",
            "It should be straightforward, and in fact it is.",
            "Suppose we have one such instances.",
            "We just go through the instance by instance, and for each of them we just scan from position one to position M. And for each document, we first find It's document index and then we check if it's clicked or not.",
            "If it's clicked we just increment this counter by 1.",
            "Otherwise, we increment one of one of the rest, count by one, and that count is found by the preceding click position and the distance to the precede."
        ],
        [
            "Position.",
            "The best way to to to explain an algorithm maybe is through an example, so here is a concrete concrete example.",
            "Here we have three instances, and dark color means click the document, while lighter means unclicked.",
            "And here is the beta matrix.",
            "And let's see whether we can compute the posterior for the relevance of U4.",
            "So we first initialize the counter vector of U, 420 zeros.",
            "Here this is Len and this is another matrix correspond to the beta.",
            "And then we scan the search instance one by one in the first one.",
            "Since you 4 is not here, so we don't need to do anything.",
            "So nothing changed for the second such instance.",
            "Since you 4 appears here.",
            "Which means an it's not clicked, so we have.",
            "So we need to increment the count.",
            "In one of them, and which one to increment, it depends.",
            "It's proceeding click position which is the two and the distance is 1.",
            "So we need to increment this count by 1.",
            "And for the third one, since you 4 is clicked, we need to increment this counter from zero to 1.",
            "So in the end we got the count vector like this which correspond to the.",
            "Unnormalized posterior R 4 * 1 -- R Four.",
            "And we can normalize it and even plot it.",
            "And this will have the posterior in closed form and give and even plotted.",
            "As you can see, this is a one pass algorithm.",
            "We just scan the data once and get all the posterior of all the documents."
        ],
        [
            "Old altogether, but we want to further scale up.",
            "So can we run that in parallel?",
            "The answer is yes.",
            "This is algorithm using map read."
        ],
        [
            "But still, let's use an example.",
            "Suppose these are these three examples.",
            "Three search instance we care about, and in the map function.",
            "We just do this simultaneously in the map function we generate, we emit key value pairs where the key is the document and the value is the index into the account vector where the value should be incremented.",
            "So since this is click document it script its corresponding index will be 0.",
            "And in the map.",
            "And then we do the reduce where we just put all the.",
            "Key value pairs with the same for the same document together.",
            "And this role is just shows.",
            "What are the indices where the value should be incremented?",
            "And the counter vector has a one to one correspondence to the to the posterior, so we will get all the posterior in parallel way."
        ],
        [
            "So for some experiments, we compared with the user browsing model proposed by do parade and pure whiskey in this last year CR this week, this UBM and this model actually share the same dependence share the same dependence, shared the same dependence, But the only difference is that they use the point estimation for the document relevance so.",
            "As a consequence, they have to resort to the approximate inference algorithm and do that through iterations.",
            "We will compare with their model in terms of both effectiveness and efficiency.",
            "And for the data we use too much data from last year and we only care about we don't care about the 10 algorithmic result.",
            "And for each query we split this session, we split the instances according to the time stamp first half as training and the rest."
        ],
        [
            "As testing.",
            "And we use the log likelihood as a evaluation metric of of the model quality.",
            "The larger the better.",
            "Basically we train the model between both models on the same training data set and then calculate the test likelihood for the testing set.",
            "And we run that intended batches and the left figure shows the log likelihood comparison across these 20 batches.",
            "We see that BBM is consistently better than UBM, and the average improvement ratio is 29.2%."
        ],
        [
            "Well, this one shows the in general.",
            "Overall you BBM is much better than UBM.",
            "We want to see under what circumstances does.",
            "PBM other performs JVM.",
            "Another in cases where we are in fear to the JVM, one intuition under this kind of testing is that in general.",
            "In general, it would be relatively easy to predict clicks for frequent queries.",
            "While it would be somehow hard for infrequent queries, so for this.",
            "From this consideration, we just divide the testing set or two several bins according to their query frequency and test the performance for each query frequency.",
            "And we still see that uniformly our model BBM is better than JVM and it specially the improvement becomes larger and larger when the query become less and less frequent.",
            "We regard this as very valuable because in the search, most of the queries are tailed."
        ],
        [
            "Trees.",
            "Besides the effectiveness, we also compare on the efficiency.",
            "We see that our method actually runs 57 times faster than UBM.",
            "There are two factors contributing to this speed up.",
            "First for the UBM, they needed the linear multiple iterations to converge and usually it needs 20.",
            "Somehow iterations and 2nd for even for each iteration of UBM.",
            "It's their complicated.",
            "Their computation is much more complicated than BBM, because in our model our just scanner data and count.",
            "So this this explains why we are 50."
        ],
        [
            "Sometimes faster.",
            "Finally, we want to demonstrate how scalable our model is in a parallel way.",
            "So for this purpose we collect 8 weeks data and construct it.",
            "We construct it 8 jobs to run that in on the MapReduce cluster.",
            "The job K actually just take the first K WK data and do the model inference.",
            "Notice that the largest experiment we do we did is.",
            "The data size was 265 terabytes.",
            "The running platform is scope whose detail is described in this weird paper."
        ],
        [
            "And this graph shows the scalability of BBM.",
            "The left figure shows the total computation time with respect to the data size for each job.",
            "The total number of time, the total time is all the time spent on all the machines adding together.",
            "So we see that as we process more and more data, the total computation load grows linearly, but the right figure shows the elapsed time once we run it.",
            "Shows the elapsed time when we are running this in the in parallel, we see that at the beginning the Times pick up.",
            "But when it becomes become stable.",
            "This elapsed time roughly stay within two and three hours.",
            "No matter how large data sizes, I mean, if we have more and more machines, we can even reduce the time.",
            "And for the largest one, it takes less than three hours to scan 265 terabytes data and output the posterior of four one point 15 building."
        ],
        [
            "Are you all pairs?",
            "So."
        ],
        [
            "So to conclude, in this study, we propose the Bayesian browsing model for search streams.",
            "We show that we can actually do the inference.",
            "We can do the exact Bayesian inference.",
            "And get the joint posterior in close form.",
            "This is uncommon.",
            "Consider just consider the dependence structure of the BBM.",
            "It's pretty complicated and usually approximate inference that that needs iterations to converge are needed.",
            "But we managed to get the close form so that we can do all the inference with one scan of the data.",
            "And Furthermore we can scale it up by using the map reduce.",
            "And for another thing, things for each query you are pairs.",
            "We only need to keep account vector so our model can be incrementally updated.",
            "All those things added together.",
            "We think it would be very suitable for mining click streams in the future we're gonna investigate into other stream data on the web, for example the browsing log or the twittering actions.",
            "Or the Web 2.0 interactions between people.",
            "All of these are data stream in the sense that they keep coming in.",
            "Either server or the client side has the lock store, so there going to be some way to leverage this kind of data.",
            "So I think I better."
        ],
        [
            "Stop here and be open to any questions, thanks.",
            "You have just mentioned that.",
            "Runs much faster than paper.",
            "See properly or 8 and you have mentioned that there were two points for your problem runs much faster, but you did not mention that.",
            "The only thing is improved by parallel parallel programming.",
            "You have just mentioned that your program could be running power railway.",
            "I wonder the reason why and I wonder in your real life applications you will you will change it into relevant application and will run the algorithm in the power level.",
            "OK, so here maybe something I need to clarify so this efficiency comparison is on the same machine.",
            "One single machine run run sequentially.",
            "There's no parallel is here, so I run.",
            "We implement the algorithm and check it with the author and run that Randy algorithm and run hours and compute the time needed so there's no parallelism in neither algorithms.",
            "And your second question is, in practice whether we will run that in parallel, right?",
            "Yeah, definitely because in the in practice we are dealing with hundreds of terabytes or because we are analyzing over half a year, which runs into 1.7 petabytes.",
            "So sure, this is actually algorithm we used in practice.",
            "This is just to compare to compare the algorithm at our action on the on the same machine.",
            "And the algorithm can be implemented in the powerless way, but this still need multiple iterations to finally converge.",
            "So the time cost would still be much higher than the hours.",
            "So in your city or you do not consider the inference may change overtime so.",
            "You comment on whether how these kind of models can be extended, or yeah, sure, that would be a great question.",
            "So here we show that.",
            "Since the four posterior is described by by this formula, maybe I showed it the other way."
        ],
        [
            "Which is larger, so the posterior is only characterized by these numbers.",
            "So just consider in a data stream mining fashion.",
            "We just keep this kind of vector over the stream and once new data comes in, we can decay.",
            "Somehow the past accounts and add new accounts so that if we set this eigenfactor appropriately, then it could.",
            "It could track the click pattern, for example for the given query.",
            "If the user intention actually changes or shifts overtime, the the past data, the old data will count less and less while we focus more and more on new data.",
            "So this can be automatically updated or adapted.",
            "In a data stream anyway.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once conditioned on the sea.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, no previous clicks outcome and it's current SI so using this independence property we get this quantity in the analytical form.",
                    "label": 0
                },
                {
                    "sent": "And then we can just apply the chain rule to get a piece.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Given as, just use the chain rule across this M position and get them further, we use the mapping from S2R to convert this pcgames into PC give R. And this is actually the likelihood of one such instance given all the document relevance.",
                    "label": 1
                },
                {
                    "sent": "Then putting things together, we get the posterior in the following form.",
                    "label": 0
                },
                {
                    "sent": "In comparison with the last formula, what?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different is only here we just go over all day in such instances.",
                    "label": 0
                },
                {
                    "sent": "Take a look at this formula.",
                    "label": 0
                },
                {
                    "sent": "No matter, no matter what fun Ki is, it must be one of the one of the index from one to Big N. And this number is the number of times where DJ was not clicked.",
                    "label": 0
                },
                {
                    "sent": "One is at position R + D and the preceding clip position is R.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what does this tell us?",
                    "label": 0
                },
                {
                    "sent": "First, it tells us we actually got the joint posterior of all the documents for this query includes form through exact inference.",
                    "label": 1
                },
                {
                    "sent": "There's no approximation, and no iterations needed.",
                    "label": 0
                },
                {
                    "sent": "A second it shows that they join the posterior of all the documents actually factorize so that they posterior for all the documents are mutually independent.",
                    "label": 0
                },
                {
                    "sent": "And notice this formula.",
                    "label": 0
                },
                {
                    "sent": "This is a posterior for the document J.",
                    "label": 1
                },
                {
                    "sent": "In order to fully characterize this posterior, we only need to remember these numbers.",
                    "label": 0
                },
                {
                    "sent": "So if we can count these numbers and keep record of these numbers, we are done with the post.",
                    "label": 0
                },
                {
                    "sent": "We're done with the posterior, so we can somehow arrange these numbers as a vector, which we call the counter vector.",
                    "label": 0
                },
                {
                    "sent": "And in the actual algorithm, we only need to accumulate and count these kind of these numbers.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing else.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see what's what's the algorithm looks like.",
                    "label": 0
                },
                {
                    "sent": "So here is the.",
                    "label": 0
                },
                {
                    "sent": "Since we already have the close form, we don't need to count these numbers.",
                    "label": 0
                },
                {
                    "sent": "It should be straightforward, and in fact it is.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have one such instances.",
                    "label": 0
                },
                {
                    "sent": "We just go through the instance by instance, and for each of them we just scan from position one to position M. And for each document, we first find It's document index and then we check if it's clicked or not.",
                    "label": 0
                },
                {
                    "sent": "If it's clicked we just increment this counter by 1.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we increment one of one of the rest, count by one, and that count is found by the preceding click position and the distance to the precede.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Position.",
                    "label": 0
                },
                {
                    "sent": "The best way to to to explain an algorithm maybe is through an example, so here is a concrete concrete example.",
                    "label": 1
                },
                {
                    "sent": "Here we have three instances, and dark color means click the document, while lighter means unclicked.",
                    "label": 0
                },
                {
                    "sent": "And here is the beta matrix.",
                    "label": 0
                },
                {
                    "sent": "And let's see whether we can compute the posterior for the relevance of U4.",
                    "label": 0
                },
                {
                    "sent": "So we first initialize the counter vector of U, 420 zeros.",
                    "label": 0
                },
                {
                    "sent": "Here this is Len and this is another matrix correspond to the beta.",
                    "label": 0
                },
                {
                    "sent": "And then we scan the search instance one by one in the first one.",
                    "label": 0
                },
                {
                    "sent": "Since you 4 is not here, so we don't need to do anything.",
                    "label": 0
                },
                {
                    "sent": "So nothing changed for the second such instance.",
                    "label": 0
                },
                {
                    "sent": "Since you 4 appears here.",
                    "label": 0
                },
                {
                    "sent": "Which means an it's not clicked, so we have.",
                    "label": 0
                },
                {
                    "sent": "So we need to increment the count.",
                    "label": 0
                },
                {
                    "sent": "In one of them, and which one to increment, it depends.",
                    "label": 0
                },
                {
                    "sent": "It's proceeding click position which is the two and the distance is 1.",
                    "label": 0
                },
                {
                    "sent": "So we need to increment this count by 1.",
                    "label": 0
                },
                {
                    "sent": "And for the third one, since you 4 is clicked, we need to increment this counter from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So in the end we got the count vector like this which correspond to the.",
                    "label": 1
                },
                {
                    "sent": "Unnormalized posterior R 4 * 1 -- R Four.",
                    "label": 0
                },
                {
                    "sent": "And we can normalize it and even plot it.",
                    "label": 0
                },
                {
                    "sent": "And this will have the posterior in closed form and give and even plotted.",
                    "label": 0
                },
                {
                    "sent": "As you can see, this is a one pass algorithm.",
                    "label": 0
                },
                {
                    "sent": "We just scan the data once and get all the posterior of all the documents.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Old altogether, but we want to further scale up.",
                    "label": 0
                },
                {
                    "sent": "So can we run that in parallel?",
                    "label": 0
                },
                {
                    "sent": "The answer is yes.",
                    "label": 0
                },
                {
                    "sent": "This is algorithm using map read.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But still, let's use an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose these are these three examples.",
                    "label": 0
                },
                {
                    "sent": "Three search instance we care about, and in the map function.",
                    "label": 0
                },
                {
                    "sent": "We just do this simultaneously in the map function we generate, we emit key value pairs where the key is the document and the value is the index into the account vector where the value should be incremented.",
                    "label": 0
                },
                {
                    "sent": "So since this is click document it script its corresponding index will be 0.",
                    "label": 0
                },
                {
                    "sent": "And in the map.",
                    "label": 0
                },
                {
                    "sent": "And then we do the reduce where we just put all the.",
                    "label": 0
                },
                {
                    "sent": "Key value pairs with the same for the same document together.",
                    "label": 0
                },
                {
                    "sent": "And this role is just shows.",
                    "label": 0
                },
                {
                    "sent": "What are the indices where the value should be incremented?",
                    "label": 0
                },
                {
                    "sent": "And the counter vector has a one to one correspondence to the to the posterior, so we will get all the posterior in parallel way.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for some experiments, we compared with the user browsing model proposed by do parade and pure whiskey in this last year CR this week, this UBM and this model actually share the same dependence share the same dependence, shared the same dependence, But the only difference is that they use the point estimation for the document relevance so.",
                    "label": 1
                },
                {
                    "sent": "As a consequence, they have to resort to the approximate inference algorithm and do that through iterations.",
                    "label": 0
                },
                {
                    "sent": "We will compare with their model in terms of both effectiveness and efficiency.",
                    "label": 0
                },
                {
                    "sent": "And for the data we use too much data from last year and we only care about we don't care about the 10 algorithmic result.",
                    "label": 1
                },
                {
                    "sent": "And for each query we split this session, we split the instances according to the time stamp first half as training and the rest.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As testing.",
                    "label": 0
                },
                {
                    "sent": "And we use the log likelihood as a evaluation metric of of the model quality.",
                    "label": 0
                },
                {
                    "sent": "The larger the better.",
                    "label": 0
                },
                {
                    "sent": "Basically we train the model between both models on the same training data set and then calculate the test likelihood for the testing set.",
                    "label": 0
                },
                {
                    "sent": "And we run that intended batches and the left figure shows the log likelihood comparison across these 20 batches.",
                    "label": 1
                },
                {
                    "sent": "We see that BBM is consistently better than UBM, and the average improvement ratio is 29.2%.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, this one shows the in general.",
                    "label": 0
                },
                {
                    "sent": "Overall you BBM is much better than UBM.",
                    "label": 0
                },
                {
                    "sent": "We want to see under what circumstances does.",
                    "label": 0
                },
                {
                    "sent": "PBM other performs JVM.",
                    "label": 0
                },
                {
                    "sent": "Another in cases where we are in fear to the JVM, one intuition under this kind of testing is that in general.",
                    "label": 0
                },
                {
                    "sent": "In general, it would be relatively easy to predict clicks for frequent queries.",
                    "label": 1
                },
                {
                    "sent": "While it would be somehow hard for infrequent queries, so for this.",
                    "label": 0
                },
                {
                    "sent": "From this consideration, we just divide the testing set or two several bins according to their query frequency and test the performance for each query frequency.",
                    "label": 0
                },
                {
                    "sent": "And we still see that uniformly our model BBM is better than JVM and it specially the improvement becomes larger and larger when the query become less and less frequent.",
                    "label": 0
                },
                {
                    "sent": "We regard this as very valuable because in the search, most of the queries are tailed.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trees.",
                    "label": 0
                },
                {
                    "sent": "Besides the effectiveness, we also compare on the efficiency.",
                    "label": 0
                },
                {
                    "sent": "We see that our method actually runs 57 times faster than UBM.",
                    "label": 1
                },
                {
                    "sent": "There are two factors contributing to this speed up.",
                    "label": 0
                },
                {
                    "sent": "First for the UBM, they needed the linear multiple iterations to converge and usually it needs 20.",
                    "label": 0
                },
                {
                    "sent": "Somehow iterations and 2nd for even for each iteration of UBM.",
                    "label": 0
                },
                {
                    "sent": "It's their complicated.",
                    "label": 0
                },
                {
                    "sent": "Their computation is much more complicated than BBM, because in our model our just scanner data and count.",
                    "label": 0
                },
                {
                    "sent": "So this this explains why we are 50.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sometimes faster.",
                    "label": 0
                },
                {
                    "sent": "Finally, we want to demonstrate how scalable our model is in a parallel way.",
                    "label": 0
                },
                {
                    "sent": "So for this purpose we collect 8 weeks data and construct it.",
                    "label": 1
                },
                {
                    "sent": "We construct it 8 jobs to run that in on the MapReduce cluster.",
                    "label": 1
                },
                {
                    "sent": "The job K actually just take the first K WK data and do the model inference.",
                    "label": 0
                },
                {
                    "sent": "Notice that the largest experiment we do we did is.",
                    "label": 0
                },
                {
                    "sent": "The data size was 265 terabytes.",
                    "label": 0
                },
                {
                    "sent": "The running platform is scope whose detail is described in this weird paper.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this graph shows the scalability of BBM.",
                    "label": 1
                },
                {
                    "sent": "The left figure shows the total computation time with respect to the data size for each job.",
                    "label": 0
                },
                {
                    "sent": "The total number of time, the total time is all the time spent on all the machines adding together.",
                    "label": 0
                },
                {
                    "sent": "So we see that as we process more and more data, the total computation load grows linearly, but the right figure shows the elapsed time once we run it.",
                    "label": 0
                },
                {
                    "sent": "Shows the elapsed time when we are running this in the in parallel, we see that at the beginning the Times pick up.",
                    "label": 0
                },
                {
                    "sent": "But when it becomes become stable.",
                    "label": 0
                },
                {
                    "sent": "This elapsed time roughly stay within two and three hours.",
                    "label": 0
                },
                {
                    "sent": "No matter how large data sizes, I mean, if we have more and more machines, we can even reduce the time.",
                    "label": 0
                },
                {
                    "sent": "And for the largest one, it takes less than three hours to scan 265 terabytes data and output the posterior of four one point 15 building.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are you all pairs?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, in this study, we propose the Bayesian browsing model for search streams.",
                    "label": 1
                },
                {
                    "sent": "We show that we can actually do the inference.",
                    "label": 1
                },
                {
                    "sent": "We can do the exact Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "And get the joint posterior in close form.",
                    "label": 0
                },
                {
                    "sent": "This is uncommon.",
                    "label": 0
                },
                {
                    "sent": "Consider just consider the dependence structure of the BBM.",
                    "label": 0
                },
                {
                    "sent": "It's pretty complicated and usually approximate inference that that needs iterations to converge are needed.",
                    "label": 0
                },
                {
                    "sent": "But we managed to get the close form so that we can do all the inference with one scan of the data.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore we can scale it up by using the map reduce.",
                    "label": 0
                },
                {
                    "sent": "And for another thing, things for each query you are pairs.",
                    "label": 0
                },
                {
                    "sent": "We only need to keep account vector so our model can be incrementally updated.",
                    "label": 0
                },
                {
                    "sent": "All those things added together.",
                    "label": 1
                },
                {
                    "sent": "We think it would be very suitable for mining click streams in the future we're gonna investigate into other stream data on the web, for example the browsing log or the twittering actions.",
                    "label": 0
                },
                {
                    "sent": "Or the Web 2.0 interactions between people.",
                    "label": 0
                },
                {
                    "sent": "All of these are data stream in the sense that they keep coming in.",
                    "label": 0
                },
                {
                    "sent": "Either server or the client side has the lock store, so there going to be some way to leverage this kind of data.",
                    "label": 0
                },
                {
                    "sent": "So I think I better.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stop here and be open to any questions, thanks.",
                    "label": 0
                },
                {
                    "sent": "You have just mentioned that.",
                    "label": 0
                },
                {
                    "sent": "Runs much faster than paper.",
                    "label": 0
                },
                {
                    "sent": "See properly or 8 and you have mentioned that there were two points for your problem runs much faster, but you did not mention that.",
                    "label": 0
                },
                {
                    "sent": "The only thing is improved by parallel parallel programming.",
                    "label": 0
                },
                {
                    "sent": "You have just mentioned that your program could be running power railway.",
                    "label": 0
                },
                {
                    "sent": "I wonder the reason why and I wonder in your real life applications you will you will change it into relevant application and will run the algorithm in the power level.",
                    "label": 0
                },
                {
                    "sent": "OK, so here maybe something I need to clarify so this efficiency comparison is on the same machine.",
                    "label": 0
                },
                {
                    "sent": "One single machine run run sequentially.",
                    "label": 0
                },
                {
                    "sent": "There's no parallel is here, so I run.",
                    "label": 0
                },
                {
                    "sent": "We implement the algorithm and check it with the author and run that Randy algorithm and run hours and compute the time needed so there's no parallelism in neither algorithms.",
                    "label": 0
                },
                {
                    "sent": "And your second question is, in practice whether we will run that in parallel, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, definitely because in the in practice we are dealing with hundreds of terabytes or because we are analyzing over half a year, which runs into 1.7 petabytes.",
                    "label": 0
                },
                {
                    "sent": "So sure, this is actually algorithm we used in practice.",
                    "label": 0
                },
                {
                    "sent": "This is just to compare to compare the algorithm at our action on the on the same machine.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm can be implemented in the powerless way, but this still need multiple iterations to finally converge.",
                    "label": 0
                },
                {
                    "sent": "So the time cost would still be much higher than the hours.",
                    "label": 0
                },
                {
                    "sent": "So in your city or you do not consider the inference may change overtime so.",
                    "label": 0
                },
                {
                    "sent": "You comment on whether how these kind of models can be extended, or yeah, sure, that would be a great question.",
                    "label": 0
                },
                {
                    "sent": "So here we show that.",
                    "label": 0
                },
                {
                    "sent": "Since the four posterior is described by by this formula, maybe I showed it the other way.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is larger, so the posterior is only characterized by these numbers.",
                    "label": 0
                },
                {
                    "sent": "So just consider in a data stream mining fashion.",
                    "label": 0
                },
                {
                    "sent": "We just keep this kind of vector over the stream and once new data comes in, we can decay.",
                    "label": 0
                },
                {
                    "sent": "Somehow the past accounts and add new accounts so that if we set this eigenfactor appropriately, then it could.",
                    "label": 0
                },
                {
                    "sent": "It could track the click pattern, for example for the given query.",
                    "label": 0
                },
                {
                    "sent": "If the user intention actually changes or shifts overtime, the the past data, the old data will count less and less while we focus more and more on new data.",
                    "label": 0
                },
                {
                    "sent": "So this can be automatically updated or adapted.",
                    "label": 0
                },
                {
                    "sent": "In a data stream anyway.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}