{
    "id": "62luwvdl7yzclz5pftdlmatup6lyxzoy",
    "title": "Online Kernel Selection for Bayesian Reinforcement Learning",
    "info": {
        "author": [
            "Joseph Reisinger, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml08_reisinger_oks/",
    "segmentation": [
        [
            "Alright, thanks for coming after lunch.",
            "My name is Joe Risinger and I'm going to talk a little bit about some work that I've done at University, Texas with Peter Stone an Risto Mcaleenan.",
            "And basically what I'm going to talking about is Gaussian process, reinforcement learning, and sort of how to.",
            "What's the best way to pick something called the kernel?"
        ],
        [
            "Which is important for this?",
            "So here is the entire summary of the talk.",
            "I mean, Gaussian process enforcement learning.",
            "There's this thing called the kernel, which is which specifies something called the prior covariance.",
            "Choosing this.",
            "Huge impact on performance and so we need some ways of choosing them sufficiently during learning, and in particular, how can we?",
            "How can we choose it online and sort of fit in with the RL paradigm?",
            "So here's the talk.",
            "Outline 1st I'm going to talk about RL for like maybe 2 minutes.",
            "I'm going to choose Gaussian processes for a bit longer and then running into this problem of."
        ],
        [
            "Choosing the kernel.",
            "Alright, So what is reinforcement learning?",
            "Hopefully all of you have seen slides like this before.",
            "I'm going to present what's probably.",
            "I guess you can consider the model for a value function sort of story for reinforcement learning, although I know there's a ton of others.",
            "So the basic problem is that we have some sequential decision tasks and we have some agent that's is trying to act in some world.",
            "Ann has basically some state that it's in, and then some set of actions that can perform.",
            "It picks an action and then it goes to another state and then from there has another set of actions.",
            "So the whole idea is basically that you're not told after each transition how well you did, but you'll accumulate something called reward.",
            "Now and then, maybe say I got to the goal of some maze or something like that.",
            "I might get a reward for doing that, and then I need to figure out what was the sequence of actions that I took in which States and how.",
            "How did that contribute to me getting this reward?",
            "And so basically, we're going to model this task as a set of States and then transition probabilities.",
            "So given that I took some action in some state, there's some probability of seeing some other state of the world, and our goal is to do is to find a policy which says for each state.",
            "For each, for each element in this in this thing called in this State cross action space.",
            "What is with probability?",
            "Should I?",
            "Should that be observed?",
            "And basically what we want to do is to maximize something that we're going to call the value.",
            "Function, which is the expected discounted return of this policy and what is expected discounted channel.",
            "Basically it's if I'm in.",
            "If I can transition to some state X.",
            "What is the expected long-term reward?",
            "So not just the reward for being in that state, but the reward for any state I can get to subsequently after that, properly discounted for sort of this time horizon, and so that's given by these last few equations.",
            "So hopefully everyone's sort of at least comfortable at that story."
        ],
        [
            "So this value function thing basically.",
            "You can think of it as sort of a regression function that we're trying to learn an where the dependent variables are basically the state class action space, and then the independent variable is basically what is the?",
            "What is the value of the expected?",
            "This kind of return for choosing that action in that state, there's a lot of different ways of doing this.",
            "For example, you have these tabular methods where you literally just record every possible state in action.",
            "There is an.",
            "You sort of, you compute.",
            "Sufficient statistics for the value and another thing you can do is sort of use something called function approximation or basically what we call regression in that you pick kind of some class of functions.",
            "And then you say so I make some observations and then I sort of interpolate between those observations and say that's my value function.",
            "So one really nice way of doing this is using something called a Gaussian process.",
            "And so, well, what is a Gaussian process, right?"
        ],
        [
            "Basically, the idea is that we want to do regression, but we don't want to make any complexity assumptions without having seen the data and so basically what we're going to having is sort of a regression that has a number of parameters equal to the number of data points that we've observed, and so basically what happens is the model becomes more and more complex."
        ],
        [
            "As we observe more data and the fit becomes."
        ],
        [
            "Better I also become more confident sort of in how and what the fit is going to be.",
            "And so typically when you see stuff like this when you have a lot of data points, if you think of like polynomial interpolation, you have this idea of overfitting where you know it gets this really wildly wiggly things that tries to fit all the data points exactly.",
            "By using Gaussian process it's a Bayesian method, so we have this prior which is going to say.",
            "I'm just going to act as a regularizer and sort of make it less likely that we see a very wiggly functions, and so we're only going to look at these kind of smooth looking ones.",
            "So it's like a smoother, right?"
        ],
        [
            "Um?",
            "So how does that Gaussian process work?",
            "Basically, the main point is that you have some set of data, fits nonparametric so you have some set of data which are going to keep.",
            "And then for every day to every pair of data points, you're going to compute the covariance, which you're going to, which is going to be computed by.",
            "In this matrix K. And its entry in this metric scale is going to be computed by your kernel, which is just literally specifies a priority.",
            "What you think, how you think 2 states are going to be similar in terms of their values.",
            "And so now using this big matrix K and some.",
            "Some observations Y which is going to be the reward.",
            "We can actually take an unknown attest pointwise star, and we can actually represent it using Gaussian process.",
            "This is just a normal distribution where all of the observed data points have their covariances in K and then these small bold case are the covariances of the observed data points.",
            "With the new test point.",
            "Why and we can use something called the?",
            "Dallas mark up them to compute the posteriors for this and so basically all this is saying the sort of important summary of this is that given some observations, for example rewards.",
            "And then K, which specifies sort of how those states that generated these observations were related.",
            "Are related given that we can compute the expectation of his wife star, so some new test point we can compute what the value is at this new point.",
            "We haven't had not observed, but we can also compute the variance, which is sort of our confidence in how how good that prediction is a very high variance, for example, would mean that maybe we don't have good confidence here.",
            "So."
        ],
        [
            "The.",
            "Important thing basically from all this math is these two is these two objects.",
            "The set of data.",
            "And the kernel function.",
            "This is all we need to compute a Gaussian process.",
            "On and so the data is easy.",
            "Where does the kernel come from?",
            "That sort of the point of this talk?",
            "I should also mention in passing that the view I gave of Gaussian processes is sort of the general regression view, and there's been.",
            "There is a elegant but sort of nontrivial extension of this of this object to reinforcement learning.",
            "That kind of takes into account the.",
            "Sort of how you backup values along the.",
            "Along the sort of the chain of observation.",
            "So basically, if I observe some reward signal at the end, then I need to back up that reward.",
            "Across all of the states that I visited and I need to do that in a properly discounted way.",
            "And so this just takes that into account.",
            "This DRL which is the Gaussian process reinforcement learning, also does something called sparsification which if you remember so we're saving all of our data points to complete this Gaussian process and then we're computing this kernel function over them.",
            "But the problem with saving all the data points is that reinforcement learning kind of setting.",
            "There's just way too much data to save, so we need to sort of pick and choose."
        ],
        [
            "Hi, so this kernel function sort of what?",
            "What does this control right?",
            "So I'm going to show.",
            "Here are some examples drawn of functions that are drawn from the Gaussian process prior.",
            "So this is basically you can imagine the kind of functions you would see given some particular kernel instantiation.",
            "Like this is like sort of a priority.",
            "What's in there and then given some conditioned on some data.",
            "This is the kind of function this is kind of fit that I get back out and so here we have a much sort of wiggler.",
            "Prior and so it's going to make this sort of fit that actually tries to go through all the data points and then has these kinds of properties, and then again first maybe something intermediate.",
            "We get something like this.",
            "So sort of the properties of the learn function, and in particular the generalization properties of that function depend really critically on the kernel, and so it's not just these properties like smoothness like like what I showed you here, but you can even build in more knowledge like such that maybe I know that there are some symmetries or some kind of structure in my domain, and so I know a priority that some states over here and some states over here may be correlated or salmon.",
            "And navigation domain where I know that nearby states are correlated, so this."
        ],
        [
            "This kernel is basically defining a metric over.",
            "Here's your joint state in action space, and it's sort of saying.",
            "Again, I'll priore what, how, how much.",
            "How related are two States and so if you know, for example, two states are nearby in a map, then maybe their values are going to be very close by right?",
            "So we actually build these kinds of intuitions and via the kernel.",
            "And so and so, where do these kernels come from then?",
            "So, like you can imagine lots of different ways of doing this, there's lots of different smoothers.",
            "There's lots of different parametric forms we need some way to do a model selection step, which is just to take to kind of search through this space of kernels and then sort of pick the best one for the task at hand.",
            "And there are basically model selection steps and model selection criterion for doing this.",
            "Cross validations are really simple and that most people probably know, but they weren't really designed to work online.",
            "That is sort of you don't have a way.",
            "Constellation doesn't say like if this kernel did well in this kernel, didn't do well.",
            "What should I do next right?",
            "It doesn't really suggest that you just sort of enumerate and then you run over the entire set."
        ],
        [
            "So what I'm going to reduce or reintroduce is a.",
            "Really simple method that uses sequential Monte Carlo.",
            "Especially particle filter.",
            "And we're going to run this over the over some kernel space.",
            "This model space, and in particular we're going to.",
            "We're going to have these kernels Theta and this model space big data and we're not going to put any restrictions really on what this space can look like, so we're going to.",
            "This is a really simple method, and if we if we added additional assumptions about the model space looked like we could actually do better.",
            "Sort of get better theoretical guarantees, but we're not going to do that.",
            "We're going to have this very simple method here.",
            "So how does this work?",
            "How does a special Monte Carlo method work?",
            "You basically have some prior over this model space, which is sort of in our case is going to be uniform, so you have even numerated all these different kernels that you want to look at, and then you put some prior over them.",
            "So is the probability of seeing them was the probability of any of them being the correct kernel without any knowledge.",
            "And then we're going to do is draw a sample of some size from that prior, and that's going to be these little yellow dots here.",
            "And then basically we're going to calculate something called await, awaiting for each each one of these kernels.",
            "And that's going to be generated from this sort of messy looking thing.",
            "This is basically just.",
            "So what's gonna be the likelihood of the observations that we make given this particular kernel settings times the prior and then normalized?",
            "We're going to make a really big simplification here that.",
            "I think has some interesting implications as well.",
            "We're going to use average reward instead of the data likelihood.",
            "So what this is going to be is sort of more like a predictive likelihood, right?",
            "So kind of how well is this kernel going to do so?",
            "Basically what you can imagine this weight being is how well does this kernel do in this particular domain compared to all the others I've tested and we're going to wait them relatively low."
        ],
        [
            "And so then, after this we are going to re sample.",
            "We have a resampling step which basically for each of these dots we're going to pretend that we're going to take this empirical distribution here and we're going to sample from it.",
            "And so the larger dots with the higher weights are going to naturally generate more samples and the smaller dots are going to disappear.",
            "Something like that, right?",
            "And then some of them are going to sample that also.",
            "Then they're going to disappear completely from the from this set from this empirical distribution and then.",
            "Out.",
            "Hello.",
            "So 1 interesting Step 1.",
            "Interesting thing here is that.",
            "We can actually, so each one of these points.",
            "Remember, we're accumulating this Dictionary of of training points, which is defining a Gaussian process.",
            "We can actually inherit here, and so like this point can just give all of the data that is given to the new to whatever children that it produces, and in doing this we can actually save data, so we don't have to throw away all the training that we've had up until now, and so this familiar with methods like need plus que this kind of overcomes things.",
            "So why you have to retrain from scratch every time you come up with a new architecture?"
        ],
        [
            "Anyway, so finally what we're going to do is apply something called a transition kernel, which is basically going to sort of take.",
            "Take the kernels that we have already found and their weightings, and then kind of perturb them slightly and it's going to it's going to do so in some whatever way that we think is a good way to search through this model space.",
            "So depends what your parameter structure looks like.",
            "Well, what's the best way to do this?",
            "But you can imagine it just being some kind of random noise on the parameters.",
            "If you have a smooth space for example.",
            "And then we're going to have.",
            "This is gonna be our new sample for the next state and we're just going to keep repeating this over and over again.",
            "This is going to naturally sort of bias our exploration towards areas in the model space where we saw good performance, and since it's sequential method, it can actually.",
            "If your environment is changing overtime, or if so, yeah, it's ending overtime, then you can actually sort of adapt."
        ],
        [
            "To this.",
            "Night, so we basically around this time.",
            "We we ran GPS or so, which is basically Gaussian process reinforcement learning in a model freeze.",
            "It kind of sort of sarsa sense.",
            "And we gave it sort of a grid search just over its kernel parameters, and so we kind of we had no more integrated some like space.",
            "Overall the overall premise things could be and we sort of tried all of a man pick the best and that's resulting to show.",
            "And then we have this other other two methods.",
            "This TCR L which is going to be Gaussian process reinforcement learning.",
            "Plus this model selection step that I just this sequential Monte Carlo model selection step that I just showed you.",
            "And then we have two variants were going to RL and this EP TCR L which has that trick that I showed you of saving the dictionary points at each at each step and we're going to do in these in these two methods with model selection we're actually going to spend the kernel set, won't expand the set of kernel parameters that exist.",
            "So for example, if we have this kind of Gaussian kernel here, that would GPS will be using.",
            "It has one parameter which is the variance parameter.",
            "Whereas we're going to do in our Corel is sort of expand out of all the dimensions and you're going to have something kind of like.",
            "You're going to have basically automatic sort of relevance detection here.",
            "It's like basically you're expanding out the we're having one parameter per dimension of your state space, and sort of.",
            "I guess the simplest way you can kind of make these kernels more complex and make it.",
            "And for example, for high dimensional state space is it's really hard to grid search even even on this kind of kernel.",
            "So even on this kind of trivial.",
            "My."
        ],
        [
            "More complex version.",
            "So around three demands, the first is of course mountain car, and these are the kind of results that we see.",
            "So.",
            "Basically, in all cases EPR KRL does significantly better than the other two methods.",
            "And it's fairly consistently close to some of the, I guess."
        ],
        [
            "Performance cars out there.",
            "Um the next?",
            "The next thing we tested, the next problem that we looked at with sailboat steering, which is basically you're trying to Orient sailboat, I think.",
            "And you need to say that in One Direction, and there's some kind of wind, and this is a slightly more complicated domain has three dimensional state space and the action space actually is continuous.",
            "But we discretize it.",
            "We discussed as it fairly, fairly finally, so we have about 430 different actions that are possible, and again, what we see here is that I think in all cases except for this particular Gaussian kernel that I showed you before.",
            "These are also significantly different, different significant differences in performance, and in particular APR KRL is doing."
        ],
        [
            "Much better.",
            "So the final domain that we looked at was sort of slightly less of a Toyota Min.",
            "Now we're looking at this game called Capture Go, which is sort of a very simple variant of go where you spend on very small board and the goal is just to surround the opponent stones and or some group of them.",
            "And once you do that you win and so this is a 25 dimensional state space, it's China I guess.",
            "And there's 25 possible actions maximum.",
            "We're playing it so random opponent so it's also stochastic domain an again we see here.",
            "These are again in all cases significant differences.",
            "And if you are, Carol is doing much better than GP Sorcerer."
        ],
        [
            "OK.",
            "So basically that was it.",
            "We introduced this Gaussian process.",
            "RL Ranger scouts process our Elan.",
            "We showed how the kernel can be selected efficiently on line and such that it yields significantly better performance, least empirically.",
            "And that's practical even in the case of having 25 parameters."
        ],
        [
            "I think it's going to be, thanks.",
            "Time for a few quests."
        ],
        [
            "I think you said that amount for results.",
            "Approximately or in the same range as the best, yeah."
        ],
        [
            "As good, I'm just curious why?",
            "So the reason it doesn't do as well.",
            "I guess.",
            "For one thing we sort of fixed the only five slides, but we have fixed the parameters for reinforcement learning parameters we fix.",
            "The Epsilon triathlon greedy action selection.",
            "Here we should fix epsilon to be very low I think compared to other methods that have done.",
            "Better on this and we don't anneal either, so we set it very low initially and we just kind of stick with that.",
            "And so in the time frame that we're looking to do this, it never actually gets beyond.",
            "I think negative 60 or something like that, which is not quite as good.",
            "I think is the sum of the better results, but mainly the main reason we did that matters.",
            "We fixed kind of parameters was so that we could fix them across all the runs across all the domains and we didn't want to worry about sort of of optimizing over them as well.",
            "The Y axis is the final performance of the explorations.",
            "This is the.",
            "So basically, asymptotically performance on the last think 100 episodes after after training.",
            "No, it hasn't posted.",
            "Online model selection.",
            "What's the status of an methods in the literature?",
            "Are there other methods for model selection an regression?",
            "So I'm not too familiar with methods that do it in the online case.",
            "I think this is sort of maybe some of the first steps in that direction.",
            "Um?",
            "There are.",
            "I mean, there's a.",
            "There's a whole slew of methods, I think, from statistics, for example, that.",
            "Have I think nicer properties then this kind of thing that we showed, but they typically don't aren't adapted to the online case.",
            "Trevor teach me how you do this last election.",
            "Yeah.",
            "And then one after each other you are waiting there.",
            "This is this building.",
            "Yeah, so you can do it that way.",
            "You can do it.",
            "You can also paralyze depending on sort of what your definition of online is going to be.",
            "If you have if you can do parallel evaluations, this will support that if you need to do sequential evaluations, then you can also yeah.",
            "You would need to switch from mom or Dad to the next right, and so there's an issue there.",
            "I guess a slight complication when you do that full online case that I didn't.",
            "I kind of glossed over here.",
            "These are parallel, right?",
            "Yeah, it sort of makes it easier for us to talk about, right?",
            "So the.",
            "Definitely can do this, right, right?",
            "So yeah, I guess the slight complication is that if you if I ate something earlier and you're comparing to get something those evaluated later, there's some kind of natural performance difference just because of the data you're accumulating, and so you need to kind of control for that.",
            "And so it's actually pretty easy to do 'cause you're you're controlling.",
            "The size of this dictionary, and so you can say like how much sort of how much training had compared to each other.",
            "Do a quick question to ask about the arm is going to switch, so the kernel here should more or less information about them.",
            "Right?",
            "Yeah, in fact we had some wonderful.",
            "Some results I didn't get to show that they are in the paper about sort of what?",
            "What?",
            "What are the learned kernels and what kind of the properties of the kernels that are learned?",
            "We do like what it was there any structure for example, and also was there.",
            "Are they transferable to other like they kind of overfitting the data that they've seen in more general?",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, thanks for coming after lunch.",
                    "label": 0
                },
                {
                    "sent": "My name is Joe Risinger and I'm going to talk a little bit about some work that I've done at University, Texas with Peter Stone an Risto Mcaleenan.",
                    "label": 0
                },
                {
                    "sent": "And basically what I'm going to talking about is Gaussian process, reinforcement learning, and sort of how to.",
                    "label": 0
                },
                {
                    "sent": "What's the best way to pick something called the kernel?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is important for this?",
                    "label": 1
                },
                {
                    "sent": "So here is the entire summary of the talk.",
                    "label": 1
                },
                {
                    "sent": "I mean, Gaussian process enforcement learning.",
                    "label": 0
                },
                {
                    "sent": "There's this thing called the kernel, which is which specifies something called the prior covariance.",
                    "label": 0
                },
                {
                    "sent": "Choosing this.",
                    "label": 0
                },
                {
                    "sent": "Huge impact on performance and so we need some ways of choosing them sufficiently during learning, and in particular, how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we choose it online and sort of fit in with the RL paradigm?",
                    "label": 1
                },
                {
                    "sent": "So here's the talk.",
                    "label": 0
                },
                {
                    "sent": "Outline 1st I'm going to talk about RL for like maybe 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "I'm going to choose Gaussian processes for a bit longer and then running into this problem of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Choosing the kernel.",
                    "label": 0
                },
                {
                    "sent": "Alright, So what is reinforcement learning?",
                    "label": 1
                },
                {
                    "sent": "Hopefully all of you have seen slides like this before.",
                    "label": 0
                },
                {
                    "sent": "I'm going to present what's probably.",
                    "label": 0
                },
                {
                    "sent": "I guess you can consider the model for a value function sort of story for reinforcement learning, although I know there's a ton of others.",
                    "label": 0
                },
                {
                    "sent": "So the basic problem is that we have some sequential decision tasks and we have some agent that's is trying to act in some world.",
                    "label": 0
                },
                {
                    "sent": "Ann has basically some state that it's in, and then some set of actions that can perform.",
                    "label": 0
                },
                {
                    "sent": "It picks an action and then it goes to another state and then from there has another set of actions.",
                    "label": 0
                },
                {
                    "sent": "So the whole idea is basically that you're not told after each transition how well you did, but you'll accumulate something called reward.",
                    "label": 0
                },
                {
                    "sent": "Now and then, maybe say I got to the goal of some maze or something like that.",
                    "label": 0
                },
                {
                    "sent": "I might get a reward for doing that, and then I need to figure out what was the sequence of actions that I took in which States and how.",
                    "label": 0
                },
                {
                    "sent": "How did that contribute to me getting this reward?",
                    "label": 0
                },
                {
                    "sent": "And so basically, we're going to model this task as a set of States and then transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "So given that I took some action in some state, there's some probability of seeing some other state of the world, and our goal is to do is to find a policy which says for each state.",
                    "label": 1
                },
                {
                    "sent": "For each, for each element in this in this thing called in this State cross action space.",
                    "label": 0
                },
                {
                    "sent": "What is with probability?",
                    "label": 0
                },
                {
                    "sent": "Should I?",
                    "label": 0
                },
                {
                    "sent": "Should that be observed?",
                    "label": 0
                },
                {
                    "sent": "And basically what we want to do is to maximize something that we're going to call the value.",
                    "label": 0
                },
                {
                    "sent": "Function, which is the expected discounted return of this policy and what is expected discounted channel.",
                    "label": 1
                },
                {
                    "sent": "Basically it's if I'm in.",
                    "label": 0
                },
                {
                    "sent": "If I can transition to some state X.",
                    "label": 0
                },
                {
                    "sent": "What is the expected long-term reward?",
                    "label": 0
                },
                {
                    "sent": "So not just the reward for being in that state, but the reward for any state I can get to subsequently after that, properly discounted for sort of this time horizon, and so that's given by these last few equations.",
                    "label": 0
                },
                {
                    "sent": "So hopefully everyone's sort of at least comfortable at that story.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this value function thing basically.",
                    "label": 1
                },
                {
                    "sent": "You can think of it as sort of a regression function that we're trying to learn an where the dependent variables are basically the state class action space, and then the independent variable is basically what is the?",
                    "label": 0
                },
                {
                    "sent": "What is the value of the expected?",
                    "label": 1
                },
                {
                    "sent": "This kind of return for choosing that action in that state, there's a lot of different ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "For example, you have these tabular methods where you literally just record every possible state in action.",
                    "label": 0
                },
                {
                    "sent": "There is an.",
                    "label": 0
                },
                {
                    "sent": "You sort of, you compute.",
                    "label": 0
                },
                {
                    "sent": "Sufficient statistics for the value and another thing you can do is sort of use something called function approximation or basically what we call regression in that you pick kind of some class of functions.",
                    "label": 1
                },
                {
                    "sent": "And then you say so I make some observations and then I sort of interpolate between those observations and say that's my value function.",
                    "label": 0
                },
                {
                    "sent": "So one really nice way of doing this is using something called a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And so, well, what is a Gaussian process, right?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, the idea is that we want to do regression, but we don't want to make any complexity assumptions without having seen the data and so basically what we're going to having is sort of a regression that has a number of parameters equal to the number of data points that we've observed, and so basically what happens is the model becomes more and more complex.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we observe more data and the fit becomes.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Better I also become more confident sort of in how and what the fit is going to be.",
                    "label": 0
                },
                {
                    "sent": "And so typically when you see stuff like this when you have a lot of data points, if you think of like polynomial interpolation, you have this idea of overfitting where you know it gets this really wildly wiggly things that tries to fit all the data points exactly.",
                    "label": 0
                },
                {
                    "sent": "By using Gaussian process it's a Bayesian method, so we have this prior which is going to say.",
                    "label": 1
                },
                {
                    "sent": "I'm just going to act as a regularizer and sort of make it less likely that we see a very wiggly functions, and so we're only going to look at these kind of smooth looking ones.",
                    "label": 0
                },
                {
                    "sent": "So it's like a smoother, right?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So how does that Gaussian process work?",
                    "label": 0
                },
                {
                    "sent": "Basically, the main point is that you have some set of data, fits nonparametric so you have some set of data which are going to keep.",
                    "label": 0
                },
                {
                    "sent": "And then for every day to every pair of data points, you're going to compute the covariance, which you're going to, which is going to be computed by.",
                    "label": 0
                },
                {
                    "sent": "In this matrix K. And its entry in this metric scale is going to be computed by your kernel, which is just literally specifies a priority.",
                    "label": 0
                },
                {
                    "sent": "What you think, how you think 2 states are going to be similar in terms of their values.",
                    "label": 0
                },
                {
                    "sent": "And so now using this big matrix K and some.",
                    "label": 0
                },
                {
                    "sent": "Some observations Y which is going to be the reward.",
                    "label": 0
                },
                {
                    "sent": "We can actually take an unknown attest pointwise star, and we can actually represent it using Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "This is just a normal distribution where all of the observed data points have their covariances in K and then these small bold case are the covariances of the observed data points.",
                    "label": 0
                },
                {
                    "sent": "With the new test point.",
                    "label": 0
                },
                {
                    "sent": "Why and we can use something called the?",
                    "label": 0
                },
                {
                    "sent": "Dallas mark up them to compute the posteriors for this and so basically all this is saying the sort of important summary of this is that given some observations, for example rewards.",
                    "label": 0
                },
                {
                    "sent": "And then K, which specifies sort of how those states that generated these observations were related.",
                    "label": 0
                },
                {
                    "sent": "Are related given that we can compute the expectation of his wife star, so some new test point we can compute what the value is at this new point.",
                    "label": 0
                },
                {
                    "sent": "We haven't had not observed, but we can also compute the variance, which is sort of our confidence in how how good that prediction is a very high variance, for example, would mean that maybe we don't have good confidence here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Important thing basically from all this math is these two is these two objects.",
                    "label": 0
                },
                {
                    "sent": "The set of data.",
                    "label": 0
                },
                {
                    "sent": "And the kernel function.",
                    "label": 0
                },
                {
                    "sent": "This is all we need to compute a Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "On and so the data is easy.",
                    "label": 0
                },
                {
                    "sent": "Where does the kernel come from?",
                    "label": 0
                },
                {
                    "sent": "That sort of the point of this talk?",
                    "label": 0
                },
                {
                    "sent": "I should also mention in passing that the view I gave of Gaussian processes is sort of the general regression view, and there's been.",
                    "label": 1
                },
                {
                    "sent": "There is a elegant but sort of nontrivial extension of this of this object to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "That kind of takes into account the.",
                    "label": 0
                },
                {
                    "sent": "Sort of how you backup values along the.",
                    "label": 0
                },
                {
                    "sent": "Along the sort of the chain of observation.",
                    "label": 0
                },
                {
                    "sent": "So basically, if I observe some reward signal at the end, then I need to back up that reward.",
                    "label": 0
                },
                {
                    "sent": "Across all of the states that I visited and I need to do that in a properly discounted way.",
                    "label": 0
                },
                {
                    "sent": "And so this just takes that into account.",
                    "label": 0
                },
                {
                    "sent": "This DRL which is the Gaussian process reinforcement learning, also does something called sparsification which if you remember so we're saving all of our data points to complete this Gaussian process and then we're computing this kernel function over them.",
                    "label": 0
                },
                {
                    "sent": "But the problem with saving all the data points is that reinforcement learning kind of setting.",
                    "label": 0
                },
                {
                    "sent": "There's just way too much data to save, so we need to sort of pick and choose.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi, so this kernel function sort of what?",
                    "label": 0
                },
                {
                    "sent": "What does this control right?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to show.",
                    "label": 0
                },
                {
                    "sent": "Here are some examples drawn of functions that are drawn from the Gaussian process prior.",
                    "label": 0
                },
                {
                    "sent": "So this is basically you can imagine the kind of functions you would see given some particular kernel instantiation.",
                    "label": 0
                },
                {
                    "sent": "Like this is like sort of a priority.",
                    "label": 0
                },
                {
                    "sent": "What's in there and then given some conditioned on some data.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of function this is kind of fit that I get back out and so here we have a much sort of wiggler.",
                    "label": 0
                },
                {
                    "sent": "Prior and so it's going to make this sort of fit that actually tries to go through all the data points and then has these kinds of properties, and then again first maybe something intermediate.",
                    "label": 0
                },
                {
                    "sent": "We get something like this.",
                    "label": 0
                },
                {
                    "sent": "So sort of the properties of the learn function, and in particular the generalization properties of that function depend really critically on the kernel, and so it's not just these properties like smoothness like like what I showed you here, but you can even build in more knowledge like such that maybe I know that there are some symmetries or some kind of structure in my domain, and so I know a priority that some states over here and some states over here may be correlated or salmon.",
                    "label": 0
                },
                {
                    "sent": "And navigation domain where I know that nearby states are correlated, so this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kernel is basically defining a metric over.",
                    "label": 1
                },
                {
                    "sent": "Here's your joint state in action space, and it's sort of saying.",
                    "label": 0
                },
                {
                    "sent": "Again, I'll priore what, how, how much.",
                    "label": 0
                },
                {
                    "sent": "How related are two States and so if you know, for example, two states are nearby in a map, then maybe their values are going to be very close by right?",
                    "label": 0
                },
                {
                    "sent": "So we actually build these kinds of intuitions and via the kernel.",
                    "label": 0
                },
                {
                    "sent": "And so and so, where do these kernels come from then?",
                    "label": 0
                },
                {
                    "sent": "So, like you can imagine lots of different ways of doing this, there's lots of different smoothers.",
                    "label": 0
                },
                {
                    "sent": "There's lots of different parametric forms we need some way to do a model selection step, which is just to take to kind of search through this space of kernels and then sort of pick the best one for the task at hand.",
                    "label": 1
                },
                {
                    "sent": "And there are basically model selection steps and model selection criterion for doing this.",
                    "label": 0
                },
                {
                    "sent": "Cross validations are really simple and that most people probably know, but they weren't really designed to work online.",
                    "label": 1
                },
                {
                    "sent": "That is sort of you don't have a way.",
                    "label": 0
                },
                {
                    "sent": "Constellation doesn't say like if this kernel did well in this kernel, didn't do well.",
                    "label": 0
                },
                {
                    "sent": "What should I do next right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't really suggest that you just sort of enumerate and then you run over the entire set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to reduce or reintroduce is a.",
                    "label": 0
                },
                {
                    "sent": "Really simple method that uses sequential Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Especially particle filter.",
                    "label": 0
                },
                {
                    "sent": "And we're going to run this over the over some kernel space.",
                    "label": 0
                },
                {
                    "sent": "This model space, and in particular we're going to.",
                    "label": 1
                },
                {
                    "sent": "We're going to have these kernels Theta and this model space big data and we're not going to put any restrictions really on what this space can look like, so we're going to.",
                    "label": 0
                },
                {
                    "sent": "This is a really simple method, and if we if we added additional assumptions about the model space looked like we could actually do better.",
                    "label": 0
                },
                {
                    "sent": "Sort of get better theoretical guarantees, but we're not going to do that.",
                    "label": 0
                },
                {
                    "sent": "We're going to have this very simple method here.",
                    "label": 0
                },
                {
                    "sent": "So how does this work?",
                    "label": 0
                },
                {
                    "sent": "How does a special Monte Carlo method work?",
                    "label": 0
                },
                {
                    "sent": "You basically have some prior over this model space, which is sort of in our case is going to be uniform, so you have even numerated all these different kernels that you want to look at, and then you put some prior over them.",
                    "label": 0
                },
                {
                    "sent": "So is the probability of seeing them was the probability of any of them being the correct kernel without any knowledge.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do is draw a sample of some size from that prior, and that's going to be these little yellow dots here.",
                    "label": 0
                },
                {
                    "sent": "And then basically we're going to calculate something called await, awaiting for each each one of these kernels.",
                    "label": 0
                },
                {
                    "sent": "And that's going to be generated from this sort of messy looking thing.",
                    "label": 0
                },
                {
                    "sent": "This is basically just.",
                    "label": 0
                },
                {
                    "sent": "So what's gonna be the likelihood of the observations that we make given this particular kernel settings times the prior and then normalized?",
                    "label": 0
                },
                {
                    "sent": "We're going to make a really big simplification here that.",
                    "label": 0
                },
                {
                    "sent": "I think has some interesting implications as well.",
                    "label": 0
                },
                {
                    "sent": "We're going to use average reward instead of the data likelihood.",
                    "label": 1
                },
                {
                    "sent": "So what this is going to be is sort of more like a predictive likelihood, right?",
                    "label": 0
                },
                {
                    "sent": "So kind of how well is this kernel going to do so?",
                    "label": 0
                },
                {
                    "sent": "Basically what you can imagine this weight being is how well does this kernel do in this particular domain compared to all the others I've tested and we're going to wait them relatively low.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so then, after this we are going to re sample.",
                    "label": 0
                },
                {
                    "sent": "We have a resampling step which basically for each of these dots we're going to pretend that we're going to take this empirical distribution here and we're going to sample from it.",
                    "label": 0
                },
                {
                    "sent": "And so the larger dots with the higher weights are going to naturally generate more samples and the smaller dots are going to disappear.",
                    "label": 0
                },
                {
                    "sent": "Something like that, right?",
                    "label": 0
                },
                {
                    "sent": "And then some of them are going to sample that also.",
                    "label": 0
                },
                {
                    "sent": "Then they're going to disappear completely from the from this set from this empirical distribution and then.",
                    "label": 0
                },
                {
                    "sent": "Out.",
                    "label": 0
                },
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "So 1 interesting Step 1.",
                    "label": 0
                },
                {
                    "sent": "Interesting thing here is that.",
                    "label": 0
                },
                {
                    "sent": "We can actually, so each one of these points.",
                    "label": 0
                },
                {
                    "sent": "Remember, we're accumulating this Dictionary of of training points, which is defining a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "We can actually inherit here, and so like this point can just give all of the data that is given to the new to whatever children that it produces, and in doing this we can actually save data, so we don't have to throw away all the training that we've had up until now, and so this familiar with methods like need plus que this kind of overcomes things.",
                    "label": 0
                },
                {
                    "sent": "So why you have to retrain from scratch every time you come up with a new architecture?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anyway, so finally what we're going to do is apply something called a transition kernel, which is basically going to sort of take.",
                    "label": 1
                },
                {
                    "sent": "Take the kernels that we have already found and their weightings, and then kind of perturb them slightly and it's going to it's going to do so in some whatever way that we think is a good way to search through this model space.",
                    "label": 0
                },
                {
                    "sent": "So depends what your parameter structure looks like.",
                    "label": 0
                },
                {
                    "sent": "Well, what's the best way to do this?",
                    "label": 0
                },
                {
                    "sent": "But you can imagine it just being some kind of random noise on the parameters.",
                    "label": 0
                },
                {
                    "sent": "If you have a smooth space for example.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to have.",
                    "label": 0
                },
                {
                    "sent": "This is gonna be our new sample for the next state and we're just going to keep repeating this over and over again.",
                    "label": 0
                },
                {
                    "sent": "This is going to naturally sort of bias our exploration towards areas in the model space where we saw good performance, and since it's sequential method, it can actually.",
                    "label": 1
                },
                {
                    "sent": "If your environment is changing overtime, or if so, yeah, it's ending overtime, then you can actually sort of adapt.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To this.",
                    "label": 0
                },
                {
                    "sent": "Night, so we basically around this time.",
                    "label": 0
                },
                {
                    "sent": "We we ran GPS or so, which is basically Gaussian process reinforcement learning in a model freeze.",
                    "label": 0
                },
                {
                    "sent": "It kind of sort of sarsa sense.",
                    "label": 0
                },
                {
                    "sent": "And we gave it sort of a grid search just over its kernel parameters, and so we kind of we had no more integrated some like space.",
                    "label": 1
                },
                {
                    "sent": "Overall the overall premise things could be and we sort of tried all of a man pick the best and that's resulting to show.",
                    "label": 0
                },
                {
                    "sent": "And then we have this other other two methods.",
                    "label": 0
                },
                {
                    "sent": "This TCR L which is going to be Gaussian process reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Plus this model selection step that I just this sequential Monte Carlo model selection step that I just showed you.",
                    "label": 0
                },
                {
                    "sent": "And then we have two variants were going to RL and this EP TCR L which has that trick that I showed you of saving the dictionary points at each at each step and we're going to do in these in these two methods with model selection we're actually going to spend the kernel set, won't expand the set of kernel parameters that exist.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we have this kind of Gaussian kernel here, that would GPS will be using.",
                    "label": 0
                },
                {
                    "sent": "It has one parameter which is the variance parameter.",
                    "label": 0
                },
                {
                    "sent": "Whereas we're going to do in our Corel is sort of expand out of all the dimensions and you're going to have something kind of like.",
                    "label": 0
                },
                {
                    "sent": "You're going to have basically automatic sort of relevance detection here.",
                    "label": 0
                },
                {
                    "sent": "It's like basically you're expanding out the we're having one parameter per dimension of your state space, and sort of.",
                    "label": 0
                },
                {
                    "sent": "I guess the simplest way you can kind of make these kernels more complex and make it.",
                    "label": 0
                },
                {
                    "sent": "And for example, for high dimensional state space is it's really hard to grid search even even on this kind of kernel.",
                    "label": 0
                },
                {
                    "sent": "So even on this kind of trivial.",
                    "label": 0
                },
                {
                    "sent": "My.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More complex version.",
                    "label": 0
                },
                {
                    "sent": "So around three demands, the first is of course mountain car, and these are the kind of results that we see.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically, in all cases EPR KRL does significantly better than the other two methods.",
                    "label": 0
                },
                {
                    "sent": "And it's fairly consistently close to some of the, I guess.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance cars out there.",
                    "label": 0
                },
                {
                    "sent": "Um the next?",
                    "label": 0
                },
                {
                    "sent": "The next thing we tested, the next problem that we looked at with sailboat steering, which is basically you're trying to Orient sailboat, I think.",
                    "label": 0
                },
                {
                    "sent": "And you need to say that in One Direction, and there's some kind of wind, and this is a slightly more complicated domain has three dimensional state space and the action space actually is continuous.",
                    "label": 0
                },
                {
                    "sent": "But we discretize it.",
                    "label": 0
                },
                {
                    "sent": "We discussed as it fairly, fairly finally, so we have about 430 different actions that are possible, and again, what we see here is that I think in all cases except for this particular Gaussian kernel that I showed you before.",
                    "label": 0
                },
                {
                    "sent": "These are also significantly different, different significant differences in performance, and in particular APR KRL is doing.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much better.",
                    "label": 0
                },
                {
                    "sent": "So the final domain that we looked at was sort of slightly less of a Toyota Min.",
                    "label": 0
                },
                {
                    "sent": "Now we're looking at this game called Capture Go, which is sort of a very simple variant of go where you spend on very small board and the goal is just to surround the opponent stones and or some group of them.",
                    "label": 0
                },
                {
                    "sent": "And once you do that you win and so this is a 25 dimensional state space, it's China I guess.",
                    "label": 0
                },
                {
                    "sent": "And there's 25 possible actions maximum.",
                    "label": 0
                },
                {
                    "sent": "We're playing it so random opponent so it's also stochastic domain an again we see here.",
                    "label": 0
                },
                {
                    "sent": "These are again in all cases significant differences.",
                    "label": 0
                },
                {
                    "sent": "And if you are, Carol is doing much better than GP Sorcerer.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically that was it.",
                    "label": 0
                },
                {
                    "sent": "We introduced this Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "RL Ranger scouts process our Elan.",
                    "label": 0
                },
                {
                    "sent": "We showed how the kernel can be selected efficiently on line and such that it yields significantly better performance, least empirically.",
                    "label": 0
                },
                {
                    "sent": "And that's practical even in the case of having 25 parameters.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think it's going to be, thanks.",
                    "label": 0
                },
                {
                    "sent": "Time for a few quests.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think you said that amount for results.",
                    "label": 0
                },
                {
                    "sent": "Approximately or in the same range as the best, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As good, I'm just curious why?",
                    "label": 0
                },
                {
                    "sent": "So the reason it doesn't do as well.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "For one thing we sort of fixed the only five slides, but we have fixed the parameters for reinforcement learning parameters we fix.",
                    "label": 0
                },
                {
                    "sent": "The Epsilon triathlon greedy action selection.",
                    "label": 0
                },
                {
                    "sent": "Here we should fix epsilon to be very low I think compared to other methods that have done.",
                    "label": 0
                },
                {
                    "sent": "Better on this and we don't anneal either, so we set it very low initially and we just kind of stick with that.",
                    "label": 0
                },
                {
                    "sent": "And so in the time frame that we're looking to do this, it never actually gets beyond.",
                    "label": 0
                },
                {
                    "sent": "I think negative 60 or something like that, which is not quite as good.",
                    "label": 0
                },
                {
                    "sent": "I think is the sum of the better results, but mainly the main reason we did that matters.",
                    "label": 0
                },
                {
                    "sent": "We fixed kind of parameters was so that we could fix them across all the runs across all the domains and we didn't want to worry about sort of of optimizing over them as well.",
                    "label": 0
                },
                {
                    "sent": "The Y axis is the final performance of the explorations.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "So basically, asymptotically performance on the last think 100 episodes after after training.",
                    "label": 0
                },
                {
                    "sent": "No, it hasn't posted.",
                    "label": 0
                },
                {
                    "sent": "Online model selection.",
                    "label": 0
                },
                {
                    "sent": "What's the status of an methods in the literature?",
                    "label": 0
                },
                {
                    "sent": "Are there other methods for model selection an regression?",
                    "label": 0
                },
                {
                    "sent": "So I'm not too familiar with methods that do it in the online case.",
                    "label": 0
                },
                {
                    "sent": "I think this is sort of maybe some of the first steps in that direction.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a whole slew of methods, I think, from statistics, for example, that.",
                    "label": 0
                },
                {
                    "sent": "Have I think nicer properties then this kind of thing that we showed, but they typically don't aren't adapted to the online case.",
                    "label": 0
                },
                {
                    "sent": "Trevor teach me how you do this last election.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And then one after each other you are waiting there.",
                    "label": 0
                },
                {
                    "sent": "This is this building.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can do it that way.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "You can also paralyze depending on sort of what your definition of online is going to be.",
                    "label": 0
                },
                {
                    "sent": "If you have if you can do parallel evaluations, this will support that if you need to do sequential evaluations, then you can also yeah.",
                    "label": 0
                },
                {
                    "sent": "You would need to switch from mom or Dad to the next right, and so there's an issue there.",
                    "label": 0
                },
                {
                    "sent": "I guess a slight complication when you do that full online case that I didn't.",
                    "label": 0
                },
                {
                    "sent": "I kind of glossed over here.",
                    "label": 0
                },
                {
                    "sent": "These are parallel, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it sort of makes it easier for us to talk about, right?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Definitely can do this, right, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, I guess the slight complication is that if you if I ate something earlier and you're comparing to get something those evaluated later, there's some kind of natural performance difference just because of the data you're accumulating, and so you need to kind of control for that.",
                    "label": 0
                },
                {
                    "sent": "And so it's actually pretty easy to do 'cause you're you're controlling.",
                    "label": 0
                },
                {
                    "sent": "The size of this dictionary, and so you can say like how much sort of how much training had compared to each other.",
                    "label": 0
                },
                {
                    "sent": "Do a quick question to ask about the arm is going to switch, so the kernel here should more or less information about them.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact we had some wonderful.",
                    "label": 0
                },
                {
                    "sent": "Some results I didn't get to show that they are in the paper about sort of what?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "What are the learned kernels and what kind of the properties of the kernels that are learned?",
                    "label": 0
                },
                {
                    "sent": "We do like what it was there any structure for example, and also was there.",
                    "label": 0
                },
                {
                    "sent": "Are they transferable to other like they kind of overfitting the data that they've seen in more general?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}