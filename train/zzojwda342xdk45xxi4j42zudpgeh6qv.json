{
    "id": "zzojwda342xdk45xxi4j42zudpgeh6qv",
    "title": "Outlier Detection Techniques",
    "info": {
        "author": [
            "Peer Kroger, Ludwig-Maximilians Universit\u00e4t"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/kdd2010_krogel_odt/",
    "segmentation": [
        [
            "OK, maybe we start right on time.",
            "My I have a tight schedule today because my flight back to Munich is tonight at eight 8:00 PM.",
            "So I have to worry about it a little bit, so welcome to the tutorial on outlier detection techniques.",
            "My name is Pierre Cougar and I present this tutorial for you on behalf of Hospital Keegan.",
            "My boss and automaker Arthur is also around here, but he is involved in a workshop this afternoon so but he will stay the other day.",
            "So if you have any questions coming to your mind afterwards, not today, then just go to Ottawa.",
            "I hope he will show up at least shortly.",
            "At the coffee break, so you see his face and can talk to him.",
            "I don't have a watch, so somebody has to watch the Clock because I think at three we should make coffee break.",
            "So maybe one of you could.",
            "Have a look on that or have an eye on this issue.",
            "OK so."
        ],
        [
            "Some general issues before we start, please feel free to ask any questions at anytime I want to make it a little bit casual so you can.",
            "We can discuss issues here.",
            "If you have questions, just interrupt me and the general aim of this tutorial is yeah to get more or less a big picture of outlier detection, which is a broad area of research so I won't cover all details or methods or algorithms here.",
            "But we try to give you.",
            "Kind of big picture, hopefully it works.",
            "And the big picture is not in terms of listing some methods and algorithms.",
            "You will get a list of methods and algorithms, but the main point here is in terms of getting a big picture in in terms of basic approaches and modeling approaches for outlier detection.",
            "And as I said some, some algorithms will also be provided.",
            "But the selection is quite arbitrary, so if you if your favorite algorithm is not on the list, please don't mind OK. Yeah.",
            "And well, usually these slides should be made available through the KDD at the website, but I've heard about that there were some problems.",
            "So just visit our website.",
            "Maybe I skip to the first page here.",
            "The website at www.dps.",
            "Stands for database systems at."
        ],
        [
            "The Institute for Informatics at the Munich.",
            "And then you will see.",
            "Somewhere on our home page is the latest slides, hopefully at the end of this week or.",
            "At the beginning of next week."
        ],
        [
            "OK, so let's start, yeah.",
            "Pune.",
            "Just navigate to a 2 hour my personal website.",
            "You will find my personal website on this on this side and then or autosar to us home page and then you will find this.",
            "OK I will try to make it very clearly OK. OK, so let's start with a short discussion on what is an outlier?",
            "What's what's all about here?",
            "And mostly any paper that deals with outlier detection sites.",
            "The definition of Hawkins.",
            "Who said simply says that an author is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.",
            "So Hawkins is statistics.",
            "So we had a statistical into our statistics based intuition on this task.",
            "And yeah, if we take a closer look on this on this definition, we see that he or other colleagues also assume that the normal data.",
            "Should not be outliers.",
            "Usually follow a special mechanism that generates these normal data points or objects?",
            "For example, some given statistical process and the outliers are those objects that deviate from these generating mechanisms.",
            "So maybe are generated by a different statistical process or whatever.",
            "Let's take a."
        ],
        [
            "Closer look by considering example.",
            "This is the case of Mr. Headlam versus Mrs. Headlon, which was discussed by Barnett, and a very interesting paper on outlier detection.",
            "What was the point?",
            "So yeah, the birth of a child to Miss Headlam happened strangely.",
            "349 days after Mr had left for military service.",
            "And if you consider that the average human gas station.",
            "Is about 40 weeks, so 280 days.",
            "Mr. Headlam has obviously some.",
            "Yeah, that feelings with this and of course thought that this baby is not his baby and the court has to decide if it's possible or not.",
            "And Interestingly that the judges did was they ask the doctors.",
            "Is it possible from medicine point of view?",
            "Is it possible that this baby is Mr Adams baby and the doctor said theoretically it could happen?",
            "So what the judges did was to reject Mr. Adams objective and.",
            "Barnett, in 78 so 30 years ago 30 years later."
        ],
        [
            "Pick that up and analyze this by some statistics.",
            "So what he did was just to take some 30,013.",
            "Sell 1000 something, sorry.",
            "Situations of gestation periods, limitation periods which are plotted here by a histogram and you see that this is.",
            "This follows quite nicely.",
            "A Gaussian distribution around the normal mean of 40 weeks.",
            "And yeah, the case of Helen versus Harlem is 50 weeks.",
            "So from a statistical sense, from a statistical point of view, this is clearly an outlier, but as I said, the doctor said it's theoretically possible so.",
            "We cannot.",
            "Make a difference so.",
            "Taking a closer look to put it in the in the direction of Hawkins definition, what do we have here?",
            "So we have the statistical basis of 13,000 observations of gas station periods.",
            "In blue again, the histogram and green is a Gaussian distribution fitted to this histogram, so this is the underlying process.",
            "The generating mechanism.",
            "And you see.",
            "With a very low probability for the birth of Mr. Mrs Hallam, Adams shines child for being.",
            "Generated by this mechanism, or in other words, to be produced by Mr. Mr headlam.",
            "And in red, this is the assumption of Mr. Adam.",
            "What was the assumption of him?",
            "He said that there was.",
            "At.",
            "The observation.",
            "What's training generated by this mechanism?",
            "Which means that in this case, if you if you have this mechanism here, which is different from from this mechanism, here is is a high, very very high probability of this gas station.",
            "So once again we have in in Hawkins sends one generating mechanism.",
            "Modeled here by Gaussian distribution in green and the outlier was.",
            "Produced by a different mechanism, in this case a red.",
            "The red question distribution."
        ],
        [
            "So, um, obviously nowadays there are better ways to prove if one is the father of a child or not.",
            "Not only outlier detection but some genetic tests, of course, so are there other applications for outlier detection?",
            "Yes, of course there are, so there are many applications for outlier detection, just to name a view.",
            "So fraud detection, for example here you can or you try to detect credit card abuse.",
            "By looking at the at the at the purchasing behavior of users.",
            "In medicine, for example, you could have unusual symptoms or test results that may indicate some diseases, potential health problems, whatever.",
            "In this case, it's interesting that not only the measurement itself, the abnormal value of the measurement itself is important to decide if it's an outlier or not, or if it's.",
            "A potential health problem or not, but also other features of the patient.",
            "For example, gender, age or whatever could be interesting, so you have typically more than one variable more than one attribute per object, per patient, or whatever public health.",
            "Also very common application, for example the occurrence of a particular disease scattered across various hospitals of the city may indicate some problems with the corresponding vaccination program.",
            "Again, here you usually have to consider spatial or also frequency correlations, not only the measurement itself but also other features are important to decide whether an outlier is really an outlier is really a signal or not."
        ],
        [
            "Sports statistics may be you also very familiar with that, so in many sports of course, various parameters are measured.",
            "Four players and you can emulate performances as outstanding and positive or negative sense, of course.",
            "And yeah.",
            "In this case, sometimes players should show only only exceptional values in positive or negative sense.",
            "Only a subset of.",
            "Parameters or combinations of parameters.",
            "So not all parameters, not all values are important here.",
            "Another very important point, application for outlier detection is measurement error detection.",
            "For example, in sensor data you usually have a lot of measurement errors, and here in that case outlier detection could indicate measurement errors so that you can remove, for example the measurements.",
            "Or you can.",
            "Make something out of it, whatever.",
            "Do a special treatment on the for these measurements.",
            "But this is something like two sides of metal, because yeah, you have this very famous statement.",
            "One person's noise could be another one person signal.",
            "That means if you detect an outlier and say this is noise, another person could say no, no this is not noise.",
            "This is a clear signal or this is a knowledge which I want to know don't remove it or whatever.",
            "So outlier detection is quite quite subjective, not really objective.",
            "OK and it heavily depends on the application domains and applications.",
            "If the detected signal detected outliers are really interesting or not."
        ],
        [
            "So I'm coming back to the basic intuition of Hawkins in the light of these applications.",
            "Then we have to see that data is usually multidimensional.",
            "So you have a lot of measures per object or point, and if you recall, in the in the basic model, you just have a 1 dimensional data set.",
            "So in the case of Helen was Helen, you just measure the length of the station.",
            "Just one measure for each.",
            "And but usually this is not the case.",
            "You usually have a multivariate data set.",
            "Usually you are the basic intuition also assumes that you have just one normal generating process, so in this case only the green process is the process that generates the normal objects and normal observations.",
            "But in general there could be more than one normal process.",
            "OK, usually you have a mixture of normal process is and not only one.",
            "Yes, and 3rd animals, sorry, outliers, outliers may represent different classes of generating mechanisms.",
            "So the outlines that the set of outlier points may be very large, and in the basic model usually you assume that outliers are rare observations and not set of outliers are not very large, so these three basic points here should illustrate the basic intuition behind the statistical approach inspired by Hawkins is very limited and.",
            "You need more or.",
            "Other techniques too.",
            "Conquer nowadays data."
        ],
        [
            "So the consequences is that a lot of models and approaches have evolved in the in the past years.",
            "Order to exceed these limitations.",
            "And yeah, I think not that easy to keep track with this evolution.",
            "So one.",
            "Aim of this tutorial, of course, is to try to give you some overview of this.",
            "And what's very important is if you look at specific outlier detection models and approaches, is that.",
            "Each of them usually involves sometimes knew, but very typical assumptions and restrictions and.",
            "Very often these assumptions restrictions are hidden.",
            "They're not explicitly discussed in papers, but usually each paper or each approach has one at least one.",
            "So you have to keep this in mind when we.",
            "Discuss different a different approaches to outlier detection after."
        ],
        [
            "So there are some general application scenarios.",
            "First of all, we have a supervised scenario because in some applications you just have training data of normal and abnormal data.",
            "And in this case you can train a classifier and this is a supervised task.",
            "Usually this is an unbalanced classification problem 'cause you usually have a lot more normal observations in the training data than abnormal observations.",
            "But OK, you have to deal with that.",
            "Second scenarios seem supervised, so in this case usually you have just training data for one class, either the the normal distribution or the abnormal data points.",
            "And you have to learn again some rules or classifiers to do the job.",
            "And finally we have the answer unsupervised scenario where we don't have any training data at hand and in most applications this is the case.",
            "You don't have any training data at hand and in this tutorial we focus on the unsupervised scenario, so you won't find in this.",
            "Tutorial nothing about supervised.",
            "Semi supervised approach is purely unsupervised."
        ],
        [
            "Yeah, if we're talking about unsupervised learning or unsupervised data mining, then maybe you just have to have the question that is, it is outlier detection very related or even similar to clustering.",
            "So are outliers not just the side product of clustering algorithms?",
            "Well that's true on the first side, maybe because many clustering algorithms do not assign all points of course but can account for noise object and you.",
            "Might.",
            "For the option that applying one of those algorithm and retrieve the noise set as outliers.",
            "But there are some problems with this because clustering algorithms are usually optimized to find clusters and not to find outliers so.",
            "It's more orthogonal than it's related, of course, but it's these two problems are very detection.",
            "Clustering are orthogonal, I would say.",
            "And that means that the accuracy of outlier detection depends on how good the clustering algorithm captures the structure of the cluster and not how good it identifies outliers.",
            "And again, a set of many abnormal data objects that are similar to each other that would these these observations would be grouped into one cluster, maybe and are would not be recognized as noise, and in this case you would lose them.",
            "OK, so.",
            "I think these these problems illustrate that we need or that the outlier detection problem is really a problem on its own, and it's not just a side product of clustering."
        ],
        [
            "Yeah, in the following global as I said, we try to give you a big picture so we will focus on three different classification approaches.",
            "Here we try to classify.",
            "Listing approaches and if some approaches that you are acquainted with are not on the list of our in our tutorial here, you should.",
            "Have it easy to on tour classify these approaches also in these classification hierarchy or it's not really a high kyriarchy, it's just three different.",
            "Binary classifications or mostly binary.",
            "So the first one considers the set of of reference objects that are considered to judge the outliners of 1 object.",
            "The second one considers the output of an algorithm.",
            "What is the output of the algorithm and the third classification just considers the concept based on which the outliers are modeled OK and we will.",
            "I will go into more details right now.",
            "But just to note here, we focus on models and methods for Euclidean data, but many of those approaches here can also be used for any data types because usually they don't.",
            "They only require distance.",
            "Measure or similarity measure.",
            "So if you have a similarity similarity measure for your objects, then it.",
            "They need not to be you cleaning."
        ],
        [
            "So getting a little bit more into details about these classifications here, so the first one as a set considers the resolution that is the size of the set of points that are considered in order to judge.",
            "If another points in outline or not, so we have basically global approaches where the reference set contains all other data objects, so one object is judged the outlanes of 1 object object is evaluated on in terms of all other objects in the data set.",
            "This is a global approach, so the basic assumption here is that there is only one normal mechanism which generates all normal data and these are considered serve as reference set.",
            "And the basic problems here are obviously that other outliers could also be in the reference set, and those outliers may falsify across the the results.",
            "We come back to this point later and in turn the local approaches are those approaches that whether reference set contains only a small subset of data objects.",
            "So here is usually no assumption or not really an explicit assumption on the number of normal.",
            "Mechanisms but the basic problem or the obvious obvious problem is how to choose a proper reference set.",
            "OK, and some approaches try to get best of two worlds, so some approaches are somewhat in between.",
            "For example, some approaches try to vary the resolution from global to local, for example from a single object to the entire database automatically.",
            "Or by user defined input."
        ],
        [
            "Then the other classification is or considers the output.",
            "So here we usually have two classes, one class of objects usually.",
            "Output label.",
            "Outlier, no outlier.",
            "So this is binary label and the other approaches are scoring approaches, which means that they output a score which is usually continuous.",
            "So for each object an outlier score, for example, the probability of being an outlier, something like that are reported.",
            "And then data objects can of course be sorted according to these scores.",
            "Yeah, some notes here.",
            "Many scoring approaches focus on determining the top N outliers, so give me the top N outlets that the most important outliers with the highest outliers score, where N is usually given by the users.",
            "And another note is that scoring approaches can usually be very easily adapted to labeling approaches, because you just have to define a threshold for your scoring or for your scores and.",
            "Then below or above is outlier and the other way around is inlier.",
            "So this is very straightforward of course."
        ],
        [
            "And third classification schema is.",
            "Judging judging the algorithms according to the intuition behind the model modeling approach and here we have several approaches.",
            "The first one are the model based approaches where the idea is to apply some model to represent normal data points, for example Gaussian distribution or whatever.",
            "And the outliers are then those objects that do not fit to that model.",
            "And some sample approaches here are probabilistic tests based on statistical models.",
            "Depth based approaches which will be described later.",
            "Deviation based approaches also described later."
        ],
        [
            "Second class here is proximity based approaches.",
            "So the idea here is that we examined the spatial proximity of each object in this space and if the proximity of an object is considered or considered deviates from the proximity of other objects, it's it is considered an outlier is also straightforward and some sample approaches here which will be covered later are distance based approaches, density based approaches and also other approaches.",
            "And third kind of approaches.",
            "A third class of approaches are so called angle based approaches.",
            "Here, the spectrum of pairwise angles between a given point in all other points or a sample of other points are examined or is examined and outliers are points that have a spectrum featuring a high fluctuation.",
            "Again, we will come to more details later on."
        ],
        [
            "So here is the outline.",
            "We already survived the introduction and.",
            "In the rest of the tutorial, I will follow the model of the third classification, where we have the model based approaches, the proximity based approaches and as a third kind of class.",
            "Here, adoptions adoptions of different models to a special problem just.",
            "Picked out one special problem as a sample and I just picked high dimensional points because we are working in so it's not an advertisement for work, but I will try to motivate why I think high dimensional data is still challenging for for outlier detection afterwards anyway.",
            "We will discuss these approaches and give you some sample models.",
            "Some sample algorithms here, and we also try to keep in mind the other two class classification schemas because yeah, so the output and the resolution of the orphan said are still considered in the slides."
        ],
        [
            "OK, so first of all, statistical tests.",
            "So the general idea of statistical tests for outlier detection is OK given a certain kind of statistical distribution, for example Gaussian distribution.",
            "Then we just compute the parameters assuming all data points have been generated by this statistical distribution in terms in.",
            "In the case of caution or Gaussian distribution, we just compute the mean and the standard deviation.",
            "And then outliers are points that have a low probability to be generated by this overall distribution.",
            "So this is very similar to the idea we had seen before in Barnett's discussion of the case numbers Headlam, where we have a Gaussian distribution of Union Station periods and the observation that deviates very much in 10 weeks had a very low probability to be generated by this mechanism, and this would be considered as an outlier.",
            "The basic assumption obviously is that the normal data objects follow a given distribution, which should be known of course, and occur in a high probability region of this model of course, and the outliers on the other hand deviate strongly from distribution and then have a low probability to be generated by this."
        ],
        [
            "And this basic idea has been implemented in a huge number of different tests that are available.",
            "The most important difference of these tests are which type of test data distribution.",
            "Is the number or which number of variables, univariate or multivariate data?",
            "The dimension of the other data objects of course.",
            "Which number of distributions only one normal distribution or one?",
            "Distribution on one mechanism for for normal objects or or several ones.",
            "And also do we have a parameter?",
            "Parametric or nonparametric approach nonparametric approach would be a histogram.",
            "Or parametric approach would be yeah computing, for example for Gaussian distribution mean and standard deviation.",
            "And yeah, I will try to give you a very short example which is very similar to the one we saw in in barnetts discussion.",
            "So we just assume a Gaussian distribution multi variate data, just one model, and we use parametric."
        ],
        [
            "Approach and what we can do is OK.",
            "The probability density function of a multivariate normal distribution is quite well known.",
            "Formally here we have to compute the mean and the governance metrics and then we can compute the modern obvious distance of a point of any point to the mean.",
            "And Interestingly, this distance function or not interesting email This distance function just follows a chi squared distribution of degrees of freedom dies.",
            "The dimension of the data space and then a typical statistical test is that points that have distance MONOVISC distance.",
            "Larger than a given threshold to the mean are outliers."
        ],
        [
            "And this is just a visualization.",
            "So you compute compute the mean on the left hand side.",
            "The cross is the mean of these observations here and on the right hand side you have the density distribution.",
            "And yeah, the points A&B.",
            "Will have a very low probability to be generated by this mechanism because they have a very high distance to the mean.",
            "In this case A would have a distance of 35 to this mean and be list of 24.",
            "OK, whatever.",
            "But this is the basic."
        ],
        [
            "Dear.",
            "You have some problems of this general idea, just some sketch here, so we unfortunately face the curse of dimensionality, so this is this only works very good for low dimensional data, usually because the larger the degree of freedom, the more similar the distance values.",
            "R4 for all points.",
            "So here on this on this.",
            "Pictures here you have the.",
            "Chi squared distribution for two degrees of freedom here, which is quite nice, because here you have some outliers and most of the points have a very small value.",
            "A very small distance to the mean, but for four.",
            "Well, for it also looks OK, but increasing the number of attributes for objects that degree of freedom for the Chi square distribution and you see that the distances to the mean converges to a kind of Gaussian distribution, so they.",
            "They all become very similar and yeah, this is the.",
            "Typically.",
            "This is 1.",
            "One problem of the curse of dimensionality that distances in high dimension just get very very similar and there is no discriminative power for distances anymore in high dimensional space.",
            "So this is kind of visualization of the cursor of this aspect of the cursor."
        ],
        [
            "Another problem is robustness.",
            "So mean and standard deviation in our case are very sensitive to outliers.",
            "But when we computed the mean and standard deviation, we did not remove the outliers before because we compute them in order to know where are the outliers so.",
            "Yeah, the the outliers may falsify their the mean and standard deviation and this is not a good good good deal.",
            "So a lot of work has been done in, especially the statistical community too.",
            "To increase the robustness of these tests, so for example, just one to mention, here is the minimum covariance determinant, virus orphaned and Leroy, but there are other work that has been done.",
            "To increase the robustness of this test.",
            "So yeah, for example.",
            "Yeah, OK, so short discussion here.",
            "The data distribution is usually fixed.",
            "You have to decide before which kind of distribution you want to apply.",
            "You have a kind of very low flexibility for this example.",
            "Of course, you can have more than one model, but then you are all again, you have to specify how many normal, how many.",
            "Models you allow how many processes you allow.",
            "For normal objects.",
            "It is a global method, obviously, and the output could could be a label, but also could be a score.",
            "For example the distance to the to the mean or also the probability density or the probability itself.",
            "Here's just a visualization for the problem here that we have.",
            "Low flexibility, so for example, if you hear decide that the normal objects have only been generated by one mechanism, then you would compute the mean for this.",
            "For these data points and you see here that the mean itself is the outlier.",
            "So the mean that they are sorry.",
            "The outlier point has a very small distance to the mean and would not be.",
            "Considered as outlier in this in this model.",
            "OK, so in this case you have to know that you have two processes that generated to different processes that generated the normal objects.",
            "This one here in this one here and you have to fit the points to these two distributions using EM or whatever.",
            "OK. Good."
        ],
        [
            "And so then death based approaches are very related to the status."
        ],
        [
            "So based approaches.",
            "Main idea here is to get independent of the of the data distribution.",
            "OK, so in in the statistical tests you have to say Oh my objects follow a normal distribution, the Gaussian distribution.",
            "So apply Gaussian distribution.",
            "But sometimes you cannot or you don't know what to do.",
            "And this was the general idea of death based approaches.",
            "So you just search for outliers at the border of the data space but.",
            "Independent of of the data distribution, go ahead.",
            "I think you're sure you have to use the microphone because of the sorry.",
            "Oh that sorry.",
            "Now I can repeat it just just go ahead.",
            "That's right, that seems to.",
            "So the question was that seems to contradict to the curse of dimensionality because it's completely right in high dimensional spaces that points tend to move at the borders of the of the points of the of the data space.",
            "Actually, these death based approaches are only applicable to lower dimensional points, not only because of the model, but you will see that these points rely on on convex Hull computation, and again those the only algorithm efficient algorithms for convex Hull computation in low dimensional space.",
            "So, but you're completely right also from the model it's a little bit of contradiction to the curse of dimensionality, but in low dimensional spaces.",
            "It might, it might work.",
            "So anyway, I'm presenting a tutorial so I'm neutral about all these approaches, but of course you can ask my personal opinion and I would say yeah, this is just for small for low dimensional data.",
            "OK, so yeah, that's what I said that the basic idea is to organize data objects in convex Hull layers and objects on outer layers are outliers and on inner layers are the inliers or the normal points, and obviously the basic assumption is that the authors are at the border of the data space and the normal objects are inside in the in the middle of the day in the center of the of the data space.",
            "Yeah, some."
        ],
        [
            "Things to the model here.",
            "How are these convex Hull layers defined?",
            "Well, you just compute the convex Hull of the full data space and all points on these on this convex Hull.",
            "For example, take this picture here.",
            "So this is the convex Hull for all data objects and all the objects on this.",
            "On this how have depth one.",
            "Layer one, then you remove these points and compute again the convex Hull, which end up ends up in this convex Hull.",
            "Here all the points on this second convex Hull have depth two or layer two, and so on and so on.",
            "OK. And then you can say OK, points having a depth of at most K are the outliers and the rest of the in line.",
            "So this is the basic idea."
        ],
        [
            "And there are some simple algorithms that basically differ in how to efficiently compute convex hulls.",
            "In terms of not to scan the data all the times for, for, for, knew convex Hull layers, but to do it in one scanner in one run.",
            "So here is for example easier depth, but also FDC.",
            "Maybe there are other other algorithms but.",
            "Those are.",
            "Potentially the most well known one.",
            "So short discussion.",
            "The idea is very similar to the classical statistical approaches.",
            "For one distribution, one distribution of normal objects.",
            "But independent from the from the type of distribution.",
            "Convex Hull computation, of course is only efficient as we said only in lower dimensional space is really efficient in in lower dimensional spaces, and while the output is usually a label, but can easily be extended to a scoring output.",
            "For example, you take the depth S as the scoring value, and then you're done.",
            "But you can think of course even more sophisticated scoring's, but that can be done very easy.",
            "And obviously the scope the resolution is global, so it's a global approach to outlier detection, because it considers all objects.",
            "Probably one point is an outlier.",
            "Obviously I did it so the question is, would it be?",
            "Would it remove some normal objects?",
            "And obviously yes.",
            "So again the basic assume."
        ],
        [
            "And here is outliers are at the border and inliers are in the center.",
            "If this assumption holds, then this could work.",
            "If not, I strongly doubt that it's not working.",
            "You're completely right.",
            "OK. Good."
        ],
        [
            "So then we already approach deviation based approaches so that."
        ],
        [
            "The idea here is that.",
            "Are we given a set of data points?",
            "Of course local group or the global sets don't care about it.",
            "And then outliers are those points that do not fit to the general correct characteristic of of this reference set.",
            "OK, for example, the variance of the set is minimized when removing the outliers.",
            "That would be 1.",
            "Characteristics the variance of the of the of this set and if you remove the outliers then the variance would significantly decrease and this is meant by the outliers.",
            "Do not fit to the to this general characteristics and the basic assumption again is that the outliers are the outermost points of the data set."
        ],
        [
            "Let's take 'em.",
            "Little bit more closer look at the model.",
            "So usually you are given or you have smoothing factor.",
            "And this smoothing factor just computes for any.",
            "Subset of data points.",
            "How much the variance of the whole data set is decreased when this subset is removed OK?",
            "And yeah, if two sets have an equal smoothing factor, then just take the smaller set anyway.",
            "You're now looking for that set.",
            "Which is called the exception set for which the smoothing factor is minimized.",
            "OK, you're looking for.",
            "Sorry, the set of points.",
            "So that if you remove this set of points, that smoothing factor is all the variance is decreased.",
            "Most of the smoothing factories is high.",
            "So the idea is is very similar like the statistical approach is.",
            "Again for usually one distribution, but again independent of the type of distribution and the problem here is you usually cannot apply.",
            "I know you solution for this because here you you just you just look for sets of points that you remove.",
            "And the naive solution would test.",
            "Of course, any subset of points, so you have to.",
            "If I evaluate the smoothing factor of all subsets of all subsets of all points and this is of course exponential in the number of data points, so exhaustive search is not feasible.",
            "So there are some papers that propose heuristics like random sampling or best first search starting from single.",
            "Objects and then add other objects to this exception.",
            "Exception set in the best first search manner.",
            "So you need some risztics and that of course means that you usually end up in not the optimal solution, but sub optimal solution of course.",
            "Yeah, the good thing is applicable to any type of data, just depending on the on the definition of the smoothing factor.",
            "And yeah, it was originally designed as a global method, but you can also think of of local methods here where you just have to.",
            "Just take not the whole database as as this reference set here but small reference sense.",
            "But again then the question is how to determine these references.",
            "Yeah, and the output here is a label, so you get the set of outliers and the rest are the inlines.",
            "OK. Yeah."
        ],
        [
            "Now we took one.",
            "Just to remind you, one should one of you should watch the Clock because yeah.",
            "Oh perfect, I'm just ahead of schedule.",
            "Perfect, looks good for my flight.",
            "OK, but now it's getting a little bit more.",
            "Now little bit more meat in the in the.",
            "In the next chapter, so distance based."
        ],
        [
            "Just the general idea of these approaches is.",
            "Yeah, to evaluate points based on the distances to the neighbors of this point of the point.",
            "And there are several variants.",
            "Which we will discuss right now, but just before the basic assumption here is that the normal data objects have a dense neighborhood means the neighbors are very near closely.",
            "And the outliers are far apart from their neighbors.",
            "OK, so have less dense neighborhoods."
        ],
        [
            "And one of the maybe most prominent and.",
            "Inspiring approaches here in this distance based class is the density based out or distance or distance based outlier?",
            "Proposed by Norand Rim Dang, where we have two parameters, distance radius at a distance threshold epsilon and a percentage P. And now a point is just considered an outlier.",
            "If at most P percent of all other points have a distance to P less than epsilon, OK, you just consider the neighborhood of P of all points that have a distance less than epsilon, and if they are.",
            "Less than a given threshold in this epsilon neighborhood, then it's an outlier.",
            "Otherwise it's normal object so.",
            "Here, for example, for these two points here, this is there, the epsilon parameter.",
            "So you consider the neighborhood of P. Or around each point with radius epsilon and you see that in this case for P1 and P2 you have zero points in the neighborhood and usually have a threshold above 0.",
            "So in this case these two points would be considered as outliers.",
            "Yeah, OK so.",
            "This formula here you just compute the range query for each point with radius epsilon, count the number of points, normalize it and then if it exceeds a threshold then it's an inlier.",
            "Otherwise it's an outlier."
        ],
        [
            "Yeah, there are some several algorithms to compute these distance based outliers.",
            "One approach is index based, which just computes a join.",
            "So what do you have to do so you have to compute the epsilon neighborhood of all points and this is this problem is known as a similarity join between points.",
            "In this case, the distance range join between points and this can be done quite efficiently using spatial index structures.",
            "If the points are Euclidean.",
            "If not, if you have a metric distance function, then you can also use metric index structures, but OK anyway, and the idea of this algorithm is to use the distance range join and exclude points from further consideration that have already enough points in the epsilon neighborhood so that an epsilon range join usually traverses the data set twice, so to say.",
            "You just to have two copies of your data set, and especially next structure, and the joint reverses both index structures and keeps track for each object if yeah.",
            "How many points are?",
            "In that same neighborhood, based on approximations, and if you are sure that the point cannot be an outlier, then you just can discard it from further or prune it from further consideration.",
            "A second, a second, sorry a second algorithm, is a nested loop based algorithm.",
            "Where?",
            "Now the main memory buffer is divided into parts and the second part is just used to scan and compare all points with points from the first part.",
            "So you just load one part of the data base in the first part of the buffer and then the second part of the buffer is used to go through the whole data set and compare points.",
            "OK, so classical nested loop join.",
            "And the third kind of algorithm is a grid based approach, so here the idea is to build a grid such that any two points from the same grid cell have a distance of at most epsilon to each other.",
            "And then it's clear if you think about it a second time, it's clear that points need only or any point need only compare with points from neighboring cells because.",
            "Other cells.",
            "Can only contain points that have distance of more than epsilon to the point.",
            "OK, so this just reduces the number of comparisons between points in order to compute these epsilon neighborhoods of points."
        ],
        [
            "Yeah, then there is an interesting extension to the original work, which is not really an outlier model, but I think it's worth mentioning because it's one of the core points of data mining.",
            "Office here.",
            "Came up with a solution to derive knowledge about the outliers.",
            "To declare or to describe the outliers that are found.",
            "So there the idea is based on this on this distance based outlier model we just discussed here and.",
            "What what they did is they try to find minimal subsets of attributes that explains the outline Ness of a point so.",
            "Some sets of attributes in which the point is still an outlier, so maybe I just try to make it clear by this example here.",
            "So for example, the following players in this is just a sports statistics data set for some hockey players you have some measurements like power play goals, shorthanded goals, whatever.",
            "We need not to be in hockey expert, two or two, or understand that.",
            "So the point is well, these four guys are.",
            "Detected as outliers, but this model here and the intentional knowledge derived from this result is the following.",
            "So Marion Linear is an outlier already in the 1 dimensional space of Power play goal, so just considering only this attribute means or would already find this player as an outlier and also the two dimensional space of shorthanded goals in game winning goals.",
            "So just considering two these two.",
            "Attributes here would also be enough to identify Mario Lemieux as as an outlier.",
            "OK, so.",
            "And I hope it's not a little bit clearer, so the method tries to find the minimum subset of attributes that already is is already sufficient to make this point an outlier.",
            "And the same for the other.",
            "For the other guys here.",
            "And this is of course very interesting, because that gives the domain expert little bit more insight into the data set.",
            "Telling a hockey freak, Mario Lemieux is an outlier.",
            "The frequency?",
            "Yeah, of course.",
            "There's an outstanding player, but on the other side you can say OK, he's outstanding because he has once a power play.",
            "Goals seems to have a lot of power, play goals and a lot of short handed goals and whatever.",
            "So you give him a little bit more than just the outliers, but also an explanation on what constitutes the outlier."
        ],
        [
            "Yeah, other outlier, other distance based approaches are usually based on the distance to the K nearest neighbors of objects.",
            "So are there?",
            "Basically 22 variants here.",
            "The first model just takes the K nearest neighbor distance, so this is the distance to the K nearest neighbor of a point at the K nearest neighbor distance of a point is an outlier score.",
            "The higher the distance, the higher the outline of the point, the smaller the distance, the more the point is a normal observation.",
            "And the second variant here is that not only the K nearest neighbor distance is considered, but you compute the 1, two, or three, and so on up to the K nearest neighbor.",
            "The distances and then aggregate these distances.",
            "For example, take the mean, the average or whatever.",
            "So these are the basic variants here, but.",
            "Both rely on the idea of.",
            "Evaluating the key nearest neighbor distances of points.",
            "There are a lot of algorithms here.",
            "Because again, you have the problem that you have to compute the K nearest neighbors of all objects.",
            "This is again a kind of similarity join.",
            "In this case K nearest neighbor join and there are several approaches to speed this up.",
            "So the basic ideas of these approaches is a kind of nested loop algorithm.",
            "So for each object computer can use neighbors with a sequential scan or with a spatial index structure.",
            "And.",
            "Um?",
            "Most approaches try to shrink down the search space here, not to compute.",
            "For each object, all K nearest neighbors, but 22.",
            "To minimize the number of distance comparisons and so on.",
            "And the second general approach here is a petition based approach.",
            "So usually these approaches try to partition the data into a kind of micro clusters and then they try to aggregate some certain information for each of these petitions.",
            "For example, minimum bounding rectangles of the points in a micro cluster, and these aggregated information of course should allow to prune micro clusters that cannot qualify when searching for the K nearest neighbors of a particular point.",
            "Which is very similar to the idea of using spatial index structures but not using index structures in that case, but using micro clusters but.",
            "Clustering and and indexing objects is a very, very related problems.",
            "So here are just some sampling."
        ],
        [
            "Algorithms, some sample algorithms.",
            "Usually these algorithms compute the top end outliers, so they have this restriction in order to again prune points or or minimize the number of computations necessary.",
            "We have nested loop algorithms by.",
            "Swami at all.",
            "We have a kind of linearization of a multidimensional data set using a space filling curve.",
            "In order to get a 1 dimensional representation, which is that then petitioned.",
            "Into micro clusters and this is done by anjuli and sodium.",
            "Orca by benj.",
            "Schwab has very well known, which is kind of nested loop algorithm with the randomization and a very simple pruning schema in order to to get the top N outliers quite fast.",
            "And the nice thing with this algorithm is that it's it's really applicable for both Kane Anderson's models and also the distance based on their model of Nora name."
        ],
        [
            "And then there are some.",
            "Variance of our car, for example RBRP.",
            "Which increases the pruning power.",
            "Minimizes the number of computations even more, and other approaches, for example, based on my micro clustering, though, so partitioning the data into micro clusters or also approximate solutions.",
            "For example, one work by John Pay and colleagues to based on reference points.",
            "But all these algorithms are not really knew models, they just compute outliers based on these K&M.",
            "Inspired models, they just are.",
            "Algorithms for speeding up the computation?",
            "OK, so don't get disturbed.",
            "None of these algorithms, or none of these papers introduce new models, new outlier models.",
            "They all use these KNN based based outline models.",
            "They just propose new algorithms.",
            "If more efficient algorithms for further computation, we just have to introduce a threshold saying OK points that have a KNN distance more than the threshold.",
            "Are outliers and the others are inliers?",
            "Approaches are usually local, and the resolution that the size of the reference set can be adjusted by the user via these these input parameters for for the density based for the distance based approaches.",
            "This is the epsilon.",
            "Which somehow defines how big the reference set is, but of course it's not really obvious how big it will be.",
            "If you set epsilon to whatever 5 or so, then it's not really obvious how many points will be in the five in the radius of five.",
            "Points and of course for many poor for different points.",
            "The size of the reference set could could be different because of this distance threshold absolute and for for the K nearest neighbor distance models.",
            "Of course you can specify the number of K. Nearest neighbors that are considered so in this case all points have a similar sized reference set in this case."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, maybe we start right on time.",
                    "label": 0
                },
                {
                    "sent": "My I have a tight schedule today because my flight back to Munich is tonight at eight 8:00 PM.",
                    "label": 0
                },
                {
                    "sent": "So I have to worry about it a little bit, so welcome to the tutorial on outlier detection techniques.",
                    "label": 1
                },
                {
                    "sent": "My name is Pierre Cougar and I present this tutorial for you on behalf of Hospital Keegan.",
                    "label": 0
                },
                {
                    "sent": "My boss and automaker Arthur is also around here, but he is involved in a workshop this afternoon so but he will stay the other day.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions coming to your mind afterwards, not today, then just go to Ottawa.",
                    "label": 0
                },
                {
                    "sent": "I hope he will show up at least shortly.",
                    "label": 0
                },
                {
                    "sent": "At the coffee break, so you see his face and can talk to him.",
                    "label": 0
                },
                {
                    "sent": "I don't have a watch, so somebody has to watch the Clock because I think at three we should make coffee break.",
                    "label": 0
                },
                {
                    "sent": "So maybe one of you could.",
                    "label": 0
                },
                {
                    "sent": "Have a look on that or have an eye on this issue.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some general issues before we start, please feel free to ask any questions at anytime I want to make it a little bit casual so you can.",
                    "label": 1
                },
                {
                    "sent": "We can discuss issues here.",
                    "label": 0
                },
                {
                    "sent": "If you have questions, just interrupt me and the general aim of this tutorial is yeah to get more or less a big picture of outlier detection, which is a broad area of research so I won't cover all details or methods or algorithms here.",
                    "label": 0
                },
                {
                    "sent": "But we try to give you.",
                    "label": 0
                },
                {
                    "sent": "Kind of big picture, hopefully it works.",
                    "label": 1
                },
                {
                    "sent": "And the big picture is not in terms of listing some methods and algorithms.",
                    "label": 0
                },
                {
                    "sent": "You will get a list of methods and algorithms, but the main point here is in terms of getting a big picture in in terms of basic approaches and modeling approaches for outlier detection.",
                    "label": 1
                },
                {
                    "sent": "And as I said some, some algorithms will also be provided.",
                    "label": 1
                },
                {
                    "sent": "But the selection is quite arbitrary, so if you if your favorite algorithm is not on the list, please don't mind OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "And well, usually these slides should be made available through the KDD at the website, but I've heard about that there were some problems.",
                    "label": 0
                },
                {
                    "sent": "So just visit our website.",
                    "label": 1
                },
                {
                    "sent": "Maybe I skip to the first page here.",
                    "label": 0
                },
                {
                    "sent": "The website at www.dps.",
                    "label": 0
                },
                {
                    "sent": "Stands for database systems at.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Institute for Informatics at the Munich.",
                    "label": 1
                },
                {
                    "sent": "And then you will see.",
                    "label": 0
                },
                {
                    "sent": "Somewhere on our home page is the latest slides, hopefully at the end of this week or.",
                    "label": 0
                },
                {
                    "sent": "At the beginning of next week.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's start, yeah.",
                    "label": 0
                },
                {
                    "sent": "Pune.",
                    "label": 0
                },
                {
                    "sent": "Just navigate to a 2 hour my personal website.",
                    "label": 0
                },
                {
                    "sent": "You will find my personal website on this on this side and then or autosar to us home page and then you will find this.",
                    "label": 0
                },
                {
                    "sent": "OK I will try to make it very clearly OK. OK, so let's start with a short discussion on what is an outlier?",
                    "label": 0
                },
                {
                    "sent": "What's what's all about here?",
                    "label": 0
                },
                {
                    "sent": "And mostly any paper that deals with outlier detection sites.",
                    "label": 0
                },
                {
                    "sent": "The definition of Hawkins.",
                    "label": 0
                },
                {
                    "sent": "Who said simply says that an author is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism.",
                    "label": 1
                },
                {
                    "sent": "So Hawkins is statistics.",
                    "label": 0
                },
                {
                    "sent": "So we had a statistical into our statistics based intuition on this task.",
                    "label": 0
                },
                {
                    "sent": "And yeah, if we take a closer look on this on this definition, we see that he or other colleagues also assume that the normal data.",
                    "label": 0
                },
                {
                    "sent": "Should not be outliers.",
                    "label": 0
                },
                {
                    "sent": "Usually follow a special mechanism that generates these normal data points or objects?",
                    "label": 0
                },
                {
                    "sent": "For example, some given statistical process and the outliers are those objects that deviate from these generating mechanisms.",
                    "label": 0
                },
                {
                    "sent": "So maybe are generated by a different statistical process or whatever.",
                    "label": 0
                },
                {
                    "sent": "Let's take a.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Closer look by considering example.",
                    "label": 0
                },
                {
                    "sent": "This is the case of Mr. Headlam versus Mrs. Headlon, which was discussed by Barnett, and a very interesting paper on outlier detection.",
                    "label": 0
                },
                {
                    "sent": "What was the point?",
                    "label": 0
                },
                {
                    "sent": "So yeah, the birth of a child to Miss Headlam happened strangely.",
                    "label": 1
                },
                {
                    "sent": "349 days after Mr had left for military service.",
                    "label": 1
                },
                {
                    "sent": "And if you consider that the average human gas station.",
                    "label": 0
                },
                {
                    "sent": "Is about 40 weeks, so 280 days.",
                    "label": 0
                },
                {
                    "sent": "Mr. Headlam has obviously some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that feelings with this and of course thought that this baby is not his baby and the court has to decide if it's possible or not.",
                    "label": 0
                },
                {
                    "sent": "And Interestingly that the judges did was they ask the doctors.",
                    "label": 0
                },
                {
                    "sent": "Is it possible from medicine point of view?",
                    "label": 0
                },
                {
                    "sent": "Is it possible that this baby is Mr Adams baby and the doctor said theoretically it could happen?",
                    "label": 0
                },
                {
                    "sent": "So what the judges did was to reject Mr. Adams objective and.",
                    "label": 0
                },
                {
                    "sent": "Barnett, in 78 so 30 years ago 30 years later.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pick that up and analyze this by some statistics.",
                    "label": 0
                },
                {
                    "sent": "So what he did was just to take some 30,013.",
                    "label": 0
                },
                {
                    "sent": "Sell 1000 something, sorry.",
                    "label": 0
                },
                {
                    "sent": "Situations of gestation periods, limitation periods which are plotted here by a histogram and you see that this is.",
                    "label": 0
                },
                {
                    "sent": "This follows quite nicely.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian distribution around the normal mean of 40 weeks.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the case of Helen versus Harlem is 50 weeks.",
                    "label": 0
                },
                {
                    "sent": "So from a statistical sense, from a statistical point of view, this is clearly an outlier, but as I said, the doctor said it's theoretically possible so.",
                    "label": 0
                },
                {
                    "sent": "We cannot.",
                    "label": 0
                },
                {
                    "sent": "Make a difference so.",
                    "label": 0
                },
                {
                    "sent": "Taking a closer look to put it in the in the direction of Hawkins definition, what do we have here?",
                    "label": 0
                },
                {
                    "sent": "So we have the statistical basis of 13,000 observations of gas station periods.",
                    "label": 0
                },
                {
                    "sent": "In blue again, the histogram and green is a Gaussian distribution fitted to this histogram, so this is the underlying process.",
                    "label": 0
                },
                {
                    "sent": "The generating mechanism.",
                    "label": 0
                },
                {
                    "sent": "And you see.",
                    "label": 0
                },
                {
                    "sent": "With a very low probability for the birth of Mr. Mrs Hallam, Adams shines child for being.",
                    "label": 1
                },
                {
                    "sent": "Generated by this mechanism, or in other words, to be produced by Mr. Mr headlam.",
                    "label": 0
                },
                {
                    "sent": "And in red, this is the assumption of Mr. Adam.",
                    "label": 0
                },
                {
                    "sent": "What was the assumption of him?",
                    "label": 0
                },
                {
                    "sent": "He said that there was.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "The observation.",
                    "label": 0
                },
                {
                    "sent": "What's training generated by this mechanism?",
                    "label": 0
                },
                {
                    "sent": "Which means that in this case, if you if you have this mechanism here, which is different from from this mechanism, here is is a high, very very high probability of this gas station.",
                    "label": 0
                },
                {
                    "sent": "So once again we have in in Hawkins sends one generating mechanism.",
                    "label": 0
                },
                {
                    "sent": "Modeled here by Gaussian distribution in green and the outlier was.",
                    "label": 0
                },
                {
                    "sent": "Produced by a different mechanism, in this case a red.",
                    "label": 0
                },
                {
                    "sent": "The red question distribution.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um, obviously nowadays there are better ways to prove if one is the father of a child or not.",
                    "label": 0
                },
                {
                    "sent": "Not only outlier detection but some genetic tests, of course, so are there other applications for outlier detection?",
                    "label": 0
                },
                {
                    "sent": "Yes, of course there are, so there are many applications for outlier detection, just to name a view.",
                    "label": 0
                },
                {
                    "sent": "So fraud detection, for example here you can or you try to detect credit card abuse.",
                    "label": 0
                },
                {
                    "sent": "By looking at the at the at the purchasing behavior of users.",
                    "label": 0
                },
                {
                    "sent": "In medicine, for example, you could have unusual symptoms or test results that may indicate some diseases, potential health problems, whatever.",
                    "label": 1
                },
                {
                    "sent": "In this case, it's interesting that not only the measurement itself, the abnormal value of the measurement itself is important to decide if it's an outlier or not, or if it's.",
                    "label": 0
                },
                {
                    "sent": "A potential health problem or not, but also other features of the patient.",
                    "label": 0
                },
                {
                    "sent": "For example, gender, age or whatever could be interesting, so you have typically more than one variable more than one attribute per object, per patient, or whatever public health.",
                    "label": 0
                },
                {
                    "sent": "Also very common application, for example the occurrence of a particular disease scattered across various hospitals of the city may indicate some problems with the corresponding vaccination program.",
                    "label": 1
                },
                {
                    "sent": "Again, here you usually have to consider spatial or also frequency correlations, not only the measurement itself but also other features are important to decide whether an outlier is really an outlier is really a signal or not.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sports statistics may be you also very familiar with that, so in many sports of course, various parameters are measured.",
                    "label": 1
                },
                {
                    "sent": "Four players and you can emulate performances as outstanding and positive or negative sense, of course.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 1
                },
                {
                    "sent": "In this case, sometimes players should show only only exceptional values in positive or negative sense.",
                    "label": 0
                },
                {
                    "sent": "Only a subset of.",
                    "label": 0
                },
                {
                    "sent": "Parameters or combinations of parameters.",
                    "label": 1
                },
                {
                    "sent": "So not all parameters, not all values are important here.",
                    "label": 1
                },
                {
                    "sent": "Another very important point, application for outlier detection is measurement error detection.",
                    "label": 1
                },
                {
                    "sent": "For example, in sensor data you usually have a lot of measurement errors, and here in that case outlier detection could indicate measurement errors so that you can remove, for example the measurements.",
                    "label": 0
                },
                {
                    "sent": "Or you can.",
                    "label": 0
                },
                {
                    "sent": "Make something out of it, whatever.",
                    "label": 0
                },
                {
                    "sent": "Do a special treatment on the for these measurements.",
                    "label": 0
                },
                {
                    "sent": "But this is something like two sides of metal, because yeah, you have this very famous statement.",
                    "label": 1
                },
                {
                    "sent": "One person's noise could be another one person signal.",
                    "label": 0
                },
                {
                    "sent": "That means if you detect an outlier and say this is noise, another person could say no, no this is not noise.",
                    "label": 0
                },
                {
                    "sent": "This is a clear signal or this is a knowledge which I want to know don't remove it or whatever.",
                    "label": 0
                },
                {
                    "sent": "So outlier detection is quite quite subjective, not really objective.",
                    "label": 0
                },
                {
                    "sent": "OK and it heavily depends on the application domains and applications.",
                    "label": 0
                },
                {
                    "sent": "If the detected signal detected outliers are really interesting or not.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm coming back to the basic intuition of Hawkins in the light of these applications.",
                    "label": 0
                },
                {
                    "sent": "Then we have to see that data is usually multidimensional.",
                    "label": 1
                },
                {
                    "sent": "So you have a lot of measures per object or point, and if you recall, in the in the basic model, you just have a 1 dimensional data set.",
                    "label": 0
                },
                {
                    "sent": "So in the case of Helen was Helen, you just measure the length of the station.",
                    "label": 0
                },
                {
                    "sent": "Just one measure for each.",
                    "label": 0
                },
                {
                    "sent": "And but usually this is not the case.",
                    "label": 0
                },
                {
                    "sent": "You usually have a multivariate data set.",
                    "label": 1
                },
                {
                    "sent": "Usually you are the basic intuition also assumes that you have just one normal generating process, so in this case only the green process is the process that generates the normal objects and normal observations.",
                    "label": 1
                },
                {
                    "sent": "But in general there could be more than one normal process.",
                    "label": 0
                },
                {
                    "sent": "OK, usually you have a mixture of normal process is and not only one.",
                    "label": 0
                },
                {
                    "sent": "Yes, and 3rd animals, sorry, outliers, outliers may represent different classes of generating mechanisms.",
                    "label": 1
                },
                {
                    "sent": "So the outlines that the set of outlier points may be very large, and in the basic model usually you assume that outliers are rare observations and not set of outliers are not very large, so these three basic points here should illustrate the basic intuition behind the statistical approach inspired by Hawkins is very limited and.",
                    "label": 0
                },
                {
                    "sent": "You need more or.",
                    "label": 0
                },
                {
                    "sent": "Other techniques too.",
                    "label": 0
                },
                {
                    "sent": "Conquer nowadays data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the consequences is that a lot of models and approaches have evolved in the in the past years.",
                    "label": 1
                },
                {
                    "sent": "Order to exceed these limitations.",
                    "label": 1
                },
                {
                    "sent": "And yeah, I think not that easy to keep track with this evolution.",
                    "label": 0
                },
                {
                    "sent": "So one.",
                    "label": 0
                },
                {
                    "sent": "Aim of this tutorial, of course, is to try to give you some overview of this.",
                    "label": 0
                },
                {
                    "sent": "And what's very important is if you look at specific outlier detection models and approaches, is that.",
                    "label": 0
                },
                {
                    "sent": "Each of them usually involves sometimes knew, but very typical assumptions and restrictions and.",
                    "label": 0
                },
                {
                    "sent": "Very often these assumptions restrictions are hidden.",
                    "label": 0
                },
                {
                    "sent": "They're not explicitly discussed in papers, but usually each paper or each approach has one at least one.",
                    "label": 0
                },
                {
                    "sent": "So you have to keep this in mind when we.",
                    "label": 0
                },
                {
                    "sent": "Discuss different a different approaches to outlier detection after.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some general application scenarios.",
                    "label": 1
                },
                {
                    "sent": "First of all, we have a supervised scenario because in some applications you just have training data of normal and abnormal data.",
                    "label": 1
                },
                {
                    "sent": "And in this case you can train a classifier and this is a supervised task.",
                    "label": 0
                },
                {
                    "sent": "Usually this is an unbalanced classification problem 'cause you usually have a lot more normal observations in the training data than abnormal observations.",
                    "label": 0
                },
                {
                    "sent": "But OK, you have to deal with that.",
                    "label": 0
                },
                {
                    "sent": "Second scenarios seem supervised, so in this case usually you have just training data for one class, either the the normal distribution or the abnormal data points.",
                    "label": 1
                },
                {
                    "sent": "And you have to learn again some rules or classifiers to do the job.",
                    "label": 0
                },
                {
                    "sent": "And finally we have the answer unsupervised scenario where we don't have any training data at hand and in most applications this is the case.",
                    "label": 0
                },
                {
                    "sent": "You don't have any training data at hand and in this tutorial we focus on the unsupervised scenario, so you won't find in this.",
                    "label": 1
                },
                {
                    "sent": "Tutorial nothing about supervised.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised approach is purely unsupervised.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, if we're talking about unsupervised learning or unsupervised data mining, then maybe you just have to have the question that is, it is outlier detection very related or even similar to clustering.",
                    "label": 0
                },
                {
                    "sent": "So are outliers not just the side product of clustering algorithms?",
                    "label": 1
                },
                {
                    "sent": "Well that's true on the first side, maybe because many clustering algorithms do not assign all points of course but can account for noise object and you.",
                    "label": 1
                },
                {
                    "sent": "Might.",
                    "label": 0
                },
                {
                    "sent": "For the option that applying one of those algorithm and retrieve the noise set as outliers.",
                    "label": 1
                },
                {
                    "sent": "But there are some problems with this because clustering algorithms are usually optimized to find clusters and not to find outliers so.",
                    "label": 0
                },
                {
                    "sent": "It's more orthogonal than it's related, of course, but it's these two problems are very detection.",
                    "label": 0
                },
                {
                    "sent": "Clustering are orthogonal, I would say.",
                    "label": 0
                },
                {
                    "sent": "And that means that the accuracy of outlier detection depends on how good the clustering algorithm captures the structure of the cluster and not how good it identifies outliers.",
                    "label": 1
                },
                {
                    "sent": "And again, a set of many abnormal data objects that are similar to each other that would these these observations would be grouped into one cluster, maybe and are would not be recognized as noise, and in this case you would lose them.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I think these these problems illustrate that we need or that the outlier detection problem is really a problem on its own, and it's not just a side product of clustering.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, in the following global as I said, we try to give you a big picture so we will focus on three different classification approaches.",
                    "label": 1
                },
                {
                    "sent": "Here we try to classify.",
                    "label": 0
                },
                {
                    "sent": "Listing approaches and if some approaches that you are acquainted with are not on the list of our in our tutorial here, you should.",
                    "label": 0
                },
                {
                    "sent": "Have it easy to on tour classify these approaches also in these classification hierarchy or it's not really a high kyriarchy, it's just three different.",
                    "label": 0
                },
                {
                    "sent": "Binary classifications or mostly binary.",
                    "label": 1
                },
                {
                    "sent": "So the first one considers the set of of reference objects that are considered to judge the outliners of 1 object.",
                    "label": 0
                },
                {
                    "sent": "The second one considers the output of an algorithm.",
                    "label": 1
                },
                {
                    "sent": "What is the output of the algorithm and the third classification just considers the concept based on which the outliers are modeled OK and we will.",
                    "label": 0
                },
                {
                    "sent": "I will go into more details right now.",
                    "label": 0
                },
                {
                    "sent": "But just to note here, we focus on models and methods for Euclidean data, but many of those approaches here can also be used for any data types because usually they don't.",
                    "label": 1
                },
                {
                    "sent": "They only require distance.",
                    "label": 0
                },
                {
                    "sent": "Measure or similarity measure.",
                    "label": 0
                },
                {
                    "sent": "So if you have a similarity similarity measure for your objects, then it.",
                    "label": 0
                },
                {
                    "sent": "They need not to be you cleaning.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So getting a little bit more into details about these classifications here, so the first one as a set considers the resolution that is the size of the set of points that are considered in order to judge.",
                    "label": 0
                },
                {
                    "sent": "If another points in outline or not, so we have basically global approaches where the reference set contains all other data objects, so one object is judged the outlanes of 1 object object is evaluated on in terms of all other objects in the data set.",
                    "label": 1
                },
                {
                    "sent": "This is a global approach, so the basic assumption here is that there is only one normal mechanism which generates all normal data and these are considered serve as reference set.",
                    "label": 1
                },
                {
                    "sent": "And the basic problems here are obviously that other outliers could also be in the reference set, and those outliers may falsify across the the results.",
                    "label": 1
                },
                {
                    "sent": "We come back to this point later and in turn the local approaches are those approaches that whether reference set contains only a small subset of data objects.",
                    "label": 1
                },
                {
                    "sent": "So here is usually no assumption or not really an explicit assumption on the number of normal.",
                    "label": 1
                },
                {
                    "sent": "Mechanisms but the basic problem or the obvious obvious problem is how to choose a proper reference set.",
                    "label": 1
                },
                {
                    "sent": "OK, and some approaches try to get best of two worlds, so some approaches are somewhat in between.",
                    "label": 0
                },
                {
                    "sent": "For example, some approaches try to vary the resolution from global to local, for example from a single object to the entire database automatically.",
                    "label": 0
                },
                {
                    "sent": "Or by user defined input.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the other classification is or considers the output.",
                    "label": 1
                },
                {
                    "sent": "So here we usually have two classes, one class of objects usually.",
                    "label": 0
                },
                {
                    "sent": "Output label.",
                    "label": 0
                },
                {
                    "sent": "Outlier, no outlier.",
                    "label": 0
                },
                {
                    "sent": "So this is binary label and the other approaches are scoring approaches, which means that they output a score which is usually continuous.",
                    "label": 0
                },
                {
                    "sent": "So for each object an outlier score, for example, the probability of being an outlier, something like that are reported.",
                    "label": 1
                },
                {
                    "sent": "And then data objects can of course be sorted according to these scores.",
                    "label": 0
                },
                {
                    "sent": "Yeah, some notes here.",
                    "label": 0
                },
                {
                    "sent": "Many scoring approaches focus on determining the top N outliers, so give me the top N outlets that the most important outliers with the highest outliers score, where N is usually given by the users.",
                    "label": 1
                },
                {
                    "sent": "And another note is that scoring approaches can usually be very easily adapted to labeling approaches, because you just have to define a threshold for your scoring or for your scores and.",
                    "label": 0
                },
                {
                    "sent": "Then below or above is outlier and the other way around is inlier.",
                    "label": 0
                },
                {
                    "sent": "So this is very straightforward of course.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And third classification schema is.",
                    "label": 0
                },
                {
                    "sent": "Judging judging the algorithms according to the intuition behind the model modeling approach and here we have several approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one are the model based approaches where the idea is to apply some model to represent normal data points, for example Gaussian distribution or whatever.",
                    "label": 1
                },
                {
                    "sent": "And the outliers are then those objects that do not fit to that model.",
                    "label": 1
                },
                {
                    "sent": "And some sample approaches here are probabilistic tests based on statistical models.",
                    "label": 0
                },
                {
                    "sent": "Depth based approaches which will be described later.",
                    "label": 0
                },
                {
                    "sent": "Deviation based approaches also described later.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second class here is proximity based approaches.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we examined the spatial proximity of each object in this space and if the proximity of an object is considered or considered deviates from the proximity of other objects, it's it is considered an outlier is also straightforward and some sample approaches here which will be covered later are distance based approaches, density based approaches and also other approaches.",
                    "label": 1
                },
                {
                    "sent": "And third kind of approaches.",
                    "label": 0
                },
                {
                    "sent": "A third class of approaches are so called angle based approaches.",
                    "label": 0
                },
                {
                    "sent": "Here, the spectrum of pairwise angles between a given point in all other points or a sample of other points are examined or is examined and outliers are points that have a spectrum featuring a high fluctuation.",
                    "label": 1
                },
                {
                    "sent": "Again, we will come to more details later on.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the outline.",
                    "label": 0
                },
                {
                    "sent": "We already survived the introduction and.",
                    "label": 0
                },
                {
                    "sent": "In the rest of the tutorial, I will follow the model of the third classification, where we have the model based approaches, the proximity based approaches and as a third kind of class.",
                    "label": 0
                },
                {
                    "sent": "Here, adoptions adoptions of different models to a special problem just.",
                    "label": 1
                },
                {
                    "sent": "Picked out one special problem as a sample and I just picked high dimensional points because we are working in so it's not an advertisement for work, but I will try to motivate why I think high dimensional data is still challenging for for outlier detection afterwards anyway.",
                    "label": 0
                },
                {
                    "sent": "We will discuss these approaches and give you some sample models.",
                    "label": 0
                },
                {
                    "sent": "Some sample algorithms here, and we also try to keep in mind the other two class classification schemas because yeah, so the output and the resolution of the orphan said are still considered in the slides.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so first of all, statistical tests.",
                    "label": 0
                },
                {
                    "sent": "So the general idea of statistical tests for outlier detection is OK given a certain kind of statistical distribution, for example Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "Then we just compute the parameters assuming all data points have been generated by this statistical distribution in terms in.",
                    "label": 1
                },
                {
                    "sent": "In the case of caution or Gaussian distribution, we just compute the mean and the standard deviation.",
                    "label": 0
                },
                {
                    "sent": "And then outliers are points that have a low probability to be generated by this overall distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is very similar to the idea we had seen before in Barnett's discussion of the case numbers Headlam, where we have a Gaussian distribution of Union Station periods and the observation that deviates very much in 10 weeks had a very low probability to be generated by this mechanism, and this would be considered as an outlier.",
                    "label": 0
                },
                {
                    "sent": "The basic assumption obviously is that the normal data objects follow a given distribution, which should be known of course, and occur in a high probability region of this model of course, and the outliers on the other hand deviate strongly from distribution and then have a low probability to be generated by this.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this basic idea has been implemented in a huge number of different tests that are available.",
                    "label": 1
                },
                {
                    "sent": "The most important difference of these tests are which type of test data distribution.",
                    "label": 1
                },
                {
                    "sent": "Is the number or which number of variables, univariate or multivariate data?",
                    "label": 0
                },
                {
                    "sent": "The dimension of the other data objects of course.",
                    "label": 0
                },
                {
                    "sent": "Which number of distributions only one normal distribution or one?",
                    "label": 0
                },
                {
                    "sent": "Distribution on one mechanism for for normal objects or or several ones.",
                    "label": 0
                },
                {
                    "sent": "And also do we have a parameter?",
                    "label": 0
                },
                {
                    "sent": "Parametric or nonparametric approach nonparametric approach would be a histogram.",
                    "label": 0
                },
                {
                    "sent": "Or parametric approach would be yeah computing, for example for Gaussian distribution mean and standard deviation.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I will try to give you a very short example which is very similar to the one we saw in in barnetts discussion.",
                    "label": 0
                },
                {
                    "sent": "So we just assume a Gaussian distribution multi variate data, just one model, and we use parametric.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach and what we can do is OK.",
                    "label": 0
                },
                {
                    "sent": "The probability density function of a multivariate normal distribution is quite well known.",
                    "label": 1
                },
                {
                    "sent": "Formally here we have to compute the mean and the governance metrics and then we can compute the modern obvious distance of a point of any point to the mean.",
                    "label": 1
                },
                {
                    "sent": "And Interestingly, this distance function or not interesting email This distance function just follows a chi squared distribution of degrees of freedom dies.",
                    "label": 0
                },
                {
                    "sent": "The dimension of the data space and then a typical statistical test is that points that have distance MONOVISC distance.",
                    "label": 0
                },
                {
                    "sent": "Larger than a given threshold to the mean are outliers.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a visualization.",
                    "label": 0
                },
                {
                    "sent": "So you compute compute the mean on the left hand side.",
                    "label": 0
                },
                {
                    "sent": "The cross is the mean of these observations here and on the right hand side you have the density distribution.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the points A&B.",
                    "label": 0
                },
                {
                    "sent": "Will have a very low probability to be generated by this mechanism because they have a very high distance to the mean.",
                    "label": 0
                },
                {
                    "sent": "In this case A would have a distance of 35 to this mean and be list of 24.",
                    "label": 0
                },
                {
                    "sent": "OK, whatever.",
                    "label": 0
                },
                {
                    "sent": "But this is the basic.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dear.",
                    "label": 0
                },
                {
                    "sent": "You have some problems of this general idea, just some sketch here, so we unfortunately face the curse of dimensionality, so this is this only works very good for low dimensional data, usually because the larger the degree of freedom, the more similar the distance values.",
                    "label": 1
                },
                {
                    "sent": "R4 for all points.",
                    "label": 0
                },
                {
                    "sent": "So here on this on this.",
                    "label": 0
                },
                {
                    "sent": "Pictures here you have the.",
                    "label": 0
                },
                {
                    "sent": "Chi squared distribution for two degrees of freedom here, which is quite nice, because here you have some outliers and most of the points have a very small value.",
                    "label": 0
                },
                {
                    "sent": "A very small distance to the mean, but for four.",
                    "label": 0
                },
                {
                    "sent": "Well, for it also looks OK, but increasing the number of attributes for objects that degree of freedom for the Chi square distribution and you see that the distances to the mean converges to a kind of Gaussian distribution, so they.",
                    "label": 0
                },
                {
                    "sent": "They all become very similar and yeah, this is the.",
                    "label": 0
                },
                {
                    "sent": "Typically.",
                    "label": 0
                },
                {
                    "sent": "This is 1.",
                    "label": 0
                },
                {
                    "sent": "One problem of the curse of dimensionality that distances in high dimension just get very very similar and there is no discriminative power for distances anymore in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of visualization of the cursor of this aspect of the cursor.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another problem is robustness.",
                    "label": 0
                },
                {
                    "sent": "So mean and standard deviation in our case are very sensitive to outliers.",
                    "label": 1
                },
                {
                    "sent": "But when we computed the mean and standard deviation, we did not remove the outliers before because we compute them in order to know where are the outliers so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the the outliers may falsify their the mean and standard deviation and this is not a good good good deal.",
                    "label": 0
                },
                {
                    "sent": "So a lot of work has been done in, especially the statistical community too.",
                    "label": 0
                },
                {
                    "sent": "To increase the robustness of these tests, so for example, just one to mention, here is the minimum covariance determinant, virus orphaned and Leroy, but there are other work that has been done.",
                    "label": 0
                },
                {
                    "sent": "To increase the robustness of this test.",
                    "label": 0
                },
                {
                    "sent": "So yeah, for example.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so short discussion here.",
                    "label": 1
                },
                {
                    "sent": "The data distribution is usually fixed.",
                    "label": 0
                },
                {
                    "sent": "You have to decide before which kind of distribution you want to apply.",
                    "label": 0
                },
                {
                    "sent": "You have a kind of very low flexibility for this example.",
                    "label": 0
                },
                {
                    "sent": "Of course, you can have more than one model, but then you are all again, you have to specify how many normal, how many.",
                    "label": 0
                },
                {
                    "sent": "Models you allow how many processes you allow.",
                    "label": 1
                },
                {
                    "sent": "For normal objects.",
                    "label": 0
                },
                {
                    "sent": "It is a global method, obviously, and the output could could be a label, but also could be a score.",
                    "label": 0
                },
                {
                    "sent": "For example the distance to the to the mean or also the probability density or the probability itself.",
                    "label": 0
                },
                {
                    "sent": "Here's just a visualization for the problem here that we have.",
                    "label": 0
                },
                {
                    "sent": "Low flexibility, so for example, if you hear decide that the normal objects have only been generated by one mechanism, then you would compute the mean for this.",
                    "label": 0
                },
                {
                    "sent": "For these data points and you see here that the mean itself is the outlier.",
                    "label": 0
                },
                {
                    "sent": "So the mean that they are sorry.",
                    "label": 0
                },
                {
                    "sent": "The outlier point has a very small distance to the mean and would not be.",
                    "label": 0
                },
                {
                    "sent": "Considered as outlier in this in this model.",
                    "label": 0
                },
                {
                    "sent": "OK, so in this case you have to know that you have two processes that generated to different processes that generated the normal objects.",
                    "label": 0
                },
                {
                    "sent": "This one here in this one here and you have to fit the points to these two distributions using EM or whatever.",
                    "label": 0
                },
                {
                    "sent": "OK. Good.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so then death based approaches are very related to the status.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based approaches.",
                    "label": 0
                },
                {
                    "sent": "Main idea here is to get independent of the of the data distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so in in the statistical tests you have to say Oh my objects follow a normal distribution, the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So apply Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "But sometimes you cannot or you don't know what to do.",
                    "label": 0
                },
                {
                    "sent": "And this was the general idea of death based approaches.",
                    "label": 0
                },
                {
                    "sent": "So you just search for outliers at the border of the data space but.",
                    "label": 1
                },
                {
                    "sent": "Independent of of the data distribution, go ahead.",
                    "label": 0
                },
                {
                    "sent": "I think you're sure you have to use the microphone because of the sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh that sorry.",
                    "label": 0
                },
                {
                    "sent": "Now I can repeat it just just go ahead.",
                    "label": 0
                },
                {
                    "sent": "That's right, that seems to.",
                    "label": 0
                },
                {
                    "sent": "So the question was that seems to contradict to the curse of dimensionality because it's completely right in high dimensional spaces that points tend to move at the borders of the of the points of the of the data space.",
                    "label": 0
                },
                {
                    "sent": "Actually, these death based approaches are only applicable to lower dimensional points, not only because of the model, but you will see that these points rely on on convex Hull computation, and again those the only algorithm efficient algorithms for convex Hull computation in low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So, but you're completely right also from the model it's a little bit of contradiction to the curse of dimensionality, but in low dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "It might, it might work.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I'm presenting a tutorial so I'm neutral about all these approaches, but of course you can ask my personal opinion and I would say yeah, this is just for small for low dimensional data.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, that's what I said that the basic idea is to organize data objects in convex Hull layers and objects on outer layers are outliers and on inner layers are the inliers or the normal points, and obviously the basic assumption is that the authors are at the border of the data space and the normal objects are inside in the in the middle of the day in the center of the of the data space.",
                    "label": 1
                },
                {
                    "sent": "Yeah, some.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things to the model here.",
                    "label": 0
                },
                {
                    "sent": "How are these convex Hull layers defined?",
                    "label": 0
                },
                {
                    "sent": "Well, you just compute the convex Hull of the full data space and all points on these on this convex Hull.",
                    "label": 1
                },
                {
                    "sent": "For example, take this picture here.",
                    "label": 0
                },
                {
                    "sent": "So this is the convex Hull for all data objects and all the objects on this.",
                    "label": 0
                },
                {
                    "sent": "On this how have depth one.",
                    "label": 0
                },
                {
                    "sent": "Layer one, then you remove these points and compute again the convex Hull, which end up ends up in this convex Hull.",
                    "label": 0
                },
                {
                    "sent": "Here all the points on this second convex Hull have depth two or layer two, and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "OK. And then you can say OK, points having a depth of at most K are the outliers and the rest of the in line.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic idea.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are some simple algorithms that basically differ in how to efficiently compute convex hulls.",
                    "label": 0
                },
                {
                    "sent": "In terms of not to scan the data all the times for, for, for, knew convex Hull layers, but to do it in one scanner in one run.",
                    "label": 0
                },
                {
                    "sent": "So here is for example easier depth, but also FDC.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are other other algorithms but.",
                    "label": 0
                },
                {
                    "sent": "Those are.",
                    "label": 0
                },
                {
                    "sent": "Potentially the most well known one.",
                    "label": 0
                },
                {
                    "sent": "So short discussion.",
                    "label": 0
                },
                {
                    "sent": "The idea is very similar to the classical statistical approaches.",
                    "label": 1
                },
                {
                    "sent": "For one distribution, one distribution of normal objects.",
                    "label": 0
                },
                {
                    "sent": "But independent from the from the type of distribution.",
                    "label": 1
                },
                {
                    "sent": "Convex Hull computation, of course is only efficient as we said only in lower dimensional space is really efficient in in lower dimensional spaces, and while the output is usually a label, but can easily be extended to a scoring output.",
                    "label": 1
                },
                {
                    "sent": "For example, you take the depth S as the scoring value, and then you're done.",
                    "label": 0
                },
                {
                    "sent": "But you can think of course even more sophisticated scoring's, but that can be done very easy.",
                    "label": 0
                },
                {
                    "sent": "And obviously the scope the resolution is global, so it's a global approach to outlier detection, because it considers all objects.",
                    "label": 0
                },
                {
                    "sent": "Probably one point is an outlier.",
                    "label": 0
                },
                {
                    "sent": "Obviously I did it so the question is, would it be?",
                    "label": 0
                },
                {
                    "sent": "Would it remove some normal objects?",
                    "label": 0
                },
                {
                    "sent": "And obviously yes.",
                    "label": 0
                },
                {
                    "sent": "So again the basic assume.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here is outliers are at the border and inliers are in the center.",
                    "label": 1
                },
                {
                    "sent": "If this assumption holds, then this could work.",
                    "label": 0
                },
                {
                    "sent": "If not, I strongly doubt that it's not working.",
                    "label": 0
                },
                {
                    "sent": "You're completely right.",
                    "label": 0
                },
                {
                    "sent": "OK. Good.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then we already approach deviation based approaches so that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea here is that.",
                    "label": 0
                },
                {
                    "sent": "Are we given a set of data points?",
                    "label": 1
                },
                {
                    "sent": "Of course local group or the global sets don't care about it.",
                    "label": 0
                },
                {
                    "sent": "And then outliers are those points that do not fit to the general correct characteristic of of this reference set.",
                    "label": 0
                },
                {
                    "sent": "OK, for example, the variance of the set is minimized when removing the outliers.",
                    "label": 1
                },
                {
                    "sent": "That would be 1.",
                    "label": 0
                },
                {
                    "sent": "Characteristics the variance of the of the of this set and if you remove the outliers then the variance would significantly decrease and this is meant by the outliers.",
                    "label": 0
                },
                {
                    "sent": "Do not fit to the to this general characteristics and the basic assumption again is that the outliers are the outermost points of the data set.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's take 'em.",
                    "label": 0
                },
                {
                    "sent": "Little bit more closer look at the model.",
                    "label": 0
                },
                {
                    "sent": "So usually you are given or you have smoothing factor.",
                    "label": 0
                },
                {
                    "sent": "And this smoothing factor just computes for any.",
                    "label": 0
                },
                {
                    "sent": "Subset of data points.",
                    "label": 0
                },
                {
                    "sent": "How much the variance of the whole data set is decreased when this subset is removed OK?",
                    "label": 1
                },
                {
                    "sent": "And yeah, if two sets have an equal smoothing factor, then just take the smaller set anyway.",
                    "label": 1
                },
                {
                    "sent": "You're now looking for that set.",
                    "label": 0
                },
                {
                    "sent": "Which is called the exception set for which the smoothing factor is minimized.",
                    "label": 0
                },
                {
                    "sent": "OK, you're looking for.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the set of points.",
                    "label": 0
                },
                {
                    "sent": "So that if you remove this set of points, that smoothing factor is all the variance is decreased.",
                    "label": 0
                },
                {
                    "sent": "Most of the smoothing factories is high.",
                    "label": 0
                },
                {
                    "sent": "So the idea is is very similar like the statistical approach is.",
                    "label": 1
                },
                {
                    "sent": "Again for usually one distribution, but again independent of the type of distribution and the problem here is you usually cannot apply.",
                    "label": 0
                },
                {
                    "sent": "I know you solution for this because here you you just you just look for sets of points that you remove.",
                    "label": 0
                },
                {
                    "sent": "And the naive solution would test.",
                    "label": 0
                },
                {
                    "sent": "Of course, any subset of points, so you have to.",
                    "label": 1
                },
                {
                    "sent": "If I evaluate the smoothing factor of all subsets of all subsets of all points and this is of course exponential in the number of data points, so exhaustive search is not feasible.",
                    "label": 0
                },
                {
                    "sent": "So there are some papers that propose heuristics like random sampling or best first search starting from single.",
                    "label": 1
                },
                {
                    "sent": "Objects and then add other objects to this exception.",
                    "label": 0
                },
                {
                    "sent": "Exception set in the best first search manner.",
                    "label": 0
                },
                {
                    "sent": "So you need some risztics and that of course means that you usually end up in not the optimal solution, but sub optimal solution of course.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the good thing is applicable to any type of data, just depending on the on the definition of the smoothing factor.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it was originally designed as a global method, but you can also think of of local methods here where you just have to.",
                    "label": 0
                },
                {
                    "sent": "Just take not the whole database as as this reference set here but small reference sense.",
                    "label": 0
                },
                {
                    "sent": "But again then the question is how to determine these references.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and the output here is a label, so you get the set of outliers and the rest are the inlines.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we took one.",
                    "label": 0
                },
                {
                    "sent": "Just to remind you, one should one of you should watch the Clock because yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh perfect, I'm just ahead of schedule.",
                    "label": 0
                },
                {
                    "sent": "Perfect, looks good for my flight.",
                    "label": 0
                },
                {
                    "sent": "OK, but now it's getting a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Now little bit more meat in the in the.",
                    "label": 0
                },
                {
                    "sent": "In the next chapter, so distance based.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just the general idea of these approaches is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, to evaluate points based on the distances to the neighbors of this point of the point.",
                    "label": 1
                },
                {
                    "sent": "And there are several variants.",
                    "label": 0
                },
                {
                    "sent": "Which we will discuss right now, but just before the basic assumption here is that the normal data objects have a dense neighborhood means the neighbors are very near closely.",
                    "label": 1
                },
                {
                    "sent": "And the outliers are far apart from their neighbors.",
                    "label": 1
                },
                {
                    "sent": "OK, so have less dense neighborhoods.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the maybe most prominent and.",
                    "label": 0
                },
                {
                    "sent": "Inspiring approaches here in this distance based class is the density based out or distance or distance based outlier?",
                    "label": 0
                },
                {
                    "sent": "Proposed by Norand Rim Dang, where we have two parameters, distance radius at a distance threshold epsilon and a percentage P. And now a point is just considered an outlier.",
                    "label": 0
                },
                {
                    "sent": "If at most P percent of all other points have a distance to P less than epsilon, OK, you just consider the neighborhood of P of all points that have a distance less than epsilon, and if they are.",
                    "label": 1
                },
                {
                    "sent": "Less than a given threshold in this epsilon neighborhood, then it's an outlier.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's normal object so.",
                    "label": 0
                },
                {
                    "sent": "Here, for example, for these two points here, this is there, the epsilon parameter.",
                    "label": 0
                },
                {
                    "sent": "So you consider the neighborhood of P. Or around each point with radius epsilon and you see that in this case for P1 and P2 you have zero points in the neighborhood and usually have a threshold above 0.",
                    "label": 0
                },
                {
                    "sent": "So in this case these two points would be considered as outliers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                },
                {
                    "sent": "This formula here you just compute the range query for each point with radius epsilon, count the number of points, normalize it and then if it exceeds a threshold then it's an inlier.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's an outlier.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, there are some several algorithms to compute these distance based outliers.",
                    "label": 0
                },
                {
                    "sent": "One approach is index based, which just computes a join.",
                    "label": 0
                },
                {
                    "sent": "So what do you have to do so you have to compute the epsilon neighborhood of all points and this is this problem is known as a similarity join between points.",
                    "label": 0
                },
                {
                    "sent": "In this case, the distance range join between points and this can be done quite efficiently using spatial index structures.",
                    "label": 1
                },
                {
                    "sent": "If the points are Euclidean.",
                    "label": 0
                },
                {
                    "sent": "If not, if you have a metric distance function, then you can also use metric index structures, but OK anyway, and the idea of this algorithm is to use the distance range join and exclude points from further consideration that have already enough points in the epsilon neighborhood so that an epsilon range join usually traverses the data set twice, so to say.",
                    "label": 0
                },
                {
                    "sent": "You just to have two copies of your data set, and especially next structure, and the joint reverses both index structures and keeps track for each object if yeah.",
                    "label": 0
                },
                {
                    "sent": "How many points are?",
                    "label": 0
                },
                {
                    "sent": "In that same neighborhood, based on approximations, and if you are sure that the point cannot be an outlier, then you just can discard it from further or prune it from further consideration.",
                    "label": 0
                },
                {
                    "sent": "A second, a second, sorry a second algorithm, is a nested loop based algorithm.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "Now the main memory buffer is divided into parts and the second part is just used to scan and compare all points with points from the first part.",
                    "label": 1
                },
                {
                    "sent": "So you just load one part of the data base in the first part of the buffer and then the second part of the buffer is used to go through the whole data set and compare points.",
                    "label": 0
                },
                {
                    "sent": "OK, so classical nested loop join.",
                    "label": 0
                },
                {
                    "sent": "And the third kind of algorithm is a grid based approach, so here the idea is to build a grid such that any two points from the same grid cell have a distance of at most epsilon to each other.",
                    "label": 1
                },
                {
                    "sent": "And then it's clear if you think about it a second time, it's clear that points need only or any point need only compare with points from neighboring cells because.",
                    "label": 0
                },
                {
                    "sent": "Other cells.",
                    "label": 0
                },
                {
                    "sent": "Can only contain points that have distance of more than epsilon to the point.",
                    "label": 0
                },
                {
                    "sent": "OK, so this just reduces the number of comparisons between points in order to compute these epsilon neighborhoods of points.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, then there is an interesting extension to the original work, which is not really an outlier model, but I think it's worth mentioning because it's one of the core points of data mining.",
                    "label": 0
                },
                {
                    "sent": "Office here.",
                    "label": 0
                },
                {
                    "sent": "Came up with a solution to derive knowledge about the outliers.",
                    "label": 0
                },
                {
                    "sent": "To declare or to describe the outliers that are found.",
                    "label": 0
                },
                {
                    "sent": "So there the idea is based on this on this distance based outlier model we just discussed here and.",
                    "label": 0
                },
                {
                    "sent": "What what they did is they try to find minimal subsets of attributes that explains the outline Ness of a point so.",
                    "label": 1
                },
                {
                    "sent": "Some sets of attributes in which the point is still an outlier, so maybe I just try to make it clear by this example here.",
                    "label": 0
                },
                {
                    "sent": "So for example, the following players in this is just a sports statistics data set for some hockey players you have some measurements like power play goals, shorthanded goals, whatever.",
                    "label": 0
                },
                {
                    "sent": "We need not to be in hockey expert, two or two, or understand that.",
                    "label": 0
                },
                {
                    "sent": "So the point is well, these four guys are.",
                    "label": 0
                },
                {
                    "sent": "Detected as outliers, but this model here and the intentional knowledge derived from this result is the following.",
                    "label": 0
                },
                {
                    "sent": "So Marion Linear is an outlier already in the 1 dimensional space of Power play goal, so just considering only this attribute means or would already find this player as an outlier and also the two dimensional space of shorthanded goals in game winning goals.",
                    "label": 0
                },
                {
                    "sent": "So just considering two these two.",
                    "label": 0
                },
                {
                    "sent": "Attributes here would also be enough to identify Mario Lemieux as as an outlier.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And I hope it's not a little bit clearer, so the method tries to find the minimum subset of attributes that already is is already sufficient to make this point an outlier.",
                    "label": 0
                },
                {
                    "sent": "And the same for the other.",
                    "label": 0
                },
                {
                    "sent": "For the other guys here.",
                    "label": 0
                },
                {
                    "sent": "And this is of course very interesting, because that gives the domain expert little bit more insight into the data set.",
                    "label": 0
                },
                {
                    "sent": "Telling a hockey freak, Mario Lemieux is an outlier.",
                    "label": 0
                },
                {
                    "sent": "The frequency?",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course.",
                    "label": 0
                },
                {
                    "sent": "There's an outstanding player, but on the other side you can say OK, he's outstanding because he has once a power play.",
                    "label": 0
                },
                {
                    "sent": "Goals seems to have a lot of power, play goals and a lot of short handed goals and whatever.",
                    "label": 0
                },
                {
                    "sent": "So you give him a little bit more than just the outliers, but also an explanation on what constitutes the outlier.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, other outlier, other distance based approaches are usually based on the distance to the K nearest neighbors of objects.",
                    "label": 0
                },
                {
                    "sent": "So are there?",
                    "label": 0
                },
                {
                    "sent": "Basically 22 variants here.",
                    "label": 0
                },
                {
                    "sent": "The first model just takes the K nearest neighbor distance, so this is the distance to the K nearest neighbor of a point at the K nearest neighbor distance of a point is an outlier score.",
                    "label": 1
                },
                {
                    "sent": "The higher the distance, the higher the outline of the point, the smaller the distance, the more the point is a normal observation.",
                    "label": 0
                },
                {
                    "sent": "And the second variant here is that not only the K nearest neighbor distance is considered, but you compute the 1, two, or three, and so on up to the K nearest neighbor.",
                    "label": 1
                },
                {
                    "sent": "The distances and then aggregate these distances.",
                    "label": 0
                },
                {
                    "sent": "For example, take the mean, the average or whatever.",
                    "label": 0
                },
                {
                    "sent": "So these are the basic variants here, but.",
                    "label": 0
                },
                {
                    "sent": "Both rely on the idea of.",
                    "label": 0
                },
                {
                    "sent": "Evaluating the key nearest neighbor distances of points.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of algorithms here.",
                    "label": 0
                },
                {
                    "sent": "Because again, you have the problem that you have to compute the K nearest neighbors of all objects.",
                    "label": 0
                },
                {
                    "sent": "This is again a kind of similarity join.",
                    "label": 0
                },
                {
                    "sent": "In this case K nearest neighbor join and there are several approaches to speed this up.",
                    "label": 0
                },
                {
                    "sent": "So the basic ideas of these approaches is a kind of nested loop algorithm.",
                    "label": 0
                },
                {
                    "sent": "So for each object computer can use neighbors with a sequential scan or with a spatial index structure.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Most approaches try to shrink down the search space here, not to compute.",
                    "label": 0
                },
                {
                    "sent": "For each object, all K nearest neighbors, but 22.",
                    "label": 0
                },
                {
                    "sent": "To minimize the number of distance comparisons and so on.",
                    "label": 1
                },
                {
                    "sent": "And the second general approach here is a petition based approach.",
                    "label": 0
                },
                {
                    "sent": "So usually these approaches try to partition the data into a kind of micro clusters and then they try to aggregate some certain information for each of these petitions.",
                    "label": 0
                },
                {
                    "sent": "For example, minimum bounding rectangles of the points in a micro cluster, and these aggregated information of course should allow to prune micro clusters that cannot qualify when searching for the K nearest neighbors of a particular point.",
                    "label": 1
                },
                {
                    "sent": "Which is very similar to the idea of using spatial index structures but not using index structures in that case, but using micro clusters but.",
                    "label": 0
                },
                {
                    "sent": "Clustering and and indexing objects is a very, very related problems.",
                    "label": 0
                },
                {
                    "sent": "So here are just some sampling.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Algorithms, some sample algorithms.",
                    "label": 0
                },
                {
                    "sent": "Usually these algorithms compute the top end outliers, so they have this restriction in order to again prune points or or minimize the number of computations necessary.",
                    "label": 0
                },
                {
                    "sent": "We have nested loop algorithms by.",
                    "label": 0
                },
                {
                    "sent": "Swami at all.",
                    "label": 0
                },
                {
                    "sent": "We have a kind of linearization of a multidimensional data set using a space filling curve.",
                    "label": 1
                },
                {
                    "sent": "In order to get a 1 dimensional representation, which is that then petitioned.",
                    "label": 1
                },
                {
                    "sent": "Into micro clusters and this is done by anjuli and sodium.",
                    "label": 1
                },
                {
                    "sent": "Orca by benj.",
                    "label": 0
                },
                {
                    "sent": "Schwab has very well known, which is kind of nested loop algorithm with the randomization and a very simple pruning schema in order to to get the top N outliers quite fast.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing with this algorithm is that it's it's really applicable for both Kane Anderson's models and also the distance based on their model of Nora name.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then there are some.",
                    "label": 0
                },
                {
                    "sent": "Variance of our car, for example RBRP.",
                    "label": 0
                },
                {
                    "sent": "Which increases the pruning power.",
                    "label": 1
                },
                {
                    "sent": "Minimizes the number of computations even more, and other approaches, for example, based on my micro clustering, though, so partitioning the data into micro clusters or also approximate solutions.",
                    "label": 1
                },
                {
                    "sent": "For example, one work by John Pay and colleagues to based on reference points.",
                    "label": 0
                },
                {
                    "sent": "But all these algorithms are not really knew models, they just compute outliers based on these K&M.",
                    "label": 0
                },
                {
                    "sent": "Inspired models, they just are.",
                    "label": 0
                },
                {
                    "sent": "Algorithms for speeding up the computation?",
                    "label": 0
                },
                {
                    "sent": "OK, so don't get disturbed.",
                    "label": 0
                },
                {
                    "sent": "None of these algorithms, or none of these papers introduce new models, new outlier models.",
                    "label": 0
                },
                {
                    "sent": "They all use these KNN based based outline models.",
                    "label": 1
                },
                {
                    "sent": "They just propose new algorithms.",
                    "label": 0
                },
                {
                    "sent": "If more efficient algorithms for further computation, we just have to introduce a threshold saying OK points that have a KNN distance more than the threshold.",
                    "label": 0
                },
                {
                    "sent": "Are outliers and the others are inliers?",
                    "label": 0
                },
                {
                    "sent": "Approaches are usually local, and the resolution that the size of the reference set can be adjusted by the user via these these input parameters for for the density based for the distance based approaches.",
                    "label": 1
                },
                {
                    "sent": "This is the epsilon.",
                    "label": 0
                },
                {
                    "sent": "Which somehow defines how big the reference set is, but of course it's not really obvious how big it will be.",
                    "label": 0
                },
                {
                    "sent": "If you set epsilon to whatever 5 or so, then it's not really obvious how many points will be in the five in the radius of five.",
                    "label": 0
                },
                {
                    "sent": "Points and of course for many poor for different points.",
                    "label": 0
                },
                {
                    "sent": "The size of the reference set could could be different because of this distance threshold absolute and for for the K nearest neighbor distance models.",
                    "label": 0
                },
                {
                    "sent": "Of course you can specify the number of K. Nearest neighbors that are considered so in this case all points have a similar sized reference set in this case.",
                    "label": 0
                }
            ]
        }
    }
}