{
    "id": "uffeqbzyk73qshqzxs6fadxfssflm43f",
    "title": "Fast Support Vector Machines for Structural Kernels",
    "info": {
        "author": [
            "Aliaksei Severyn, Department of Information Engineering and Computer Science (DISI), University of Trento"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_severyn_fast/",
    "segmentation": [
        [
            "Good morning everybody.",
            "I'm Alexis everything and this is joint work with Alessandra Mascetti.",
            "So in this presentation about I'll talk about a fast algorithm to train SVM's when we want to apply for learning unstructured data, and we apply structural kernels so."
        ],
        [
            "So.",
            "Much of real data is actually structured, so if you think of sequences like DNA sequences and biology or sequences of words in a sentence or even trees when you speak about syntactic parse trees of natural language sentences, or for example XML, documents, graphs an you want to exploit this structure in the input data to build a more accurate classifiers.",
            "An typical way to go is kind of.",
            "Do some preprocessing and feature engineering and feature extraction to embed this objects into a real vector space where we can apply learning algorithms.",
            "But this is not typically an easy job to do, because you might know the domain well.",
            "So feature engine can be a tedious and difficult task to do."
        ],
        [
            "So, for example, consider this task in natural language processing of predicate argument identification.",
            "So given a sentence gives a nice talk and here you have.",
            "A parse tree over this sentence.",
            "And you want to extract and exploit the dependencies between the words.",
            "So one I."
        ],
        [
            "Idea is to generate all kinds of tree fragments.",
            "Anne."
        ],
        [
            "And build linear space vector that represents this structure.",
            "But imagine you have large trees.",
            "And a big number of examples.",
            "So building this vector space explicitly will be very tedious, and the feature vectors they will be extremely high dimensional, so typical way to go is to use kernels for this stuff."
        ],
        [
            "How and kernels have been applied in many tasks, especially in natural language processing, and they show very good results in such tasks as relation extraction, coreference resolution, semantical labeling to name a few.",
            "Also information retrieval in by informatics."
        ],
        [
            "However, there is a problem when you want to apply to large data kernel methods.",
            "They typically scale quadratically in the number of examples.",
            "And this fact prohibits application of VM's with structural kernels on large data."
        ],
        [
            "So the idea of this paper to make an algorithm to train SVM so with structural kernels much faster so you can apply it for training on large data.",
            "So the first 10 key idea of this paper is this idea of compact model representation an we use a directed acyclic graphs, an we show that we can achieve great speedups in both training and classification.",
            "The second point is we show how it is easy to parallelize the algorithm and also we propose an alternative strategy for handling class imbalance problem, which often arises in many an OPI tasks."
        ],
        [
            "So as the training algorithm, we focus on cutting plane algorithm, so I will shortly introduce the main idea behind this algorithm.",
            "For people who are not familiar with it.",
            "It was introduced in the context of structural SVM's in 2004, and the nice thing about this algorithm is that it gives linear training time with linear kernels.",
            "So when you apply linear kernels, it's very fast algorithm and the training times are very short."
        ],
        [
            "So I want to give you a feeling how this algorithm works, so you start off.",
            "So this is the optimization problem you are trying to solve.",
            "So there are many constraints, and the optimal point life somewhere near so.",
            "The key idea here is that there are actually very small number of constraints that define the feasible region where your optimal data point lies.",
            "And the current plane algorithm.",
            "What it is doing is it starts off with unconstrained optimization problem.",
            "So you don't have any constraints at all.",
            "You find optimal point."
        ],
        [
            "And an at the next step step you find most violated constraint.",
            "You add it to your optimization problem.",
            "You resolve the quadratic programming problem."
        ],
        [
            "An iterate adding most violated constraints at each iteration."
        ],
        [
            "Until you arrive at solution where no constraints are violated."
        ],
        [
            "So the key bottleneck in this algorithm is computing this current plane, so this most violated constraint at each iteration, and this is an inner product between your model or your weight vector, and you have to do this for each training example in order to build the cutting plane algorithm.",
            "So imagine you're at iteration T, so you have T cutting plane models in your active set, and you have to.",
            "Compute this inner product for each example, so this G is exactly exactly corresponds to each pattern plane model in your active set."
        ],
        [
            "Constraints an it's a linear combination of your training examples where this variable CK it's a binary variable.",
            "It's zero or one.",
            "An indicates if the training examples included in the model or not."
        ],
        [
            "So this is the at the bottom.",
            "You see this key formula.",
            "It's kind of complex, but.",
            "The main point is that you have to compute this quantity for each training example and you can see that it's linear in the number of kernel evaluations you have to do.",
            "But since you have to do this for each training example, it's quadratic in the number of training exam."
        ],
        [
            "So this is a represents a main bottleneck."
        ],
        [
            "And.",
            "And recently, in 29 UN Jakeem.",
            "So they proposed an approximate cutting plane algorithm.",
            "So the idea is to use samples.",
            "So instead of computing exact plane algorithm for entire training set, you do sampling.",
            "So the complexity goes down from being quadratic in the number of training points to quadratic in the number of samples in your."
        ],
        [
            "Apple.",
            "So we apply this algorithm in 2010 for SVM's with structural kernels on very large data.",
            "Millions of examples an we show that it achieves very good speedups up to 10 over conventional SVM such as SVM light.",
            "So to give give you are feeling the training time goes down from 7.5 days to half a day, so it's already very nice, but the question can we do better than this?",
            "And the answer is yes."
        ],
        [
            "And one idea to do this is to exploit this inherent structure in your input data.",
            "So the captivation is that structured data such as sequences, trees and graphs.",
            "Many examples that they share common substructures and the key idea to reduce the number of expensive kernel evaluations is to avoid computations over repeating substructures and what you can do is you can use directed.",
            "Cyclic graphs to compact collection of trees.",
            "And I would like to know that compacting a collection of trees into a single dog gives you exact kernel evaluation and the proof short proof is in."
        ],
        [
            "Paper.",
            "So here on this slide you have an example of three syntactic trees and how they can be compacted into a single dog.",
            "So the only extra information you have to maintain is frequencies of the corresponding subtrees rooted at each respective node."
        ],
        [
            "So let's say this blue tree is this blue cloud in the equivalent."
        ],
        [
            "Tag.",
            "This read the trees."
        ],
        [
            "Red Cloud An the green tree is in this rain cloud, so you have compacted this collection of three trees into a single equivalent dog that yields exact kernel evaluations."
        ],
        [
            "So I want to recall you this main bottleneck that you have to compute for each training example this inner product between your model and training point.",
            "And by using this idea of compacting the model into a dog, you can actually speed up the algorithm.",
            "So there are two ways to do this.",
            "So the 1st way is to compact each kind of plane model into each single."
        ],
        [
            "Respective doc.",
            "So in red you see the current plane model."
        ],
        [
            "And you can compact this combination of training points into a single dog, so the formula reduces to some of kernel evaluations between Doug and training points.",
            "So this algorithm is called as Doug."
        ],
        [
            "Another even better idea is to compact all cutting plane models in your working set into a single dog.",
            "So you're taking this whole sum over all cutting plane models over all training examples in each cutting."
        ],
        [
            "Model and you compacted into one big dog so such that the inner product between between your model and training example reduces to a single kernel evaluation."
        ],
        [
            "So as an example, given.",
            "Training point X and your model dog."
        ],
        [
            "So when you compute the kernel.",
            "I mean this example and you're a model.",
            "You are considering different substructures, different subtrees, and you just match them instead of matching between the collection of trees, you match them with.",
            "In your example, an Doug."
        ],
        [
            "So here you can see that the computational savings come from that you have to consider this substructure in the equivalent drug only once."
        ],
        [
            "So now I move on to the experimental setup.",
            "We carried out our experiments on three datasets.",
            "The first data set is semantic role labeling data set.",
            "Then the task is predicate argument identification.",
            "That's what I showed in the very beginning.",
            "Also, the other data set we tested our algorithms on is Yahoo.",
            "Answers and the task was Question, Answer classification and we also did question classification on track data."
        ],
        [
            "So I want to talk about a little bit more about this first data set.",
            "Semantic role labeling datasets.",
            "So given a sentence, the task is.",
            "Identifying argument boundaries.",
            "It's this data set consists of prop bank, Penn Treebank and charniak parse trees as from kernel 2005.",
            "And the training set we tested on is 100,000 examples.",
            "And there are two test sets that we worked on."
        ],
        [
            "So these are results on this semantic role labeling data set.",
            "So I want to stress that since the accuracy is the same so the quantity to compare, we are interested in is only the training and classification times.",
            "So the bottom line is.",
            "The algorithm we want to compare it to is in blue, it's called USVM.",
            "It's a very strong algorithm to compare with its algorithm.",
            "In our previous paper, and it's already much faster than conventional SVM and this bread and green bars you can see is these are two algorithms that we propose in the paper as DACA NASDAQ plus an.",
            "You can see that this.",
            "Speed up so you can get they vary with respect to the sample size.",
            "So the larger the sample size you used to approximate the cutting plane algorithm, the cutting plane at each step, the larger speed up your obtain.",
            "So you can arrive up to speed up up to 20.",
            "So this is for training."
        ],
        [
            "And also for classification and here you see that as Duck Plus does much better job becausw it compacts all cutting plane models in your model into one single DAG.",
            "So you obtain an equivalent and more compact model representation an.",
            "By doing so, you get even bigger speedups."
        ],
        [
            "So the next small point is parallelization.",
            "We observe that 95% of time of computational time is spent computing cutting plane models at each iteration and the cutting plane algorithm allows a straightforward way to paralyze.",
            "At such that you can bring down the complexity from being squared in the sample size up to squared divided by the number of CPUs you have at hand."
        ],
        [
            "And this graph shows you how.",
            "It scales with respect to the number of CPUs, so by using eight CPUs you can speed up.",
            "You can get a speedup up to 7.",
            "And the larger your sample size is, the better speedup you get."
        ],
        [
            "And the last point I want to talk about is how to handle glass imbalance problem.",
            "So optimization problem you're actually solving by applying cutting plane algorithm is slightly different from conventional optimization problem that SVM light is solving so it's difficult to include penalties for examples from different classes.",
            "So this idea of out waiting positive and negative examples is difficult to apply in this setting.",
            "So however, sampling to build approximate cutting planes at each iteration suggests a very straightforward solution using importance sampling such that in each sample you have a very nice control in which proportion examples from different classes enter your sample an in the paper we actually show that this alternative sampling.",
            "Strategy.",
            "It preserves theoretical convergence bounds."
        ],
        [
            "And our experiments on track and Yahoo answers data set showed that it actually outperforms the original algorithm when tuning is needed.",
            "It's still as fast as the original algorithm an it actually gives you a more flexible control over precision and recall than SVM light."
        ],
        [
            "So to conclude, you have seen two learning algorithms, namely as Doug NASDAQ Plus.",
            "Of that compact cutting plane model at each iteration, by using directed acyclic graphs and this gives much faster training and classification times.",
            "Another thing is that is very important when you want to train SVM's with kernels.",
            "Is parallelization becausw SVM light or as family?",
            "They are not parallel.",
            "Also, we shown alternative sampling strategy to Bender to better handle class imbalance data at large scale.",
            "And.",
            "And there is software available, so if you're interested you can download it and it will be available soon after the conference."
        ],
        [
            "So thank you so the future work we plan to extend this DAG approach to more general kernels, for example, partial tree kernel.",
            "We also would like to explore other structural kernels on graphs and we would like to also to experiment with other tax tasks, for example from NLP, such as a relation extraction and coreference resolution.",
            "An also an important observation is that here I've shown the currently in algorithm but actually this compact model representation using drugs is not confined only to the cutting plane algorithm.",
            "So as long as your algorithm.",
            "Build some model that is a linear combination of training examples.",
            "You can apply this idea also."
        ],
        [
            "So thank you an if you have questions welcome.",
            "Thank you for the talk.",
            "The question is actually for each paper unstructured kernels.",
            "We have the same remark.",
            "There is a different domain which comes from graphical models.",
            "So in the case of of sequences definitely you could test easiest model like CRF.",
            "Where the inference is exact and definitely would get better results.",
            "So question is very simple.",
            "Have you ever tried to compare your methods with any of existing packages in graphical models?",
            "Like CRF, conditional random fields.",
            "OK, so the distinction I want to clarify is that by structure or kernels we want to capture structure in the input, so the output is still very simple.",
            "It's we're dealing with binary classification.",
            "You trade structure in the label in the input, so there is no problem of inference here.",
            "More questions.",
            "Can you clarify a little bit your last point about the imbalance?",
            "So what you do there is kind of under sampling of the majority class.",
            "Yes, but since we draw samples in the iterative fashion.",
            "Such that like for example, typical conventional sampling methods, you've given the training set you either under sample over sample, and if you're under sample, you kind of lose the important information present in the examples that didn't end up in your sample, But the nice property of the cutting plane algorithm is that.",
            "At each iteration you draw a sample from your training set.",
            "And by others, by oversampling the minority class you have this flexibility to build actually a sample where examples from different classes enter in the desired proportion.",
            "So you get a balanced proportion between positive and negative examples.",
            "But since you are doing this in iterative fashion.",
            "Actually this gives you.",
            "The nice property of.",
            "Maintaining the overall look on the entire training set.",
            "So basically it's kind of.",
            "Sampling method, but it's directly built into the optimization process.",
            "So it's a distinctive property of this weight class imbalance problem.",
            "Alright, well let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "I'm Alexis everything and this is joint work with Alessandra Mascetti.",
                    "label": 0
                },
                {
                    "sent": "So in this presentation about I'll talk about a fast algorithm to train SVM's when we want to apply for learning unstructured data, and we apply structural kernels so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Much of real data is actually structured, so if you think of sequences like DNA sequences and biology or sequences of words in a sentence or even trees when you speak about syntactic parse trees of natural language sentences, or for example XML, documents, graphs an you want to exploit this structure in the input data to build a more accurate classifiers.",
                    "label": 0
                },
                {
                    "sent": "An typical way to go is kind of.",
                    "label": 0
                },
                {
                    "sent": "Do some preprocessing and feature engineering and feature extraction to embed this objects into a real vector space where we can apply learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "But this is not typically an easy job to do, because you might know the domain well.",
                    "label": 0
                },
                {
                    "sent": "So feature engine can be a tedious and difficult task to do.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, for example, consider this task in natural language processing of predicate argument identification.",
                    "label": 0
                },
                {
                    "sent": "So given a sentence gives a nice talk and here you have.",
                    "label": 0
                },
                {
                    "sent": "A parse tree over this sentence.",
                    "label": 0
                },
                {
                    "sent": "And you want to extract and exploit the dependencies between the words.",
                    "label": 0
                },
                {
                    "sent": "So one I.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Idea is to generate all kinds of tree fragments.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And build linear space vector that represents this structure.",
                    "label": 0
                },
                {
                    "sent": "But imagine you have large trees.",
                    "label": 0
                },
                {
                    "sent": "And a big number of examples.",
                    "label": 0
                },
                {
                    "sent": "So building this vector space explicitly will be very tedious, and the feature vectors they will be extremely high dimensional, so typical way to go is to use kernels for this stuff.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How and kernels have been applied in many tasks, especially in natural language processing, and they show very good results in such tasks as relation extraction, coreference resolution, semantical labeling to name a few.",
                    "label": 0
                },
                {
                    "sent": "Also information retrieval in by informatics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, there is a problem when you want to apply to large data kernel methods.",
                    "label": 0
                },
                {
                    "sent": "They typically scale quadratically in the number of examples.",
                    "label": 1
                },
                {
                    "sent": "And this fact prohibits application of VM's with structural kernels on large data.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea of this paper to make an algorithm to train SVM so with structural kernels much faster so you can apply it for training on large data.",
                    "label": 1
                },
                {
                    "sent": "So the first 10 key idea of this paper is this idea of compact model representation an we use a directed acyclic graphs, an we show that we can achieve great speedups in both training and classification.",
                    "label": 0
                },
                {
                    "sent": "The second point is we show how it is easy to parallelize the algorithm and also we propose an alternative strategy for handling class imbalance problem, which often arises in many an OPI tasks.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as the training algorithm, we focus on cutting plane algorithm, so I will shortly introduce the main idea behind this algorithm.",
                    "label": 0
                },
                {
                    "sent": "For people who are not familiar with it.",
                    "label": 0
                },
                {
                    "sent": "It was introduced in the context of structural SVM's in 2004, and the nice thing about this algorithm is that it gives linear training time with linear kernels.",
                    "label": 1
                },
                {
                    "sent": "So when you apply linear kernels, it's very fast algorithm and the training times are very short.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to give you a feeling how this algorithm works, so you start off.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization problem you are trying to solve.",
                    "label": 0
                },
                {
                    "sent": "So there are many constraints, and the optimal point life somewhere near so.",
                    "label": 0
                },
                {
                    "sent": "The key idea here is that there are actually very small number of constraints that define the feasible region where your optimal data point lies.",
                    "label": 0
                },
                {
                    "sent": "And the current plane algorithm.",
                    "label": 0
                },
                {
                    "sent": "What it is doing is it starts off with unconstrained optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So you don't have any constraints at all.",
                    "label": 0
                },
                {
                    "sent": "You find optimal point.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And an at the next step step you find most violated constraint.",
                    "label": 1
                },
                {
                    "sent": "You add it to your optimization problem.",
                    "label": 0
                },
                {
                    "sent": "You resolve the quadratic programming problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An iterate adding most violated constraints at each iteration.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until you arrive at solution where no constraints are violated.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key bottleneck in this algorithm is computing this current plane, so this most violated constraint at each iteration, and this is an inner product between your model or your weight vector, and you have to do this for each training example in order to build the cutting plane algorithm.",
                    "label": 1
                },
                {
                    "sent": "So imagine you're at iteration T, so you have T cutting plane models in your active set, and you have to.",
                    "label": 0
                },
                {
                    "sent": "Compute this inner product for each example, so this G is exactly exactly corresponds to each pattern plane model in your active set.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraints an it's a linear combination of your training examples where this variable CK it's a binary variable.",
                    "label": 0
                },
                {
                    "sent": "It's zero or one.",
                    "label": 0
                },
                {
                    "sent": "An indicates if the training examples included in the model or not.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the at the bottom.",
                    "label": 0
                },
                {
                    "sent": "You see this key formula.",
                    "label": 0
                },
                {
                    "sent": "It's kind of complex, but.",
                    "label": 0
                },
                {
                    "sent": "The main point is that you have to compute this quantity for each training example and you can see that it's linear in the number of kernel evaluations you have to do.",
                    "label": 0
                },
                {
                    "sent": "But since you have to do this for each training example, it's quadratic in the number of training exam.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a represents a main bottleneck.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And recently, in 29 UN Jakeem.",
                    "label": 0
                },
                {
                    "sent": "So they proposed an approximate cutting plane algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to use samples.",
                    "label": 0
                },
                {
                    "sent": "So instead of computing exact plane algorithm for entire training set, you do sampling.",
                    "label": 0
                },
                {
                    "sent": "So the complexity goes down from being quadratic in the number of training points to quadratic in the number of samples in your.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apple.",
                    "label": 0
                },
                {
                    "sent": "So we apply this algorithm in 2010 for SVM's with structural kernels on very large data.",
                    "label": 1
                },
                {
                    "sent": "Millions of examples an we show that it achieves very good speedups up to 10 over conventional SVM such as SVM light.",
                    "label": 1
                },
                {
                    "sent": "So to give give you are feeling the training time goes down from 7.5 days to half a day, so it's already very nice, but the question can we do better than this?",
                    "label": 0
                },
                {
                    "sent": "And the answer is yes.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one idea to do this is to exploit this inherent structure in your input data.",
                    "label": 0
                },
                {
                    "sent": "So the captivation is that structured data such as sequences, trees and graphs.",
                    "label": 1
                },
                {
                    "sent": "Many examples that they share common substructures and the key idea to reduce the number of expensive kernel evaluations is to avoid computations over repeating substructures and what you can do is you can use directed.",
                    "label": 1
                },
                {
                    "sent": "Cyclic graphs to compact collection of trees.",
                    "label": 0
                },
                {
                    "sent": "And I would like to know that compacting a collection of trees into a single dog gives you exact kernel evaluation and the proof short proof is in.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper.",
                    "label": 0
                },
                {
                    "sent": "So here on this slide you have an example of three syntactic trees and how they can be compacted into a single dog.",
                    "label": 0
                },
                {
                    "sent": "So the only extra information you have to maintain is frequencies of the corresponding subtrees rooted at each respective node.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say this blue tree is this blue cloud in the equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tag.",
                    "label": 0
                },
                {
                    "sent": "This read the trees.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Red Cloud An the green tree is in this rain cloud, so you have compacted this collection of three trees into a single equivalent dog that yields exact kernel evaluations.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to recall you this main bottleneck that you have to compute for each training example this inner product between your model and training point.",
                    "label": 0
                },
                {
                    "sent": "And by using this idea of compacting the model into a dog, you can actually speed up the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there are two ways to do this.",
                    "label": 0
                },
                {
                    "sent": "So the 1st way is to compact each kind of plane model into each single.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Respective doc.",
                    "label": 0
                },
                {
                    "sent": "So in red you see the current plane model.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can compact this combination of training points into a single dog, so the formula reduces to some of kernel evaluations between Doug and training points.",
                    "label": 0
                },
                {
                    "sent": "So this algorithm is called as Doug.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another even better idea is to compact all cutting plane models in your working set into a single dog.",
                    "label": 0
                },
                {
                    "sent": "So you're taking this whole sum over all cutting plane models over all training examples in each cutting.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Model and you compacted into one big dog so such that the inner product between between your model and training example reduces to a single kernel evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as an example, given.",
                    "label": 0
                },
                {
                    "sent": "Training point X and your model dog.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you compute the kernel.",
                    "label": 0
                },
                {
                    "sent": "I mean this example and you're a model.",
                    "label": 0
                },
                {
                    "sent": "You are considering different substructures, different subtrees, and you just match them instead of matching between the collection of trees, you match them with.",
                    "label": 0
                },
                {
                    "sent": "In your example, an Doug.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here you can see that the computational savings come from that you have to consider this substructure in the equivalent drug only once.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I move on to the experimental setup.",
                    "label": 1
                },
                {
                    "sent": "We carried out our experiments on three datasets.",
                    "label": 1
                },
                {
                    "sent": "The first data set is semantic role labeling data set.",
                    "label": 0
                },
                {
                    "sent": "Then the task is predicate argument identification.",
                    "label": 0
                },
                {
                    "sent": "That's what I showed in the very beginning.",
                    "label": 0
                },
                {
                    "sent": "Also, the other data set we tested our algorithms on is Yahoo.",
                    "label": 0
                },
                {
                    "sent": "Answers and the task was Question, Answer classification and we also did question classification on track data.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to talk about a little bit more about this first data set.",
                    "label": 0
                },
                {
                    "sent": "Semantic role labeling datasets.",
                    "label": 0
                },
                {
                    "sent": "So given a sentence, the task is.",
                    "label": 0
                },
                {
                    "sent": "Identifying argument boundaries.",
                    "label": 0
                },
                {
                    "sent": "It's this data set consists of prop bank, Penn Treebank and charniak parse trees as from kernel 2005.",
                    "label": 1
                },
                {
                    "sent": "And the training set we tested on is 100,000 examples.",
                    "label": 1
                },
                {
                    "sent": "And there are two test sets that we worked on.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are results on this semantic role labeling data set.",
                    "label": 0
                },
                {
                    "sent": "So I want to stress that since the accuracy is the same so the quantity to compare, we are interested in is only the training and classification times.",
                    "label": 0
                },
                {
                    "sent": "So the bottom line is.",
                    "label": 0
                },
                {
                    "sent": "The algorithm we want to compare it to is in blue, it's called USVM.",
                    "label": 0
                },
                {
                    "sent": "It's a very strong algorithm to compare with its algorithm.",
                    "label": 0
                },
                {
                    "sent": "In our previous paper, and it's already much faster than conventional SVM and this bread and green bars you can see is these are two algorithms that we propose in the paper as DACA NASDAQ plus an.",
                    "label": 0
                },
                {
                    "sent": "You can see that this.",
                    "label": 0
                },
                {
                    "sent": "Speed up so you can get they vary with respect to the sample size.",
                    "label": 0
                },
                {
                    "sent": "So the larger the sample size you used to approximate the cutting plane algorithm, the cutting plane at each step, the larger speed up your obtain.",
                    "label": 0
                },
                {
                    "sent": "So you can arrive up to speed up up to 20.",
                    "label": 0
                },
                {
                    "sent": "So this is for training.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also for classification and here you see that as Duck Plus does much better job becausw it compacts all cutting plane models in your model into one single DAG.",
                    "label": 0
                },
                {
                    "sent": "So you obtain an equivalent and more compact model representation an.",
                    "label": 0
                },
                {
                    "sent": "By doing so, you get even bigger speedups.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next small point is parallelization.",
                    "label": 0
                },
                {
                    "sent": "We observe that 95% of time of computational time is spent computing cutting plane models at each iteration and the cutting plane algorithm allows a straightforward way to paralyze.",
                    "label": 1
                },
                {
                    "sent": "At such that you can bring down the complexity from being squared in the sample size up to squared divided by the number of CPUs you have at hand.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this graph shows you how.",
                    "label": 0
                },
                {
                    "sent": "It scales with respect to the number of CPUs, so by using eight CPUs you can speed up.",
                    "label": 1
                },
                {
                    "sent": "You can get a speedup up to 7.",
                    "label": 0
                },
                {
                    "sent": "And the larger your sample size is, the better speedup you get.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last point I want to talk about is how to handle glass imbalance problem.",
                    "label": 0
                },
                {
                    "sent": "So optimization problem you're actually solving by applying cutting plane algorithm is slightly different from conventional optimization problem that SVM light is solving so it's difficult to include penalties for examples from different classes.",
                    "label": 0
                },
                {
                    "sent": "So this idea of out waiting positive and negative examples is difficult to apply in this setting.",
                    "label": 1
                },
                {
                    "sent": "So however, sampling to build approximate cutting planes at each iteration suggests a very straightforward solution using importance sampling such that in each sample you have a very nice control in which proportion examples from different classes enter your sample an in the paper we actually show that this alternative sampling.",
                    "label": 1
                },
                {
                    "sent": "Strategy.",
                    "label": 1
                },
                {
                    "sent": "It preserves theoretical convergence bounds.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our experiments on track and Yahoo answers data set showed that it actually outperforms the original algorithm when tuning is needed.",
                    "label": 0
                },
                {
                    "sent": "It's still as fast as the original algorithm an it actually gives you a more flexible control over precision and recall than SVM light.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, you have seen two learning algorithms, namely as Doug NASDAQ Plus.",
                    "label": 1
                },
                {
                    "sent": "Of that compact cutting plane model at each iteration, by using directed acyclic graphs and this gives much faster training and classification times.",
                    "label": 1
                },
                {
                    "sent": "Another thing is that is very important when you want to train SVM's with kernels.",
                    "label": 0
                },
                {
                    "sent": "Is parallelization becausw SVM light or as family?",
                    "label": 0
                },
                {
                    "sent": "They are not parallel.",
                    "label": 1
                },
                {
                    "sent": "Also, we shown alternative sampling strategy to Bender to better handle class imbalance data at large scale.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And there is software available, so if you're interested you can download it and it will be available soon after the conference.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you so the future work we plan to extend this DAG approach to more general kernels, for example, partial tree kernel.",
                    "label": 1
                },
                {
                    "sent": "We also would like to explore other structural kernels on graphs and we would like to also to experiment with other tax tasks, for example from NLP, such as a relation extraction and coreference resolution.",
                    "label": 0
                },
                {
                    "sent": "An also an important observation is that here I've shown the currently in algorithm but actually this compact model representation using drugs is not confined only to the cutting plane algorithm.",
                    "label": 0
                },
                {
                    "sent": "So as long as your algorithm.",
                    "label": 0
                },
                {
                    "sent": "Build some model that is a linear combination of training examples.",
                    "label": 0
                },
                {
                    "sent": "You can apply this idea also.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thank you an if you have questions welcome.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the talk.",
                    "label": 1
                },
                {
                    "sent": "The question is actually for each paper unstructured kernels.",
                    "label": 0
                },
                {
                    "sent": "We have the same remark.",
                    "label": 0
                },
                {
                    "sent": "There is a different domain which comes from graphical models.",
                    "label": 0
                },
                {
                    "sent": "So in the case of of sequences definitely you could test easiest model like CRF.",
                    "label": 0
                },
                {
                    "sent": "Where the inference is exact and definitely would get better results.",
                    "label": 0
                },
                {
                    "sent": "So question is very simple.",
                    "label": 0
                },
                {
                    "sent": "Have you ever tried to compare your methods with any of existing packages in graphical models?",
                    "label": 0
                },
                {
                    "sent": "Like CRF, conditional random fields.",
                    "label": 0
                },
                {
                    "sent": "OK, so the distinction I want to clarify is that by structure or kernels we want to capture structure in the input, so the output is still very simple.",
                    "label": 0
                },
                {
                    "sent": "It's we're dealing with binary classification.",
                    "label": 0
                },
                {
                    "sent": "You trade structure in the label in the input, so there is no problem of inference here.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Can you clarify a little bit your last point about the imbalance?",
                    "label": 0
                },
                {
                    "sent": "So what you do there is kind of under sampling of the majority class.",
                    "label": 0
                },
                {
                    "sent": "Yes, but since we draw samples in the iterative fashion.",
                    "label": 0
                },
                {
                    "sent": "Such that like for example, typical conventional sampling methods, you've given the training set you either under sample over sample, and if you're under sample, you kind of lose the important information present in the examples that didn't end up in your sample, But the nice property of the cutting plane algorithm is that.",
                    "label": 0
                },
                {
                    "sent": "At each iteration you draw a sample from your training set.",
                    "label": 0
                },
                {
                    "sent": "And by others, by oversampling the minority class you have this flexibility to build actually a sample where examples from different classes enter in the desired proportion.",
                    "label": 0
                },
                {
                    "sent": "So you get a balanced proportion between positive and negative examples.",
                    "label": 0
                },
                {
                    "sent": "But since you are doing this in iterative fashion.",
                    "label": 0
                },
                {
                    "sent": "Actually this gives you.",
                    "label": 0
                },
                {
                    "sent": "The nice property of.",
                    "label": 0
                },
                {
                    "sent": "Maintaining the overall look on the entire training set.",
                    "label": 0
                },
                {
                    "sent": "So basically it's kind of.",
                    "label": 0
                },
                {
                    "sent": "Sampling method, but it's directly built into the optimization process.",
                    "label": 0
                },
                {
                    "sent": "So it's a distinctive property of this weight class imbalance problem.",
                    "label": 0
                },
                {
                    "sent": "Alright, well let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}