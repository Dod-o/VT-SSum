{
    "id": "i36sdllz3lu3ibkhodfui2qf2plsvlyf",
    "title": "Uncorrelated Multilinear Principal Component Analysis through Successive Variance Maximization",
    "info": {
        "author": [
            "Haiping Lu, Department of Computer Science, University of Toronto"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Principal Component Analysis"
        ]
    },
    "url": "http://videolectures.net/icml08_lu_ump/",
    "segmentation": [
        [
            "My name is Hyatt in Lou and from Union University of Toronto and the paper I'm going to present is uncorrelated, multilinear principal component analysis through successes variance maximization."
        ],
        [
            "So here is an outline of my talk.",
            "A star is the motivations of this work and then I propose the uncorrelated multilinear PCA algorithm.",
            "And next, the experimental evaluations will be presented.",
            "And finally I will draw the conclusions."
        ],
        [
            "So here is the data that we consider in this work.",
            "So it is tensorial data feeds tensor have various definitions in various fields that answer in this work we refer to multidimensional arrays and into the generalization of vector and matrix concept.",
            "So if there are any modes then we call it an NTH order tensor.",
            "So vector Eva 1st order tensor and matrix.",
            "Either 2nd order tensor.",
            "And tensorial data?",
            "It has a wide range of applications such as application involving images, video sequences and streaming data or data mining data.",
            "And here I show the two examples of tensors.",
            "The first one is a second order tensor.",
            "It has two modes.",
            "The column spatial columns and spatial rumors.",
            "So this is a typical grade level 6 image and the second one.",
            "It's the 3rd order tensor, which is a binary silhouette sequence is a gauge sequence, and has three modes.",
            "The special colour spatial role and the time modes, right?",
            "So this is a throughout."
        ],
        [
            "Tensor.",
            "So the problem we're going to solve is dimensionality reduction.",
            "So why do we want to reduce the dimensionality the tensor objects?",
            "They are usually very high dimensional.",
            "This leads to the problem of curse of dimensionality.",
            "So why this is a curse?",
            "'cause it's so this kind of hard data is computationally very expensive to handle.",
            "An many classifiers be performed very poorly in high dimensional space, giving a small number of training samples, which is often true in practice."
        ],
        [
            "But in fact, if we look at a class of tensor objects, they are mostly highly constrained to a subspace.",
            "A manifold of intrinsically low dimension, like the face, the gate.",
            "We are also very constrained, right?",
            "And so dimensionality the popular methods.",
            "And they thought also some kind of feature extraction.",
            "Dimensionality will transform these high dimensional data into a low dimensional space.",
            "While you can do transformation with the objective, trying to retain most of the underlying structure."
        ],
        [
            "And the focus of this work is unsupervised dimensionality reduction, and in particular PC based algorithms.",
            "So PC is a well known linear method, either produce.",
            "Uncorrelated features it retain as much as possible the variations in the original data.",
            "But there's one.",
            "One thing we should be aware that PC is a linear method, so we need to reshape this tensor no matter is 2 D 3D or even higher.",
            "They need to receive them into vectors.",
            "So this will result in very high dimensionality and lead to high computational and memory demand.",
            "And another thing is that this kind of receiving the usually briefly natural structure in the original data."
        ],
        [
            "So this kind of observation leads to the development of multilinear algorithms.",
            "They extract features directly from the tensor objects from the tensor representation.",
            "So here are list of the existing works, the tensor rank one.",
            "The competition is probably in 2001 and the two dimensional PCA is in 2004 and the generalized low rank approximation of metrics is, which is.",
            "I think it's best the student of wallpaper in XML 2005 and also the generalized PCA and the other one is a code concurrent subspace analysis is proposed in 2005 and another one.",
            "Is multilinear kiss a property in 2008 with Luismi?"
        ],
        [
            "So.",
            "There's a difference between this."
        ],
        [
            "So.",
            "Existing mounting method as a linear methods.",
            "That one property that this kind of multilinear matter they do not share with the original particle PC derivation that is the original PCD derives uncorrelated features.",
            "But none of these methods they produce."
        ],
        [
            "Read the features.",
            "So why I would prefer uncorrelated features?",
            "This is cause on correlated features.",
            "The resulting minimum redundancy.",
            "We ensure linear independence among features and we simplify the classification task.",
            "Follow the feature extraction process.",
            "So the question we would like to ask is can we extract uncorrelated features?",
            "Directly from tensor objects and in an unsupervised way, and the answer is yes."
        ],
        [
            "Otherwise, my people will not get accepted right?",
            "So I will propose the Unpc algorithm now.",
            "So before going to the algorithm we need to, I need to introduce the projection first."
        ],
        [
            "Answer To vector projection and before we introduce the projection, I think it's better to introduce the notations so that it's easier to follow because it's multilinear algebra, so we have some kind of conventions and further vector we use lowercase poses and four matches.",
            "We use uppercase boldface an for 10s of years, calligraphic to later, and I use superscript key to use as a feature index and.",
            "Subscript subscript N is the training center in tests and superscript N is the mode, so it's 50 is the transpose and this kind of operation is N mode multiplication."
        ],
        [
            "So here is the basic problem we need to consider.",
            "So how can we?",
            "Project the tensor without vectorization into a scalar.",
            "This can be achieved through what I called elementary multi linear projection with elementary projection.",
            "How to project you use this third order tensor example project this tensor into a scalar.",
            "We need 3 vectors, so we use the first vector to project the first mode right product.",
            "Each of these so called mode vector and we get a metrics.",
            "So the order from three to two and then we use the 2nd.",
            "Vector predicted matches.",
            "Into a vector, we get a first order tensor, right?",
            "And we use the third projection author projection vector to further protect this vector into scalar to achieve."
        ],
        [
            "So two scalar projection and how about tensor to vector projection projection?",
            "So imagine we have.",
            "A number of amps we have P of them.",
            "Then we get the scalar.",
            "So this piece killer the former vector.",
            "So we have 10 through 12."
        ],
        [
            "Vector projection."
        ],
        [
            "So here is the unpc problem formulation.",
            "The input is tensorial training samples, and we also need to specify the desired dimensionality."
        ],
        [
            "And the objective is same as PC where you want to what we want to maximize either total scatter.",
            "It is a major part in measure of the variance and it is defined based on scalar in the projective space is a projective sample, right?",
            "This is a scalar.",
            "It's defined."
        ],
        [
            "Based on scalars.",
            "And we also need to enforce the constraint.",
            "The first constraint is to make sure that this kind of scatter will not be sensitive to scaling and the certain constraint is the title.",
            "In my paper 'cause we want to enforce the constraint of zero correlation.",
            "So we want it to be uncorrelated.",
            "It is defined."
        ],
        [
            "Please down the.",
            "Calling it vector, which is a projection from the piece.",
            "Yeah."
        ],
        [
            "P. So the output of this algorithm is a tensor to vector projections that satisfy this objective.",
            "So this is this is."
        ],
        [
            "Problem formulation so how to solve this problem?",
            "The approach I have take is success is maximization.",
            "We saw first the first EMP without any constraint and trying to maximize the projection.",
            "The scatter of the first projection and then we continue to determine the second EMP.",
            "By Miss Maggie the schedule of the second projection, but subject to the constraint, the second set of features will be uncorrelated with the first set, and then we continue to the third one, maximizing the scatter and something the constraint that the third one will be uncorrelated.",
            "First one and."
        ],
        [
            "The second one.",
            "And how to solve each individual in case we take the approach of alternating projection?",
            "So this is a iterative solution.",
            "So why do we need the iterative solution?",
            "This is cause for each NP.",
            "If you remember we have North set of projects in vector to solve so, but to simultaneously determine this any set of parameters which currently infeasible.",
            "So it is, uh, the approach of alternating projection.",
            "So we solve one set of parameters with all the other parameters fixed and then iterate.",
            "This is originated from the alternating least square.",
            "Method proposing in the."
        ],
        [
            "90s.",
            "So here is how it works.",
            "We assume that except mode and star or the project vectors are given.",
            "And we project the training centers into this N -- 1 one mode, so we get.",
            "Vectors right?",
            "We get N vectors.",
            "Then we determine a projection vector that we can project this.",
            "These vectors underline that will maximize the variance and subject to the constraint of zero correlation.",
            "So this problem is.",
            "Either classical pspa with the input of the vectors Y and the the corresponding respective scatter matches is also defined based on this."
        ],
        [
            "Vectors of Y.",
            "And so to solve this problem for the case of equal to 1, eight is easy, right?",
            "There's no constraint, so we can just maximize the objective function.",
            "So the first projection projection projection is obtained by setting the projector equal to the unit.",
            "Adulator of ST1 theater right and associated with the largest eigenvalue."
        ],
        [
            "How about for P greater than one?",
            "We formally liking PC right?",
            "We form a data metrics by stacking this an vectors and."
        ],
        [
            "We reformulate the original problem, so this maximization problem is formulated in terms of the scatter metrics defined based on Y and the."
        ],
        [
            "Constraint is also revised.",
            "Coordinate this one keep kept unchanged and this the constraint is revised with according to the data matrix Y an now."
        ],
        [
            "To solve this problem we propose we propose the theorem in the paper.",
            "This theorem states that the solution to this unpc problem that typically for P greater than one is the unit length eigenvector corresponding to the largest eigenvalue of this.",
            "Argumenty problem.",
            "So here this symbol.",
            "Fee is a fine."
        ],
        [
            "Rent.",
            "As follows, the definition is basically based on the data metrics and the coordinate metrics.",
            "G consists of the coordinate vector.",
            "I'm not going to give the details of it."
        ],
        [
            "Vision and you can read the paper.",
            "I'll come to the poster just outside of these thought so.",
            "So by now I have proposed the Unpc algorithm, so it's time to evaluate the property the propose."
        ],
        [
            "Angry them.",
            "So this is the experimental setup.",
            "We choose a fair face recognition problem.",
            "The unsupervised face recognition problem.",
            "The database we have chooses.",
            "The Ferret database and we choose a subset so that the maximum post variation is 15 degrees.",
            "And the minimum number of 50 images per subject is 8."
        ],
        [
            "So this results in 700 and 2150 images from 70 subjects and we are focusing on the feature extraction so we do pre processing by menu cropping and alignment and it is all the images are normalized to 80 by 80 pixels with two 156th grade grade levels per pixel."
        ],
        [
            "And for the classification we focus on features and so we use simple classification method, nearest neighbor classifier, Euclidean distance measure and for the performance evaluation of classification we use the report.",
            "The rank one identification rich."
        ],
        [
            "So."
        ],
        [
            "Here are the results.",
            "First, for LT1 L here you know the number of training samples perceptive, so L you could.",
            "1 means that only one thing example per subject.",
            "This so this is a extremely small sample size scenario, so it's very difficult and.",
            "The actually mentioned that the supervised method cannot solve the conventional supervised method cannot solve this problem because with only one sample you cannot estimate the weaving class gather.",
            "And here that we reduced the first one is for dimensionality from one to 10.",
            "The right figure is from 15 to up to 80.",
            "And the proposed method is marked by diamond, so expensive diamonds and from the result we have.",
            "Seeing that this proposed Unpc 8 outperforms the other three methods.",
            "PC, NPC and TR."
        ],
        [
            "He and we further examine the results for L equal to 7, and here are the results similarly shown.",
            "And we have the similar of the vision.",
            "The Unpc outperform the other three method as well, and for the other results from two to six is outside of this also."
        ],
        [
            "You can come to the poster.",
            "And since we this method maximized variation."
        ],
        [
            "And actually, there's an observation from the previous results.",
            "We observed that the unpc it saturates.",
            "Earlier than the other method, the performance."
        ],
        [
            "Fat rich earlier around 2030, etc.",
            "It refers so."
        ],
        [
            "This motivated us to examine the variance captured by each method.",
            "So here are the results.",
            "We plot the variation captured, measured by the scatter in log scales, data log scales, and these 4X1X2.",
            "And again, the proper method is marked by diamonds, and we can also from this.",
            "Variations approach there.",
            "The version captured by Unpc is much lower than the other method, right?",
            "And this is due to the very constrained solution.",
            "So 'cause we have constant zero correlation and we also have the country of it must be a tensor to vector direct projection.",
            "And this too low variation is explained.",
            "The saturation, because when it's too low, the version 8, your limited contribution."
        ],
        [
            "Either recognition so we clean these, uncorrelated with mathematical proof.",
            "Here are some simulation studies.",
            "We applaud the correlation.",
            "Average correlations between pairwise features and here are the results.",
            "Obvious PC and unpc the option on quality features.",
            "The other two methods.",
            "Their features are correlated."
        ],
        [
            "So here, then a summary of my talk.",
            "So I propose a unpc algorithm.",
            "The threat on correlated features directly from tensor objects through a tensor vector projection.",
            "And the solution to this unpc problem is through successes.",
            "Various mathematician and alternating projection method and further evaluation.",
            "We have observed that the Unpc outperformed PC and PC and PROD in unsupervised face recognition task and especially is very effective in lower dimension."
        ],
        [
            "So I would like to conclude with the future works.",
            "So this is a new algorithm an would like to see its application to other unsupervised learning tasks such as clustering.",
            "And we would also like to investigate the design issues such as initialization, protection order, antenna termination, which I did not mention in the talk.",
            "And finally it's interesting to study the combination of unpc features with PC and PCR TRD features.",
            "Thank you just a little bit.",
            "Yes, traditionally the people actually extended the multilinear extensions of the single ability.",
            "Composition is a parasect.",
            "Yeah, there's a fact right?",
            "Yeah yeah, this kind of thing they are, they are.",
            "They are in the 70s.",
            "We have extensively studied is a factor in the factorization settings, but there are some difference between subsidy learning and factorization.",
            "I would like to summarize there are streaming difference.",
            "The first one is that.",
            "VR works by.",
            "There are previous work, like tensor phase.",
            "It's very popular paper, very well cited.",
            "It's also multilinear setting, but it will treat the tensors no matter if the image.",
            "Oh wait, no matter the dimension, they will still vectorize it, but they will form the tensor through various modality.",
            "Or like the factors involved in the formation in the formation of the data, right?",
            "So it's a it's dealing with different problem and this kind of factorization.",
            "We have.",
            "Another key difference is that they require a lot of data.",
            "For training we have to find the data to form these kind of.",
            "If we have 35 illumination, three posts and 10 people, we have to find so many samples for each subject right?",
            "So they require lots of samples and correspondingly there.",
            "American is very huge.",
            "The so the processing cost is higher than we can kind of subsidy learning and we think it also have limitation in is good in modeling and understanding the problem, but it may not be well practical nursing practice.",
            "Usually you have only a few samples and you do not have luxury to form this kind of.",
            "Sensors with various factors from each person."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Hyatt in Lou and from Union University of Toronto and the paper I'm going to present is uncorrelated, multilinear principal component analysis through successes variance maximization.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is an outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "A star is the motivations of this work and then I propose the uncorrelated multilinear PCA algorithm.",
                    "label": 1
                },
                {
                    "sent": "And next, the experimental evaluations will be presented.",
                    "label": 0
                },
                {
                    "sent": "And finally I will draw the conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is the data that we consider in this work.",
                    "label": 0
                },
                {
                    "sent": "So it is tensorial data feeds tensor have various definitions in various fields that answer in this work we refer to multidimensional arrays and into the generalization of vector and matrix concept.",
                    "label": 1
                },
                {
                    "sent": "So if there are any modes then we call it an NTH order tensor.",
                    "label": 0
                },
                {
                    "sent": "So vector Eva 1st order tensor and matrix.",
                    "label": 0
                },
                {
                    "sent": "Either 2nd order tensor.",
                    "label": 0
                },
                {
                    "sent": "And tensorial data?",
                    "label": 0
                },
                {
                    "sent": "It has a wide range of applications such as application involving images, video sequences and streaming data or data mining data.",
                    "label": 1
                },
                {
                    "sent": "And here I show the two examples of tensors.",
                    "label": 0
                },
                {
                    "sent": "The first one is a second order tensor.",
                    "label": 0
                },
                {
                    "sent": "It has two modes.",
                    "label": 0
                },
                {
                    "sent": "The column spatial columns and spatial rumors.",
                    "label": 0
                },
                {
                    "sent": "So this is a typical grade level 6 image and the second one.",
                    "label": 0
                },
                {
                    "sent": "It's the 3rd order tensor, which is a binary silhouette sequence is a gauge sequence, and has three modes.",
                    "label": 0
                },
                {
                    "sent": "The special colour spatial role and the time modes, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a throughout.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tensor.",
                    "label": 0
                },
                {
                    "sent": "So the problem we're going to solve is dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "So why do we want to reduce the dimensionality the tensor objects?",
                    "label": 0
                },
                {
                    "sent": "They are usually very high dimensional.",
                    "label": 1
                },
                {
                    "sent": "This leads to the problem of curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "So why this is a curse?",
                    "label": 1
                },
                {
                    "sent": "'cause it's so this kind of hard data is computationally very expensive to handle.",
                    "label": 0
                },
                {
                    "sent": "An many classifiers be performed very poorly in high dimensional space, giving a small number of training samples, which is often true in practice.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in fact, if we look at a class of tensor objects, they are mostly highly constrained to a subspace.",
                    "label": 1
                },
                {
                    "sent": "A manifold of intrinsically low dimension, like the face, the gate.",
                    "label": 0
                },
                {
                    "sent": "We are also very constrained, right?",
                    "label": 0
                },
                {
                    "sent": "And so dimensionality the popular methods.",
                    "label": 0
                },
                {
                    "sent": "And they thought also some kind of feature extraction.",
                    "label": 0
                },
                {
                    "sent": "Dimensionality will transform these high dimensional data into a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "While you can do transformation with the objective, trying to retain most of the underlying structure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the focus of this work is unsupervised dimensionality reduction, and in particular PC based algorithms.",
                    "label": 0
                },
                {
                    "sent": "So PC is a well known linear method, either produce.",
                    "label": 1
                },
                {
                    "sent": "Uncorrelated features it retain as much as possible the variations in the original data.",
                    "label": 1
                },
                {
                    "sent": "But there's one.",
                    "label": 0
                },
                {
                    "sent": "One thing we should be aware that PC is a linear method, so we need to reshape this tensor no matter is 2 D 3D or even higher.",
                    "label": 1
                },
                {
                    "sent": "They need to receive them into vectors.",
                    "label": 1
                },
                {
                    "sent": "So this will result in very high dimensionality and lead to high computational and memory demand.",
                    "label": 0
                },
                {
                    "sent": "And another thing is that this kind of receiving the usually briefly natural structure in the original data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this kind of observation leads to the development of multilinear algorithms.",
                    "label": 0
                },
                {
                    "sent": "They extract features directly from the tensor objects from the tensor representation.",
                    "label": 1
                },
                {
                    "sent": "So here are list of the existing works, the tensor rank one.",
                    "label": 0
                },
                {
                    "sent": "The competition is probably in 2001 and the two dimensional PCA is in 2004 and the generalized low rank approximation of metrics is, which is.",
                    "label": 1
                },
                {
                    "sent": "I think it's best the student of wallpaper in XML 2005 and also the generalized PCA and the other one is a code concurrent subspace analysis is proposed in 2005 and another one.",
                    "label": 0
                },
                {
                    "sent": "Is multilinear kiss a property in 2008 with Luismi?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's a difference between this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Existing mounting method as a linear methods.",
                    "label": 0
                },
                {
                    "sent": "That one property that this kind of multilinear matter they do not share with the original particle PC derivation that is the original PCD derives uncorrelated features.",
                    "label": 0
                },
                {
                    "sent": "But none of these methods they produce.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Read the features.",
                    "label": 0
                },
                {
                    "sent": "So why I would prefer uncorrelated features?",
                    "label": 0
                },
                {
                    "sent": "This is cause on correlated features.",
                    "label": 0
                },
                {
                    "sent": "The resulting minimum redundancy.",
                    "label": 0
                },
                {
                    "sent": "We ensure linear independence among features and we simplify the classification task.",
                    "label": 1
                },
                {
                    "sent": "Follow the feature extraction process.",
                    "label": 1
                },
                {
                    "sent": "So the question we would like to ask is can we extract uncorrelated features?",
                    "label": 0
                },
                {
                    "sent": "Directly from tensor objects and in an unsupervised way, and the answer is yes.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Otherwise, my people will not get accepted right?",
                    "label": 0
                },
                {
                    "sent": "So I will propose the Unpc algorithm now.",
                    "label": 0
                },
                {
                    "sent": "So before going to the algorithm we need to, I need to introduce the projection first.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Answer To vector projection and before we introduce the projection, I think it's better to introduce the notations so that it's easier to follow because it's multilinear algebra, so we have some kind of conventions and further vector we use lowercase poses and four matches.",
                    "label": 0
                },
                {
                    "sent": "We use uppercase boldface an for 10s of years, calligraphic to later, and I use superscript key to use as a feature index and.",
                    "label": 1
                },
                {
                    "sent": "Subscript subscript N is the training center in tests and superscript N is the mode, so it's 50 is the transpose and this kind of operation is N mode multiplication.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the basic problem we need to consider.",
                    "label": 0
                },
                {
                    "sent": "So how can we?",
                    "label": 0
                },
                {
                    "sent": "Project the tensor without vectorization into a scalar.",
                    "label": 0
                },
                {
                    "sent": "This can be achieved through what I called elementary multi linear projection with elementary projection.",
                    "label": 0
                },
                {
                    "sent": "How to project you use this third order tensor example project this tensor into a scalar.",
                    "label": 0
                },
                {
                    "sent": "We need 3 vectors, so we use the first vector to project the first mode right product.",
                    "label": 0
                },
                {
                    "sent": "Each of these so called mode vector and we get a metrics.",
                    "label": 0
                },
                {
                    "sent": "So the order from three to two and then we use the 2nd.",
                    "label": 0
                },
                {
                    "sent": "Vector predicted matches.",
                    "label": 0
                },
                {
                    "sent": "Into a vector, we get a first order tensor, right?",
                    "label": 0
                },
                {
                    "sent": "And we use the third projection author projection vector to further protect this vector into scalar to achieve.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So two scalar projection and how about tensor to vector projection projection?",
                    "label": 0
                },
                {
                    "sent": "So imagine we have.",
                    "label": 0
                },
                {
                    "sent": "A number of amps we have P of them.",
                    "label": 0
                },
                {
                    "sent": "Then we get the scalar.",
                    "label": 0
                },
                {
                    "sent": "So this piece killer the former vector.",
                    "label": 0
                },
                {
                    "sent": "So we have 10 through 12.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vector projection.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the unpc problem formulation.",
                    "label": 0
                },
                {
                    "sent": "The input is tensorial training samples, and we also need to specify the desired dimensionality.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the objective is same as PC where you want to what we want to maximize either total scatter.",
                    "label": 0
                },
                {
                    "sent": "It is a major part in measure of the variance and it is defined based on scalar in the projective space is a projective sample, right?",
                    "label": 0
                },
                {
                    "sent": "This is a scalar.",
                    "label": 0
                },
                {
                    "sent": "It's defined.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on scalars.",
                    "label": 0
                },
                {
                    "sent": "And we also need to enforce the constraint.",
                    "label": 0
                },
                {
                    "sent": "The first constraint is to make sure that this kind of scatter will not be sensitive to scaling and the certain constraint is the title.",
                    "label": 0
                },
                {
                    "sent": "In my paper 'cause we want to enforce the constraint of zero correlation.",
                    "label": 0
                },
                {
                    "sent": "So we want it to be uncorrelated.",
                    "label": 0
                },
                {
                    "sent": "It is defined.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please down the.",
                    "label": 0
                },
                {
                    "sent": "Calling it vector, which is a projection from the piece.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "P. So the output of this algorithm is a tensor to vector projections that satisfy this objective.",
                    "label": 0
                },
                {
                    "sent": "So this is this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem formulation so how to solve this problem?",
                    "label": 0
                },
                {
                    "sent": "The approach I have take is success is maximization.",
                    "label": 0
                },
                {
                    "sent": "We saw first the first EMP without any constraint and trying to maximize the projection.",
                    "label": 1
                },
                {
                    "sent": "The scatter of the first projection and then we continue to determine the second EMP.",
                    "label": 0
                },
                {
                    "sent": "By Miss Maggie the schedule of the second projection, but subject to the constraint, the second set of features will be uncorrelated with the first set, and then we continue to the third one, maximizing the scatter and something the constraint that the third one will be uncorrelated.",
                    "label": 1
                },
                {
                    "sent": "First one and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second one.",
                    "label": 0
                },
                {
                    "sent": "And how to solve each individual in case we take the approach of alternating projection?",
                    "label": 0
                },
                {
                    "sent": "So this is a iterative solution.",
                    "label": 1
                },
                {
                    "sent": "So why do we need the iterative solution?",
                    "label": 0
                },
                {
                    "sent": "This is cause for each NP.",
                    "label": 0
                },
                {
                    "sent": "If you remember we have North set of projects in vector to solve so, but to simultaneously determine this any set of parameters which currently infeasible.",
                    "label": 1
                },
                {
                    "sent": "So it is, uh, the approach of alternating projection.",
                    "label": 0
                },
                {
                    "sent": "So we solve one set of parameters with all the other parameters fixed and then iterate.",
                    "label": 1
                },
                {
                    "sent": "This is originated from the alternating least square.",
                    "label": 0
                },
                {
                    "sent": "Method proposing in the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "90s.",
                    "label": 0
                },
                {
                    "sent": "So here is how it works.",
                    "label": 0
                },
                {
                    "sent": "We assume that except mode and star or the project vectors are given.",
                    "label": 0
                },
                {
                    "sent": "And we project the training centers into this N -- 1 one mode, so we get.",
                    "label": 0
                },
                {
                    "sent": "Vectors right?",
                    "label": 0
                },
                {
                    "sent": "We get N vectors.",
                    "label": 0
                },
                {
                    "sent": "Then we determine a projection vector that we can project this.",
                    "label": 0
                },
                {
                    "sent": "These vectors underline that will maximize the variance and subject to the constraint of zero correlation.",
                    "label": 0
                },
                {
                    "sent": "So this problem is.",
                    "label": 0
                },
                {
                    "sent": "Either classical pspa with the input of the vectors Y and the the corresponding respective scatter matches is also defined based on this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vectors of Y.",
                    "label": 0
                },
                {
                    "sent": "And so to solve this problem for the case of equal to 1, eight is easy, right?",
                    "label": 0
                },
                {
                    "sent": "There's no constraint, so we can just maximize the objective function.",
                    "label": 0
                },
                {
                    "sent": "So the first projection projection projection is obtained by setting the projector equal to the unit.",
                    "label": 1
                },
                {
                    "sent": "Adulator of ST1 theater right and associated with the largest eigenvalue.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How about for P greater than one?",
                    "label": 0
                },
                {
                    "sent": "We formally liking PC right?",
                    "label": 0
                },
                {
                    "sent": "We form a data metrics by stacking this an vectors and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We reformulate the original problem, so this maximization problem is formulated in terms of the scatter metrics defined based on Y and the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Constraint is also revised.",
                    "label": 0
                },
                {
                    "sent": "Coordinate this one keep kept unchanged and this the constraint is revised with according to the data matrix Y an now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve this problem we propose we propose the theorem in the paper.",
                    "label": 0
                },
                {
                    "sent": "This theorem states that the solution to this unpc problem that typically for P greater than one is the unit length eigenvector corresponding to the largest eigenvalue of this.",
                    "label": 1
                },
                {
                    "sent": "Argumenty problem.",
                    "label": 0
                },
                {
                    "sent": "So here this symbol.",
                    "label": 0
                },
                {
                    "sent": "Fee is a fine.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rent.",
                    "label": 0
                },
                {
                    "sent": "As follows, the definition is basically based on the data metrics and the coordinate metrics.",
                    "label": 0
                },
                {
                    "sent": "G consists of the coordinate vector.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to give the details of it.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vision and you can read the paper.",
                    "label": 0
                },
                {
                    "sent": "I'll come to the poster just outside of these thought so.",
                    "label": 0
                },
                {
                    "sent": "So by now I have proposed the Unpc algorithm, so it's time to evaluate the property the propose.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Angry them.",
                    "label": 0
                },
                {
                    "sent": "So this is the experimental setup.",
                    "label": 1
                },
                {
                    "sent": "We choose a fair face recognition problem.",
                    "label": 0
                },
                {
                    "sent": "The unsupervised face recognition problem.",
                    "label": 0
                },
                {
                    "sent": "The database we have chooses.",
                    "label": 1
                },
                {
                    "sent": "The Ferret database and we choose a subset so that the maximum post variation is 15 degrees.",
                    "label": 0
                },
                {
                    "sent": "And the minimum number of 50 images per subject is 8.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this results in 700 and 2150 images from 70 subjects and we are focusing on the feature extraction so we do pre processing by menu cropping and alignment and it is all the images are normalized to 80 by 80 pixels with two 156th grade grade levels per pixel.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the classification we focus on features and so we use simple classification method, nearest neighbor classifier, Euclidean distance measure and for the performance evaluation of classification we use the report.",
                    "label": 0
                },
                {
                    "sent": "The rank one identification rich.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "First, for LT1 L here you know the number of training samples perceptive, so L you could.",
                    "label": 1
                },
                {
                    "sent": "1 means that only one thing example per subject.",
                    "label": 0
                },
                {
                    "sent": "This so this is a extremely small sample size scenario, so it's very difficult and.",
                    "label": 1
                },
                {
                    "sent": "The actually mentioned that the supervised method cannot solve the conventional supervised method cannot solve this problem because with only one sample you cannot estimate the weaving class gather.",
                    "label": 0
                },
                {
                    "sent": "And here that we reduced the first one is for dimensionality from one to 10.",
                    "label": 0
                },
                {
                    "sent": "The right figure is from 15 to up to 80.",
                    "label": 1
                },
                {
                    "sent": "And the proposed method is marked by diamond, so expensive diamonds and from the result we have.",
                    "label": 0
                },
                {
                    "sent": "Seeing that this proposed Unpc 8 outperforms the other three methods.",
                    "label": 0
                },
                {
                    "sent": "PC, NPC and TR.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He and we further examine the results for L equal to 7, and here are the results similarly shown.",
                    "label": 1
                },
                {
                    "sent": "And we have the similar of the vision.",
                    "label": 0
                },
                {
                    "sent": "The Unpc outperform the other three method as well, and for the other results from two to six is outside of this also.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can come to the poster.",
                    "label": 0
                },
                {
                    "sent": "And since we this method maximized variation.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually, there's an observation from the previous results.",
                    "label": 0
                },
                {
                    "sent": "We observed that the unpc it saturates.",
                    "label": 0
                },
                {
                    "sent": "Earlier than the other method, the performance.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fat rich earlier around 2030, etc.",
                    "label": 0
                },
                {
                    "sent": "It refers so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This motivated us to examine the variance captured by each method.",
                    "label": 0
                },
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "We plot the variation captured, measured by the scatter in log scales, data log scales, and these 4X1X2.",
                    "label": 0
                },
                {
                    "sent": "And again, the proper method is marked by diamonds, and we can also from this.",
                    "label": 0
                },
                {
                    "sent": "Variations approach there.",
                    "label": 0
                },
                {
                    "sent": "The version captured by Unpc is much lower than the other method, right?",
                    "label": 1
                },
                {
                    "sent": "And this is due to the very constrained solution.",
                    "label": 0
                },
                {
                    "sent": "So 'cause we have constant zero correlation and we also have the country of it must be a tensor to vector direct projection.",
                    "label": 1
                },
                {
                    "sent": "And this too low variation is explained.",
                    "label": 0
                },
                {
                    "sent": "The saturation, because when it's too low, the version 8, your limited contribution.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Either recognition so we clean these, uncorrelated with mathematical proof.",
                    "label": 0
                },
                {
                    "sent": "Here are some simulation studies.",
                    "label": 0
                },
                {
                    "sent": "We applaud the correlation.",
                    "label": 0
                },
                {
                    "sent": "Average correlations between pairwise features and here are the results.",
                    "label": 0
                },
                {
                    "sent": "Obvious PC and unpc the option on quality features.",
                    "label": 0
                },
                {
                    "sent": "The other two methods.",
                    "label": 0
                },
                {
                    "sent": "Their features are correlated.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, then a summary of my talk.",
                    "label": 0
                },
                {
                    "sent": "So I propose a unpc algorithm.",
                    "label": 0
                },
                {
                    "sent": "The threat on correlated features directly from tensor objects through a tensor vector projection.",
                    "label": 1
                },
                {
                    "sent": "And the solution to this unpc problem is through successes.",
                    "label": 1
                },
                {
                    "sent": "Various mathematician and alternating projection method and further evaluation.",
                    "label": 0
                },
                {
                    "sent": "We have observed that the Unpc outperformed PC and PC and PROD in unsupervised face recognition task and especially is very effective in lower dimension.",
                    "label": 1
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I would like to conclude with the future works.",
                    "label": 1
                },
                {
                    "sent": "So this is a new algorithm an would like to see its application to other unsupervised learning tasks such as clustering.",
                    "label": 1
                },
                {
                    "sent": "And we would also like to investigate the design issues such as initialization, protection order, antenna termination, which I did not mention in the talk.",
                    "label": 1
                },
                {
                    "sent": "And finally it's interesting to study the combination of unpc features with PC and PCR TRD features.",
                    "label": 0
                },
                {
                    "sent": "Thank you just a little bit.",
                    "label": 0
                },
                {
                    "sent": "Yes, traditionally the people actually extended the multilinear extensions of the single ability.",
                    "label": 0
                },
                {
                    "sent": "Composition is a parasect.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a fact right?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, this kind of thing they are, they are.",
                    "label": 0
                },
                {
                    "sent": "They are in the 70s.",
                    "label": 0
                },
                {
                    "sent": "We have extensively studied is a factor in the factorization settings, but there are some difference between subsidy learning and factorization.",
                    "label": 0
                },
                {
                    "sent": "I would like to summarize there are streaming difference.",
                    "label": 0
                },
                {
                    "sent": "The first one is that.",
                    "label": 0
                },
                {
                    "sent": "VR works by.",
                    "label": 0
                },
                {
                    "sent": "There are previous work, like tensor phase.",
                    "label": 0
                },
                {
                    "sent": "It's very popular paper, very well cited.",
                    "label": 0
                },
                {
                    "sent": "It's also multilinear setting, but it will treat the tensors no matter if the image.",
                    "label": 0
                },
                {
                    "sent": "Oh wait, no matter the dimension, they will still vectorize it, but they will form the tensor through various modality.",
                    "label": 0
                },
                {
                    "sent": "Or like the factors involved in the formation in the formation of the data, right?",
                    "label": 0
                },
                {
                    "sent": "So it's a it's dealing with different problem and this kind of factorization.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Another key difference is that they require a lot of data.",
                    "label": 0
                },
                {
                    "sent": "For training we have to find the data to form these kind of.",
                    "label": 0
                },
                {
                    "sent": "If we have 35 illumination, three posts and 10 people, we have to find so many samples for each subject right?",
                    "label": 0
                },
                {
                    "sent": "So they require lots of samples and correspondingly there.",
                    "label": 0
                },
                {
                    "sent": "American is very huge.",
                    "label": 0
                },
                {
                    "sent": "The so the processing cost is higher than we can kind of subsidy learning and we think it also have limitation in is good in modeling and understanding the problem, but it may not be well practical nursing practice.",
                    "label": 0
                },
                {
                    "sent": "Usually you have only a few samples and you do not have luxury to form this kind of.",
                    "label": 0
                },
                {
                    "sent": "Sensors with various factors from each person.",
                    "label": 0
                }
            ]
        }
    }
}