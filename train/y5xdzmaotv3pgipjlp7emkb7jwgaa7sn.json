{
    "id": "y5xdzmaotv3pgipjlp7emkb7jwgaa7sn",
    "title": "Learning Halfspaces Under Log-Concave Densities: Polynomial Approximations and Moment Matching",
    "info": {
        "author": [
            "Raghu Meka, School of Mathematics, Institute for Advanced Study, University of Amsterdam"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_meka_matching/",
    "segmentation": [
        [
            "Hi dude, I'll talk about learning intersections of spaces."
        ],
        [
            "And the main object of study here or half spaces.",
            "These are Boolean functions which take can vector of length and output the sign of an affine function evaluator on your inputs.",
            "An geometrically you have a hyperplane and dimensions shown in figure here.",
            "And all points on one side of the hyperplane or label positive and all points in the opposite side are labeled negative and half spaces have been instrumental in the development of many important tools in learning such as perceptrons.",
            "Boosting algorithm, support vector machines and so on.",
            "And here we are going to talk about learning halfspaces.",
            "So this has been studied a lot in computational learning theory and will focus here."
        ],
        [
            "On two natural generalizations of the problem, the first is to learning intersections of half spaces.",
            "So now instead of having a single half space, you have several half spaces, and you're looking at the function, which is the end of all of these half spaces.",
            "In the second generalization is to learn half spaces in the presence of adverse aerial noise.",
            "Or in diagnostic model or non realizable model?",
            "And one of the central questions in computational learning theory is too packed.",
            "Learn intersections of two half spaces.",
            "We can learn 1/2 space, but we cannot do it for two half spaces yet."
        ],
        [
            "So this is a very well studied question.",
            "It has a rich history starting with the work of Bohmann 90 Bowman Cannon in 97 and so on.",
            "And there's a big table with all the results here, but I don't want you to spend too much time reading it.",
            "And the summary here."
        ],
        [
            "Is that we do not have any polynomial time algorithms for learning the intersections of two Hospice is under lock and key distributions, and this is true in the pack model.",
            "An more so in diagnostic model and we look at what locker distributions are and why we should care about them in a few more slides."
        ],
        [
            "There's also previous work on hardness.",
            "For instance, it's known that proper agnostic learning of single halfspaces, NP hard and this was shown by the work of Felman Gopalan fireman at all, and Boris Foreman, Raghavendra, and they're also cryptographic hardness of hardness results for learning intersections of several half spaces."
        ],
        [
            "But we should ask ourselves, given this results are there is a learning problem really hard in practice?",
            "This is the point is that here 1st result is proper learning hardness and the second assumes gives harness under some strange distributions.",
            "So perhaps if you make the right assumption on the distribution, the problem becomes tractable and that is what we focus on here."
        ],
        [
            "And most previous works in this philosophy in this line have focused on learning product distributions.",
            "Do you know anybody with this title online?",
            "Well, maybe not intersections, but our work.",
            "Also, I'm focusing on intersections because they're simply orange and simply describing a talk, but our work also works for arbitrary functions and depth to neural networks is a special case of this.",
            "I'm focusing on intersections because they are just easier to describe.",
            "And product distributions are too restrictive.",
            "I mean often your coordinates has some dependence in them and there won't be independent of one another."
        ],
        [
            "And here that's what we'll try and do.",
            "Will introduce will give a learning algorithm for intersections of half spaces and smooth analysis model andfor LA concave distributions."
        ],
        [
            "So let me first talk about log concave distributions, so a distribution is locked on care if log of the density function is concave.",
            "For example, the Gaussian distribution is log concave, 'cause log of the density is minus X squared.",
            "Exponential distributions are log concave because lock of the dialogue of the densities minus X and so on."
        ],
        [
            "So this is a rich class of distributions, not a single distribution, but a class of distributions and any uniform distribution on a convex set is log concave.",
            "For example, you can take the uniform distribution of the ball as cylinder and so on."
        ],
        [
            "And one of the reasons why previous methods don't work here is that locker distributions are no product structure and then the coordinates can have arbitrary dependencies and they do not have nice symmetric properties that the Gaussian distribution has.",
            "An further details are detailed behavior of lock on your distributions is also not as nice As for the Gaussian case, so none of the previous methods work and we have to develop new methods for.",
            "Learning for agnostically learning intersections, fast passes under lock and key distributions, and I think that should be the main takeaway from this talk, which is this new methods rather than specific result."
        ],
        [
            "And our main theorem will be a polynomial time algorithm for agnostically learning intersections of a constant number of spaces under arbitrary log concave distributions.",
            "So this is the first main result."
        ],
        [
            "And our learning algorithm is actually something that many of you know, which is just a support vector machine with the polynomial kernel.",
            "An what could heuristically view this as another explanation as to why support vector machines do much better than we expect them to.",
            "Fenomena N N1 website or just polynomial in N for constant error constant error and constant number of spaces.",
            "Yeah, it's the polynomial depends on epsilon.",
            "Richard, yeah, I'm going to get to the next slide.",
            "Any other questions?",
            "Alright, let me now talk about smooth analysis."
        ],
        [
            "Framework.",
            "So the smooth analysis framework was introduced by Spellman and thing as a kind of an intermediate point between worst case analysis.",
            "An average case analysis and the philosophy here is that whenever you have some input measurements, you there is some noise in your measurements.",
            "So let's try and actually exploit that to our benefit.",
            "So in the basic basic problem you have your input points shown on the left here and you add some Gaussian noise to the points to get your final data set, and you're trying to solve your problem on this perturbed data set.",
            "And this has been very widely used.",
            "For example, the smooth complexity of simplex algorithm is known to be polynomial time.",
            "There much better guarantees known for Gaussian elimination.",
            "For K means algorithm and so on."
        ],
        [
            "And here we are going to talk about smooth analysis framework for learning, and this was formalized in the work of Bloom, blah blah.",
            "Manton again, and the model is as follows.",
            "So you have an unknown distribution D and will try to learn not against the distribution D as in the pack model, but learn against the distribution which is D plus some added noise.",
            "So let's say the distribution is the one shown on the left here and you had some Gaussian noise with standard deviation Sigma."
        ],
        [
            "And we want to learn against the distribution D. Of Sigma."
        ],
        [
            "And here I didn't stress in the normalization, but the Sigma has to be proportional to the covariance matrix of the original distribution D. So the error is measured this effectively and training is expected to be passed.",
            "Both are measured with respect to D plus noise, so we're just assuming that the eventual distribution you had to learn against isde of Sigma."
        ],
        [
            "And I think it's a very nice model and there has not been much work on it, unfortunately.",
            "Against the plus noise while I'm just add a lot of noise and then there's nothing there."
        ],
        [
            "So you want the noise to be smaller?",
            "I mean you have to control the noise, so as long as the noise is so, your runtime will depend on how small the noises.",
            "If the noise is zero, you can.",
            "You might not have any guarantees, but if the noise is not to negligible you should compare against the noise model.",
            "No, compare the runtime or the performance.",
            "Before the convergence, the convergence.",
            "Well, if you're comparing against the non noisy model you can just add noise to your data.",
            "Once you get it, so it's not going to buy you much.",
            "You get your examples that just add noise.",
            "Running this algorithm, and then you're not gaining anything, so you're also comparing against the noisy version.",
            "That should be the right model.",
            "Variance?",
            "There is major.",
            "They don't allow that functionality there.",
            "No assumptions on the DSO.",
            "The right normalization is that noise.",
            "You add the covariance of that should be at least Sigma times the covariance of the distribution D in any direction.",
            "It's kind of normalization aspect of it.",
            "And then there was some kind of a signal.",
            "Yeah, in the runtime and the guarantees you get.",
            "Ideally, should not go into the exponent.",
            "Haven't stated the result, but I will.",
            "If you let it.",
            "I don't mind getting not getting to it, but.",
            "Any other questions?"
        ],
        [
            "So this thing.",
            "So Lemon, darling and showdown again showed that.",
            "The perceptron algorithm converges with in polynomial time with high probability in the smooth analysis framework, even if you start with no margin distributions.",
            "So usually when you start the perceptron algorithm, you have margin assumption.",
            "But here even if you don't imagine assumption as long as you add noise to it, you're going to converge in polynomial time with high probability.",
            "So our result is that is a polynomial time algorithm for learning any function of halfspaces, constant number of spaces under smooth, subexponential distributions as before for constant epsilon and constant Sigma.",
            "And here this sub exponential distribution and distribution is subexponential if the tails of the distribution fall fast enough and these are widely used in statistics and their other natural distributions which fall in this class but are not log concave as before.",
            "OK."
        ],
        [
            "So here's the outline of the remainder of the talk.",
            "1st, I'll talk about polynomial approximation, which is the main tool we use for designing our algorithms.",
            "Then I'll talk about two new methods for showing existence of good polynomial approximations.",
            "One is approximation by convolution, another is approximation by moment matching polynomials.",
            "That's what we call it."
        ],
        [
            "So let's start with the first part."
        ],
        [
            "And.",
            "The basic fact we will exploit here is that if the function you are trying to learn has good polynomial approximation as a low degree polynomial approximator under the distribution, then you can learn the function in time, which is N to the degree of the polynomial.",
            "And the."
        ],
        [
            "So is using the similar workers Linial, Mansour, Mansour and Nissan to learn AC zero circuits in quasipolynomial time by Kush Levison Mansour to run learn sparse polynomials.",
            "Independence of queries.",
            "And for US clients alike Livens Monstrance already or showed that if you have a low degree approximate or then you also get learning in the agnostic model.",
            "So if you have a low degree approximate or you can learn the function F agnostically even in the presence of adversarial noise.",
            "And most known agnostic algorithms to this day use this approach.",
            "Go by a polynomial approximation."
        ],
        [
            "So our goal is to show that you have a function F. Think of it as intersection of several half spaces, and we're trying to show the existence of a low degree polynomial which approximates F under various distributions.",
            "Dee Dee is log concave is smooth, subexponential and so on.",
            "And there are other most previous works.",
            "Are we sure the existence of such polynomials, but only for proper product distributions in the main reason for that, was if you have a product distribution is easy to guess what they write, polynomial should be.",
            "It comes in before your analysis.",
            "If you look at the Fourier expansion of your polynomial and they'll tell you exactly what the best approximator is.",
            "But if you have a non product structure, this method completely breaks down and we need to do something else."
        ],
        [
            "So does the high level overview of the technique and let me."
        ],
        [
            "Talk about how to show the existence of good polynomial approximations."
        ],
        [
            "So first is let's focus on lock on care distributions.",
            "So was the goal.",
            "You have a distribution D on our end and you have a function F. Then carpet as intersections of EM have spaces and we want to show the existence of a good polynomial approximator."
        ],
        [
            "The case should be em care, but the first thing to note is that the evaluation of the intersection function depends only on the projection in M directions.",
            "You just look at the inner product along the hyperplane vectors and that will determine completely what the original function evaluates too.",
            "So we can think of our function as different only from RM2R.",
            "And the reason why we can do this?"
        ],
        [
            "That the projection of a lock conquer distribution is also lock on care.",
            "So you start with the distribution Deon RN.",
            "You can assume it was projected down to RM and you're trying to learn a function F which is different from RM2R.",
            "So this is our simplified goal an in the."
        ],
        [
            "Stock, I'll actually focus on the simpler case where M = 1.",
            "This might seem like a toy case, but our methods will be sufficiently generic that they will also extend to higher dimensions.",
            "Alright, I made it by this type here.",
            "This should be M sorry.",
            "It's what?",
            "So this slide is correct.",
            "M is the number of spaces, M is the number of hospitals.",
            "Yeah, well, it's just a single half space now.",
            "So here is the goal.",
            "So I just have a single have space on R2R.",
            "This is a court case for the purpose of this talk.",
            "OK so I just want to illustrate it for this simple case.",
            "There is no K here there is.",
            "I know it's a typo and sorry it's a typo, yeah?",
            "Forget K, there's OK here.",
            "K will come later.",
            "Now I don't say that because they will come later.",
            "This small type of it.",
            "Yeah, yeah, right right.",
            "So unfortunately it's difficult to extend that technique that analysis to higher things.",
            "It's not effective, but that's just not.",
            "That's not the only issue.",
            "I don't know how to generalize that to higher things, right?",
            "But this is just an illustration of a technique, and I'm saying this proof extends here."
        ],
        [
            "So what's the goal?",
            "So the goal is you have your over R. If you have a half spaces just looking at the step function right and my step function is shown in red here, and I want a low degree polynomial P such as the blue curve which approximates my step function under the distribution D. OK."
        ],
        [
            "And.",
            "So our goal, and if you wanted to do this before many people did this before, but they use techniques like polynomials and other things, and most of these techniques fail badly if your distribution D does not have sufficiently good tails."
        ],
        [
            "And the reason is that if you take a polynomial at Infinity, it's going to go to Infinity and the error is going to go to Infinity as well compared to the step function.",
            "And you want your distribution D to fall off sufficiently fast to kill off this error, and for subexponential distributions or log concave distributions this does not happen if you do it in a simple way.",
            "And that's the main bottleneck in using PS technique."
        ],
        [
            "And at the very, very high level, the first method is to insert approximate, not F but F convolved with the power of a smooth function.",
            "So."
        ],
        [
            "Analysis goes into steps.",
            "If I spend a polynomial P which approximates F convolved with the power of a smooth function, and then show that the convolution of F with the smooth function is close to F in the first place.",
            "So I won't be able to go into too many more details here because I want to go to the next step.",
            "This is just a high level sketch."
        ],
        [
            "So this is the third part which is approximation by moment matching."
        ],
        [
            "And here the main motivation motivation comes from Pseudorandomness.",
            "There's this connection established body in 2009 for showing the existence of God.",
            "So random generators that.",
            "Uh.",
            "Connection between polynomial approximation and designing good so random generators.",
            "To motivate this, let me define let me introduce the definition, which is kind of important for the rest of the talk.",
            "So here are two distributions, Di, di Prime on RN will say they are K moment matching if all first came moments match."
        ],
        [
            "So for example, if you look at the moment of any monomial of total degree at most K under D is the same as moment of the minimial under D prime.",
            "So then we'll say there K moment matching."
        ],
        [
            "And Bonsey observe that if your function F is fooled by moment matching distributions, then you immediately get good polynomial approximations."
        ],
        [
            "Concretely showed that if your function F cannot distinguish between two distributions, D&D Prime, Witcher came moment matching the expectation of F is the same under both distributions.",
            "Then you are F as loading your approximation as polynomial approximations of degree at most K."
        ],
        [
            "And the proof of this uses LP duality.",
            "Anbody did this for the domain of F was a finite size and for us the domain of F is in finite so you get an infinite LP and you have to be careful to ensure that duality holds in this setting to which we can do."
        ],
        [
            "So what's the punchline here?",
            "We've reduced the problem of showing polynomial approximation to pseudo randomness.",
            "Just show that given just the first few moments of your distribution D, it determines the bias of your function F under the distribution up to small error.",
            "So where does log concavity come in?",
            "It comes in Kated on the fact that those moments right exactly will come in and just hold on for a couple more slides.",
            "If I can get to them."
        ],
        [
            "So to show some randomness, will you something you will appeal to the literature on the classical moment problem and.",
            "The classical moment problem, so it's a rich area starting from the 1900s, and the question here is when do the moments of a distribution uniquely specify the distribution?",
            "So I give you a bunch of numbers which are purported to be the moments of a distribution, and there has to be unique distribution which matches up these moments.",
            "And."
        ],
        [
            "This was basically solved in 1922 by Karloman's condition, which says that the distribution is unique if and only if the sum decimation this infinite series infinite sum diverges.",
            "And don't I mean, if you don't want to spend too much time about this too much time thinking about this, some the punchline is that if the moments don't grow too fast, then you get uniqueness.",
            "OK. And for."
        ],
        [
            "Once we're actually, we're not going to get all possible moments matching, but we're only going to look at finite moment matching.",
            "So we need a finite or robust version of this and."
        ],
        [
            "There is such a result by in probability theory by clever now and.",
            "It's very hard to pronounce Russian name.",
            "It starts with M. And there are two random variables X&Y on R and let's say all their first game moments match.",
            "And then they give a guarantee on the distance between these two random variables where the distance is measured in terms of levy distance and once again don't spend too much time on the error bound.",
            "But it says that if your moments don't go too fast, then the error is small."
        ],
        [
            "And the levy distance is defined here, but let's not get into the."
        ],
        [
            "And then this is a very powerful result.",
            "It also extends to multi dimensional case which is important for us when we handle several halfspaces and we can combine this with LP duality to get our result."
        ],
        [
            "And let me just skip ahead and talk about anchors."
        ],
        [
            "Option, which is where does Lacan cavity come in?",
            "And.",
            "And LA concavity comes in, just cause you have subexponential tails.",
            "So if you have subexponential tails that immediately implies bounce on how fast the moments grow.",
            "And once you have these bonds, you can plug it into the error bump to get closeness."
        ],
        [
            "Well, you have this guarantee on level distance but."
        ],
        [
            "You also need either property of LA Concavity, which is that the period density probability density function is not too peaked.",
            "It's going to be smooth throughout, and that's also important for us and we come."
        ],
        [
            "And this to get our final result.",
            "Let me just stop here."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi dude, I'll talk about learning intersections of spaces.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the main object of study here or half spaces.",
                    "label": 0
                },
                {
                    "sent": "These are Boolean functions which take can vector of length and output the sign of an affine function evaluator on your inputs.",
                    "label": 0
                },
                {
                    "sent": "An geometrically you have a hyperplane and dimensions shown in figure here.",
                    "label": 0
                },
                {
                    "sent": "And all points on one side of the hyperplane or label positive and all points in the opposite side are labeled negative and half spaces have been instrumental in the development of many important tools in learning such as perceptrons.",
                    "label": 0
                },
                {
                    "sent": "Boosting algorithm, support vector machines and so on.",
                    "label": 1
                },
                {
                    "sent": "And here we are going to talk about learning halfspaces.",
                    "label": 0
                },
                {
                    "sent": "So this has been studied a lot in computational learning theory and will focus here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On two natural generalizations of the problem, the first is to learning intersections of half spaces.",
                    "label": 0
                },
                {
                    "sent": "So now instead of having a single half space, you have several half spaces, and you're looking at the function, which is the end of all of these half spaces.",
                    "label": 0
                },
                {
                    "sent": "In the second generalization is to learn half spaces in the presence of adverse aerial noise.",
                    "label": 0
                },
                {
                    "sent": "Or in diagnostic model or non realizable model?",
                    "label": 0
                },
                {
                    "sent": "And one of the central questions in computational learning theory is too packed.",
                    "label": 0
                },
                {
                    "sent": "Learn intersections of two half spaces.",
                    "label": 1
                },
                {
                    "sent": "We can learn 1/2 space, but we cannot do it for two half spaces yet.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a very well studied question.",
                    "label": 0
                },
                {
                    "sent": "It has a rich history starting with the work of Bohmann 90 Bowman Cannon in 97 and so on.",
                    "label": 0
                },
                {
                    "sent": "And there's a big table with all the results here, but I don't want you to spend too much time reading it.",
                    "label": 0
                },
                {
                    "sent": "And the summary here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that we do not have any polynomial time algorithms for learning the intersections of two Hospice is under lock and key distributions, and this is true in the pack model.",
                    "label": 0
                },
                {
                    "sent": "An more so in diagnostic model and we look at what locker distributions are and why we should care about them in a few more slides.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's also previous work on hardness.",
                    "label": 0
                },
                {
                    "sent": "For instance, it's known that proper agnostic learning of single halfspaces, NP hard and this was shown by the work of Felman Gopalan fireman at all, and Boris Foreman, Raghavendra, and they're also cryptographic hardness of hardness results for learning intersections of several half spaces.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we should ask ourselves, given this results are there is a learning problem really hard in practice?",
                    "label": 1
                },
                {
                    "sent": "This is the point is that here 1st result is proper learning hardness and the second assumes gives harness under some strange distributions.",
                    "label": 0
                },
                {
                    "sent": "So perhaps if you make the right assumption on the distribution, the problem becomes tractable and that is what we focus on here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And most previous works in this philosophy in this line have focused on learning product distributions.",
                    "label": 0
                },
                {
                    "sent": "Do you know anybody with this title online?",
                    "label": 0
                },
                {
                    "sent": "Well, maybe not intersections, but our work.",
                    "label": 0
                },
                {
                    "sent": "Also, I'm focusing on intersections because they're simply orange and simply describing a talk, but our work also works for arbitrary functions and depth to neural networks is a special case of this.",
                    "label": 0
                },
                {
                    "sent": "I'm focusing on intersections because they are just easier to describe.",
                    "label": 0
                },
                {
                    "sent": "And product distributions are too restrictive.",
                    "label": 1
                },
                {
                    "sent": "I mean often your coordinates has some dependence in them and there won't be independent of one another.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here that's what we'll try and do.",
                    "label": 0
                },
                {
                    "sent": "Will introduce will give a learning algorithm for intersections of half spaces and smooth analysis model andfor LA concave distributions.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me first talk about log concave distributions, so a distribution is locked on care if log of the density function is concave.",
                    "label": 0
                },
                {
                    "sent": "For example, the Gaussian distribution is log concave, 'cause log of the density is minus X squared.",
                    "label": 1
                },
                {
                    "sent": "Exponential distributions are log concave because lock of the dialogue of the densities minus X and so on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a rich class of distributions, not a single distribution, but a class of distributions and any uniform distribution on a convex set is log concave.",
                    "label": 0
                },
                {
                    "sent": "For example, you can take the uniform distribution of the ball as cylinder and so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the reasons why previous methods don't work here is that locker distributions are no product structure and then the coordinates can have arbitrary dependencies and they do not have nice symmetric properties that the Gaussian distribution has.",
                    "label": 1
                },
                {
                    "sent": "An further details are detailed behavior of lock on your distributions is also not as nice As for the Gaussian case, so none of the previous methods work and we have to develop new methods for.",
                    "label": 0
                },
                {
                    "sent": "Learning for agnostically learning intersections, fast passes under lock and key distributions, and I think that should be the main takeaway from this talk, which is this new methods rather than specific result.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our main theorem will be a polynomial time algorithm for agnostically learning intersections of a constant number of spaces under arbitrary log concave distributions.",
                    "label": 0
                },
                {
                    "sent": "So this is the first main result.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And our learning algorithm is actually something that many of you know, which is just a support vector machine with the polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "An what could heuristically view this as another explanation as to why support vector machines do much better than we expect them to.",
                    "label": 0
                },
                {
                    "sent": "Fenomena N N1 website or just polynomial in N for constant error constant error and constant number of spaces.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's the polynomial depends on epsilon.",
                    "label": 0
                },
                {
                    "sent": "Richard, yeah, I'm going to get to the next slide.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright, let me now talk about smooth analysis.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "So the smooth analysis framework was introduced by Spellman and thing as a kind of an intermediate point between worst case analysis.",
                    "label": 0
                },
                {
                    "sent": "An average case analysis and the philosophy here is that whenever you have some input measurements, you there is some noise in your measurements.",
                    "label": 0
                },
                {
                    "sent": "So let's try and actually exploit that to our benefit.",
                    "label": 0
                },
                {
                    "sent": "So in the basic basic problem you have your input points shown on the left here and you add some Gaussian noise to the points to get your final data set, and you're trying to solve your problem on this perturbed data set.",
                    "label": 0
                },
                {
                    "sent": "And this has been very widely used.",
                    "label": 0
                },
                {
                    "sent": "For example, the smooth complexity of simplex algorithm is known to be polynomial time.",
                    "label": 0
                },
                {
                    "sent": "There much better guarantees known for Gaussian elimination.",
                    "label": 1
                },
                {
                    "sent": "For K means algorithm and so on.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here we are going to talk about smooth analysis framework for learning, and this was formalized in the work of Bloom, blah blah.",
                    "label": 0
                },
                {
                    "sent": "Manton again, and the model is as follows.",
                    "label": 0
                },
                {
                    "sent": "So you have an unknown distribution D and will try to learn not against the distribution D as in the pack model, but learn against the distribution which is D plus some added noise.",
                    "label": 1
                },
                {
                    "sent": "So let's say the distribution is the one shown on the left here and you had some Gaussian noise with standard deviation Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we want to learn against the distribution D. Of Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here I didn't stress in the normalization, but the Sigma has to be proportional to the covariance matrix of the original distribution D. So the error is measured this effectively and training is expected to be passed.",
                    "label": 0
                },
                {
                    "sent": "Both are measured with respect to D plus noise, so we're just assuming that the eventual distribution you had to learn against isde of Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think it's a very nice model and there has not been much work on it, unfortunately.",
                    "label": 0
                },
                {
                    "sent": "Against the plus noise while I'm just add a lot of noise and then there's nothing there.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you want the noise to be smaller?",
                    "label": 0
                },
                {
                    "sent": "I mean you have to control the noise, so as long as the noise is so, your runtime will depend on how small the noises.",
                    "label": 0
                },
                {
                    "sent": "If the noise is zero, you can.",
                    "label": 0
                },
                {
                    "sent": "You might not have any guarantees, but if the noise is not to negligible you should compare against the noise model.",
                    "label": 0
                },
                {
                    "sent": "No, compare the runtime or the performance.",
                    "label": 0
                },
                {
                    "sent": "Before the convergence, the convergence.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're comparing against the non noisy model you can just add noise to your data.",
                    "label": 0
                },
                {
                    "sent": "Once you get it, so it's not going to buy you much.",
                    "label": 0
                },
                {
                    "sent": "You get your examples that just add noise.",
                    "label": 0
                },
                {
                    "sent": "Running this algorithm, and then you're not gaining anything, so you're also comparing against the noisy version.",
                    "label": 0
                },
                {
                    "sent": "That should be the right model.",
                    "label": 0
                },
                {
                    "sent": "Variance?",
                    "label": 0
                },
                {
                    "sent": "There is major.",
                    "label": 0
                },
                {
                    "sent": "They don't allow that functionality there.",
                    "label": 0
                },
                {
                    "sent": "No assumptions on the DSO.",
                    "label": 0
                },
                {
                    "sent": "The right normalization is that noise.",
                    "label": 0
                },
                {
                    "sent": "You add the covariance of that should be at least Sigma times the covariance of the distribution D in any direction.",
                    "label": 0
                },
                {
                    "sent": "It's kind of normalization aspect of it.",
                    "label": 0
                },
                {
                    "sent": "And then there was some kind of a signal.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the runtime and the guarantees you get.",
                    "label": 0
                },
                {
                    "sent": "Ideally, should not go into the exponent.",
                    "label": 0
                },
                {
                    "sent": "Haven't stated the result, but I will.",
                    "label": 0
                },
                {
                    "sent": "If you let it.",
                    "label": 0
                },
                {
                    "sent": "I don't mind getting not getting to it, but.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this thing.",
                    "label": 0
                },
                {
                    "sent": "So Lemon, darling and showdown again showed that.",
                    "label": 0
                },
                {
                    "sent": "The perceptron algorithm converges with in polynomial time with high probability in the smooth analysis framework, even if you start with no margin distributions.",
                    "label": 1
                },
                {
                    "sent": "So usually when you start the perceptron algorithm, you have margin assumption.",
                    "label": 0
                },
                {
                    "sent": "But here even if you don't imagine assumption as long as you add noise to it, you're going to converge in polynomial time with high probability.",
                    "label": 0
                },
                {
                    "sent": "So our result is that is a polynomial time algorithm for learning any function of halfspaces, constant number of spaces under smooth, subexponential distributions as before for constant epsilon and constant Sigma.",
                    "label": 0
                },
                {
                    "sent": "And here this sub exponential distribution and distribution is subexponential if the tails of the distribution fall fast enough and these are widely used in statistics and their other natural distributions which fall in this class but are not log concave as before.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the outline of the remainder of the talk.",
                    "label": 0
                },
                {
                    "sent": "1st, I'll talk about polynomial approximation, which is the main tool we use for designing our algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk about two new methods for showing existence of good polynomial approximations.",
                    "label": 0
                },
                {
                    "sent": "One is approximation by convolution, another is approximation by moment matching polynomials.",
                    "label": 1
                },
                {
                    "sent": "That's what we call it.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start with the first part.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The basic fact we will exploit here is that if the function you are trying to learn has good polynomial approximation as a low degree polynomial approximator under the distribution, then you can learn the function in time, which is N to the degree of the polynomial.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So is using the similar workers Linial, Mansour, Mansour and Nissan to learn AC zero circuits in quasipolynomial time by Kush Levison Mansour to run learn sparse polynomials.",
                    "label": 0
                },
                {
                    "sent": "Independence of queries.",
                    "label": 0
                },
                {
                    "sent": "And for US clients alike Livens Monstrance already or showed that if you have a low degree approximate or then you also get learning in the agnostic model.",
                    "label": 0
                },
                {
                    "sent": "So if you have a low degree approximate or you can learn the function F agnostically even in the presence of adversarial noise.",
                    "label": 0
                },
                {
                    "sent": "And most known agnostic algorithms to this day use this approach.",
                    "label": 1
                },
                {
                    "sent": "Go by a polynomial approximation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goal is to show that you have a function F. Think of it as intersection of several half spaces, and we're trying to show the existence of a low degree polynomial which approximates F under various distributions.",
                    "label": 0
                },
                {
                    "sent": "Dee Dee is log concave is smooth, subexponential and so on.",
                    "label": 0
                },
                {
                    "sent": "And there are other most previous works.",
                    "label": 1
                },
                {
                    "sent": "Are we sure the existence of such polynomials, but only for proper product distributions in the main reason for that, was if you have a product distribution is easy to guess what they write, polynomial should be.",
                    "label": 0
                },
                {
                    "sent": "It comes in before your analysis.",
                    "label": 0
                },
                {
                    "sent": "If you look at the Fourier expansion of your polynomial and they'll tell you exactly what the best approximator is.",
                    "label": 0
                },
                {
                    "sent": "But if you have a non product structure, this method completely breaks down and we need to do something else.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So does the high level overview of the technique and let me.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about how to show the existence of good polynomial approximations.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first is let's focus on lock on care distributions.",
                    "label": 0
                },
                {
                    "sent": "So was the goal.",
                    "label": 0
                },
                {
                    "sent": "You have a distribution D on our end and you have a function F. Then carpet as intersections of EM have spaces and we want to show the existence of a good polynomial approximator.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The case should be em care, but the first thing to note is that the evaluation of the intersection function depends only on the projection in M directions.",
                    "label": 1
                },
                {
                    "sent": "You just look at the inner product along the hyperplane vectors and that will determine completely what the original function evaluates too.",
                    "label": 0
                },
                {
                    "sent": "So we can think of our function as different only from RM2R.",
                    "label": 0
                },
                {
                    "sent": "And the reason why we can do this?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That the projection of a lock conquer distribution is also lock on care.",
                    "label": 0
                },
                {
                    "sent": "So you start with the distribution Deon RN.",
                    "label": 0
                },
                {
                    "sent": "You can assume it was projected down to RM and you're trying to learn a function F which is different from RM2R.",
                    "label": 0
                },
                {
                    "sent": "So this is our simplified goal an in the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stock, I'll actually focus on the simpler case where M = 1.",
                    "label": 0
                },
                {
                    "sent": "This might seem like a toy case, but our methods will be sufficiently generic that they will also extend to higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "Alright, I made it by this type here.",
                    "label": 0
                },
                {
                    "sent": "This should be M sorry.",
                    "label": 0
                },
                {
                    "sent": "It's what?",
                    "label": 0
                },
                {
                    "sent": "So this slide is correct.",
                    "label": 0
                },
                {
                    "sent": "M is the number of spaces, M is the number of hospitals.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, it's just a single half space now.",
                    "label": 0
                },
                {
                    "sent": "So here is the goal.",
                    "label": 0
                },
                {
                    "sent": "So I just have a single have space on R2R.",
                    "label": 0
                },
                {
                    "sent": "This is a court case for the purpose of this talk.",
                    "label": 0
                },
                {
                    "sent": "OK so I just want to illustrate it for this simple case.",
                    "label": 0
                },
                {
                    "sent": "There is no K here there is.",
                    "label": 0
                },
                {
                    "sent": "I know it's a typo and sorry it's a typo, yeah?",
                    "label": 0
                },
                {
                    "sent": "Forget K, there's OK here.",
                    "label": 0
                },
                {
                    "sent": "K will come later.",
                    "label": 0
                },
                {
                    "sent": "Now I don't say that because they will come later.",
                    "label": 0
                },
                {
                    "sent": "This small type of it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, right right.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately it's difficult to extend that technique that analysis to higher things.",
                    "label": 0
                },
                {
                    "sent": "It's not effective, but that's just not.",
                    "label": 0
                },
                {
                    "sent": "That's not the only issue.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to generalize that to higher things, right?",
                    "label": 0
                },
                {
                    "sent": "But this is just an illustration of a technique, and I'm saying this proof extends here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the goal?",
                    "label": 0
                },
                {
                    "sent": "So the goal is you have your over R. If you have a half spaces just looking at the step function right and my step function is shown in red here, and I want a low degree polynomial P such as the blue curve which approximates my step function under the distribution D. OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So our goal, and if you wanted to do this before many people did this before, but they use techniques like polynomials and other things, and most of these techniques fail badly if your distribution D does not have sufficiently good tails.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the reason is that if you take a polynomial at Infinity, it's going to go to Infinity and the error is going to go to Infinity as well compared to the step function.",
                    "label": 0
                },
                {
                    "sent": "And you want your distribution D to fall off sufficiently fast to kill off this error, and for subexponential distributions or log concave distributions this does not happen if you do it in a simple way.",
                    "label": 0
                },
                {
                    "sent": "And that's the main bottleneck in using PS technique.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And at the very, very high level, the first method is to insert approximate, not F but F convolved with the power of a smooth function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Analysis goes into steps.",
                    "label": 0
                },
                {
                    "sent": "If I spend a polynomial P which approximates F convolved with the power of a smooth function, and then show that the convolution of F with the smooth function is close to F in the first place.",
                    "label": 1
                },
                {
                    "sent": "So I won't be able to go into too many more details here because I want to go to the next step.",
                    "label": 0
                },
                {
                    "sent": "This is just a high level sketch.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the third part which is approximation by moment matching.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here the main motivation motivation comes from Pseudorandomness.",
                    "label": 0
                },
                {
                    "sent": "There's this connection established body in 2009 for showing the existence of God.",
                    "label": 0
                },
                {
                    "sent": "So random generators that.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Connection between polynomial approximation and designing good so random generators.",
                    "label": 1
                },
                {
                    "sent": "To motivate this, let me define let me introduce the definition, which is kind of important for the rest of the talk.",
                    "label": 1
                },
                {
                    "sent": "So here are two distributions, Di, di Prime on RN will say they are K moment matching if all first came moments match.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, if you look at the moment of any monomial of total degree at most K under D is the same as moment of the minimial under D prime.",
                    "label": 0
                },
                {
                    "sent": "So then we'll say there K moment matching.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Bonsey observe that if your function F is fooled by moment matching distributions, then you immediately get good polynomial approximations.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concretely showed that if your function F cannot distinguish between two distributions, D&D Prime, Witcher came moment matching the expectation of F is the same under both distributions.",
                    "label": 0
                },
                {
                    "sent": "Then you are F as loading your approximation as polynomial approximations of degree at most K.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the proof of this uses LP duality.",
                    "label": 0
                },
                {
                    "sent": "Anbody did this for the domain of F was a finite size and for us the domain of F is in finite so you get an infinite LP and you have to be careful to ensure that duality holds in this setting to which we can do.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's the punchline here?",
                    "label": 0
                },
                {
                    "sent": "We've reduced the problem of showing polynomial approximation to pseudo randomness.",
                    "label": 0
                },
                {
                    "sent": "Just show that given just the first few moments of your distribution D, it determines the bias of your function F under the distribution up to small error.",
                    "label": 0
                },
                {
                    "sent": "So where does log concavity come in?",
                    "label": 0
                },
                {
                    "sent": "It comes in Kated on the fact that those moments right exactly will come in and just hold on for a couple more slides.",
                    "label": 0
                },
                {
                    "sent": "If I can get to them.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to show some randomness, will you something you will appeal to the literature on the classical moment problem and.",
                    "label": 0
                },
                {
                    "sent": "The classical moment problem, so it's a rich area starting from the 1900s, and the question here is when do the moments of a distribution uniquely specify the distribution?",
                    "label": 1
                },
                {
                    "sent": "So I give you a bunch of numbers which are purported to be the moments of a distribution, and there has to be unique distribution which matches up these moments.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was basically solved in 1922 by Karloman's condition, which says that the distribution is unique if and only if the sum decimation this infinite series infinite sum diverges.",
                    "label": 1
                },
                {
                    "sent": "And don't I mean, if you don't want to spend too much time about this too much time thinking about this, some the punchline is that if the moments don't grow too fast, then you get uniqueness.",
                    "label": 0
                },
                {
                    "sent": "OK. And for.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we're actually, we're not going to get all possible moments matching, but we're only going to look at finite moment matching.",
                    "label": 0
                },
                {
                    "sent": "So we need a finite or robust version of this and.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is such a result by in probability theory by clever now and.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to pronounce Russian name.",
                    "label": 0
                },
                {
                    "sent": "It starts with M. And there are two random variables X&Y on R and let's say all their first game moments match.",
                    "label": 0
                },
                {
                    "sent": "And then they give a guarantee on the distance between these two random variables where the distance is measured in terms of levy distance and once again don't spend too much time on the error bound.",
                    "label": 0
                },
                {
                    "sent": "But it says that if your moments don't go too fast, then the error is small.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the levy distance is defined here, but let's not get into the.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is a very powerful result.",
                    "label": 0
                },
                {
                    "sent": "It also extends to multi dimensional case which is important for us when we handle several halfspaces and we can combine this with LP duality to get our result.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And let me just skip ahead and talk about anchors.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Option, which is where does Lacan cavity come in?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And LA concavity comes in, just cause you have subexponential tails.",
                    "label": 1
                },
                {
                    "sent": "So if you have subexponential tails that immediately implies bounce on how fast the moments grow.",
                    "label": 0
                },
                {
                    "sent": "And once you have these bonds, you can plug it into the error bump to get closeness.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you have this guarantee on level distance but.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You also need either property of LA Concavity, which is that the period density probability density function is not too peaked.",
                    "label": 0
                },
                {
                    "sent": "It's going to be smooth throughout, and that's also important for us and we come.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this to get our final result.",
                    "label": 0
                },
                {
                    "sent": "Let me just stop here.",
                    "label": 0
                }
            ]
        }
    }
}