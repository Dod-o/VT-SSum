{
    "id": "wrpp6uuyop3ayduykl5wnpb7sixwheow",
    "title": "Top-down vs. bottom-up methods for hierarchical classification",
    "info": {
        "author": [
            "Claudio Gentile, University of Insubria"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output"
        ]
    },
    "url": "http://videolectures.net/oh06_gentile_tdvbm/",
    "segmentation": [
        [
            "Shall we start out from?",
            "Yeah, I'll be talking about hierarchical classification and basically.",
            "I'll tell you about recent feature recent research that we've been doing that I've been doing with these two guys.",
            "Nickel oceans of Yankee and.",
            "Luca is anybody?",
            "OK. Hierarchical classification is a very well studied research subject.",
            "There have been many papers on hierarchical classifications."
        ],
        [
            "This is colder in Sami domain chain.",
            "This wasn't Acnl, 97, Hoffman Szeremeta and all.",
            "This is horosho at all.",
            "I'm more recent stuff so there are many many models of hierarchy of classification.",
            "There is no real.",
            "I would say agreement of what the best model is and many models means many algorithms, many approaches and so on.",
            "Top down approaches, bottom up approaches local global.",
            "Online match pachinko allocation methods and so on there are quite a lot.",
            "The outline of this talk is the following.",
            "First of all, I will introduce my own hierarchical classification framework.",
            "Then I'll be talking about bottom up algorithm explaining the moment, but bottom up is which we call BS VM.",
            "It's a combination of.",
            "Bayes optimal classifier for a given.",
            "Modal generating the labels will see in a moment with SVM.",
            "It's based optimal classifier with respect to, you know.",
            "Model that generates subtrees as labels, 'cause here labels are subtrees.",
            "And then I report some report on some experiments we've made comparing this PST EM algorithm, which is a bottom up and global approach to a baseline algorithm which is called HFM, which.",
            "Was mentioned the previous talk as well on both artificial and reward medium size datasets, and this is basically experimental work.",
            "And then I'll talk about some, you know.",
            "More theoretical work.",
            "It's an online.",
            "It's a regression analysis of an online algorithm.",
            "Talk about the model specific parametric model for the labels.",
            "An algorithm derived from this model, and I regret analysis.",
            "So it's a half and half.",
            "Experimental theoretical."
        ],
        [
            "So the model.",
            "In our case, hierarchy is given to us ahead of time.",
            "You're given a hierarchy hierarchy, just a.",
            "The taxonomy is just a tree forest, a bunch of trees.",
            "You see?",
            "Plant taxonomy on the left.",
            "The same taxonomy is on the right label.",
            "What we call a multilabel, actually it's just a way of.",
            "Picking nodes within this taxonomy in some way that is consistent with that taxonomy.",
            "Idea of structuring things and multi label is a legal multi label if it is a union of paths within the tree within the trees.",
            "So for instance 1 two is a path 1 three is another path, six 810 is another path and so the multi label 12368 ten is a legal multi label OK. What is one thing that is upward closed?",
            "Yes, basically it's Supper Club Union of Path, meaning that whenever you choose something here, everything on.",
            "On the top should be to be chosen.",
            "On the right hand side you see.",
            "And illegal multi label not up or closed.",
            "Basically if you choose this one, you have to choose this one as well.",
            "And this is not a legal multilabel.",
            "We can, of course, associated with each you know, multi label binary vectoring with the obvious meaning.",
            "This one is chosen.",
            "This one is chosen.",
            "This one, this one is not chosen and so on, OK. An example.",
            "Repair X is an instance vector for simplicity and the is the associated legal multilabel.",
            "OK, one question.",
            "I'm doing also with party so just with trees.",
            "Moralistic case would be hiring dogs, for instance.",
            "Yeah we could.",
            "We could extend it to dogs, but this case I'm talking bout threes as well.",
            "There are some subtleties around in theoretical side I think we're pretty confident that we could replace dogs.",
            "So in the input X is a tree or what is the axis?",
            "Whatever you like, you can kernelized this thing.",
            "So X could be, you know, everything that is compatible with.",
            "You know, discover the machinery you can think of X to be a bunch of real numbers.",
            "So where is the tree structure here?",
            "Is that restructure the output?",
            "So I mean it may be mentioned in the document classifications, I will I will OK X is just a way of encoding and document.",
            "OK, bag of words, something and document can be classified, not just being a single topic they talk about.",
            "No soccer here, but it can talk about, you know.",
            "What's what's about sports, sports and politics and soccer as well?",
            "That really is part of the prior knowledge.",
            "Yeah, yeah, it's given it's given."
        ],
        [
            "How do we generate multi labels?",
            "We are given an instance, say a document for instance.",
            "And we associate with multi label random binary vector Capital B and this capital N is the number of topics, the number of nodes in the tree.",
            "And we basically do the following.",
            "The conditional distribution of G of V given X is the product of these probability factors.",
            "Here what we do is to associate.",
            "We basically build a simple generative model for the labels.",
            "We ask the shade with each node in the hierarchy.",
            "A conditional distribution piece of X is the probability is the distribution of.",
            "Visa by the value associated with this node I given the value associated with the parent node.",
            "And the instance BX.",
            "OK, so for instance here.",
            "Yeah.",
            "You see this little hierarchy on the on the left.",
            "Basically do.",
            "The distribution of V1V5.",
            "Since we have only five nodes, even X is given by.",
            "Being one of X, let me write it.",
            "So the probability of.",
            "The one in X times the probability of B2 given the parent that parent node.",
            "So V2 given V1 and then X times the probability of V3 even the 1X times the probability.",
            "Before even the parent node which is 3V3 and X times probability V5, given node V3, which is still the parent node and X.",
            "So we have this.",
            "We had this number so we have this distributions here.",
            "Associated with each node.",
            "And.",
            "This implicit is saying that.",
            "Whenever we are given.",
            "The value.",
            "On the parent node, or even no apparent node, say we are given the value of three.",
            "Here then.",
            "By given X means the probability is all of them are one.",
            "Not really, not really, not yet.",
            "The probability that V1 we fight take on some value which is either 01.",
            "I'll fix this problem in the bottom line here.",
            "And I think I think you got that.",
            "If we fix the value of three.",
            "Then conditioned on the value of three being something, say one, then these two guys.",
            "The value of these two children are independent variable.",
            "OK, this would not be a good assumption for hierarchies.",
            "But recall that here label might be a subtree as well, so if we choose this node, we might well choose this one as well.",
            "So they're not, say, mutually exclusive.",
            "We might enforce some, you know, negative correlation among children, but this we didn't do.",
            "It's an open question, let's say.",
            "And of course, as she was sort of mentioning, we want to generate with this model illegal multi label in the sense of the previous previous slide.",
            "So we want to sort of enforce that if this guy is not chosen is this guy gets labeled zero, then everything underneath gets labeled zero as well, right?",
            "If this guy is 1, then these two are independent, can be.",
            "I think value 01 independently.",
            "If this is zero, then these two guys are zero for sure.",
            "OK. And this is basically about this blog online thing.",
            "For every possible instance vector."
        ],
        [
            "Loss function.",
            "So we have a way of generating labels.",
            "How do we measure?",
            "The accuracy of our algorithms.",
            "We had this hierarchical loss function H loss.",
            "And this is it works as follows.",
            "Basically you had to want to compare two labelings to legal labelings to legal multi labels.",
            "You have a blue prediction and the red Label two label so.",
            "In comparing these two is sort of comparing the discrepancy between the two.",
            "What you have to do this is loss function.",
            "See, just consider as you go downwards the tree.",
            "You just consider the node where you first encounter a mistake.",
            "So if I go downwards here from 124, this is a mistake, and now because it's not taken here while it is taken here, this is a mistake and node and.",
            "Everything underneath this node.",
            "This mistake in node is irrelevant.",
            "OK, this is counted as a mistake and it is weighted accordingly.",
            "But then all later mistakes underneath the subtree rooted at this, this node four are irrelevant.",
            "For instance, 8 is a mistaken node as well, because it is taking here is not taking here.",
            "But this does not count as a mistake OK. Is symmetric is false positive and negative?",
            "What do you mean by symmetric?",
            "The same loss if you do not label the node, it was.",
            "Should have been labeled or if you label and know that should not have been.",
            "I so yeah, from this point of view, yeah, this is symmetric from this point of view, this symmetric.",
            "In other words, if this is not taken, this is taken, yet it will be the same.",
            "Another example is.",
            "Madden is another.",
            "Mistaken note.",
            "You say that you only count the first node.",
            "If it's a mistake in the other direction, then it's most severe.",
            "If you keep making ones download.",
            "It is not implied by being the one.",
            "Yeah, we.",
            "I think we could.",
            "We could wait the two.",
            "Yeah we could wait two and a different way but.",
            "If you add the lights full then you are forced to have zeros eight and nine absolute, but here's one is full.",
            "Then there are two different cases.",
            "One is flowing zeros and 89 or one is full, obviously, but then they count them as the same mistake and it is.",
            "The cost is the same.",
            "Yeah, we.",
            "I think we could.",
            "We could extend it.",
            "I think we could extend it, by the way.",
            "OK, seven mistaken.",
            "These guys do not count are not taking into consideration and you also might want to.",
            "You know, put weights on the.",
            "The nodes.",
            "Cost coefficients in a node.",
            "Two, you know.",
            "To account for the relative importance of the mistakes that you are making.",
            "So if you're making mistakes at the roots are more severe mistakes than if you made mistakes, at least something like this.",
            "What people do I mean?",
            "In the standard age loss, they the weights are higher at the roots then at the.",
            "Well, you mean the standard HL?",
            "No standard.",
            "Case of Yankee.",
            "We had OK. We had to ski.",
            "Friends of the table.",
            "OK, we have two schemes here.",
            "Either we chose this this coefficient to be one.",
            "Everything is 1 or we choose.",
            "Say we give one costs one to this, we've cost 1/2 to this and we give cost 1 third, 116 in a way that the sum of these three guys is exactly the same as.",
            "But how many you know other alternative schemes?",
            "So each node has just one parent.",
            "In this talk, yes.",
            "You could you could generalize, and of course everything actually goes through with, you know with the generative model as simple as saying.",
            "Would have to make different choices.",
            "Yeah, probably probably.",
            "2 pounds."
        ],
        [
            "So once we have the you know this simple generative model for the labels and we have loss.",
            "We can define Bayes optimal classifier for this.",
            "Which is obviously, you know, the one that the labeling that minimizes the expected loss given given X being the.",
            "The inspector and it turns out that it can be computed very easily as a standard bottom up message passing algorithm.",
            "So it was like this.",
            "I'd like to recall it because then our algorithm is based on this scheme, so we are given this conditional distributions associated with nodes, which I call pizza by.",
            "So this is.",
            "The one piece of one piece of two and so on piece of three, and so on.",
            "The deep these are all for the P1 is equal to water heaters before right?",
            "He said again, please.",
            "This is only for if we want equal to 1.",
            "Well, at this stage this takes, yeah yeah.",
            "Also this guy will generate legal multi labels anyway, so this definition of P1 X.",
            "Well this holds in general, but then you had to choose whether you want to be 0 here or not.",
            "Well, I mean pizza by of X can be either.",
            "Sandy traditional can be 0 depending on whether the parent was zero, not.",
            "But for Fortune, is P1 of X is the probability that the one is 1?",
            "So yeah, OK, OK when I got there.",
            "So yeah, it's probably that this guy condition on this B1.",
            "That's what you go yeah, yeah, yeah yeah yeah OK. And it works like this.",
            "We're given these probabilities basically, and leaves get labeled as follows.",
            "Pizza by say pizza at 7.",
            "Is larger than 1/2, then it gets labeled 1, otherwise yes, label OK.",
            "So for instance, this is less than 1/2 and so they get gas level G. Or this is large and 1/2 gets a little one.",
            "And then we build messages."
        ],
        [
            "Based on these values, so the message is passed upwards.",
            "Is either 1 -- P seven.",
            "If this guy is labeled one or P7, if this guy gets labeled zero, it was labeled one, so it's 1 -- P seven.",
            "So 1 -- P Seven is passed upwards.",
            "This guy was labeled 0 so P8 is is the message minus B9 vehicles.",
            "Level one and these three messages are collected by the node six which computes the sum Sigma.",
            "OK, by the way, this is assuming that the cost coefficients are one or old one.",
            "If we have cost coefficients, we had to multiply each message by the cost coefficient associated with each node.",
            "OK, these things.",
            "OK, these messages are collected by 6.",
            "And six computes its own label.",
            "According to this rule."
        ],
        [
            "Is if its corresponding piece by piece of six is larger than 1 / 2?",
            "Minus Sigma and it gets labeled 1 otherwise gets labeled zero Sigma.",
            "I recall the Sigma was this some of the messages that.",
            "Wet flowing upwards.",
            "OK. And then P6.",
            "Second phase there.",
            "Then you go back down, correct things below and or.",
            "If I mean, let's say that was labeled zero, what would happen to the one here?",
            "Yeah, then everything gets labeled 0, so you have to go back and correct the certain amount that one.",
            "Pretty much, yeah, yeah.",
            "Yeah, I forgot to mention this line.",
            "If this guy gets labeled zero by some sheer accident, then everything is underneath.",
            "Yes label sealer."
        ],
        [
            "OK, so you know this message gets passed upwards and so on."
        ],
        [
            "So what's this algorithm be SVN?",
            "So BSM works as follows.",
            "We have an SVN sitting at each node.",
            "On the hierarchy and SCM keana.",
            "See em here.",
            "And so on.",
            "Why SVN?",
            "There's nothing special in this game, by the way.",
            "She could be pretty much everything at this point.",
            "Any say any linear classifier will work?",
            "Anyway, we have an SVN and each SVN is delivering a weight vector including, say, biased or something.",
            "Does this work?",
            "We feed each SCN with a subset of the training set.",
            "So each node filters out examples for its kids.",
            "Basically.",
            "So this guy here gets trained only with those examples.",
            "That the parent node G. Has labeled one OK.",
            "It just says 0, so you don't even pass this downward.",
            "OK.",
            "So basically the root nodes.",
            "Our training with all labels the least.",
            "Our trade with a small subset of labels.",
            "They are less important so.",
            "And then we associate.",
            "Sorry, then we approximate these pizza by of X, which of course are unavailable with the outcome of the SVM at each node by fitting a sigmoid using the so-called Platt method.",
            "You know we have this sigmoid here.",
            "This is the weight vector produced by the SVM.",
            "And yeah, there's a way of producing probabilities out of SVN, and we have to fit parameters here and we do it through cross validation on the training set.",
            "Just a useful just question.",
            "Why you wanting sweater?",
            "Yeah, yeah, we're playing tourism now for now.",
            "We don't have any.",
            "Well, yeah, yeah yeah, we're planning to do so actually.",
            "Why I do or why I don't want to do that instead of just using logistic?",
            "No disable software, just our first attempt to do that.",
            "We didn't do logistic regression.",
            "This this won't well.",
            "It's difficult as well.",
            "You are really doing theoretical work.",
            "You know this is not theoretical, come on.",
            "You want to save his consistent other stuff that you do this way.",
            "Don't be consistent.",
            "Yeah, yeah, I know.",
            "I know this is not.",
            "I'm not playing this theoretical role.",
            "But I am really concerned about running time here, so it may be logistic regression, but maybe you know more than me on this.",
            "Additional cost really.",
            "I'm planning to do that.",
            "Thing is that the student was playing was supposed to do that just left so.",
            "Please classify all those stuff they want to do this.",
            "It won't get you there.",
            "Yeah yeah, he's just a way of, you know, sort of making things have principles in a way, that's all.",
            "Amazon.",
            "Only me, but others are that well yeah, there it says the Holy Church.",
            "Ilisy involve is that essentially if they do this way, when the computer, the probability it would be consistent.",
            "Anyway.",
            "And then once we fit this invoice.",
            "We play this bottom up game.",
            "We had these things instead of that.",
            "Rupee isibaya we propagate upwards.",
            "So once you.",
            "Once you start thinking of how these things work, you realize that what this algorithm is actually doing is is trying to infer some good thresholds for SVN's at each node.",
            "So basically this boils down to."
        ],
        [
            "Using this bottom up scheme boils down to using SVM with modified thresholds at each node.",
            "Well, the threshold at node J does actually depend on the behavior of children underneath.",
            "So your claim is that our Jays, independent of the particular example, or Kouji, is inferred from the from the training set.",
            "So we aren't evaluation phase now.",
            "OK question is how can you determine before you see the test example.",
            "No, you have to see the test example.",
            "You had to see what the children, how the children work on the test example.",
            "So it said they do actually depend on what the children are doing.",
            "Basically, it works as follows.",
            "If the children here.",
            "Have a very small margin in magnitude, say pizza by had disclosed 1/2.",
            "This means that this guy here is close to 0 in magnitude.",
            "This gives a 1/2.",
            "This means that the children are not very, you know, opinionated.",
            "Well, I don't know.",
            "Maybe.",
            "Maybe yes, maybe no.",
            "And this forces.",
            "The parent node to have a very positive threshold.",
            "Very positive threshold, meaning that that makes it harder for him to be one.",
            "OK, if the kids are not.",
            "Basically, they're not.",
            "They don't know what they're doing.",
            "The parent node says, well, maybe?",
            "It's better I stick to zero and.",
            "Stick to 0 by everything underneath OK?",
            "'cause I'm undecided as well.",
            "On the other hand, if.",
            "This margin here.",
            "Is either very positive or negative.",
            "This means that this piece of X is very either is either close to 0 or close to 1.",
            "Then the threshold will be easier with the closest zero.",
            "And.",
            "The parent node has the freedom to be, you know, to be labeled according to its own local room.",
            "This is a.",
            "The intuition for the VI of X is very of the subshell being very.",
            "Positive, right obvious intuition, like if one of the children thinks that it's a while, then obviously want Tobias J also to be aware of this.",
            "Says come be zero.",
            "I mean the I can't be one in J0 right?",
            "So right there doesn't seem to be a reason for the other way around.",
            "But yeah, but think about the children independent here, so.",
            "So the way I see it is that there's really an implication from below to above, right logical indication of child is 1, then parent must be one.",
            "Yes, that's why it only works one way, because station doesn't work the other way round.",
            "O and then implies nothing about that.",
            "Yeah, yeah, exactly thanks, not second words tends to imply that if the child is very sure of being zero, then that OJ is also the threshold is also reduces tend to make it counted more like that's the thing.",
            "Right, yeah, I don't know how to fix this.",
            "By the way.",
            "Yeah, so I don't have an answer to this.",
            "We thought about.",
            "If you be glad to hear suggestions for you, but that effect should be more pronounced if there are fewer children right then that tends to be true if there are only two and one of them already says 0 then.",
            "The relay on the other one.",
            "So yeah, if there are more children, if they're more children, basically the message that they're passing are affecting the magnet of this."
        ],
        [
            "Out here and yeah, that makes a difference.",
            "Because they are adding up the messages coming from children.",
            "Anymore.",
            "This one I skipped.",
            "And then what we did was to experimentally compare this bottom up."
        ],
        [
            "Scheme with the top down scheme, which was this H SCN which is basically the same training scheme you have an SVN sitting at each node and each SVN is fed with examples that are not filtered out by the parent node.",
            "But then what changes is the label assignment, namely the way we assign labels.",
            "Once we had trained.",
            "Things once we had trainees at the end, so it's top down instead of being bottom up.",
            "So each label each sorry node gets labeled according to the value of a linear threshold function, either if it's a route, so it has no parent, or if it is not a root and the parent node has been labeled one.",
            "Otherwise, guest labels you.",
            "OK, that's very reasonable.",
            "Think.",
            "And unfortunately, I should say this scheme here is independent of the cost coefficients of loss function.",
            "Which I don't know how to incorporate in this evaluation scheme.",
            "So if anyone has an idea?",
            "Glad to hear.",
            "Let me tell."
        ],
        [
            "About the experiments, we made experiments on four datasets.",
            "Basically two of them were real world.",
            "I subset of the writer corpus, loading one the first 100,000 documents.",
            "This resulted in a hierarchy of 100 nodes, 4 trees height 3.",
            "And there is this interesting parameter here.",
            "Just the the average number of paths for label recorder.",
            "That label is just a subtree, so it's union of paths.",
            "So the average number of paths per label here is 1.5, so it's a multi label and multipath labeling.",
            "Think we turned this into a?",
            "I3 by the way, it was a dog originally.",
            "Or maybe this one I don't remember.",
            "I don't want or this one it was.",
            "It was a dog and we turned into a tree.",
            "Yeah.",
            "But we could perhaps play with without this with the other one is the ultimate corpus, actually specific subtree on the ultimate corpus or medical abstracts.",
            "And while you see.",
            "55,000 documents, 94 nodes.",
            "The death was for the average number of paths for multi labels 1.53 and we took on the first case 5 order chunks.",
            "So we trained on one chunk and tested on the next chunk and we did it five times.",
            "On the second one, we took five random splits.",
            "And then all the results are basically averaged over the chunks over the random over the random splits or over the chunks.",
            "And then we also.",
            "Generated two synthetic datasets.",
            "Why did we do that?",
            "Well, it's because we we are not very happy of the results we got here so we.",
            "We try and find a better function with synthetic datasets.",
            "Turns out with this.",
            "OK, do synthetic datasets.",
            "40,000 examples, three completes or no trees.",
            "939 nodes height 2 an.",
            "We played around with.",
            "You know this little.",
            "Parameters generating things so we wanted to generate datasets having there.",
            "A different number of different numbers of paths per label.",
            "2.66 On the first one synthetic one and one point 28.",
            "For the second one.",
            "Again, we took four order chunks trained on one chunk and test on the next one.",
            "Sorry."
        ],
        [
            "Don't have lots.",
            "I only have numbers here.",
            "And what you can see from this?",
            "From these preliminary experiments is that basically.",
            "The bottom up approach.",
            "BSM is always beating is always beating the top down approach.",
            "Sometimes the difference between the two is really marginal.",
            "Here is really marginalis.",
            "Here is sort of significant on these two is really.",
            "Is very large.",
            "Are you using different now?",
            "These are age loss values here.",
            "CIS yeah OK, the CIS here are chosen to be one I think for for both in order.",
            "Yeah, because the compressor would be unfair otherwise.",
            "Innocence right?",
            "So we chose values, see eye to be one for the SVN.",
            "I don't think it makes a huge amount of difference.",
            "Yeah, yeah.",
            "Not yet.",
            "Yeah, I agree.",
            "I mean, I'm not claiming any anything.",
            "See I was able to one, so this is this a source.",
            "If you made this stupid Phoenix in this way.",
            "No, no sorry.",
            "OK, if the see if the Costco features are one.",
            "These are three mistakes, so the H loss is 3 here."
        ],
        [
            "So I mean, if you could tell me would create an interpretation that you don't have even the root nodes.",
            "Yeah, that's true, that's true, that's correct, that's correct.",
            "Yeah, but you know, yeah, of course the challenge is to get something less than one."
        ],
        [
            "How do we get there?",
            "We had more than one here.",
            "Yeah, I don't know why it will happen here.",
            "So this is larger than one.",
            "And this is larger one as well.",
            "Yeah, The thing is that I didn't.",
            "I didn't run the experiments myself this way.",
            "Let's see."
        ],
        [
            "OK, I don't remember exactly what happened, maybe.",
            "But this is just a guess.",
            "Maybe instead of setting the coefficients to one.",
            "We use this other scheme here."
        ],
        [
            "This is something like one.",
            "This is a one half 1/2.",
            "This is again 1/2.",
            "This is 161 over six 1 / 6 and 1 / 6.",
            "Yeah.",
            "Because they only charge for the yeah, yeah, right, yeah, right yeah, right yeah, right.",
            "I. Yeah, right, I don't remember.",
            "I had to, sorry.",
            "I had to check the paper.",
            "Or is it the forest?"
        ],
        [
            "Sorry.",
            "Singletree yeah, I mean you could add top top.",
            "That mean old?",
            "If you do not compensate for his being apart.",
            "Oh, I see yeah yeah yeah OK you save me.",
            "OK great OK yeah you have multiple trees so yeah multiple routes.",
            "Not meaningful unless we know that we want to have run a fair comparison, you know.",
            "And so basically we didn't cheat at all in this.",
            "You didn't want to cheat in some sense.",
            "And yeah, the difference between the real world data set is not is not as significant perhaps, so we try and see what happens chunk wise for each.",
            "Chunk"
        ],
        [
            "And.",
            "So the difference between.",
            "The performance of the two on each chunk is not very large.",
            "Sometimes it's ridiculous, you know, sometimes it's the same.",
            "Still, there is a definite statistical trend that sort of suggests that the B as the end at the bottom up approach is doing always.",
            "We didn't do any statistics over this, no.",
            "I mean I'm not claiming again this is a significant what we report we're reporting.",
            "Here is the result for each chunk.",
            "And I guess you can see that on all chunks.",
            "They are doing better by a small amount, of course, but they're doing better.",
            "Here doing I would say significantly better, But basically this one was the one that we were trying to understand this.",
            "Now this data set.",
            "The differences here are not really significant.",
            "I'm not claiming their significance.",
            "So on this data set, basically 2, the two approaches are are the same.",
            "On this one, they're not."
        ],
        [
            "And you know, one could argue about.",
            "And what happens?",
            "Level wise.",
            "I don't wanna go into this these numbers here, but basically one.",
            "What one can conclude is that.",
            "The two algorithms are sort of performing similarly at the roots."
        ],
        [
            "But at least nodes the bottom up approach tends to outperform the top down one.",
            "Maybe it's just because it's starting from the least, starting off from the leaves.",
            "But it's it's better out at lower levels.",
            "So they are similar at the roots, which are counting more, but the BSM is slightly better on.",
            "At the leaves.",
            "OK."
        ],
        [
            "And now we turn to some more theoretical work.",
            "One can argue I can.",
            "Can you use the logistic regression here as well?",
            "Yeah.",
            "Well, we have an analysis for a special case for special parametric model.",
            "Updates of these probabilities here.",
            "So basically what we do is to associate with each node.",
            "Parameter vector normalized to one and claim that the probability that the right node gets level one given.",
            "The value of the parent node is 1.",
            "Is obtained this way one plus user by times dot X / 2.",
            "So this is a probability value.",
            "OK, this is a parameter.",
            "So parametric models, simple parametric model, linear parametric model.",
            "And we did this Pearl notes.",
            "And once again, we want to enforce legal multi labels and so we have this condition as well.",
            "So."
        ],
        [
            "What we wanted to do was to try and learn.",
            "In an online protocol.",
            "I like good hierarchical predictor.",
            "And align protocol works as follows.",
            "You there's an algorithm here that receives at each time step an instance vector can.",
            "It is required to produce a prediction, which is a legal multi label.",
            "Then it receives that feedback and then you know it keeps iterating.",
            "This way it updates its internal state and so on.",
            "And we measure.",
            "The accuracy of this algorithm.",
            "Against the H loss function that I mentioned.",
            "But we do not measure DHL how much time do I have?",
            "Five 10510 we do not measure.",
            "The H loss per say, but we actually measure the age.",
            "Lots of the algorithm compared to the H loss of a given comparison predictor.",
            "So it's a regret analysis.",
            "Accumulative regret analysis.",
            "The comparison predictors that top down compared are that knows basically knows."
        ],
        [
            "The parameter is sitting at each node here, so it knows.",
            "Hopes."
        ],
        [
            "It knows the parametric model here."
        ],
        [
            "But it is not the biggest optimal classifier for this.",
            "Parametric model.",
            "So it's basically mimicking this HVN way of.",
            "Label label assignment.",
            "It's top down.",
            "Once again, node is labeled according to the value of the actual function.",
            "If it's a root or.",
            "It's the it's the child of a node that has been labeled one.",
            "It is not based off timmel, at least because it does not depend on the cost coefficients.",
            "So we are comparing really comparing our classifier to a classifier, which is not the base optimal one.",
            "Which we again do not know how to compute.",
            "OK, so the best we could do is this.",
            "And again we measure the cumulative regret the sum overall try overall examples of the expected of the risk.",
            "Basically the expected loss of the algorithm compared discounted by the expected loss of this top down compared to here.",
            "And since we wanted to compare to a top down comparator whereas natural way.",
            "To go is to use a top down predictor.",
            "Well, the top down predictor basically is doing.",
            "Local approximation to each parameter here.",
            "So it stores at each node weight vector which is meant to approximate the corresponding parameters."
        ],
        [
            "You survive.",
            "And it will stop down this in the same way.",
            "OK. And the update is basically similar to what H SVM was doing.",
            "There is a filtering rule here.",
            "And now an example is passed to a node.",
            "I only if the parent node was labeled one.",
            "On that example.",
            "OK, otherwise not best.",
            "And it's top down, and once again, independent.",
            "All the seats are by.",
            "Which we don't know how to incorporate.",
            "In the top down scheme.",
            "And.",
            "The algorithm."
        ],
        [
            "Is.",
            "Naturally.",
            "He is naturally derived from the parametric model because we have a basically a regularizer, square squares predictor sitting at each node, which is a.",
            "Nothing topically unbiased estimator.",
            "Of the parameter at each node.",
            "Well, I want to get into the details here.",
            "It naturally arises from the parametric model because this margin.",
            "This estimated margin is almost conditionally unbiased estimate of the true margin at each node and its accuracy actually depends on the number of examples that each node sees.",
            "Remember that root nodes see old labels while leaf nodes see.",
            "Only a small amount of labels.",
            "So we are very good at at root labels part.",
            "We are poor at at the leaf nodes and you know it can be, you know, running dual variables and all these things."
        ],
        [
            "And then we have a bound.",
            "Which actually.",
            "Makes no assumption, no assumptions on the way.",
            "The instance vectors are generated.",
            "So there are pointwise bounds.",
            "So this guy here is constant.",
            "This guy is a random variable generated according to the generative model blah blah blah.",
            "But The X Factor can be worst case as well.",
            "Sorry can be worst case worst case, but it has to be generated ahead of time, so we let an adversary generate the instance vectors here.",
            "Now we force the adversary too.",
            "To choose them ahead of time before knowing what the algorithm is doing.",
            "OK, so it's a lose adversary, so weak adversary.",
            "And then what we can prove?",
            "Is a regret bound or the following four?",
            "We had two three ingredients in this bound.",
            "One is Delta squared, the other one is this capital C sub.",
            "I and the third ingredient is this eigenvalue thing.",
            "Yeah, you have that.",
            "I inverse dependence on the minimal margin.",
            "Overall examples you have dependence on the contribution of.",
            "The cost coefficients sitting at each node and the subtree root underneath.",
            "And you also have a contribution due today.",
            "The eigen structure of the examples that each node.",
            "Seize after training.",
            "OK, and by the way, this is a so called logarithmic cumulative regret, so it's instantaneously.",
            "This is would be a fast rate of convergence.",
            "OK.",
            "The expectation there is over everything over order these over these at the end and the labels over these guys, right?",
            "So so on a particular run of the algorithm, obviously, yeah.",
            "Conclusions.",
            "A framework.",
            "I wouldn't say it's great, but it's a framework."
        ],
        [
            "Hierarchical classification.",
            "We've been trying to improve a baseline top down.",
            "Label assignment scheme, known as HSPN by Bottom up base optimal like algorithm.",
            "And I believe what we did was provide some sort of a modular approach.",
            "Instead of being principled, it more affecting machine learning practice.",
            "I mean, the PSTN thing is more like a practical thing.",
            "We could replace SVM, my other things as well.",
            "Logistic regression, of course, would be one of the candidates.",
            "Now we made some preliminary experiments with that love to have more experiments on this and they also show they know line, top down algorithm and its regret analysis.",
            "Based on our regularizer squared algorithm.",
            "OK, open questions.",
            "Got to kind of two kinds of open questions.",
            "Experimental."
        ],
        [
            "OK, there's a clear advantage of the bottom of the bottom up approach on synthetic datasets, and it's less clear on real world datasets.",
            "Which we are clearly more interested in.",
            "I don't know why exactly we do not know why that might be the case that the real world datasets are just noisy.",
            "Too noisy too to make this base like scheme to work well, or it might well be the case that since we are.",
            "We have this independence assumption.",
            "This cannot be.",
            "Wait to go.",
            "Once I fixed the value of this value.",
            "Once I said this to one, the value of these guys are independent random variables, which is not need not be a good assumption in hierarchy or classification.",
            "But it simplifies matters quite a lot.",
            "And of course, as Tom was mentioning, for instance, replace SVM by better algorithms that are more suitable for probability estimations, such as logistic regression as well.",
            "Thanks.",
            "On the theoretical side.",
            "Our regret analysis.",
            "Was not refering to the base optimal classifier.",
            "We are not comparing against the base optimal classifier for that particular model, which we do not know how to compute.",
            "Maybe a logistic model again?",
            "Will be a good.",
            "Candidate to try to.",
            "To compare to.",
            "Once we have an analysis for logistic model.",
            "And once again, even in the theoretical work, we would like to remove this independence assumption, which is perhaps not.",
            "Promising promising Ave thank you, I'm done.",
            "Please yes.",
            "So it seems that in your classification problem you want to compute the maximum posterior solution or product over the designers of the piece, right?",
            "You're talking about the BSM thing, doesn't matter, but it doesn't matter, OK?",
            "But that's kind of the goal to compute the maximum posteriori.",
            "Yeah, yeah, yeah, it's it's basically captured by this thing.",
            "Yes, please, thank you.",
            "So, so why would you want them to use kind of?",
            "Understanding algorithm.",
            "For that kind of standard algorithm for models.",
            "Yeah, this is a tree, but there is an equivalent of 50 to be automated.",
            "This in this it.",
            "If you do that, then also So what did you do?",
            "So why don't you do bother her and then go back again when she doing that's fine.",
            "Did you do that?",
            "No, I I I I I go bottom up and then top down only if this guy gets labeled 0.",
            "So you actually do.",
            "This is Dennis Pok\u00e9mon and this stuff down.",
            "It's because I want to know whether one is better than the other.",
            "What do you mean?",
            "So which one is equivalent to that area?",
            "Part I?",
            "I guess this one, the bottom up one I guess.",
            "Yeah, because you make a hard assignment, but you wouldn't do it.",
            "You would want to build a model of the joint probability and then calculate the marginals with having babies in the markers, not even the math assignment of the labels.",
            "Message.",
            "The last byte by message passing.",
            "We put exactly come out right right right.",
            "If you formulated it as effective raw and it.",
            "But do you agree that this is the definition of the Bayes optimal classifier?",
            "For this this line here?",
            "Is the one that minimizes the the conditional risk.",
            "If you agree with that, then you should agree with this algorithm as well.",
            "It's a hard assignment.",
            "OK. And then after that.",
            "You have this algorithm for polymer.",
            "Yeah, this is basically the bottom up.",
            "Let's say I'm inspired by this bottomup scheme to build an algorithm that is sort of.",
            "Combining.",
            "Probabilities.",
            "You know?",
            "Yeah."
        ],
        [
            "My problem is I want to combine what's happening at each local Y at each node in some way.",
            "In some way, either by combining them bottom up special or in the top down or whatever else, something.",
            "But I think that if you would apply.",
            "The algorithm is speed would go back and forth, and then you're done, and then you have a map.",
            "So I don't know why this is.",
            "I believe this is the Viterbi thing.",
            "OK, so why do you need something else?",
            "Because there's not, this seems to be only going by the way.",
            "Well, why do I need something else?",
            "'cause I, I'm not sure this is the best thing you can do.",
            "I know you're busy and but no, I'm just kidding, sorry.",
            "I'm not, I'm not being Bayesian here.",
            "You know 'cause I'm approximating.",
            "Everything here is approximated, so it's just a scheme that sort of inspires the way of combining.",
            "The local the local information here.",
            "So this would be the best thing you could do if you had this piece of pie available, OK. Yeah, and then yeah.",
            "And then, that's that's you're perfectly right then you're done.",
            "But these things are not available.",
            "We are approximating and in some weird way and still we are applying.",
            "We are still running this game over bottom, nothing, but we are not at all, you know.",
            "Anti that the combined thing would be would be a good thing to do.",
            "OK, is that?",
            "Because I mean if you would would look at it as kind of presuming estimation and then do inference right then.",
            "This part.",
            "That we had would be the same after estimation and then you will get some influence and influence.",
            "It's kind of.",
            "Then probably the thing that you do is saying that you have to do it.",
            "Yeah, but that's still do you think?",
            "Do you think this is a?",
            "A good way of estimating this, for instance, a good way of estimating this probabilities.",
            "Once I had this probabilities, then I'm in good shape, sure.",
            "Pay phone gives me a an approximation to it.",
            "Why should I wear?",
            "Should I should I play bottom up?",
            "I mean, there's no, there's no.",
            "It seems to me there's no guarantee that.",
            "That the overall thing would still work, so I think I think.",
            "Then if you do inference, you need to pass process box.",
            "It depends how you define the probability of a child.",
            "Being negative, given the parents negative or something happened.",
            "If you have the the probability that a child is negative given the parent was classified negative a 0 which is in this model, then you don't need to do to pass it OK, because that so there is this magic parameter.",
            "We tried doing it, getting the probabilities out and doing inference.",
            "Then you have this magic parameter which is OK sometimes where my my parents classified as negative.",
            "I actually want to change my child, flip the label of my child because it makes the whole thing more likely.",
            "Correct parent labels in some sense, but you have this magic parameter that you gotta pick out of hand and they work any better.",
            "Or once you pick.",
            "If you find this problem, yeah, yeah.",
            "No obvious way to center, but once you once you set in hindsight that there's a definite yes.",
            "Basically you can.",
            "It depends on how, but if you've got a really big tree and you made a mistake up there somewhere right then, if there's enough kind of probability, mass inference will flip.",
            "But if you you know that that's popped up right, yeah?",
            "If you, if you say the parents, if the parents may be classified as negative, then the child is negative, but with probability one.",
            "Right, yeah, you don't have to do that.",
            "It has become the dominant states.",
            "OK, then welcome up.",
            "Is this the equipment is equipment?",
            "Yeah, yeah, you just propagated.",
            "Going down never changes.",
            "Down a bit.",
            "You had to go down.",
            "Maybe it's a little bit, yeah.",
            "Because you have, yeah.",
            "Negative.",
            "Correct, if there were any children.",
            "So yes, yes yes yes yes absolutely yes.",
            "One thing is just one there.",
            "You do it once.",
            "Once you get it once.",
            "Any suggestions?",
            "I really would be other people.",
            "There are some people.",
            "Who are they?",
            "Who are they?",
            "OK. Yo, yo.",
            "Model that has a global method to try to do even more complicated.",
            "Glad to.",
            "Basically you have a complete solid model.",
            "Right, right, right, right.",
            "People more favorable about global methods rather than they say go local method like this won't work.",
            "Joint distribution for Yyyy end.",
            "Yeah, yeah.",
            "Well this bottom up thing that I showed you was a sort of a global method because because of the thresholds.",
            "Where is it?",
            "Use the global estimates in the training in the training, yeah?"
        ],
        [
            "They will not have the conditional.",
            "Yeah, this is a weak global method.",
            "There's somebody believe that's at least what about running time?",
            "Server running.",
            "So looks kind of strange to me is that you you need to build this.",
            "Different inference.",
            "Framework.",
            "So the inference and the training do not necessarily match, so the structure.",
            "That's right, that's right framework.",
            "Those captured as well?",
            "Yeah, yeah.",
            "Yeah, you build a logistic regression into it.",
            "You could rise it as one big graphical model.",
            "You could do joint inference.",
            "In that moment.",
            "I mean, I don't know if it makes any big difference in practice, but it would kind of unify this thing alright, but would it work in practice?",
            "I mean and would it be practical?",
            "You think you think it would be practical doing local message passing, right?",
            "And what about saying?",
            "What about training?",
            "Yeah also.",
            "Sing CRF.",
            "OK.",
            "I did some work on that now.",
            "Hierarchical.",
            "Sequences experimentally they do not come out to be honest.",
            "Base optimality is one advantage.",
            "Maybe one comment from my side.",
            "So basically we are also working.",
            "But we deal with this car is which are as if 4230 is bigger, so let's say.",
            "600,600 thousand million documents on every day up to.",
            "Yeah, very patient.",
            "You're very patient.",
            "Like this, otherwise it's not useful, so so this would be used for labeling search results or.",
            "Assigning.",
            "Labels the documents or so.",
            "Do things in different ways so that the force.",
            "Setting so.",
            "This would be the focus and tation about this.",
            "I didn't want to talk about this here but still.",
            "So how somehow we combine little bit of indexing methods so so the nature of data changes.",
            "Large, very large hierarchies.",
            "Usually you cannot infer anything from the top level.",
            "The balls, let's say 15 levels deep.",
            "Basically everything is lost.",
            "How, how deep are the hierarchies?",
            "78 So we cannot comment much on this transition because it's, let's say on the bottom.",
            "So this is then.",
            "$500,000, six, 100,000 classes.",
            "Somehow similar or connected but.",
            "Presenting the results.",
            "Usually this kind of.",
            "Already."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shall we start out from?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll be talking about hierarchical classification and basically.",
                    "label": 1
                },
                {
                    "sent": "I'll tell you about recent feature recent research that we've been doing that I've been doing with these two guys.",
                    "label": 0
                },
                {
                    "sent": "Nickel oceans of Yankee and.",
                    "label": 0
                },
                {
                    "sent": "Luca is anybody?",
                    "label": 0
                },
                {
                    "sent": "OK. Hierarchical classification is a very well studied research subject.",
                    "label": 0
                },
                {
                    "sent": "There have been many papers on hierarchical classifications.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is colder in Sami domain chain.",
                    "label": 0
                },
                {
                    "sent": "This wasn't Acnl, 97, Hoffman Szeremeta and all.",
                    "label": 0
                },
                {
                    "sent": "This is horosho at all.",
                    "label": 0
                },
                {
                    "sent": "I'm more recent stuff so there are many many models of hierarchy of classification.",
                    "label": 0
                },
                {
                    "sent": "There is no real.",
                    "label": 0
                },
                {
                    "sent": "I would say agreement of what the best model is and many models means many algorithms, many approaches and so on.",
                    "label": 0
                },
                {
                    "sent": "Top down approaches, bottom up approaches local global.",
                    "label": 0
                },
                {
                    "sent": "Online match pachinko allocation methods and so on there are quite a lot.",
                    "label": 0
                },
                {
                    "sent": "The outline of this talk is the following.",
                    "label": 0
                },
                {
                    "sent": "First of all, I will introduce my own hierarchical classification framework.",
                    "label": 1
                },
                {
                    "sent": "Then I'll be talking about bottom up algorithm explaining the moment, but bottom up is which we call BS VM.",
                    "label": 0
                },
                {
                    "sent": "It's a combination of.",
                    "label": 1
                },
                {
                    "sent": "Bayes optimal classifier for a given.",
                    "label": 0
                },
                {
                    "sent": "Modal generating the labels will see in a moment with SVM.",
                    "label": 0
                },
                {
                    "sent": "It's based optimal classifier with respect to, you know.",
                    "label": 1
                },
                {
                    "sent": "Model that generates subtrees as labels, 'cause here labels are subtrees.",
                    "label": 0
                },
                {
                    "sent": "And then I report some report on some experiments we've made comparing this PST EM algorithm, which is a bottom up and global approach to a baseline algorithm which is called HFM, which.",
                    "label": 0
                },
                {
                    "sent": "Was mentioned the previous talk as well on both artificial and reward medium size datasets, and this is basically experimental work.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk about some, you know.",
                    "label": 0
                },
                {
                    "sent": "More theoretical work.",
                    "label": 0
                },
                {
                    "sent": "It's an online.",
                    "label": 0
                },
                {
                    "sent": "It's a regression analysis of an online algorithm.",
                    "label": 0
                },
                {
                    "sent": "Talk about the model specific parametric model for the labels.",
                    "label": 0
                },
                {
                    "sent": "An algorithm derived from this model, and I regret analysis.",
                    "label": 0
                },
                {
                    "sent": "So it's a half and half.",
                    "label": 0
                },
                {
                    "sent": "Experimental theoretical.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the model.",
                    "label": 0
                },
                {
                    "sent": "In our case, hierarchy is given to us ahead of time.",
                    "label": 0
                },
                {
                    "sent": "You're given a hierarchy hierarchy, just a.",
                    "label": 0
                },
                {
                    "sent": "The taxonomy is just a tree forest, a bunch of trees.",
                    "label": 1
                },
                {
                    "sent": "You see?",
                    "label": 0
                },
                {
                    "sent": "Plant taxonomy on the left.",
                    "label": 0
                },
                {
                    "sent": "The same taxonomy is on the right label.",
                    "label": 0
                },
                {
                    "sent": "What we call a multilabel, actually it's just a way of.",
                    "label": 0
                },
                {
                    "sent": "Picking nodes within this taxonomy in some way that is consistent with that taxonomy.",
                    "label": 0
                },
                {
                    "sent": "Idea of structuring things and multi label is a legal multi label if it is a union of paths within the tree within the trees.",
                    "label": 1
                },
                {
                    "sent": "So for instance 1 two is a path 1 three is another path, six 810 is another path and so the multi label 12368 ten is a legal multi label OK. What is one thing that is upward closed?",
                    "label": 0
                },
                {
                    "sent": "Yes, basically it's Supper Club Union of Path, meaning that whenever you choose something here, everything on.",
                    "label": 0
                },
                {
                    "sent": "On the top should be to be chosen.",
                    "label": 0
                },
                {
                    "sent": "On the right hand side you see.",
                    "label": 0
                },
                {
                    "sent": "And illegal multi label not up or closed.",
                    "label": 0
                },
                {
                    "sent": "Basically if you choose this one, you have to choose this one as well.",
                    "label": 1
                },
                {
                    "sent": "And this is not a legal multilabel.",
                    "label": 0
                },
                {
                    "sent": "We can, of course, associated with each you know, multi label binary vectoring with the obvious meaning.",
                    "label": 0
                },
                {
                    "sent": "This one is chosen.",
                    "label": 0
                },
                {
                    "sent": "This one is chosen.",
                    "label": 0
                },
                {
                    "sent": "This one, this one is not chosen and so on, OK. An example.",
                    "label": 0
                },
                {
                    "sent": "Repair X is an instance vector for simplicity and the is the associated legal multilabel.",
                    "label": 0
                },
                {
                    "sent": "OK, one question.",
                    "label": 0
                },
                {
                    "sent": "I'm doing also with party so just with trees.",
                    "label": 0
                },
                {
                    "sent": "Moralistic case would be hiring dogs, for instance.",
                    "label": 0
                },
                {
                    "sent": "Yeah we could.",
                    "label": 0
                },
                {
                    "sent": "We could extend it to dogs, but this case I'm talking bout threes as well.",
                    "label": 0
                },
                {
                    "sent": "There are some subtleties around in theoretical side I think we're pretty confident that we could replace dogs.",
                    "label": 0
                },
                {
                    "sent": "So in the input X is a tree or what is the axis?",
                    "label": 0
                },
                {
                    "sent": "Whatever you like, you can kernelized this thing.",
                    "label": 0
                },
                {
                    "sent": "So X could be, you know, everything that is compatible with.",
                    "label": 0
                },
                {
                    "sent": "You know, discover the machinery you can think of X to be a bunch of real numbers.",
                    "label": 0
                },
                {
                    "sent": "So where is the tree structure here?",
                    "label": 0
                },
                {
                    "sent": "Is that restructure the output?",
                    "label": 0
                },
                {
                    "sent": "So I mean it may be mentioned in the document classifications, I will I will OK X is just a way of encoding and document.",
                    "label": 0
                },
                {
                    "sent": "OK, bag of words, something and document can be classified, not just being a single topic they talk about.",
                    "label": 0
                },
                {
                    "sent": "No soccer here, but it can talk about, you know.",
                    "label": 0
                },
                {
                    "sent": "What's what's about sports, sports and politics and soccer as well?",
                    "label": 0
                },
                {
                    "sent": "That really is part of the prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, it's given it's given.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we generate multi labels?",
                    "label": 0
                },
                {
                    "sent": "We are given an instance, say a document for instance.",
                    "label": 0
                },
                {
                    "sent": "And we associate with multi label random binary vector Capital B and this capital N is the number of topics, the number of nodes in the tree.",
                    "label": 0
                },
                {
                    "sent": "And we basically do the following.",
                    "label": 0
                },
                {
                    "sent": "The conditional distribution of G of V given X is the product of these probability factors.",
                    "label": 0
                },
                {
                    "sent": "Here what we do is to associate.",
                    "label": 0
                },
                {
                    "sent": "We basically build a simple generative model for the labels.",
                    "label": 0
                },
                {
                    "sent": "We ask the shade with each node in the hierarchy.",
                    "label": 0
                },
                {
                    "sent": "A conditional distribution piece of X is the probability is the distribution of.",
                    "label": 0
                },
                {
                    "sent": "Visa by the value associated with this node I given the value associated with the parent node.",
                    "label": 0
                },
                {
                    "sent": "And the instance BX.",
                    "label": 0
                },
                {
                    "sent": "OK, so for instance here.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You see this little hierarchy on the on the left.",
                    "label": 0
                },
                {
                    "sent": "Basically do.",
                    "label": 0
                },
                {
                    "sent": "The distribution of V1V5.",
                    "label": 0
                },
                {
                    "sent": "Since we have only five nodes, even X is given by.",
                    "label": 0
                },
                {
                    "sent": "Being one of X, let me write it.",
                    "label": 0
                },
                {
                    "sent": "So the probability of.",
                    "label": 0
                },
                {
                    "sent": "The one in X times the probability of B2 given the parent that parent node.",
                    "label": 0
                },
                {
                    "sent": "So V2 given V1 and then X times the probability of V3 even the 1X times the probability.",
                    "label": 0
                },
                {
                    "sent": "Before even the parent node which is 3V3 and X times probability V5, given node V3, which is still the parent node and X.",
                    "label": 0
                },
                {
                    "sent": "So we have this.",
                    "label": 0
                },
                {
                    "sent": "We had this number so we have this distributions here.",
                    "label": 0
                },
                {
                    "sent": "Associated with each node.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This implicit is saying that.",
                    "label": 0
                },
                {
                    "sent": "Whenever we are given.",
                    "label": 0
                },
                {
                    "sent": "The value.",
                    "label": 0
                },
                {
                    "sent": "On the parent node, or even no apparent node, say we are given the value of three.",
                    "label": 0
                },
                {
                    "sent": "Here then.",
                    "label": 0
                },
                {
                    "sent": "By given X means the probability is all of them are one.",
                    "label": 0
                },
                {
                    "sent": "Not really, not really, not yet.",
                    "label": 0
                },
                {
                    "sent": "The probability that V1 we fight take on some value which is either 01.",
                    "label": 0
                },
                {
                    "sent": "I'll fix this problem in the bottom line here.",
                    "label": 0
                },
                {
                    "sent": "And I think I think you got that.",
                    "label": 0
                },
                {
                    "sent": "If we fix the value of three.",
                    "label": 0
                },
                {
                    "sent": "Then conditioned on the value of three being something, say one, then these two guys.",
                    "label": 0
                },
                {
                    "sent": "The value of these two children are independent variable.",
                    "label": 0
                },
                {
                    "sent": "OK, this would not be a good assumption for hierarchies.",
                    "label": 0
                },
                {
                    "sent": "But recall that here label might be a subtree as well, so if we choose this node, we might well choose this one as well.",
                    "label": 0
                },
                {
                    "sent": "So they're not, say, mutually exclusive.",
                    "label": 0
                },
                {
                    "sent": "We might enforce some, you know, negative correlation among children, but this we didn't do.",
                    "label": 0
                },
                {
                    "sent": "It's an open question, let's say.",
                    "label": 0
                },
                {
                    "sent": "And of course, as she was sort of mentioning, we want to generate with this model illegal multi label in the sense of the previous previous slide.",
                    "label": 0
                },
                {
                    "sent": "So we want to sort of enforce that if this guy is not chosen is this guy gets labeled zero, then everything underneath gets labeled zero as well, right?",
                    "label": 0
                },
                {
                    "sent": "If this guy is 1, then these two are independent, can be.",
                    "label": 0
                },
                {
                    "sent": "I think value 01 independently.",
                    "label": 0
                },
                {
                    "sent": "If this is zero, then these two guys are zero for sure.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is basically about this blog online thing.",
                    "label": 0
                },
                {
                    "sent": "For every possible instance vector.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Loss function.",
                    "label": 0
                },
                {
                    "sent": "So we have a way of generating labels.",
                    "label": 0
                },
                {
                    "sent": "How do we measure?",
                    "label": 0
                },
                {
                    "sent": "The accuracy of our algorithms.",
                    "label": 0
                },
                {
                    "sent": "We had this hierarchical loss function H loss.",
                    "label": 1
                },
                {
                    "sent": "And this is it works as follows.",
                    "label": 0
                },
                {
                    "sent": "Basically you had to want to compare two labelings to legal labelings to legal multi labels.",
                    "label": 0
                },
                {
                    "sent": "You have a blue prediction and the red Label two label so.",
                    "label": 0
                },
                {
                    "sent": "In comparing these two is sort of comparing the discrepancy between the two.",
                    "label": 0
                },
                {
                    "sent": "What you have to do this is loss function.",
                    "label": 0
                },
                {
                    "sent": "See, just consider as you go downwards the tree.",
                    "label": 0
                },
                {
                    "sent": "You just consider the node where you first encounter a mistake.",
                    "label": 0
                },
                {
                    "sent": "So if I go downwards here from 124, this is a mistake, and now because it's not taken here while it is taken here, this is a mistake and node and.",
                    "label": 0
                },
                {
                    "sent": "Everything underneath this node.",
                    "label": 0
                },
                {
                    "sent": "This mistake in node is irrelevant.",
                    "label": 0
                },
                {
                    "sent": "OK, this is counted as a mistake and it is weighted accordingly.",
                    "label": 0
                },
                {
                    "sent": "But then all later mistakes underneath the subtree rooted at this, this node four are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "For instance, 8 is a mistaken node as well, because it is taking here is not taking here.",
                    "label": 0
                },
                {
                    "sent": "But this does not count as a mistake OK. Is symmetric is false positive and negative?",
                    "label": 0
                },
                {
                    "sent": "What do you mean by symmetric?",
                    "label": 0
                },
                {
                    "sent": "The same loss if you do not label the node, it was.",
                    "label": 0
                },
                {
                    "sent": "Should have been labeled or if you label and know that should not have been.",
                    "label": 0
                },
                {
                    "sent": "I so yeah, from this point of view, yeah, this is symmetric from this point of view, this symmetric.",
                    "label": 0
                },
                {
                    "sent": "In other words, if this is not taken, this is taken, yet it will be the same.",
                    "label": 0
                },
                {
                    "sent": "Another example is.",
                    "label": 0
                },
                {
                    "sent": "Madden is another.",
                    "label": 0
                },
                {
                    "sent": "Mistaken note.",
                    "label": 0
                },
                {
                    "sent": "You say that you only count the first node.",
                    "label": 0
                },
                {
                    "sent": "If it's a mistake in the other direction, then it's most severe.",
                    "label": 0
                },
                {
                    "sent": "If you keep making ones download.",
                    "label": 0
                },
                {
                    "sent": "It is not implied by being the one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we.",
                    "label": 0
                },
                {
                    "sent": "I think we could.",
                    "label": 0
                },
                {
                    "sent": "We could wait the two.",
                    "label": 0
                },
                {
                    "sent": "Yeah we could wait two and a different way but.",
                    "label": 0
                },
                {
                    "sent": "If you add the lights full then you are forced to have zeros eight and nine absolute, but here's one is full.",
                    "label": 0
                },
                {
                    "sent": "Then there are two different cases.",
                    "label": 0
                },
                {
                    "sent": "One is flowing zeros and 89 or one is full, obviously, but then they count them as the same mistake and it is.",
                    "label": 0
                },
                {
                    "sent": "The cost is the same.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we.",
                    "label": 0
                },
                {
                    "sent": "I think we could.",
                    "label": 0
                },
                {
                    "sent": "We could extend it.",
                    "label": 0
                },
                {
                    "sent": "I think we could extend it, by the way.",
                    "label": 0
                },
                {
                    "sent": "OK, seven mistaken.",
                    "label": 0
                },
                {
                    "sent": "These guys do not count are not taking into consideration and you also might want to.",
                    "label": 0
                },
                {
                    "sent": "You know, put weights on the.",
                    "label": 0
                },
                {
                    "sent": "The nodes.",
                    "label": 0
                },
                {
                    "sent": "Cost coefficients in a node.",
                    "label": 0
                },
                {
                    "sent": "Two, you know.",
                    "label": 0
                },
                {
                    "sent": "To account for the relative importance of the mistakes that you are making.",
                    "label": 0
                },
                {
                    "sent": "So if you're making mistakes at the roots are more severe mistakes than if you made mistakes, at least something like this.",
                    "label": 0
                },
                {
                    "sent": "What people do I mean?",
                    "label": 0
                },
                {
                    "sent": "In the standard age loss, they the weights are higher at the roots then at the.",
                    "label": 0
                },
                {
                    "sent": "Well, you mean the standard HL?",
                    "label": 0
                },
                {
                    "sent": "No standard.",
                    "label": 0
                },
                {
                    "sent": "Case of Yankee.",
                    "label": 0
                },
                {
                    "sent": "We had OK. We had to ski.",
                    "label": 0
                },
                {
                    "sent": "Friends of the table.",
                    "label": 0
                },
                {
                    "sent": "OK, we have two schemes here.",
                    "label": 0
                },
                {
                    "sent": "Either we chose this this coefficient to be one.",
                    "label": 0
                },
                {
                    "sent": "Everything is 1 or we choose.",
                    "label": 0
                },
                {
                    "sent": "Say we give one costs one to this, we've cost 1/2 to this and we give cost 1 third, 116 in a way that the sum of these three guys is exactly the same as.",
                    "label": 0
                },
                {
                    "sent": "But how many you know other alternative schemes?",
                    "label": 0
                },
                {
                    "sent": "So each node has just one parent.",
                    "label": 0
                },
                {
                    "sent": "In this talk, yes.",
                    "label": 0
                },
                {
                    "sent": "You could you could generalize, and of course everything actually goes through with, you know with the generative model as simple as saying.",
                    "label": 0
                },
                {
                    "sent": "Would have to make different choices.",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably probably.",
                    "label": 0
                },
                {
                    "sent": "2 pounds.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have the you know this simple generative model for the labels and we have loss.",
                    "label": 0
                },
                {
                    "sent": "We can define Bayes optimal classifier for this.",
                    "label": 1
                },
                {
                    "sent": "Which is obviously, you know, the one that the labeling that minimizes the expected loss given given X being the.",
                    "label": 0
                },
                {
                    "sent": "The inspector and it turns out that it can be computed very easily as a standard bottom up message passing algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it was like this.",
                    "label": 0
                },
                {
                    "sent": "I'd like to recall it because then our algorithm is based on this scheme, so we are given this conditional distributions associated with nodes, which I call pizza by.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "The one piece of one piece of two and so on piece of three, and so on.",
                    "label": 0
                },
                {
                    "sent": "The deep these are all for the P1 is equal to water heaters before right?",
                    "label": 0
                },
                {
                    "sent": "He said again, please.",
                    "label": 0
                },
                {
                    "sent": "This is only for if we want equal to 1.",
                    "label": 0
                },
                {
                    "sent": "Well, at this stage this takes, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Also this guy will generate legal multi labels anyway, so this definition of P1 X.",
                    "label": 0
                },
                {
                    "sent": "Well this holds in general, but then you had to choose whether you want to be 0 here or not.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean pizza by of X can be either.",
                    "label": 0
                },
                {
                    "sent": "Sandy traditional can be 0 depending on whether the parent was zero, not.",
                    "label": 0
                },
                {
                    "sent": "But for Fortune, is P1 of X is the probability that the one is 1?",
                    "label": 0
                },
                {
                    "sent": "So yeah, OK, OK when I got there.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's probably that this guy condition on this B1.",
                    "label": 0
                },
                {
                    "sent": "That's what you go yeah, yeah, yeah yeah yeah OK. And it works like this.",
                    "label": 0
                },
                {
                    "sent": "We're given these probabilities basically, and leaves get labeled as follows.",
                    "label": 0
                },
                {
                    "sent": "Pizza by say pizza at 7.",
                    "label": 0
                },
                {
                    "sent": "Is larger than 1/2, then it gets labeled 1, otherwise yes, label OK.",
                    "label": 0
                },
                {
                    "sent": "So for instance, this is less than 1/2 and so they get gas level G. Or this is large and 1/2 gets a little one.",
                    "label": 0
                },
                {
                    "sent": "And then we build messages.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on these values, so the message is passed upwards.",
                    "label": 0
                },
                {
                    "sent": "Is either 1 -- P seven.",
                    "label": 0
                },
                {
                    "sent": "If this guy is labeled one or P7, if this guy gets labeled zero, it was labeled one, so it's 1 -- P seven.",
                    "label": 0
                },
                {
                    "sent": "So 1 -- P Seven is passed upwards.",
                    "label": 0
                },
                {
                    "sent": "This guy was labeled 0 so P8 is is the message minus B9 vehicles.",
                    "label": 0
                },
                {
                    "sent": "Level one and these three messages are collected by the node six which computes the sum Sigma.",
                    "label": 0
                },
                {
                    "sent": "OK, by the way, this is assuming that the cost coefficients are one or old one.",
                    "label": 0
                },
                {
                    "sent": "If we have cost coefficients, we had to multiply each message by the cost coefficient associated with each node.",
                    "label": 0
                },
                {
                    "sent": "OK, these things.",
                    "label": 0
                },
                {
                    "sent": "OK, these messages are collected by 6.",
                    "label": 0
                },
                {
                    "sent": "And six computes its own label.",
                    "label": 0
                },
                {
                    "sent": "According to this rule.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is if its corresponding piece by piece of six is larger than 1 / 2?",
                    "label": 0
                },
                {
                    "sent": "Minus Sigma and it gets labeled 1 otherwise gets labeled zero Sigma.",
                    "label": 0
                },
                {
                    "sent": "I recall the Sigma was this some of the messages that.",
                    "label": 0
                },
                {
                    "sent": "Wet flowing upwards.",
                    "label": 0
                },
                {
                    "sent": "OK. And then P6.",
                    "label": 0
                },
                {
                    "sent": "Second phase there.",
                    "label": 0
                },
                {
                    "sent": "Then you go back down, correct things below and or.",
                    "label": 0
                },
                {
                    "sent": "If I mean, let's say that was labeled zero, what would happen to the one here?",
                    "label": 0
                },
                {
                    "sent": "Yeah, then everything gets labeled 0, so you have to go back and correct the certain amount that one.",
                    "label": 0
                },
                {
                    "sent": "Pretty much, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I forgot to mention this line.",
                    "label": 0
                },
                {
                    "sent": "If this guy gets labeled zero by some sheer accident, then everything is underneath.",
                    "label": 0
                },
                {
                    "sent": "Yes label sealer.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you know this message gets passed upwards and so on.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's this algorithm be SVN?",
                    "label": 0
                },
                {
                    "sent": "So BSM works as follows.",
                    "label": 0
                },
                {
                    "sent": "We have an SVN sitting at each node.",
                    "label": 1
                },
                {
                    "sent": "On the hierarchy and SCM keana.",
                    "label": 0
                },
                {
                    "sent": "See em here.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "Why SVN?",
                    "label": 0
                },
                {
                    "sent": "There's nothing special in this game, by the way.",
                    "label": 0
                },
                {
                    "sent": "She could be pretty much everything at this point.",
                    "label": 0
                },
                {
                    "sent": "Any say any linear classifier will work?",
                    "label": 0
                },
                {
                    "sent": "Anyway, we have an SVN and each SVN is delivering a weight vector including, say, biased or something.",
                    "label": 0
                },
                {
                    "sent": "Does this work?",
                    "label": 0
                },
                {
                    "sent": "We feed each SCN with a subset of the training set.",
                    "label": 0
                },
                {
                    "sent": "So each node filters out examples for its kids.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "So this guy here gets trained only with those examples.",
                    "label": 0
                },
                {
                    "sent": "That the parent node G. Has labeled one OK.",
                    "label": 0
                },
                {
                    "sent": "It just says 0, so you don't even pass this downward.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically the root nodes.",
                    "label": 0
                },
                {
                    "sent": "Our training with all labels the least.",
                    "label": 0
                },
                {
                    "sent": "Our trade with a small subset of labels.",
                    "label": 0
                },
                {
                    "sent": "They are less important so.",
                    "label": 0
                },
                {
                    "sent": "And then we associate.",
                    "label": 0
                },
                {
                    "sent": "Sorry, then we approximate these pizza by of X, which of course are unavailable with the outcome of the SVM at each node by fitting a sigmoid using the so-called Platt method.",
                    "label": 0
                },
                {
                    "sent": "You know we have this sigmoid here.",
                    "label": 0
                },
                {
                    "sent": "This is the weight vector produced by the SVM.",
                    "label": 1
                },
                {
                    "sent": "And yeah, there's a way of producing probabilities out of SVN, and we have to fit parameters here and we do it through cross validation on the training set.",
                    "label": 0
                },
                {
                    "sent": "Just a useful just question.",
                    "label": 0
                },
                {
                    "sent": "Why you wanting sweater?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, we're playing tourism now for now.",
                    "label": 0
                },
                {
                    "sent": "We don't have any.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, yeah yeah, we're planning to do so actually.",
                    "label": 0
                },
                {
                    "sent": "Why I do or why I don't want to do that instead of just using logistic?",
                    "label": 0
                },
                {
                    "sent": "No disable software, just our first attempt to do that.",
                    "label": 0
                },
                {
                    "sent": "We didn't do logistic regression.",
                    "label": 0
                },
                {
                    "sent": "This this won't well.",
                    "label": 0
                },
                {
                    "sent": "It's difficult as well.",
                    "label": 0
                },
                {
                    "sent": "You are really doing theoretical work.",
                    "label": 0
                },
                {
                    "sent": "You know this is not theoretical, come on.",
                    "label": 0
                },
                {
                    "sent": "You want to save his consistent other stuff that you do this way.",
                    "label": 0
                },
                {
                    "sent": "Don't be consistent.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I know.",
                    "label": 0
                },
                {
                    "sent": "I know this is not.",
                    "label": 0
                },
                {
                    "sent": "I'm not playing this theoretical role.",
                    "label": 0
                },
                {
                    "sent": "But I am really concerned about running time here, so it may be logistic regression, but maybe you know more than me on this.",
                    "label": 0
                },
                {
                    "sent": "Additional cost really.",
                    "label": 0
                },
                {
                    "sent": "I'm planning to do that.",
                    "label": 0
                },
                {
                    "sent": "Thing is that the student was playing was supposed to do that just left so.",
                    "label": 0
                },
                {
                    "sent": "Please classify all those stuff they want to do this.",
                    "label": 0
                },
                {
                    "sent": "It won't get you there.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, he's just a way of, you know, sort of making things have principles in a way, that's all.",
                    "label": 0
                },
                {
                    "sent": "Amazon.",
                    "label": 0
                },
                {
                    "sent": "Only me, but others are that well yeah, there it says the Holy Church.",
                    "label": 0
                },
                {
                    "sent": "Ilisy involve is that essentially if they do this way, when the computer, the probability it would be consistent.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "And then once we fit this invoice.",
                    "label": 0
                },
                {
                    "sent": "We play this bottom up game.",
                    "label": 0
                },
                {
                    "sent": "We had these things instead of that.",
                    "label": 0
                },
                {
                    "sent": "Rupee isibaya we propagate upwards.",
                    "label": 0
                },
                {
                    "sent": "So once you.",
                    "label": 0
                },
                {
                    "sent": "Once you start thinking of how these things work, you realize that what this algorithm is actually doing is is trying to infer some good thresholds for SVN's at each node.",
                    "label": 0
                },
                {
                    "sent": "So basically this boils down to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using this bottom up scheme boils down to using SVM with modified thresholds at each node.",
                    "label": 1
                },
                {
                    "sent": "Well, the threshold at node J does actually depend on the behavior of children underneath.",
                    "label": 0
                },
                {
                    "sent": "So your claim is that our Jays, independent of the particular example, or Kouji, is inferred from the from the training set.",
                    "label": 0
                },
                {
                    "sent": "So we aren't evaluation phase now.",
                    "label": 0
                },
                {
                    "sent": "OK question is how can you determine before you see the test example.",
                    "label": 0
                },
                {
                    "sent": "No, you have to see the test example.",
                    "label": 0
                },
                {
                    "sent": "You had to see what the children, how the children work on the test example.",
                    "label": 0
                },
                {
                    "sent": "So it said they do actually depend on what the children are doing.",
                    "label": 0
                },
                {
                    "sent": "Basically, it works as follows.",
                    "label": 0
                },
                {
                    "sent": "If the children here.",
                    "label": 0
                },
                {
                    "sent": "Have a very small margin in magnitude, say pizza by had disclosed 1/2.",
                    "label": 0
                },
                {
                    "sent": "This means that this guy here is close to 0 in magnitude.",
                    "label": 0
                },
                {
                    "sent": "This gives a 1/2.",
                    "label": 0
                },
                {
                    "sent": "This means that the children are not very, you know, opinionated.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe yes, maybe no.",
                    "label": 0
                },
                {
                    "sent": "And this forces.",
                    "label": 0
                },
                {
                    "sent": "The parent node to have a very positive threshold.",
                    "label": 0
                },
                {
                    "sent": "Very positive threshold, meaning that that makes it harder for him to be one.",
                    "label": 0
                },
                {
                    "sent": "OK, if the kids are not.",
                    "label": 0
                },
                {
                    "sent": "Basically, they're not.",
                    "label": 0
                },
                {
                    "sent": "They don't know what they're doing.",
                    "label": 0
                },
                {
                    "sent": "The parent node says, well, maybe?",
                    "label": 0
                },
                {
                    "sent": "It's better I stick to zero and.",
                    "label": 0
                },
                {
                    "sent": "Stick to 0 by everything underneath OK?",
                    "label": 0
                },
                {
                    "sent": "'cause I'm undecided as well.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if.",
                    "label": 0
                },
                {
                    "sent": "This margin here.",
                    "label": 0
                },
                {
                    "sent": "Is either very positive or negative.",
                    "label": 0
                },
                {
                    "sent": "This means that this piece of X is very either is either close to 0 or close to 1.",
                    "label": 0
                },
                {
                    "sent": "Then the threshold will be easier with the closest zero.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The parent node has the freedom to be, you know, to be labeled according to its own local room.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "The intuition for the VI of X is very of the subshell being very.",
                    "label": 0
                },
                {
                    "sent": "Positive, right obvious intuition, like if one of the children thinks that it's a while, then obviously want Tobias J also to be aware of this.",
                    "label": 0
                },
                {
                    "sent": "Says come be zero.",
                    "label": 0
                },
                {
                    "sent": "I mean the I can't be one in J0 right?",
                    "label": 0
                },
                {
                    "sent": "So right there doesn't seem to be a reason for the other way around.",
                    "label": 0
                },
                {
                    "sent": "But yeah, but think about the children independent here, so.",
                    "label": 0
                },
                {
                    "sent": "So the way I see it is that there's really an implication from below to above, right logical indication of child is 1, then parent must be one.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's why it only works one way, because station doesn't work the other way round.",
                    "label": 0
                },
                {
                    "sent": "O and then implies nothing about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, exactly thanks, not second words tends to imply that if the child is very sure of being zero, then that OJ is also the threshold is also reduces tend to make it counted more like that's the thing.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, I don't know how to fix this.",
                    "label": 0
                },
                {
                    "sent": "By the way.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I don't have an answer to this.",
                    "label": 0
                },
                {
                    "sent": "We thought about.",
                    "label": 0
                },
                {
                    "sent": "If you be glad to hear suggestions for you, but that effect should be more pronounced if there are fewer children right then that tends to be true if there are only two and one of them already says 0 then.",
                    "label": 0
                },
                {
                    "sent": "The relay on the other one.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if there are more children, if they're more children, basically the message that they're passing are affecting the magnet of this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out here and yeah, that makes a difference.",
                    "label": 0
                },
                {
                    "sent": "Because they are adding up the messages coming from children.",
                    "label": 0
                },
                {
                    "sent": "Anymore.",
                    "label": 0
                },
                {
                    "sent": "This one I skipped.",
                    "label": 0
                },
                {
                    "sent": "And then what we did was to experimentally compare this bottom up.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scheme with the top down scheme, which was this H SCN which is basically the same training scheme you have an SVN sitting at each node and each SVN is fed with examples that are not filtered out by the parent node.",
                    "label": 1
                },
                {
                    "sent": "But then what changes is the label assignment, namely the way we assign labels.",
                    "label": 0
                },
                {
                    "sent": "Once we had trained.",
                    "label": 0
                },
                {
                    "sent": "Things once we had trainees at the end, so it's top down instead of being bottom up.",
                    "label": 0
                },
                {
                    "sent": "So each label each sorry node gets labeled according to the value of a linear threshold function, either if it's a route, so it has no parent, or if it is not a root and the parent node has been labeled one.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, guest labels you.",
                    "label": 0
                },
                {
                    "sent": "OK, that's very reasonable.",
                    "label": 1
                },
                {
                    "sent": "Think.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately, I should say this scheme here is independent of the cost coefficients of loss function.",
                    "label": 0
                },
                {
                    "sent": "Which I don't know how to incorporate in this evaluation scheme.",
                    "label": 0
                },
                {
                    "sent": "So if anyone has an idea?",
                    "label": 0
                },
                {
                    "sent": "Glad to hear.",
                    "label": 0
                },
                {
                    "sent": "Let me tell.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the experiments, we made experiments on four datasets.",
                    "label": 0
                },
                {
                    "sent": "Basically two of them were real world.",
                    "label": 0
                },
                {
                    "sent": "I subset of the writer corpus, loading one the first 100,000 documents.",
                    "label": 0
                },
                {
                    "sent": "This resulted in a hierarchy of 100 nodes, 4 trees height 3.",
                    "label": 1
                },
                {
                    "sent": "And there is this interesting parameter here.",
                    "label": 0
                },
                {
                    "sent": "Just the the average number of paths for label recorder.",
                    "label": 0
                },
                {
                    "sent": "That label is just a subtree, so it's union of paths.",
                    "label": 0
                },
                {
                    "sent": "So the average number of paths per label here is 1.5, so it's a multi label and multipath labeling.",
                    "label": 0
                },
                {
                    "sent": "Think we turned this into a?",
                    "label": 0
                },
                {
                    "sent": "I3 by the way, it was a dog originally.",
                    "label": 0
                },
                {
                    "sent": "Or maybe this one I don't remember.",
                    "label": 0
                },
                {
                    "sent": "I don't want or this one it was.",
                    "label": 0
                },
                {
                    "sent": "It was a dog and we turned into a tree.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But we could perhaps play with without this with the other one is the ultimate corpus, actually specific subtree on the ultimate corpus or medical abstracts.",
                    "label": 0
                },
                {
                    "sent": "And while you see.",
                    "label": 0
                },
                {
                    "sent": "55,000 documents, 94 nodes.",
                    "label": 0
                },
                {
                    "sent": "The death was for the average number of paths for multi labels 1.53 and we took on the first case 5 order chunks.",
                    "label": 0
                },
                {
                    "sent": "So we trained on one chunk and tested on the next chunk and we did it five times.",
                    "label": 1
                },
                {
                    "sent": "On the second one, we took five random splits.",
                    "label": 0
                },
                {
                    "sent": "And then all the results are basically averaged over the chunks over the random over the random splits or over the chunks.",
                    "label": 0
                },
                {
                    "sent": "And then we also.",
                    "label": 0
                },
                {
                    "sent": "Generated two synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "Why did we do that?",
                    "label": 1
                },
                {
                    "sent": "Well, it's because we we are not very happy of the results we got here so we.",
                    "label": 0
                },
                {
                    "sent": "We try and find a better function with synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "Turns out with this.",
                    "label": 0
                },
                {
                    "sent": "OK, do synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "40,000 examples, three completes or no trees.",
                    "label": 1
                },
                {
                    "sent": "939 nodes height 2 an.",
                    "label": 0
                },
                {
                    "sent": "We played around with.",
                    "label": 0
                },
                {
                    "sent": "You know this little.",
                    "label": 0
                },
                {
                    "sent": "Parameters generating things so we wanted to generate datasets having there.",
                    "label": 0
                },
                {
                    "sent": "A different number of different numbers of paths per label.",
                    "label": 0
                },
                {
                    "sent": "2.66 On the first one synthetic one and one point 28.",
                    "label": 0
                },
                {
                    "sent": "For the second one.",
                    "label": 0
                },
                {
                    "sent": "Again, we took four order chunks trained on one chunk and test on the next one.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't have lots.",
                    "label": 0
                },
                {
                    "sent": "I only have numbers here.",
                    "label": 0
                },
                {
                    "sent": "And what you can see from this?",
                    "label": 0
                },
                {
                    "sent": "From these preliminary experiments is that basically.",
                    "label": 0
                },
                {
                    "sent": "The bottom up approach.",
                    "label": 0
                },
                {
                    "sent": "BSM is always beating is always beating the top down approach.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the difference between the two is really marginal.",
                    "label": 0
                },
                {
                    "sent": "Here is really marginalis.",
                    "label": 0
                },
                {
                    "sent": "Here is sort of significant on these two is really.",
                    "label": 0
                },
                {
                    "sent": "Is very large.",
                    "label": 0
                },
                {
                    "sent": "Are you using different now?",
                    "label": 0
                },
                {
                    "sent": "These are age loss values here.",
                    "label": 0
                },
                {
                    "sent": "CIS yeah OK, the CIS here are chosen to be one I think for for both in order.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because the compressor would be unfair otherwise.",
                    "label": 0
                },
                {
                    "sent": "Innocence right?",
                    "label": 0
                },
                {
                    "sent": "So we chose values, see eye to be one for the SVN.",
                    "label": 0
                },
                {
                    "sent": "I don't think it makes a huge amount of difference.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Not yet.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm not claiming any anything.",
                    "label": 0
                },
                {
                    "sent": "See I was able to one, so this is this a source.",
                    "label": 0
                },
                {
                    "sent": "If you made this stupid Phoenix in this way.",
                    "label": 0
                },
                {
                    "sent": "No, no sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, if the see if the Costco features are one.",
                    "label": 0
                },
                {
                    "sent": "These are three mistakes, so the H loss is 3 here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mean, if you could tell me would create an interpretation that you don't have even the root nodes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true, that's true, that's correct, that's correct.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you know, yeah, of course the challenge is to get something less than one.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we get there?",
                    "label": 0
                },
                {
                    "sent": "We had more than one here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know why it will happen here.",
                    "label": 0
                },
                {
                    "sent": "So this is larger than one.",
                    "label": 0
                },
                {
                    "sent": "And this is larger one as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, The thing is that I didn't.",
                    "label": 0
                },
                {
                    "sent": "I didn't run the experiments myself this way.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I don't remember exactly what happened, maybe.",
                    "label": 0
                },
                {
                    "sent": "But this is just a guess.",
                    "label": 0
                },
                {
                    "sent": "Maybe instead of setting the coefficients to one.",
                    "label": 0
                },
                {
                    "sent": "We use this other scheme here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is something like one.",
                    "label": 0
                },
                {
                    "sent": "This is a one half 1/2.",
                    "label": 0
                },
                {
                    "sent": "This is again 1/2.",
                    "label": 0
                },
                {
                    "sent": "This is 161 over six 1 / 6 and 1 / 6.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Because they only charge for the yeah, yeah, right, yeah, right yeah, right yeah, right.",
                    "label": 0
                },
                {
                    "sent": "I. Yeah, right, I don't remember.",
                    "label": 0
                },
                {
                    "sent": "I had to, sorry.",
                    "label": 0
                },
                {
                    "sent": "I had to check the paper.",
                    "label": 0
                },
                {
                    "sent": "Or is it the forest?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Singletree yeah, I mean you could add top top.",
                    "label": 0
                },
                {
                    "sent": "That mean old?",
                    "label": 0
                },
                {
                    "sent": "If you do not compensate for his being apart.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see yeah yeah yeah OK you save me.",
                    "label": 0
                },
                {
                    "sent": "OK great OK yeah you have multiple trees so yeah multiple routes.",
                    "label": 0
                },
                {
                    "sent": "Not meaningful unless we know that we want to have run a fair comparison, you know.",
                    "label": 0
                },
                {
                    "sent": "And so basically we didn't cheat at all in this.",
                    "label": 0
                },
                {
                    "sent": "You didn't want to cheat in some sense.",
                    "label": 0
                },
                {
                    "sent": "And yeah, the difference between the real world data set is not is not as significant perhaps, so we try and see what happens chunk wise for each.",
                    "label": 0
                },
                {
                    "sent": "Chunk",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the difference between.",
                    "label": 0
                },
                {
                    "sent": "The performance of the two on each chunk is not very large.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's ridiculous, you know, sometimes it's the same.",
                    "label": 0
                },
                {
                    "sent": "Still, there is a definite statistical trend that sort of suggests that the B as the end at the bottom up approach is doing always.",
                    "label": 0
                },
                {
                    "sent": "We didn't do any statistics over this, no.",
                    "label": 0
                },
                {
                    "sent": "I mean I'm not claiming again this is a significant what we report we're reporting.",
                    "label": 0
                },
                {
                    "sent": "Here is the result for each chunk.",
                    "label": 0
                },
                {
                    "sent": "And I guess you can see that on all chunks.",
                    "label": 0
                },
                {
                    "sent": "They are doing better by a small amount, of course, but they're doing better.",
                    "label": 0
                },
                {
                    "sent": "Here doing I would say significantly better, But basically this one was the one that we were trying to understand this.",
                    "label": 0
                },
                {
                    "sent": "Now this data set.",
                    "label": 0
                },
                {
                    "sent": "The differences here are not really significant.",
                    "label": 0
                },
                {
                    "sent": "I'm not claiming their significance.",
                    "label": 0
                },
                {
                    "sent": "So on this data set, basically 2, the two approaches are are the same.",
                    "label": 0
                },
                {
                    "sent": "On this one, they're not.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know, one could argue about.",
                    "label": 0
                },
                {
                    "sent": "And what happens?",
                    "label": 0
                },
                {
                    "sent": "Level wise.",
                    "label": 0
                },
                {
                    "sent": "I don't wanna go into this these numbers here, but basically one.",
                    "label": 0
                },
                {
                    "sent": "What one can conclude is that.",
                    "label": 0
                },
                {
                    "sent": "The two algorithms are sort of performing similarly at the roots.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But at least nodes the bottom up approach tends to outperform the top down one.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's just because it's starting from the least, starting off from the leaves.",
                    "label": 0
                },
                {
                    "sent": "But it's it's better out at lower levels.",
                    "label": 0
                },
                {
                    "sent": "So they are similar at the roots, which are counting more, but the BSM is slightly better on.",
                    "label": 0
                },
                {
                    "sent": "At the leaves.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we turn to some more theoretical work.",
                    "label": 0
                },
                {
                    "sent": "One can argue I can.",
                    "label": 0
                },
                {
                    "sent": "Can you use the logistic regression here as well?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, we have an analysis for a special case for special parametric model.",
                    "label": 0
                },
                {
                    "sent": "Updates of these probabilities here.",
                    "label": 0
                },
                {
                    "sent": "So basically what we do is to associate with each node.",
                    "label": 0
                },
                {
                    "sent": "Parameter vector normalized to one and claim that the probability that the right node gets level one given.",
                    "label": 0
                },
                {
                    "sent": "The value of the parent node is 1.",
                    "label": 0
                },
                {
                    "sent": "Is obtained this way one plus user by times dot X / 2.",
                    "label": 0
                },
                {
                    "sent": "So this is a probability value.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a parameter.",
                    "label": 0
                },
                {
                    "sent": "So parametric models, simple parametric model, linear parametric model.",
                    "label": 0
                },
                {
                    "sent": "And we did this Pearl notes.",
                    "label": 0
                },
                {
                    "sent": "And once again, we want to enforce legal multi labels and so we have this condition as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we wanted to do was to try and learn.",
                    "label": 0
                },
                {
                    "sent": "In an online protocol.",
                    "label": 0
                },
                {
                    "sent": "I like good hierarchical predictor.",
                    "label": 0
                },
                {
                    "sent": "And align protocol works as follows.",
                    "label": 0
                },
                {
                    "sent": "You there's an algorithm here that receives at each time step an instance vector can.",
                    "label": 0
                },
                {
                    "sent": "It is required to produce a prediction, which is a legal multi label.",
                    "label": 0
                },
                {
                    "sent": "Then it receives that feedback and then you know it keeps iterating.",
                    "label": 0
                },
                {
                    "sent": "This way it updates its internal state and so on.",
                    "label": 0
                },
                {
                    "sent": "And we measure.",
                    "label": 0
                },
                {
                    "sent": "The accuracy of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Against the H loss function that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "But we do not measure DHL how much time do I have?",
                    "label": 0
                },
                {
                    "sent": "Five 10510 we do not measure.",
                    "label": 0
                },
                {
                    "sent": "The H loss per say, but we actually measure the age.",
                    "label": 0
                },
                {
                    "sent": "Lots of the algorithm compared to the H loss of a given comparison predictor.",
                    "label": 0
                },
                {
                    "sent": "So it's a regret analysis.",
                    "label": 0
                },
                {
                    "sent": "Accumulative regret analysis.",
                    "label": 0
                },
                {
                    "sent": "The comparison predictors that top down compared are that knows basically knows.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The parameter is sitting at each node here, so it knows.",
                    "label": 0
                },
                {
                    "sent": "Hopes.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It knows the parametric model here.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it is not the biggest optimal classifier for this.",
                    "label": 0
                },
                {
                    "sent": "Parametric model.",
                    "label": 0
                },
                {
                    "sent": "So it's basically mimicking this HVN way of.",
                    "label": 0
                },
                {
                    "sent": "Label label assignment.",
                    "label": 0
                },
                {
                    "sent": "It's top down.",
                    "label": 0
                },
                {
                    "sent": "Once again, node is labeled according to the value of the actual function.",
                    "label": 0
                },
                {
                    "sent": "If it's a root or.",
                    "label": 0
                },
                {
                    "sent": "It's the it's the child of a node that has been labeled one.",
                    "label": 0
                },
                {
                    "sent": "It is not based off timmel, at least because it does not depend on the cost coefficients.",
                    "label": 0
                },
                {
                    "sent": "So we are comparing really comparing our classifier to a classifier, which is not the base optimal one.",
                    "label": 0
                },
                {
                    "sent": "Which we again do not know how to compute.",
                    "label": 0
                },
                {
                    "sent": "OK, so the best we could do is this.",
                    "label": 0
                },
                {
                    "sent": "And again we measure the cumulative regret the sum overall try overall examples of the expected of the risk.",
                    "label": 0
                },
                {
                    "sent": "Basically the expected loss of the algorithm compared discounted by the expected loss of this top down compared to here.",
                    "label": 0
                },
                {
                    "sent": "And since we wanted to compare to a top down comparator whereas natural way.",
                    "label": 0
                },
                {
                    "sent": "To go is to use a top down predictor.",
                    "label": 0
                },
                {
                    "sent": "Well, the top down predictor basically is doing.",
                    "label": 0
                },
                {
                    "sent": "Local approximation to each parameter here.",
                    "label": 0
                },
                {
                    "sent": "So it stores at each node weight vector which is meant to approximate the corresponding parameters.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You survive.",
                    "label": 0
                },
                {
                    "sent": "And it will stop down this in the same way.",
                    "label": 0
                },
                {
                    "sent": "OK. And the update is basically similar to what H SVM was doing.",
                    "label": 0
                },
                {
                    "sent": "There is a filtering rule here.",
                    "label": 0
                },
                {
                    "sent": "And now an example is passed to a node.",
                    "label": 1
                },
                {
                    "sent": "I only if the parent node was labeled one.",
                    "label": 0
                },
                {
                    "sent": "On that example.",
                    "label": 0
                },
                {
                    "sent": "OK, otherwise not best.",
                    "label": 0
                },
                {
                    "sent": "And it's top down, and once again, independent.",
                    "label": 1
                },
                {
                    "sent": "All the seats are by.",
                    "label": 0
                },
                {
                    "sent": "Which we don't know how to incorporate.",
                    "label": 0
                },
                {
                    "sent": "In the top down scheme.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Naturally.",
                    "label": 0
                },
                {
                    "sent": "He is naturally derived from the parametric model because we have a basically a regularizer, square squares predictor sitting at each node, which is a.",
                    "label": 0
                },
                {
                    "sent": "Nothing topically unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "Of the parameter at each node.",
                    "label": 0
                },
                {
                    "sent": "Well, I want to get into the details here.",
                    "label": 0
                },
                {
                    "sent": "It naturally arises from the parametric model because this margin.",
                    "label": 1
                },
                {
                    "sent": "This estimated margin is almost conditionally unbiased estimate of the true margin at each node and its accuracy actually depends on the number of examples that each node sees.",
                    "label": 0
                },
                {
                    "sent": "Remember that root nodes see old labels while leaf nodes see.",
                    "label": 0
                },
                {
                    "sent": "Only a small amount of labels.",
                    "label": 0
                },
                {
                    "sent": "So we are very good at at root labels part.",
                    "label": 0
                },
                {
                    "sent": "We are poor at at the leaf nodes and you know it can be, you know, running dual variables and all these things.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have a bound.",
                    "label": 0
                },
                {
                    "sent": "Which actually.",
                    "label": 0
                },
                {
                    "sent": "Makes no assumption, no assumptions on the way.",
                    "label": 0
                },
                {
                    "sent": "The instance vectors are generated.",
                    "label": 0
                },
                {
                    "sent": "So there are pointwise bounds.",
                    "label": 0
                },
                {
                    "sent": "So this guy here is constant.",
                    "label": 0
                },
                {
                    "sent": "This guy is a random variable generated according to the generative model blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "But The X Factor can be worst case as well.",
                    "label": 0
                },
                {
                    "sent": "Sorry can be worst case worst case, but it has to be generated ahead of time, so we let an adversary generate the instance vectors here.",
                    "label": 0
                },
                {
                    "sent": "Now we force the adversary too.",
                    "label": 0
                },
                {
                    "sent": "To choose them ahead of time before knowing what the algorithm is doing.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a lose adversary, so weak adversary.",
                    "label": 0
                },
                {
                    "sent": "And then what we can prove?",
                    "label": 0
                },
                {
                    "sent": "Is a regret bound or the following four?",
                    "label": 0
                },
                {
                    "sent": "We had two three ingredients in this bound.",
                    "label": 0
                },
                {
                    "sent": "One is Delta squared, the other one is this capital C sub.",
                    "label": 0
                },
                {
                    "sent": "I and the third ingredient is this eigenvalue thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have that.",
                    "label": 0
                },
                {
                    "sent": "I inverse dependence on the minimal margin.",
                    "label": 0
                },
                {
                    "sent": "Overall examples you have dependence on the contribution of.",
                    "label": 0
                },
                {
                    "sent": "The cost coefficients sitting at each node and the subtree root underneath.",
                    "label": 0
                },
                {
                    "sent": "And you also have a contribution due today.",
                    "label": 0
                },
                {
                    "sent": "The eigen structure of the examples that each node.",
                    "label": 0
                },
                {
                    "sent": "Seize after training.",
                    "label": 0
                },
                {
                    "sent": "OK, and by the way, this is a so called logarithmic cumulative regret, so it's instantaneously.",
                    "label": 0
                },
                {
                    "sent": "This is would be a fast rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The expectation there is over everything over order these over these at the end and the labels over these guys, right?",
                    "label": 0
                },
                {
                    "sent": "So so on a particular run of the algorithm, obviously, yeah.",
                    "label": 0
                },
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "A framework.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say it's great, but it's a framework.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hierarchical classification.",
                    "label": 0
                },
                {
                    "sent": "We've been trying to improve a baseline top down.",
                    "label": 0
                },
                {
                    "sent": "Label assignment scheme, known as HSPN by Bottom up base optimal like algorithm.",
                    "label": 1
                },
                {
                    "sent": "And I believe what we did was provide some sort of a modular approach.",
                    "label": 0
                },
                {
                    "sent": "Instead of being principled, it more affecting machine learning practice.",
                    "label": 0
                },
                {
                    "sent": "I mean, the PSTN thing is more like a practical thing.",
                    "label": 0
                },
                {
                    "sent": "We could replace SVM, my other things as well.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression, of course, would be one of the candidates.",
                    "label": 0
                },
                {
                    "sent": "Now we made some preliminary experiments with that love to have more experiments on this and they also show they know line, top down algorithm and its regret analysis.",
                    "label": 0
                },
                {
                    "sent": "Based on our regularizer squared algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, open questions.",
                    "label": 0
                },
                {
                    "sent": "Got to kind of two kinds of open questions.",
                    "label": 0
                },
                {
                    "sent": "Experimental.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, there's a clear advantage of the bottom of the bottom up approach on synthetic datasets, and it's less clear on real world datasets.",
                    "label": 1
                },
                {
                    "sent": "Which we are clearly more interested in.",
                    "label": 0
                },
                {
                    "sent": "I don't know why exactly we do not know why that might be the case that the real world datasets are just noisy.",
                    "label": 0
                },
                {
                    "sent": "Too noisy too to make this base like scheme to work well, or it might well be the case that since we are.",
                    "label": 0
                },
                {
                    "sent": "We have this independence assumption.",
                    "label": 0
                },
                {
                    "sent": "This cannot be.",
                    "label": 0
                },
                {
                    "sent": "Wait to go.",
                    "label": 0
                },
                {
                    "sent": "Once I fixed the value of this value.",
                    "label": 0
                },
                {
                    "sent": "Once I said this to one, the value of these guys are independent random variables, which is not need not be a good assumption in hierarchy or classification.",
                    "label": 0
                },
                {
                    "sent": "But it simplifies matters quite a lot.",
                    "label": 1
                },
                {
                    "sent": "And of course, as Tom was mentioning, for instance, replace SVM by better algorithms that are more suitable for probability estimations, such as logistic regression as well.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "On the theoretical side.",
                    "label": 0
                },
                {
                    "sent": "Our regret analysis.",
                    "label": 0
                },
                {
                    "sent": "Was not refering to the base optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "We are not comparing against the base optimal classifier for that particular model, which we do not know how to compute.",
                    "label": 0
                },
                {
                    "sent": "Maybe a logistic model again?",
                    "label": 0
                },
                {
                    "sent": "Will be a good.",
                    "label": 0
                },
                {
                    "sent": "Candidate to try to.",
                    "label": 0
                },
                {
                    "sent": "To compare to.",
                    "label": 0
                },
                {
                    "sent": "Once we have an analysis for logistic model.",
                    "label": 0
                },
                {
                    "sent": "And once again, even in the theoretical work, we would like to remove this independence assumption, which is perhaps not.",
                    "label": 0
                },
                {
                    "sent": "Promising promising Ave thank you, I'm done.",
                    "label": 0
                },
                {
                    "sent": "Please yes.",
                    "label": 0
                },
                {
                    "sent": "So it seems that in your classification problem you want to compute the maximum posterior solution or product over the designers of the piece, right?",
                    "label": 0
                },
                {
                    "sent": "You're talking about the BSM thing, doesn't matter, but it doesn't matter, OK?",
                    "label": 0
                },
                {
                    "sent": "But that's kind of the goal to compute the maximum posteriori.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, it's it's basically captured by this thing.",
                    "label": 0
                },
                {
                    "sent": "Yes, please, thank you.",
                    "label": 0
                },
                {
                    "sent": "So, so why would you want them to use kind of?",
                    "label": 0
                },
                {
                    "sent": "Understanding algorithm.",
                    "label": 0
                },
                {
                    "sent": "For that kind of standard algorithm for models.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a tree, but there is an equivalent of 50 to be automated.",
                    "label": 0
                },
                {
                    "sent": "This in this it.",
                    "label": 0
                },
                {
                    "sent": "If you do that, then also So what did you do?",
                    "label": 0
                },
                {
                    "sent": "So why don't you do bother her and then go back again when she doing that's fine.",
                    "label": 0
                },
                {
                    "sent": "Did you do that?",
                    "label": 0
                },
                {
                    "sent": "No, I I I I I go bottom up and then top down only if this guy gets labeled 0.",
                    "label": 0
                },
                {
                    "sent": "So you actually do.",
                    "label": 0
                },
                {
                    "sent": "This is Dennis Pok\u00e9mon and this stuff down.",
                    "label": 0
                },
                {
                    "sent": "It's because I want to know whether one is better than the other.",
                    "label": 0
                },
                {
                    "sent": "What do you mean?",
                    "label": 0
                },
                {
                    "sent": "So which one is equivalent to that area?",
                    "label": 0
                },
                {
                    "sent": "Part I?",
                    "label": 0
                },
                {
                    "sent": "I guess this one, the bottom up one I guess.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because you make a hard assignment, but you wouldn't do it.",
                    "label": 0
                },
                {
                    "sent": "You would want to build a model of the joint probability and then calculate the marginals with having babies in the markers, not even the math assignment of the labels.",
                    "label": 0
                },
                {
                    "sent": "Message.",
                    "label": 0
                },
                {
                    "sent": "The last byte by message passing.",
                    "label": 0
                },
                {
                    "sent": "We put exactly come out right right right.",
                    "label": 0
                },
                {
                    "sent": "If you formulated it as effective raw and it.",
                    "label": 0
                },
                {
                    "sent": "But do you agree that this is the definition of the Bayes optimal classifier?",
                    "label": 0
                },
                {
                    "sent": "For this this line here?",
                    "label": 0
                },
                {
                    "sent": "Is the one that minimizes the the conditional risk.",
                    "label": 0
                },
                {
                    "sent": "If you agree with that, then you should agree with this algorithm as well.",
                    "label": 0
                },
                {
                    "sent": "It's a hard assignment.",
                    "label": 0
                },
                {
                    "sent": "OK. And then after that.",
                    "label": 0
                },
                {
                    "sent": "You have this algorithm for polymer.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is basically the bottom up.",
                    "label": 0
                },
                {
                    "sent": "Let's say I'm inspired by this bottomup scheme to build an algorithm that is sort of.",
                    "label": 0
                },
                {
                    "sent": "Combining.",
                    "label": 0
                },
                {
                    "sent": "Probabilities.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My problem is I want to combine what's happening at each local Y at each node in some way.",
                    "label": 1
                },
                {
                    "sent": "In some way, either by combining them bottom up special or in the top down or whatever else, something.",
                    "label": 0
                },
                {
                    "sent": "But I think that if you would apply.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is speed would go back and forth, and then you're done, and then you have a map.",
                    "label": 0
                },
                {
                    "sent": "So I don't know why this is.",
                    "label": 0
                },
                {
                    "sent": "I believe this is the Viterbi thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so why do you need something else?",
                    "label": 0
                },
                {
                    "sent": "Because there's not, this seems to be only going by the way.",
                    "label": 0
                },
                {
                    "sent": "Well, why do I need something else?",
                    "label": 0
                },
                {
                    "sent": "'cause I, I'm not sure this is the best thing you can do.",
                    "label": 0
                },
                {
                    "sent": "I know you're busy and but no, I'm just kidding, sorry.",
                    "label": 0
                },
                {
                    "sent": "I'm not, I'm not being Bayesian here.",
                    "label": 0
                },
                {
                    "sent": "You know 'cause I'm approximating.",
                    "label": 0
                },
                {
                    "sent": "Everything here is approximated, so it's just a scheme that sort of inspires the way of combining.",
                    "label": 0
                },
                {
                    "sent": "The local the local information here.",
                    "label": 0
                },
                {
                    "sent": "So this would be the best thing you could do if you had this piece of pie available, OK. Yeah, and then yeah.",
                    "label": 0
                },
                {
                    "sent": "And then, that's that's you're perfectly right then you're done.",
                    "label": 0
                },
                {
                    "sent": "But these things are not available.",
                    "label": 0
                },
                {
                    "sent": "We are approximating and in some weird way and still we are applying.",
                    "label": 0
                },
                {
                    "sent": "We are still running this game over bottom, nothing, but we are not at all, you know.",
                    "label": 0
                },
                {
                    "sent": "Anti that the combined thing would be would be a good thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK, is that?",
                    "label": 0
                },
                {
                    "sent": "Because I mean if you would would look at it as kind of presuming estimation and then do inference right then.",
                    "label": 0
                },
                {
                    "sent": "This part.",
                    "label": 0
                },
                {
                    "sent": "That we had would be the same after estimation and then you will get some influence and influence.",
                    "label": 0
                },
                {
                    "sent": "It's kind of.",
                    "label": 0
                },
                {
                    "sent": "Then probably the thing that you do is saying that you have to do it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but that's still do you think?",
                    "label": 0
                },
                {
                    "sent": "Do you think this is a?",
                    "label": 0
                },
                {
                    "sent": "A good way of estimating this, for instance, a good way of estimating this probabilities.",
                    "label": 0
                },
                {
                    "sent": "Once I had this probabilities, then I'm in good shape, sure.",
                    "label": 0
                },
                {
                    "sent": "Pay phone gives me a an approximation to it.",
                    "label": 0
                },
                {
                    "sent": "Why should I wear?",
                    "label": 0
                },
                {
                    "sent": "Should I should I play bottom up?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's no, there's no.",
                    "label": 0
                },
                {
                    "sent": "It seems to me there's no guarantee that.",
                    "label": 0
                },
                {
                    "sent": "That the overall thing would still work, so I think I think.",
                    "label": 0
                },
                {
                    "sent": "Then if you do inference, you need to pass process box.",
                    "label": 0
                },
                {
                    "sent": "It depends how you define the probability of a child.",
                    "label": 0
                },
                {
                    "sent": "Being negative, given the parents negative or something happened.",
                    "label": 0
                },
                {
                    "sent": "If you have the the probability that a child is negative given the parent was classified negative a 0 which is in this model, then you don't need to do to pass it OK, because that so there is this magic parameter.",
                    "label": 0
                },
                {
                    "sent": "We tried doing it, getting the probabilities out and doing inference.",
                    "label": 0
                },
                {
                    "sent": "Then you have this magic parameter which is OK sometimes where my my parents classified as negative.",
                    "label": 0
                },
                {
                    "sent": "I actually want to change my child, flip the label of my child because it makes the whole thing more likely.",
                    "label": 0
                },
                {
                    "sent": "Correct parent labels in some sense, but you have this magic parameter that you gotta pick out of hand and they work any better.",
                    "label": 0
                },
                {
                    "sent": "Or once you pick.",
                    "label": 0
                },
                {
                    "sent": "If you find this problem, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "No obvious way to center, but once you once you set in hindsight that there's a definite yes.",
                    "label": 0
                },
                {
                    "sent": "Basically you can.",
                    "label": 0
                },
                {
                    "sent": "It depends on how, but if you've got a really big tree and you made a mistake up there somewhere right then, if there's enough kind of probability, mass inference will flip.",
                    "label": 0
                },
                {
                    "sent": "But if you you know that that's popped up right, yeah?",
                    "label": 0
                },
                {
                    "sent": "If you, if you say the parents, if the parents may be classified as negative, then the child is negative, but with probability one.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah, you don't have to do that.",
                    "label": 0
                },
                {
                    "sent": "It has become the dominant states.",
                    "label": 0
                },
                {
                    "sent": "OK, then welcome up.",
                    "label": 0
                },
                {
                    "sent": "Is this the equipment is equipment?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, you just propagated.",
                    "label": 0
                },
                {
                    "sent": "Going down never changes.",
                    "label": 0
                },
                {
                    "sent": "Down a bit.",
                    "label": 0
                },
                {
                    "sent": "You had to go down.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a little bit, yeah.",
                    "label": 0
                },
                {
                    "sent": "Because you have, yeah.",
                    "label": 0
                },
                {
                    "sent": "Negative.",
                    "label": 0
                },
                {
                    "sent": "Correct, if there were any children.",
                    "label": 0
                },
                {
                    "sent": "So yes, yes yes yes yes absolutely yes.",
                    "label": 0
                },
                {
                    "sent": "One thing is just one there.",
                    "label": 0
                },
                {
                    "sent": "You do it once.",
                    "label": 0
                },
                {
                    "sent": "Once you get it once.",
                    "label": 0
                },
                {
                    "sent": "Any suggestions?",
                    "label": 0
                },
                {
                    "sent": "I really would be other people.",
                    "label": 0
                },
                {
                    "sent": "There are some people.",
                    "label": 0
                },
                {
                    "sent": "Who are they?",
                    "label": 0
                },
                {
                    "sent": "Who are they?",
                    "label": 0
                },
                {
                    "sent": "OK. Yo, yo.",
                    "label": 0
                },
                {
                    "sent": "Model that has a global method to try to do even more complicated.",
                    "label": 0
                },
                {
                    "sent": "Glad to.",
                    "label": 0
                },
                {
                    "sent": "Basically you have a complete solid model.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right, right.",
                    "label": 0
                },
                {
                    "sent": "People more favorable about global methods rather than they say go local method like this won't work.",
                    "label": 0
                },
                {
                    "sent": "Joint distribution for Yyyy end.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well this bottom up thing that I showed you was a sort of a global method because because of the thresholds.",
                    "label": 0
                },
                {
                    "sent": "Where is it?",
                    "label": 0
                },
                {
                    "sent": "Use the global estimates in the training in the training, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They will not have the conditional.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is a weak global method.",
                    "label": 0
                },
                {
                    "sent": "There's somebody believe that's at least what about running time?",
                    "label": 0
                },
                {
                    "sent": "Server running.",
                    "label": 0
                },
                {
                    "sent": "So looks kind of strange to me is that you you need to build this.",
                    "label": 0
                },
                {
                    "sent": "Different inference.",
                    "label": 0
                },
                {
                    "sent": "Framework.",
                    "label": 0
                },
                {
                    "sent": "So the inference and the training do not necessarily match, so the structure.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's right framework.",
                    "label": 0
                },
                {
                    "sent": "Those captured as well?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you build a logistic regression into it.",
                    "label": 0
                },
                {
                    "sent": "You could rise it as one big graphical model.",
                    "label": 0
                },
                {
                    "sent": "You could do joint inference.",
                    "label": 0
                },
                {
                    "sent": "In that moment.",
                    "label": 0
                },
                {
                    "sent": "I mean, I don't know if it makes any big difference in practice, but it would kind of unify this thing alright, but would it work in practice?",
                    "label": 0
                },
                {
                    "sent": "I mean and would it be practical?",
                    "label": 0
                },
                {
                    "sent": "You think you think it would be practical doing local message passing, right?",
                    "label": 0
                },
                {
                    "sent": "And what about saying?",
                    "label": 0
                },
                {
                    "sent": "What about training?",
                    "label": 0
                },
                {
                    "sent": "Yeah also.",
                    "label": 0
                },
                {
                    "sent": "Sing CRF.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I did some work on that now.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical.",
                    "label": 0
                },
                {
                    "sent": "Sequences experimentally they do not come out to be honest.",
                    "label": 0
                },
                {
                    "sent": "Base optimality is one advantage.",
                    "label": 0
                },
                {
                    "sent": "Maybe one comment from my side.",
                    "label": 0
                },
                {
                    "sent": "So basically we are also working.",
                    "label": 0
                },
                {
                    "sent": "But we deal with this car is which are as if 4230 is bigger, so let's say.",
                    "label": 0
                },
                {
                    "sent": "600,600 thousand million documents on every day up to.",
                    "label": 0
                },
                {
                    "sent": "Yeah, very patient.",
                    "label": 0
                },
                {
                    "sent": "You're very patient.",
                    "label": 0
                },
                {
                    "sent": "Like this, otherwise it's not useful, so so this would be used for labeling search results or.",
                    "label": 0
                },
                {
                    "sent": "Assigning.",
                    "label": 0
                },
                {
                    "sent": "Labels the documents or so.",
                    "label": 0
                },
                {
                    "sent": "Do things in different ways so that the force.",
                    "label": 0
                },
                {
                    "sent": "Setting so.",
                    "label": 0
                },
                {
                    "sent": "This would be the focus and tation about this.",
                    "label": 0
                },
                {
                    "sent": "I didn't want to talk about this here but still.",
                    "label": 0
                },
                {
                    "sent": "So how somehow we combine little bit of indexing methods so so the nature of data changes.",
                    "label": 0
                },
                {
                    "sent": "Large, very large hierarchies.",
                    "label": 0
                },
                {
                    "sent": "Usually you cannot infer anything from the top level.",
                    "label": 0
                },
                {
                    "sent": "The balls, let's say 15 levels deep.",
                    "label": 0
                },
                {
                    "sent": "Basically everything is lost.",
                    "label": 0
                },
                {
                    "sent": "How, how deep are the hierarchies?",
                    "label": 0
                },
                {
                    "sent": "78 So we cannot comment much on this transition because it's, let's say on the bottom.",
                    "label": 0
                },
                {
                    "sent": "So this is then.",
                    "label": 0
                },
                {
                    "sent": "$500,000, six, 100,000 classes.",
                    "label": 0
                },
                {
                    "sent": "Somehow similar or connected but.",
                    "label": 0
                },
                {
                    "sent": "Presenting the results.",
                    "label": 0
                },
                {
                    "sent": "Usually this kind of.",
                    "label": 0
                },
                {
                    "sent": "Already.",
                    "label": 0
                }
            ]
        }
    }
}