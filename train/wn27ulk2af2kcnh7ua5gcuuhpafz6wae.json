{
    "id": "wn27ulk2af2kcnh7ua5gcuuhpafz6wae",
    "title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks",
    "info": {
        "author": [
            "Rajarshi Das, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_das_neural_networks/",
    "segmentation": [
        [
            "I'm resolution, I'm going to talk about chains of reasoning over entities, relation and text using recurrent neural networks.",
            "This is a joint work with David Belanger and my advisor Andrew McCallum."
        ],
        [
            "So the high level problem which we are addressing here is relation extraction and so given the corporate tax documents, the first step in relation extraction is to identify the entities in the document.",
            "Now after this step.",
            "After extracting the entities, we often ask the question we are interested in finding the relations between the entities.",
            "For example, we might be interested in the CEO relation, or we might be interested in the friends or coworkers relation.",
            "Now.",
            "Often this relation comes from some predefined schema, such as the Wikipedia infobox tables, or it comes from knowledge basis such as the schema defining knowledge basis such as Freebase.",
            "Or it could just be relations defined in free text such as the Openi textual relations.",
            "Now, not all schemas are equal.",
            "For example, one of the schema might just have the friends relation and not the coworkers, and vice versa.",
            "But we could compromise and make and map them to a common schema, but that would be throwing away information for example.",
            "So for example, not all friends or coworkers and not all coworkers are friends.",
            "So instead of making this compromise, universal schema represents the union of all relation types from all structured sources.",
            "An also relations in text.",
            "First in our work the relation types could be present from could come from any predefined schema or it could be textual relations.",
            "So for completion, relation extraction is defined as the task of predicting relations between entities based on their mentions in text."
        ],
        [
            "Now, previous approaches to relation extraction often rely on a single source of evidence.",
            "For example, predicting the fact that Microsoft is based in Seattle.",
            "From the fact that it is headquartered in Seattle right now, we can generalize much more.",
            "For example, here is another piece of text, possibly from a different document.",
            "And by combining this, multiple pieces of evidence, we can predict the fact that Melinda Gates lives in Seattle.",
            "Now we achieve this an and this fact cannot be inferred from any single document alone.",
            "Now to achieve this?",
            "We reasoned over multiple hops.",
            "On in a path in a knowledge graph between Melinda and Seattle.",
            "And hence we refer to this as multihop inference.",
            "The previous case is basically a special case which we call as single hop inference.",
            "Now this is also analagous to learning general purpose on clauses.",
            "For example, if A is a spouse of B&B, is the chairman of C&C is headquartered in D then a lives in D?",
            "No two classic work which actually aimed at learning this general purpose on clauses from free text are sherlockian path ranking algorithm.",
            "Now the disadvantage of these methods are there symbolic and they don't generalize.",
            "So they treat each path as unique features without any parameter sharing.",
            "So at Test time if you don't see this part before, for example, this is a similar path, just that I have replaced Chairman with CEO.",
            "They don't generalize, they contradict the, they can't predict the lives in relation.",
            "Now, to overcome this we need to learn low dimensional representations of relations."
        ],
        [
            "Now I'm going to introduce our approach of composing relations at each step of the path using recurrent neural networks.",
            "Now, at each step the relation vector is fed into RNN and thereby outputting a hidden vector and when we reach the end of the path.",
            "We obtain a vector representation of the path of the Knowledge Graph.",
            "Now when we have the path, we compare it with the relation we want to predict.",
            "In this case, it's lives in and we obtain a similarity score.",
            "In our case, the similarity metric is just the simple dot product followed by sigmoid operation, but it could be anything.",
            "It could be a multilayer perceptron here.",
            "Now this this can be seen as doing some kind of logical inference in vector space instead of over symbols.",
            "Now, this particular model was introduced by Neil Continental from our lab last year.",
            "Now we live."
        ],
        [
            "Around the contributions of this work, first is we learn a single high capacity recurrent neural network for relation extraction, and I'll go into each of these.",
            "Secondly, we combine evidence from multiple paths between entity pairs, and Thirdly, we make the model entity aware.",
            "Overall, accumulatively all these lead to 13.7% increase in performance and this is mean average precision."
        ],
        [
            "Now coming to the first, so the previous model which I described learns like different set of parameters for, let's say when they're predicting the lives in relation or the works in relation.",
            "Or let's say the shops in relation.",
            "Now clearly they will share a lot of parameters between them so."
        ],
        [
            "Instead, we learn a single single model for all the relations and we achieve."
        ],
        [
            "This by.",
            "Learning."
        ],
        [
            "The representation of the relations and also a single composition matrix for all the query relation.",
            "Also at the last layer instead of having a query vector, we have a matrix of output embeddings."
        ],
        [
            "Now let's look at the results of on adding this learning a single.",
            "Model so our baselines are path ranking algorithm man this is extension with bigram features, and so the evaluation is mean mean average precision.",
            "It's not accuracy, so we evaluate a rank list.",
            "So pass ranking algorithm and its extension gets around 65.",
            "The individual models are in path which I refer to it as our own path.",
            "Model gets a similar score a little bit of improvement.",
            "With parameter sharing we get around 5% improvement here."
        ],
        [
            "Next, I describe how we combine evidence from multiple paths now till now."
        ],
        [
            "I explained in such a way that there is only one path in the knowledge base between Melinda and Seattle."
        ],
        [
            "There are thousands of bots, for example, this is another path.",
            "Melinda is a sea of Gates Foundation, which is located in Seattle or this is a fictitious path.",
            "Melinda friend Jane Doe switch back to John Doe, who also was born in Seattle, so there could be 10s of thousands of path and most of the parts are uninformed, uninformative, so."
        ],
        [
            "Our encoder will actually have give for each path.",
            "We have representation an for each path will have a similarity score.",
            "Right now the next step is to combine them."
        ],
        [
            "Two to output one score.",
            "Now the first thing which we tried was Max pooling.",
            "So basically only considering the path which would give the maximum score.",
            "Now the disadvantage of this method is during the backdrop step, during the gradient step, only the parameters associated with the maximum scoring parts are updated.",
            "There are other straightforward pooling approaches we tried, such as averaging all the scores, so we have 10,000 parts and we average all the scores or or.",
            "Considering the top five or seven paths."
        ],
        [
            "The last pooling function which word well is the lock some X function.",
            "So here it considers all the path, but during the gradient step.",
            "Depart with which is which is actually the path which are important.",
            "Gets higher share of the gradient, as can be seen from the partial derivative of the locks function, which reduces through the softmax function.",
            "Also, since all the parts are getting updated, this leads to faster training."
        ],
        [
            "Looking at the results, so we left at 65 and the same model that is single individual models for each relation with lock, some exploding gave to us from 65 to around 68.",
            "These are the different polling results on the single Shared parameter model.",
            "We see that average didn't perform well and that is the cause.",
            "Giving equal importance to all the parts in the knowledge base between entity pair is a bad idea.",
            "With locks, Amex, we hit the 70 mark so that was nice.",
            "Thirdly sure.",
            "Right, the 2nd row is Yep."
        ],
        [
            "So the third contribution is entity aware.",
            "Now let me know."
        ],
        [
            "Motivated so looking at this piece of the Knowledge Graph this path we see that John F Kennedy Airport is located in New York City, which is located in New York and the question we ask is whether JFK Airport serves the location and why.",
            "And when we input this path or model, it really gives a high score.",
            "Now the same path except the fact that we replaced JFK with Yankee Stadium.",
            "We get the same score.",
            "That is because our model is agnostic of the entities which occur in the chain, so it doesn't know Yankee Stadium is not a airport.",
            "So basically"
        ],
        [
            "We have to build in entity representations into the model."
        ],
        [
            "The first thing that we do is the straightforward thing is to learn separate representations for each entity.",
            "But the problem with this is we can learn good representations for Melinda Gates, but for a not so frequently occurring entity, such as the fictitious Jane Doe here.",
            "We don't learn really good representations because of the fact they are not so frequent in the corpus.",
            "The second thing which we try is representing entities by the annotated types.",
            "So in knowledge base we have every entity is annotated with a bunch of types.",
            "For example, in Freebase the types of Melinda she's a CEO, she's a philanthropist.",
            "She's a alumni of the University, she's American citizen, and for the fictitious Jane Doe, she might be an American citizen, an A small business owner."
        ],
        [
            "Now the representation of entity is just a simple sum of its types.",
            "Now."
        ],
        [
            "This is how now this is the complete model is it looks like we have.",
            "In addition to the representations of the relations, we also have embeddings for the entities here."
        ],
        [
            "Looking at the results, the single model with locks, Amex had 70 learning representations.",
            "Entity gives us a one percentage point boots with types given the maximum score of 73.26."
        ],
        [
            "Looking at the all of the experiments together, our baselines for 65 an with parameter sharing, it gave us some boost with locks, MX more boost and finally we got a 13.7% improvement.",
            "Now."
        ],
        [
            "Let's look at some of the clauses or some some of the high scoring path switch our model learned.",
            "For example, we learned that if A was born in place X, an X is commonly known as B in the place of birth of ASP.",
            "Another thing to note about this learnt clauses that the left hand side of the clauses, previous solution and the right inside our textual relation.",
            "So this shows the power of Universal Schema which connects relations present in schemas and text.",
            "This is another non trivial class.",
            "I'm not going to go into the details which we learned.",
            "Now let's look at path learn by the Entity aware model.",
            "So the question here was whether Sandy Lake Airport serves this place.",
            "Sandy Lake First Nation.",
            "It supplies in Canada now.",
            "The the path which we scored the maximum by the Entity Aware model is this it says San Diego Airport is in Ontario and Sandy Lake.",
            "First Nation is in the northwestern part of Ontario.",
            "But the model which didn't have the notion of entity it learned this long and absurd path really didn't make sense.",
            "It went to a Music Hall and a water body.",
            "Now."
        ],
        [
            "In conclusion, we introduced a single high capacity recurrent unit model for relation extraction.",
            "And we combined evidence among multiple paths and possibly sources of evidence spanning multiple documents an our model is entity aware, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm resolution, I'm going to talk about chains of reasoning over entities, relation and text using recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with David Belanger and my advisor Andrew McCallum.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the high level problem which we are addressing here is relation extraction and so given the corporate tax documents, the first step in relation extraction is to identify the entities in the document.",
                    "label": 0
                },
                {
                    "sent": "Now after this step.",
                    "label": 0
                },
                {
                    "sent": "After extracting the entities, we often ask the question we are interested in finding the relations between the entities.",
                    "label": 0
                },
                {
                    "sent": "For example, we might be interested in the CEO relation, or we might be interested in the friends or coworkers relation.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Often this relation comes from some predefined schema, such as the Wikipedia infobox tables, or it comes from knowledge basis such as the schema defining knowledge basis such as Freebase.",
                    "label": 0
                },
                {
                    "sent": "Or it could just be relations defined in free text such as the Openi textual relations.",
                    "label": 0
                },
                {
                    "sent": "Now, not all schemas are equal.",
                    "label": 0
                },
                {
                    "sent": "For example, one of the schema might just have the friends relation and not the coworkers, and vice versa.",
                    "label": 0
                },
                {
                    "sent": "But we could compromise and make and map them to a common schema, but that would be throwing away information for example.",
                    "label": 0
                },
                {
                    "sent": "So for example, not all friends or coworkers and not all coworkers are friends.",
                    "label": 0
                },
                {
                    "sent": "So instead of making this compromise, universal schema represents the union of all relation types from all structured sources.",
                    "label": 1
                },
                {
                    "sent": "An also relations in text.",
                    "label": 0
                },
                {
                    "sent": "First in our work the relation types could be present from could come from any predefined schema or it could be textual relations.",
                    "label": 1
                },
                {
                    "sent": "So for completion, relation extraction is defined as the task of predicting relations between entities based on their mentions in text.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, previous approaches to relation extraction often rely on a single source of evidence.",
                    "label": 0
                },
                {
                    "sent": "For example, predicting the fact that Microsoft is based in Seattle.",
                    "label": 0
                },
                {
                    "sent": "From the fact that it is headquartered in Seattle right now, we can generalize much more.",
                    "label": 0
                },
                {
                    "sent": "For example, here is another piece of text, possibly from a different document.",
                    "label": 0
                },
                {
                    "sent": "And by combining this, multiple pieces of evidence, we can predict the fact that Melinda Gates lives in Seattle.",
                    "label": 0
                },
                {
                    "sent": "Now we achieve this an and this fact cannot be inferred from any single document alone.",
                    "label": 0
                },
                {
                    "sent": "Now to achieve this?",
                    "label": 0
                },
                {
                    "sent": "We reasoned over multiple hops.",
                    "label": 0
                },
                {
                    "sent": "On in a path in a knowledge graph between Melinda and Seattle.",
                    "label": 1
                },
                {
                    "sent": "And hence we refer to this as multihop inference.",
                    "label": 0
                },
                {
                    "sent": "The previous case is basically a special case which we call as single hop inference.",
                    "label": 0
                },
                {
                    "sent": "Now this is also analagous to learning general purpose on clauses.",
                    "label": 0
                },
                {
                    "sent": "For example, if A is a spouse of B&B, is the chairman of C&C is headquartered in D then a lives in D?",
                    "label": 0
                },
                {
                    "sent": "No two classic work which actually aimed at learning this general purpose on clauses from free text are sherlockian path ranking algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the disadvantage of these methods are there symbolic and they don't generalize.",
                    "label": 0
                },
                {
                    "sent": "So they treat each path as unique features without any parameter sharing.",
                    "label": 0
                },
                {
                    "sent": "So at Test time if you don't see this part before, for example, this is a similar path, just that I have replaced Chairman with CEO.",
                    "label": 0
                },
                {
                    "sent": "They don't generalize, they contradict the, they can't predict the lives in relation.",
                    "label": 0
                },
                {
                    "sent": "Now, to overcome this we need to learn low dimensional representations of relations.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to introduce our approach of composing relations at each step of the path using recurrent neural networks.",
                    "label": 0
                },
                {
                    "sent": "Now, at each step the relation vector is fed into RNN and thereby outputting a hidden vector and when we reach the end of the path.",
                    "label": 0
                },
                {
                    "sent": "We obtain a vector representation of the path of the Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "Now when we have the path, we compare it with the relation we want to predict.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's lives in and we obtain a similarity score.",
                    "label": 0
                },
                {
                    "sent": "In our case, the similarity metric is just the simple dot product followed by sigmoid operation, but it could be anything.",
                    "label": 0
                },
                {
                    "sent": "It could be a multilayer perceptron here.",
                    "label": 0
                },
                {
                    "sent": "Now this this can be seen as doing some kind of logical inference in vector space instead of over symbols.",
                    "label": 1
                },
                {
                    "sent": "Now, this particular model was introduced by Neil Continental from our lab last year.",
                    "label": 0
                },
                {
                    "sent": "Now we live.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Around the contributions of this work, first is we learn a single high capacity recurrent neural network for relation extraction, and I'll go into each of these.",
                    "label": 0
                },
                {
                    "sent": "Secondly, we combine evidence from multiple paths between entity pairs, and Thirdly, we make the model entity aware.",
                    "label": 1
                },
                {
                    "sent": "Overall, accumulatively all these lead to 13.7% increase in performance and this is mean average precision.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now coming to the first, so the previous model which I described learns like different set of parameters for, let's say when they're predicting the lives in relation or the works in relation.",
                    "label": 0
                },
                {
                    "sent": "Or let's say the shops in relation.",
                    "label": 0
                },
                {
                    "sent": "Now clearly they will share a lot of parameters between them so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead, we learn a single single model for all the relations and we achieve.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This by.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The representation of the relations and also a single composition matrix for all the query relation.",
                    "label": 0
                },
                {
                    "sent": "Also at the last layer instead of having a query vector, we have a matrix of output embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at the results of on adding this learning a single.",
                    "label": 0
                },
                {
                    "sent": "Model so our baselines are path ranking algorithm man this is extension with bigram features, and so the evaluation is mean mean average precision.",
                    "label": 0
                },
                {
                    "sent": "It's not accuracy, so we evaluate a rank list.",
                    "label": 0
                },
                {
                    "sent": "So pass ranking algorithm and its extension gets around 65.",
                    "label": 0
                },
                {
                    "sent": "The individual models are in path which I refer to it as our own path.",
                    "label": 0
                },
                {
                    "sent": "Model gets a similar score a little bit of improvement.",
                    "label": 0
                },
                {
                    "sent": "With parameter sharing we get around 5% improvement here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, I describe how we combine evidence from multiple paths now till now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I explained in such a way that there is only one path in the knowledge base between Melinda and Seattle.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are thousands of bots, for example, this is another path.",
                    "label": 0
                },
                {
                    "sent": "Melinda is a sea of Gates Foundation, which is located in Seattle or this is a fictitious path.",
                    "label": 0
                },
                {
                    "sent": "Melinda friend Jane Doe switch back to John Doe, who also was born in Seattle, so there could be 10s of thousands of path and most of the parts are uninformed, uninformative, so.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our encoder will actually have give for each path.",
                    "label": 0
                },
                {
                    "sent": "We have representation an for each path will have a similarity score.",
                    "label": 0
                },
                {
                    "sent": "Right now the next step is to combine them.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two to output one score.",
                    "label": 0
                },
                {
                    "sent": "Now the first thing which we tried was Max pooling.",
                    "label": 0
                },
                {
                    "sent": "So basically only considering the path which would give the maximum score.",
                    "label": 0
                },
                {
                    "sent": "Now the disadvantage of this method is during the backdrop step, during the gradient step, only the parameters associated with the maximum scoring parts are updated.",
                    "label": 0
                },
                {
                    "sent": "There are other straightforward pooling approaches we tried, such as averaging all the scores, so we have 10,000 parts and we average all the scores or or.",
                    "label": 0
                },
                {
                    "sent": "Considering the top five or seven paths.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last pooling function which word well is the lock some X function.",
                    "label": 0
                },
                {
                    "sent": "So here it considers all the path, but during the gradient step.",
                    "label": 0
                },
                {
                    "sent": "Depart with which is which is actually the path which are important.",
                    "label": 0
                },
                {
                    "sent": "Gets higher share of the gradient, as can be seen from the partial derivative of the locks function, which reduces through the softmax function.",
                    "label": 0
                },
                {
                    "sent": "Also, since all the parts are getting updated, this leads to faster training.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at the results, so we left at 65 and the same model that is single individual models for each relation with lock, some exploding gave to us from 65 to around 68.",
                    "label": 0
                },
                {
                    "sent": "These are the different polling results on the single Shared parameter model.",
                    "label": 0
                },
                {
                    "sent": "We see that average didn't perform well and that is the cause.",
                    "label": 0
                },
                {
                    "sent": "Giving equal importance to all the parts in the knowledge base between entity pair is a bad idea.",
                    "label": 0
                },
                {
                    "sent": "With locks, Amex, we hit the 70 mark so that was nice.",
                    "label": 0
                },
                {
                    "sent": "Thirdly sure.",
                    "label": 0
                },
                {
                    "sent": "Right, the 2nd row is Yep.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the third contribution is entity aware.",
                    "label": 0
                },
                {
                    "sent": "Now let me know.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Motivated so looking at this piece of the Knowledge Graph this path we see that John F Kennedy Airport is located in New York City, which is located in New York and the question we ask is whether JFK Airport serves the location and why.",
                    "label": 1
                },
                {
                    "sent": "And when we input this path or model, it really gives a high score.",
                    "label": 0
                },
                {
                    "sent": "Now the same path except the fact that we replaced JFK with Yankee Stadium.",
                    "label": 0
                },
                {
                    "sent": "We get the same score.",
                    "label": 0
                },
                {
                    "sent": "That is because our model is agnostic of the entities which occur in the chain, so it doesn't know Yankee Stadium is not a airport.",
                    "label": 0
                },
                {
                    "sent": "So basically",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have to build in entity representations into the model.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first thing that we do is the straightforward thing is to learn separate representations for each entity.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this is we can learn good representations for Melinda Gates, but for a not so frequently occurring entity, such as the fictitious Jane Doe here.",
                    "label": 0
                },
                {
                    "sent": "We don't learn really good representations because of the fact they are not so frequent in the corpus.",
                    "label": 0
                },
                {
                    "sent": "The second thing which we try is representing entities by the annotated types.",
                    "label": 0
                },
                {
                    "sent": "So in knowledge base we have every entity is annotated with a bunch of types.",
                    "label": 0
                },
                {
                    "sent": "For example, in Freebase the types of Melinda she's a CEO, she's a philanthropist.",
                    "label": 0
                },
                {
                    "sent": "She's a alumni of the University, she's American citizen, and for the fictitious Jane Doe, she might be an American citizen, an A small business owner.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the representation of entity is just a simple sum of its types.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is how now this is the complete model is it looks like we have.",
                    "label": 0
                },
                {
                    "sent": "In addition to the representations of the relations, we also have embeddings for the entities here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at the results, the single model with locks, Amex had 70 learning representations.",
                    "label": 0
                },
                {
                    "sent": "Entity gives us a one percentage point boots with types given the maximum score of 73.26.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at the all of the experiments together, our baselines for 65 an with parameter sharing, it gave us some boost with locks, MX more boost and finally we got a 13.7% improvement.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at some of the clauses or some some of the high scoring path switch our model learned.",
                    "label": 0
                },
                {
                    "sent": "For example, we learned that if A was born in place X, an X is commonly known as B in the place of birth of ASP.",
                    "label": 0
                },
                {
                    "sent": "Another thing to note about this learnt clauses that the left hand side of the clauses, previous solution and the right inside our textual relation.",
                    "label": 0
                },
                {
                    "sent": "So this shows the power of Universal Schema which connects relations present in schemas and text.",
                    "label": 0
                },
                {
                    "sent": "This is another non trivial class.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details which we learned.",
                    "label": 0
                },
                {
                    "sent": "Now let's look at path learn by the Entity aware model.",
                    "label": 0
                },
                {
                    "sent": "So the question here was whether Sandy Lake Airport serves this place.",
                    "label": 0
                },
                {
                    "sent": "Sandy Lake First Nation.",
                    "label": 0
                },
                {
                    "sent": "It supplies in Canada now.",
                    "label": 0
                },
                {
                    "sent": "The the path which we scored the maximum by the Entity Aware model is this it says San Diego Airport is in Ontario and Sandy Lake.",
                    "label": 1
                },
                {
                    "sent": "First Nation is in the northwestern part of Ontario.",
                    "label": 0
                },
                {
                    "sent": "But the model which didn't have the notion of entity it learned this long and absurd path really didn't make sense.",
                    "label": 0
                },
                {
                    "sent": "It went to a Music Hall and a water body.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In conclusion, we introduced a single high capacity recurrent unit model for relation extraction.",
                    "label": 0
                },
                {
                    "sent": "And we combined evidence among multiple paths and possibly sources of evidence spanning multiple documents an our model is entity aware, thanks.",
                    "label": 0
                }
            ]
        }
    }
}