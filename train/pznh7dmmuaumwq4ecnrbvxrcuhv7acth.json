{
    "id": "pznh7dmmuaumwq4ecnrbvxrcuhv7acth",
    "title": "Parameter Learning in Probabilistic Databases: A Least Squares Approach",
    "info": {
        "author": [
            "Bernd Gutmann, Department of Computer Science, KU Leuven"
        ],
        "published": "Aug. 25, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlg08_gutmann_plpd/",
    "segmentation": [
        [
            "So this is John Berger.",
            "Angelica look and question and I'm going to talk about something which might sound familiar.",
            "Given that you just heard about probabilistic graph that you just heard something about prob log.",
            "So I know was briefly mentioning that some future work might be to learn the para meters or probabilistic graph, and in fact that is what we are doing in this work here and I want to motivate our setting for the various."
        ],
        [
            "For example.",
            "So.",
            "Imagine you live in a world with only two diseases, and.",
            "You know that there is a new disease X and you are interested in figuring out what causes distances.",
            "You might read the paper which says something about a gene, and this gene might cause a particular disease and you as a medic.",
            "Could conclude maybe there's a connection between these two diseases."
        ],
        [
            "So.",
            "You think these two edges might be in the graph?"
        ],
        [
            "You're not sure.",
            "SSR is research experiments for amedick.",
            "It means you.",
            "Have a look at some people and you will make."
        ],
        [
            "Tests which tells you something about genes.",
            "So you might figure out."
        ],
        [
            "That three of those people carry this particular gene."
        ],
        [
            "And your test will tell you that one of these three suffers from this disease.",
            "So you might conclude that there is a connection between this gene and this disease with 33%."
        ],
        [
            "Another test might tell you that Twitter people of this subgroup suffer from this energy series.",
            "So you might conclude there is a connection between.",
            "Your gene and this other disease with 66% probability so."
        ],
        [
            "Situation here."
        ],
        [
            "Looks is falling.",
            "You have some domain knowledge which is represented now is a graph.",
            "Where you have entities like genes and diseases and maybe some other things, but for the sake of simplicity let."
        ],
        [
            "Not here.",
            "You have some training examples which.",
            "Tell you.",
            "There is a path between nodes in this graph and you know the probability from your tests and your experiments.",
            "Furthermore"
        ],
        [
            "There is uncertainty about your training examples.",
            "For instance you look at.",
            "This training example here.",
            "It might be the case that connection between the gene and this new unknown disease is directly.",
            "Or it might be the case that there is a connection wire.",
            "It is well known as this one.",
            "So it is the setting we are facing and we proposing our learning algorithm which address."
        ],
        [
            "This is.",
            "So you take your graph, you take your training examples and you."
        ],
        [
            "Put them in our system and what you will get back is a probabilistic graph which then has one probability for each edge.",
            "OK, Our system is called Luke Block which is an obvious extension of Pro Block and it means least square perimeter estimation for problem."
        ],
        [
            "OK, here's the outline for the rest of my talk.",
            "I will go into more details with Pro Block and then I will tell you how we are doing this parameter estimation and I will show some experimental."
        ],
        [
            "It's.",
            "OK, you already seen problem and handle slides.",
            "I just wanna know.",
            "Go into a little bit more details.",
            "This small graph from my motivation can be encoded in problem problem in the following way.",
            "You have one fact for each edge and every effect carries a probability.",
            "Furthermore, you have some additional background knowledge for a graph it tells you.",
            "What is the path between edges?",
            "But problem is not only applicable for graphs, you can apply it in any domain.",
            "Then your background knowledge might look different."
        ],
        [
            "What problem defines is a semantic which tells you how likely it is to sample a particular subprogram.",
            "In the graphic sample it tells you with which probability you can sample sub graph.",
            "For instance, you can sample this sub graph with.",
            "Probability written there and you're doing this using tossing a biased coin."
        ],
        [
            "Freddy"
        ],
        [
            "Act"
        ],
        [
            "OK, so you can do this a lot of times and then you get some distribution."
        ],
        [
            "You can use this to answer queries.",
            "In a graph you could ask.",
            "How likely it is that there is a path from?",
            "Gene.",
            "To the disease in a randomly sampled subgraph.",
            "And your semantic tells you that you can do it in the following way."
        ],
        [
            "You look."
        ],
        [
            "At all."
        ],
        [
            "Older."
        ],
        [
            "Subgraphs which have such a path.",
            "In this case, there are five subgraphs and then you simply add up the probability."
        ],
        [
            "Or officer crafts.",
            "However, this is not feasible because there are exponentially many subgraphs, so that's why.",
            "People develop this efficient algorithm using BDD's."
        ],
        [
            "And this algorithm works in the following way.",
            "Here's a here's a graph and you want to know the probability for a path from the gene to the disease.",
            "First step you're doing is you search for all the proofs for this particular theory.",
            "Then you build ability which.",
            "I'm combines all these queries in a very compact way.",
            "Ability is a compact representation of a decision tree.",
            "You can think of starting with Decision Tree and merging similar subtrees.",
            "When you look at the speedy, you can."
        ],
        [
            "Identify this part here, which corresponds to the direct way from the gene to the disease X."
        ],
        [
            "And you can see the second proof in case.",
            "Depicted here, which corresponds to the path while deceased one note.",
            "Once you have this ability, you can use it to calculate the probability for experience, so that's the one working there.",
            "That's what we're interested."
        ],
        [
            "Now."
        ],
        [
            "OK, so the way you're doing is you go three ability bottom up and for every node in your ability you calculate probability up to."
        ],
        [
            "To that note.",
            "So you do this part."
        ],
        [
            "Up and when you end up at the root node, you have the probability for this particular query."
        ],
        [
            "OK, so that's problem we want to do a parimeter learning for problem.",
            "It looks as falling.",
            "You have your.",
            "Proper program as written before, but you don't know the probabilities.",
            "That's why there are question marks.",
            "But what you have is."
        ],
        [
            "You know you have this training examples.",
            "As I said before, in the motivation, the training examples are tuple, switch.",
            "I have a probability and which also have a theory."
        ],
        [
            "I just explained how you can calculate probabilities and prob log and your task now is to.",
            "Adjust the weights or to adjust the probabilities for each fact in a way that these two distributions match.",
            "OK, so you can define an error term.",
            "What we?"
        ],
        [
            "That is, we simply take the mean squared error over this distribution.",
            "When this error term is zero, we have.",
            "Learn our we can represent our training samples.",
            "But in general, this is a.",
            "This is a non convex problem so.",
            "The local minima and also it might be the case that you're not able to completely represent your."
        ],
        [
            "Owning data.",
            "OK.",
            "Given now we have this error term.",
            "There's a standard way too.",
            "To optimize it, simply gradient descent.",
            "And."
        ],
        [
            "It works in the following way.",
            "You initialize your probabilities for effects randomly, so you start somewhere at in your space."
        ],
        [
            "Then as long as your error is high.",
            "You simply calculate the gradient.",
            "The gradient tells you in which direction you have to work to.",
            "To get an improvement.",
            "In order to calculate it, creating."
        ],
        [
            "Our case you have to find the proofs for your queries and you have to build a speedy size short for."
        ],
        [
            "Once you have this PDZ, we can calculate the gradient never showed it on next slide.",
            "When you have to grade."
        ],
        [
            "And you added to the probabilities of the facts.",
            "So you take one step in the direction."
        ],
        [
            "And you iterate.",
            "And in the end you return the probabilities you just learned.",
            "OK, calculating the gradient.",
            "Is again very hard given that you have exponentially many subprograms, right?",
            "But you can easily adapt the algorithm I showed before to calculate the gradient."
        ],
        [
            "It works way.",
            "You have your graph and you want to calculate the gradient of this query with respect to particular edge.",
            "So you want to.",
            "You want to say capture gradient with respect to the edge between disease one and this is X, so you have to build the speedy.",
            "Anne.",
            "You then basically run the same algorithm to calculate the value of the gradient."
        ],
        [
            "But what you have to do is you need to do some additional bookkeeping for every note you have to keep track of whether or not you can reach the node you want to derive the gradient for on this path up to there."
        ],
        [
            "When you reach the root node and you have marked the root node, you are sure that you can reach the.",
            "Know what you want to drive the gradient for.",
            "Otherwise you have to return zero and again the value you attach to the root node is to well yester gradient.",
            "OK, so.",
            "Now we have a fast way to calculate the value of the gradient and we can use this gradient descent algorithm to optimize the error.",
            "An interesting feature of the setting is.",
            "That you can also learn from proofs.",
            "What does it mean talking about furious witch?",
            "Means that you.",
            "Are asking for, let's say a pop and you're interested in any path, right?",
            "But a proof tells you not only that there is a path, it would also tell you the exact path.",
            "And.",
            "The interesting.",
            "Feature of this is that you can easily incorporate proofs in your training."
        ],
        [
            "Samples.",
            "So have a look at this query here."
        ],
        [
            "There are two explanations.",
            "Either you can have the direct path between these two genes so it can have to pathwire deceased one node.",
            "So and there is some uncertainty, but as I said, in some applications you might have to be exact."
        ],
        [
            "Anne."
        ],
        [
            "And.",
            "You can then represent this proof as a query is simply the conjunction of all facts on this path."
        ],
        [
            "OK, and what you can do with this is you.",
            "Can add this proofs for your training examples instead of the queries.",
            "This will then speed up your learning process because you don't have to calculate BDD.",
            "You don't have to search for the proofs, so you will be faster and it's also easier to.",
            "For the learning algorithm, requested problem is less constrained in the sense.",
            "Sorry, more constrained.",
            "OK."
        ],
        [
            "So, um.",
            "Wait?",
            "Get some experience with with this.",
            "I was talking about this bio mine project with this huge graph we are learning here so we have to talk about smaller problems.",
            "So we extracted the subgraph from.",
            "Graph.",
            "And we did this by picking some jeans in this graph, namely some asthma genes, and then taking the surrounding of this jeans.",
            "And this gave us is sub graph of 241 inches and 127 nodes.",
            "And I want to point out that.",
            "Learning in this setting means that you have to adjust 241 para meters because you have to adjust 241 probabilities.",
            "OK, what we did after we extracted this sub graph, we sampled randomly just uniformly pairs of nodes in this graph and calculated the path probability between these two nodes.",
            "So we calculated how likely it is that there is a path between two nodes.",
            "So this gave us 500 training samples as I mentioned in my introduction.",
            "Then we threw away the probabilities of this graph and we tried to recover the probabilities again using our algorithm.",
            "Um?",
            "We did this.",
            "Public implementation and adding this calculation of the gradient.",
            "All this is implemented in rolark and straightforward adaption."
        ],
        [
            "Um?",
            "OK.",
            "Here's the result on the probabilities.",
            "On X axis you can see the number of iterations and you can see on the Y axis.",
            "The error on the probabilities, the lower the error.",
            "The closer you are to the original probabilities of the facts.",
            "What you can see is as the algorithm proceeds.",
            "The error goes down.",
            "What you can also see is that the more training examples you give your algorithm, the closer you come to original probability.",
            "The reason for this is we want to learn.",
            "As I said, 241 probabilities with only 90 training examples.",
            "So we have a highly underspecified problem, right?",
            "And we're not actually optimizing the.",
            "The.",
            "We're not actually recovering the original probability probabilities.",
            "We try to match our distribution in a way that we can fit the training examples.",
            "So it's not surprising that you cannot reach the original probabilities with little training.",
            "But when you give your ever more training examples, you can get closer, but you can also see is that with more training examples to plot starts earlier simply.",
            "Doing all this because the operations is very expensive and that's also reason why it's good when you have proof that you can use your proof."
        ],
        [
            "OK, the second plot I want to show you is the error on the test data.",
            "It's the same experiment, just.",
            "Another error criterion, as you can see, is.",
            "And with every iteration, your algorithm reduces the error on the test set more and more.",
            "What you can also see is with more training examples.",
            "Error is reduced faster.",
            "Um?",
            "Here on this plot the error goes down close to zero when you give more training examples.",
            "That our algorithm simply recovers or album simply tries to fit this.",
            "The probabilities such that this error is minimized, so you can expect this error goes down.",
            "I was mentioning before that you can incorporate proofs and your training examples and that you can get better."
        ],
        [
            "Did an experiment on that and here's the results for this.",
            "On the Y on the X axis you can see.",
            "How much of your training data?",
            "What was given as proof when you have?",
            "At zero you give every example as a query.",
            "Every example as a proof.",
            "And you can see that the more examples you give as a proof.",
            "The lower your arrogance.",
            "Both on the on the training on the test data.",
            "And on the probabilities of your effects.",
            "So this is an indicator for.",
            "For what I said.",
            "That when you have proofs you.",
            "You you can make use of it.",
            "What this doesn't show is that with proofs you can also reduce your runtime because you don't have to search for proof simply because you know them and you don't have to build the BDS because when you have this prove it's a very simple cure, but just as a conjunction of the facts and you can.",
            "Calculate the value of your.",
            "Note that the probability of such a query and the gradient of this query just by going once over all facts and multiplying the probabilities.",
            "OK."
        ],
        [
            "OK to summarize.",
            "Um?",
            "I was talking about her meter running for.",
            "Prop block.",
            "And I was talking about.",
            "Learning this probabilities for the graphs.",
            "In fact, our approach is not only.",
            "Applicable in this graph domain, you can apply it everywhere where you can write down your pro on your problem as a proper program.",
            "Um?",
            "You saw that we can easily incorporate proofs and we can make use of this additional information.",
            "Our learning algorithm is.",
            "As efficient as the inference algorithm, because we can make use of the same structure in the same BDS.",
            "And.",
            "In principle you can use the same setting for every probabilistic database.",
            "We can calculate the value of where you can calculate gradient of career.",
            "OK thanks.",
            "So did you need to use any special mechanism to keep the probabilities in the 01 range?",
            "Yes, I didn't talk about that just to keep it simple.",
            "In fact, you just repairment riser such space using the sigmoid function.",
            "And then you can go over the complete range of the real numbers.",
            "And not just, it's just one.",
            "Just plug in the sigmoid function in your probabilities and it's just a chain rule, so it's straightforward.",
            "How come is there no overfitting?",
            "I mean, you can reduce the test set error arbitrarily well.",
            "You thought it over."
        ],
        [
            "Kidding, I just didn't name it.",
            "Um?",
            "So this graph just shows you the error on the test set, but you can see the error on the training set always goes down to 0, right?",
            "And what you see is only 90 training examples and 10 test examples.",
            "The error on your test set is on average much higher than when you give more training examples and therefore also more test examples.",
            "So.",
            "There is overfitting.",
            "But because we have a very limited search space, you just have probabilities and you cannot do any structure, search or fitting is not that much problem in this application."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is John Berger.",
                    "label": 0
                },
                {
                    "sent": "Angelica look and question and I'm going to talk about something which might sound familiar.",
                    "label": 0
                },
                {
                    "sent": "Given that you just heard about probabilistic graph that you just heard something about prob log.",
                    "label": 0
                },
                {
                    "sent": "So I know was briefly mentioning that some future work might be to learn the para meters or probabilistic graph, and in fact that is what we are doing in this work here and I want to motivate our setting for the various.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Imagine you live in a world with only two diseases, and.",
                    "label": 0
                },
                {
                    "sent": "You know that there is a new disease X and you are interested in figuring out what causes distances.",
                    "label": 0
                },
                {
                    "sent": "You might read the paper which says something about a gene, and this gene might cause a particular disease and you as a medic.",
                    "label": 0
                },
                {
                    "sent": "Could conclude maybe there's a connection between these two diseases.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You think these two edges might be in the graph?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're not sure.",
                    "label": 0
                },
                {
                    "sent": "SSR is research experiments for amedick.",
                    "label": 0
                },
                {
                    "sent": "It means you.",
                    "label": 0
                },
                {
                    "sent": "Have a look at some people and you will make.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tests which tells you something about genes.",
                    "label": 0
                },
                {
                    "sent": "So you might figure out.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That three of those people carry this particular gene.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And your test will tell you that one of these three suffers from this disease.",
                    "label": 0
                },
                {
                    "sent": "So you might conclude that there is a connection between this gene and this disease with 33%.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another test might tell you that Twitter people of this subgroup suffer from this energy series.",
                    "label": 0
                },
                {
                    "sent": "So you might conclude there is a connection between.",
                    "label": 0
                },
                {
                    "sent": "Your gene and this other disease with 66% probability so.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Situation here.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks is falling.",
                    "label": 0
                },
                {
                    "sent": "You have some domain knowledge which is represented now is a graph.",
                    "label": 1
                },
                {
                    "sent": "Where you have entities like genes and diseases and maybe some other things, but for the sake of simplicity let.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not here.",
                    "label": 0
                },
                {
                    "sent": "You have some training examples which.",
                    "label": 0
                },
                {
                    "sent": "Tell you.",
                    "label": 0
                },
                {
                    "sent": "There is a path between nodes in this graph and you know the probability from your tests and your experiments.",
                    "label": 0
                },
                {
                    "sent": "Furthermore",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is uncertainty about your training examples.",
                    "label": 0
                },
                {
                    "sent": "For instance you look at.",
                    "label": 0
                },
                {
                    "sent": "This training example here.",
                    "label": 0
                },
                {
                    "sent": "It might be the case that connection between the gene and this new unknown disease is directly.",
                    "label": 0
                },
                {
                    "sent": "Or it might be the case that there is a connection wire.",
                    "label": 0
                },
                {
                    "sent": "It is well known as this one.",
                    "label": 0
                },
                {
                    "sent": "So it is the setting we are facing and we proposing our learning algorithm which address.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "So you take your graph, you take your training examples and you.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put them in our system and what you will get back is a probabilistic graph which then has one probability for each edge.",
                    "label": 0
                },
                {
                    "sent": "OK, Our system is called Luke Block which is an obvious extension of Pro Block and it means least square perimeter estimation for problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's the outline for the rest of my talk.",
                    "label": 0
                },
                {
                    "sent": "I will go into more details with Pro Block and then I will tell you how we are doing this parameter estimation and I will show some experimental.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "OK, you already seen problem and handle slides.",
                    "label": 0
                },
                {
                    "sent": "I just wanna know.",
                    "label": 0
                },
                {
                    "sent": "Go into a little bit more details.",
                    "label": 0
                },
                {
                    "sent": "This small graph from my motivation can be encoded in problem problem in the following way.",
                    "label": 0
                },
                {
                    "sent": "You have one fact for each edge and every effect carries a probability.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, you have some additional background knowledge for a graph it tells you.",
                    "label": 0
                },
                {
                    "sent": "What is the path between edges?",
                    "label": 0
                },
                {
                    "sent": "But problem is not only applicable for graphs, you can apply it in any domain.",
                    "label": 0
                },
                {
                    "sent": "Then your background knowledge might look different.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What problem defines is a semantic which tells you how likely it is to sample a particular subprogram.",
                    "label": 0
                },
                {
                    "sent": "In the graphic sample it tells you with which probability you can sample sub graph.",
                    "label": 0
                },
                {
                    "sent": "For instance, you can sample this sub graph with.",
                    "label": 0
                },
                {
                    "sent": "Probability written there and you're doing this using tossing a biased coin.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Freddy",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Act",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you can do this a lot of times and then you get some distribution.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can use this to answer queries.",
                    "label": 0
                },
                {
                    "sent": "In a graph you could ask.",
                    "label": 0
                },
                {
                    "sent": "How likely it is that there is a path from?",
                    "label": 0
                },
                {
                    "sent": "Gene.",
                    "label": 0
                },
                {
                    "sent": "To the disease in a randomly sampled subgraph.",
                    "label": 0
                },
                {
                    "sent": "And your semantic tells you that you can do it in the following way.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You look.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At all.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Older.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subgraphs which have such a path.",
                    "label": 0
                },
                {
                    "sent": "In this case, there are five subgraphs and then you simply add up the probability.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or officer crafts.",
                    "label": 0
                },
                {
                    "sent": "However, this is not feasible because there are exponentially many subgraphs, so that's why.",
                    "label": 0
                },
                {
                    "sent": "People develop this efficient algorithm using BDD's.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this algorithm works in the following way.",
                    "label": 0
                },
                {
                    "sent": "Here's a here's a graph and you want to know the probability for a path from the gene to the disease.",
                    "label": 0
                },
                {
                    "sent": "First step you're doing is you search for all the proofs for this particular theory.",
                    "label": 0
                },
                {
                    "sent": "Then you build ability which.",
                    "label": 0
                },
                {
                    "sent": "I'm combines all these queries in a very compact way.",
                    "label": 0
                },
                {
                    "sent": "Ability is a compact representation of a decision tree.",
                    "label": 0
                },
                {
                    "sent": "You can think of starting with Decision Tree and merging similar subtrees.",
                    "label": 0
                },
                {
                    "sent": "When you look at the speedy, you can.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Identify this part here, which corresponds to the direct way from the gene to the disease X.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can see the second proof in case.",
                    "label": 0
                },
                {
                    "sent": "Depicted here, which corresponds to the path while deceased one note.",
                    "label": 0
                },
                {
                    "sent": "Once you have this ability, you can use it to calculate the probability for experience, so that's the one working there.",
                    "label": 0
                },
                {
                    "sent": "That's what we're interested.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the way you're doing is you go three ability bottom up and for every node in your ability you calculate probability up to.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To that note.",
                    "label": 0
                },
                {
                    "sent": "So you do this part.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up and when you end up at the root node, you have the probability for this particular query.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's problem we want to do a parimeter learning for problem.",
                    "label": 0
                },
                {
                    "sent": "It looks as falling.",
                    "label": 0
                },
                {
                    "sent": "You have your.",
                    "label": 0
                },
                {
                    "sent": "Proper program as written before, but you don't know the probabilities.",
                    "label": 0
                },
                {
                    "sent": "That's why there are question marks.",
                    "label": 0
                },
                {
                    "sent": "But what you have is.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know you have this training examples.",
                    "label": 0
                },
                {
                    "sent": "As I said before, in the motivation, the training examples are tuple, switch.",
                    "label": 0
                },
                {
                    "sent": "I have a probability and which also have a theory.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just explained how you can calculate probabilities and prob log and your task now is to.",
                    "label": 0
                },
                {
                    "sent": "Adjust the weights or to adjust the probabilities for each fact in a way that these two distributions match.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can define an error term.",
                    "label": 0
                },
                {
                    "sent": "What we?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, we simply take the mean squared error over this distribution.",
                    "label": 0
                },
                {
                    "sent": "When this error term is zero, we have.",
                    "label": 0
                },
                {
                    "sent": "Learn our we can represent our training samples.",
                    "label": 0
                },
                {
                    "sent": "But in general, this is a.",
                    "label": 0
                },
                {
                    "sent": "This is a non convex problem so.",
                    "label": 0
                },
                {
                    "sent": "The local minima and also it might be the case that you're not able to completely represent your.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Owning data.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Given now we have this error term.",
                    "label": 0
                },
                {
                    "sent": "There's a standard way too.",
                    "label": 0
                },
                {
                    "sent": "To optimize it, simply gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works in the following way.",
                    "label": 0
                },
                {
                    "sent": "You initialize your probabilities for effects randomly, so you start somewhere at in your space.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then as long as your error is high.",
                    "label": 0
                },
                {
                    "sent": "You simply calculate the gradient.",
                    "label": 0
                },
                {
                    "sent": "The gradient tells you in which direction you have to work to.",
                    "label": 0
                },
                {
                    "sent": "To get an improvement.",
                    "label": 0
                },
                {
                    "sent": "In order to calculate it, creating.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our case you have to find the proofs for your queries and you have to build a speedy size short for.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once you have this PDZ, we can calculate the gradient never showed it on next slide.",
                    "label": 0
                },
                {
                    "sent": "When you have to grade.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you added to the probabilities of the facts.",
                    "label": 0
                },
                {
                    "sent": "So you take one step in the direction.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you iterate.",
                    "label": 0
                },
                {
                    "sent": "And in the end you return the probabilities you just learned.",
                    "label": 0
                },
                {
                    "sent": "OK, calculating the gradient.",
                    "label": 0
                },
                {
                    "sent": "Is again very hard given that you have exponentially many subprograms, right?",
                    "label": 0
                },
                {
                    "sent": "But you can easily adapt the algorithm I showed before to calculate the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It works way.",
                    "label": 0
                },
                {
                    "sent": "You have your graph and you want to calculate the gradient of this query with respect to particular edge.",
                    "label": 0
                },
                {
                    "sent": "So you want to.",
                    "label": 0
                },
                {
                    "sent": "You want to say capture gradient with respect to the edge between disease one and this is X, so you have to build the speedy.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You then basically run the same algorithm to calculate the value of the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what you have to do is you need to do some additional bookkeeping for every note you have to keep track of whether or not you can reach the node you want to derive the gradient for on this path up to there.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When you reach the root node and you have marked the root node, you are sure that you can reach the.",
                    "label": 0
                },
                {
                    "sent": "Know what you want to drive the gradient for.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you have to return zero and again the value you attach to the root node is to well yester gradient.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now we have a fast way to calculate the value of the gradient and we can use this gradient descent algorithm to optimize the error.",
                    "label": 1
                },
                {
                    "sent": "An interesting feature of the setting is.",
                    "label": 0
                },
                {
                    "sent": "That you can also learn from proofs.",
                    "label": 0
                },
                {
                    "sent": "What does it mean talking about furious witch?",
                    "label": 0
                },
                {
                    "sent": "Means that you.",
                    "label": 0
                },
                {
                    "sent": "Are asking for, let's say a pop and you're interested in any path, right?",
                    "label": 0
                },
                {
                    "sent": "But a proof tells you not only that there is a path, it would also tell you the exact path.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The interesting.",
                    "label": 0
                },
                {
                    "sent": "Feature of this is that you can easily incorporate proofs in your training.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples.",
                    "label": 0
                },
                {
                    "sent": "So have a look at this query here.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are two explanations.",
                    "label": 0
                },
                {
                    "sent": "Either you can have the direct path between these two genes so it can have to pathwire deceased one node.",
                    "label": 0
                },
                {
                    "sent": "So and there is some uncertainty, but as I said, in some applications you might have to be exact.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "You can then represent this proof as a query is simply the conjunction of all facts on this path.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and what you can do with this is you.",
                    "label": 0
                },
                {
                    "sent": "Can add this proofs for your training examples instead of the queries.",
                    "label": 0
                },
                {
                    "sent": "This will then speed up your learning process because you don't have to calculate BDD.",
                    "label": 0
                },
                {
                    "sent": "You don't have to search for the proofs, so you will be faster and it's also easier to.",
                    "label": 0
                },
                {
                    "sent": "For the learning algorithm, requested problem is less constrained in the sense.",
                    "label": 0
                },
                {
                    "sent": "Sorry, more constrained.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "Get some experience with with this.",
                    "label": 0
                },
                {
                    "sent": "I was talking about this bio mine project with this huge graph we are learning here so we have to talk about smaller problems.",
                    "label": 0
                },
                {
                    "sent": "So we extracted the subgraph from.",
                    "label": 0
                },
                {
                    "sent": "Graph.",
                    "label": 0
                },
                {
                    "sent": "And we did this by picking some jeans in this graph, namely some asthma genes, and then taking the surrounding of this jeans.",
                    "label": 0
                },
                {
                    "sent": "And this gave us is sub graph of 241 inches and 127 nodes.",
                    "label": 0
                },
                {
                    "sent": "And I want to point out that.",
                    "label": 0
                },
                {
                    "sent": "Learning in this setting means that you have to adjust 241 para meters because you have to adjust 241 probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK, what we did after we extracted this sub graph, we sampled randomly just uniformly pairs of nodes in this graph and calculated the path probability between these two nodes.",
                    "label": 0
                },
                {
                    "sent": "So we calculated how likely it is that there is a path between two nodes.",
                    "label": 0
                },
                {
                    "sent": "So this gave us 500 training samples as I mentioned in my introduction.",
                    "label": 0
                },
                {
                    "sent": "Then we threw away the probabilities of this graph and we tried to recover the probabilities again using our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We did this.",
                    "label": 0
                },
                {
                    "sent": "Public implementation and adding this calculation of the gradient.",
                    "label": 0
                },
                {
                    "sent": "All this is implemented in rolark and straightforward adaption.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Here's the result on the probabilities.",
                    "label": 0
                },
                {
                    "sent": "On X axis you can see the number of iterations and you can see on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "The error on the probabilities, the lower the error.",
                    "label": 0
                },
                {
                    "sent": "The closer you are to the original probabilities of the facts.",
                    "label": 0
                },
                {
                    "sent": "What you can see is as the algorithm proceeds.",
                    "label": 0
                },
                {
                    "sent": "The error goes down.",
                    "label": 0
                },
                {
                    "sent": "What you can also see is that the more training examples you give your algorithm, the closer you come to original probability.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is we want to learn.",
                    "label": 0
                },
                {
                    "sent": "As I said, 241 probabilities with only 90 training examples.",
                    "label": 0
                },
                {
                    "sent": "So we have a highly underspecified problem, right?",
                    "label": 0
                },
                {
                    "sent": "And we're not actually optimizing the.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "We're not actually recovering the original probability probabilities.",
                    "label": 0
                },
                {
                    "sent": "We try to match our distribution in a way that we can fit the training examples.",
                    "label": 0
                },
                {
                    "sent": "So it's not surprising that you cannot reach the original probabilities with little training.",
                    "label": 0
                },
                {
                    "sent": "But when you give your ever more training examples, you can get closer, but you can also see is that with more training examples to plot starts earlier simply.",
                    "label": 0
                },
                {
                    "sent": "Doing all this because the operations is very expensive and that's also reason why it's good when you have proof that you can use your proof.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the second plot I want to show you is the error on the test data.",
                    "label": 0
                },
                {
                    "sent": "It's the same experiment, just.",
                    "label": 0
                },
                {
                    "sent": "Another error criterion, as you can see, is.",
                    "label": 0
                },
                {
                    "sent": "And with every iteration, your algorithm reduces the error on the test set more and more.",
                    "label": 0
                },
                {
                    "sent": "What you can also see is with more training examples.",
                    "label": 0
                },
                {
                    "sent": "Error is reduced faster.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Here on this plot the error goes down close to zero when you give more training examples.",
                    "label": 0
                },
                {
                    "sent": "That our algorithm simply recovers or album simply tries to fit this.",
                    "label": 0
                },
                {
                    "sent": "The probabilities such that this error is minimized, so you can expect this error goes down.",
                    "label": 0
                },
                {
                    "sent": "I was mentioning before that you can incorporate proofs and your training examples and that you can get better.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did an experiment on that and here's the results for this.",
                    "label": 0
                },
                {
                    "sent": "On the Y on the X axis you can see.",
                    "label": 0
                },
                {
                    "sent": "How much of your training data?",
                    "label": 0
                },
                {
                    "sent": "What was given as proof when you have?",
                    "label": 0
                },
                {
                    "sent": "At zero you give every example as a query.",
                    "label": 0
                },
                {
                    "sent": "Every example as a proof.",
                    "label": 1
                },
                {
                    "sent": "And you can see that the more examples you give as a proof.",
                    "label": 0
                },
                {
                    "sent": "The lower your arrogance.",
                    "label": 0
                },
                {
                    "sent": "Both on the on the training on the test data.",
                    "label": 0
                },
                {
                    "sent": "And on the probabilities of your effects.",
                    "label": 0
                },
                {
                    "sent": "So this is an indicator for.",
                    "label": 0
                },
                {
                    "sent": "For what I said.",
                    "label": 0
                },
                {
                    "sent": "That when you have proofs you.",
                    "label": 0
                },
                {
                    "sent": "You you can make use of it.",
                    "label": 0
                },
                {
                    "sent": "What this doesn't show is that with proofs you can also reduce your runtime because you don't have to search for proof simply because you know them and you don't have to build the BDS because when you have this prove it's a very simple cure, but just as a conjunction of the facts and you can.",
                    "label": 0
                },
                {
                    "sent": "Calculate the value of your.",
                    "label": 0
                },
                {
                    "sent": "Note that the probability of such a query and the gradient of this query just by going once over all facts and multiplying the probabilities.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK to summarize.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I was talking about her meter running for.",
                    "label": 0
                },
                {
                    "sent": "Prop block.",
                    "label": 0
                },
                {
                    "sent": "And I was talking about.",
                    "label": 0
                },
                {
                    "sent": "Learning this probabilities for the graphs.",
                    "label": 0
                },
                {
                    "sent": "In fact, our approach is not only.",
                    "label": 0
                },
                {
                    "sent": "Applicable in this graph domain, you can apply it everywhere where you can write down your pro on your problem as a proper program.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You saw that we can easily incorporate proofs and we can make use of this additional information.",
                    "label": 0
                },
                {
                    "sent": "Our learning algorithm is.",
                    "label": 0
                },
                {
                    "sent": "As efficient as the inference algorithm, because we can make use of the same structure in the same BDS.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In principle you can use the same setting for every probabilistic database.",
                    "label": 0
                },
                {
                    "sent": "We can calculate the value of where you can calculate gradient of career.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "So did you need to use any special mechanism to keep the probabilities in the 01 range?",
                    "label": 0
                },
                {
                    "sent": "Yes, I didn't talk about that just to keep it simple.",
                    "label": 0
                },
                {
                    "sent": "In fact, you just repairment riser such space using the sigmoid function.",
                    "label": 0
                },
                {
                    "sent": "And then you can go over the complete range of the real numbers.",
                    "label": 0
                },
                {
                    "sent": "And not just, it's just one.",
                    "label": 0
                },
                {
                    "sent": "Just plug in the sigmoid function in your probabilities and it's just a chain rule, so it's straightforward.",
                    "label": 0
                },
                {
                    "sent": "How come is there no overfitting?",
                    "label": 0
                },
                {
                    "sent": "I mean, you can reduce the test set error arbitrarily well.",
                    "label": 0
                },
                {
                    "sent": "You thought it over.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kidding, I just didn't name it.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this graph just shows you the error on the test set, but you can see the error on the training set always goes down to 0, right?",
                    "label": 0
                },
                {
                    "sent": "And what you see is only 90 training examples and 10 test examples.",
                    "label": 1
                },
                {
                    "sent": "The error on your test set is on average much higher than when you give more training examples and therefore also more test examples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There is overfitting.",
                    "label": 0
                },
                {
                    "sent": "But because we have a very limited search space, you just have probabilities and you cannot do any structure, search or fitting is not that much problem in this application.",
                    "label": 0
                }
            ]
        }
    }
}