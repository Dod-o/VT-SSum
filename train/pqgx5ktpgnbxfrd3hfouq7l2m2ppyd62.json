{
    "id": "pqgx5ktpgnbxfrd3hfouq7l2m2ppyd62",
    "title": "Non-Isometric Manifold Learning: Analysis and an Algorithm",
    "info": {
        "author": [
            "Piotr Doll\u00e1r, University of California"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Manifold Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_dollar_niml/",
    "segmentation": [
        [
            "Papers on manifold and reduction in the 1st paper is by a group from UCS DQ dollar.",
            "Vincent Obolon sociology have written the paper non isometric manifold learning analysis algorithms and children under percent.",
            "Hi, thank you for the introduction."
        ],
        [
            "So in this work, we're going to be focusing on two things.",
            "The first is we wanted to extend a little bit manifold nonlinear manifold learning to something that is beyond embedding.",
            "We have shown a lot of very nice applications of embedding nonlinear manifolds into a lower dimensional space, but our intuition is that there's a lot more we could actually do with nonlinear manifolds.",
            "Other than embedding and to do so, one of the first things we need to establish some kind of either test error or generalization performance.",
            "Some objective way to say if our manifold learning technique is doing something reasonable?",
            "If we're doing just embedding that we could potentially just look at the quality of an embedding and say just visually through visual assessment.",
            "Say this is a reasonable embedding, but we're going to be trying to do operations on non isometric animals were not going to be able to do embedding or any kind of visualization.",
            "Really want more of some kind of notion of some objective notion of how well is it doing and some some ability to generalize beyond maybe just the data we've seen?",
            "So what is it that we actually want to do with nonlinear manifold learning it?"
        ],
        [
            "Not embedding well, let's consider saying what we can do with actually linear manifolds, i.e.",
            "Just if we know the data lies on a subspace.",
            "So with this kind of data you know you might be given data and you noisy data and you find just using PCA or something.",
            "Just find subspace that best fits the data that minimize the various and then operations.",
            "You might want to do is project onto that subspace, measured the distance between two points after projection, measure distance to the subspace which you can project is the same thing.",
            "And importantly, we want to be able to do it.",
            "You know, we might learn the subspace from some set of data, but we might want to be able to.",
            "We have a very good idea of what that subspace looks everywhere else.",
            "So even in regions where either we did, we just had data around it, but not right there, or kind of extrapolate beyond where we had data in for a linear subspace that's very obvious."
        ],
        [
            "Right, so the idea is well, can we extend this to basically same?",
            "Operations, but for a nonlinear manifold, so project onto the manifold.",
            "In this case of projection, need not be unique.",
            "Measure distance to manifold, just the distance of the closest projection merger geodesic distance on a manifold, and again generalize to unseen regions, and so the idea here is we can do this for linear manifolds.",
            "And can we do this for nonlinear manifolds?"
        ],
        [
            "So the representation we use for manifolds that allows us to do these operations is going to be representing a manifold by its tangent space, and this is work we presented earlier year ago at NIPS and Bang.",
            "You and others had some work prior to that at NIPS 05.",
            "So I'm going to just give up pretty brief overview of learning the tangent space.",
            "More details are in the paper."
        ],
        [
            "Or the poster.",
            "But the basic idea is we know we have data that lies in a low dimensional manifold.",
            "Dimensional space, so it'll be throughout this presentation to represent the dimensionality of the manifold and big data represents dimensionality of the space.",
            "And so the idea is what we're going to want to do is learn a mapping from a point."
        ],
        [
            "The Manifold 2 its tangent basis.",
            "So point in amount of folders.",
            "Just some point in our big D. Ann is tangent basis is a matrix with little D. Columns where each column is just some basis vector of the manifold of the tangent space at that point on a manifold.",
            "So what exactly does that mean?",
            "That means every one of those vectors is just a derivative.",
            "If you so X is just, there's a mapping from Y which is in the lower dimensional space.",
            "And if we take that derivative respect to the various components of why that will give us the tangent vectors?",
            "So pretty straightforward, and so now the idea is to learn this function Curly H. That will take us from this point in high dimensional space to each tangent space.",
            "And we're going to learn it from points from the data.",
            "We're just going to be points samples from the manifold, as in the case of when people do embedding and we're going to learn this function H."
        ],
        [
            "Based on that.",
            "So what's the idea?",
            "Well, the idea is if you're given 2 nearby points, then the difference between those points.",
            "So if you're given two points on a manifold XI XJ that you know in nearby, then the difference between those points to Delta should be a linear combination of your tangent vectors at that point on the manifold.",
            "So this is pretty intuitive and no, the idea is if.",
            "If you actually were given in some sense the line tangent vectors, as in the case on the left here, then you can learn a regression from the point on the manifold to the first tangent vector and from the point the second tender vector, and so on.",
            "The problem is in real data all you have is vectors, kind of pointing to your neighbors these Delta vectors, so they're kind of pointing in all different directions, so you can't just learn a straight regression.",
            "But if you knew the alignment, then you could get.",
            "You could learn the regression, and if you knew the regression, you can learn to alignment.",
            "So that's the intuition behind the algorithm.",
            "So the actual loss function that we defined to learn this is we say, so H again H of data.",
            "That's going to be a function we're learning.",
            "Theta is the parameters for that function of a point X bar.",
            "Those are the tangent vectors.",
            "The parameter epsilon is a free parameter that's going to be the alignment for the vectors and that should match the deltas for all neighboring points.",
            "I all points that are near each other, so we don't want to do this for points that are far apart, 'cause it's probably only hold if points are close together enough together that essentially linear approximations to manifold is good, and in fact actually we define it this way by the tangent that between the two points we actually just need a quadratic approximations.",
            "The manifold told it's just.",
            "Centered in Uncentered derivative.",
            "So we're just going to minimizing this express."
        ],
        [
            "And I actually haven't said what the form for HR regression function is, and so it's actually we're just going to use a very simple form, a linear function over some features, and our features are just going to be using radial basis functions, so the details of the math here aren't so important.",
            "I don't expect anyone followed by following the notation, but it actually turns out that this is the entirety of the algorithm.",
            "Once you do derivations.",
            "So essentially at each phase.",
            "So the first each so there's.",
            "It's an alternating minimization in each of the two iterations.",
            "You're doing least squares problem, so it's very fast, so you solve least squares to get the epsilon.",
            "To get the alignment, and then you solve the least squares to get the regression.",
            "Since just least function, you just keep alternating between these until you get to a local minima and you might restart this a couple of times so you don't get stuck in a bad minimum.",
            "So again, the details aren't important, but it actually turns out to be a very nice simple algorithm that works very quickly and is fairly effective."
        ],
        [
            "As hopefully I'll be able to show up.",
            "OK, yes."
        ],
        [
            "I'm sorry.",
            "Yes, and I'll go into that a little more.",
            "A little later, yes, but for this you actually need to know little deep when you're doing this."
        ],
        [
            "So the first thing we said, well, OK, so we have this algorithm, but how do we actually know if it's doing well?",
            "So with embedding, people have defined a lot of different loss functions they can optimize to get a different embedding and they can say, you know, we find the global minima to this loss function, but how do you actually then compare which one is better?",
            "You know, because we're going to have different parameters like the number of basis functions, the number of neighbors, and so on.",
            "And again, we're just going to apply this methodology to non isometric manifolds, which isn't just for embedding so."
        ],
        [
            "Start with something very simple and intuitive, which is suppose we had a manifold that we can densely sample so.",
            "Kind of either.",
            "We know the transformation that's generating the manifold, or we know the analytic form of the manifold.",
            "So in some sense controlled data.",
            "Now what we can do now is sample two sets of data, one that we use for training and one that we used to evaluate the air.",
            "An idea is the one to value the error is going to be very very large sample set in the training set or which we're going to use for embedding or.",
            "However, when a trainer algorithm is going to be smaller measuring.",
            "Finite sample performance, and so the idea is very simple if.",
            "Let's say you know.",
            "Well, so first of all the ISOMAP algorithm by Tenenbaum at all let's you compute geodesic distance between points on a manifold just by doing the shortest path right?",
            "And then you could embed that using multidimensional scaling.",
            "But the point is if you have a very highly densely sampled manifold, you can in fact get a very very accurate estimate that's guaranteed to converge as the number of samples goes to Infinity of the geodesic distance.",
            "Now if you have a manifold.",
            "That you know is isometric to convex subset of Euclidean space.",
            "That means that if you embed it, you can embed it without introducing any kind of distortion in such a way that after embedding the Euclidean distance in embedded space is equivalent to the geodesic distance on the manifold, so we'd expect so various embedding algorithms can work with data that's not isometric to convex set, or that's not even isometric to Euclidean space.",
            "But the point is, for this subset, this restricted subset of manifold, we'd expect that any man bedding method or any other process should at least.",
            "Embed these without introducing distortion so we can use this week and basically compare using our very large set, which I'm just informally called S Infinity.",
            "We compute the ground truth geodesic distance and then we can come using our smaller set.",
            "We can run our embedding algorithm or whatever kind of algorithm we're using and then measure the Euclidean distance and the difference between those gives us some very objective sense of error so.",
            "We rounded so the first thing we did was said, OK, well, let's just for interest sake, compare the embedding performance of a number of algorithms.",
            "And again the focus of our work is not necessarily embedding, but."
        ],
        [
            "Still kind of an interesting thing to do, So what I'm showing here.",
            "This is a very well known half to Death S curve.",
            "And what I'm plotting here is this error GD which I defined the previous slide versus the number of samples we use for embedding for Eli maximum variance unfolding ISOMAP and LML.",
            "And so you know, as you can see, as the number of samples, an increase is all the algorithms except perhaps Eli get very low air and what's kind of interesting and this is a log scale on the X axis and what's interesting is a performance for low values of N and there's two versions of Elsa melon that's not so important here.",
            "But as you can see.",
            "Allison Melton so perform better than other methods.",
            "Isomap, second, best, and envy you.",
            "To be fair, and EU is more applicable.",
            "More generally applicable than Isomap and Ellie is more generally applicable than to remainder.",
            "Of course, also Mel.",
            "We want it to be applicable to non isometric manifolds, so a much larger class of manifold.",
            "So."
        ],
        [
            "So.",
            "Nothing we might want to do is say, well, all of these methods have at least one parameter, and that's a number of neighbors you use when you construct the initial graph.",
            "The initial neighborhood graph you can imagine if you pick too small of a K is going to give you bad results, and if you could pick two large accounts going back to results, pretty intuitive.",
            "So you get this U shaped curve.",
            "Although these aren't perfect.",
            "And the idea is, well, here we had some objective error measures.",
            "So here we can use that measure to pick the best value of K, and you know that's pretty standard machine learning that we want some objective way of picking rather than trying different values of K. Looking at embedding and picking.",
            "So.",
            "Having this kind of measure of errors very nice if you you want to, you know there's this bias variance tradeoff.",
            "If you want to control that, but this error measure, although it's very.",
            "Crisp and it's it's it's what we want.",
            "It's not always available, right?",
            "We can't in general, sample very large sets of data to evaluate an error measure.",
            "I mean, that's great for kind of these toy experiments, but we can't in general do that so."
        ],
        [
            "What can we do?",
            "Well, at least rail.",
            "So now we can define this test error, which is the same as our original loss function, except not computed or sub computed on a separate set of data.",
            "So we take our original points sampled from the manifold.",
            "We split that into two sets of training set and a test set.",
            "And now we're just evaluating that same error.",
            "The same loss function that we're minimizing, but on the test set, and that becomes our test there.",
            "So it's nice we can always compute this because all we have to do is just leave some subset of the data.",
            "Out that we don't use for training and use that as a validation set, or use another completely separate set for testing.",
            "So this is always available for any kind of manifold.",
            "But now, is this a reasonable thing to do?",
            "Is it's a reasonable notion of test there?",
            "Well, it turns out that this form of the error in the error we defined previously are actually very strongly correlated, so this is a little Mobius strip, so it's not embeddable anything.",
            "But we can still evaluate.",
            "We considered a tangent field over it and evaluate the air.",
            "So what I'm showing on the right here is one of the parameters are method is a number of radial basis functions.",
            "And that roughly controls the smoothness of the mapping we learn I it's implicitly controlling the smoothness of the manifold that we're learning, so again, you can imagine that too many and you're going to overfit.",
            "And too few you're just not going to be able to represent your data.",
            "So it's nice here is that the two errors in fact have roughly the same minimum size, scaled them.",
            "They're on different axes.",
            "There, apples and oranges.",
            "So you can't compare directly.",
            "But the point is that they have.",
            "When one goes down, the other one goes down and one goes up the other one goes up.",
            "So this is confirmation that this area has Mel is something reasonable to use for a notion of test there.",
            "There's a little bit of a caveat here."
        ],
        [
            "It's that you can't use this to actually select the dimensionality little D because as in K means clustering and K means clustering.",
            "You have some kind of loss function as you increase K, you can always increase the number of clusters.",
            "You can always decrease the value of that loss function.",
            "Same thing here as we increase the number of basis vectors, we can always make the data fit better, so we can't use this loss function to select little D, But there's been some work in how to actually find little D. Independent of ours, just like in K means you kind of look at the cut off point so you can look at a plot of little D versus air.",
            "And as you increase little D for learning, that error is going to go down, but there's going to be a little point where it flattens out."
        ],
        [
            "OK, so.",
            "We learned a tangent space and we have some way of, you know, for a given problem we can do cross validation to pick the parameters so we can you know whether we can visualize it or not.",
            "We can learn this.",
            "So how can we actually use this to do the things?",
            "I kind of stated as our objectives at the beginning of the talk projection denoising."
        ],
        [
            "Jurassic distance so in fact most of these things.",
            "Once you have attention, space become simple gradient descent optimization problems.",
            "So suppose you want to project a point onto the manifold in a projection onto a nonlinear manifold.",
            "So for linear manifold, the projection is unique.",
            "For nonlinear manifold projection is just defined as a point where locally the projection is a minimum distance.",
            "So you can imagine you could have more than one spot.",
            "In fact you're projecting a point in the center of a circle onto a circle.",
            "You would actually have an infinite number of projections.",
            "But in general, we just want to find a point X prime on the manifold that is a minimum distance to X and it's very simple.",
            "What we do, we just start some point next time and basically move it along the tangent space.",
            "Just simple gradient descent so we know kind of the tangent of the manifold.",
            "We have a point where we can move it along that tangent and basically move it in such a way to always minimize the distance.",
            "So basically we take the vector pointing from XX prime and we project that onto the tangent space.",
            "And we update a little bit anyway.",
            "It's a even if the math here just kind of rapid that I'm throwing at you.",
            "It's a fairly simple operation to do once."
        ],
        [
            "I've learned the tangent space.",
            "Another thing you can do is manifold denoising.",
            "So suppose you're given these red points and this is your training data, and from this training data you learn your tangent space and now you have your tangent space.",
            "You want to say why won't actually recover points that are denoise.",
            "Remove the.",
            "Clearly that if we assume that the slides in a 1 dimensional manifold, there's actually some noise hear from that manifold.",
            "And can we recover that?",
            "Well, again, we could just define some.",
            "Define these two terms, which one it just says.",
            "Well, we want the points to be close to the original points.",
            "Another term to say, well, we want the points to satisfy the tangent space, IE if the tangent space was horizontal and we had two points like this, they wouldn't really satisfy the tangent space where we had two points that were horizontal, they would.",
            "So we had two points like this and the tangent space was horizontal.",
            "Would be kind of forcing the points to be more horizontal, so we have these two terms.",
            "I mean I didn't put the actual derivatives here, but it's just a linear algebra and gradient descent and you can actually get pretty nice results.",
            "So these are some visual results.",
            "It's kind of hard to show these things.",
            "I showed that for these two simple cases, but the paper has a little more numerical analysis."
        ],
        [
            "For this and it works quite well.",
            "What are the things you can do is compute accurately the geodesic distance on a manifold and again this becomes very simple.",
            "I won't go into details, but a very simple optimization where you start out with some kind of path connecting two points and you generate in such a way as to minimize the length of the path and so you start out with something on the left and you get something on the right and you can do this.",
            "You know you don't need a lot of."
        ],
        [
            "Points to be able to perform this kind of thing.",
            "Showing some other results here.",
            "I am almost out of time.",
            "So this allows you to embed with something.",
            "Usually when people show embeddings for these S curves or something you know they use 500 points, 1000 points.",
            "We can do it actually pretty accurately with 50 using this geodesic distance calculation.",
            "So something like the structured data ends up being pretty hard to for a number of reasons.",
            "For a lot of algorithms, and it's not a problem, so all we're doing here, I should say all we're doing is computing geodesics distance between every pair of points and then using multidimensional scaling for these particular examples."
        ],
        [
            "Learn tangent fields.",
            "And then here I just wanted to show this the last slide, but I wanted to show there is some ability to generalize beyond the data you were given.",
            "So in this example on the bottom left here, this is a hemisphere with the top chopped off an.",
            "Learn to tangents field and then I asked what's a geodesic distance between two points.",
            "Kind of on opposite sides of the circle and when I ran the gradient descent it gave me that little path spanning where we would imagine that off the hemispheres, so we didn't have any training data, so we can't actually say this is the correct thing to do, although if we had test data we could compute the validation error there, But anyway it extends pretty well, and for this the one on the bottom right.",
            "Actually what we did was we trained on the points shown and then we kind of grew out the manifold.",
            "Just drop an inkblot.",
            "Onto a tangent field and you can grow something out and it grew out something.",
            "This extra loop and again you know it's hard to say if that's correct or not.",
            "We didn't have any test data there, but just kind of showing there is some ability to generalize.",
            "Be pretty far beyond where you saw any data."
        ],
        [
            "So I just I hope in this talk I was able to convince you little bit that perhaps when we think of non isometric manifold learning we can think beyond just embedding.",
            "What else can we do with it?",
            "That's the motivation.",
            "Thank you.",
            "Question comments.",
            "So your extrapolation beyond the training data to get these, you know extended services on your last slide there.",
            "Nice idea, but.",
            "I mean, the particular shape he is is of course a function of the.",
            "You know class occurs, yes.",
            "Yes.",
            "So you are choosing like where there's a quadratic term in there, right?",
            "So is is this.",
            "Did it.",
            "Basically going to be quadratic, so not exactly, but something like that, yeah?",
            "You know, pick more general, more general basis so you could get.",
            "Right, I mean I think the key to this is that you know there's really no correct thing.",
            "If you don't have any data in that region.",
            "Right, so in fact, I mean, this actually doesn't use anything.",
            "This just assumes that the tangent field extends smoothly, so it's not actually making quadratic assumption.",
            "Is just making an assumption a smoothness assumption.",
            "So basically you can imagine in this little swirl, tangent vectors are kind of changing like this, and so if you were to smoothly extend that out, it might continue being this world, and that's why you got this kind of results.",
            "I guess I don't see that because it seems like these sort of ultimately smooth.",
            "Right, but remember it has to be smooth by.",
            "I see where you're coming from, but it has to be smooth, like it has information here and it has information here.",
            "So the information here that it's missing has to be smooth.",
            "Not just from this but also to this.",
            "So you want it to be somehow smooth in between this and this, which happens to be this.",
            "I was wondering if you you have a model selection problem.",
            "Yes, yes.",
            "Rising some part of your manifold.",
            "How do you handle the regularization which is basically controlled by the number of basis functions?",
            "That's a really good question, so there's that.",
            "And then we actually have an explicit regularization term that further enforces smoothness that I don't talk about here, and so these are basically so we do assume that the noise levels constant over the whole manifold, so we can't do anything different for different parcel that you could imagine doing something like that.",
            "But we use just are we just leave some portion of data out and use cross validation to pick.",
            "The smoothness terms that basically give us the lowest test error.",
            "Um?",
            "OK, I don't see any more questions, but thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Papers on manifold and reduction in the 1st paper is by a group from UCS DQ dollar.",
                    "label": 0
                },
                {
                    "sent": "Vincent Obolon sociology have written the paper non isometric manifold learning analysis algorithms and children under percent.",
                    "label": 1
                },
                {
                    "sent": "Hi, thank you for the introduction.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this work, we're going to be focusing on two things.",
                    "label": 0
                },
                {
                    "sent": "The first is we wanted to extend a little bit manifold nonlinear manifold learning to something that is beyond embedding.",
                    "label": 0
                },
                {
                    "sent": "We have shown a lot of very nice applications of embedding nonlinear manifolds into a lower dimensional space, but our intuition is that there's a lot more we could actually do with nonlinear manifolds.",
                    "label": 0
                },
                {
                    "sent": "Other than embedding and to do so, one of the first things we need to establish some kind of either test error or generalization performance.",
                    "label": 1
                },
                {
                    "sent": "Some objective way to say if our manifold learning technique is doing something reasonable?",
                    "label": 0
                },
                {
                    "sent": "If we're doing just embedding that we could potentially just look at the quality of an embedding and say just visually through visual assessment.",
                    "label": 0
                },
                {
                    "sent": "Say this is a reasonable embedding, but we're going to be trying to do operations on non isometric animals were not going to be able to do embedding or any kind of visualization.",
                    "label": 0
                },
                {
                    "sent": "Really want more of some kind of notion of some objective notion of how well is it doing and some some ability to generalize beyond maybe just the data we've seen?",
                    "label": 0
                },
                {
                    "sent": "So what is it that we actually want to do with nonlinear manifold learning it?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not embedding well, let's consider saying what we can do with actually linear manifolds, i.e.",
                    "label": 1
                },
                {
                    "sent": "Just if we know the data lies on a subspace.",
                    "label": 0
                },
                {
                    "sent": "So with this kind of data you know you might be given data and you noisy data and you find just using PCA or something.",
                    "label": 0
                },
                {
                    "sent": "Just find subspace that best fits the data that minimize the various and then operations.",
                    "label": 0
                },
                {
                    "sent": "You might want to do is project onto that subspace, measured the distance between two points after projection, measure distance to the subspace which you can project is the same thing.",
                    "label": 1
                },
                {
                    "sent": "And importantly, we want to be able to do it.",
                    "label": 0
                },
                {
                    "sent": "You know, we might learn the subspace from some set of data, but we might want to be able to.",
                    "label": 0
                },
                {
                    "sent": "We have a very good idea of what that subspace looks everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So even in regions where either we did, we just had data around it, but not right there, or kind of extrapolate beyond where we had data in for a linear subspace that's very obvious.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so the idea is well, can we extend this to basically same?",
                    "label": 0
                },
                {
                    "sent": "Operations, but for a nonlinear manifold, so project onto the manifold.",
                    "label": 0
                },
                {
                    "sent": "In this case of projection, need not be unique.",
                    "label": 0
                },
                {
                    "sent": "Measure distance to manifold, just the distance of the closest projection merger geodesic distance on a manifold, and again generalize to unseen regions, and so the idea here is we can do this for linear manifolds.",
                    "label": 1
                },
                {
                    "sent": "And can we do this for nonlinear manifolds?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the representation we use for manifolds that allows us to do these operations is going to be representing a manifold by its tangent space, and this is work we presented earlier year ago at NIPS and Bang.",
                    "label": 1
                },
                {
                    "sent": "You and others had some work prior to that at NIPS 05.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to just give up pretty brief overview of learning the tangent space.",
                    "label": 0
                },
                {
                    "sent": "More details are in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or the poster.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is we know we have data that lies in a low dimensional manifold.",
                    "label": 0
                },
                {
                    "sent": "Dimensional space, so it'll be throughout this presentation to represent the dimensionality of the manifold and big data represents dimensionality of the space.",
                    "label": 0
                },
                {
                    "sent": "And so the idea is what we're going to want to do is learn a mapping from a point.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Manifold 2 its tangent basis.",
                    "label": 0
                },
                {
                    "sent": "So point in amount of folders.",
                    "label": 0
                },
                {
                    "sent": "Just some point in our big D. Ann is tangent basis is a matrix with little D. Columns where each column is just some basis vector of the manifold of the tangent space at that point on a manifold.",
                    "label": 1
                },
                {
                    "sent": "So what exactly does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means every one of those vectors is just a derivative.",
                    "label": 0
                },
                {
                    "sent": "If you so X is just, there's a mapping from Y which is in the lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And if we take that derivative respect to the various components of why that will give us the tangent vectors?",
                    "label": 0
                },
                {
                    "sent": "So pretty straightforward, and so now the idea is to learn this function Curly H. That will take us from this point in high dimensional space to each tangent space.",
                    "label": 0
                },
                {
                    "sent": "And we're going to learn it from points from the data.",
                    "label": 0
                },
                {
                    "sent": "We're just going to be points samples from the manifold, as in the case of when people do embedding and we're going to learn this function H.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on that.",
                    "label": 0
                },
                {
                    "sent": "So what's the idea?",
                    "label": 0
                },
                {
                    "sent": "Well, the idea is if you're given 2 nearby points, then the difference between those points.",
                    "label": 0
                },
                {
                    "sent": "So if you're given two points on a manifold XI XJ that you know in nearby, then the difference between those points to Delta should be a linear combination of your tangent vectors at that point on the manifold.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty intuitive and no, the idea is if.",
                    "label": 0
                },
                {
                    "sent": "If you actually were given in some sense the line tangent vectors, as in the case on the left here, then you can learn a regression from the point on the manifold to the first tangent vector and from the point the second tender vector, and so on.",
                    "label": 0
                },
                {
                    "sent": "The problem is in real data all you have is vectors, kind of pointing to your neighbors these Delta vectors, so they're kind of pointing in all different directions, so you can't just learn a straight regression.",
                    "label": 0
                },
                {
                    "sent": "But if you knew the alignment, then you could get.",
                    "label": 0
                },
                {
                    "sent": "You could learn the regression, and if you knew the regression, you can learn to alignment.",
                    "label": 0
                },
                {
                    "sent": "So that's the intuition behind the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the actual loss function that we defined to learn this is we say, so H again H of data.",
                    "label": 0
                },
                {
                    "sent": "That's going to be a function we're learning.",
                    "label": 0
                },
                {
                    "sent": "Theta is the parameters for that function of a point X bar.",
                    "label": 0
                },
                {
                    "sent": "Those are the tangent vectors.",
                    "label": 0
                },
                {
                    "sent": "The parameter epsilon is a free parameter that's going to be the alignment for the vectors and that should match the deltas for all neighboring points.",
                    "label": 0
                },
                {
                    "sent": "I all points that are near each other, so we don't want to do this for points that are far apart, 'cause it's probably only hold if points are close together enough together that essentially linear approximations to manifold is good, and in fact actually we define it this way by the tangent that between the two points we actually just need a quadratic approximations.",
                    "label": 0
                },
                {
                    "sent": "The manifold told it's just.",
                    "label": 0
                },
                {
                    "sent": "Centered in Uncentered derivative.",
                    "label": 0
                },
                {
                    "sent": "So we're just going to minimizing this express.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I actually haven't said what the form for HR regression function is, and so it's actually we're just going to use a very simple form, a linear function over some features, and our features are just going to be using radial basis functions, so the details of the math here aren't so important.",
                    "label": 0
                },
                {
                    "sent": "I don't expect anyone followed by following the notation, but it actually turns out that this is the entirety of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Once you do derivations.",
                    "label": 0
                },
                {
                    "sent": "So essentially at each phase.",
                    "label": 0
                },
                {
                    "sent": "So the first each so there's.",
                    "label": 0
                },
                {
                    "sent": "It's an alternating minimization in each of the two iterations.",
                    "label": 0
                },
                {
                    "sent": "You're doing least squares problem, so it's very fast, so you solve least squares to get the epsilon.",
                    "label": 0
                },
                {
                    "sent": "To get the alignment, and then you solve the least squares to get the regression.",
                    "label": 0
                },
                {
                    "sent": "Since just least function, you just keep alternating between these until you get to a local minima and you might restart this a couple of times so you don't get stuck in a bad minimum.",
                    "label": 0
                },
                {
                    "sent": "So again, the details aren't important, but it actually turns out to be a very nice simple algorithm that works very quickly and is fairly effective.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As hopefully I'll be able to show up.",
                    "label": 0
                },
                {
                    "sent": "OK, yes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes, and I'll go into that a little more.",
                    "label": 0
                },
                {
                    "sent": "A little later, yes, but for this you actually need to know little deep when you're doing this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first thing we said, well, OK, so we have this algorithm, but how do we actually know if it's doing well?",
                    "label": 0
                },
                {
                    "sent": "So with embedding, people have defined a lot of different loss functions they can optimize to get a different embedding and they can say, you know, we find the global minima to this loss function, but how do you actually then compare which one is better?",
                    "label": 0
                },
                {
                    "sent": "You know, because we're going to have different parameters like the number of basis functions, the number of neighbors, and so on.",
                    "label": 0
                },
                {
                    "sent": "And again, we're just going to apply this methodology to non isometric manifolds, which isn't just for embedding so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start with something very simple and intuitive, which is suppose we had a manifold that we can densely sample so.",
                    "label": 0
                },
                {
                    "sent": "Kind of either.",
                    "label": 0
                },
                {
                    "sent": "We know the transformation that's generating the manifold, or we know the analytic form of the manifold.",
                    "label": 0
                },
                {
                    "sent": "So in some sense controlled data.",
                    "label": 0
                },
                {
                    "sent": "Now what we can do now is sample two sets of data, one that we use for training and one that we used to evaluate the air.",
                    "label": 0
                },
                {
                    "sent": "An idea is the one to value the error is going to be very very large sample set in the training set or which we're going to use for embedding or.",
                    "label": 0
                },
                {
                    "sent": "However, when a trainer algorithm is going to be smaller measuring.",
                    "label": 0
                },
                {
                    "sent": "Finite sample performance, and so the idea is very simple if.",
                    "label": 1
                },
                {
                    "sent": "Let's say you know.",
                    "label": 0
                },
                {
                    "sent": "Well, so first of all the ISOMAP algorithm by Tenenbaum at all let's you compute geodesic distance between points on a manifold just by doing the shortest path right?",
                    "label": 0
                },
                {
                    "sent": "And then you could embed that using multidimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "But the point is if you have a very highly densely sampled manifold, you can in fact get a very very accurate estimate that's guaranteed to converge as the number of samples goes to Infinity of the geodesic distance.",
                    "label": 0
                },
                {
                    "sent": "Now if you have a manifold.",
                    "label": 0
                },
                {
                    "sent": "That you know is isometric to convex subset of Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "That means that if you embed it, you can embed it without introducing any kind of distortion in such a way that after embedding the Euclidean distance in embedded space is equivalent to the geodesic distance on the manifold, so we'd expect so various embedding algorithms can work with data that's not isometric to convex set, or that's not even isometric to Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "But the point is, for this subset, this restricted subset of manifold, we'd expect that any man bedding method or any other process should at least.",
                    "label": 0
                },
                {
                    "sent": "Embed these without introducing distortion so we can use this week and basically compare using our very large set, which I'm just informally called S Infinity.",
                    "label": 0
                },
                {
                    "sent": "We compute the ground truth geodesic distance and then we can come using our smaller set.",
                    "label": 0
                },
                {
                    "sent": "We can run our embedding algorithm or whatever kind of algorithm we're using and then measure the Euclidean distance and the difference between those gives us some very objective sense of error so.",
                    "label": 0
                },
                {
                    "sent": "We rounded so the first thing we did was said, OK, well, let's just for interest sake, compare the embedding performance of a number of algorithms.",
                    "label": 0
                },
                {
                    "sent": "And again the focus of our work is not necessarily embedding, but.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Still kind of an interesting thing to do, So what I'm showing here.",
                    "label": 0
                },
                {
                    "sent": "This is a very well known half to Death S curve.",
                    "label": 0
                },
                {
                    "sent": "And what I'm plotting here is this error GD which I defined the previous slide versus the number of samples we use for embedding for Eli maximum variance unfolding ISOMAP and LML.",
                    "label": 0
                },
                {
                    "sent": "And so you know, as you can see, as the number of samples, an increase is all the algorithms except perhaps Eli get very low air and what's kind of interesting and this is a log scale on the X axis and what's interesting is a performance for low values of N and there's two versions of Elsa melon that's not so important here.",
                    "label": 0
                },
                {
                    "sent": "But as you can see.",
                    "label": 0
                },
                {
                    "sent": "Allison Melton so perform better than other methods.",
                    "label": 0
                },
                {
                    "sent": "Isomap, second, best, and envy you.",
                    "label": 0
                },
                {
                    "sent": "To be fair, and EU is more applicable.",
                    "label": 0
                },
                {
                    "sent": "More generally applicable than Isomap and Ellie is more generally applicable than to remainder.",
                    "label": 0
                },
                {
                    "sent": "Of course, also Mel.",
                    "label": 0
                },
                {
                    "sent": "We want it to be applicable to non isometric manifolds, so a much larger class of manifold.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Nothing we might want to do is say, well, all of these methods have at least one parameter, and that's a number of neighbors you use when you construct the initial graph.",
                    "label": 0
                },
                {
                    "sent": "The initial neighborhood graph you can imagine if you pick too small of a K is going to give you bad results, and if you could pick two large accounts going back to results, pretty intuitive.",
                    "label": 0
                },
                {
                    "sent": "So you get this U shaped curve.",
                    "label": 0
                },
                {
                    "sent": "Although these aren't perfect.",
                    "label": 0
                },
                {
                    "sent": "And the idea is, well, here we had some objective error measures.",
                    "label": 0
                },
                {
                    "sent": "So here we can use that measure to pick the best value of K, and you know that's pretty standard machine learning that we want some objective way of picking rather than trying different values of K. Looking at embedding and picking.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Having this kind of measure of errors very nice if you you want to, you know there's this bias variance tradeoff.",
                    "label": 0
                },
                {
                    "sent": "If you want to control that, but this error measure, although it's very.",
                    "label": 0
                },
                {
                    "sent": "Crisp and it's it's it's what we want.",
                    "label": 0
                },
                {
                    "sent": "It's not always available, right?",
                    "label": 0
                },
                {
                    "sent": "We can't in general, sample very large sets of data to evaluate an error measure.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's great for kind of these toy experiments, but we can't in general do that so.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What can we do?",
                    "label": 0
                },
                {
                    "sent": "Well, at least rail.",
                    "label": 0
                },
                {
                    "sent": "So now we can define this test error, which is the same as our original loss function, except not computed or sub computed on a separate set of data.",
                    "label": 1
                },
                {
                    "sent": "So we take our original points sampled from the manifold.",
                    "label": 0
                },
                {
                    "sent": "We split that into two sets of training set and a test set.",
                    "label": 0
                },
                {
                    "sent": "And now we're just evaluating that same error.",
                    "label": 0
                },
                {
                    "sent": "The same loss function that we're minimizing, but on the test set, and that becomes our test there.",
                    "label": 0
                },
                {
                    "sent": "So it's nice we can always compute this because all we have to do is just leave some subset of the data.",
                    "label": 0
                },
                {
                    "sent": "Out that we don't use for training and use that as a validation set, or use another completely separate set for testing.",
                    "label": 0
                },
                {
                    "sent": "So this is always available for any kind of manifold.",
                    "label": 0
                },
                {
                    "sent": "But now, is this a reasonable thing to do?",
                    "label": 0
                },
                {
                    "sent": "Is it's a reasonable notion of test there?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out that this form of the error in the error we defined previously are actually very strongly correlated, so this is a little Mobius strip, so it's not embeddable anything.",
                    "label": 0
                },
                {
                    "sent": "But we can still evaluate.",
                    "label": 0
                },
                {
                    "sent": "We considered a tangent field over it and evaluate the air.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing on the right here is one of the parameters are method is a number of radial basis functions.",
                    "label": 0
                },
                {
                    "sent": "And that roughly controls the smoothness of the mapping we learn I it's implicitly controlling the smoothness of the manifold that we're learning, so again, you can imagine that too many and you're going to overfit.",
                    "label": 0
                },
                {
                    "sent": "And too few you're just not going to be able to represent your data.",
                    "label": 0
                },
                {
                    "sent": "So it's nice here is that the two errors in fact have roughly the same minimum size, scaled them.",
                    "label": 0
                },
                {
                    "sent": "They're on different axes.",
                    "label": 0
                },
                {
                    "sent": "There, apples and oranges.",
                    "label": 0
                },
                {
                    "sent": "So you can't compare directly.",
                    "label": 0
                },
                {
                    "sent": "But the point is that they have.",
                    "label": 0
                },
                {
                    "sent": "When one goes down, the other one goes down and one goes up the other one goes up.",
                    "label": 0
                },
                {
                    "sent": "So this is confirmation that this area has Mel is something reasonable to use for a notion of test there.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of a caveat here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's that you can't use this to actually select the dimensionality little D because as in K means clustering and K means clustering.",
                    "label": 0
                },
                {
                    "sent": "You have some kind of loss function as you increase K, you can always increase the number of clusters.",
                    "label": 0
                },
                {
                    "sent": "You can always decrease the value of that loss function.",
                    "label": 0
                },
                {
                    "sent": "Same thing here as we increase the number of basis vectors, we can always make the data fit better, so we can't use this loss function to select little D, But there's been some work in how to actually find little D. Independent of ours, just like in K means you kind of look at the cut off point so you can look at a plot of little D versus air.",
                    "label": 0
                },
                {
                    "sent": "And as you increase little D for learning, that error is going to go down, but there's going to be a little point where it flattens out.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We learned a tangent space and we have some way of, you know, for a given problem we can do cross validation to pick the parameters so we can you know whether we can visualize it or not.",
                    "label": 0
                },
                {
                    "sent": "We can learn this.",
                    "label": 0
                },
                {
                    "sent": "So how can we actually use this to do the things?",
                    "label": 0
                },
                {
                    "sent": "I kind of stated as our objectives at the beginning of the talk projection denoising.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jurassic distance so in fact most of these things.",
                    "label": 0
                },
                {
                    "sent": "Once you have attention, space become simple gradient descent optimization problems.",
                    "label": 0
                },
                {
                    "sent": "So suppose you want to project a point onto the manifold in a projection onto a nonlinear manifold.",
                    "label": 0
                },
                {
                    "sent": "So for linear manifold, the projection is unique.",
                    "label": 0
                },
                {
                    "sent": "For nonlinear manifold projection is just defined as a point where locally the projection is a minimum distance.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine you could have more than one spot.",
                    "label": 0
                },
                {
                    "sent": "In fact you're projecting a point in the center of a circle onto a circle.",
                    "label": 0
                },
                {
                    "sent": "You would actually have an infinite number of projections.",
                    "label": 0
                },
                {
                    "sent": "But in general, we just want to find a point X prime on the manifold that is a minimum distance to X and it's very simple.",
                    "label": 0
                },
                {
                    "sent": "What we do, we just start some point next time and basically move it along the tangent space.",
                    "label": 0
                },
                {
                    "sent": "Just simple gradient descent so we know kind of the tangent of the manifold.",
                    "label": 0
                },
                {
                    "sent": "We have a point where we can move it along that tangent and basically move it in such a way to always minimize the distance.",
                    "label": 0
                },
                {
                    "sent": "So basically we take the vector pointing from XX prime and we project that onto the tangent space.",
                    "label": 0
                },
                {
                    "sent": "And we update a little bit anyway.",
                    "label": 0
                },
                {
                    "sent": "It's a even if the math here just kind of rapid that I'm throwing at you.",
                    "label": 0
                },
                {
                    "sent": "It's a fairly simple operation to do once.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I've learned the tangent space.",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do is manifold denoising.",
                    "label": 0
                },
                {
                    "sent": "So suppose you're given these red points and this is your training data, and from this training data you learn your tangent space and now you have your tangent space.",
                    "label": 0
                },
                {
                    "sent": "You want to say why won't actually recover points that are denoise.",
                    "label": 0
                },
                {
                    "sent": "Remove the.",
                    "label": 0
                },
                {
                    "sent": "Clearly that if we assume that the slides in a 1 dimensional manifold, there's actually some noise hear from that manifold.",
                    "label": 0
                },
                {
                    "sent": "And can we recover that?",
                    "label": 0
                },
                {
                    "sent": "Well, again, we could just define some.",
                    "label": 0
                },
                {
                    "sent": "Define these two terms, which one it just says.",
                    "label": 0
                },
                {
                    "sent": "Well, we want the points to be close to the original points.",
                    "label": 0
                },
                {
                    "sent": "Another term to say, well, we want the points to satisfy the tangent space, IE if the tangent space was horizontal and we had two points like this, they wouldn't really satisfy the tangent space where we had two points that were horizontal, they would.",
                    "label": 0
                },
                {
                    "sent": "So we had two points like this and the tangent space was horizontal.",
                    "label": 0
                },
                {
                    "sent": "Would be kind of forcing the points to be more horizontal, so we have these two terms.",
                    "label": 0
                },
                {
                    "sent": "I mean I didn't put the actual derivatives here, but it's just a linear algebra and gradient descent and you can actually get pretty nice results.",
                    "label": 0
                },
                {
                    "sent": "So these are some visual results.",
                    "label": 0
                },
                {
                    "sent": "It's kind of hard to show these things.",
                    "label": 0
                },
                {
                    "sent": "I showed that for these two simple cases, but the paper has a little more numerical analysis.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this and it works quite well.",
                    "label": 0
                },
                {
                    "sent": "What are the things you can do is compute accurately the geodesic distance on a manifold and again this becomes very simple.",
                    "label": 1
                },
                {
                    "sent": "I won't go into details, but a very simple optimization where you start out with some kind of path connecting two points and you generate in such a way as to minimize the length of the path and so you start out with something on the left and you get something on the right and you can do this.",
                    "label": 0
                },
                {
                    "sent": "You know you don't need a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points to be able to perform this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Showing some other results here.",
                    "label": 0
                },
                {
                    "sent": "I am almost out of time.",
                    "label": 0
                },
                {
                    "sent": "So this allows you to embed with something.",
                    "label": 0
                },
                {
                    "sent": "Usually when people show embeddings for these S curves or something you know they use 500 points, 1000 points.",
                    "label": 0
                },
                {
                    "sent": "We can do it actually pretty accurately with 50 using this geodesic distance calculation.",
                    "label": 0
                },
                {
                    "sent": "So something like the structured data ends up being pretty hard to for a number of reasons.",
                    "label": 0
                },
                {
                    "sent": "For a lot of algorithms, and it's not a problem, so all we're doing here, I should say all we're doing is computing geodesics distance between every pair of points and then using multidimensional scaling for these particular examples.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learn tangent fields.",
                    "label": 0
                },
                {
                    "sent": "And then here I just wanted to show this the last slide, but I wanted to show there is some ability to generalize beyond the data you were given.",
                    "label": 0
                },
                {
                    "sent": "So in this example on the bottom left here, this is a hemisphere with the top chopped off an.",
                    "label": 0
                },
                {
                    "sent": "Learn to tangents field and then I asked what's a geodesic distance between two points.",
                    "label": 0
                },
                {
                    "sent": "Kind of on opposite sides of the circle and when I ran the gradient descent it gave me that little path spanning where we would imagine that off the hemispheres, so we didn't have any training data, so we can't actually say this is the correct thing to do, although if we had test data we could compute the validation error there, But anyway it extends pretty well, and for this the one on the bottom right.",
                    "label": 0
                },
                {
                    "sent": "Actually what we did was we trained on the points shown and then we kind of grew out the manifold.",
                    "label": 0
                },
                {
                    "sent": "Just drop an inkblot.",
                    "label": 0
                },
                {
                    "sent": "Onto a tangent field and you can grow something out and it grew out something.",
                    "label": 0
                },
                {
                    "sent": "This extra loop and again you know it's hard to say if that's correct or not.",
                    "label": 0
                },
                {
                    "sent": "We didn't have any test data there, but just kind of showing there is some ability to generalize.",
                    "label": 0
                },
                {
                    "sent": "Be pretty far beyond where you saw any data.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I just I hope in this talk I was able to convince you little bit that perhaps when we think of non isometric manifold learning we can think beyond just embedding.",
                    "label": 0
                },
                {
                    "sent": "What else can we do with it?",
                    "label": 0
                },
                {
                    "sent": "That's the motivation.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Question comments.",
                    "label": 0
                },
                {
                    "sent": "So your extrapolation beyond the training data to get these, you know extended services on your last slide there.",
                    "label": 0
                },
                {
                    "sent": "Nice idea, but.",
                    "label": 0
                },
                {
                    "sent": "I mean, the particular shape he is is of course a function of the.",
                    "label": 0
                },
                {
                    "sent": "You know class occurs, yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So you are choosing like where there's a quadratic term in there, right?",
                    "label": 0
                },
                {
                    "sent": "So is is this.",
                    "label": 0
                },
                {
                    "sent": "Did it.",
                    "label": 0
                },
                {
                    "sent": "Basically going to be quadratic, so not exactly, but something like that, yeah?",
                    "label": 0
                },
                {
                    "sent": "You know, pick more general, more general basis so you could get.",
                    "label": 0
                },
                {
                    "sent": "Right, I mean I think the key to this is that you know there's really no correct thing.",
                    "label": 0
                },
                {
                    "sent": "If you don't have any data in that region.",
                    "label": 0
                },
                {
                    "sent": "Right, so in fact, I mean, this actually doesn't use anything.",
                    "label": 0
                },
                {
                    "sent": "This just assumes that the tangent field extends smoothly, so it's not actually making quadratic assumption.",
                    "label": 0
                },
                {
                    "sent": "Is just making an assumption a smoothness assumption.",
                    "label": 0
                },
                {
                    "sent": "So basically you can imagine in this little swirl, tangent vectors are kind of changing like this, and so if you were to smoothly extend that out, it might continue being this world, and that's why you got this kind of results.",
                    "label": 0
                },
                {
                    "sent": "I guess I don't see that because it seems like these sort of ultimately smooth.",
                    "label": 0
                },
                {
                    "sent": "Right, but remember it has to be smooth by.",
                    "label": 0
                },
                {
                    "sent": "I see where you're coming from, but it has to be smooth, like it has information here and it has information here.",
                    "label": 0
                },
                {
                    "sent": "So the information here that it's missing has to be smooth.",
                    "label": 0
                },
                {
                    "sent": "Not just from this but also to this.",
                    "label": 0
                },
                {
                    "sent": "So you want it to be somehow smooth in between this and this, which happens to be this.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if you you have a model selection problem.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Rising some part of your manifold.",
                    "label": 0
                },
                {
                    "sent": "How do you handle the regularization which is basically controlled by the number of basis functions?",
                    "label": 0
                },
                {
                    "sent": "That's a really good question, so there's that.",
                    "label": 0
                },
                {
                    "sent": "And then we actually have an explicit regularization term that further enforces smoothness that I don't talk about here, and so these are basically so we do assume that the noise levels constant over the whole manifold, so we can't do anything different for different parcel that you could imagine doing something like that.",
                    "label": 0
                },
                {
                    "sent": "But we use just are we just leave some portion of data out and use cross validation to pick.",
                    "label": 0
                },
                {
                    "sent": "The smoothness terms that basically give us the lowest test error.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, I don't see any more questions, but thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}