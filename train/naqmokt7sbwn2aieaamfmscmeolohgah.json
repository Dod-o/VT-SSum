{
    "id": "naqmokt7sbwn2aieaamfmscmeolohgah",
    "title": "Non Smooth, Non Finite, and Non Convex Optimization",
    "info": {
        "author": [
            "Mark Schmidt, Department of Computer Science, University of British Columbia"
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_schmidt_nonsmooth_nonfinite/",
    "segmentation": [
        [
            "So today we're going to talk.",
            "So yesterday we sort of made some assumptions that made our life easier, but possibly less realistic for a lot of applications.",
            "So today we're going to talk about when those strong assumptions we make are not satisfied, and what happens then?",
            "And as before, I'm going to kind of try and focus on what the theory says with a few guidelines for my practical experience, but these are hard problems, so I think nobody really knows what the true answers to a lot of these things are."
        ],
        [
            "Before I get into that, I just want to tie up a few loose ends from yesterday, so these are some of the things that I talked about, but not necessarily on the slides, and I want to go into a bit more detail on.",
            "So one thing that came up in our discussion was this thing called the complex step derivative.",
            "So usual finite difference approximation is this formula, which I've sure you've seen before, 'cause you use it to check your derivatives when you write your code, and you can motivate that just from a Taylor expansion around X where you have this over H ^2, so the error is actually pretty good.",
            "If I make H very small, this formula is very accurate.",
            "If I make H1 E to the minus 8 then it's Juanita minus 16 accurate.",
            "It's great.",
            "The problem is when you do the subtraction.",
            "So if you actually want to find the difference at the level of 1 E to the minus 8, but you're comparing things that are much larger, you lose a whole lot of accuracy when you do the subtraction."
        ],
        [
            "So the complex step derivative is a very simple change.",
            "We're going to replace.",
            "We're just going to add I the square root of negative one here.",
            "I can still do the Taylor expansion for the class of functions called analytic functions, which includes a lot of the things we care about.",
            "And now the neat thing is I don't need to do these two evaluations.",
            "I just do this one evaluation.",
            "The real part by solving for it in this equation is going to give me F of X + / 8 squared and the imaginary part of that one evaluation is going to give me the gradient and that is going to give me the gradient in that direction divided by H. So it's still over 8 squared, but."
        ],
        [
            "The nice thing is there's no cancellation now.",
            "You don't have to do this.",
            "The subtraction here, and what that means is you can actually use an extremely small age.",
            "So if you look in my code it actually uses 10 to the minus 150 as your H. Because you don't have to, there's no subtraction there, and it actually gives you the derivative to that accuracy, so it's effectively an exact derivative method, and so here I've written it for the 1st order derivative.",
            "But if you want to Hessian vector product, or if you want the derivative in a particular direction, the same thing applies the same argument."
        ],
        [
            "I also mentioned subgradients of convex functions, so this is the sub gradient formula.",
            "It's a vector D satisfying this formula for all Y&X for any reasonable convex function like say that it's not Infinity everywhere.",
            "These are going to exist at any point X but for non convex functions this definition doesn't quite work because we remember we said that convex functions are above their subgradient whereas non convex functions.",
            "You might go back down so there may be no actual subgradient at a point."
        ],
        [
            "But people actually do take them, take the Max of the two things, like when you do Relu.",
            "So what does that actually doing?",
            "And that's sometimes called a Clark subgradient or a generalized gradient.",
            "And what you do is.",
            "You add this part here so it's actually a concave lower bound.",
            "You can think of this as being as lower bound that's linear at your current point, but then it goes down.",
            "And what that does an you can let it go down arbitrarily fast.",
            "And then what that does is it lets you define subgradients for convex or non convex functions."
        ],
        [
            "I mentioned the convergence rate of stochastic gradient with constant step size and I said that was sort of hidden in the 2000 paper of net chamber tzikas.",
            "I said the report I wrote was four pages long, but it's actually just two pages long, so this is the entire paper on the slide, and the proof is just Section 3 here of the result I said yesterday so it's you can go through it at home.",
            "Everything you need to prove it is is in the paper.",
            "There I also said you don't need smoothness to show the result, that's section.",
            "4 here so you know if you want a very simple treatment of that, there it is."
        ],
        [
            "And then I wanted to talk a little bit more about stochastic variance, reduced gradient, but we were sort of running short on time, so I just want to briefly mention a few things here.",
            "So there's an inner and an outer loop.",
            "The inner loop is actually running a true stochastic gradient method.",
            "It doesn't look like a stochastic gradient method because we have replaced the stochastic rate with this weird thing.",
            "But you basically set it up so the expectation of this term is equal to this term.",
            "So in expectation this is zero in expectation, this is your true gradient.",
            "So this is actually unlike sag.",
            "This is a true stochastic gradient method.",
            "We just added these extra 2 terms and the purpose of those is to make your noise go to 0.",
            "So it's kind of it's kind of like having an increasing batch size that makes your noise go to zero.",
            "But here sort of happens automatically and this D is just the average of all your functions at some point.",
            "So you've got this."
        ],
        [
            "Scheme where you're doing occasionally, you're going through a full pass of the data set, and then you run a whole bunch of stochastic gradient methods.",
            "Now you have to pick how many iterations you do, and the theory says you should pick that roughly proportional to the condition number.",
            "In practice, people actually just set M to the number of data points they have, so you do one passive stochastic gradient.",
            "Then you compute a full gradient, one passive stochastic gradient computer, full gradient, and so on.",
            "So it's it seems kind of wasteful.",
            "It is kind of wasteful.",
            "You can start talking about ways to reduce those gradient calculations and.",
            "And the."
        ],
        [
            "Work if you do them properly.",
            "And I want to.",
            "I want to size the practical issues or similar to sag.",
            "So I talked about this thing like when you try to adaptively estimate how often we want to sample the data points, we talked about how to do better mini batching's you can automatically set the set the step size.",
            "It's really a black box, there's no tuning required, at least in principle.",
            "So I said those for sag, but really they apply for SVR G2.",
            "It's a very similar algorithm, you've just kind of got rid of the memory and introduced some extra gradient calculations."
        ],
        [
            "OK, so part one we talked about how you can get a low iteration cost and a linear convergence rate in this very restrictive setting.",
            "And today we're going to try and relax those assumptions zero before I start.",
            "Are there any questions or comments or burning issues from yesterday?",
            "I know it's early.",
            "OK, everyone is just eating.",
            "Calculate.",
            "Yeah, I probably should have included that in this list, so if you look up Fisto, there's nothing called Fisto FCA.",
            "They're the ones who sort of popularized this approach, so they don't really give you an approximation of the Lipschitz constant, but they give you a.",
            "A bound on the Lipschitz constant that works to make your algorithm converge as fast as it could.",
            "And it's very simple.",
            "You take one of those inequality's that Lipschitz continuous functions should satisfy, and you look along the direction you're going.",
            "If it's not satisfied, you make it bigger.",
            "Until until it's satisfied and you can prove that that that's a OK thing to do.",
            "So the one thing I'll say is."
        ],
        [
            "If you apply the usual rules of sub of computing sub gradients, like if you have a Max function, you just take the gradient of the one that achieves the Max that will satisfy this.",
            "So this is more like notation to say that we can define subgradients locally instead of globally, and that lets us handle non convex functions.",
            "Is that what you're getting at?",
            "The actual paper gives a much more complicated definition of this, but this is equivalent definition that's much simpler to understand.",
            "In my opinion.",
            "Anymore.",
            "OK, let's get started."
        ],
        [
            "OK, so we're going to tackle Nonsmooth first, which is in some sense the easiest of these ones to handle."
        ],
        [
            "OK, so that was our general framework.",
            "Some data fitting terms, some regularizer.",
            "And often our regularizer is non smooth, so we might do."
        ],
        [
            "Something like L1 regularization because we want sparse solutions, so it's nice you got the regularising effect.",
            "You get zeros in your solution, so your coefficients are sparse, but the absolute value function is non smooth, so so the things we talked about yesterday don't really."
        ],
        [
            "Ally.",
            "You can apply subgradient methods.",
            "It's very trivial to do so, but they're very slow.",
            "They had these these awful sub linear rates that we talked about last time."
        ],
        [
            "So.",
            "The problem is the subgradient method is actually optimal black box method.",
            "You can't do better by like clever cutting plane or bundle methods or anything like that.",
            "But if you start talking about specific nonsmooth problems Now, you can go faster, and the L1 regularization is a case where in fact you can go substantially fast."
        ],
        [
            "So let's just talk about the first thing you would think of, which is to replace your nonsmooth function with some smooth approximation, and then you just apply the method that you know works well for smooth optimization."
        ],
        [
            "So if you have the absolute value function, a very old approximation is this function, called the multi quadric function, and."
        ],
        [
            "That just looks like this red function, so it almost exactly follows the absolute value and then around zero.",
            "It's just smooth it out a bit.",
            "One thing you'll notice is.",
            "You will actually lose the sparsity.",
            "It won't set things to exactly 0, and that's kind of a generic disadvantage of."
        ],
        [
            "Moving methods.",
            "If you want to approximate the Max function, the log some exponential is one option, or softmax if you want to approximate the hinge function.",
            "What you do is you just take your.",
            "You can use the log some axe or you can just take the linear part.",
            "A little quadratic piece and then the other linear part.",
            "So it's almost exactly it up to some small piece.",
            "Yeah.",
            "That's true, the terminology is overloaded.",
            "I'm not sure which is the right one.",
            "I'll believe you.",
            "Oh, I see.",
            "Right, that's a good point.",
            "Yeah.",
            "That's right, that's a very good point.",
            "I had never thought of that before.",
            "Um?"
        ],
        [
            "OK, so if you don't know how to smooth your function, what you can do is you can take what's called the convex conjugate, which, despite its name, does not rely on the function actually being convex, and then you apply a strongly convex regularizer to that, and then when you go back to the primal it gives you a smooth function.",
            "It's very magical, it only applies in special cases, but it is a generic method to smooth any nonsmooth function.",
            "Assuming you can compute this thing."
        ],
        [
            "So nesterov.",
            "That is a very complicated question.",
            "There's different ways you can do it so.",
            "If you actually sample a bunch of points in an area, it's like convolving with a Gaussian.",
            "So if you take the average of those samples, it is like you're doing a smoothing.",
            "Unfortunately, in the stochastic case, smoothing doesn't help you.",
            "So even if even if you were thinking of smoothing it in the stochastic case, it doesn't improve your rate.",
            "So yes, it is a form of smoothing, but.",
            "Yes.",
            "OK, So what Nesterov showed is that if you smooth the function and then apply a gradient method, you actually get the same rate as the subgradient method.",
            "So the problem is in order to approximate a non smooth function with a smooth function, the smooth function has to be so badly conditioned that you can't really solve it very efficiently.",
            "But he showed is if you do the accelerated gradient method you can get a faster rate.",
            "So you can smooth, but you have to apply a very fast algorithm to benefit, and that's that kind of matches intuition."
        ],
        [
            "There's no results showing that smoothing helps in the stochastic case, except for one paper by John Duchi.",
            "On the smooth problem?",
            "Yeah, 'cause accelerating rate you need smoothness so you can't apply it to the original problem.",
            "So John, do.",
            "She showed that if you do the smoothing trick that that was mentioned then you can improve like the variance in some sense.",
            "Or you can benefit from parallelization.",
            "But you're still in sort of the slow convergence rate regime.",
            "In practice, this is not what you do.",
            "You'll usually slowly increase the level of smoothing or the decrease the level of smoothing.",
            "So you start with a very smooth function, you slowly make it converge to original function.",
            "We don't really have theory justifying why that works better, but it does an you won't use accelerated gradient method.",
            "You'll use some fast method instead."
        ],
        [
            "There's a related result if you have saddle point problems.",
            "I just want to mention that, but I think this is less relevant for this crowd, but it's kind of become a very hot topic lately."
        ],
        [
            "Another thing you can think of doing is rewriting your nonsmooth problem is a constrained problem, which almost seems like a crazy thing to do.",
            "'cause constraint problems are not so easy to solve."
        ],
        [
            "But a bunch of constraint problems actually are not too much worse.",
            "So let's think of I have some generic differentiable function an I add an L1 regularizer to that.",
            "It seems like it's not so bad we're actually smooth along any orthant, it's just when we cross or fence."
        ],
        [
            "Turn on smooth.",
            "So I can rewrite that in various ways.",
            "I can rewrite it with non negative variables, a smooth function plus another smooth function."
        ],
        [
            "There's another, there's a few other ways you can write it as a constraint problem."
        ],
        [
            "Basically you end up with a smooth objective with simple constraints and.",
            "These are actually much easier to solve than nonsmooth problems, and we'll talk about what other simple constraints are moment."
        ],
        [
            "Thoroughly.",
            "But first, let's show the algorithm.",
            "So yesterday we motivated gradient descent as solving some sort of quadratic upper bound on the function, and if you have a generic stepsize Alpha instead of L, your algorithm can be written like that.",
            "That gives you gradient descent."
        ],
        [
            "It suggests a very simple constrained alternative.",
            "This is our approximation to the function F. Instead of minimizing over, the real space will just minimize that function over the convex set, right?",
            "This is our upper bound on the function.",
            "We have some constraints.",
            "Let's minimize their upper bound but satisfy the constraints."
        ],
        [
            "So that's called the projected gradient algorithm, so you can rewrite this as you take a gradient step and then you compute what's called the projection.",
            "You find the closest point inside your constraints to the point you move to."
        ],
        [
            "As a picture, we have our function in some point X."
        ],
        [
            "We have a feasible set, so some convex set that we want our parameters to lie in."
        ],
        [
            "We take our gradient step and that will move us outside the constraints.",
            "So we need to live in this blue region, but we've gone outside of it, So what you do is."
        ],
        [
            "You find the closest point inside the blue set to the green point and."
        ],
        [
            "You follow that direction.",
            "And as you can see, it's going to stay inside the set for obvious reasons.",
            "But also it's going to make it's not going to make more than a 90 degree angle with the green line, so you know that it moves inside the level curves and it's going to improve the objective.",
            "For what sort of constraints?",
            "This is on the parameters, yeah?",
            "The that that would be a little bit weird, that's called.",
            "There are methods to do that, I'm not.",
            "I don't think I'll.",
            "Go over them today though, it gets into some weird issues when you do stuff like that that can work, though there is theory saying that that's an OK thing to do.",
            "Normalizing by the gradient norm.",
            "That that can be OK.",
            "I don't know exactly what grading clipping is, so like.",
            "Yeah, that can work.",
            "You have to be a little bit careful, but that can work."
        ],
        [
            "OK, so the thing that's really nice.",
            "What projected gradient method is that has the same convergence properties as the gradient method?",
            "So all the things I talked about yesterday for smooth functions, they basically apply almost with the exact same constants for projected gradient method."
        ],
        [
            "So you can start to do things like, well, let's make a projected version of sag to solve constraint problems, or SVR G. Or will do Barzilai Borwein step size for projected gradient line search acceleration.",
            "All this stuff I could put like a whole bunch of references, but it's basically all the convergence properties get get."
        ],
        [
            "Reserved.",
            "You can also talk about projected Newton methods, but those are get a little bit more complicated because if you want to project and properly you can't project under Euclidean norm.",
            "You have to project under Norm that's defined by the Hessian, and that is basically always hard.",
            "So, but there's two methods that kind of work in practice.",
            "If you just have something like bound constraints.",
            "If you just want your your variables to stay between upper and lower balance, there's something called 2 metric projection, and then if you have other simple constraints in a very costly objective, there's there's inexact projected Newton methods."
        ],
        [
            "So what are simple sets?",
            "What are sets that we can quickly compute the projection onto?",
            "Well, you can have upper and lower balance on the variables.",
            "You can have a few linear constraints.",
            "You can say that your norm has to be bounded above by some value, or you can say that your norm has to be bounded above by something that's also a variable, which is called a norm cone.",
            "You can say that your valuable stuff too.",
            "Many norms.",
            "We'll say so.",
            "So in my thesis I give how to do this for kind of the standard norms you would think of, like LL 1L2L affinity and like mixed L12 and things like that.",
            "You can also have things if you want your variables to be probabilities or if you want a disjoint intersection of all these.",
            "So if you want if you have blocks of variables and you want each of those to be a probability, that's OK, but if they overlap it gets a bit more complicated.",
            "So if you have any of these constraints, we actually solve very large problems as fast as if we didn't have the constraints there.",
            "So you're going to run into like a cubic type of thing if you go bigger, so you can have like a million linear constraints, but you can have like 1000.",
            "Will depend on the number of constraints and the number of variables.",
            "'cause that will be the size your matrix representing the constraints.",
            "It's cubic in the like?",
            "Yeah, yeah, either the number of constraints are the number of variables.",
            "OK, so yeah, these are these are easy constraints.",
            "If you have these constraints, you don't even worry about it, just do projected grading or projected stochastic."
        ],
        [
            "Radiant.",
            "If you have the intersection of these, there's something called Dijkstra's algorithm, which is not the Dijkstra's algorithm graph theory.",
            "It's the Dexter's album from projection, and he's a statistician.",
            "I believe it's from the 80s, and it's also a very famous Dijkstra's algorithm.",
            "There's a Y like it's spelled differently to disambiguate if you're writing it."
        ],
        [
            "OK, so.",
            "There's a generalization of projected gradient methods called proximal gradient methods, so they include projected gradient methods as a special case and therefore functions that look like this.",
            "You're minimizing F of X, which is smooth and RX, which might be nonsmooth ORX could be the L1 or RX could be this weird function that zero if you satisfy your constraints and it's Infinity if you don't satisfy your constraints, and that will give you projected gradient as a special case.",
            "So this R can be a very general convex function."
        ],
        [
            "And the update looks like this.",
            "We have our usual gradient update here are approximation of the smooth function and we're just going to add the nonsmooth function to our approximation.",
            "We're not going to, really.",
            "Approximate that at all.",
            "We're just going to use the nonsmooth function exactly.",
            "Where when you want to prove the rates you assume it's convex.",
            "If you want the item to work.",
            "It's a little bit complicated, but as long as F is deterministic it's fine.",
            "And if F is the castec, it's basically almost always fine too."
        ],
        [
            "Oh sorry, yeah.",
            "So in this stochastic non convex case you have to watch out a little bit.",
            "Your noise isn't too big.",
            "Which is just a very subtle point that's not really well known, and.",
            "In practice, it seems to work anyways, but the theory says that some things can go wrong there.",
            "OK, so we can rewrite this as we do a gradient step on F. We take the grading of our smooth function.",
            "We take a gradient step and then we do this thing called the proximal operator on our nonsmooth function R. For a lot of functions, we can compute this proximal operator and the convergence rate is going to be the same as we were, just as if we were just minimizing F. So even though our can be this weird linear function that's like Infinity somewhere in some places, so it's definitely not Lipschitz continuous in any order.",
            "It can be nonsmooth, whatever, it's almost as if we just care about the properties of F. So if F is strongly convex Lipschitz continuous gradient, you get linear convergence rates, so it's solving certain nonsmooth problems at the speed that we solve smooth problems, and I think this is almost like the most elegant idea in nonsmooth convex optimization."
        ],
        [
            "So the proximal operator is that thing."
        ],
        [
            "I'll just show you what it is for L1 regularization.",
            "It's actually something called iterative soft thresholding which was invented for L1 regularization.",
            "Before we had this nice theory of what proximal gradient methods were.",
            "So you're taking your gradient step and you're applying what's called the soft threshold operator, which."
        ],
        [
            "I can just show it with an example, so if this is the result of our gradient step."
        ],
        [
            "The standard threshold operator with the regularization parameter of one would be if you're less than one in absolute value.",
            "We set you to 0, otherwise you keep your value."
        ],
        [
            "And the soft threshold is well.",
            "You also move those two zero with a value of 1.",
            "So very simple, very cheap operation.",
            "And the nice thing is, if you're thinking about doing L1 regularization, the algorithm is actually setting things to exactly 0.",
            "So a subgradient method will only set things to zero.",
            "In the limit.",
            "A smoothing method will never really set things to 0.",
            "This algorithm is explicitly setting values to 0."
        ],
        [
            "OK, so for what problems can we apply those?"
        ],
        [
            "Says, well, there's L1 and group L1 or the big ones.",
            "There's also a few."
        ],
        [
            "Weird matrix problems, but they get a bit more expensive.",
            "There's all the things we had before and a few other simple regularizers.",
            "Basically, anytime your regularizer is a sum of a function applied to individual variables, you can compute that proxamol operate."
        ],
        [
            "So we can solve all those nonsmooth problems, the same rate as if we just had a smooth problem."
        ],
        [
            "And you can again do a lot of the same tricks.",
            "In this setting.",
            "You can do your line search.",
            "You can do like a two metric method if you want to.",
            "Newton style version of this you can apply Sagan SVR, G and so on.",
            "Some of these are more recent results, but the the idea that you can really treat this class of nonsmooth problems as if they were smooth is very nice, and you can do all the same.",
            "Fast things.",
            "OK, are there questions or comments on the proximal gradient method so something you can you can very easily add to your stochastic gradient method if you want to have an L1 regularizer or something like that.",
            "That's right, yeah.",
            "So I think.",
            "I think it first appeared in a paper by Ducey and Singer, and they called it Phobos.",
            "But proximal gradient is sort of the name that we're encouraging everyone to use so that we don't have.",
            "This is another name that was reinvented many times, and there's a million names for it, so proximal gradient is kind of the.",
            "The standard method.",
            "That's a great question.",
            "So so not true.",
            "Optionality conditions gets into some nasty stuff.",
            "In principle.",
            "What you do is you take the subgradient you set.",
            "You want to find if zero is in the subgradient.",
            "So for L1 that ends up being not too hard to do, you sort of.",
            "You take the subgradient and you can work out.",
            "Work it out and for a few of these constraints it's when you start talking with constraints you have to get into weird things like normal cones and stuff so it's actually not always easy to do.",
            "One thing you can always do in one day is just to buy section to compute these things.",
            "So you can compute it numerically very quickly.",
            "This is just a 1D problem.",
            "You can have very fast convergence.",
            "Other questions or comments on proximal gradient?",
            "So yeah, just just you can just add this to your stochastic gradient, it's mostly OK."
        ],
        [
            "Another thing you might hear of is this alternating direction method of multipliers.",
            "So this is for optimizing a function F of X + R, Y where there's some linear constraints on X&Y that might be tying them together.",
            "And the method really just alternates between a sort of approximal operator on F and approximal operator on R. And then there's some update of LaGrange multiplier."
        ],
        [
            "There.",
            "It doesn't look like our standard machine learning problem, but what you can do is you can actually introduce constraints to turn your problem into this form.",
            "So if I have something like F of X plus RX, well I introduce new variables Y and I constrain them so that X is equal to a Y.",
            "And now I can apply this method and that's a very common thing when your regularizer or your function is quite complicated.",
            "And there's versions where instead of applying the proximal operator, you just take a gradient step on each each thing, and that's called linearized."
        ],
        [
            "Adm, so there's."
        ],
        [
            "Various ways you can convert.",
            "So for maybe for more complicated constraints that's sort of appealing method to use.",
            "It's definitely not my favorite method 'cause it gets a bit more messy, but but sometimes your choices are either this or some sort of inexact proximal gradient method."
        ],
        [
            "There's another thing you'll hear if you go to NIPS and ICML called the Frank Wolf method.",
            "It's not.",
            "I was really debating whether to include this 'cause it's not so clear it's relevant in deep learning, but the idea is sometimes this projection can be hard to compute."
        ],
        [
            "And So what you do is you just ignore that last term, you just minimize a linear function over your convex set, and in some cases you can do that efficiently.",
            "So if you talk about structured SVM's, you can do that efficiently, and this is a very nice method in that case, and under some conditions you can still get fast convergence rates for this type of problem.",
            "So, so that's kind of more for weird constraint objective function interactions.",
            "OK, so there's a bunch of other tricks, but I don't want to go over them 'cause I don't think they're really that relevant for this audience, at least from my perspective.",
            "So I just want to conclude this section section by saying that there's really no black box method to beat subgradient methods.",
            "So if you just give me a nonsmooth convex function, I can never design A method that only works on subgradients.",
            "That works better than the basic subgradient method, which doesn't work very well.",
            "But for most objective functions, once you actually see the objective function, you know something about it.",
            "There's a bunch of tricks you can do to actually solve it.",
            "Very cool."
        ],
        [
            "Glee.",
            "And the problem is, there's no elegant answer.",
            "In this case, you can't beat the black box method.",
            "You always have to know something about your function.",
            "So if you want to solve nonsmooth methods efficiently, you unfortunately have to either memorize these tricks or wait for someone to write a code that implements all the tricks and somehow analyzes your problem, which people are working on.",
            "But I don't.",
            "I wouldn't wait for that.",
            "You can probably solve your problem faster by just if you can recognize the problem class, you can do it, and I've sort of highlighted these.",
            "Four 'cause I think those are those are the ones that I've really used in practice and I think work really well and.",
            "I think those are the most important ones, so two metric projection.",
            "That's where if you have like bound constraints or L1 type of things.",
            "Proximal Newton is simple constraints but expensive objectives.",
            "Frank Wolf is maybe less simple constraints and then you can also go to the smooth dual.",
            "So if you have a nonsmooth problem that strongly convex, the dual problem has is actually convex and smooth, which is a nice property.",
            "So if you have a nonsmooth problem, and you can take the dual, you can convert that into a smooth problem.",
            "This is in principle true.",
            "Like neural networks.",
            "With Relu, you can go to the dual, the dual is convex, the dual is smooth.",
            "Well, that's actually the dual may not be smooth, but the dual is convex.",
            "I just have no idea what it looks like, or if you can compute it or anything like that, and I don't know if people have looked into that or not.",
            "Yeah, OK, maybe I'll send an email to Nicola and he can think about it on one of his biking trips.",
            "OK, are there other questions or comments about the Nonsmooth case?",
            "So unfortunately, it's not like the smooth case.",
            "You don't really get a very satisfying answer to solving nonsmooth problems.",
            "It's definitely less elegant.",
            "There is an entire field dedicated to that question.",
            "Oh OK, so there's two issues.",
            "There's one issue is whether you're recovering the true variables in like, a signal processing sense, and then there's one like does your you know the optimal solution has some set of nonzero variables?",
            "Does to category and actually find those nonzero variables.",
            "So which one are?",
            "No, you don't.",
            "So if you place the cast agreeing with the proximal operator, you keep setting things non 0.",
            "Always there's always some probability you move something away from zero 'cause you're decreasing the stepsides so the proximal operator gets weaker.",
            "You keep.",
            "You don't really ever identify the non zero variables.",
            "This is why you.",
            "This is why the Phobos thing doesn't really identify the nonzero variables.",
            "It gives you sparse iterations, but the sparsity pattern change can change on every single iteration.",
            "There's something called dual averaging, which basically you take the average of the gradients and do the proximal operator on that.",
            "That will give you the sparsity pattern, or you can run Sagan SVR G. They also give you the sparsity pattern in a finite number of iterations.",
            "There needs to be some sort of averaging 'cause random stochastic gradient can point you point you in any direction and make things done 0.",
            "So so most.",
            "So it depends what algorithm you're using.",
            "If you're using regular stochastic gradient descent, there's no reason to actually smooth it.",
            "You can actually just use the subgradient method.",
            "If you're using something that can get a faster rate, like sag, or you're using a deterministic method or a growing batch method, then you want to think about these issues.",
            "'cause then there's gains to be had.",
            "OK."
        ],
        [
            "So.",
            "That stuff I'm kind of very comfortable with, so let's start to move on to the two topics that I've only recently started to really think about.",
            "So let's talk about the Nonfinite case."
        ],
        [
            "An by that we're going to.",
            "We're going to go back to our smooth and strongly convex assumption.",
            "So I really wrote and in the title, but I'm going to do these three things separately as and or so smooth and strongly convex, and I really want to minimize our truly stochastic objective.",
            "So this could be that I you know there's some error in how I'm measuring the functions.",
            "Or it could be that you know I have.",
            "I have an infinite stream of data coming in.",
            "I'm only looking, I can only I can only approximate in that.",
            "So this includes the generalization error in machine learning, so you don't really care about how you perform on the training data.",
            "You want to know how you do on the test data.",
            "You want to know how well you do on this expectation, your performance, overall data you could possibly ever see I have some puzzled looks.",
            "Is there a comment coming?",
            "So so so.",
            "If we want to start talking talking about noise in the test data, then this is 1 framework to look in."
        ],
        [
            "And there's this very nice paper by both 2 and biscuit or a couple papers and they show that if you want to start analyzing this function, you can decompose your error into two parts.",
            "You've got the optimization error, which is exclusively what we've been talking about up to this point, and you've got also got the estimate estimation error, so the optimization errors you have N samples you saw.",
            "How well do you solve the optimization problem restricted to those end samples?",
            "The customization error is saying how well do those end samples approximate the true error?",
            "If you really want to talk about generalization error, you also have to talk about how well your model can approximate the true underlying function.",
            "I'm not going to worry about that today because I don't have many things to say about that.",
            "You can, you can kind of hide that into estimation error, but analyzing this thing is a hard issue.",
            "So usually you just look at these two."
        ],
        [
            "Terms.",
            "OK so boom boom Buscat looked at these two sort of strategies.",
            "So one strategy which is kind of what we talked about the first time is OK. You can generate as much data as you want.",
            "What should you do?",
            "So one thing you can think of doing is you have to have a certain time restriction T you can.",
            "You can evaluate gradients and that's how much time you have to run your algorithm.",
            "So what you can do or or well.",
            "Not not quite.",
            "You can generate T samples of your function, so one strategy is you generate those T samples and now you run LBF GS on them or something like that and you just exactly solve your approximation on that training set.",
            "So you can get your optimization to zero an analyzing the estimation error is a little bit more complicated, but under certain assumptions you can get it to 1 / T and we have reasons to believe from the Cramer Rao bound that it's probably not lower than 1 / T in some settings.",
            "Right, so I'll get to that on the next slide.",
            "I'm definitely getting there, so I just want to."
        ],
        [
            "Cross that with the other strategy, which is you have your tea samples.",
            "Let's just run stochastic gradient through RT samples one time.",
            "So our optimization error as we showed last time was 1 / T or estimation error.",
            "Still depends on those T samples or estimation error is in principle the same as it was.",
            "In this case we're just using T samples and that suggests the strategy that Joshua just mentioned that if we only have can evaluate gradients well, this is definitely going to take at least the gradients, probably more than T gradients.",
            "So why not just do this 'cause it seems like it's we're doing better in terms of epsilon there.",
            "And.",
            "Answer."
        ],
        [
            "Question is.",
            "This result seems to indicate you should just just go through your data set one time with stochastic gradient, and this paper actually made it very."
        ],
        [
            "Hard to get optimization papers accepted for a few years.",
            "Because it sort of gave theoretical justification for this motivation that well, if we improve on the optimization error, it doesn't really help us.",
            "'cause ultimately doesn't affect the test error.",
            "Old.",
            "Oh yeah, good point.",
            "So~ means there might be a log factor hidden in there, so it might be like log T / T or something like that.",
            "So you know there's a more you can Wikipedia over~ but yeah, there's there's something there might be something like a log T in there, or log T squared or something like that.",
            "So something that is not really very big.",
            "No, because the estimation error is not a property of the algorithm.",
            "The optimization error depends on the algorithm you chose the estimates.",
            "The estimation error only depends on the samples you chose, which, and then you can apply any algorithm you want to them.",
            "So these two are really the same.",
            "It's only these two that are variable."
        ],
        [
            "OK, so let's do a survey.",
            "How many people in the room only go through your data set one time?",
            "Do a little size.",
            "That's a great point.",
            "So if you have an infinite data set, then the answer is actually you know you don't even go through your data set one time.",
            "But let's say you don't have an infinite data set.",
            "Do you go through the data set more than once?",
            "I see some hesitant nods, nodding and some groggy smiles, smirks, and things like that, OK?"
        ],
        [
            "So I like this quote 'cause it is from some of some some people who really know what they're talking about, more so than me in this infinite data case.",
            "So the overwhelming empirical evidence is that for all actual data, the ERM is actually better in terms of test there, and we have no understanding of why this happens.",
            "So.",
            "It's potentially not completely useless to run an optimization algorithm that's faster than stochastic gradient."
        ],
        [
            "And the reason is actually pretty simple.",
            "It's that the constants matter.",
            "So let's say stochastic gradient is optimal up to a factor of 2.",
            "It sounds really nice from like a complexity theory perspective, but if I tell you well, you can do this very fast algorithm, But it's going to get you your double test error.",
            "It's going to double your test error going from 10% to 20%.",
            "Practitioners care about these constants, right?",
            "If I'm going to double your error, you don't want to do that.",
            "So those constants matter, so stochastic gradient, it's true, it's it is optimal in terms of sample size, but there's other things that matter.",
            "An you may want to have good dependency on those 'cause stochastic rate actually doesn't have very good dependency on those."
        ],
        [
            "So there's been a few.",
            "There's been this birthday all said that, well, the erm, the stochastic gradient are not the only possible things you can think about using growing batch sizes, and they showed that with growing batch sizes, you can basically achieve the same thing you do with stochastic gradient method, possibly with lower optimization error.",
            "Another thing you could think of doing is well, let's just instead of taking T samples, I'll take some some number less than T and I'll revisit some of the examples.",
            "I'll apply sag to a smaller data set.",
            "So now my optimization accuracy is way better.",
            "I have linear convergence rate, but my my estimation accuracy is worse.",
            "It's now 1 / N instead of 1 / T. You're going to ask me that again next summer.",
            "I have a student who really wants learning, but I haven't gone through that literature yet, so I don't.",
            "OK yeah, I actually have no idea.",
            "That would make sense.",
            "It's very hard to beat Cramer Rao bound.",
            "I'll show you how you can beat it in two slides, but it requires a very crazy assumption.",
            "OK, so the idea here is, well, we made our estimation error worse.",
            "But if we can make our optimization error much better, we can.",
            "We can improve the sum of those two, so there's no.",
            "There's no proof here, it's just sort of obvious observations."
        ],
        [
            "So if you're in the case where actually your optimization problem is very difficult, in principle, you can do better on the test error by having a fancier optimization algorithm.",
            "Are people.",
            "If your optimization problem is very difficult, so when you run stochastic gradient it just is doing terrible at solving the optimization problem, then you can do better by looking at fewer samples and solving the optimization problem more accurately.",
            "That just follows from.",
            "It's the sum of the two terms.",
            "There's there's a.",
            "You can make one bigger if you make the other, you can make the sound bigger if you make one bigger and the other one much smaller.",
            "Yeah.",
            "Just resting the arm.",
            "Anyone else resting arms?",
            "So he's literally like this.",
            "Just in case you think I'm the I'm crazy or something.",
            "I have it's good to interact with the audience, right?",
            "It's good to answer questions and things like that."
        ],
        [
            "OK so here is, I think the most fascinating result that's come out since the both 2 and boost get paper as is just this year.",
            "It's called the streaming S PRG algorithm.",
            "So we start with some initial point and some initial sample size end."
        ],
        [
            "Not the same as the previous size.",
            "Think think like one or two or something like that."
        ],
        [
            "We're going to run SVR G. Instead of computing before this D, which is based on the full set of data set, we're going to end fresh samples and compute Dr Full gradient on those end samples.",
            "Then we're gonna run SVG for EM iterations on fresh samples, we're going to generate new data points, so it's almost like a true stochastic gradient method, except for occasionally we stop and we compute the average of a bunch of gradients, and we're going to increase our sample size as we go.",
            "Um?",
            "Step.",
            "Of the older iterations.",
            "So this is what is going to let us control our variance, so we're going to take a bunch of samples, take the average of those so that we know what the general overall direction that we should be going in is, and then we run stochastic gradient.",
            "But we're going to correct it by the direction, the overall direction that we know we should be going, and then this is almost like I don't want to call it a second order approximation, but it's definitely a difference in gradients that sort of correcting the overall direction based on the current sample in the current.",
            "Current data data point that we sample.",
            "Oh, it's different than regular SVG 'cause we keep generating new samples.",
            "We're not.",
            "We don't have a finite data set anymore.",
            "We can just run this forever if we have an infinite data set.",
            "You can also.",
            "So so, so regular SVR G. You would do this step on all of your end samples and then you'd sample these from those same end samples.",
            "So here the.",
            "So in the normal 1, they're roughly the same size.",
            "Here M is fixed and N is getting bigger.",
            "So we're actually spending less time doing stochastic gradient and more time fearing what the average direction we should be is overtime.",
            "So in the beginning, we're doing a ton of stochastic gradient steps, but as we go along, we're more trying to figure out what the direction is to keep making progress.",
            "Please try out this album.",
            "I'm very curious to hear how it works."
        ],
        [
            "'cause the empirical properties are fascinating.",
            "So I mentioned the polyak Ruppert result yesterday, which is that if you do, if you start averaging your stochastic gradient iterations after a while, we know that that has the asymptotically optimal efficiency in a statistical sense.",
            "That's in some sense the best thing you can do after a certain point.",
            "This is the first algorithm that is in some sense achieves that in a non asymptotic regime.",
            "It does does as well as the ERM in the non asymptotic case, even when you care about the constants.",
            "So I think this is wild.",
            "I think the.",
            "The yeah it's crazy and there was this comment yesterday about, well, you know all these things you're talking about how these crazy strong assumptions do they ever actually apply?",
            "And I think this result is kind of interesting, because where did this algorithm come from?",
            "It came from trying to develop better algorithms for the finite sum problem based on the stochastic method, and then SVG came along and sort of fixed the memory of SAG, and Now this comes along and it's actually saying something more powerful in the infinite data case than we ever had before.",
            "I'm very curious to know if this actually works.",
            "I have no idea if it does.",
            "I haven't tried it out.",
            "Oh, because it achieved.",
            "If you have second order methods, in principle you can get the optimal efficiency, But this already gets the optimal efficiency.",
            "This is already the best you can do in terms of test there.",
            "Um?",
            "I don't know, this worked too well, so the.",
            "Um?",
            "It's a little bit more than that.",
            "It's because the the asymptotic regime you care about, the Fisher information matrix.",
            "So you have some norm defined in terms of the Fisher information matrix.",
            "And when you do Newton, you can sort of cancel those effects out, but you can also achieve that asymptotically with averaging stochastic gradient, and this result is saying that you can sort of cancel that out with this algorithm in a non asymptotic way, which is a result we never had even for 2nd order methods before.",
            "Yeah, let's say at like a high level that I agree with that, but.",
            "Maybe making that mathematically precise I'm I'm less clear.",
            "One thing that's true is Sagan SVR definitely doing something like momentum there just sort of have cleaned it up a bit to to take into account the finite data set.",
            "Yeah, so you can.",
            "You can definitely think of DS is like your momentum, but now your omentum is actually an explicit older gradient approximation that's getting more accurate overtime.",
            "And at the same time these two things are getting closer and closer together, so you're more focusing on the DS Anyways.",
            "Yeah.",
            "Well, there's there's many ways you can talk about repeating the samples, so if you really just have a finite set of samples, then you're in the regular SVG setting and all the stuff we talked about yesterday applies.",
            "So.",
            "Yeah.",
            "You can also do that for SVG.",
            "There's a little bit of theory that needs to be done to show that's OK, but my student did it last summer.",
            "So you can also do that for SVG.",
            "You can just increase the batch size to make it quite a bit faster on big datasets.",
            "Was.",
            "I don't know.",
            "I'm not sure, so I only learned about this a few weeks ago, 'cause 'cause Cham Kakati gave a talk in the same session as me at the optimization conference, but.",
            "I I kind of assume that you can beat this definitely in practice by by recycling things like or or reusing computation somehow.",
            "This is definitely not.",
            "Absolutely, and The thing is, you're in some sense almost wasting more computation overtime.",
            "'cause these accesses are getting closer together, and these devices are getting exponentially larger overtime.",
            "So yeah, I mean these are theoreticians they're not so worried about the constants or how it works in practice, but as a theoretical result, I think.",
            "I think it's amazing, but there's definitely ways you can attack it.",
            "Prove to make it work better in practice.",
            "This.",
            "If you want to start talking with test error.",
            "If you're happy talking about training error, you can do your fixed.",
            "You can do your growing batch size on your fixed subset and look at more and more samples either here or mini batches here if you want to either either one.",
            "Yeah, that's true, but the theory would still apply.",
            "This is a non asymptotic theory, so in principle if you go through data set once with this thing, that should give you approximately optimal empirical risk minimizer.",
            "That should do as well as anything can do on your data set.",
            "But again, we care about the constants.",
            "With 11 pass, you only do one pass, you just do this algorithm for one pass.",
            "In principle, that's this is even more optimal than stochastic gradient in some sense, But the constants matter like there's like some one 60s hidden in this analysis.",
            "I think you're still.",
            "I think you can do that, but I think you still lose the test error.",
            "Things 'cause you're still biased by that finite set.",
            "Oh yeah, so you can definitely like run this on like like treat your finite sample as an infinite sample.",
            "Run this algorithm.",
            "But I'm not sure if you would want to do that as opposed to just running SVR G because the.",
            "This once.",
            "Yeah.",
            "That's true, but as soon as you start recycling things then you introduce a bias based on the training set.",
            "So yeah, so in principle this algorithm cannot overfit.",
            "'cause you're always using new samples.",
            "Right, you know you you do.",
            "Look, you do look at this example twice to commute this difference, but you're always looking at new data.",
            "So in terms of if you're never revisiting data points, you can't adapt to those specific data points, you can't fit the noise in the data points.",
            "If you get an infinite number of such samples, you.",
            "You win, right?",
            "You've solved the problem.",
            "Right?",
            "Yes, yes, so so there there are regularity conditions for sure, so that's the statement I made.",
            "Is is too strong, but it's not.",
            "It is in the right direction in the sense that if you keep using.",
            "Yes, that's right.",
            "Yes, well.",
            "Assuming that your model is correct.",
            "Right, so so there was there was there was three."
        ],
        [
            "There's three terms here, so if if you have the right model.",
            "Yeah, even then there could be a problem because it could be that the one with the best generalization error in your family is not the one that does the best on the samples.",
            "Like minimize minimizing the minimum of the model class might not be the best thing in your model class when it comes to test time.",
            "So you know if your model is correct, you just have these two terms and then it truly is minimizing the test error.",
            "Like assuming your loss function is right, but all bets are off if your if your model is not correct.",
            "So it's the closest thing we have to directly minimizing the test there, but we don't have anything that can do that.",
            "There's always strong assumptions hidden somewhere.",
            "You have to try though, right?"
        ],
        [
            "OK, are there other comments on this?",
            "Yes.",
            "To calculate.",
            "Oh I, I didn't say how to do it.",
            "I think it's something like 1 / L, But I would have to look at the paper.",
            "The Lipschitz constant of the maximum over the gradients.",
            "Yeah.",
            "Right, OK?",
            "Yeah, so we.",
            "There's definitely IID assumption here.",
            "So you can.",
            "You can do the best in terms of on those datasets in terms of the noise you add, But if that's if your test data set doesn't come is not noised versions of your training set, then it doesn't mean much in terms of test errors so.",
            "Yeah, so you can learn to do those better on your training data.",
            "But that's probably not what you want.",
            "You probably want to look at new images, and in fact, like the data we have in practice is almost never IID.",
            "So, so all of this is approximations.",
            "Yeah.",
            "So I put my student in charge of figure out how to build a black box, neural that optimizer, and so he's trying to implement everything like we have, like a degrad momentum Adam are prop, RMS, prop, SVR, G and so on.",
            "So and then I lost.",
            "I met with him.",
            "I'm like OK, you've done all these things.",
            "If I give you a neural network problem, I tell you nothing about it.",
            "You just get functioning gradients.",
            "You want to use it.",
            "A black box method to solve it and you want it to work the first time, no tuning.",
            "Which method would you choose and he gave Me 2 and one of them was a sphere G. The other one is an algorithm we haven't published yet.",
            "And actually we decided not to publish that SVG result because we think we can do better than the existing one.",
            "OK. One more question, maybe?",
            "And then I need to move on.",
            "I think it's good to stop and appreciate this.",
            "This is a very nice result I think.",
            "Very complicated paper.",
            "I have definitely not gone through the proof in the subtleties."
        ],
        [
            "OK, and I just want to say that.",
            "You can beat these balance.",
            "You can be 1 / T you need.",
            "You need some sort of strong assumption to do it, but in principle you know the Cramer Rao Cramer Rao bound relies on assumptions and apply.",
            "So a big hobby of mine optimization is beating these lower bounds that people come up in optimization algorithms.",
            "But you can also do the same thing for the generalization error.",
            "So me and Nicola Ru student here who is my office mate in Paris.",
            "We did a lot of fun things one afternoon.",
            "We looked at this crazy assumption that appears in some old optimization papers.",
            "I'm going to say that all the gradients, the norm of the gradient of every individual function is less than some number.",
            "BB can be whatever you want times the true gradient at that point.",
            "It seems reasonable until you think about it and you plug in, the gradient is 0 and then you realize you actually need that.",
            "Your optimal solution to minimize every single function.",
            "So you have exactly fit the training set.",
            "Very strong, yeah you should.",
            "You should not be happy with the assumption we wrote the paper.",
            "We put it on.",
            "Archive is actually got a few citations and people asking us why we didn't publish it, but it doesn't seem to apply in like real scenarios, but the."
        ],
        [
            "I want to make is.",
            "For every I yeah OK yeah that's true for every X you have this condition.",
            "And then for every eye it has to be a minimizer of the true function.",
            "That's true, and that's what I'm getting too.",
            "'cause I notice during the summer school like I just added this last night, 'cause in the summer school you've always talked about, how well you don't really want to solve the problem 'cause you that overfits.",
            "So that's."
        ],
        [
            "That's what what when I get to our result which which you know mean Nicola like did this in an afternoon so we worked on such harder problems.",
            "This was kind of like a fun thing to unwind after sag.",
            "I guess that's what you do in Paris.",
            "If you're really nerdy.",
            "OK, you get a rate that looks like this.",
            "So if I didn't have this be squared here, this is exactly the rate of gradient descent.",
            "So looking at the entire data set, taking my exact gradient step, linear convergence very fast and you have this fudge factor based on B, ^2 B squared is 1.",
            "It actually is the gradient method if all your functions are exactly the same.",
            "It's also the gradient method, but you're just looking at one function instead of looking at the mall, so you lose a little bit factor, but you get a very fast rate.",
            "So this should not be taken as a like formal statement of approval or anything, because obviously this assumption is a little bit crazy, but if you're really expecting to overfit, maybe a constant step size is all you need, as Yoshua kind of just said.",
            "Like if you're if you really think your model is going to overfit and you really only care about getting up to a certain accuracy.",
            "Maybe you should just use a constant stepsize.",
            "Maybe you don't worry bout convergence at all.",
            "If you want to get good test there.",
            "This is really a result on the expectation in the test error setting.",
            "This saying my test error is going down exponentially fast if I have this condition.",
            "Up to up to that.",
            "Model being correct assumption.",
            "So I don't want to spend too much time on this.",
            "I just wanted emphasize that there's more going on.",
            "That's true.",
            "Yes, that's right.",
            "Yeah, definitely in this setting you can definitely converge faster.",
            "Yeah, I hadn't even thought about that.",
            "Actually there is.",
            "There is a paper this year on that incremental noon, but they only show linear convergence, which is weird.",
            "'cause in principle you should get super linear convergence in that setting.",
            "Extra is the minimum of the function F the minimum of the expectation.",
            "Note that this is the true average F the true function F0F was this this expectation."
        ],
        [
            "Generalization error here, sorry I guess I missed that notation, so F of X is the expectation of F of XF of X is.",
            "The expectation over examples this is your generalization error.",
            "F of X is 1 function and F of X is the test is the generalization error.",
            "So this is."
        ],
        [
            "This is saying that if you have this crazy assumption, you're.",
            "Oh so X star by our definition minimizes F. And the and the extra thing implied by this assumption is the XR also minimizes each FI.",
            "So saying you're fitting each data point exactly, no, no zero training error, absolute zero training error.",
            "You yeah.",
            "Yeah.",
            "No, I think this.",
            "I never really thought about this before, but maybe this could actually be used as somewhat of a justification for just using a constant stepsize and then just stopping at some point.",
            "Every single.",
            "I do, I don't know if I do.",
            "As I said, I you know the type of person who might enjoy the Montreal nightlife rather than go to summer school, but you probably do.",
            "OK."
        ],
        [
            "I actually don't want to talk about this too much, but there is some work on if the data is not IID in this."
        ],
        [
            "Online convo?"
        ],
        [
            "Optimization where time T you make a prediction X and then someone makes you evaluate your X at some random convex function and you can actually prove."
        ],
        [
            "Kind of weak things 'cause you can only compare to how you would do with the best X, but you can still prove some sort of regret bounds in that setting, but I think I won't want to move on for time, but I just want to mention that if you if you do care bout ID data, there are some analysis methods that do let you analyze that setting."
        ],
        [
            "OK, so let's get to the last part which is non convex functions.",
            "What everybody is here to see right?"
        ],
        [
            "OK, so the two classic."
        ],
        [
            "Perspectives on non convex optimization.",
            "So one is the local perspective.",
            "So we're going to apply some method that has good methods, good properties for minimizing convex functions.",
            "We might do like random restarts, or we might do some modification to make sure it eventually gets near a local minimizer.",
            "But roughly you've got some first phase that's going to try and get you near minimizer once you're near minimizer, you have the second phase that's going to play a nice method for convex functions to just sort of find that local minimizer."
        ],
        [
            "Nicely.",
            "And the question is, how long does the first phase take like?",
            "Does that require doing a multiple restart and like every single error of your space?",
            "Or does it require running stochastic gradient?",
            "Like for an extremely long time?",
            "Something like that?"
        ],
        [
            "And then there's the global perspective, which is even more ambitious is let's actually search for the global minimum of some function class.",
            "So I I just have like some weird function.",
            "You probably assume it's athletes list it's continuous or something like that, and I really want to find the global minimum.",
            "And as I mentioned yesterday, you get these sort of rates that scale very badly with the dimension."
        ],
        [
            "So you can really only solve low dimensional problems and So what I wanted to go over is to maybe think about this framework and discuss well water.",
            "What are some recent things that have been done in the local, the global and hybrid settings?",
            "Yeah, it's kind of overloaded, so there's one.",
            "If you say global convergence rate.",
            "Oh yeah, so I would put those in this category of like global nonconvex.",
            "Well, they often have guarantees like this or worse than they actually rely on eventually randomly visiting every single point in this space.",
            "Right, yes, I agree.",
            "I'm not saying this is a good rate, I'm just saying this is what they have and that's why those types of methods they can only solve low dimensional problems.",
            "'cause often those type of methods have this fall back where they make sure they eventually randomly sample everything that can happen and then they say oh we do converge and that's true in some existential setting."
        ],
        [
            "Incense.",
            "OK, so let's talk about the first type of method 1st and we made this assumption called strong convexity, which looks like this."
        ],
        [
            "In a lot of proofs, what you do.",
            "Is you minimize both sides of that with respect to X to get an inequality that looks like this.",
            "And that's sort of the crucial point where strong convexity enters into the proof."
        ],
        [
            "So recently and actually going back even less recently, there's a bunch of extra assumptions people have made to basically get to an inequality like this, so there's things called essential strong convexity, optimal strong convexity, restricted secant equality, and a few others."
        ],
        [
            "So this is only my postdoc has been looking at, and Steven Wright actually sort of mentioned in his talk at the optimization conference this year that he's also been thinking about this is.",
            "Why don't we just assume this inequality holds?",
            "So forget about strong convexity.",
            "Forget anything like that if that's all I need to prove my convergence rate, let's just assume that that happens somehow, and then let's think about what functions actually satisfy that.",
            "So I've been, you know, we both said we haven't found where this actually comes from that it sort of just pops out of the proof, but I did a lot of digging lately and it appears in these these two papers in 1963, which I haven't found yet, because as you might guess from the date and the author names, these are not so easy papers to find.",
            "I had to look up how to make this character in Latech for example.",
            "If you have the strong property, if you just assume this, it's actually weaker than all of these things, so all of these imply that, so it's weaker than having all these things.",
            "It doesn't imply the solution is unique.",
            "So for example, if you have a strongly convex F and javax.",
            "So like a least squares with a singular matrix, it still satisfies this inequality.",
            "It it it does allow you to have plateaus.",
            "Oh, it allows you to plateau at F star, but you can't have a plateau far away.",
            "Yes, that's true.",
            "Yeah, so it's not.",
            "This is.",
            "Yeah, so I'm not saying this is a completely general thing, I'm just saying it's definitely more general than strong convexity, and in particular it actually doesn't imply convexity.",
            "You don't need convexity for this inequality to be true."
        ],
        [
            "So yesterday I think there was a comment that, well, you know, these linear convergence rates even if you're talking about the local sense, they rely on your function looking like this near the optimum, so you assume that it's kind of a bowl, like there's a unique optimum.",
            "The function is going faster than a quadratic function as you go away from it."
        ],
        [
            "It's convex, so if you start talking about things that satisfy that property, you can start talking about things that look like this.",
            "So you don't need a unique minimizer, you don't need it to be convex.",
            "You can have concave parts, and you need the function to be growing a little bit faster than a linear function as you as you go away in any particular direction.",
            "So I'm not saying that this is not going to let you solve all non convex problems, but I'm saying is for certain non convex problems.",
            "We can actually get this exponential convergence rate and in particular for a lot of the problems we care about in machine learning the convex problems, least squares and so on.",
            "You can just solve them and the second point is.",
            "You don't need to be near minimizer like this.",
            "You can be near minimizers like this, and you'll still converge there exponentially fast.",
            "We can define it for.",
            "OK, so there is a generalization of this property for non differentiable functions called the."
        ],
        [
            "Whatever this is pronounced, some other name, I can't pronounce inequality which is useful."
        ],
        [
            "But it doesn't seem to preserve the properties you want."
        ],
        [
            "But we've defined a property for that smooth plus nonsmooth function, which does let you have this property.",
            "So, for example, people were really interested in proving linear convergence of lasso problems, least squares, plus L1 regularization, and they made all sorts of assumptions to prove this is true.",
            "There was restricted isometry property.",
            "There was all these like restricted strong convexity.",
            "There was like statistical estimation rates.",
            "All these crazy things.",
            "Actually, turns out it's just true for that whole problem class.",
            "As long as a does not equal 0.",
            "As long as you have at least one nonzero in your matrix, you're already getting linear convergence by applying proximal gradient to lasso problems.",
            "So not published yet, but my postdoc is furiously writing it up because it's it's kind of neat and there's there's a lot of generalizations you can think of so, so mainly I'm not saying this is going to give you better methods for nonconvex problems, but I'm saying is.",
            "Actually, you can be in kind of a rough area of a local minimum that could look kind of nasty and you should still converge very quickly to it with standard methods."
        ],
        [
            "OK, so can you talk about global non convex rates and I mean global as in like they apply from iteration one.",
            "We know the constants and so on.",
            "So we talked the first day about how for strongly convex problems you can prove that you know the function value has linear convergence.",
            "You can also show that the iterations converge linearly and also the gradient converges linearly.",
            "And then we said well convex functions are not quite as nice.",
            "You now can only show the function values have this 1 / T and the gradient squared also has this 1 / T I actually gradient square is a little bit faster but.",
            "That's not too important, so can we say something like this for non convex problems can actually put in?",
            "Oh well, we know these constants for nonconvex problems and get something like this.",
            "And the answer is no, because proving this would be NP hard.",
            "But let's assume our function grading is Lipschitz continuous.",
            "An R function is bounded below."
        ],
        [
            "And there's some this result by Gadime Alanin.",
            "I've found some older results that are similar, but this is this is a horrible paper to read, but if you look through it carefully, they have this result hidden in here.",
            "That the minimum over your iterations of the normal your gradient is going down as 1 / T, so you're running for T iterations, and you're saying the minimum of the gradient is going to be small and getting smaller overtime.",
            "So.",
            "I'll get the saddle points in the next slide, but roughly this is saying.",
            "This is this is like gradient descent.",
            "For stochastic gradient descent, you need to make sure the noise is not too small.",
            "To get this right, but you can also get it first casted gradient descent.",
            "So saying you run the algorithm long enough at some point in time the algorithm is going to be something that looks close to a stationary point.",
            "It's not saying anything you want minimum, maximum, saddle point, whatever, but it is saying you're going to get to a region where gradient descent goes really slow at some point in time.",
            "You can view this as like a positive or a negative result, right?",
            "You can say like, oh, this is an approximate stationary point, so that seems like a good thing.",
            "Gradient probably is not going to be attracted to a maximum, so at least it's maybe a saddle point or minimizer.",
            "Or you can say that like oh look gradient descent with is is going to get stopped moving at some point in time.",
            "It's going to stop making measurable progress at some point.",
            "Or or at least it's going to be in a region where it's not making measurable progress.",
            "So even if you have a beautiful function at some point in time, it has to stop making progress."
        ],
        [
            "OK, which relates to OK, so so one comment about that.",
            "There's no dimension independent dependent, so so even if you're in high dimensions, that result is still OK.",
            "But of course you're giving up on optimality, so if you have an approximate saddle point, it definitely says that you can just converge to approximate saddle Point, which is something we know."
        ],
        [
            "So let's talk about escaping from saddle points.",
            "So the classical approach is use a trust region method and that lets you have negative eigenvalues in your Hessian, and if you have that, you can actually not.",
            "You don't really necessarily converge to saddle points, you can show that you find like a strict minimizer.",
            "There was also this work by the group here, showing how to modify the spectrum if you have negative eigenvalues, I think you take the absolute value, so in some sense you have the component of the direction you need in.",
            "Inside your things, you're going to go in that direction, and there was a paper this year saying if we're doing stochastic gradient and you take random noise around your point, that can let you escape saddle points where you have at least one negative eigenvalue.",
            "I don't remember if they say you do that quickly, but the idea is the random directions will have some component in this like direction that gets you off the Cliff and that in principle can get you out of saddle points.",
            "I think so.",
            "I think it would have to be like a weird case for this to actually apply.",
            "Absolutely.",
            "But I don't know how to analyze that.",
            "That's that's what they're showing.",
            "They require that you have at least one negative eigenvalue, right?",
            "You could have a saddle point that's like a long flat Valley or something like or something weird like that so.",
            "What's that?",
            "I don't know.",
            "Yeah, so you need that direction going down for this result to apply.",
            "If it's just flat in every direction then.",
            "Yes.",
            "Yeah, of course.",
            "OK."
        ],
        [
            "So here's here's a or do you want to talk about?"
        ],
        [
            "Little bit more.",
            "Yeah.",
            "So again, you do need the negative curvature eyes.",
            "I don't know if saddle points are defined as actually having a negative direction or not.",
            "If you allowed to be 0, but if you if you have a negative direction then you're supposed to minimize your quadratic approximation over the trust region.",
            "So if you've got a negative direction it should pick the negative direction and get out of there.",
            "Yeah, so we're not.",
            "We're not well, yeah it's true so it's so it's not a Newton step it's a it's it's you're minimizing this approximation of the function subject to stay in the trust region.",
            "And unlike a lot of Linesearch Newton methods where you have to modify the eigenvalues to be positive definite in trust region methods you can have a negative.",
            "You can have negative eigenvalues and yes in approximation.",
            "And then when you minimize it it get can get you out of there.",
            "More comments on saddle points.",
            "OK, so let's get to this method.",
            "So it was proposed in 2006.",
            "I really wish it would have been proposed in the 1800s because it would have made optimization so much nicer.",
            "This is what Newton's method really should have been.",
            "So remember, we talked about grading method being.",
            "Your current function value this linear approximation and then I had an L here, but now I'm going to say I'm going to put the exact test in there, so that's Newton's method which is not about on the function, it's just some approximation of the function that could in principle go crazy.",
            "Let's assume that the Hessian is Lipschitz continuous.",
            "Not, not necessarily even the gradient anymore.",
            "We're just assume that the Hessian is Lipschitz continuous.",
            "So now I can do the same trick before I go to the third order term in the Taylor expansion, and I just say, well, the Hessian can't change by more than L times time can't change by more than L. If I do like the three way tensor thing, so it's almost like a completely obvious thing to do if you saw the previous perspective on gradient methods, but people didn't.",
            "I mean there were third order and 4th order methods in the 80s, but they didn't quite have this nice perspective 'cause they think they were biased by the classic.",
            "Like two Phase Newton convergence thing.",
            "So the beautiful thing about this algorithm is now the.",
            "You know Newtons method guaranteed to decrease the objective.",
            "This is an upper bound on the objective function.",
            "There's no line search or trust region or anything like that, it's just guaranteed to work.",
            "If you have a convex functions, you gotta linear convergence and then a superlinear convergence once you're close to the solution, we can start talking about the constants are which is often hard for Newton's method.",
            "So it's very elegant and it's very surprised that it took so long to actually come up with.",
            "It's very obvious in hindsight, but in this paper what they show is if this algorithm.",
            "Gets within some ball of a saddle point, so once it gets there some radius around each saddle point, such that when it gets within that radius, the next step is not only going to move out of the ball, but it's going to move to a function value that's lower than the saddle point.",
            "So there's there's no.",
            "This is, you know, formally proved there.",
            "It's all you know.",
            "You can just look at this paper.",
            "This is a theorem saying if you're close to the point we're going to move away from that point, so they actually cannot converge to saddle points.",
            "That's right, yeah.",
            "And also this looks a bit scary too, 'cause this is nonconvex, so I haven't gone through the computation in detail.",
            "Supposedly you can solve these for the same cost as Newton's method, but again, you do need the Hessian and.",
            "There is work on like you know, Hessian, free quasi Newton, whatever.",
            "I don't know if they maintain this property, though I think there was a question up there.",
            "There is.",
            "I mean, you can do pretty much as you like.",
            "You probably have to add a line search or some sort of making sure that the conditioning doesn't go crazy here, and I'm not sure if you maintain this property in that setting.",
            "Other comments I think you know you've probably never seen this before, and it's kind of an obvious thing to do.",
            "Maybe I'll maybe I'll just pause 'cause I feel like we should talk about a bit more.",
            "If XK is a local minimum.",
            "This will be 000 when you solve for D. 'cause 'cause?",
            "Remember if these if D is very small, these two terms are dominated by this term which which will be.",
            "Which will be positive in every direction.",
            "And these two these two terms are always.",
            "I guess this doesn't necessarily have to be positive.",
            "I didn't think about that case, but it will be.",
            "It will be non negative if you're at a minimum.",
            "And S drive does show that this method actually solves some non convex problems.",
            "But they look kind of like the ones I showed before where you have the flat thing and the sort of approximately convex type of thing.",
            "OK.",
            "So that's about all I have to say about these sort of two phase methods, so let's move on to methods which actually solve globally optimal problems, even though they're non convex and these there's been a whole bunch of work lately on matrix style problems."
        ],
        [
            "So the Classic One is PCA.",
            "So PCA anyway.",
            "You write it.",
            "You gotta nonconvex problem, we can compute PCA.",
            "You know you just type SVD into Matlab.",
            "It's not a problem.",
            "There was a paper recently gave like basically a version of SVG for PCA, so you can.",
            "You can actually convert PCA very quickly in the finite data regime."
        ],
        [
            "So let's talk about something more interesting than PCA.",
            "So Buren Montero.",
            "Unfortunately, I think they didn't.",
            "They could have wrote their paper differently, so they emphasize this a bit more.",
            "They sort of set it as a side point.",
            "If I want to minimize some function of a matrix, and I want my matrix to be positive semidefinite an have bounded rank.",
            "One thing that's very common is if the matrix is huge.",
            "Use this representation VV transpose so you have some sort of like low rank approximation to X and they showed that under some fairly general conditions that actually doesn't introduce spurious local minimum, so you can you can optimize this V. It's a way faster and it still actually globally solves your problem.",
            "So I think that's a very neat result.",
            "Well, it's not just the orthogonality constraint, it's actually that we're maximizing a convex function, so this is really a concave minimization, so you'd have to fix both things, I think.",
            "OK.",
            "I would suspect it's easier without orthogonality, but I'm not sure.",
            "Or it might even be equivalent.",
            "I mean 'cause the the.",
            "Yeah.",
            "So I am not completely sure the answer to that question.",
            "OK, so so for certain matrix problems, even though if we've turned a convex problem into a nonconvex problem, you can still solve the nonconvex problem."
        ],
        [
            "Then you can also talk about going the other way, and there's a whole bunch of works that do this, But this this recent ICML paper was kind of.",
            "Is A is a nice reference where they actually say, well, let's just assume I have a problem like this and they focus on this particular case and they show that as long as you do the initialization in a reasonable way, you're actually going to solve the nonconvex problem, even though we're not referencing some original convex problem, but."
        ],
        [
            "Sure.",
            "If you start talking UV transpose like the like, the Netflix Style collaborative filtering you have to make a little bit more assumptions to actually say that you're solving it, and you're sort of solve it in a weaker sense, But there still are results in that flavor, and they usually are two phase methods where there is a first phase where you try to find some good initial value, and then the second phase where you apply alternating minimization on U&V.",
            "And then there's also been some recent work on things like Hmm's, another.",
            "Often probabilistic models, showing that you can, you can solve those problems in some sense with some weird parameterisation.",
            "It doesn't really give you the parameters, but you can get your probabilities right as the data set size goes.",
            "So I kind of want to mention that I have no idea if this is relevant in deep learning or not, I just think it's interesting and in this vein.",
            "Translation yeah, kind of.",
            "It's nonparametric.",
            "It's not.",
            "It's not nonparametric in the number of training examples the way we normally think it's in the number of variables somehow.",
            "Yeah, I just read this this HMM paper finally last week.",
            "So I think my understanding is that you just need the.",
            "You just need to estimate the empirical moments and then you format that as a matrix problem.",
            "I'd have to think about if that's how that's related to non parametric I, I'm not.",
            "Seeing connection immediately.",
            "OK."
        ],
        [
            "So moving on, you can start talking about convex relaxation.",
            "So here you trying to approximate a nonconvex problem with the convex problem.",
            "That's very common, and that has been done for deep learning by by the lab here many years ago, and there was also a nice EML or I think it was actually.",
            "And nips paper and archive paper in the last year on this.",
            "Sometimes the restrictions can be very strong to do this, or they may not be a good approximation or something like that, so there."
        ],
        [
            "Always tradeoffs there.",
            "Another thing you can do is you solve the convex dual so for certain non convex problems you have strong duality's.",
            "You can solve the dual problem and it solves the original problem.",
            "Sometimes the dual has nicer problems.",
            "I'm not sure if how to compute for neural networks, so I'm."
        ],
        [
            "That time OK?",
            "Also, you can exactly right any non convex problem.",
            "Any reason why is a convex problem but the size might be enormous."
        ],
        [
            "So at the end."
        ],
        [
            "I just mentioned."
        ],
        [
            "You want to.",
            "If you're doing Bayesian optimization, that might make you go faster than grid search, or it might not, depending on how smooth the function.",
            "So if you're in Bayesian optimization, you think this solves nonconvex problems.",
            "It's not quite true unless you have very nice function."
        ],
        [
            "OK, so that's the summary and I'll stop there 'cause Joshua just stood up."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today we're going to talk.",
                    "label": 0
                },
                {
                    "sent": "So yesterday we sort of made some assumptions that made our life easier, but possibly less realistic for a lot of applications.",
                    "label": 0
                },
                {
                    "sent": "So today we're going to talk about when those strong assumptions we make are not satisfied, and what happens then?",
                    "label": 0
                },
                {
                    "sent": "And as before, I'm going to kind of try and focus on what the theory says with a few guidelines for my practical experience, but these are hard problems, so I think nobody really knows what the true answers to a lot of these things are.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I get into that, I just want to tie up a few loose ends from yesterday, so these are some of the things that I talked about, but not necessarily on the slides, and I want to go into a bit more detail on.",
                    "label": 0
                },
                {
                    "sent": "So one thing that came up in our discussion was this thing called the complex step derivative.",
                    "label": 0
                },
                {
                    "sent": "So usual finite difference approximation is this formula, which I've sure you've seen before, 'cause you use it to check your derivatives when you write your code, and you can motivate that just from a Taylor expansion around X where you have this over H ^2, so the error is actually pretty good.",
                    "label": 0
                },
                {
                    "sent": "If I make H very small, this formula is very accurate.",
                    "label": 0
                },
                {
                    "sent": "If I make H1 E to the minus 8 then it's Juanita minus 16 accurate.",
                    "label": 0
                },
                {
                    "sent": "It's great.",
                    "label": 0
                },
                {
                    "sent": "The problem is when you do the subtraction.",
                    "label": 0
                },
                {
                    "sent": "So if you actually want to find the difference at the level of 1 E to the minus 8, but you're comparing things that are much larger, you lose a whole lot of accuracy when you do the subtraction.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the complex step derivative is a very simple change.",
                    "label": 0
                },
                {
                    "sent": "We're going to replace.",
                    "label": 0
                },
                {
                    "sent": "We're just going to add I the square root of negative one here.",
                    "label": 0
                },
                {
                    "sent": "I can still do the Taylor expansion for the class of functions called analytic functions, which includes a lot of the things we care about.",
                    "label": 0
                },
                {
                    "sent": "And now the neat thing is I don't need to do these two evaluations.",
                    "label": 0
                },
                {
                    "sent": "I just do this one evaluation.",
                    "label": 0
                },
                {
                    "sent": "The real part by solving for it in this equation is going to give me F of X + / 8 squared and the imaginary part of that one evaluation is going to give me the gradient and that is going to give me the gradient in that direction divided by H. So it's still over 8 squared, but.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The nice thing is there's no cancellation now.",
                    "label": 0
                },
                {
                    "sent": "You don't have to do this.",
                    "label": 0
                },
                {
                    "sent": "The subtraction here, and what that means is you can actually use an extremely small age.",
                    "label": 0
                },
                {
                    "sent": "So if you look in my code it actually uses 10 to the minus 150 as your H. Because you don't have to, there's no subtraction there, and it actually gives you the derivative to that accuracy, so it's effectively an exact derivative method, and so here I've written it for the 1st order derivative.",
                    "label": 0
                },
                {
                    "sent": "But if you want to Hessian vector product, or if you want the derivative in a particular direction, the same thing applies the same argument.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I also mentioned subgradients of convex functions, so this is the sub gradient formula.",
                    "label": 1
                },
                {
                    "sent": "It's a vector D satisfying this formula for all Y&X for any reasonable convex function like say that it's not Infinity everywhere.",
                    "label": 0
                },
                {
                    "sent": "These are going to exist at any point X but for non convex functions this definition doesn't quite work because we remember we said that convex functions are above their subgradient whereas non convex functions.",
                    "label": 0
                },
                {
                    "sent": "You might go back down so there may be no actual subgradient at a point.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But people actually do take them, take the Max of the two things, like when you do Relu.",
                    "label": 0
                },
                {
                    "sent": "So what does that actually doing?",
                    "label": 0
                },
                {
                    "sent": "And that's sometimes called a Clark subgradient or a generalized gradient.",
                    "label": 1
                },
                {
                    "sent": "And what you do is.",
                    "label": 0
                },
                {
                    "sent": "You add this part here so it's actually a concave lower bound.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as being as lower bound that's linear at your current point, but then it goes down.",
                    "label": 0
                },
                {
                    "sent": "And what that does an you can let it go down arbitrarily fast.",
                    "label": 1
                },
                {
                    "sent": "And then what that does is it lets you define subgradients for convex or non convex functions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mentioned the convergence rate of stochastic gradient with constant step size and I said that was sort of hidden in the 2000 paper of net chamber tzikas.",
                    "label": 0
                },
                {
                    "sent": "I said the report I wrote was four pages long, but it's actually just two pages long, so this is the entire paper on the slide, and the proof is just Section 3 here of the result I said yesterday so it's you can go through it at home.",
                    "label": 0
                },
                {
                    "sent": "Everything you need to prove it is is in the paper.",
                    "label": 0
                },
                {
                    "sent": "There I also said you don't need smoothness to show the result, that's section.",
                    "label": 0
                },
                {
                    "sent": "4 here so you know if you want a very simple treatment of that, there it is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I wanted to talk a little bit more about stochastic variance, reduced gradient, but we were sort of running short on time, so I just want to briefly mention a few things here.",
                    "label": 0
                },
                {
                    "sent": "So there's an inner and an outer loop.",
                    "label": 0
                },
                {
                    "sent": "The inner loop is actually running a true stochastic gradient method.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look like a stochastic gradient method because we have replaced the stochastic rate with this weird thing.",
                    "label": 0
                },
                {
                    "sent": "But you basically set it up so the expectation of this term is equal to this term.",
                    "label": 0
                },
                {
                    "sent": "So in expectation this is zero in expectation, this is your true gradient.",
                    "label": 0
                },
                {
                    "sent": "So this is actually unlike sag.",
                    "label": 0
                },
                {
                    "sent": "This is a true stochastic gradient method.",
                    "label": 0
                },
                {
                    "sent": "We just added these extra 2 terms and the purpose of those is to make your noise go to 0.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of it's kind of like having an increasing batch size that makes your noise go to zero.",
                    "label": 0
                },
                {
                    "sent": "But here sort of happens automatically and this D is just the average of all your functions at some point.",
                    "label": 0
                },
                {
                    "sent": "So you've got this.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scheme where you're doing occasionally, you're going through a full pass of the data set, and then you run a whole bunch of stochastic gradient methods.",
                    "label": 0
                },
                {
                    "sent": "Now you have to pick how many iterations you do, and the theory says you should pick that roughly proportional to the condition number.",
                    "label": 0
                },
                {
                    "sent": "In practice, people actually just set M to the number of data points they have, so you do one passive stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "Then you compute a full gradient, one passive stochastic gradient computer, full gradient, and so on.",
                    "label": 0
                },
                {
                    "sent": "So it's it seems kind of wasteful.",
                    "label": 0
                },
                {
                    "sent": "It is kind of wasteful.",
                    "label": 0
                },
                {
                    "sent": "You can start talking about ways to reduce those gradient calculations and.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work if you do them properly.",
                    "label": 0
                },
                {
                    "sent": "And I want to.",
                    "label": 0
                },
                {
                    "sent": "I want to size the practical issues or similar to sag.",
                    "label": 1
                },
                {
                    "sent": "So I talked about this thing like when you try to adaptively estimate how often we want to sample the data points, we talked about how to do better mini batching's you can automatically set the set the step size.",
                    "label": 0
                },
                {
                    "sent": "It's really a black box, there's no tuning required, at least in principle.",
                    "label": 0
                },
                {
                    "sent": "So I said those for sag, but really they apply for SVR G2.",
                    "label": 0
                },
                {
                    "sent": "It's a very similar algorithm, you've just kind of got rid of the memory and introduced some extra gradient calculations.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so part one we talked about how you can get a low iteration cost and a linear convergence rate in this very restrictive setting.",
                    "label": 1
                },
                {
                    "sent": "And today we're going to try and relax those assumptions zero before I start.",
                    "label": 0
                },
                {
                    "sent": "Are there any questions or comments or burning issues from yesterday?",
                    "label": 0
                },
                {
                    "sent": "I know it's early.",
                    "label": 0
                },
                {
                    "sent": "OK, everyone is just eating.",
                    "label": 0
                },
                {
                    "sent": "Calculate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I probably should have included that in this list, so if you look up Fisto, there's nothing called Fisto FCA.",
                    "label": 0
                },
                {
                    "sent": "They're the ones who sort of popularized this approach, so they don't really give you an approximation of the Lipschitz constant, but they give you a.",
                    "label": 0
                },
                {
                    "sent": "A bound on the Lipschitz constant that works to make your algorithm converge as fast as it could.",
                    "label": 0
                },
                {
                    "sent": "And it's very simple.",
                    "label": 0
                },
                {
                    "sent": "You take one of those inequality's that Lipschitz continuous functions should satisfy, and you look along the direction you're going.",
                    "label": 0
                },
                {
                    "sent": "If it's not satisfied, you make it bigger.",
                    "label": 0
                },
                {
                    "sent": "Until until it's satisfied and you can prove that that that's a OK thing to do.",
                    "label": 0
                },
                {
                    "sent": "So the one thing I'll say is.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you apply the usual rules of sub of computing sub gradients, like if you have a Max function, you just take the gradient of the one that achieves the Max that will satisfy this.",
                    "label": 0
                },
                {
                    "sent": "So this is more like notation to say that we can define subgradients locally instead of globally, and that lets us handle non convex functions.",
                    "label": 0
                },
                {
                    "sent": "Is that what you're getting at?",
                    "label": 0
                },
                {
                    "sent": "The actual paper gives a much more complicated definition of this, but this is equivalent definition that's much simpler to understand.",
                    "label": 0
                },
                {
                    "sent": "In my opinion.",
                    "label": 0
                },
                {
                    "sent": "Anymore.",
                    "label": 0
                },
                {
                    "sent": "OK, let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we're going to tackle Nonsmooth first, which is in some sense the easiest of these ones to handle.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that was our general framework.",
                    "label": 0
                },
                {
                    "sent": "Some data fitting terms, some regularizer.",
                    "label": 0
                },
                {
                    "sent": "And often our regularizer is non smooth, so we might do.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Something like L1 regularization because we want sparse solutions, so it's nice you got the regularising effect.",
                    "label": 0
                },
                {
                    "sent": "You get zeros in your solution, so your coefficients are sparse, but the absolute value function is non smooth, so so the things we talked about yesterday don't really.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ally.",
                    "label": 0
                },
                {
                    "sent": "You can apply subgradient methods.",
                    "label": 0
                },
                {
                    "sent": "It's very trivial to do so, but they're very slow.",
                    "label": 0
                },
                {
                    "sent": "They had these these awful sub linear rates that we talked about last time.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The problem is the subgradient method is actually optimal black box method.",
                    "label": 0
                },
                {
                    "sent": "You can't do better by like clever cutting plane or bundle methods or anything like that.",
                    "label": 0
                },
                {
                    "sent": "But if you start talking about specific nonsmooth problems Now, you can go faster, and the L1 regularization is a case where in fact you can go substantially fast.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just talk about the first thing you would think of, which is to replace your nonsmooth function with some smooth approximation, and then you just apply the method that you know works well for smooth optimization.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you have the absolute value function, a very old approximation is this function, called the multi quadric function, and.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That just looks like this red function, so it almost exactly follows the absolute value and then around zero.",
                    "label": 1
                },
                {
                    "sent": "It's just smooth it out a bit.",
                    "label": 0
                },
                {
                    "sent": "One thing you'll notice is.",
                    "label": 0
                },
                {
                    "sent": "You will actually lose the sparsity.",
                    "label": 0
                },
                {
                    "sent": "It won't set things to exactly 0, and that's kind of a generic disadvantage of.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moving methods.",
                    "label": 0
                },
                {
                    "sent": "If you want to approximate the Max function, the log some exponential is one option, or softmax if you want to approximate the hinge function.",
                    "label": 0
                },
                {
                    "sent": "What you do is you just take your.",
                    "label": 0
                },
                {
                    "sent": "You can use the log some axe or you can just take the linear part.",
                    "label": 0
                },
                {
                    "sent": "A little quadratic piece and then the other linear part.",
                    "label": 0
                },
                {
                    "sent": "So it's almost exactly it up to some small piece.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's true, the terminology is overloaded.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure which is the right one.",
                    "label": 0
                },
                {
                    "sent": "I'll believe you.",
                    "label": 0
                },
                {
                    "sent": "Oh, I see.",
                    "label": 0
                },
                {
                    "sent": "Right, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's right, that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "I had never thought of that before.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you don't know how to smooth your function, what you can do is you can take what's called the convex conjugate, which, despite its name, does not rely on the function actually being convex, and then you apply a strongly convex regularizer to that, and then when you go back to the primal it gives you a smooth function.",
                    "label": 0
                },
                {
                    "sent": "It's very magical, it only applies in special cases, but it is a generic method to smooth any nonsmooth function.",
                    "label": 0
                },
                {
                    "sent": "Assuming you can compute this thing.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So nesterov.",
                    "label": 0
                },
                {
                    "sent": "That is a very complicated question.",
                    "label": 0
                },
                {
                    "sent": "There's different ways you can do it so.",
                    "label": 0
                },
                {
                    "sent": "If you actually sample a bunch of points in an area, it's like convolving with a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So if you take the average of those samples, it is like you're doing a smoothing.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, in the stochastic case, smoothing doesn't help you.",
                    "label": 0
                },
                {
                    "sent": "So even if even if you were thinking of smoothing it in the stochastic case, it doesn't improve your rate.",
                    "label": 0
                },
                {
                    "sent": "So yes, it is a form of smoothing, but.",
                    "label": 1
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, So what Nesterov showed is that if you smooth the function and then apply a gradient method, you actually get the same rate as the subgradient method.",
                    "label": 0
                },
                {
                    "sent": "So the problem is in order to approximate a non smooth function with a smooth function, the smooth function has to be so badly conditioned that you can't really solve it very efficiently.",
                    "label": 0
                },
                {
                    "sent": "But he showed is if you do the accelerated gradient method you can get a faster rate.",
                    "label": 1
                },
                {
                    "sent": "So you can smooth, but you have to apply a very fast algorithm to benefit, and that's that kind of matches intuition.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's no results showing that smoothing helps in the stochastic case, except for one paper by John Duchi.",
                    "label": 1
                },
                {
                    "sent": "On the smooth problem?",
                    "label": 0
                },
                {
                    "sent": "Yeah, 'cause accelerating rate you need smoothness so you can't apply it to the original problem.",
                    "label": 0
                },
                {
                    "sent": "So John, do.",
                    "label": 0
                },
                {
                    "sent": "She showed that if you do the smoothing trick that that was mentioned then you can improve like the variance in some sense.",
                    "label": 0
                },
                {
                    "sent": "Or you can benefit from parallelization.",
                    "label": 0
                },
                {
                    "sent": "But you're still in sort of the slow convergence rate regime.",
                    "label": 1
                },
                {
                    "sent": "In practice, this is not what you do.",
                    "label": 0
                },
                {
                    "sent": "You'll usually slowly increase the level of smoothing or the decrease the level of smoothing.",
                    "label": 1
                },
                {
                    "sent": "So you start with a very smooth function, you slowly make it converge to original function.",
                    "label": 0
                },
                {
                    "sent": "We don't really have theory justifying why that works better, but it does an you won't use accelerated gradient method.",
                    "label": 0
                },
                {
                    "sent": "You'll use some fast method instead.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a related result if you have saddle point problems.",
                    "label": 0
                },
                {
                    "sent": "I just want to mention that, but I think this is less relevant for this crowd, but it's kind of become a very hot topic lately.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing you can think of doing is rewriting your nonsmooth problem is a constrained problem, which almost seems like a crazy thing to do.",
                    "label": 0
                },
                {
                    "sent": "'cause constraint problems are not so easy to solve.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But a bunch of constraint problems actually are not too much worse.",
                    "label": 0
                },
                {
                    "sent": "So let's think of I have some generic differentiable function an I add an L1 regularizer to that.",
                    "label": 0
                },
                {
                    "sent": "It seems like it's not so bad we're actually smooth along any orthant, it's just when we cross or fence.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turn on smooth.",
                    "label": 0
                },
                {
                    "sent": "So I can rewrite that in various ways.",
                    "label": 0
                },
                {
                    "sent": "I can rewrite it with non negative variables, a smooth function plus another smooth function.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's another, there's a few other ways you can write it as a constraint problem.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically you end up with a smooth objective with simple constraints and.",
                    "label": 0
                },
                {
                    "sent": "These are actually much easier to solve than nonsmooth problems, and we'll talk about what other simple constraints are moment.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thoroughly.",
                    "label": 0
                },
                {
                    "sent": "But first, let's show the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So yesterday we motivated gradient descent as solving some sort of quadratic upper bound on the function, and if you have a generic stepsize Alpha instead of L, your algorithm can be written like that.",
                    "label": 0
                },
                {
                    "sent": "That gives you gradient descent.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It suggests a very simple constrained alternative.",
                    "label": 0
                },
                {
                    "sent": "This is our approximation to the function F. Instead of minimizing over, the real space will just minimize that function over the convex set, right?",
                    "label": 0
                },
                {
                    "sent": "This is our upper bound on the function.",
                    "label": 0
                },
                {
                    "sent": "We have some constraints.",
                    "label": 0
                },
                {
                    "sent": "Let's minimize their upper bound but satisfy the constraints.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's called the projected gradient algorithm, so you can rewrite this as you take a gradient step and then you compute what's called the projection.",
                    "label": 0
                },
                {
                    "sent": "You find the closest point inside your constraints to the point you move to.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a picture, we have our function in some point X.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have a feasible set, so some convex set that we want our parameters to lie in.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We take our gradient step and that will move us outside the constraints.",
                    "label": 0
                },
                {
                    "sent": "So we need to live in this blue region, but we've gone outside of it, So what you do is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You find the closest point inside the blue set to the green point and.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You follow that direction.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, it's going to stay inside the set for obvious reasons.",
                    "label": 0
                },
                {
                    "sent": "But also it's going to make it's not going to make more than a 90 degree angle with the green line, so you know that it moves inside the level curves and it's going to improve the objective.",
                    "label": 0
                },
                {
                    "sent": "For what sort of constraints?",
                    "label": 0
                },
                {
                    "sent": "This is on the parameters, yeah?",
                    "label": 0
                },
                {
                    "sent": "The that that would be a little bit weird, that's called.",
                    "label": 0
                },
                {
                    "sent": "There are methods to do that, I'm not.",
                    "label": 0
                },
                {
                    "sent": "I don't think I'll.",
                    "label": 0
                },
                {
                    "sent": "Go over them today though, it gets into some weird issues when you do stuff like that that can work, though there is theory saying that that's an OK thing to do.",
                    "label": 0
                },
                {
                    "sent": "Normalizing by the gradient norm.",
                    "label": 0
                },
                {
                    "sent": "That that can be OK.",
                    "label": 0
                },
                {
                    "sent": "I don't know exactly what grading clipping is, so like.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that can work.",
                    "label": 0
                },
                {
                    "sent": "You have to be a little bit careful, but that can work.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the thing that's really nice.",
                    "label": 0
                },
                {
                    "sent": "What projected gradient method is that has the same convergence properties as the gradient method?",
                    "label": 0
                },
                {
                    "sent": "So all the things I talked about yesterday for smooth functions, they basically apply almost with the exact same constants for projected gradient method.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can start to do things like, well, let's make a projected version of sag to solve constraint problems, or SVR G. Or will do Barzilai Borwein step size for projected gradient line search acceleration.",
                    "label": 0
                },
                {
                    "sent": "All this stuff I could put like a whole bunch of references, but it's basically all the convergence properties get get.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reserved.",
                    "label": 0
                },
                {
                    "sent": "You can also talk about projected Newton methods, but those are get a little bit more complicated because if you want to project and properly you can't project under Euclidean norm.",
                    "label": 0
                },
                {
                    "sent": "You have to project under Norm that's defined by the Hessian, and that is basically always hard.",
                    "label": 0
                },
                {
                    "sent": "So, but there's two methods that kind of work in practice.",
                    "label": 0
                },
                {
                    "sent": "If you just have something like bound constraints.",
                    "label": 0
                },
                {
                    "sent": "If you just want your your variables to stay between upper and lower balance, there's something called 2 metric projection, and then if you have other simple constraints in a very costly objective, there's there's inexact projected Newton methods.",
                    "label": 1
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are simple sets?",
                    "label": 1
                },
                {
                    "sent": "What are sets that we can quickly compute the projection onto?",
                    "label": 1
                },
                {
                    "sent": "Well, you can have upper and lower balance on the variables.",
                    "label": 0
                },
                {
                    "sent": "You can have a few linear constraints.",
                    "label": 0
                },
                {
                    "sent": "You can say that your norm has to be bounded above by some value, or you can say that your norm has to be bounded above by something that's also a variable, which is called a norm cone.",
                    "label": 0
                },
                {
                    "sent": "You can say that your valuable stuff too.",
                    "label": 0
                },
                {
                    "sent": "Many norms.",
                    "label": 0
                },
                {
                    "sent": "We'll say so.",
                    "label": 0
                },
                {
                    "sent": "So in my thesis I give how to do this for kind of the standard norms you would think of, like LL 1L2L affinity and like mixed L12 and things like that.",
                    "label": 0
                },
                {
                    "sent": "You can also have things if you want your variables to be probabilities or if you want a disjoint intersection of all these.",
                    "label": 0
                },
                {
                    "sent": "So if you want if you have blocks of variables and you want each of those to be a probability, that's OK, but if they overlap it gets a bit more complicated.",
                    "label": 0
                },
                {
                    "sent": "So if you have any of these constraints, we actually solve very large problems as fast as if we didn't have the constraints there.",
                    "label": 1
                },
                {
                    "sent": "So you're going to run into like a cubic type of thing if you go bigger, so you can have like a million linear constraints, but you can have like 1000.",
                    "label": 0
                },
                {
                    "sent": "Will depend on the number of constraints and the number of variables.",
                    "label": 0
                },
                {
                    "sent": "'cause that will be the size your matrix representing the constraints.",
                    "label": 0
                },
                {
                    "sent": "It's cubic in the like?",
                    "label": 1
                },
                {
                    "sent": "Yeah, yeah, either the number of constraints are the number of variables.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, these are these are easy constraints.",
                    "label": 0
                },
                {
                    "sent": "If you have these constraints, you don't even worry about it, just do projected grading or projected stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radiant.",
                    "label": 0
                },
                {
                    "sent": "If you have the intersection of these, there's something called Dijkstra's algorithm, which is not the Dijkstra's algorithm graph theory.",
                    "label": 0
                },
                {
                    "sent": "It's the Dexter's album from projection, and he's a statistician.",
                    "label": 0
                },
                {
                    "sent": "I believe it's from the 80s, and it's also a very famous Dijkstra's algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's a Y like it's spelled differently to disambiguate if you're writing it.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's a generalization of projected gradient methods called proximal gradient methods, so they include projected gradient methods as a special case and therefore functions that look like this.",
                    "label": 0
                },
                {
                    "sent": "You're minimizing F of X, which is smooth and RX, which might be nonsmooth ORX could be the L1 or RX could be this weird function that zero if you satisfy your constraints and it's Infinity if you don't satisfy your constraints, and that will give you projected gradient as a special case.",
                    "label": 0
                },
                {
                    "sent": "So this R can be a very general convex function.",
                    "label": 1
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the update looks like this.",
                    "label": 1
                },
                {
                    "sent": "We have our usual gradient update here are approximation of the smooth function and we're just going to add the nonsmooth function to our approximation.",
                    "label": 0
                },
                {
                    "sent": "We're not going to, really.",
                    "label": 0
                },
                {
                    "sent": "Approximate that at all.",
                    "label": 0
                },
                {
                    "sent": "We're just going to use the nonsmooth function exactly.",
                    "label": 0
                },
                {
                    "sent": "Where when you want to prove the rates you assume it's convex.",
                    "label": 0
                },
                {
                    "sent": "If you want the item to work.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit complicated, but as long as F is deterministic it's fine.",
                    "label": 1
                },
                {
                    "sent": "And if F is the castec, it's basically almost always fine too.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh sorry, yeah.",
                    "label": 0
                },
                {
                    "sent": "So in this stochastic non convex case you have to watch out a little bit.",
                    "label": 0
                },
                {
                    "sent": "Your noise isn't too big.",
                    "label": 0
                },
                {
                    "sent": "Which is just a very subtle point that's not really well known, and.",
                    "label": 0
                },
                {
                    "sent": "In practice, it seems to work anyways, but the theory says that some things can go wrong there.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can rewrite this as we do a gradient step on F. We take the grading of our smooth function.",
                    "label": 0
                },
                {
                    "sent": "We take a gradient step and then we do this thing called the proximal operator on our nonsmooth function R. For a lot of functions, we can compute this proximal operator and the convergence rate is going to be the same as we were, just as if we were just minimizing F. So even though our can be this weird linear function that's like Infinity somewhere in some places, so it's definitely not Lipschitz continuous in any order.",
                    "label": 0
                },
                {
                    "sent": "It can be nonsmooth, whatever, it's almost as if we just care about the properties of F. So if F is strongly convex Lipschitz continuous gradient, you get linear convergence rates, so it's solving certain nonsmooth problems at the speed that we solve smooth problems, and I think this is almost like the most elegant idea in nonsmooth convex optimization.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the proximal operator is that thing.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll just show you what it is for L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "It's actually something called iterative soft thresholding which was invented for L1 regularization.",
                    "label": 1
                },
                {
                    "sent": "Before we had this nice theory of what proximal gradient methods were.",
                    "label": 0
                },
                {
                    "sent": "So you're taking your gradient step and you're applying what's called the soft threshold operator, which.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can just show it with an example, so if this is the result of our gradient step.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The standard threshold operator with the regularization parameter of one would be if you're less than one in absolute value.",
                    "label": 0
                },
                {
                    "sent": "We set you to 0, otherwise you keep your value.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the soft threshold is well.",
                    "label": 0
                },
                {
                    "sent": "You also move those two zero with a value of 1.",
                    "label": 0
                },
                {
                    "sent": "So very simple, very cheap operation.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing is, if you're thinking about doing L1 regularization, the algorithm is actually setting things to exactly 0.",
                    "label": 0
                },
                {
                    "sent": "So a subgradient method will only set things to zero.",
                    "label": 0
                },
                {
                    "sent": "In the limit.",
                    "label": 0
                },
                {
                    "sent": "A smoothing method will never really set things to 0.",
                    "label": 0
                },
                {
                    "sent": "This algorithm is explicitly setting values to 0.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for what problems can we apply those?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Says, well, there's L1 and group L1 or the big ones.",
                    "label": 0
                },
                {
                    "sent": "There's also a few.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weird matrix problems, but they get a bit more expensive.",
                    "label": 0
                },
                {
                    "sent": "There's all the things we had before and a few other simple regularizers.",
                    "label": 1
                },
                {
                    "sent": "Basically, anytime your regularizer is a sum of a function applied to individual variables, you can compute that proxamol operate.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can solve all those nonsmooth problems, the same rate as if we just had a smooth problem.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can again do a lot of the same tricks.",
                    "label": 1
                },
                {
                    "sent": "In this setting.",
                    "label": 0
                },
                {
                    "sent": "You can do your line search.",
                    "label": 0
                },
                {
                    "sent": "You can do like a two metric method if you want to.",
                    "label": 0
                },
                {
                    "sent": "Newton style version of this you can apply Sagan SVR, G and so on.",
                    "label": 0
                },
                {
                    "sent": "Some of these are more recent results, but the the idea that you can really treat this class of nonsmooth problems as if they were smooth is very nice, and you can do all the same.",
                    "label": 0
                },
                {
                    "sent": "Fast things.",
                    "label": 0
                },
                {
                    "sent": "OK, are there questions or comments on the proximal gradient method so something you can you can very easily add to your stochastic gradient method if you want to have an L1 regularizer or something like that.",
                    "label": 0
                },
                {
                    "sent": "That's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I think.",
                    "label": 0
                },
                {
                    "sent": "I think it first appeared in a paper by Ducey and Singer, and they called it Phobos.",
                    "label": 0
                },
                {
                    "sent": "But proximal gradient is sort of the name that we're encouraging everyone to use so that we don't have.",
                    "label": 0
                },
                {
                    "sent": "This is another name that was reinvented many times, and there's a million names for it, so proximal gradient is kind of the.",
                    "label": 0
                },
                {
                    "sent": "The standard method.",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                },
                {
                    "sent": "So so not true.",
                    "label": 0
                },
                {
                    "sent": "Optionality conditions gets into some nasty stuff.",
                    "label": 0
                },
                {
                    "sent": "In principle.",
                    "label": 0
                },
                {
                    "sent": "What you do is you take the subgradient you set.",
                    "label": 0
                },
                {
                    "sent": "You want to find if zero is in the subgradient.",
                    "label": 0
                },
                {
                    "sent": "So for L1 that ends up being not too hard to do, you sort of.",
                    "label": 0
                },
                {
                    "sent": "You take the subgradient and you can work out.",
                    "label": 0
                },
                {
                    "sent": "Work it out and for a few of these constraints it's when you start talking with constraints you have to get into weird things like normal cones and stuff so it's actually not always easy to do.",
                    "label": 0
                },
                {
                    "sent": "One thing you can always do in one day is just to buy section to compute these things.",
                    "label": 0
                },
                {
                    "sent": "So you can compute it numerically very quickly.",
                    "label": 0
                },
                {
                    "sent": "This is just a 1D problem.",
                    "label": 0
                },
                {
                    "sent": "You can have very fast convergence.",
                    "label": 0
                },
                {
                    "sent": "Other questions or comments on proximal gradient?",
                    "label": 0
                },
                {
                    "sent": "So yeah, just just you can just add this to your stochastic gradient, it's mostly OK.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another thing you might hear of is this alternating direction method of multipliers.",
                    "label": 1
                },
                {
                    "sent": "So this is for optimizing a function F of X + R, Y where there's some linear constraints on X&Y that might be tying them together.",
                    "label": 0
                },
                {
                    "sent": "And the method really just alternates between a sort of approximal operator on F and approximal operator on R. And then there's some update of LaGrange multiplier.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "It doesn't look like our standard machine learning problem, but what you can do is you can actually introduce constraints to turn your problem into this form.",
                    "label": 1
                },
                {
                    "sent": "So if I have something like F of X plus RX, well I introduce new variables Y and I constrain them so that X is equal to a Y.",
                    "label": 0
                },
                {
                    "sent": "And now I can apply this method and that's a very common thing when your regularizer or your function is quite complicated.",
                    "label": 0
                },
                {
                    "sent": "And there's versions where instead of applying the proximal operator, you just take a gradient step on each each thing, and that's called linearized.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adm, so there's.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Various ways you can convert.",
                    "label": 0
                },
                {
                    "sent": "So for maybe for more complicated constraints that's sort of appealing method to use.",
                    "label": 0
                },
                {
                    "sent": "It's definitely not my favorite method 'cause it gets a bit more messy, but but sometimes your choices are either this or some sort of inexact proximal gradient method.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's another thing you'll hear if you go to NIPS and ICML called the Frank Wolf method.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "I was really debating whether to include this 'cause it's not so clear it's relevant in deep learning, but the idea is sometimes this projection can be hard to compute.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what you do is you just ignore that last term, you just minimize a linear function over your convex set, and in some cases you can do that efficiently.",
                    "label": 1
                },
                {
                    "sent": "So if you talk about structured SVM's, you can do that efficiently, and this is a very nice method in that case, and under some conditions you can still get fast convergence rates for this type of problem.",
                    "label": 0
                },
                {
                    "sent": "So, so that's kind of more for weird constraint objective function interactions.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a bunch of other tricks, but I don't want to go over them 'cause I don't think they're really that relevant for this audience, at least from my perspective.",
                    "label": 0
                },
                {
                    "sent": "So I just want to conclude this section section by saying that there's really no black box method to beat subgradient methods.",
                    "label": 0
                },
                {
                    "sent": "So if you just give me a nonsmooth convex function, I can never design A method that only works on subgradients.",
                    "label": 0
                },
                {
                    "sent": "That works better than the basic subgradient method, which doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "But for most objective functions, once you actually see the objective function, you know something about it.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of tricks you can do to actually solve it.",
                    "label": 0
                },
                {
                    "sent": "Very cool.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Glee.",
                    "label": 0
                },
                {
                    "sent": "And the problem is, there's no elegant answer.",
                    "label": 0
                },
                {
                    "sent": "In this case, you can't beat the black box method.",
                    "label": 0
                },
                {
                    "sent": "You always have to know something about your function.",
                    "label": 0
                },
                {
                    "sent": "So if you want to solve nonsmooth methods efficiently, you unfortunately have to either memorize these tricks or wait for someone to write a code that implements all the tricks and somehow analyzes your problem, which people are working on.",
                    "label": 0
                },
                {
                    "sent": "But I don't.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't wait for that.",
                    "label": 0
                },
                {
                    "sent": "You can probably solve your problem faster by just if you can recognize the problem class, you can do it, and I've sort of highlighted these.",
                    "label": 0
                },
                {
                    "sent": "Four 'cause I think those are those are the ones that I've really used in practice and I think work really well and.",
                    "label": 0
                },
                {
                    "sent": "I think those are the most important ones, so two metric projection.",
                    "label": 0
                },
                {
                    "sent": "That's where if you have like bound constraints or L1 type of things.",
                    "label": 0
                },
                {
                    "sent": "Proximal Newton is simple constraints but expensive objectives.",
                    "label": 0
                },
                {
                    "sent": "Frank Wolf is maybe less simple constraints and then you can also go to the smooth dual.",
                    "label": 0
                },
                {
                    "sent": "So if you have a nonsmooth problem that strongly convex, the dual problem has is actually convex and smooth, which is a nice property.",
                    "label": 0
                },
                {
                    "sent": "So if you have a nonsmooth problem, and you can take the dual, you can convert that into a smooth problem.",
                    "label": 0
                },
                {
                    "sent": "This is in principle true.",
                    "label": 0
                },
                {
                    "sent": "Like neural networks.",
                    "label": 0
                },
                {
                    "sent": "With Relu, you can go to the dual, the dual is convex, the dual is smooth.",
                    "label": 0
                },
                {
                    "sent": "Well, that's actually the dual may not be smooth, but the dual is convex.",
                    "label": 0
                },
                {
                    "sent": "I just have no idea what it looks like, or if you can compute it or anything like that, and I don't know if people have looked into that or not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, maybe I'll send an email to Nicola and he can think about it on one of his biking trips.",
                    "label": 0
                },
                {
                    "sent": "OK, are there other questions or comments about the Nonsmooth case?",
                    "label": 0
                },
                {
                    "sent": "So unfortunately, it's not like the smooth case.",
                    "label": 0
                },
                {
                    "sent": "You don't really get a very satisfying answer to solving nonsmooth problems.",
                    "label": 0
                },
                {
                    "sent": "It's definitely less elegant.",
                    "label": 0
                },
                {
                    "sent": "There is an entire field dedicated to that question.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so there's two issues.",
                    "label": 0
                },
                {
                    "sent": "There's one issue is whether you're recovering the true variables in like, a signal processing sense, and then there's one like does your you know the optimal solution has some set of nonzero variables?",
                    "label": 0
                },
                {
                    "sent": "Does to category and actually find those nonzero variables.",
                    "label": 0
                },
                {
                    "sent": "So which one are?",
                    "label": 0
                },
                {
                    "sent": "No, you don't.",
                    "label": 0
                },
                {
                    "sent": "So if you place the cast agreeing with the proximal operator, you keep setting things non 0.",
                    "label": 0
                },
                {
                    "sent": "Always there's always some probability you move something away from zero 'cause you're decreasing the stepsides so the proximal operator gets weaker.",
                    "label": 0
                },
                {
                    "sent": "You keep.",
                    "label": 0
                },
                {
                    "sent": "You don't really ever identify the non zero variables.",
                    "label": 0
                },
                {
                    "sent": "This is why you.",
                    "label": 0
                },
                {
                    "sent": "This is why the Phobos thing doesn't really identify the nonzero variables.",
                    "label": 0
                },
                {
                    "sent": "It gives you sparse iterations, but the sparsity pattern change can change on every single iteration.",
                    "label": 0
                },
                {
                    "sent": "There's something called dual averaging, which basically you take the average of the gradients and do the proximal operator on that.",
                    "label": 0
                },
                {
                    "sent": "That will give you the sparsity pattern, or you can run Sagan SVR G. They also give you the sparsity pattern in a finite number of iterations.",
                    "label": 0
                },
                {
                    "sent": "There needs to be some sort of averaging 'cause random stochastic gradient can point you point you in any direction and make things done 0.",
                    "label": 0
                },
                {
                    "sent": "So so most.",
                    "label": 0
                },
                {
                    "sent": "So it depends what algorithm you're using.",
                    "label": 0
                },
                {
                    "sent": "If you're using regular stochastic gradient descent, there's no reason to actually smooth it.",
                    "label": 0
                },
                {
                    "sent": "You can actually just use the subgradient method.",
                    "label": 0
                },
                {
                    "sent": "If you're using something that can get a faster rate, like sag, or you're using a deterministic method or a growing batch method, then you want to think about these issues.",
                    "label": 0
                },
                {
                    "sent": "'cause then there's gains to be had.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That stuff I'm kind of very comfortable with, so let's start to move on to the two topics that I've only recently started to really think about.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about the Nonfinite case.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An by that we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to go back to our smooth and strongly convex assumption.",
                    "label": 0
                },
                {
                    "sent": "So I really wrote and in the title, but I'm going to do these three things separately as and or so smooth and strongly convex, and I really want to minimize our truly stochastic objective.",
                    "label": 0
                },
                {
                    "sent": "So this could be that I you know there's some error in how I'm measuring the functions.",
                    "label": 0
                },
                {
                    "sent": "Or it could be that you know I have.",
                    "label": 0
                },
                {
                    "sent": "I have an infinite stream of data coming in.",
                    "label": 0
                },
                {
                    "sent": "I'm only looking, I can only I can only approximate in that.",
                    "label": 0
                },
                {
                    "sent": "So this includes the generalization error in machine learning, so you don't really care about how you perform on the training data.",
                    "label": 0
                },
                {
                    "sent": "You want to know how you do on the test data.",
                    "label": 0
                },
                {
                    "sent": "You want to know how well you do on this expectation, your performance, overall data you could possibly ever see I have some puzzled looks.",
                    "label": 0
                },
                {
                    "sent": "Is there a comment coming?",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "If we want to start talking talking about noise in the test data, then this is 1 framework to look in.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's this very nice paper by both 2 and biscuit or a couple papers and they show that if you want to start analyzing this function, you can decompose your error into two parts.",
                    "label": 0
                },
                {
                    "sent": "You've got the optimization error, which is exclusively what we've been talking about up to this point, and you've got also got the estimate estimation error, so the optimization errors you have N samples you saw.",
                    "label": 0
                },
                {
                    "sent": "How well do you solve the optimization problem restricted to those end samples?",
                    "label": 0
                },
                {
                    "sent": "The customization error is saying how well do those end samples approximate the true error?",
                    "label": 0
                },
                {
                    "sent": "If you really want to talk about generalization error, you also have to talk about how well your model can approximate the true underlying function.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to worry about that today because I don't have many things to say about that.",
                    "label": 0
                },
                {
                    "sent": "You can, you can kind of hide that into estimation error, but analyzing this thing is a hard issue.",
                    "label": 0
                },
                {
                    "sent": "So usually you just look at these two.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Terms.",
                    "label": 0
                },
                {
                    "sent": "OK so boom boom Buscat looked at these two sort of strategies.",
                    "label": 0
                },
                {
                    "sent": "So one strategy which is kind of what we talked about the first time is OK. You can generate as much data as you want.",
                    "label": 0
                },
                {
                    "sent": "What should you do?",
                    "label": 0
                },
                {
                    "sent": "So one thing you can think of doing is you have to have a certain time restriction T you can.",
                    "label": 0
                },
                {
                    "sent": "You can evaluate gradients and that's how much time you have to run your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what you can do or or well.",
                    "label": 0
                },
                {
                    "sent": "Not not quite.",
                    "label": 0
                },
                {
                    "sent": "You can generate T samples of your function, so one strategy is you generate those T samples and now you run LBF GS on them or something like that and you just exactly solve your approximation on that training set.",
                    "label": 0
                },
                {
                    "sent": "So you can get your optimization to zero an analyzing the estimation error is a little bit more complicated, but under certain assumptions you can get it to 1 / T and we have reasons to believe from the Cramer Rao bound that it's probably not lower than 1 / T in some settings.",
                    "label": 0
                },
                {
                    "sent": "Right, so I'll get to that on the next slide.",
                    "label": 0
                },
                {
                    "sent": "I'm definitely getting there, so I just want to.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cross that with the other strategy, which is you have your tea samples.",
                    "label": 0
                },
                {
                    "sent": "Let's just run stochastic gradient through RT samples one time.",
                    "label": 0
                },
                {
                    "sent": "So our optimization error as we showed last time was 1 / T or estimation error.",
                    "label": 1
                },
                {
                    "sent": "Still depends on those T samples or estimation error is in principle the same as it was.",
                    "label": 0
                },
                {
                    "sent": "In this case we're just using T samples and that suggests the strategy that Joshua just mentioned that if we only have can evaluate gradients well, this is definitely going to take at least the gradients, probably more than T gradients.",
                    "label": 0
                },
                {
                    "sent": "So why not just do this 'cause it seems like it's we're doing better in terms of epsilon there.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Answer.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question is.",
                    "label": 0
                },
                {
                    "sent": "This result seems to indicate you should just just go through your data set one time with stochastic gradient, and this paper actually made it very.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hard to get optimization papers accepted for a few years.",
                    "label": 0
                },
                {
                    "sent": "Because it sort of gave theoretical justification for this motivation that well, if we improve on the optimization error, it doesn't really help us.",
                    "label": 0
                },
                {
                    "sent": "'cause ultimately doesn't affect the test error.",
                    "label": 0
                },
                {
                    "sent": "Old.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, good point.",
                    "label": 0
                },
                {
                    "sent": "So~ means there might be a log factor hidden in there, so it might be like log T / T or something like that.",
                    "label": 0
                },
                {
                    "sent": "So you know there's a more you can Wikipedia over~ but yeah, there's there's something there might be something like a log T in there, or log T squared or something like that.",
                    "label": 0
                },
                {
                    "sent": "So something that is not really very big.",
                    "label": 0
                },
                {
                    "sent": "No, because the estimation error is not a property of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The optimization error depends on the algorithm you chose the estimates.",
                    "label": 0
                },
                {
                    "sent": "The estimation error only depends on the samples you chose, which, and then you can apply any algorithm you want to them.",
                    "label": 0
                },
                {
                    "sent": "So these two are really the same.",
                    "label": 0
                },
                {
                    "sent": "It's only these two that are variable.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's do a survey.",
                    "label": 0
                },
                {
                    "sent": "How many people in the room only go through your data set one time?",
                    "label": 0
                },
                {
                    "sent": "Do a little size.",
                    "label": 0
                },
                {
                    "sent": "That's a great point.",
                    "label": 0
                },
                {
                    "sent": "So if you have an infinite data set, then the answer is actually you know you don't even go through your data set one time.",
                    "label": 0
                },
                {
                    "sent": "But let's say you don't have an infinite data set.",
                    "label": 0
                },
                {
                    "sent": "Do you go through the data set more than once?",
                    "label": 0
                },
                {
                    "sent": "I see some hesitant nods, nodding and some groggy smiles, smirks, and things like that, OK?",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I like this quote 'cause it is from some of some some people who really know what they're talking about, more so than me in this infinite data case.",
                    "label": 0
                },
                {
                    "sent": "So the overwhelming empirical evidence is that for all actual data, the ERM is actually better in terms of test there, and we have no understanding of why this happens.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's potentially not completely useless to run an optimization algorithm that's faster than stochastic gradient.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the reason is actually pretty simple.",
                    "label": 0
                },
                {
                    "sent": "It's that the constants matter.",
                    "label": 0
                },
                {
                    "sent": "So let's say stochastic gradient is optimal up to a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "It sounds really nice from like a complexity theory perspective, but if I tell you well, you can do this very fast algorithm, But it's going to get you your double test error.",
                    "label": 0
                },
                {
                    "sent": "It's going to double your test error going from 10% to 20%.",
                    "label": 0
                },
                {
                    "sent": "Practitioners care about these constants, right?",
                    "label": 0
                },
                {
                    "sent": "If I'm going to double your error, you don't want to do that.",
                    "label": 0
                },
                {
                    "sent": "So those constants matter, so stochastic gradient, it's true, it's it is optimal in terms of sample size, but there's other things that matter.",
                    "label": 0
                },
                {
                    "sent": "An you may want to have good dependency on those 'cause stochastic rate actually doesn't have very good dependency on those.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been a few.",
                    "label": 0
                },
                {
                    "sent": "There's been this birthday all said that, well, the erm, the stochastic gradient are not the only possible things you can think about using growing batch sizes, and they showed that with growing batch sizes, you can basically achieve the same thing you do with stochastic gradient method, possibly with lower optimization error.",
                    "label": 1
                },
                {
                    "sent": "Another thing you could think of doing is well, let's just instead of taking T samples, I'll take some some number less than T and I'll revisit some of the examples.",
                    "label": 0
                },
                {
                    "sent": "I'll apply sag to a smaller data set.",
                    "label": 0
                },
                {
                    "sent": "So now my optimization accuracy is way better.",
                    "label": 0
                },
                {
                    "sent": "I have linear convergence rate, but my my estimation accuracy is worse.",
                    "label": 0
                },
                {
                    "sent": "It's now 1 / N instead of 1 / T. You're going to ask me that again next summer.",
                    "label": 0
                },
                {
                    "sent": "I have a student who really wants learning, but I haven't gone through that literature yet, so I don't.",
                    "label": 1
                },
                {
                    "sent": "OK yeah, I actually have no idea.",
                    "label": 0
                },
                {
                    "sent": "That would make sense.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to beat Cramer Rao bound.",
                    "label": 0
                },
                {
                    "sent": "I'll show you how you can beat it in two slides, but it requires a very crazy assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea here is, well, we made our estimation error worse.",
                    "label": 0
                },
                {
                    "sent": "But if we can make our optimization error much better, we can.",
                    "label": 0
                },
                {
                    "sent": "We can improve the sum of those two, so there's no.",
                    "label": 0
                },
                {
                    "sent": "There's no proof here, it's just sort of obvious observations.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you're in the case where actually your optimization problem is very difficult, in principle, you can do better on the test error by having a fancier optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Are people.",
                    "label": 0
                },
                {
                    "sent": "If your optimization problem is very difficult, so when you run stochastic gradient it just is doing terrible at solving the optimization problem, then you can do better by looking at fewer samples and solving the optimization problem more accurately.",
                    "label": 0
                },
                {
                    "sent": "That just follows from.",
                    "label": 0
                },
                {
                    "sent": "It's the sum of the two terms.",
                    "label": 0
                },
                {
                    "sent": "There's there's a.",
                    "label": 0
                },
                {
                    "sent": "You can make one bigger if you make the other, you can make the sound bigger if you make one bigger and the other one much smaller.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Just resting the arm.",
                    "label": 0
                },
                {
                    "sent": "Anyone else resting arms?",
                    "label": 0
                },
                {
                    "sent": "So he's literally like this.",
                    "label": 0
                },
                {
                    "sent": "Just in case you think I'm the I'm crazy or something.",
                    "label": 0
                },
                {
                    "sent": "I have it's good to interact with the audience, right?",
                    "label": 0
                },
                {
                    "sent": "It's good to answer questions and things like that.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here is, I think the most fascinating result that's come out since the both 2 and boost get paper as is just this year.",
                    "label": 0
                },
                {
                    "sent": "It's called the streaming S PRG algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we start with some initial point and some initial sample size end.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not the same as the previous size.",
                    "label": 0
                },
                {
                    "sent": "Think think like one or two or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to run SVR G. Instead of computing before this D, which is based on the full set of data set, we're going to end fresh samples and compute Dr Full gradient on those end samples.",
                    "label": 0
                },
                {
                    "sent": "Then we're gonna run SVG for EM iterations on fresh samples, we're going to generate new data points, so it's almost like a true stochastic gradient method, except for occasionally we stop and we compute the average of a bunch of gradients, and we're going to increase our sample size as we go.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Step.",
                    "label": 0
                },
                {
                    "sent": "Of the older iterations.",
                    "label": 0
                },
                {
                    "sent": "So this is what is going to let us control our variance, so we're going to take a bunch of samples, take the average of those so that we know what the general overall direction that we should be going in is, and then we run stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "But we're going to correct it by the direction, the overall direction that we know we should be going, and then this is almost like I don't want to call it a second order approximation, but it's definitely a difference in gradients that sort of correcting the overall direction based on the current sample in the current.",
                    "label": 0
                },
                {
                    "sent": "Current data data point that we sample.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's different than regular SVG 'cause we keep generating new samples.",
                    "label": 0
                },
                {
                    "sent": "We're not.",
                    "label": 0
                },
                {
                    "sent": "We don't have a finite data set anymore.",
                    "label": 0
                },
                {
                    "sent": "We can just run this forever if we have an infinite data set.",
                    "label": 0
                },
                {
                    "sent": "You can also.",
                    "label": 0
                },
                {
                    "sent": "So so, so regular SVR G. You would do this step on all of your end samples and then you'd sample these from those same end samples.",
                    "label": 0
                },
                {
                    "sent": "So here the.",
                    "label": 0
                },
                {
                    "sent": "So in the normal 1, they're roughly the same size.",
                    "label": 0
                },
                {
                    "sent": "Here M is fixed and N is getting bigger.",
                    "label": 0
                },
                {
                    "sent": "So we're actually spending less time doing stochastic gradient and more time fearing what the average direction we should be is overtime.",
                    "label": 0
                },
                {
                    "sent": "So in the beginning, we're doing a ton of stochastic gradient steps, but as we go along, we're more trying to figure out what the direction is to keep making progress.",
                    "label": 0
                },
                {
                    "sent": "Please try out this album.",
                    "label": 0
                },
                {
                    "sent": "I'm very curious to hear how it works.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "'cause the empirical properties are fascinating.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned the polyak Ruppert result yesterday, which is that if you do, if you start averaging your stochastic gradient iterations after a while, we know that that has the asymptotically optimal efficiency in a statistical sense.",
                    "label": 0
                },
                {
                    "sent": "That's in some sense the best thing you can do after a certain point.",
                    "label": 0
                },
                {
                    "sent": "This is the first algorithm that is in some sense achieves that in a non asymptotic regime.",
                    "label": 0
                },
                {
                    "sent": "It does does as well as the ERM in the non asymptotic case, even when you care about the constants.",
                    "label": 0
                },
                {
                    "sent": "So I think this is wild.",
                    "label": 0
                },
                {
                    "sent": "I think the.",
                    "label": 0
                },
                {
                    "sent": "The yeah it's crazy and there was this comment yesterday about, well, you know all these things you're talking about how these crazy strong assumptions do they ever actually apply?",
                    "label": 0
                },
                {
                    "sent": "And I think this result is kind of interesting, because where did this algorithm come from?",
                    "label": 0
                },
                {
                    "sent": "It came from trying to develop better algorithms for the finite sum problem based on the stochastic method, and then SVG came along and sort of fixed the memory of SAG, and Now this comes along and it's actually saying something more powerful in the infinite data case than we ever had before.",
                    "label": 0
                },
                {
                    "sent": "I'm very curious to know if this actually works.",
                    "label": 0
                },
                {
                    "sent": "I have no idea if it does.",
                    "label": 0
                },
                {
                    "sent": "I haven't tried it out.",
                    "label": 0
                },
                {
                    "sent": "Oh, because it achieved.",
                    "label": 0
                },
                {
                    "sent": "If you have second order methods, in principle you can get the optimal efficiency, But this already gets the optimal efficiency.",
                    "label": 0
                },
                {
                    "sent": "This is already the best you can do in terms of test there.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I don't know, this worked too well, so the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more than that.",
                    "label": 0
                },
                {
                    "sent": "It's because the the asymptotic regime you care about, the Fisher information matrix.",
                    "label": 0
                },
                {
                    "sent": "So you have some norm defined in terms of the Fisher information matrix.",
                    "label": 0
                },
                {
                    "sent": "And when you do Newton, you can sort of cancel those effects out, but you can also achieve that asymptotically with averaging stochastic gradient, and this result is saying that you can sort of cancel that out with this algorithm in a non asymptotic way, which is a result we never had even for 2nd order methods before.",
                    "label": 0
                },
                {
                    "sent": "Yeah, let's say at like a high level that I agree with that, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe making that mathematically precise I'm I'm less clear.",
                    "label": 0
                },
                {
                    "sent": "One thing that's true is Sagan SVR definitely doing something like momentum there just sort of have cleaned it up a bit to to take into account the finite data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can definitely think of DS is like your momentum, but now your omentum is actually an explicit older gradient approximation that's getting more accurate overtime.",
                    "label": 0
                },
                {
                    "sent": "And at the same time these two things are getting closer and closer together, so you're more focusing on the DS Anyways.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, there's there's many ways you can talk about repeating the samples, so if you really just have a finite set of samples, then you're in the regular SVG setting and all the stuff we talked about yesterday applies.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "You can also do that for SVG.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of theory that needs to be done to show that's OK, but my student did it last summer.",
                    "label": 0
                },
                {
                    "sent": "So you can also do that for SVG.",
                    "label": 0
                },
                {
                    "sent": "You can just increase the batch size to make it quite a bit faster on big datasets.",
                    "label": 0
                },
                {
                    "sent": "Was.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, so I only learned about this a few weeks ago, 'cause 'cause Cham Kakati gave a talk in the same session as me at the optimization conference, but.",
                    "label": 0
                },
                {
                    "sent": "I I kind of assume that you can beat this definitely in practice by by recycling things like or or reusing computation somehow.",
                    "label": 0
                },
                {
                    "sent": "This is definitely not.",
                    "label": 0
                },
                {
                    "sent": "Absolutely, and The thing is, you're in some sense almost wasting more computation overtime.",
                    "label": 0
                },
                {
                    "sent": "'cause these accesses are getting closer together, and these devices are getting exponentially larger overtime.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I mean these are theoreticians they're not so worried about the constants or how it works in practice, but as a theoretical result, I think.",
                    "label": 0
                },
                {
                    "sent": "I think it's amazing, but there's definitely ways you can attack it.",
                    "label": 0
                },
                {
                    "sent": "Prove to make it work better in practice.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "If you want to start talking with test error.",
                    "label": 0
                },
                {
                    "sent": "If you're happy talking about training error, you can do your fixed.",
                    "label": 0
                },
                {
                    "sent": "You can do your growing batch size on your fixed subset and look at more and more samples either here or mini batches here if you want to either either one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true, but the theory would still apply.",
                    "label": 0
                },
                {
                    "sent": "This is a non asymptotic theory, so in principle if you go through data set once with this thing, that should give you approximately optimal empirical risk minimizer.",
                    "label": 0
                },
                {
                    "sent": "That should do as well as anything can do on your data set.",
                    "label": 0
                },
                {
                    "sent": "But again, we care about the constants.",
                    "label": 0
                },
                {
                    "sent": "With 11 pass, you only do one pass, you just do this algorithm for one pass.",
                    "label": 0
                },
                {
                    "sent": "In principle, that's this is even more optimal than stochastic gradient in some sense, But the constants matter like there's like some one 60s hidden in this analysis.",
                    "label": 0
                },
                {
                    "sent": "I think you're still.",
                    "label": 0
                },
                {
                    "sent": "I think you can do that, but I think you still lose the test error.",
                    "label": 0
                },
                {
                    "sent": "Things 'cause you're still biased by that finite set.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so you can definitely like run this on like like treat your finite sample as an infinite sample.",
                    "label": 0
                },
                {
                    "sent": "Run this algorithm.",
                    "label": 0
                },
                {
                    "sent": "But I'm not sure if you would want to do that as opposed to just running SVR G because the.",
                    "label": 0
                },
                {
                    "sent": "This once.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's true, but as soon as you start recycling things then you introduce a bias based on the training set.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so in principle this algorithm cannot overfit.",
                    "label": 0
                },
                {
                    "sent": "'cause you're always using new samples.",
                    "label": 0
                },
                {
                    "sent": "Right, you know you you do.",
                    "label": 0
                },
                {
                    "sent": "Look, you do look at this example twice to commute this difference, but you're always looking at new data.",
                    "label": 0
                },
                {
                    "sent": "So in terms of if you're never revisiting data points, you can't adapt to those specific data points, you can't fit the noise in the data points.",
                    "label": 0
                },
                {
                    "sent": "If you get an infinite number of such samples, you.",
                    "label": 0
                },
                {
                    "sent": "You win, right?",
                    "label": 0
                },
                {
                    "sent": "You've solved the problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so so there there are regularity conditions for sure, so that's the statement I made.",
                    "label": 0
                },
                {
                    "sent": "Is is too strong, but it's not.",
                    "label": 0
                },
                {
                    "sent": "It is in the right direction in the sense that if you keep using.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yes, well.",
                    "label": 0
                },
                {
                    "sent": "Assuming that your model is correct.",
                    "label": 0
                },
                {
                    "sent": "Right, so so there was there was there was three.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's three terms here, so if if you have the right model.",
                    "label": 0
                },
                {
                    "sent": "Yeah, even then there could be a problem because it could be that the one with the best generalization error in your family is not the one that does the best on the samples.",
                    "label": 0
                },
                {
                    "sent": "Like minimize minimizing the minimum of the model class might not be the best thing in your model class when it comes to test time.",
                    "label": 0
                },
                {
                    "sent": "So you know if your model is correct, you just have these two terms and then it truly is minimizing the test error.",
                    "label": 0
                },
                {
                    "sent": "Like assuming your loss function is right, but all bets are off if your if your model is not correct.",
                    "label": 0
                },
                {
                    "sent": "So it's the closest thing we have to directly minimizing the test there, but we don't have anything that can do that.",
                    "label": 0
                },
                {
                    "sent": "There's always strong assumptions hidden somewhere.",
                    "label": 0
                },
                {
                    "sent": "You have to try though, right?",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, are there other comments on this?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "To calculate.",
                    "label": 0
                },
                {
                    "sent": "Oh I, I didn't say how to do it.",
                    "label": 0
                },
                {
                    "sent": "I think it's something like 1 / L, But I would have to look at the paper.",
                    "label": 0
                },
                {
                    "sent": "The Lipschitz constant of the maximum over the gradients.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we.",
                    "label": 0
                },
                {
                    "sent": "There's definitely IID assumption here.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can do the best in terms of on those datasets in terms of the noise you add, But if that's if your test data set doesn't come is not noised versions of your training set, then it doesn't mean much in terms of test errors so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can learn to do those better on your training data.",
                    "label": 0
                },
                {
                    "sent": "But that's probably not what you want.",
                    "label": 0
                },
                {
                    "sent": "You probably want to look at new images, and in fact, like the data we have in practice is almost never IID.",
                    "label": 0
                },
                {
                    "sent": "So, so all of this is approximations.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I put my student in charge of figure out how to build a black box, neural that optimizer, and so he's trying to implement everything like we have, like a degrad momentum Adam are prop, RMS, prop, SVR, G and so on.",
                    "label": 0
                },
                {
                    "sent": "So and then I lost.",
                    "label": 0
                },
                {
                    "sent": "I met with him.",
                    "label": 0
                },
                {
                    "sent": "I'm like OK, you've done all these things.",
                    "label": 0
                },
                {
                    "sent": "If I give you a neural network problem, I tell you nothing about it.",
                    "label": 0
                },
                {
                    "sent": "You just get functioning gradients.",
                    "label": 0
                },
                {
                    "sent": "You want to use it.",
                    "label": 0
                },
                {
                    "sent": "A black box method to solve it and you want it to work the first time, no tuning.",
                    "label": 0
                },
                {
                    "sent": "Which method would you choose and he gave Me 2 and one of them was a sphere G. The other one is an algorithm we haven't published yet.",
                    "label": 0
                },
                {
                    "sent": "And actually we decided not to publish that SVG result because we think we can do better than the existing one.",
                    "label": 0
                },
                {
                    "sent": "OK. One more question, maybe?",
                    "label": 0
                },
                {
                    "sent": "And then I need to move on.",
                    "label": 0
                },
                {
                    "sent": "I think it's good to stop and appreciate this.",
                    "label": 0
                },
                {
                    "sent": "This is a very nice result I think.",
                    "label": 0
                },
                {
                    "sent": "Very complicated paper.",
                    "label": 0
                },
                {
                    "sent": "I have definitely not gone through the proof in the subtleties.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and I just want to say that.",
                    "label": 0
                },
                {
                    "sent": "You can beat these balance.",
                    "label": 0
                },
                {
                    "sent": "You can be 1 / T you need.",
                    "label": 0
                },
                {
                    "sent": "You need some sort of strong assumption to do it, but in principle you know the Cramer Rao Cramer Rao bound relies on assumptions and apply.",
                    "label": 0
                },
                {
                    "sent": "So a big hobby of mine optimization is beating these lower bounds that people come up in optimization algorithms.",
                    "label": 0
                },
                {
                    "sent": "But you can also do the same thing for the generalization error.",
                    "label": 0
                },
                {
                    "sent": "So me and Nicola Ru student here who is my office mate in Paris.",
                    "label": 0
                },
                {
                    "sent": "We did a lot of fun things one afternoon.",
                    "label": 0
                },
                {
                    "sent": "We looked at this crazy assumption that appears in some old optimization papers.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say that all the gradients, the norm of the gradient of every individual function is less than some number.",
                    "label": 0
                },
                {
                    "sent": "BB can be whatever you want times the true gradient at that point.",
                    "label": 0
                },
                {
                    "sent": "It seems reasonable until you think about it and you plug in, the gradient is 0 and then you realize you actually need that.",
                    "label": 0
                },
                {
                    "sent": "Your optimal solution to minimize every single function.",
                    "label": 0
                },
                {
                    "sent": "So you have exactly fit the training set.",
                    "label": 0
                },
                {
                    "sent": "Very strong, yeah you should.",
                    "label": 0
                },
                {
                    "sent": "You should not be happy with the assumption we wrote the paper.",
                    "label": 0
                },
                {
                    "sent": "We put it on.",
                    "label": 0
                },
                {
                    "sent": "Archive is actually got a few citations and people asking us why we didn't publish it, but it doesn't seem to apply in like real scenarios, but the.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to make is.",
                    "label": 0
                },
                {
                    "sent": "For every I yeah OK yeah that's true for every X you have this condition.",
                    "label": 0
                },
                {
                    "sent": "And then for every eye it has to be a minimizer of the true function.",
                    "label": 0
                },
                {
                    "sent": "That's true, and that's what I'm getting too.",
                    "label": 0
                },
                {
                    "sent": "'cause I notice during the summer school like I just added this last night, 'cause in the summer school you've always talked about, how well you don't really want to solve the problem 'cause you that overfits.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what what when I get to our result which which you know mean Nicola like did this in an afternoon so we worked on such harder problems.",
                    "label": 0
                },
                {
                    "sent": "This was kind of like a fun thing to unwind after sag.",
                    "label": 0
                },
                {
                    "sent": "I guess that's what you do in Paris.",
                    "label": 0
                },
                {
                    "sent": "If you're really nerdy.",
                    "label": 0
                },
                {
                    "sent": "OK, you get a rate that looks like this.",
                    "label": 0
                },
                {
                    "sent": "So if I didn't have this be squared here, this is exactly the rate of gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So looking at the entire data set, taking my exact gradient step, linear convergence very fast and you have this fudge factor based on B, ^2 B squared is 1.",
                    "label": 0
                },
                {
                    "sent": "It actually is the gradient method if all your functions are exactly the same.",
                    "label": 0
                },
                {
                    "sent": "It's also the gradient method, but you're just looking at one function instead of looking at the mall, so you lose a little bit factor, but you get a very fast rate.",
                    "label": 0
                },
                {
                    "sent": "So this should not be taken as a like formal statement of approval or anything, because obviously this assumption is a little bit crazy, but if you're really expecting to overfit, maybe a constant step size is all you need, as Yoshua kind of just said.",
                    "label": 0
                },
                {
                    "sent": "Like if you're if you really think your model is going to overfit and you really only care about getting up to a certain accuracy.",
                    "label": 0
                },
                {
                    "sent": "Maybe you should just use a constant stepsize.",
                    "label": 0
                },
                {
                    "sent": "Maybe you don't worry bout convergence at all.",
                    "label": 0
                },
                {
                    "sent": "If you want to get good test there.",
                    "label": 0
                },
                {
                    "sent": "This is really a result on the expectation in the test error setting.",
                    "label": 0
                },
                {
                    "sent": "This saying my test error is going down exponentially fast if I have this condition.",
                    "label": 0
                },
                {
                    "sent": "Up to up to that.",
                    "label": 0
                },
                {
                    "sent": "Model being correct assumption.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to spend too much time on this.",
                    "label": 0
                },
                {
                    "sent": "I just wanted emphasize that there's more going on.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, definitely in this setting you can definitely converge faster.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I hadn't even thought about that.",
                    "label": 0
                },
                {
                    "sent": "Actually there is.",
                    "label": 0
                },
                {
                    "sent": "There is a paper this year on that incremental noon, but they only show linear convergence, which is weird.",
                    "label": 0
                },
                {
                    "sent": "'cause in principle you should get super linear convergence in that setting.",
                    "label": 0
                },
                {
                    "sent": "Extra is the minimum of the function F the minimum of the expectation.",
                    "label": 0
                },
                {
                    "sent": "Note that this is the true average F the true function F0F was this this expectation.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalization error here, sorry I guess I missed that notation, so F of X is the expectation of F of XF of X is.",
                    "label": 0
                },
                {
                    "sent": "The expectation over examples this is your generalization error.",
                    "label": 0
                },
                {
                    "sent": "F of X is 1 function and F of X is the test is the generalization error.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is saying that if you have this crazy assumption, you're.",
                    "label": 0
                },
                {
                    "sent": "Oh so X star by our definition minimizes F. And the and the extra thing implied by this assumption is the XR also minimizes each FI.",
                    "label": 0
                },
                {
                    "sent": "So saying you're fitting each data point exactly, no, no zero training error, absolute zero training error.",
                    "label": 0
                },
                {
                    "sent": "You yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "No, I think this.",
                    "label": 0
                },
                {
                    "sent": "I never really thought about this before, but maybe this could actually be used as somewhat of a justification for just using a constant stepsize and then just stopping at some point.",
                    "label": 0
                },
                {
                    "sent": "Every single.",
                    "label": 0
                },
                {
                    "sent": "I do, I don't know if I do.",
                    "label": 0
                },
                {
                    "sent": "As I said, I you know the type of person who might enjoy the Montreal nightlife rather than go to summer school, but you probably do.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I actually don't want to talk about this too much, but there is some work on if the data is not IID in this.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Online convo?",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimization where time T you make a prediction X and then someone makes you evaluate your X at some random convex function and you can actually prove.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of weak things 'cause you can only compare to how you would do with the best X, but you can still prove some sort of regret bounds in that setting, but I think I won't want to move on for time, but I just want to mention that if you if you do care bout ID data, there are some analysis methods that do let you analyze that setting.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's get to the last part which is non convex functions.",
                    "label": 0
                },
                {
                    "sent": "What everybody is here to see right?",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the two classic.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perspectives on non convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So one is the local perspective.",
                    "label": 0
                },
                {
                    "sent": "So we're going to apply some method that has good methods, good properties for minimizing convex functions.",
                    "label": 0
                },
                {
                    "sent": "We might do like random restarts, or we might do some modification to make sure it eventually gets near a local minimizer.",
                    "label": 0
                },
                {
                    "sent": "But roughly you've got some first phase that's going to try and get you near minimizer once you're near minimizer, you have the second phase that's going to play a nice method for convex functions to just sort of find that local minimizer.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicely.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how long does the first phase take like?",
                    "label": 0
                },
                {
                    "sent": "Does that require doing a multiple restart and like every single error of your space?",
                    "label": 0
                },
                {
                    "sent": "Or does it require running stochastic gradient?",
                    "label": 0
                },
                {
                    "sent": "Like for an extremely long time?",
                    "label": 0
                },
                {
                    "sent": "Something like that?",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's the global perspective, which is even more ambitious is let's actually search for the global minimum of some function class.",
                    "label": 0
                },
                {
                    "sent": "So I I just have like some weird function.",
                    "label": 0
                },
                {
                    "sent": "You probably assume it's athletes list it's continuous or something like that, and I really want to find the global minimum.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned yesterday, you get these sort of rates that scale very badly with the dimension.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can really only solve low dimensional problems and So what I wanted to go over is to maybe think about this framework and discuss well water.",
                    "label": 0
                },
                {
                    "sent": "What are some recent things that have been done in the local, the global and hybrid settings?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's kind of overloaded, so there's one.",
                    "label": 0
                },
                {
                    "sent": "If you say global convergence rate.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so I would put those in this category of like global nonconvex.",
                    "label": 0
                },
                {
                    "sent": "Well, they often have guarantees like this or worse than they actually rely on eventually randomly visiting every single point in this space.",
                    "label": 0
                },
                {
                    "sent": "Right, yes, I agree.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying this is a good rate, I'm just saying this is what they have and that's why those types of methods they can only solve low dimensional problems.",
                    "label": 0
                },
                {
                    "sent": "'cause often those type of methods have this fall back where they make sure they eventually randomly sample everything that can happen and then they say oh we do converge and that's true in some existential setting.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Incense.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about the first type of method 1st and we made this assumption called strong convexity, which looks like this.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In a lot of proofs, what you do.",
                    "label": 0
                },
                {
                    "sent": "Is you minimize both sides of that with respect to X to get an inequality that looks like this.",
                    "label": 0
                },
                {
                    "sent": "And that's sort of the crucial point where strong convexity enters into the proof.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So recently and actually going back even less recently, there's a bunch of extra assumptions people have made to basically get to an inequality like this, so there's things called essential strong convexity, optimal strong convexity, restricted secant equality, and a few others.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is only my postdoc has been looking at, and Steven Wright actually sort of mentioned in his talk at the optimization conference this year that he's also been thinking about this is.",
                    "label": 0
                },
                {
                    "sent": "Why don't we just assume this inequality holds?",
                    "label": 0
                },
                {
                    "sent": "So forget about strong convexity.",
                    "label": 0
                },
                {
                    "sent": "Forget anything like that if that's all I need to prove my convergence rate, let's just assume that that happens somehow, and then let's think about what functions actually satisfy that.",
                    "label": 0
                },
                {
                    "sent": "So I've been, you know, we both said we haven't found where this actually comes from that it sort of just pops out of the proof, but I did a lot of digging lately and it appears in these these two papers in 1963, which I haven't found yet, because as you might guess from the date and the author names, these are not so easy papers to find.",
                    "label": 0
                },
                {
                    "sent": "I had to look up how to make this character in Latech for example.",
                    "label": 0
                },
                {
                    "sent": "If you have the strong property, if you just assume this, it's actually weaker than all of these things, so all of these imply that, so it's weaker than having all these things.",
                    "label": 0
                },
                {
                    "sent": "It doesn't imply the solution is unique.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have a strongly convex F and javax.",
                    "label": 0
                },
                {
                    "sent": "So like a least squares with a singular matrix, it still satisfies this inequality.",
                    "label": 0
                },
                {
                    "sent": "It it it does allow you to have plateaus.",
                    "label": 0
                },
                {
                    "sent": "Oh, it allows you to plateau at F star, but you can't have a plateau far away.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so it's not.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm not saying this is a completely general thing, I'm just saying it's definitely more general than strong convexity, and in particular it actually doesn't imply convexity.",
                    "label": 0
                },
                {
                    "sent": "You don't need convexity for this inequality to be true.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yesterday I think there was a comment that, well, you know, these linear convergence rates even if you're talking about the local sense, they rely on your function looking like this near the optimum, so you assume that it's kind of a bowl, like there's a unique optimum.",
                    "label": 0
                },
                {
                    "sent": "The function is going faster than a quadratic function as you go away from it.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's convex, so if you start talking about things that satisfy that property, you can start talking about things that look like this.",
                    "label": 0
                },
                {
                    "sent": "So you don't need a unique minimizer, you don't need it to be convex.",
                    "label": 0
                },
                {
                    "sent": "You can have concave parts, and you need the function to be growing a little bit faster than a linear function as you as you go away in any particular direction.",
                    "label": 0
                },
                {
                    "sent": "So I'm not saying that this is not going to let you solve all non convex problems, but I'm saying is for certain non convex problems.",
                    "label": 0
                },
                {
                    "sent": "We can actually get this exponential convergence rate and in particular for a lot of the problems we care about in machine learning the convex problems, least squares and so on.",
                    "label": 0
                },
                {
                    "sent": "You can just solve them and the second point is.",
                    "label": 0
                },
                {
                    "sent": "You don't need to be near minimizer like this.",
                    "label": 0
                },
                {
                    "sent": "You can be near minimizers like this, and you'll still converge there exponentially fast.",
                    "label": 0
                },
                {
                    "sent": "We can define it for.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is a generalization of this property for non differentiable functions called the.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whatever this is pronounced, some other name, I can't pronounce inequality which is useful.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it doesn't seem to preserve the properties you want.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we've defined a property for that smooth plus nonsmooth function, which does let you have this property.",
                    "label": 0
                },
                {
                    "sent": "So, for example, people were really interested in proving linear convergence of lasso problems, least squares, plus L1 regularization, and they made all sorts of assumptions to prove this is true.",
                    "label": 0
                },
                {
                    "sent": "There was restricted isometry property.",
                    "label": 0
                },
                {
                    "sent": "There was all these like restricted strong convexity.",
                    "label": 0
                },
                {
                    "sent": "There was like statistical estimation rates.",
                    "label": 0
                },
                {
                    "sent": "All these crazy things.",
                    "label": 0
                },
                {
                    "sent": "Actually, turns out it's just true for that whole problem class.",
                    "label": 0
                },
                {
                    "sent": "As long as a does not equal 0.",
                    "label": 0
                },
                {
                    "sent": "As long as you have at least one nonzero in your matrix, you're already getting linear convergence by applying proximal gradient to lasso problems.",
                    "label": 0
                },
                {
                    "sent": "So not published yet, but my postdoc is furiously writing it up because it's it's kind of neat and there's there's a lot of generalizations you can think of so, so mainly I'm not saying this is going to give you better methods for nonconvex problems, but I'm saying is.",
                    "label": 0
                },
                {
                    "sent": "Actually, you can be in kind of a rough area of a local minimum that could look kind of nasty and you should still converge very quickly to it with standard methods.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so can you talk about global non convex rates and I mean global as in like they apply from iteration one.",
                    "label": 0
                },
                {
                    "sent": "We know the constants and so on.",
                    "label": 0
                },
                {
                    "sent": "So we talked the first day about how for strongly convex problems you can prove that you know the function value has linear convergence.",
                    "label": 0
                },
                {
                    "sent": "You can also show that the iterations converge linearly and also the gradient converges linearly.",
                    "label": 0
                },
                {
                    "sent": "And then we said well convex functions are not quite as nice.",
                    "label": 0
                },
                {
                    "sent": "You now can only show the function values have this 1 / T and the gradient squared also has this 1 / T I actually gradient square is a little bit faster but.",
                    "label": 0
                },
                {
                    "sent": "That's not too important, so can we say something like this for non convex problems can actually put in?",
                    "label": 0
                },
                {
                    "sent": "Oh well, we know these constants for nonconvex problems and get something like this.",
                    "label": 0
                },
                {
                    "sent": "And the answer is no, because proving this would be NP hard.",
                    "label": 0
                },
                {
                    "sent": "But let's assume our function grading is Lipschitz continuous.",
                    "label": 0
                },
                {
                    "sent": "An R function is bounded below.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there's some this result by Gadime Alanin.",
                    "label": 0
                },
                {
                    "sent": "I've found some older results that are similar, but this is this is a horrible paper to read, but if you look through it carefully, they have this result hidden in here.",
                    "label": 0
                },
                {
                    "sent": "That the minimum over your iterations of the normal your gradient is going down as 1 / T, so you're running for T iterations, and you're saying the minimum of the gradient is going to be small and getting smaller overtime.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'll get the saddle points in the next slide, but roughly this is saying.",
                    "label": 0
                },
                {
                    "sent": "This is this is like gradient descent.",
                    "label": 0
                },
                {
                    "sent": "For stochastic gradient descent, you need to make sure the noise is not too small.",
                    "label": 0
                },
                {
                    "sent": "To get this right, but you can also get it first casted gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So saying you run the algorithm long enough at some point in time the algorithm is going to be something that looks close to a stationary point.",
                    "label": 0
                },
                {
                    "sent": "It's not saying anything you want minimum, maximum, saddle point, whatever, but it is saying you're going to get to a region where gradient descent goes really slow at some point in time.",
                    "label": 0
                },
                {
                    "sent": "You can view this as like a positive or a negative result, right?",
                    "label": 0
                },
                {
                    "sent": "You can say like, oh, this is an approximate stationary point, so that seems like a good thing.",
                    "label": 0
                },
                {
                    "sent": "Gradient probably is not going to be attracted to a maximum, so at least it's maybe a saddle point or minimizer.",
                    "label": 0
                },
                {
                    "sent": "Or you can say that like oh look gradient descent with is is going to get stopped moving at some point in time.",
                    "label": 0
                },
                {
                    "sent": "It's going to stop making measurable progress at some point.",
                    "label": 0
                },
                {
                    "sent": "Or or at least it's going to be in a region where it's not making measurable progress.",
                    "label": 0
                },
                {
                    "sent": "So even if you have a beautiful function at some point in time, it has to stop making progress.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, which relates to OK, so so one comment about that.",
                    "label": 0
                },
                {
                    "sent": "There's no dimension independent dependent, so so even if you're in high dimensions, that result is still OK.",
                    "label": 0
                },
                {
                    "sent": "But of course you're giving up on optimality, so if you have an approximate saddle point, it definitely says that you can just converge to approximate saddle Point, which is something we know.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about escaping from saddle points.",
                    "label": 0
                },
                {
                    "sent": "So the classical approach is use a trust region method and that lets you have negative eigenvalues in your Hessian, and if you have that, you can actually not.",
                    "label": 0
                },
                {
                    "sent": "You don't really necessarily converge to saddle points, you can show that you find like a strict minimizer.",
                    "label": 0
                },
                {
                    "sent": "There was also this work by the group here, showing how to modify the spectrum if you have negative eigenvalues, I think you take the absolute value, so in some sense you have the component of the direction you need in.",
                    "label": 0
                },
                {
                    "sent": "Inside your things, you're going to go in that direction, and there was a paper this year saying if we're doing stochastic gradient and you take random noise around your point, that can let you escape saddle points where you have at least one negative eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "I don't remember if they say you do that quickly, but the idea is the random directions will have some component in this like direction that gets you off the Cliff and that in principle can get you out of saddle points.",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "I think it would have to be like a weird case for this to actually apply.",
                    "label": 0
                },
                {
                    "sent": "Absolutely.",
                    "label": 0
                },
                {
                    "sent": "But I don't know how to analyze that.",
                    "label": 0
                },
                {
                    "sent": "That's that's what they're showing.",
                    "label": 0
                },
                {
                    "sent": "They require that you have at least one negative eigenvalue, right?",
                    "label": 0
                },
                {
                    "sent": "You could have a saddle point that's like a long flat Valley or something like or something weird like that so.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you need that direction going down for this result to apply.",
                    "label": 0
                },
                {
                    "sent": "If it's just flat in every direction then.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, of course.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's here's a or do you want to talk about?",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Little bit more.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So again, you do need the negative curvature eyes.",
                    "label": 0
                },
                {
                    "sent": "I don't know if saddle points are defined as actually having a negative direction or not.",
                    "label": 0
                },
                {
                    "sent": "If you allowed to be 0, but if you if you have a negative direction then you're supposed to minimize your quadratic approximation over the trust region.",
                    "label": 0
                },
                {
                    "sent": "So if you've got a negative direction it should pick the negative direction and get out of there.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so we're not.",
                    "label": 0
                },
                {
                    "sent": "We're not well, yeah it's true so it's so it's not a Newton step it's a it's it's you're minimizing this approximation of the function subject to stay in the trust region.",
                    "label": 0
                },
                {
                    "sent": "And unlike a lot of Linesearch Newton methods where you have to modify the eigenvalues to be positive definite in trust region methods you can have a negative.",
                    "label": 0
                },
                {
                    "sent": "You can have negative eigenvalues and yes in approximation.",
                    "label": 1
                },
                {
                    "sent": "And then when you minimize it it get can get you out of there.",
                    "label": 0
                },
                {
                    "sent": "More comments on saddle points.",
                    "label": 1
                },
                {
                    "sent": "OK, so let's get to this method.",
                    "label": 0
                },
                {
                    "sent": "So it was proposed in 2006.",
                    "label": 1
                },
                {
                    "sent": "I really wish it would have been proposed in the 1800s because it would have made optimization so much nicer.",
                    "label": 0
                },
                {
                    "sent": "This is what Newton's method really should have been.",
                    "label": 0
                },
                {
                    "sent": "So remember, we talked about grading method being.",
                    "label": 0
                },
                {
                    "sent": "Your current function value this linear approximation and then I had an L here, but now I'm going to say I'm going to put the exact test in there, so that's Newton's method which is not about on the function, it's just some approximation of the function that could in principle go crazy.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that the Hessian is Lipschitz continuous.",
                    "label": 0
                },
                {
                    "sent": "Not, not necessarily even the gradient anymore.",
                    "label": 0
                },
                {
                    "sent": "We're just assume that the Hessian is Lipschitz continuous.",
                    "label": 0
                },
                {
                    "sent": "So now I can do the same trick before I go to the third order term in the Taylor expansion, and I just say, well, the Hessian can't change by more than L times time can't change by more than L. If I do like the three way tensor thing, so it's almost like a completely obvious thing to do if you saw the previous perspective on gradient methods, but people didn't.",
                    "label": 0
                },
                {
                    "sent": "I mean there were third order and 4th order methods in the 80s, but they didn't quite have this nice perspective 'cause they think they were biased by the classic.",
                    "label": 0
                },
                {
                    "sent": "Like two Phase Newton convergence thing.",
                    "label": 0
                },
                {
                    "sent": "So the beautiful thing about this algorithm is now the.",
                    "label": 0
                },
                {
                    "sent": "You know Newtons method guaranteed to decrease the objective.",
                    "label": 0
                },
                {
                    "sent": "This is an upper bound on the objective function.",
                    "label": 0
                },
                {
                    "sent": "There's no line search or trust region or anything like that, it's just guaranteed to work.",
                    "label": 0
                },
                {
                    "sent": "If you have a convex functions, you gotta linear convergence and then a superlinear convergence once you're close to the solution, we can start talking about the constants are which is often hard for Newton's method.",
                    "label": 0
                },
                {
                    "sent": "So it's very elegant and it's very surprised that it took so long to actually come up with.",
                    "label": 0
                },
                {
                    "sent": "It's very obvious in hindsight, but in this paper what they show is if this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Gets within some ball of a saddle point, so once it gets there some radius around each saddle point, such that when it gets within that radius, the next step is not only going to move out of the ball, but it's going to move to a function value that's lower than the saddle point.",
                    "label": 0
                },
                {
                    "sent": "So there's there's no.",
                    "label": 0
                },
                {
                    "sent": "This is, you know, formally proved there.",
                    "label": 0
                },
                {
                    "sent": "It's all you know.",
                    "label": 0
                },
                {
                    "sent": "You can just look at this paper.",
                    "label": 0
                },
                {
                    "sent": "This is a theorem saying if you're close to the point we're going to move away from that point, so they actually cannot converge to saddle points.",
                    "label": 0
                },
                {
                    "sent": "That's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "And also this looks a bit scary too, 'cause this is nonconvex, so I haven't gone through the computation in detail.",
                    "label": 0
                },
                {
                    "sent": "Supposedly you can solve these for the same cost as Newton's method, but again, you do need the Hessian and.",
                    "label": 0
                },
                {
                    "sent": "There is work on like you know, Hessian, free quasi Newton, whatever.",
                    "label": 0
                },
                {
                    "sent": "I don't know if they maintain this property, though I think there was a question up there.",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can do pretty much as you like.",
                    "label": 0
                },
                {
                    "sent": "You probably have to add a line search or some sort of making sure that the conditioning doesn't go crazy here, and I'm not sure if you maintain this property in that setting.",
                    "label": 0
                },
                {
                    "sent": "Other comments I think you know you've probably never seen this before, and it's kind of an obvious thing to do.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll maybe I'll just pause 'cause I feel like we should talk about a bit more.",
                    "label": 0
                },
                {
                    "sent": "If XK is a local minimum.",
                    "label": 0
                },
                {
                    "sent": "This will be 000 when you solve for D. 'cause 'cause?",
                    "label": 0
                },
                {
                    "sent": "Remember if these if D is very small, these two terms are dominated by this term which which will be.",
                    "label": 0
                },
                {
                    "sent": "Which will be positive in every direction.",
                    "label": 0
                },
                {
                    "sent": "And these two these two terms are always.",
                    "label": 0
                },
                {
                    "sent": "I guess this doesn't necessarily have to be positive.",
                    "label": 0
                },
                {
                    "sent": "I didn't think about that case, but it will be.",
                    "label": 0
                },
                {
                    "sent": "It will be non negative if you're at a minimum.",
                    "label": 0
                },
                {
                    "sent": "And S drive does show that this method actually solves some non convex problems.",
                    "label": 0
                },
                {
                    "sent": "But they look kind of like the ones I showed before where you have the flat thing and the sort of approximately convex type of thing.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's about all I have to say about these sort of two phase methods, so let's move on to methods which actually solve globally optimal problems, even though they're non convex and these there's been a whole bunch of work lately on matrix style problems.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the Classic One is PCA.",
                    "label": 0
                },
                {
                    "sent": "So PCA anyway.",
                    "label": 0
                },
                {
                    "sent": "You write it.",
                    "label": 0
                },
                {
                    "sent": "You gotta nonconvex problem, we can compute PCA.",
                    "label": 0
                },
                {
                    "sent": "You know you just type SVD into Matlab.",
                    "label": 0
                },
                {
                    "sent": "It's not a problem.",
                    "label": 0
                },
                {
                    "sent": "There was a paper recently gave like basically a version of SVG for PCA, so you can.",
                    "label": 0
                },
                {
                    "sent": "You can actually convert PCA very quickly in the finite data regime.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk about something more interesting than PCA.",
                    "label": 0
                },
                {
                    "sent": "So Buren Montero.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I think they didn't.",
                    "label": 0
                },
                {
                    "sent": "They could have wrote their paper differently, so they emphasize this a bit more.",
                    "label": 0
                },
                {
                    "sent": "They sort of set it as a side point.",
                    "label": 0
                },
                {
                    "sent": "If I want to minimize some function of a matrix, and I want my matrix to be positive semidefinite an have bounded rank.",
                    "label": 0
                },
                {
                    "sent": "One thing that's very common is if the matrix is huge.",
                    "label": 0
                },
                {
                    "sent": "Use this representation VV transpose so you have some sort of like low rank approximation to X and they showed that under some fairly general conditions that actually doesn't introduce spurious local minimum, so you can you can optimize this V. It's a way faster and it still actually globally solves your problem.",
                    "label": 0
                },
                {
                    "sent": "So I think that's a very neat result.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not just the orthogonality constraint, it's actually that we're maximizing a convex function, so this is really a concave minimization, so you'd have to fix both things, I think.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I would suspect it's easier without orthogonality, but I'm not sure.",
                    "label": 0
                },
                {
                    "sent": "Or it might even be equivalent.",
                    "label": 0
                },
                {
                    "sent": "I mean 'cause the the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I am not completely sure the answer to that question.",
                    "label": 0
                },
                {
                    "sent": "OK, so so for certain matrix problems, even though if we've turned a convex problem into a nonconvex problem, you can still solve the nonconvex problem.",
                    "label": 1
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can also talk about going the other way, and there's a whole bunch of works that do this, But this this recent ICML paper was kind of.",
                    "label": 0
                },
                {
                    "sent": "Is A is a nice reference where they actually say, well, let's just assume I have a problem like this and they focus on this particular case and they show that as long as you do the initialization in a reasonable way, you're actually going to solve the nonconvex problem, even though we're not referencing some original convex problem, but.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "If you start talking UV transpose like the like, the Netflix Style collaborative filtering you have to make a little bit more assumptions to actually say that you're solving it, and you're sort of solve it in a weaker sense, But there still are results in that flavor, and they usually are two phase methods where there is a first phase where you try to find some good initial value, and then the second phase where you apply alternating minimization on U&V.",
                    "label": 0
                },
                {
                    "sent": "And then there's also been some recent work on things like Hmm's, another.",
                    "label": 0
                },
                {
                    "sent": "Often probabilistic models, showing that you can, you can solve those problems in some sense with some weird parameterisation.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really give you the parameters, but you can get your probabilities right as the data set size goes.",
                    "label": 0
                },
                {
                    "sent": "So I kind of want to mention that I have no idea if this is relevant in deep learning or not, I just think it's interesting and in this vein.",
                    "label": 0
                },
                {
                    "sent": "Translation yeah, kind of.",
                    "label": 0
                },
                {
                    "sent": "It's nonparametric.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's not nonparametric in the number of training examples the way we normally think it's in the number of variables somehow.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I just read this this HMM paper finally last week.",
                    "label": 0
                },
                {
                    "sent": "So I think my understanding is that you just need the.",
                    "label": 0
                },
                {
                    "sent": "You just need to estimate the empirical moments and then you format that as a matrix problem.",
                    "label": 0
                },
                {
                    "sent": "I'd have to think about if that's how that's related to non parametric I, I'm not.",
                    "label": 0
                },
                {
                    "sent": "Seeing connection immediately.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So moving on, you can start talking about convex relaxation.",
                    "label": 0
                },
                {
                    "sent": "So here you trying to approximate a nonconvex problem with the convex problem.",
                    "label": 0
                },
                {
                    "sent": "That's very common, and that has been done for deep learning by by the lab here many years ago, and there was also a nice EML or I think it was actually.",
                    "label": 0
                },
                {
                    "sent": "And nips paper and archive paper in the last year on this.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the restrictions can be very strong to do this, or they may not be a good approximation or something like that, so there.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Always tradeoffs there.",
                    "label": 0
                },
                {
                    "sent": "Another thing you can do is you solve the convex dual so for certain non convex problems you have strong duality's.",
                    "label": 0
                },
                {
                    "sent": "You can solve the dual problem and it solves the original problem.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the dual has nicer problems.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure if how to compute for neural networks, so I'm.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That time OK?",
                    "label": 0
                },
                {
                    "sent": "Also, you can exactly right any non convex problem.",
                    "label": 0
                },
                {
                    "sent": "Any reason why is a convex problem but the size might be enormous.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So at the end.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I just mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to.",
                    "label": 0
                },
                {
                    "sent": "If you're doing Bayesian optimization, that might make you go faster than grid search, or it might not, depending on how smooth the function.",
                    "label": 1
                },
                {
                    "sent": "So if you're in Bayesian optimization, you think this solves nonconvex problems.",
                    "label": 0
                },
                {
                    "sent": "It's not quite true unless you have very nice function.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's the summary and I'll stop there 'cause Joshua just stood up.",
                    "label": 0
                }
            ]
        }
    }
}