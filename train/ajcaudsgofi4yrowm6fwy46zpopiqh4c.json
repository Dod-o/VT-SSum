{
    "id": "ajcaudsgofi4yrowm6fwy46zpopiqh4c",
    "title": "Thermodynamics as a theory of bounded rational decision-making",
    "info": {
        "author": [
            "Daniel Braun, Max Planck Institute for Biological Cybernetics, Max Planck Institute",
            "Pedro A. Ortega, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Oct. 16, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Physics->Statistical Physics",
            "Top->Mathematics->Control Theory"
        ]
    },
    "url": "http://videolectures.net/cyberstat2012_braun_ortega_thermodynamics/",
    "segmentation": [
        [
            "I'm well, thank you very much for for invitation.",
            "I'm very excited about being here.",
            "Daniel told me that in the in the previous talks and you've been mainly talking about the mathematical underpinnings of the free energy framework, and I would like to give some insights that we can't.",
            "From working with the free energy framework."
        ],
        [
            "So as we all know, the mathematical foundations of economics, artificial intelligence and control.",
            "It's a theory of.",
            "Subjective, expected utility leading ultimately to the maximal expected utility principle, which tells us how to pick a policy in an optimal."
        ],
        [
            "However, the exact application of the maximum expected utility principle is intractable, and even for very simple systems, at the end of the toy, problems with larger and it explodes, and So what we really need is a theory of bounded rationality that considers the cost of choice."
        ],
        [
            "So how do we design such a theory?",
            "And there's a caveat here.",
            "So the first thing that you would like to do is to do meta reasoning.",
            "OK, so the idea consists in in penalising the costs of the choice.",
            "So let's start with the desired behavior you and then you say OK, so I'm going to define a new utility function U prime that is going to be the original utility function, and then I add the penalization term which can depend on the policy can also depend on the.",
            "In general, on the search algorithm that you use to find the optimal policy.",
            "But then you've enlarge your policy space.",
            "So essentially solution space.",
            "So you could say hey, wait a minute.",
            "I can also reason about reasoning as well.",
            "You can do reasoning about math."
        ],
        [
            "Reason so I define nothing.",
            "Stops me esentially from defining a U2 prime that does that takes EU prime before and that's a new penalization term.",
            "That is the cost of the the the search function over the search function over the policy."
        ],
        [
            "And so forth can repeat this at infinite amount.",
            "Of course, at least the problem."
        ],
        [
            "Because I can create a hierarchy of an unbounded hierarchy of meta levels, and on top of that I'm increasing the the solution spaces.",
            "So then in order to break this hierarchy, what we need to do is just ban meta reasoning.",
            "So the way we need to think about it is that we have a decision maker that effectively that really wants to maximize the utility criterion, but suddenly he gets interrupted because he runs out of resources and he's not allowed to think about these resources.",
            "And this is the message that we want to get there."
        ],
        [
            "Essentially so the question, how do we characterize behavior when the decision maker is bounded rational, so when he's processing resources are limited."
        ],
        [
            "So now answer is that the bounded rational decision maker can be thought of as maximizing the negative free energy difference with the KL control costs.",
            "So for example in the one step scenario, when you just have to make one choice, it would be expected utility penalized by the by the KL times and inverse temperature parameter and the multi step scenario.",
            "When you have a full decision tree with all these notes in between we would had.",
            "A reward penalized by a KL term with its own temperature node specific temperature.",
            "So that's the general solution we propose."
        ],
        [
            "And where does this come from?",
            "So essentially the result is based on an information theoretic assumption about the transformation costs.",
            "Essentially the costs of changing information state into different information state so."
        ],
        [
            "What do we mean by that?",
            "So our our basic assumption.",
            "Fundamental assumption here is that the difficulty of producing an event determines its probability.",
            "So in other words, the probabilities encode costs.",
            "This is a more I know that we have been talking about duality's here between costs and probabilities, but we take this interpretation interpreted in a radical way.",
            "So."
        ],
        [
            "I want to give you example, so think about for instance, that biologists.",
            "Biologists infer behavior from anatomy.",
            "So what is the underlying?",
            "What is the abstract reason why they do that?",
            "If it's because they think that energy efficient behavior is more frequent than energy inefficient behavior.",
            "Conversely, you can also you can also say that engineers design systems such that desirable behavior is cheaper than undesirable behavior.",
            "OK.",
            "So so in essence then.",
            "When we talk about changes of information state, every time you have the system interact with the world by producing an action or gathering an observation, or in general interaction.",
            "The information state changes necessarily because simply because we are able to tell the before and the after these two time instance are distinguishable to us.",
            "So there must have been some information that allows us to distinguish these two time instance."
        ],
        [
            "So what this is transformation?",
            "For instance, it could be a chemical reaction and memory update.",
            "Consulting around them, number generator, changing location or simply just advancing in time."
        ],
        [
            "My information state.",
            "I'm getting to that so in order, what do I mean by information?",
            "So this is a simple model, an abstract model.",
            "So let's think of an agent that interacts with environment that produce actions.",
            "It can receive observations, but each interaction transforms the information set, and so imagine that these actions and observations they're drawn from an alphabet from discrete alphabet, and we we label these interactions with with natural numbers.",
            "So after four interactions I can just write down.",
            "The numbers of this that identify these interactions, so we have 4 here and now.",
            "Let's imagine we write them down, interaction tape, and now let's imagine that we want to encode this.",
            "OK, we want to produce a binary encoding of this.",
            "So we have a history tape and each one of these symbols then gets translates, translated into some binary code words and after each interaction we keep appending the corresponding code words.",
            "OK, so this word corresponds with the number 6 #3.",
            "And so forth.",
            "OK, now.",
            "These interactions are encoded in a lossless way, and as you can see, what I mean now by information state is this string.",
            "OK, this string tells you what has.",
            "What is the.",
            "What is the history or what are?",
            "What is the experience that the agent has gathered so far from the perspective of an external observer.",
            "So this is not something that is represented in the agent, it's something that allows us to characterize how the agent has been evolving overtime.",
            "So also as you can see it because we are.",
            "We keep concatenating code words.",
            "No jump, no jumps back in time are allowed here.",
            "So in the case of an MDP you visit the state and you can occasionally visit the same state here.",
            "This cannot happen because.",
            "Basically because this is incremental.",
            "So let's assume now that this tape consists of identical binary storage devices.",
            "And let's think that setting a bit costs the same in each cell.",
            "So setting a 01 doesn't matter has a certain cost, say 3 jewels immediately.",
            "What we conclude from this is that each that the code word lengths are proxies for transformation costs.",
            "And.",
            "By assuming that the reason that this code word length are encoding an desirable or unexpected distribution from the environment, we can say that the code word length also have associated probabilities.",
            "So something like a channel final quote.",
            "So now.",
            "Think about this on the line model abstract model."
        ],
        [
            "And now let's move on and and try to characterize it from a measure theoretic point of view.",
            "So on this side we have a treat that represents all the possible trajectories of dynamical system, so we can all these are all the possible realizations from a measure theoretic point of view.",
            "We have a sample space where each sample corresponds to a full realization of the interaction sequence, and then, let's imagine that we have chosen 3X interactions have occurred.",
            "This in measure space corresponds to conditioning the original and sample space into three slides.",
            "So refining our knowledge about the situation, that the agent is in at the refining, the the information state.",
            "So an information state from the point of view or measure theory is just a measurable set and the transformation is just a condition on the information state.",
            "So given this image here, as you see, this sequential realizations here are models as filtrations as opposed to dynamic consistence where you have say a time operator."
        ],
        [
            "Then, based on this characterization, we state axioms of transformation costs and will not really go into the detail because there is many different ways of doing it, and you're well acquainted with it.",
            "So we assume that this there is a real valued mapping between conditional probabilities and transformation costs which we write down with the role they're additive and monotonic, and you can show them then."
        ],
        [
            "That transforming an information state B into an information state a corresponds simply to one over the inverse temperature.",
            "This Alpha parameter here, which is a real value parameter times the information content of.",
            "Of the set a condition zombie.",
            "And and so this basic this primitive of transformation, 'cause we're going to use this now to construct.",
            "Utility function."
        ],
        [
            "So keep the following image now in your mind.",
            "So let's start from from a distribution Q distribution over 2 possible outcomes.",
            "I'll come #2 is is more probable than Alpha number one.",
            "We're gonna transform it into new distribution P where now X one is more probable than X2.",
            "To understand now what is the transformation costs that we're paying what we have to do?",
            "We have to embed it into into.",
            "Into the picture that I explained before.",
            "So we embedded into probability space.",
            "Here we can think of Q as being a conditional probability measure.",
            "So Q is now asset in this embedding and P is then a subset.",
            "And now we're going to compute the information that you need to compress or to reduce or to refine the original set Q into P. And as you can see this guy here correspond to the blue guy.",
            "And this guy here corresponds to the right guy, so this allows us to think about the transformation cost."
        ],
        [
            "So I'm going to skip over this, so essentially with this."
        ],
        [
            "Instruction here what we achieve is."
        ],
        [
            "We we show that the transformation from Q into P corresponds to this function here, which is our well known free energy.",
            "It's the negative to be correct.",
            "It's the negative free energy difference."
        ],
        [
            "And then the optimal solution to the negative free energy difference with the QR control costs is the Colibri distribution, or essentially an exponential family distribution given by this quantity.",
            "Here, where that is now the normalizing constant.",
            "Essentially the partition function, and if we take this solution and we plug it back into the negative free energy difference, we obtain the log partition function.",
            "Here one over the inverse temperature temp salaak partition function now.",
            "This last part is function is important because it represents the certainty equivalent.",
            "OK, so when you have a random lottery, you want to understand what is the value of this lottery.",
            "Normally in maximum expected in expected utility theory, you would just compute the expectation over a larger, but in this case what we are claiming is that this is the correct quantity and they will give you.",
            "Try to elaborate on this further."
        ],
        [
            "If we take the negative free energy difference, the extremum and now we manipulate the parameter Alpha, the inverse temperature.",
            "What we do is we produce this sigmoid here, where.",
            "If we choose Alpha going to Infinity, what happens is that this lottery.",
            "The value of this lottery is going to be the maximum in the set of outcomes.",
            "If I choose Alpha, if Alpha tends to minus Infinity, we recover the minimum an if Alpha goes to 0, then we get the expectation.",
            "So now if you think about it a little bit.",
            "We start realizing that Alpha plays the role of how much you are on the control over the distribution.",
            "OK, so the higher Alpha is, the more controlled you have upper over the outcome, the more, the more you can pick an outcome out of this lottery.",
            "And if you have, if our zero you have no control, so you get the uncontrolled dynamics.",
            "And if Alpha is negative then you are in this situation where you think you don't have control, but you think that another adverse aerial player is going to pick that.",
            "Worst case scenario for you.",
            "So this is keep this picture in mind, and now we're going to ask ourselves again.",
            "What could be a possible operational interpretation of the inverse temperature parameter.",
            "And we have."
        ],
        [
            "Story, it's not.",
            "It's not a water tight story, but it's very close.",
            "So imagine an imagined we have an urn with different balls.",
            "All these polls are numbered and what we're going to do now is we're going to draw balls from the urn, and we're going to look at the numbers we're going to keep going to kind of keep the maximum.",
            "Obviously, if you do that, an infinite amount of times, then you gonna pick up the maximum you're going to know the maximum discern for sure.",
            "You got, yeah OK, you're going to know the maximum discern for sure, but if you take a finite amount of samples then the maximum over this finite sample is going to be obviously number part.",
            "It's going to be essentially a distribution over the possible maximum.",
            "And what we have shown is that.",
            "There collibra distribution that there are the solution to the negative free energy difference and the distribution of this maximum.",
            "After drawing Alpha samples is upper bounded by this exponentially decreasing value work?",
            "Cyan, Delta are constants.",
            "So essentially the take home message here is that Alpha can also be interpreted as iterations of the search algorithm or number of samples you draw from this Earth.",
            "Now."
        ],
        [
            "Think now let's consider sequential decision problems.",
            "So sequential decision problems are usually stay decision trees and solved using backwards induction so far.",
            "So you have several scenarios here.",
            "For instance, if you interact with the stochastic environment like with a stochastic plant, then the algorithm you use is expected mix where essentially have this decision tree.",
            "You have different nodes around nodes correspond to the expectation operator.",
            "So you take these two guys, compute expectation, you write down the number.",
            "Here you sit here as well, and when you have this triangle here.",
            "So the Max operator.",
            "So you compute the maximum of these two guys, right?",
            "The number here and so you solve the decision tree and then you know by following the optimal path you know the optimal solution.",
            "OK, if you're playing with and with an adversarial player, then you replace this expectation nodes with min nodes.",
            "OK so you repeat the same and if you playing something like backgammon then you have mixed notes here.",
            "So the question is the problem here.",
            "Is that decision rules depend on the kind of systems.",
            "It could be stochastic, cooperative, competitive.",
            "Hybrid and so forth, but this intuitive distinction between the different types of systems is formally unsatisfactory, and so decision rules can be what we want to show that this decision rules can be re expressed in a unified way using the free energy functional here, but the big question here is how do we introduce node specific temperatures and?"
        ],
        [
            "In other words, So what we want to come up with this notion of generalized decision trees.",
            "This tree here such that placing different temperatures parameters.",
            "Here we recovered the different types of decision trees.",
            "So in other words, the different operators expressed different degrees of control as we mentioned before, Max is full control, expectation is no control and so forth.",
            "The goal is to find the generalized operator that expresses the three classical degrees of control plus all the other degrees of control in between."
        ],
        [
            "And we have a.",
            "We have a theorem that explains when you keep the reference, the uncontrolled dynamics, Q if you keep it fixed, the equilibrium dynamics if you keep it fixed and you change the temperature, how do you have to change your corresponding utility criterion to reflect this change of temperature?",
            "And with this, with this theorem, we are now able to."
        ],
        [
            "Start from the initial scenario where we have a one step decision.",
            "One step decision problem.",
            "If we interpret one outcome as being a sequence then we can re express this guy as this decision tree here with homogeneous temperatures and if we now use the theorem to change the individual temperatures of each node then we can construct this generalized trees where we can then implement all these other decision rules and I think I'm going to stop here.",
            "So."
        ],
        [
            "You can also see later that this allows us to construct generalized optimality equations.",
            "I will not go into the detail."
        ],
        [
            "And yeah, OK. Would you like to carry on?"
        ],
        [
            "So."
        ],
        [
            "So as he said, decision-making is usually thought of in terms of lotteries where you pick a lottery with the highest expectation value."
        ],
        [
            "This idea is also been applied to movement control, so think about for example in aiming point of William Tell can be thought of as a lottery where you have lots of different outcomes where the arrows actually landing given by the probabilities that arise through the intrinsic noise of the motor system.",
            "And of course where the arrow Lance is also implies very different costs in this case."
        ],
        [
            "So this doesn't only work for aiming task for any kind of movement, you can think of."
        ],
        [
            "Where you can use ideas from optimal control, but the issue with optimal control is it's not only difficult to compute, but you also requires you all aware precise models of the environment and what happens if you don't have this model.",
            "And here you can also apply this free energy ideas.",
            "When we tested this in Excel."
        ],
        [
            "So if we take this free energy idea this Petro set."
        ],
        [
            "We now say you value this lottery as with the free energy."
        ],
        [
            "And that means you can also give an interpretation to this distributions is equilibrium distributions that arise and corresp."
        ],
        [
            "Once these free energies, so if you have an action, if you have an action variable, you can think about this P0 as your default policy and the Alpha as a measure of your boundedness that tells you then how much you can optimize the utility.",
            "Or you can think about an observation.",
            "Then this Pi is expressing a belief where the PO is your model that you assume initially and you're in the Alpha indicates something like the rationality parameter of your environment that you assume, or you can think about it also as a model uncertainty or ambiguity parameter.",
            "Which allows you to deviate from from your model."
        ],
        [
            "And so this free energy formula can also be approximated through a mean variance tradeoff, which is known in finance as the main website from markovits.",
            "And it makes this.",
            "It makes this temperature parameter pretty intuitive because you can see that this variability is added as a bonus by somebody who's risk seeking.",
            "And is considered as a minus by somebody who is risk averse, and this is interesting because in this case a risk seeking person can be considered as a person that considers its environment as an extension of themselves and environment is helpful where the noise is working in his advantage, whereas in the case of a risk seeking risk averse person, it's the other way around where the environment is perceived as well adversarial."
        ],
        [
            "Now here just going to mention quickly two studies where we tested ideas of this actually with human doing experiments in the laboratory in virtual reality one is a study where we looked for mean variance trade."
        ],
        [
            "Off"
        ],
        [
            "So in this study, subjects had to do a two step task in each trial.",
            "They had to decide whether in the first step to do a hitting task, whether to reach to the left, where they were hitting a target for sure, or whether they were moving to the right and try to hit a very small target, and then in the second step they had to perform an effort so they had to move to one of these circles and the higher the circle is up, the more they had to exert some effort which was unpleasant.",
            "So they wanted to.",
            "Always move to the lower circle.",
            "Now if they were going to the left they would always have to move to this effort circle, which was always on the same spot.",
            "If they were moving to the right and were able to hit the target, they could move to the nice effort circle.",
            "If they missed the target, they had to move to the bad one.",
            "Now what we could do is by manipulating the size of this target and the position of these two efforts circles, we could create many different lotteries where the variability was the same.",
            "So we had created five different classes of their abilities with very different mean values.",
            "And what we could do then is we could see whether the change of the variability subjects would change the point where they are indifferent between these two options.",
            "And of course, if they would not be a sensitive to the variability.",
            "But I only care about the mean of their for their exerting.",
            "Then this five ability classes shouldn't make any difference."
        ],
        [
            "So we have a simple prediction.",
            "We have the certain bats which was always on the left at the same spot and if they were risk sensitive and would exhibit this mean variance tradeoff, we have a prediction of this and we would predict indifferent indifference points that are basically linear functions."
        ],
        [
            "And so this is exactly what we found.",
            "So each plot here is a different subject that did the experiment on the X axis you have the five different variability levels that we tested, and the dashed lines indicates the expected behavior if they were not sensitive to the variability.",
            "But you can see there all sensitive.",
            "Most of them were sensitive to their ability, which means that at this variability level they were prepared to go for the risky lottery on the right side.",
            "Even though the mean was much higher and that means they were overly confident to hit this target that they interpreted their own motor noise as well.",
            "They thought they were better actually are.",
            "And I'm going to get."
        ],
        [
            "Skip this, then here's a."
        ],
        [
            "Another experiment, another three slides, and I'm done.",
            "Where there were a lot of studies that have shown that human sensorimotor behavior is consistent with Bayesian integration.",
            "So here we did a task where also we studied based on integration, but we were looking for deviations from the probabilistic model it subjects might have.",
            "So in this task, subjects did forward and backward movement and particularly they had to move through a target area.",
            "Now in the target area, a target at the beginning of the trial appeared.",
            "This target was drawn from a Gaussian distribution an they saw this target at the beginning of the trial, but under three different feedback conditions they could see exactly where the target is in one trial.",
            "In another trial, they could only see a cloud, a Gaussian cloud giving them only noisy feedback, or they could receive no feedback at all, so they could only rely on their prior information.",
            "So this is an experiment how it was done."
        ],
        [
            "Firstly, before and the intuition behind it is this that this is.",
            "Imagine this is the distribution where the target is strong, the prior distribution that you can learn over many trials and then you receive noisy feedback.",
            "Now here are basically likelihood functions for different feedbacks with different kinds of precision.",
            "So it's very sharp.",
            "You see exactly where the target is, say if it's very broad, then you have a very vague idea of where the target is and then the idea is if you do Bayesian inference then you combine this information and if you're.",
            "Observation is very uncertain.",
            "Then you biasing more towards your prior.",
            "Whereas if you can rely if your information for observation so reliable you more or less going to ignore the prior and you control a picture like this where you say OK.",
            "If I assume the true position is here and I'm going to move exactly here and I draw on this axis the error that I'm going to make with respect to the true position, then I'm going to.",
            "In the ideal case be exactly here with 0 error.",
            "Whereas if I just rely on my prior, I get the plot like this where I see a slope.",
            "So this kind of experiment has been done before."
        ],
        [
            "And we were also."
        ],
        [
            "Able to reproduce this kind of.",
            "Biasing towards your prior depending on the uncertainty level that you have.",
            "So this is this three plots.",
            "But now we did some."
        ],
        [
            "Hang out, we introduced in this false area of force that could either be absent, in which case we reproduce previous findings that show that you do basin integration and we introduced forces that could either go this way up, or this way down.",
            "So that means in one case movement in this direction were cheaper or in this direction they were cheap now."
        ],
        [
            "In these conditions we have two different predictions.",
            "One is the risk neutral prediction, which is just basically so we ignore.",
            "This is just a constant term that we can ignore, which is basically just the Bayesian prediction, which is the change of slope that we've seen in these pictures.",
            "Or we have a risk sensitive predictor that allows deviations from the Bayesian model, and in this case we get actually two terms.",
            "And."
        ],
        [
            "In and it is this second term that we've been studying, so in this term you allow deviations from the Bayesian estimator.",
            "Towards the cheaper movement an, but the important thing is this deviation scales with the amount of uncertainty that you have.",
            "So you see here these lines that are offset from the Bayesian line.",
            "An important thing is that the more you off, the more you're deviating, and what it means is that if subjects were not really aware where the target was, they biased their beliefs towards believing that well.",
            "If I can see it, maybe it's not, it's in the not so costly, which is a bit like.",
            "Wishful thinking and effect of risk sensitive."
        ],
        [
            "Essentially."
        ],
        [
            "OK, I'll stop there, thanks.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm well, thank you very much for for invitation.",
                    "label": 0
                },
                {
                    "sent": "I'm very excited about being here.",
                    "label": 0
                },
                {
                    "sent": "Daniel told me that in the in the previous talks and you've been mainly talking about the mathematical underpinnings of the free energy framework, and I would like to give some insights that we can't.",
                    "label": 0
                },
                {
                    "sent": "From working with the free energy framework.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as we all know, the mathematical foundations of economics, artificial intelligence and control.",
                    "label": 1
                },
                {
                    "sent": "It's a theory of.",
                    "label": 1
                },
                {
                    "sent": "Subjective, expected utility leading ultimately to the maximal expected utility principle, which tells us how to pick a policy in an optimal.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, the exact application of the maximum expected utility principle is intractable, and even for very simple systems, at the end of the toy, problems with larger and it explodes, and So what we really need is a theory of bounded rationality that considers the cost of choice.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we design such a theory?",
                    "label": 0
                },
                {
                    "sent": "And there's a caveat here.",
                    "label": 0
                },
                {
                    "sent": "So the first thing that you would like to do is to do meta reasoning.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea consists in in penalising the costs of the choice.",
                    "label": 0
                },
                {
                    "sent": "So let's start with the desired behavior you and then you say OK, so I'm going to define a new utility function U prime that is going to be the original utility function, and then I add the penalization term which can depend on the policy can also depend on the.",
                    "label": 0
                },
                {
                    "sent": "In general, on the search algorithm that you use to find the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "But then you've enlarge your policy space.",
                    "label": 0
                },
                {
                    "sent": "So essentially solution space.",
                    "label": 0
                },
                {
                    "sent": "So you could say hey, wait a minute.",
                    "label": 0
                },
                {
                    "sent": "I can also reason about reasoning as well.",
                    "label": 0
                },
                {
                    "sent": "You can do reasoning about math.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reason so I define nothing.",
                    "label": 0
                },
                {
                    "sent": "Stops me esentially from defining a U2 prime that does that takes EU prime before and that's a new penalization term.",
                    "label": 0
                },
                {
                    "sent": "That is the cost of the the the search function over the search function over the policy.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so forth can repeat this at infinite amount.",
                    "label": 0
                },
                {
                    "sent": "Of course, at least the problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because I can create a hierarchy of an unbounded hierarchy of meta levels, and on top of that I'm increasing the the solution spaces.",
                    "label": 0
                },
                {
                    "sent": "So then in order to break this hierarchy, what we need to do is just ban meta reasoning.",
                    "label": 0
                },
                {
                    "sent": "So the way we need to think about it is that we have a decision maker that effectively that really wants to maximize the utility criterion, but suddenly he gets interrupted because he runs out of resources and he's not allowed to think about these resources.",
                    "label": 0
                },
                {
                    "sent": "And this is the message that we want to get there.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially so the question, how do we characterize behavior when the decision maker is bounded rational, so when he's processing resources are limited.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now answer is that the bounded rational decision maker can be thought of as maximizing the negative free energy difference with the KL control costs.",
                    "label": 1
                },
                {
                    "sent": "So for example in the one step scenario, when you just have to make one choice, it would be expected utility penalized by the by the KL times and inverse temperature parameter and the multi step scenario.",
                    "label": 0
                },
                {
                    "sent": "When you have a full decision tree with all these notes in between we would had.",
                    "label": 0
                },
                {
                    "sent": "A reward penalized by a KL term with its own temperature node specific temperature.",
                    "label": 0
                },
                {
                    "sent": "So that's the general solution we propose.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And where does this come from?",
                    "label": 0
                },
                {
                    "sent": "So essentially the result is based on an information theoretic assumption about the transformation costs.",
                    "label": 1
                },
                {
                    "sent": "Essentially the costs of changing information state into different information state so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What do we mean by that?",
                    "label": 0
                },
                {
                    "sent": "So our our basic assumption.",
                    "label": 0
                },
                {
                    "sent": "Fundamental assumption here is that the difficulty of producing an event determines its probability.",
                    "label": 1
                },
                {
                    "sent": "So in other words, the probabilities encode costs.",
                    "label": 0
                },
                {
                    "sent": "This is a more I know that we have been talking about duality's here between costs and probabilities, but we take this interpretation interpreted in a radical way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to give you example, so think about for instance, that biologists.",
                    "label": 0
                },
                {
                    "sent": "Biologists infer behavior from anatomy.",
                    "label": 1
                },
                {
                    "sent": "So what is the underlying?",
                    "label": 0
                },
                {
                    "sent": "What is the abstract reason why they do that?",
                    "label": 1
                },
                {
                    "sent": "If it's because they think that energy efficient behavior is more frequent than energy inefficient behavior.",
                    "label": 0
                },
                {
                    "sent": "Conversely, you can also you can also say that engineers design systems such that desirable behavior is cheaper than undesirable behavior.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So so in essence then.",
                    "label": 0
                },
                {
                    "sent": "When we talk about changes of information state, every time you have the system interact with the world by producing an action or gathering an observation, or in general interaction.",
                    "label": 0
                },
                {
                    "sent": "The information state changes necessarily because simply because we are able to tell the before and the after these two time instance are distinguishable to us.",
                    "label": 0
                },
                {
                    "sent": "So there must have been some information that allows us to distinguish these two time instance.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what this is transformation?",
                    "label": 0
                },
                {
                    "sent": "For instance, it could be a chemical reaction and memory update.",
                    "label": 1
                },
                {
                    "sent": "Consulting around them, number generator, changing location or simply just advancing in time.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My information state.",
                    "label": 0
                },
                {
                    "sent": "I'm getting to that so in order, what do I mean by information?",
                    "label": 0
                },
                {
                    "sent": "So this is a simple model, an abstract model.",
                    "label": 0
                },
                {
                    "sent": "So let's think of an agent that interacts with environment that produce actions.",
                    "label": 0
                },
                {
                    "sent": "It can receive observations, but each interaction transforms the information set, and so imagine that these actions and observations they're drawn from an alphabet from discrete alphabet, and we we label these interactions with with natural numbers.",
                    "label": 0
                },
                {
                    "sent": "So after four interactions I can just write down.",
                    "label": 0
                },
                {
                    "sent": "The numbers of this that identify these interactions, so we have 4 here and now.",
                    "label": 0
                },
                {
                    "sent": "Let's imagine we write them down, interaction tape, and now let's imagine that we want to encode this.",
                    "label": 0
                },
                {
                    "sent": "OK, we want to produce a binary encoding of this.",
                    "label": 0
                },
                {
                    "sent": "So we have a history tape and each one of these symbols then gets translates, translated into some binary code words and after each interaction we keep appending the corresponding code words.",
                    "label": 0
                },
                {
                    "sent": "OK, so this word corresponds with the number 6 #3.",
                    "label": 0
                },
                {
                    "sent": "And so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "These interactions are encoded in a lossless way, and as you can see, what I mean now by information state is this string.",
                    "label": 0
                },
                {
                    "sent": "OK, this string tells you what has.",
                    "label": 0
                },
                {
                    "sent": "What is the.",
                    "label": 0
                },
                {
                    "sent": "What is the history or what are?",
                    "label": 0
                },
                {
                    "sent": "What is the experience that the agent has gathered so far from the perspective of an external observer.",
                    "label": 0
                },
                {
                    "sent": "So this is not something that is represented in the agent, it's something that allows us to characterize how the agent has been evolving overtime.",
                    "label": 0
                },
                {
                    "sent": "So also as you can see it because we are.",
                    "label": 0
                },
                {
                    "sent": "We keep concatenating code words.",
                    "label": 0
                },
                {
                    "sent": "No jump, no jumps back in time are allowed here.",
                    "label": 1
                },
                {
                    "sent": "So in the case of an MDP you visit the state and you can occasionally visit the same state here.",
                    "label": 0
                },
                {
                    "sent": "This cannot happen because.",
                    "label": 0
                },
                {
                    "sent": "Basically because this is incremental.",
                    "label": 0
                },
                {
                    "sent": "So let's assume now that this tape consists of identical binary storage devices.",
                    "label": 1
                },
                {
                    "sent": "And let's think that setting a bit costs the same in each cell.",
                    "label": 1
                },
                {
                    "sent": "So setting a 01 doesn't matter has a certain cost, say 3 jewels immediately.",
                    "label": 0
                },
                {
                    "sent": "What we conclude from this is that each that the code word lengths are proxies for transformation costs.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "By assuming that the reason that this code word length are encoding an desirable or unexpected distribution from the environment, we can say that the code word length also have associated probabilities.",
                    "label": 0
                },
                {
                    "sent": "So something like a channel final quote.",
                    "label": 0
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "Think about this on the line model abstract model.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now let's move on and and try to characterize it from a measure theoretic point of view.",
                    "label": 0
                },
                {
                    "sent": "So on this side we have a treat that represents all the possible trajectories of dynamical system, so we can all these are all the possible realizations from a measure theoretic point of view.",
                    "label": 0
                },
                {
                    "sent": "We have a sample space where each sample corresponds to a full realization of the interaction sequence, and then, let's imagine that we have chosen 3X interactions have occurred.",
                    "label": 0
                },
                {
                    "sent": "This in measure space corresponds to conditioning the original and sample space into three slides.",
                    "label": 0
                },
                {
                    "sent": "So refining our knowledge about the situation, that the agent is in at the refining, the the information state.",
                    "label": 0
                },
                {
                    "sent": "So an information state from the point of view or measure theory is just a measurable set and the transformation is just a condition on the information state.",
                    "label": 1
                },
                {
                    "sent": "So given this image here, as you see, this sequential realizations here are models as filtrations as opposed to dynamic consistence where you have say a time operator.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then, based on this characterization, we state axioms of transformation costs and will not really go into the detail because there is many different ways of doing it, and you're well acquainted with it.",
                    "label": 0
                },
                {
                    "sent": "So we assume that this there is a real valued mapping between conditional probabilities and transformation costs which we write down with the role they're additive and monotonic, and you can show them then.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That transforming an information state B into an information state a corresponds simply to one over the inverse temperature.",
                    "label": 0
                },
                {
                    "sent": "This Alpha parameter here, which is a real value parameter times the information content of.",
                    "label": 0
                },
                {
                    "sent": "Of the set a condition zombie.",
                    "label": 0
                },
                {
                    "sent": "And and so this basic this primitive of transformation, 'cause we're going to use this now to construct.",
                    "label": 0
                },
                {
                    "sent": "Utility function.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So keep the following image now in your mind.",
                    "label": 0
                },
                {
                    "sent": "So let's start from from a distribution Q distribution over 2 possible outcomes.",
                    "label": 0
                },
                {
                    "sent": "I'll come #2 is is more probable than Alpha number one.",
                    "label": 0
                },
                {
                    "sent": "We're gonna transform it into new distribution P where now X one is more probable than X2.",
                    "label": 0
                },
                {
                    "sent": "To understand now what is the transformation costs that we're paying what we have to do?",
                    "label": 0
                },
                {
                    "sent": "We have to embed it into into.",
                    "label": 0
                },
                {
                    "sent": "Into the picture that I explained before.",
                    "label": 0
                },
                {
                    "sent": "So we embedded into probability space.",
                    "label": 0
                },
                {
                    "sent": "Here we can think of Q as being a conditional probability measure.",
                    "label": 0
                },
                {
                    "sent": "So Q is now asset in this embedding and P is then a subset.",
                    "label": 0
                },
                {
                    "sent": "And now we're going to compute the information that you need to compress or to reduce or to refine the original set Q into P. And as you can see this guy here correspond to the blue guy.",
                    "label": 0
                },
                {
                    "sent": "And this guy here corresponds to the right guy, so this allows us to think about the transformation cost.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to skip over this, so essentially with this.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instruction here what we achieve is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We we show that the transformation from Q into P corresponds to this function here, which is our well known free energy.",
                    "label": 0
                },
                {
                    "sent": "It's the negative to be correct.",
                    "label": 0
                },
                {
                    "sent": "It's the negative free energy difference.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the optimal solution to the negative free energy difference with the QR control costs is the Colibri distribution, or essentially an exponential family distribution given by this quantity.",
                    "label": 1
                },
                {
                    "sent": "Here, where that is now the normalizing constant.",
                    "label": 1
                },
                {
                    "sent": "Essentially the partition function, and if we take this solution and we plug it back into the negative free energy difference, we obtain the log partition function.",
                    "label": 0
                },
                {
                    "sent": "Here one over the inverse temperature temp salaak partition function now.",
                    "label": 0
                },
                {
                    "sent": "This last part is function is important because it represents the certainty equivalent.",
                    "label": 0
                },
                {
                    "sent": "OK, so when you have a random lottery, you want to understand what is the value of this lottery.",
                    "label": 0
                },
                {
                    "sent": "Normally in maximum expected in expected utility theory, you would just compute the expectation over a larger, but in this case what we are claiming is that this is the correct quantity and they will give you.",
                    "label": 0
                },
                {
                    "sent": "Try to elaborate on this further.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we take the negative free energy difference, the extremum and now we manipulate the parameter Alpha, the inverse temperature.",
                    "label": 1
                },
                {
                    "sent": "What we do is we produce this sigmoid here, where.",
                    "label": 0
                },
                {
                    "sent": "If we choose Alpha going to Infinity, what happens is that this lottery.",
                    "label": 0
                },
                {
                    "sent": "The value of this lottery is going to be the maximum in the set of outcomes.",
                    "label": 0
                },
                {
                    "sent": "If I choose Alpha, if Alpha tends to minus Infinity, we recover the minimum an if Alpha goes to 0, then we get the expectation.",
                    "label": 0
                },
                {
                    "sent": "So now if you think about it a little bit.",
                    "label": 0
                },
                {
                    "sent": "We start realizing that Alpha plays the role of how much you are on the control over the distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so the higher Alpha is, the more controlled you have upper over the outcome, the more, the more you can pick an outcome out of this lottery.",
                    "label": 0
                },
                {
                    "sent": "And if you have, if our zero you have no control, so you get the uncontrolled dynamics.",
                    "label": 0
                },
                {
                    "sent": "And if Alpha is negative then you are in this situation where you think you don't have control, but you think that another adverse aerial player is going to pick that.",
                    "label": 0
                },
                {
                    "sent": "Worst case scenario for you.",
                    "label": 0
                },
                {
                    "sent": "So this is keep this picture in mind, and now we're going to ask ourselves again.",
                    "label": 0
                },
                {
                    "sent": "What could be a possible operational interpretation of the inverse temperature parameter.",
                    "label": 0
                },
                {
                    "sent": "And we have.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Story, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not a water tight story, but it's very close.",
                    "label": 0
                },
                {
                    "sent": "So imagine an imagined we have an urn with different balls.",
                    "label": 0
                },
                {
                    "sent": "All these polls are numbered and what we're going to do now is we're going to draw balls from the urn, and we're going to look at the numbers we're going to keep going to kind of keep the maximum.",
                    "label": 0
                },
                {
                    "sent": "Obviously, if you do that, an infinite amount of times, then you gonna pick up the maximum you're going to know the maximum discern for sure.",
                    "label": 1
                },
                {
                    "sent": "You got, yeah OK, you're going to know the maximum discern for sure, but if you take a finite amount of samples then the maximum over this finite sample is going to be obviously number part.",
                    "label": 0
                },
                {
                    "sent": "It's going to be essentially a distribution over the possible maximum.",
                    "label": 1
                },
                {
                    "sent": "And what we have shown is that.",
                    "label": 0
                },
                {
                    "sent": "There collibra distribution that there are the solution to the negative free energy difference and the distribution of this maximum.",
                    "label": 1
                },
                {
                    "sent": "After drawing Alpha samples is upper bounded by this exponentially decreasing value work?",
                    "label": 0
                },
                {
                    "sent": "Cyan, Delta are constants.",
                    "label": 1
                },
                {
                    "sent": "So essentially the take home message here is that Alpha can also be interpreted as iterations of the search algorithm or number of samples you draw from this Earth.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think now let's consider sequential decision problems.",
                    "label": 0
                },
                {
                    "sent": "So sequential decision problems are usually stay decision trees and solved using backwards induction so far.",
                    "label": 1
                },
                {
                    "sent": "So you have several scenarios here.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you interact with the stochastic environment like with a stochastic plant, then the algorithm you use is expected mix where essentially have this decision tree.",
                    "label": 0
                },
                {
                    "sent": "You have different nodes around nodes correspond to the expectation operator.",
                    "label": 0
                },
                {
                    "sent": "So you take these two guys, compute expectation, you write down the number.",
                    "label": 0
                },
                {
                    "sent": "Here you sit here as well, and when you have this triangle here.",
                    "label": 0
                },
                {
                    "sent": "So the Max operator.",
                    "label": 0
                },
                {
                    "sent": "So you compute the maximum of these two guys, right?",
                    "label": 0
                },
                {
                    "sent": "The number here and so you solve the decision tree and then you know by following the optimal path you know the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "OK, if you're playing with and with an adversarial player, then you replace this expectation nodes with min nodes.",
                    "label": 0
                },
                {
                    "sent": "OK so you repeat the same and if you playing something like backgammon then you have mixed notes here.",
                    "label": 0
                },
                {
                    "sent": "So the question is the problem here.",
                    "label": 1
                },
                {
                    "sent": "Is that decision rules depend on the kind of systems.",
                    "label": 1
                },
                {
                    "sent": "It could be stochastic, cooperative, competitive.",
                    "label": 0
                },
                {
                    "sent": "Hybrid and so forth, but this intuitive distinction between the different types of systems is formally unsatisfactory, and so decision rules can be what we want to show that this decision rules can be re expressed in a unified way using the free energy functional here, but the big question here is how do we introduce node specific temperatures and?",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In other words, So what we want to come up with this notion of generalized decision trees.",
                    "label": 0
                },
                {
                    "sent": "This tree here such that placing different temperatures parameters.",
                    "label": 0
                },
                {
                    "sent": "Here we recovered the different types of decision trees.",
                    "label": 0
                },
                {
                    "sent": "So in other words, the different operators expressed different degrees of control as we mentioned before, Max is full control, expectation is no control and so forth.",
                    "label": 1
                },
                {
                    "sent": "The goal is to find the generalized operator that expresses the three classical degrees of control plus all the other degrees of control in between.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have a.",
                    "label": 0
                },
                {
                    "sent": "We have a theorem that explains when you keep the reference, the uncontrolled dynamics, Q if you keep it fixed, the equilibrium dynamics if you keep it fixed and you change the temperature, how do you have to change your corresponding utility criterion to reflect this change of temperature?",
                    "label": 1
                },
                {
                    "sent": "And with this, with this theorem, we are now able to.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start from the initial scenario where we have a one step decision.",
                    "label": 0
                },
                {
                    "sent": "One step decision problem.",
                    "label": 0
                },
                {
                    "sent": "If we interpret one outcome as being a sequence then we can re express this guy as this decision tree here with homogeneous temperatures and if we now use the theorem to change the individual temperatures of each node then we can construct this generalized trees where we can then implement all these other decision rules and I think I'm going to stop here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can also see later that this allows us to construct generalized optimality equations.",
                    "label": 0
                },
                {
                    "sent": "I will not go into the detail.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, OK. Would you like to carry on?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as he said, decision-making is usually thought of in terms of lotteries where you pick a lottery with the highest expectation value.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This idea is also been applied to movement control, so think about for example in aiming point of William Tell can be thought of as a lottery where you have lots of different outcomes where the arrows actually landing given by the probabilities that arise through the intrinsic noise of the motor system.",
                    "label": 0
                },
                {
                    "sent": "And of course where the arrow Lance is also implies very different costs in this case.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this doesn't only work for aiming task for any kind of movement, you can think of.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where you can use ideas from optimal control, but the issue with optimal control is it's not only difficult to compute, but you also requires you all aware precise models of the environment and what happens if you don't have this model.",
                    "label": 0
                },
                {
                    "sent": "And here you can also apply this free energy ideas.",
                    "label": 0
                },
                {
                    "sent": "When we tested this in Excel.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we take this free energy idea this Petro set.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We now say you value this lottery as with the free energy.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that means you can also give an interpretation to this distributions is equilibrium distributions that arise and corresp.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once these free energies, so if you have an action, if you have an action variable, you can think about this P0 as your default policy and the Alpha as a measure of your boundedness that tells you then how much you can optimize the utility.",
                    "label": 0
                },
                {
                    "sent": "Or you can think about an observation.",
                    "label": 0
                },
                {
                    "sent": "Then this Pi is expressing a belief where the PO is your model that you assume initially and you're in the Alpha indicates something like the rationality parameter of your environment that you assume, or you can think about it also as a model uncertainty or ambiguity parameter.",
                    "label": 0
                },
                {
                    "sent": "Which allows you to deviate from from your model.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this free energy formula can also be approximated through a mean variance tradeoff, which is known in finance as the main website from markovits.",
                    "label": 0
                },
                {
                    "sent": "And it makes this.",
                    "label": 0
                },
                {
                    "sent": "It makes this temperature parameter pretty intuitive because you can see that this variability is added as a bonus by somebody who's risk seeking.",
                    "label": 0
                },
                {
                    "sent": "And is considered as a minus by somebody who is risk averse, and this is interesting because in this case a risk seeking person can be considered as a person that considers its environment as an extension of themselves and environment is helpful where the noise is working in his advantage, whereas in the case of a risk seeking risk averse person, it's the other way around where the environment is perceived as well adversarial.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now here just going to mention quickly two studies where we tested ideas of this actually with human doing experiments in the laboratory in virtual reality one is a study where we looked for mean variance trade.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Off",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this study, subjects had to do a two step task in each trial.",
                    "label": 0
                },
                {
                    "sent": "They had to decide whether in the first step to do a hitting task, whether to reach to the left, where they were hitting a target for sure, or whether they were moving to the right and try to hit a very small target, and then in the second step they had to perform an effort so they had to move to one of these circles and the higher the circle is up, the more they had to exert some effort which was unpleasant.",
                    "label": 0
                },
                {
                    "sent": "So they wanted to.",
                    "label": 0
                },
                {
                    "sent": "Always move to the lower circle.",
                    "label": 0
                },
                {
                    "sent": "Now if they were going to the left they would always have to move to this effort circle, which was always on the same spot.",
                    "label": 0
                },
                {
                    "sent": "If they were moving to the right and were able to hit the target, they could move to the nice effort circle.",
                    "label": 0
                },
                {
                    "sent": "If they missed the target, they had to move to the bad one.",
                    "label": 0
                },
                {
                    "sent": "Now what we could do is by manipulating the size of this target and the position of these two efforts circles, we could create many different lotteries where the variability was the same.",
                    "label": 0
                },
                {
                    "sent": "So we had created five different classes of their abilities with very different mean values.",
                    "label": 0
                },
                {
                    "sent": "And what we could do then is we could see whether the change of the variability subjects would change the point where they are indifferent between these two options.",
                    "label": 0
                },
                {
                    "sent": "And of course, if they would not be a sensitive to the variability.",
                    "label": 0
                },
                {
                    "sent": "But I only care about the mean of their for their exerting.",
                    "label": 0
                },
                {
                    "sent": "Then this five ability classes shouldn't make any difference.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have a simple prediction.",
                    "label": 0
                },
                {
                    "sent": "We have the certain bats which was always on the left at the same spot and if they were risk sensitive and would exhibit this mean variance tradeoff, we have a prediction of this and we would predict indifferent indifference points that are basically linear functions.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is exactly what we found.",
                    "label": 0
                },
                {
                    "sent": "So each plot here is a different subject that did the experiment on the X axis you have the five different variability levels that we tested, and the dashed lines indicates the expected behavior if they were not sensitive to the variability.",
                    "label": 0
                },
                {
                    "sent": "But you can see there all sensitive.",
                    "label": 0
                },
                {
                    "sent": "Most of them were sensitive to their ability, which means that at this variability level they were prepared to go for the risky lottery on the right side.",
                    "label": 0
                },
                {
                    "sent": "Even though the mean was much higher and that means they were overly confident to hit this target that they interpreted their own motor noise as well.",
                    "label": 0
                },
                {
                    "sent": "They thought they were better actually are.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to get.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip this, then here's a.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another experiment, another three slides, and I'm done.",
                    "label": 0
                },
                {
                    "sent": "Where there were a lot of studies that have shown that human sensorimotor behavior is consistent with Bayesian integration.",
                    "label": 0
                },
                {
                    "sent": "So here we did a task where also we studied based on integration, but we were looking for deviations from the probabilistic model it subjects might have.",
                    "label": 0
                },
                {
                    "sent": "So in this task, subjects did forward and backward movement and particularly they had to move through a target area.",
                    "label": 0
                },
                {
                    "sent": "Now in the target area, a target at the beginning of the trial appeared.",
                    "label": 0
                },
                {
                    "sent": "This target was drawn from a Gaussian distribution an they saw this target at the beginning of the trial, but under three different feedback conditions they could see exactly where the target is in one trial.",
                    "label": 0
                },
                {
                    "sent": "In another trial, they could only see a cloud, a Gaussian cloud giving them only noisy feedback, or they could receive no feedback at all, so they could only rely on their prior information.",
                    "label": 0
                },
                {
                    "sent": "So this is an experiment how it was done.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Firstly, before and the intuition behind it is this that this is.",
                    "label": 0
                },
                {
                    "sent": "Imagine this is the distribution where the target is strong, the prior distribution that you can learn over many trials and then you receive noisy feedback.",
                    "label": 0
                },
                {
                    "sent": "Now here are basically likelihood functions for different feedbacks with different kinds of precision.",
                    "label": 0
                },
                {
                    "sent": "So it's very sharp.",
                    "label": 0
                },
                {
                    "sent": "You see exactly where the target is, say if it's very broad, then you have a very vague idea of where the target is and then the idea is if you do Bayesian inference then you combine this information and if you're.",
                    "label": 0
                },
                {
                    "sent": "Observation is very uncertain.",
                    "label": 0
                },
                {
                    "sent": "Then you biasing more towards your prior.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you can rely if your information for observation so reliable you more or less going to ignore the prior and you control a picture like this where you say OK.",
                    "label": 0
                },
                {
                    "sent": "If I assume the true position is here and I'm going to move exactly here and I draw on this axis the error that I'm going to make with respect to the true position, then I'm going to.",
                    "label": 0
                },
                {
                    "sent": "In the ideal case be exactly here with 0 error.",
                    "label": 0
                },
                {
                    "sent": "Whereas if I just rely on my prior, I get the plot like this where I see a slope.",
                    "label": 0
                },
                {
                    "sent": "So this kind of experiment has been done before.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we were also.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Able to reproduce this kind of.",
                    "label": 0
                },
                {
                    "sent": "Biasing towards your prior depending on the uncertainty level that you have.",
                    "label": 0
                },
                {
                    "sent": "So this is this three plots.",
                    "label": 0
                },
                {
                    "sent": "But now we did some.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hang out, we introduced in this false area of force that could either be absent, in which case we reproduce previous findings that show that you do basin integration and we introduced forces that could either go this way up, or this way down.",
                    "label": 0
                },
                {
                    "sent": "So that means in one case movement in this direction were cheaper or in this direction they were cheap now.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In these conditions we have two different predictions.",
                    "label": 0
                },
                {
                    "sent": "One is the risk neutral prediction, which is just basically so we ignore.",
                    "label": 0
                },
                {
                    "sent": "This is just a constant term that we can ignore, which is basically just the Bayesian prediction, which is the change of slope that we've seen in these pictures.",
                    "label": 0
                },
                {
                    "sent": "Or we have a risk sensitive predictor that allows deviations from the Bayesian model, and in this case we get actually two terms.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In and it is this second term that we've been studying, so in this term you allow deviations from the Bayesian estimator.",
                    "label": 0
                },
                {
                    "sent": "Towards the cheaper movement an, but the important thing is this deviation scales with the amount of uncertainty that you have.",
                    "label": 0
                },
                {
                    "sent": "So you see here these lines that are offset from the Bayesian line.",
                    "label": 0
                },
                {
                    "sent": "An important thing is that the more you off, the more you're deviating, and what it means is that if subjects were not really aware where the target was, they biased their beliefs towards believing that well.",
                    "label": 0
                },
                {
                    "sent": "If I can see it, maybe it's not, it's in the not so costly, which is a bit like.",
                    "label": 0
                },
                {
                    "sent": "Wishful thinking and effect of risk sensitive.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'll stop there, thanks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}