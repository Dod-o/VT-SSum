{
    "id": "6hkml6aqems65yn46utiegrtihz3zb6v",
    "title": "Autonomous Exploration For Navigating In MDPs",
    "info": {
        "author": [
            "Peter Auer, Chair for Information Technology, Montanuniversit\u00e4t Leoben"
        ],
        "published": "Aug. 6, 2013",
        "recorded": "April 2013",
        "category": [
            "Top->Computer Science"
        ]
    },
    "url": "http://videolectures.net/machine_auer_autonomous_exploration/",
    "segmentation": [
        [
            "Hello, I'm Peter, all from the University in the open Austria and I'm going to tell you about our work on autonomous learning, meaning that learning without immediate goal.",
            "And this is funded by the skull and the complex project, so please."
        ],
        [
            "So the motivation is that humans and animals apparently exhibit this kind of learning, where they explore the environment without immediate goal, and we want to have to come up with a model which gives us some some handle on analyzing such behavior.",
            "So this is a very particular model, but if you will see that the algorithm actually shows some some interesting properties which.",
            "So our setup is that we should be able to do some analysis.",
            "We assume that we have in a Markov decision process without rewards and reset to a safe state like lying down.",
            "If you want to learn to work and.",
            "The goal is to find policies, strategies to go to all possible states which are reachable in a certain vicinity of this neutral state.",
            "It turns out that formula this is too ambitious, so because there might be states which are reachable only by going through intermediate states which are not really explore abtl or reachable.",
            "So we need to make an additional assumption which says that these states can actually be explored incrementally.",
            "So which means if you if you're."
        ],
        [
            "You can go to a status I go by using only policies defined on previous states, so this allows for an incremental learning and in this case we come up with an algorithm which is called UCB Explorer and which shows and which generates policies to all these states in a reasonable time and the next slide shows you this algorithm.",
            "It has in each."
        ],
        [
            "Patient has three steps, so first you this so it maintains a set of known states for which you already know how to go there.",
            "Then you look at all the neighboring states of these known states.",
            "From these, you pick the one which is each, which is the easiest to reach.",
            "You say assuming an optimistic model of your environment modeler.",
            "The experience you have already made, so you pick this easiest state.",
            "Try to go there calculated policy to go there, and then there's this evaluation phase where we actually try.",
            "Does this policy indeed work?",
            "If it works well, you have and you known state.",
            "If it doesn't work, you have learned something about your environment because you know you're optimistic assumptions were wrong, and so this.",
            "Algorithm has three nice properties, so it just is incremental learning.",
            "It has this, it sees only a small part of the environment and incrementally increases this.",
            "This known part it selects for the next step.",
            "The most promising goal which is most promising to be extremely reachable.",
            "And it does a testing of its current knowledge by trying this policy.",
            "If it actually works, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, I'm Peter, all from the University in the open Austria and I'm going to tell you about our work on autonomous learning, meaning that learning without immediate goal.",
                    "label": 0
                },
                {
                    "sent": "And this is funded by the skull and the complex project, so please.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the motivation is that humans and animals apparently exhibit this kind of learning, where they explore the environment without immediate goal, and we want to have to come up with a model which gives us some some handle on analyzing such behavior.",
                    "label": 0
                },
                {
                    "sent": "So this is a very particular model, but if you will see that the algorithm actually shows some some interesting properties which.",
                    "label": 0
                },
                {
                    "sent": "So our setup is that we should be able to do some analysis.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have in a Markov decision process without rewards and reset to a safe state like lying down.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn to work and.",
                    "label": 0
                },
                {
                    "sent": "The goal is to find policies, strategies to go to all possible states which are reachable in a certain vicinity of this neutral state.",
                    "label": 0
                },
                {
                    "sent": "It turns out that formula this is too ambitious, so because there might be states which are reachable only by going through intermediate states which are not really explore abtl or reachable.",
                    "label": 0
                },
                {
                    "sent": "So we need to make an additional assumption which says that these states can actually be explored incrementally.",
                    "label": 0
                },
                {
                    "sent": "So which means if you if you're.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can go to a status I go by using only policies defined on previous states, so this allows for an incremental learning and in this case we come up with an algorithm which is called UCB Explorer and which shows and which generates policies to all these states in a reasonable time and the next slide shows you this algorithm.",
                    "label": 0
                },
                {
                    "sent": "It has in each.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patient has three steps, so first you this so it maintains a set of known states for which you already know how to go there.",
                    "label": 1
                },
                {
                    "sent": "Then you look at all the neighboring states of these known states.",
                    "label": 0
                },
                {
                    "sent": "From these, you pick the one which is each, which is the easiest to reach.",
                    "label": 0
                },
                {
                    "sent": "You say assuming an optimistic model of your environment modeler.",
                    "label": 0
                },
                {
                    "sent": "The experience you have already made, so you pick this easiest state.",
                    "label": 0
                },
                {
                    "sent": "Try to go there calculated policy to go there, and then there's this evaluation phase where we actually try.",
                    "label": 0
                },
                {
                    "sent": "Does this policy indeed work?",
                    "label": 0
                },
                {
                    "sent": "If it works well, you have and you known state.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't work, you have learned something about your environment because you know you're optimistic assumptions were wrong, and so this.",
                    "label": 0
                },
                {
                    "sent": "Algorithm has three nice properties, so it just is incremental learning.",
                    "label": 0
                },
                {
                    "sent": "It has this, it sees only a small part of the environment and incrementally increases this.",
                    "label": 1
                },
                {
                    "sent": "This known part it selects for the next step.",
                    "label": 0
                },
                {
                    "sent": "The most promising goal which is most promising to be extremely reachable.",
                    "label": 0
                },
                {
                    "sent": "And it does a testing of its current knowledge by trying this policy.",
                    "label": 0
                },
                {
                    "sent": "If it actually works, thank you.",
                    "label": 0
                }
            ]
        }
    }
}