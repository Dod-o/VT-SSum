{
    "id": "4vdzc5uqra5kk5yzyyghrwh6cu65feso",
    "title": "Combining Distributional and Logical Semantics",
    "info": {
        "author": [
            "Mike Lewis, School of Informatics, University of Edinburgh"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_lewis_semantics/",
    "segmentation": [
        [
            "This is going to be about trying to combine traditional formal Montego being semantics with modern distributional semantics.",
            "I'm going to start out with a very quick introduction to distributional and formal semantics and trying to explain why I think it would be a good idea to combine them."
        ],
        [
            "OK, for a motivating example for this talk, Imagine we've got some sort of web based question answering system and we ask it was Obama born in Kenya?",
            "And Wikipedia's got the sentence saying that abandons birthplace isn't Kenya and obvious.",
            "Obviously we want our system to be able to answer this as new, But this turns out to be surprisingly difficult."
        ],
        [
            "First of all, I'm going to describe our distributional semantics.",
            "So most purchased the semantics try to map natural language onto some predefined ontology predicates.",
            "There's a problem with that in that it seems unlikely we'll ever build an ontology that captures every meaning.",
            "Anything could possibly say.",
            "Instead, distributional semantic seems to induce the meaning of words from text.",
            "One simple approach based on linen Pant elf to this question would be to sort of extract this with X was born in white person and X is birthplace is why person from the question and answer.",
            "Then we can look at some large corpus for all the X&Y pairs that instantiate these patterns."
        ],
        [
            "And then we can compute the similarity of those lists of XY pairs, and if they are greater than threshold will allow the inference to go through."
        ],
        [
            "But if we try this on our running example, actually gives us complete the wrong answer.",
            "For that is that within this approach is not compositional, so you can't combine the meaning for the word, not with the rest of the meaning of the words in the sentence.",
            "To understand that the Wikipedia sentence is indicated."
        ],
        [
            "A different approach to semantics, or the formal Montego Bay and semantics which I'm illustrating here with CCG.",
            "In CCG will start out with the syntactic parse we can get from a CCG parser.",
            "And then we can, based on the words and their CCG categories we can.",
            "Automatically create lexical entries for these which capture the meaning of the words.",
            "I don't see if you get the meanings of the words right.",
            "Then it's simple to compute the meaning of the whole sentence.",
            "So the problem with one sentence sentence is just the problem with understanding the individual words.",
            "Anne.",
            "What's nice about this is that if we change sentence a bit about wasn't born in Kenya, then we can introduce lexical entry for indication, which for not which captures that it's negating the predicate and then.",
            "This combines to give meaning for the whole sentence with a negation logical form."
        ],
        [
            "You can turn these logical forms to theorem prover and it will tell you that Obama wasn't born in Kenya, means the opposite thing to about being born in Kenya.",
            "You can do lots of other powerful stuff."
        ],
        [
            "This kind of thing you can also do an 1st order inference fairly easily with a bit of background knowledge.",
            "You can tell that if every US President was born in the United States, then Obama can't be born in Kenya.",
            "If you know that members of president in Kenya isn't in the United States."
        ],
        [
            "It's also very good for abstracting away from variations in the syntax, so to a dependency parser.",
            "These two sentences look very different, but to CCG, you basically get exactly the same logical form for each of these for free."
        ],
        [
            "But if we try this on our running example, it's going to say that it doesn't know the answer."
        ],
        [
            "And the reason for that is logical form for the premise has this sort of birth place symbol in it, and the logical form for the question has this born in symbol in it.",
            "And the theorem provers got no way of knowing the relation between these symbols.",
            "So basically being able to understand that you indicated one of these symbols isn't that much use.",
            "If you don't really have a good model of what your negating is.",
            "For this reason, approaches based on formal semantics have shown very low recall on actual practical applications."
        ],
        [
            "So for quick mini summary here.",
            "So the formal semantics is great for modeling function words like knots or every and it's great for modeling lots of common constructions like conjunctions or relative clauses or passives.",
            "But the big problem with it is that it doesn't really have a good model of what content words mean.",
            "Which is actually the one thing that distributional semantics is really good for, so I think that's quite powerful motivation for trying to combine these frameworks."
        ],
        [
            "Certainly not the first person to think this.",
            "There's been a couple of recent trends of work on this.",
            "Firstly, in compositional vector space models, there are lots of different papers I could have cited here these.",
            "But they still have some issues.",
            "I mean, it's not at all clear how you do first order inference with vectors C. The other strands work happens.",
            "Guys Texas involves basically doing standard formal semantics, but then.",
            "Also adding some inference rules, for example, a rule that says born in implies birthplace or something.",
            "And they learn that with distributional statistics.",
            "I think that's really cool, but.",
            "It needs another number of inference rules that grows quadratically in the size of your vocabulary, which could make it difficult to do at large scale.",
            "I'm going to be proposing something a bit different to these."
        ],
        [
            "So going back to the formal semantics, we said.",
            "Problem is that these two equivalent sentence is about Hawaii being born in Alabama have quite different looking logical forms.",
            "Intuitively we want sentences that mean the same thing to have the same semantics.",
            "So we're going to want to try and produce representations.",
            "Look close to this."
        ],
        [
            "This relation 53 is sort of an abstract between relation identifier for the concept of being born in the place."
        ],
        [
            "I'm in CCG terms.",
            "This amounts to learning lexical entries.",
            "So we want to learn that sort of lexical entry for X was born in Y&X is birthplace is why both have the same semantics."
        ],
        [
            "If you do that, you can just plug these lexical entries into the start of the derivation and the correct logical form will fall out of bottom.",
            "So where these lexical entries come from?"
        ],
        [
            "Well, we can do use the techniques of distributional semantics.",
            "What we're going to do is run a standard CCG semantics approach over a large corpus.",
            "We're going to look for everything that's in a born in relation and everything that's in a birthplace relation.",
            "Hopefully we'll get fairly similar lists of people and places."
        ],
        [
            "You can imagine plotting all the predicates we do.",
            "We get in our corpus and some high dimensional space where born in recently birthplace."
        ],
        [
            "We can then do clustering on these."
        ],
        [
            "And what we're going to do is map each of these clusters onto its own relation symbol.",
            "So because we turn these distributional statistics into a symbol, we can then negate it.",
            "We can quantify.",
            "It will take conjunctions or whatever else you can do with formal semantics.",
            "We're doing the clustering with Chinese whispers algorithm, which is very scalable and it's nonparametric, which means that we don't need to specify how many different relation types we think there are in language.",
            "With that, you just fall out of the data."
        ],
        [
            "And if we do this, then we've got something that can finally answer our running example.",
            "Um?"
        ],
        [
            "Now this kind of works.",
            "One big problem with it is that it doesn't deal with ambiguity as I've described so far, and obviously ambiguity is a huge problem.",
            "If we take these two sentences, Obama was born in Hawaii and Obama was born in 1961.",
            "This seems to be expressing different relations."
        ],
        [
            "We certainly don't want to make an inference like Obama was born in.",
            "Hawaii implies that he was born in 1961."
        ],
        [
            "The solution is going to follow some recent trends at work, and we're going to convert our born in relation, which seems to express two different things into two different lexical entries which are distinguished by the types of their arguments.",
            "So we have one born in relation between people, locations and one born in relation between people and dates.",
            "These birthplace and birthdate are just really relation 53 in relation 91.",
            "I've just written out in English for clarity."
        ],
        [
            "And then what we can do is we can just throw these two different boarding predicates into our clustering algorithm to cluster them separately, so that one definition of born in Willette birthdate and one will look like birthplace."
        ],
        [
            "Specs question of where these types going to come from.",
            "We're going to.",
            "For that example, you could get away with standard named entity types, but that won't be general enough for what we need.",
            "Instead, we're going to induce a probabilistic model of these.",
            "We're going to use a simple topic model for doing this.",
            "What we're going to do is for every document, for every predicate in the corpus, we're going to create a sort of document for it, and the words in the documents are going to be the arguments of the predicate.",
            "So we'll have a lives next document with them.",
            "A list of places a year of X document will list of dates and aborning next document that's a mixture of places and dates."
        ],
        [
            "You get through this LDA and it will do a pretty good job of learning a place type of date type and the arguments are born in it like to be a mixture of places and dates.",
            "If this looks similar to that or element gave yesterday, that's because it is.",
            "It seems we came up with the same solution to this problem independently, but concurrently.",
            "Anne."
        ],
        [
            "Here are some.",
            "Types we learned from this, which seem to have a fairly clear semantic interpretation.",
            "OK."
        ],
        [
            "So the output from the LDA is going to be distributions connecting predicates with types and connecting arguments with types and to get the true posterior over the types we need to combine these during our derivation.",
            "So for example, born in 2001, the left hand arguments are born in is probably equally likely to the right hand argument is equally likely to be a location or a date.",
            "And 2001's probably date, but could be a film in some contexts or a book."
        ],
        [
            "Using the LDA, we can assign distributions to the variables and constants in the lexicon.",
            "And then during the derivation we combine these nodes.",
            "The Y variable gets beta reduced with 2001 and when that happens we combine the type distributions and we find that in this context that born in 2001 unambiguously refers to a date."
        ],
        [
            "Similarly, for born in Washington, Washington is probably a place, but it's possible that it could be.",
            "George Washington in some Contacts.",
            "But"
        ],
        [
            "When we do the.",
            "Find these during the derivation.",
            "It's unambiguous, that's.",
            "Boarding Washington refers to a location."
        ],
        [
            "There's one final subtlety with this.",
            "So I motivated the idea of adding different types to words so that we can disambiguate different meanings based on them having different types of arguments.",
            "I've shown how we can build a probabilistic distribution there.",
            "What the types of the arguments are as part of the CCG derivation but in CCG we need to choose a lexical entry for born in before we've actually seen what is arguments are.",
            "So we need to choose one of these born entries before we know whether it's between places and dates of people in dates or places and people.",
            "Solution to this is going to be to collapse these two different lexical entries into what I'm calling a packed lexical entry.",
            "Anne.",
            "So this is a function from the semantics error function from argument types onto semantic relations.",
            "This is saying that born in if it's given.",
            "Personal location arguments will be birthplace and with personal data arguments will be.",
            "A birth date relation."
        ],
        [
            "If you run this over your whole derivation, then the output is going to be a packed logical form.",
            "This logical form catches all the distribution over meetings in the sentence according to our model of amputee.",
            "So for example, for Obama was born in Hawaii in 1961, we get the most likely interpretation is that his birthplace is why in his birthday since 1961, but we do reserve some probability mass for other possible interpretations."
        ],
        [
            "OK and just details on how we trained this.",
            "Then we use the English Giga word corpus which we passed with the CNC syntactic parser which is standard CCG parser.",
            "We changed the LDA model with 15 entity types.",
            "And we wrote down a few like school entries for closed class function words like every and not.",
            "OK, now we need to evaluate this.",
            "We're going to do 2 evaluations.",
            "Firstly model evaluating model of lexical semantics on the question, answering task, and then we're going to model.",
            "Sorry, evaluate our model of formal semantics."
        ],
        [
            "For the question answering task, we're going to generate questions at from corpus very similarly to the prudent Domingos.",
            "Unsupervised semantic parsing evaluation.",
            "We can make questions by finding sentence in the corpus and removing individual arguments so we can from sentence.",
            "Google bought YouTube we can make questions like what, what YouTube and what did Google Play.",
            "I quite like this approach to generating questions because it means we're evaluating relations in proportion to their frequency in the corpus.",
            "So understanding more common relations is more important than understanding revelations.",
            "We're going to generate thousands of these, and we're going to give the answers to people to evaluate.",
            "The answers don't have to match the original answers from the question.",
            "I mean if it if we get sentence Google bought Motorola, then we can validly answer.",
            "What did Google buy with Motorola?",
            "According to human annotators?"
        ],
        [
            "Compare that sprawling baselines and the re verb system relation LDA, which is an LDA based model for relation clustering.",
            "A baseline CCG system, which is just a standard CCG approach.",
            "Then we're going to add a word net derived inference rules to that.",
            "I finally have a system using distributional clusters, which is our full model."
        ],
        [
            "Here are the results.",
            "So the blue line here is our clustering based system.",
            "So with our model we can rank the answers in order of probability.",
            "After you marginalized out the ambiguity in the logical form.",
            "I can see the base and then you can work out what the accuracy is for the top answers and basically for any value of N we up from all our baselines and we also returned a very large number of answers that none of the other systems can return.",
            "But still, it's pretty good accuracy.",
            "The CCG baseline systems are horizontal lines because we can't order the answers by probability because they're deterministic.",
            "I also haven't put the relationality a system on this graph.",
            "That's because it returns very large number of very low precision answers.",
            "The reason for that, it seems to be that it's parametric clustering model and you have to sign every relation the corpus to one of 100 different relation types because there's a lot more than 100 different relations in the corpus.",
            "That means it generates some very large noisy clusters which can answer almost any question from.",
            "I think that's not to be using nonparametric clustering.",
            "If you want to do this kind of question answering task."
        ],
        [
            "Also, here's an example question we got right where we learned that two companies merging with each other means the same thing as one acquiring the other."
        ],
        [
            "We're also going to evaluate systems model of formal semantics to see if we can deal with function words.",
            "Well, we're going to do this on a data set called the Fracas suite, which was basically hand built by linguists to be difficult for computers.",
            "You've got quite complex multi sentence inferences involving quantifiers where.",
            "To actually make the conclusion there, you need to fully interpret all the sentences and be able to combine their meanings.",
            "I think this kind of example would be very difficult to do using vector space models, but with our approach it's basically as simple as writing down electrical entry for every and then handing our semantics to a theorem prover."
        ],
        [
            "Here the results.",
            "The only other previous work on this is the natural logic work by McCartney Manning, but even that isn't able to combine the meanings across sentences, so it's only able to attempt a subset of the data set where there's only a single premise quest sentence.",
            "And we can see that we do pretty well on this.",
            "I should say that I'm almost all the areas we got on this where recall errors or other precision errors, so incorrectly predicting that we didn't know the answer rather than saying yes or no when that was incorrect.",
            "So that's yes, you can integrate this kind of thing into applications without actually hurting your precision."
        ],
        [
            "OK, so I tried to argue that formal and distributional semantics have quite complementary strengths and weaknesses, and suggesting that we can get the best both worlds if we start modeling our content words using cluster identifiers learned from distributional statistics."
        ],
        [
            "Thank you very much questions.",
            "Yeah, thanks for a great talk.",
            "I'm really I like the model a lot too.",
            "Very sympathetic.",
            "I had a question about so you you're talking bout entailment a lot here and that's more living in the logical world, but have you thought about how you could bring the distributional similarities to do something there, but one of the big problems when we try to do entailment with distributional representations?",
            "Obviously is that the distance metrics are symmetric, so you don't have.",
            "You know one directional thing, right, right?",
            "So you know Greek to human, but your clustering approach it like I like that you don't need to think about how many concepts you're having in your language and all that, and have you thought about how you could break the symmetries and get this sort of entailment?",
            "Rather than just just doing the softening of the logic, which is cool by having several clusters per word, so you're asking if I can get entailment directions in, so if say.",
            "In conquering implies invading, but invading doesn't imply conquering that, yeah, because here you can set your softening.",
            "The fact that born in Hawaii that these different symbols in your logic actually refer to the same predicate, right?",
            "Yes, you do have to say this is a very good point.",
            "You're quite right.",
            "The solution to this is to have multiple symbols predicates.",
            "I think the right way to do it would be to join.",
            "Parent's got some nice work on learning these sort of unidirectional entailment rules.",
            "I think you could apply that directly to this system.",
            "And then you could use all the clusters that are word as in as its logical form and that will give you directions.",
            "Sorry, entailments in just One Direction, cool.",
            "Yeah, thanks.",
            "It's really very impressive work.",
            "One thing that I'm a bit puzzled about is which is a hard clustering approach.",
            "You're going to really throw away any notion of similarity that you can get from distributional similarity.",
            "So say either liking and loving are mapped to relation 53, and there exactly the same or not.",
            "And then there is different as loving and hating, right?",
            "Yeah, so I think that would be addressed by the answer I gave to at that where you sort of trying to learn that loving is a special case of liking.",
            "That kind of thing.",
            "Does that so you get some hierarchical clustering there.",
            "I am so how exactly did you obtain the the set of questions that you were answering you?",
            "You find just take random sentences from yeah, so we just sample these from a different corpus.",
            "OK, I'm just.",
            "A few simple patterns for identifying statements that might be useful questions and then just drop from the arguments from there, not questions based on just any arbitrary sentence in the in the corpus.",
            "Sorry, they're not questions based on just any arbitrary so well, I mean.",
            "The arbitrary relations.",
            "They're not.",
            "They're fairly simple question constructions, but I mean there.",
            "Hi, I'm so it's nice work but I wanted to follow up on that question of losing similarity because I actually think that's a big deal.",
            "That and it wasn't something that you mentioned as the when you had your one advantage of distribution methods.",
            "It didn't mention similarity and I I think it's not as simple as the answer you gave 'cause the reality is that lots of words have relationships so that they're sort of similar, but they're not just entailment relationship, so you know, I can describe someone at work.",
            "As busy overwhelmed not coping stressed, and all of those are a bit different, you don't want to really say that any of them imply the other, but you know in a lot of contexts they're kind of giving the same impression, and it's good to be able to treat them as very related.",
            "So I think there's more to do there.",
            "I think you're right.",
            "Yeah, I think I mean, maybe you could learn it in the context of workers in the office, that they do mean basically the same thing, and in other contexts they don't mean the same thing, and you could try and capture that with this kind of model, but.",
            "I do take your point that there seems to be something more to it than that.",
            "I have a question about the lexical representation of born and.",
            "So first of all just a clarification.",
            "So you had born in as a whole predicate, correct?",
            "So there's no compositional semantics in that case with the preposition in is that is that right?",
            "No, that's not that.",
            "This is something I simplified the slide just to make the derivation actually fit on.",
            "So there's a.",
            "The lexical entry for born have a prepositional argument, which is subcategorize is with the word in.",
            "And then the word in is just treated as semantically transparent case marker on the Now, so that doesn't actually contribute to.",
            "In doesn't construct semantics.",
            "I just is an identity function.",
            "Yeah, OK, I mean, I think that actually makes a lot of sense, because you see that these two relations are lexicalized differently in different languages, so yeah, actually, that makes me really happy.",
            "Any further questions we have time for one more.",
            "Just following up on that one in has got meaning and a basic physical meaning and then going from 2D to 3D to spatio temporal and then abstract and action and so on.",
            "Yeah, you're quite right.",
            "So it has a lot of meanings if you just yes, you can use enter in all sorts of different things.",
            "And ideally, what we'd be able to do is distinguish between went in is like a core arguments the predicate and expressing something just in that context with predicate or when it is an adjunct and justice means its normal location sense.",
            "We don't actually have a good web to sing with him between arguments magics.",
            "The CCG parser isn't that great of that kind of thing.",
            "So why didn't they just treated all prepositions as being core arguments of herbs for simplicity, but what you've done for born in would then generalize to other verbs.",
            "Now you mentioned 2001 is a film, and it could be starred in or appeared in and.",
            "And again you've got the same thing that it could be place or time, or in the sense of an important means, the same as the sense that in and starting does, it's.",
            "I mean, it kind of does, but in for lots of applications you might not want to think they have the same.",
            "Semantic relation, I'm not sure, but that's something we should definitely address in future work.",
            "OK, let's thank our speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is going to be about trying to combine traditional formal Montego being semantics with modern distributional semantics.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start out with a very quick introduction to distributional and formal semantics and trying to explain why I think it would be a good idea to combine them.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for a motivating example for this talk, Imagine we've got some sort of web based question answering system and we ask it was Obama born in Kenya?",
                    "label": 1
                },
                {
                    "sent": "And Wikipedia's got the sentence saying that abandons birthplace isn't Kenya and obvious.",
                    "label": 0
                },
                {
                    "sent": "Obviously we want our system to be able to answer this as new, But this turns out to be surprisingly difficult.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First of all, I'm going to describe our distributional semantics.",
                    "label": 0
                },
                {
                    "sent": "So most purchased the semantics try to map natural language onto some predefined ontology predicates.",
                    "label": 0
                },
                {
                    "sent": "There's a problem with that in that it seems unlikely we'll ever build an ontology that captures every meaning.",
                    "label": 0
                },
                {
                    "sent": "Anything could possibly say.",
                    "label": 0
                },
                {
                    "sent": "Instead, distributional semantic seems to induce the meaning of words from text.",
                    "label": 1
                },
                {
                    "sent": "One simple approach based on linen Pant elf to this question would be to sort of extract this with X was born in white person and X is birthplace is why person from the question and answer.",
                    "label": 0
                },
                {
                    "sent": "Then we can look at some large corpus for all the X&Y pairs that instantiate these patterns.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can compute the similarity of those lists of XY pairs, and if they are greater than threshold will allow the inference to go through.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we try this on our running example, actually gives us complete the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "For that is that within this approach is not compositional, so you can't combine the meaning for the word, not with the rest of the meaning of the words in the sentence.",
                    "label": 0
                },
                {
                    "sent": "To understand that the Wikipedia sentence is indicated.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A different approach to semantics, or the formal Montego Bay and semantics which I'm illustrating here with CCG.",
                    "label": 0
                },
                {
                    "sent": "In CCG will start out with the syntactic parse we can get from a CCG parser.",
                    "label": 0
                },
                {
                    "sent": "And then we can, based on the words and their CCG categories we can.",
                    "label": 0
                },
                {
                    "sent": "Automatically create lexical entries for these which capture the meaning of the words.",
                    "label": 0
                },
                {
                    "sent": "I don't see if you get the meanings of the words right.",
                    "label": 0
                },
                {
                    "sent": "Then it's simple to compute the meaning of the whole sentence.",
                    "label": 0
                },
                {
                    "sent": "So the problem with one sentence sentence is just the problem with understanding the individual words.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "What's nice about this is that if we change sentence a bit about wasn't born in Kenya, then we can introduce lexical entry for indication, which for not which captures that it's negating the predicate and then.",
                    "label": 0
                },
                {
                    "sent": "This combines to give meaning for the whole sentence with a negation logical form.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can turn these logical forms to theorem prover and it will tell you that Obama wasn't born in Kenya, means the opposite thing to about being born in Kenya.",
                    "label": 0
                },
                {
                    "sent": "You can do lots of other powerful stuff.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kind of thing you can also do an 1st order inference fairly easily with a bit of background knowledge.",
                    "label": 0
                },
                {
                    "sent": "You can tell that if every US President was born in the United States, then Obama can't be born in Kenya.",
                    "label": 1
                },
                {
                    "sent": "If you know that members of president in Kenya isn't in the United States.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's also very good for abstracting away from variations in the syntax, so to a dependency parser.",
                    "label": 0
                },
                {
                    "sent": "These two sentences look very different, but to CCG, you basically get exactly the same logical form for each of these for free.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we try this on our running example, it's going to say that it doesn't know the answer.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the reason for that is logical form for the premise has this sort of birth place symbol in it, and the logical form for the question has this born in symbol in it.",
                    "label": 0
                },
                {
                    "sent": "And the theorem provers got no way of knowing the relation between these symbols.",
                    "label": 0
                },
                {
                    "sent": "So basically being able to understand that you indicated one of these symbols isn't that much use.",
                    "label": 0
                },
                {
                    "sent": "If you don't really have a good model of what your negating is.",
                    "label": 0
                },
                {
                    "sent": "For this reason, approaches based on formal semantics have shown very low recall on actual practical applications.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for quick mini summary here.",
                    "label": 0
                },
                {
                    "sent": "So the formal semantics is great for modeling function words like knots or every and it's great for modeling lots of common constructions like conjunctions or relative clauses or passives.",
                    "label": 1
                },
                {
                    "sent": "But the big problem with it is that it doesn't really have a good model of what content words mean.",
                    "label": 0
                },
                {
                    "sent": "Which is actually the one thing that distributional semantics is really good for, so I think that's quite powerful motivation for trying to combine these frameworks.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Certainly not the first person to think this.",
                    "label": 0
                },
                {
                    "sent": "There's been a couple of recent trends of work on this.",
                    "label": 0
                },
                {
                    "sent": "Firstly, in compositional vector space models, there are lots of different papers I could have cited here these.",
                    "label": 1
                },
                {
                    "sent": "But they still have some issues.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's not at all clear how you do first order inference with vectors C. The other strands work happens.",
                    "label": 0
                },
                {
                    "sent": "Guys Texas involves basically doing standard formal semantics, but then.",
                    "label": 1
                },
                {
                    "sent": "Also adding some inference rules, for example, a rule that says born in implies birthplace or something.",
                    "label": 0
                },
                {
                    "sent": "And they learn that with distributional statistics.",
                    "label": 0
                },
                {
                    "sent": "I think that's really cool, but.",
                    "label": 0
                },
                {
                    "sent": "It needs another number of inference rules that grows quadratically in the size of your vocabulary, which could make it difficult to do at large scale.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be proposing something a bit different to these.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So going back to the formal semantics, we said.",
                    "label": 0
                },
                {
                    "sent": "Problem is that these two equivalent sentence is about Hawaii being born in Alabama have quite different looking logical forms.",
                    "label": 1
                },
                {
                    "sent": "Intuitively we want sentences that mean the same thing to have the same semantics.",
                    "label": 0
                },
                {
                    "sent": "So we're going to want to try and produce representations.",
                    "label": 0
                },
                {
                    "sent": "Look close to this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This relation 53 is sort of an abstract between relation identifier for the concept of being born in the place.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm in CCG terms.",
                    "label": 0
                },
                {
                    "sent": "This amounts to learning lexical entries.",
                    "label": 1
                },
                {
                    "sent": "So we want to learn that sort of lexical entry for X was born in Y&X is birthplace is why both have the same semantics.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do that, you can just plug these lexical entries into the start of the derivation and the correct logical form will fall out of bottom.",
                    "label": 0
                },
                {
                    "sent": "So where these lexical entries come from?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, we can do use the techniques of distributional semantics.",
                    "label": 1
                },
                {
                    "sent": "What we're going to do is run a standard CCG semantics approach over a large corpus.",
                    "label": 0
                },
                {
                    "sent": "We're going to look for everything that's in a born in relation and everything that's in a birthplace relation.",
                    "label": 0
                },
                {
                    "sent": "Hopefully we'll get fairly similar lists of people and places.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can imagine plotting all the predicates we do.",
                    "label": 0
                },
                {
                    "sent": "We get in our corpus and some high dimensional space where born in recently birthplace.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can then do clustering on these.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we're going to do is map each of these clusters onto its own relation symbol.",
                    "label": 0
                },
                {
                    "sent": "So because we turn these distributional statistics into a symbol, we can then negate it.",
                    "label": 0
                },
                {
                    "sent": "We can quantify.",
                    "label": 0
                },
                {
                    "sent": "It will take conjunctions or whatever else you can do with formal semantics.",
                    "label": 0
                },
                {
                    "sent": "We're doing the clustering with Chinese whispers algorithm, which is very scalable and it's nonparametric, which means that we don't need to specify how many different relation types we think there are in language.",
                    "label": 0
                },
                {
                    "sent": "With that, you just fall out of the data.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we do this, then we've got something that can finally answer our running example.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this kind of works.",
                    "label": 0
                },
                {
                    "sent": "One big problem with it is that it doesn't deal with ambiguity as I've described so far, and obviously ambiguity is a huge problem.",
                    "label": 0
                },
                {
                    "sent": "If we take these two sentences, Obama was born in Hawaii and Obama was born in 1961.",
                    "label": 1
                },
                {
                    "sent": "This seems to be expressing different relations.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We certainly don't want to make an inference like Obama was born in.",
                    "label": 0
                },
                {
                    "sent": "Hawaii implies that he was born in 1961.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The solution is going to follow some recent trends at work, and we're going to convert our born in relation, which seems to express two different things into two different lexical entries which are distinguished by the types of their arguments.",
                    "label": 0
                },
                {
                    "sent": "So we have one born in relation between people, locations and one born in relation between people and dates.",
                    "label": 0
                },
                {
                    "sent": "These birthplace and birthdate are just really relation 53 in relation 91.",
                    "label": 0
                },
                {
                    "sent": "I've just written out in English for clarity.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then what we can do is we can just throw these two different boarding predicates into our clustering algorithm to cluster them separately, so that one definition of born in Willette birthdate and one will look like birthplace.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Specs question of where these types going to come from.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "For that example, you could get away with standard named entity types, but that won't be general enough for what we need.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're going to induce a probabilistic model of these.",
                    "label": 0
                },
                {
                    "sent": "We're going to use a simple topic model for doing this.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is for every document, for every predicate in the corpus, we're going to create a sort of document for it, and the words in the documents are going to be the arguments of the predicate.",
                    "label": 0
                },
                {
                    "sent": "So we'll have a lives next document with them.",
                    "label": 0
                },
                {
                    "sent": "A list of places a year of X document will list of dates and aborning next document that's a mixture of places and dates.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get through this LDA and it will do a pretty good job of learning a place type of date type and the arguments are born in it like to be a mixture of places and dates.",
                    "label": 0
                },
                {
                    "sent": "If this looks similar to that or element gave yesterday, that's because it is.",
                    "label": 0
                },
                {
                    "sent": "It seems we came up with the same solution to this problem independently, but concurrently.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some.",
                    "label": 0
                },
                {
                    "sent": "Types we learned from this, which seem to have a fairly clear semantic interpretation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the output from the LDA is going to be distributions connecting predicates with types and connecting arguments with types and to get the true posterior over the types we need to combine these during our derivation.",
                    "label": 0
                },
                {
                    "sent": "So for example, born in 2001, the left hand arguments are born in is probably equally likely to the right hand argument is equally likely to be a location or a date.",
                    "label": 0
                },
                {
                    "sent": "And 2001's probably date, but could be a film in some contexts or a book.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the LDA, we can assign distributions to the variables and constants in the lexicon.",
                    "label": 0
                },
                {
                    "sent": "And then during the derivation we combine these nodes.",
                    "label": 0
                },
                {
                    "sent": "The Y variable gets beta reduced with 2001 and when that happens we combine the type distributions and we find that in this context that born in 2001 unambiguously refers to a date.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Similarly, for born in Washington, Washington is probably a place, but it's possible that it could be.",
                    "label": 1
                },
                {
                    "sent": "George Washington in some Contacts.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we do the.",
                    "label": 0
                },
                {
                    "sent": "Find these during the derivation.",
                    "label": 0
                },
                {
                    "sent": "It's unambiguous, that's.",
                    "label": 0
                },
                {
                    "sent": "Boarding Washington refers to a location.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's one final subtlety with this.",
                    "label": 0
                },
                {
                    "sent": "So I motivated the idea of adding different types to words so that we can disambiguate different meanings based on them having different types of arguments.",
                    "label": 0
                },
                {
                    "sent": "I've shown how we can build a probabilistic distribution there.",
                    "label": 0
                },
                {
                    "sent": "What the types of the arguments are as part of the CCG derivation but in CCG we need to choose a lexical entry for born in before we've actually seen what is arguments are.",
                    "label": 0
                },
                {
                    "sent": "So we need to choose one of these born entries before we know whether it's between places and dates of people in dates or places and people.",
                    "label": 0
                },
                {
                    "sent": "Solution to this is going to be to collapse these two different lexical entries into what I'm calling a packed lexical entry.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So this is a function from the semantics error function from argument types onto semantic relations.",
                    "label": 0
                },
                {
                    "sent": "This is saying that born in if it's given.",
                    "label": 0
                },
                {
                    "sent": "Personal location arguments will be birthplace and with personal data arguments will be.",
                    "label": 0
                },
                {
                    "sent": "A birth date relation.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you run this over your whole derivation, then the output is going to be a packed logical form.",
                    "label": 0
                },
                {
                    "sent": "This logical form catches all the distribution over meetings in the sentence according to our model of amputee.",
                    "label": 0
                },
                {
                    "sent": "So for example, for Obama was born in Hawaii in 1961, we get the most likely interpretation is that his birthplace is why in his birthday since 1961, but we do reserve some probability mass for other possible interpretations.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK and just details on how we trained this.",
                    "label": 0
                },
                {
                    "sent": "Then we use the English Giga word corpus which we passed with the CNC syntactic parser which is standard CCG parser.",
                    "label": 0
                },
                {
                    "sent": "We changed the LDA model with 15 entity types.",
                    "label": 1
                },
                {
                    "sent": "And we wrote down a few like school entries for closed class function words like every and not.",
                    "label": 0
                },
                {
                    "sent": "OK, now we need to evaluate this.",
                    "label": 0
                },
                {
                    "sent": "We're going to do 2 evaluations.",
                    "label": 0
                },
                {
                    "sent": "Firstly model evaluating model of lexical semantics on the question, answering task, and then we're going to model.",
                    "label": 0
                },
                {
                    "sent": "Sorry, evaluate our model of formal semantics.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the question answering task, we're going to generate questions at from corpus very similarly to the prudent Domingos.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised semantic parsing evaluation.",
                    "label": 0
                },
                {
                    "sent": "We can make questions by finding sentence in the corpus and removing individual arguments so we can from sentence.",
                    "label": 0
                },
                {
                    "sent": "Google bought YouTube we can make questions like what, what YouTube and what did Google Play.",
                    "label": 0
                },
                {
                    "sent": "I quite like this approach to generating questions because it means we're evaluating relations in proportion to their frequency in the corpus.",
                    "label": 0
                },
                {
                    "sent": "So understanding more common relations is more important than understanding revelations.",
                    "label": 0
                },
                {
                    "sent": "We're going to generate thousands of these, and we're going to give the answers to people to evaluate.",
                    "label": 0
                },
                {
                    "sent": "The answers don't have to match the original answers from the question.",
                    "label": 0
                },
                {
                    "sent": "I mean if it if we get sentence Google bought Motorola, then we can validly answer.",
                    "label": 0
                },
                {
                    "sent": "What did Google buy with Motorola?",
                    "label": 1
                },
                {
                    "sent": "According to human annotators?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Compare that sprawling baselines and the re verb system relation LDA, which is an LDA based model for relation clustering.",
                    "label": 0
                },
                {
                    "sent": "A baseline CCG system, which is just a standard CCG approach.",
                    "label": 1
                },
                {
                    "sent": "Then we're going to add a word net derived inference rules to that.",
                    "label": 0
                },
                {
                    "sent": "I finally have a system using distributional clusters, which is our full model.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "So the blue line here is our clustering based system.",
                    "label": 0
                },
                {
                    "sent": "So with our model we can rank the answers in order of probability.",
                    "label": 0
                },
                {
                    "sent": "After you marginalized out the ambiguity in the logical form.",
                    "label": 0
                },
                {
                    "sent": "I can see the base and then you can work out what the accuracy is for the top answers and basically for any value of N we up from all our baselines and we also returned a very large number of answers that none of the other systems can return.",
                    "label": 0
                },
                {
                    "sent": "But still, it's pretty good accuracy.",
                    "label": 0
                },
                {
                    "sent": "The CCG baseline systems are horizontal lines because we can't order the answers by probability because they're deterministic.",
                    "label": 0
                },
                {
                    "sent": "I also haven't put the relationality a system on this graph.",
                    "label": 0
                },
                {
                    "sent": "That's because it returns very large number of very low precision answers.",
                    "label": 0
                },
                {
                    "sent": "The reason for that, it seems to be that it's parametric clustering model and you have to sign every relation the corpus to one of 100 different relation types because there's a lot more than 100 different relations in the corpus.",
                    "label": 0
                },
                {
                    "sent": "That means it generates some very large noisy clusters which can answer almost any question from.",
                    "label": 0
                },
                {
                    "sent": "I think that's not to be using nonparametric clustering.",
                    "label": 0
                },
                {
                    "sent": "If you want to do this kind of question answering task.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, here's an example question we got right where we learned that two companies merging with each other means the same thing as one acquiring the other.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're also going to evaluate systems model of formal semantics to see if we can deal with function words.",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to do this on a data set called the Fracas suite, which was basically hand built by linguists to be difficult for computers.",
                    "label": 1
                },
                {
                    "sent": "You've got quite complex multi sentence inferences involving quantifiers where.",
                    "label": 0
                },
                {
                    "sent": "To actually make the conclusion there, you need to fully interpret all the sentences and be able to combine their meanings.",
                    "label": 0
                },
                {
                    "sent": "I think this kind of example would be very difficult to do using vector space models, but with our approach it's basically as simple as writing down electrical entry for every and then handing our semantics to a theorem prover.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the results.",
                    "label": 0
                },
                {
                    "sent": "The only other previous work on this is the natural logic work by McCartney Manning, but even that isn't able to combine the meanings across sentences, so it's only able to attempt a subset of the data set where there's only a single premise quest sentence.",
                    "label": 1
                },
                {
                    "sent": "And we can see that we do pretty well on this.",
                    "label": 0
                },
                {
                    "sent": "I should say that I'm almost all the areas we got on this where recall errors or other precision errors, so incorrectly predicting that we didn't know the answer rather than saying yes or no when that was incorrect.",
                    "label": 0
                },
                {
                    "sent": "So that's yes, you can integrate this kind of thing into applications without actually hurting your precision.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I tried to argue that formal and distributional semantics have quite complementary strengths and weaknesses, and suggesting that we can get the best both worlds if we start modeling our content words using cluster identifiers learned from distributional statistics.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks for a great talk.",
                    "label": 0
                },
                {
                    "sent": "I'm really I like the model a lot too.",
                    "label": 0
                },
                {
                    "sent": "Very sympathetic.",
                    "label": 0
                },
                {
                    "sent": "I had a question about so you you're talking bout entailment a lot here and that's more living in the logical world, but have you thought about how you could bring the distributional similarities to do something there, but one of the big problems when we try to do entailment with distributional representations?",
                    "label": 0
                },
                {
                    "sent": "Obviously is that the distance metrics are symmetric, so you don't have.",
                    "label": 0
                },
                {
                    "sent": "You know one directional thing, right, right?",
                    "label": 0
                },
                {
                    "sent": "So you know Greek to human, but your clustering approach it like I like that you don't need to think about how many concepts you're having in your language and all that, and have you thought about how you could break the symmetries and get this sort of entailment?",
                    "label": 0
                },
                {
                    "sent": "Rather than just just doing the softening of the logic, which is cool by having several clusters per word, so you're asking if I can get entailment directions in, so if say.",
                    "label": 0
                },
                {
                    "sent": "In conquering implies invading, but invading doesn't imply conquering that, yeah, because here you can set your softening.",
                    "label": 0
                },
                {
                    "sent": "The fact that born in Hawaii that these different symbols in your logic actually refer to the same predicate, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, you do have to say this is a very good point.",
                    "label": 0
                },
                {
                    "sent": "You're quite right.",
                    "label": 0
                },
                {
                    "sent": "The solution to this is to have multiple symbols predicates.",
                    "label": 0
                },
                {
                    "sent": "I think the right way to do it would be to join.",
                    "label": 0
                },
                {
                    "sent": "Parent's got some nice work on learning these sort of unidirectional entailment rules.",
                    "label": 0
                },
                {
                    "sent": "I think you could apply that directly to this system.",
                    "label": 0
                },
                {
                    "sent": "And then you could use all the clusters that are word as in as its logical form and that will give you directions.",
                    "label": 0
                },
                {
                    "sent": "Sorry, entailments in just One Direction, cool.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thanks.",
                    "label": 0
                },
                {
                    "sent": "It's really very impressive work.",
                    "label": 0
                },
                {
                    "sent": "One thing that I'm a bit puzzled about is which is a hard clustering approach.",
                    "label": 0
                },
                {
                    "sent": "You're going to really throw away any notion of similarity that you can get from distributional similarity.",
                    "label": 0
                },
                {
                    "sent": "So say either liking and loving are mapped to relation 53, and there exactly the same or not.",
                    "label": 0
                },
                {
                    "sent": "And then there is different as loving and hating, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think that would be addressed by the answer I gave to at that where you sort of trying to learn that loving is a special case of liking.",
                    "label": 0
                },
                {
                    "sent": "That kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Does that so you get some hierarchical clustering there.",
                    "label": 0
                },
                {
                    "sent": "I am so how exactly did you obtain the the set of questions that you were answering you?",
                    "label": 0
                },
                {
                    "sent": "You find just take random sentences from yeah, so we just sample these from a different corpus.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm just.",
                    "label": 0
                },
                {
                    "sent": "A few simple patterns for identifying statements that might be useful questions and then just drop from the arguments from there, not questions based on just any arbitrary sentence in the in the corpus.",
                    "label": 0
                },
                {
                    "sent": "Sorry, they're not questions based on just any arbitrary so well, I mean.",
                    "label": 0
                },
                {
                    "sent": "The arbitrary relations.",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "They're fairly simple question constructions, but I mean there.",
                    "label": 0
                },
                {
                    "sent": "Hi, I'm so it's nice work but I wanted to follow up on that question of losing similarity because I actually think that's a big deal.",
                    "label": 0
                },
                {
                    "sent": "That and it wasn't something that you mentioned as the when you had your one advantage of distribution methods.",
                    "label": 0
                },
                {
                    "sent": "It didn't mention similarity and I I think it's not as simple as the answer you gave 'cause the reality is that lots of words have relationships so that they're sort of similar, but they're not just entailment relationship, so you know, I can describe someone at work.",
                    "label": 0
                },
                {
                    "sent": "As busy overwhelmed not coping stressed, and all of those are a bit different, you don't want to really say that any of them imply the other, but you know in a lot of contexts they're kind of giving the same impression, and it's good to be able to treat them as very related.",
                    "label": 0
                },
                {
                    "sent": "So I think there's more to do there.",
                    "label": 0
                },
                {
                    "sent": "I think you're right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think I mean, maybe you could learn it in the context of workers in the office, that they do mean basically the same thing, and in other contexts they don't mean the same thing, and you could try and capture that with this kind of model, but.",
                    "label": 0
                },
                {
                    "sent": "I do take your point that there seems to be something more to it than that.",
                    "label": 0
                },
                {
                    "sent": "I have a question about the lexical representation of born and.",
                    "label": 0
                },
                {
                    "sent": "So first of all just a clarification.",
                    "label": 0
                },
                {
                    "sent": "So you had born in as a whole predicate, correct?",
                    "label": 0
                },
                {
                    "sent": "So there's no compositional semantics in that case with the preposition in is that is that right?",
                    "label": 0
                },
                {
                    "sent": "No, that's not that.",
                    "label": 0
                },
                {
                    "sent": "This is something I simplified the slide just to make the derivation actually fit on.",
                    "label": 0
                },
                {
                    "sent": "So there's a.",
                    "label": 0
                },
                {
                    "sent": "The lexical entry for born have a prepositional argument, which is subcategorize is with the word in.",
                    "label": 0
                },
                {
                    "sent": "And then the word in is just treated as semantically transparent case marker on the Now, so that doesn't actually contribute to.",
                    "label": 0
                },
                {
                    "sent": "In doesn't construct semantics.",
                    "label": 0
                },
                {
                    "sent": "I just is an identity function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, I mean, I think that actually makes a lot of sense, because you see that these two relations are lexicalized differently in different languages, so yeah, actually, that makes me really happy.",
                    "label": 0
                },
                {
                    "sent": "Any further questions we have time for one more.",
                    "label": 0
                },
                {
                    "sent": "Just following up on that one in has got meaning and a basic physical meaning and then going from 2D to 3D to spatio temporal and then abstract and action and so on.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're quite right.",
                    "label": 0
                },
                {
                    "sent": "So it has a lot of meanings if you just yes, you can use enter in all sorts of different things.",
                    "label": 0
                },
                {
                    "sent": "And ideally, what we'd be able to do is distinguish between went in is like a core arguments the predicate and expressing something just in that context with predicate or when it is an adjunct and justice means its normal location sense.",
                    "label": 0
                },
                {
                    "sent": "We don't actually have a good web to sing with him between arguments magics.",
                    "label": 0
                },
                {
                    "sent": "The CCG parser isn't that great of that kind of thing.",
                    "label": 0
                },
                {
                    "sent": "So why didn't they just treated all prepositions as being core arguments of herbs for simplicity, but what you've done for born in would then generalize to other verbs.",
                    "label": 0
                },
                {
                    "sent": "Now you mentioned 2001 is a film, and it could be starred in or appeared in and.",
                    "label": 0
                },
                {
                    "sent": "And again you've got the same thing that it could be place or time, or in the sense of an important means, the same as the sense that in and starting does, it's.",
                    "label": 0
                },
                {
                    "sent": "I mean, it kind of does, but in for lots of applications you might not want to think they have the same.",
                    "label": 0
                },
                {
                    "sent": "Semantic relation, I'm not sure, but that's something we should definitely address in future work.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank our speaker.",
                    "label": 0
                }
            ]
        }
    }
}