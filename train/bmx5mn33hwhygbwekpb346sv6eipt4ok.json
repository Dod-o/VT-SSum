{
    "id": "bmx5mn33hwhygbwekpb346sv6eipt4ok",
    "title": "Introduction to Kernel Methods",
    "info": {
        "author": [
            "Mikhail Belkin, Department of Computer Science and Engineering, Ohio State University"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "May 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/mlss05us_belkin_ikm/",
    "segmentation": [
        [
            "OK, so before.",
            "I started the talk.",
            "There are a few announcements to make so.",
            "So remember there will be talks in the afternoon here, starting with two o'clock, and there will be some very nice talks so.",
            "That would be a good place to go to.",
            "There is also so.",
            "Talks about.",
            "I don't remember the it's on the yeah you can go to the website.",
            "Sorry.",
            "You should come and find out.",
            "So maybe it would be a good idea to print out the schedule for the afternoon talks.",
            "So that's one.",
            "So there are talks in the afternoon.",
            "Then there will be at.",
            "Check.",
            "It.",
            "3:15 and oh, I'm sorry, that's just log in.",
            "And password.",
            "If TTI.",
            "See_0 methods and what?",
            "What I'm going to talk in the second part of the introduction.",
            "I will talk more about algorithms, so I will talk about kernel based algorithms and so kernel based algorithms.",
            "It's it's a large family of algorithms and it has been quite popular in the last perhaps 50."
        ],
        [
            "Years or so, and that's probably was is considered to be one of the biggest developments in machine learning in particular associated with support vector machines.",
            "But there are.",
            "Other algorithms like regularised least squares, of which actually I haven't talked quite a bit and kernel principle components analysis.",
            "And there are many others algorithms, and basically the problems this algorithms address are the usual problems were interested in machine learning, which is classification, regression density estimation.",
            "And there are various other things like.",
            "Outlier detection for example, and.",
            "Other things also.",
            "OK, so this is algorithms.",
            "In this talk.",
            "I will mostly talk about regularised least squares and support vector machines.",
            "OK, so."
        ],
        [
            "So what is the problem that we remind you what the problem is?",
            "Of course, you just seen it in part.",
            "Let's talk, but I think it's good to set it again.",
            "So regression and classification.",
            "And here is a problem of regression.",
            "So fundamental problem regression is to estimate a function from some space.",
            "Let's just say R to the N for simplicity into R. So you have a bunch of.",
            "They are examples which are XY pairs.",
            "X is an RNN wise and R and you want to estimate a function from this example.",
            "So estimator function means that given a new example, you will be able to tell what the value is and there are many.",
            "Many cases in which this is very are useful.",
            "For example, you know various.",
            "Maybe you have a house and you want to estimate the price of a house depending on various parameters of the House and the market and so on.",
            "So that's one example.",
            "Then maybe you have a patient and you want to find the probability that they will die depending on various parameters of their health.",
            "And so if you're an insurance company, are very interested in this questions estimate sale on Jarrett and you know if you're trying to, if a if you have a rocket and you're going into space, maybe you would like to know what your rocket is doing once you find the engine, what is it going to do in say, two months from now and where it's going to end up?",
            "All that's estimating some sort of four actually position of course, is not one real numbers.",
            "If three real numbers, but it's basically the same thing.",
            "OK, so this end classification is very similar, but you have minus one in one labels and typically you want to say separate.",
            "Images of 1 from images of two or images of digit.",
            "You know five from digit 9.",
            "Or perhaps you want to separate grammatical sentences from ungrammatical sentences.",
            "So these are classification and basically this.",
            "Sometimes they are quite similar, and what's interesting is that actually nobody really does classification people.",
            "What people do.",
            "People typically estimate real valued function.",
            "And then they threshold that real valued function like this.",
            "So if basically they say if it's greater than zero then it's class one.",
            "So this is a two class classification, just two classes.",
            "In reality you might want.",
            "To have more than two, you have faith in character recognition.",
            "You want to be able to tell all characters apart, but this is the simplest case, and perhaps the most fundamental case.",
            "And so most methods estimate actually real.",
            "Why real valued function RN to R and then threshold it?",
            "And that's what produces a classification.",
            "So you rarely do classification direct, so I will actually concentrate on getting this real valued functions and then doing close."
        ],
        [
            "Vacation, so here are some pictures.",
            "What you want to do is something like this.",
            "So let me show you this.",
            "OK, so this is your data.",
            "So this X values.",
            "And This is why wireless, right?",
            "So, Datapoint is an XY pair.",
            "This is X.",
            "This is Y.",
            "And given so, what's your goal?",
            "Given the new data point, you might want predict the wild.",
            "Well, as you can see, the data is a little noisy.",
            "It's kind of a little all over the place, but there is some pattern.",
            "The pattern is something like this perhaps.",
            "So in noise is sort of very persistent feature of the data, and I think it's important to understand that noise is everywhere in the data.",
            "Even think you can say, well, I can always tell an image of seven from an image of nine.",
            "First, it's not even true.",
            "You cannot.",
            "There are some examples where especially like, well, some of them are easier to sell than others, but four and nine can be extremely similar, for example, and sometimes you just don't know what they were trying to write.",
            "Then suppose, how do you actually know what the labels are?",
            "Maybe there is some guy in here sits down and tries to label all of the things.",
            "Well, this guy will just will make a mistake once in awhile no matter what, and so no matter what you have, your data is always noisy.",
            "Well almost always so you have to deal with the issue of noise and in some sense it's a fundamental issue.",
            "So what do you want?",
            "You want to have?",
            "Say.",
            "Something like this?",
            "So this in some sense is a function which fits this data very nicely.",
            "And so, given a new point, I just read off the value of the function, and that's my predictor.",
            "Um well.",
            "In fact, I can tell you the secret, but there's actually data was generated from this distribution.",
            "That's nice.",
            "I mean, so this is in some sense at best you can have.",
            "OK, so.",
            "What about this?",
            "So this is a very nice.",
            "Well behaved function, but it doesn't seem to fit the data very well.",
            "This actually is fitting some sort of polynomial.",
            "Well, so this is something which under fit you are not fitting well enough.",
            "It's a nice pleasant function to dillis.",
            "It's almost like a straight line, but unfortunately it doesn't work very well.",
            "Um?",
            "And now this.",
            "Is and also not such a nice function?",
            "Perhaps it fits the data perfectly.",
            "However, as you can notice, this is some polynomial of high degree which is fit exactly to the data, so it fits the data perfectly.",
            "However, well, if I have a data point here, do I really think it's going to be out there?",
            "The value maybe not.",
            "So this is an example of overfitting.",
            "You fit the data that you have perfectly, but on a new point they will probably do very badly.",
            "There, here you will.",
            "Here you will do.",
            "Basically as badly on the new point as you do on points here, so it fits pretty badly here, but on a new point it will probably not do much worse.",
            "So this is an underfit.",
            "It works consistently but badly.",
            "This works very well on the data, but will work very badly on the new points.",
            "And this is perfect fit, so I don't know what is perfect fit.",
            "It's hard to tell right?",
            "Why?",
            "I mean in this case this is actually the truth, but.",
            "Um?",
            "You wouldn't know that this is exactly the truth from this data.",
            "OK, so.",
            "I hope this is so."
        ],
        [
            "11 point to be made from all those pictures is you never should try to fit the data exactly.",
            "You fit the data exactly.",
            "Fit all the noise which is in the data.",
            "You will always overfit and in some sense that you have to be careful as part of our thing you have to be careful with maximum margin solutions because they may or if it.",
            "Alright, so now."
        ],
        [
            "So I will talk about general framework of regularization.",
            "It's a basic idea, so we have so F before we're trying to estimate some function from a high dimensional space into R and we're giving some data points.",
            "Examples XII.",
            "So this might be images, images, a point in a high dimensional space.",
            "When you take all values of grayscale values at all pixels, and this may be the label.",
            "Oh maybe this is the position of your spaceship and.",
            "10 You know 10 days in the future.",
            "Or maybe the stock price or anything that.",
            "Now you want to have some sort of hypothesis space, and then what do you want you want to so one can argue that one might want to optimize something like this.",
            "So what is this?",
            "This is fit to the data.",
            "This is the loss function and we see how well this function fits to the data.",
            "So you want the function to fit pretty well to the data.",
            "Plus there is this extra term.",
            "What does this extra term do?",
            "This extra term tells you.",
            "That's very complex.",
            "Functions are not good like the function I showed you before, which goes crazy.",
            "It fits data perfectly, so this would be 0.",
            "However, this term presumably should be very high because it's a very complex function.",
            "It goes crazy, so you want some sort of tradeoff between having good fit to the data and having low complexity.",
            "So you want something which fits the data pretty well and yet not too complex, so this is in some sense like all comes Razor saying the simpler hypothesis are better.",
            "You want to explain things in a simple way.",
            "So and this I mega incorporates our simplicity assumption.",
            "So there are many ways to do it and I will talk about reproducing kernel Hilbert space, of course, and.",
            "But it's important to keep in mind that this framework is quite general.",
            "So, for example, you might want to define a function in terms of, say, computer program.",
            "Maybe you have a computer program written and saying some computer language in C and this.",
            "You can have, so this will be the output of your computer program on a data point, so it giving the data point it produces some output and this may be the length of the computer program, so you want to minimize fit to the data plus the length of the computer program, and that's that is known as Kolmogorov complexity, and that's in some sense a very powerful prior.",
            "Unfortunately, you cannot do anything algorithmically with it, so even though that prior might be in theory extremely good, but you cannot, you cannot.",
            "What is the length of the shortest program, which I'll put something?",
            "Impossible to tell, so there are various ways to do it, and people have done things like minimum description, length and so on, and I will talk about some quite different.",
            "Obviously from this idea of simplicity, but what's important to notice is that there are many good ways to define this complexity, but we need something which can be computed efficiently, yeah?",
            "Arg mean is the minimum if F which minimizes the sum.",
            "So you have fit to the data.",
            "This is how close F approximates your data plus some sort of penalty off F. How complex it is, how much it will goes.",
            "OK, so you want function which has the smallest.",
            "This total penalties are smallest.",
            "Given in this class particular class of functions, and this may be linear functions and this may be quadratic fit and this may be the norm and then you get back to your list.",
            "Penalized regularised least squares, for example.",
            "OK, so this is regularization.",
            "This is very general sort of formulation for regularization.",
            "Now why reproducing kernel Hilbert spaces and kernels and so on?",
            "So let me just explain.",
            "Let me draw maybe a picture here.",
            "Hope it will be visible."
        ],
        [
            "So suppose you have something like this.",
            "So this is maybe your kernel.",
            "So suppose I take a Gaussian kernel and I have something like that.",
            "So this is K, so let's just say E to the minus X squared over Sigma squared.",
            "So this kernel has, well, it has some norm in this Journal space and maybe the norm of this kernel is 1.",
            "So.",
            "So this somehow has normal.",
            "Now suppose I take a function which is like this.",
            "So it has three bumps.",
            "So if this is F norm F. Equals one.",
            "Now it has three bumps, so I just add up three kernels.",
            "And I get this new function, so this function is clearly more complex than that function.",
            "It wiggles a lot more.",
            "So basically you can.",
            "It's a simple computation.",
            "You will see that the norm of this is basically the number well, square root of the number of bumps.",
            "So norm.",
            "If this is G. Norm J equals square root of.",
            "Three approximately.",
            "So basically, somehow with spaces we will deal is very, very intuitively.",
            "It's not exactly true.",
            "They're counting the number of bonds, so the more the function wiggles, the higher norm you will have.",
            "So for very complex functions we will have high norm in this space for very simple functions you have blown.",
            "OK, so now here is actually.",
            "This is now the problem from which you get support vector machines in the kernel.",
            "The kernel support vector machines and.",
            "Real."
        ],
        [
            "Colorized kernel regularised least squares, and this is the following problem.",
            "So you want to fit your data well?",
            "Using function F such that the norm of the function in an appropriate function space is not too high.",
            "I'm not too high, meaning the sum has to be small.",
            "And this space is a reproducing kernel Hilbert space H. And again, you can think of about this quantity as reflecting how much the function wiggles.",
            "So simple function has small norm, complex functions have high now.",
            "0509 so this is fit to data plus complexity penalty and complexity is controlled by.",
            "Hilbert space North.",
            "So this is the basic problem.",
            "Now, depending on the nature of this loss function you will get support vector machines.",
            "You will get regularize least squares you can get.",
            "Kernel logistic regression.",
            "You can also get other things, so then this depends on that, but this is an essential element here and this.",
            "Hilbert space norm is what allows for simple algorithms for this problems.",
            "So really you can put here.",
            "You can put some very complicated thing here like like Kolmogorov complexity save how long, how many bits does it take to write a program which writes itself.",
            "But then you can have no algorithm if you put something like this you get a beauty as you will see in a second you will see you will have a very nice simple algorithm.",
            "And this algorithm is basically based on the following fact.",
            "And this is the fact that they represent a theorem and the Rep."
        ],
        [
            "Santa theorem first.",
            "That suppose now F. So I have a function F in my Hilbert space and this function minimizes this sum to fit to the data, plus reproducing kernel Hilbert space, not.",
            "So now it turns out that the solution to this is actually of this form.",
            "F star of X equals sum Alpha IKXIX and XI.",
            "Of course, I owe data points.",
            "So what does it mean?",
            "It means basically the former.",
            "Whoops so it means that.",
            "So you have your point.",
            "And.",
            "The solution to this so you can think about the kernel being the Gaussian kernel, so it's a bump function.",
            "It's like that.",
            "So K fxy.",
            "Equals E they just say X -- y ^2.",
            "So it kernel just looks like a bump.",
            "So this is a pretty typical kernel.",
            "OK, so when I fixed one well, it's a function of two variables of course, but when I fix one of them, it just looks like a bump and other controls where this bond was centered.",
            "So what does this mean?",
            "What does it represent?",
            "A theorem?",
            "Say it says that your function which is this minimizer, which hopefully is a good function, is going to be a some of those bumps.",
            "So maybe like this basically efficient?",
            "Over.",
            "Your point.",
            "So you get a function which is a sum of this bump functions over this points.",
            "And of course, you just have to some finitely many other things, as many as you have data points, so it reduces it.",
            "Difficult, difficult, difficult.",
            "I don't know infinite dimensional problem to something which just has N parameters.",
            "So instead of having to minimize over this very complicated function space or some Hilbert space and stuff like that here, just minimize over this.",
            "Coefficients of the bumps.",
            "So Europe, what sort of linear coefficient of this bumps produces a function.",
            "Which has this minimum and this you can actually compute exactly, it's a.",
            "Fairly simple.",
            "Well.",
            "Reasonably simple problem.",
            "And.",
            "So this.",
            "If the key."
        ],
        [
            "Algorithmics fact This is why kernels are actually useful.",
            "OK, so.",
            "Now I remind you the reproducing property, so reproducing property is the following fact.",
            "If so, again we have this.",
            "We have this Hilbert space age.",
            "And this Hilbert space basically consists of all sorts of linear combinations of this bump functions.",
            "Really infinite linear combinations.",
            "Now what is the reproducing property reproducing property of the following?",
            "Suppose I have a function F in this space.",
            "This space, of course, is the Hilbert space, meaning it has some inner product.",
            "An inner product is just well for two vectors.",
            "It's like you know, like angle well angle times lags.",
            "So.",
            "It turns, I mean that that is somehow almost the definition of edge that F of X if the product K of X and I put the dot here to.",
            "This is a function of this parameter.",
            "And F so KX Dot is a function of this open.",
            "Functional, this can be Y and this is F of Y.",
            "So there this is a product on two functions KX Dot NF.",
            "Well, if you wish, you can write that Doc here.",
            "OK, and this is F of X, so this is really the why.",
            "The things work and somehow point evaluations are represented by kernels in some sense.",
            "In terms of freeze representation.",
            "OK.",
            "So now I'm actually going to prove."
        ],
        [
            "Represent Ethereum because it's such a fundamental thing that I think it's important to do the proof, it's.",
            "A simple proof, but it has to be.",
            "OK, so let me prove it.",
            "So let me just remind you what I'm trying to prove.",
            "I'm minimizing this.",
            "And I want to show that the solution is the sum of the functions over data points.",
            "Alright, so.",
            "So consider as to be the span of the bump functions over data points.",
            "And I want to show that my solution to the minimizer light so spam in all possible linear combinations of those guys and I want to show that my solution will lie in this space.",
            "That's what I'm trying to show.",
            "I'm trying to show that there is some linear combinations of this guy.",
            "Said that my solution to the minimization problem will lie here.",
            "So first, let us notice that if F. Is in the Hilbert space an F is perpendicular to S, then that means that F of X I = 0.",
            "And other ways also true?",
            "And why is this?",
            "This is because of the reproducing property.",
            "Remember this is F of XI.",
            "This is the reproducing property of the kernel.",
            "This guy is F of XI.",
            "If FKXI dot equals zero, that means that the inner product equals zero, therefore they are perpendicular.",
            "So having this product is 0.",
            "Is the same as having F of XI being there?",
            "So they perpendicular F is perpendicular to this guy KXI dot if and only if F of X = 0.",
            "I hope please ask questions if this is unclear.",
            "So now suppose F of XI is zero at each data point XI.",
            "So you have a function which is 0 on each data point.",
            "Then such a function would have to be perpendicular to each one of these guys K of XI.",
            "If it is perpendicular to each one of these guys, it is certainly perpendicular to this path.",
            "Right, if you have a vector which is perpendicular to listen to that, then it will be perpendicular to the whole plane.",
            "Spain by spanned by this to vectors.",
            "So on the other hand, if you have.",
            "Something which is perpendicular to each one of this.",
            "Then the value is 0.",
            "So basically F is perpendicular to S if and only if it takes value 0 at each data point.",
            "So this is claim one.",
            "This is the first.",
            "State.",
            "And again, please ask questions.",
            "In some sense this is very elementary, but in some other sense it think it takes some time to see it.",
            "Now.",
            "This is a subspace of H&H, is just some sort of linear space.",
            "OK, it's infinite dimensional, but doesn't matter so much."
        ],
        [
            "So you can find the complement to this, and again this is.",
            "You'll have to be a bit careful, but it's basically true, so there is.",
            "You just take this space and you take the orthogonal complement to that.",
            "Space.",
            "So S is remember S are all possible linear combinations of this bump functions.",
            "And then you just look at what sort of vectors are toggle to that.",
            "And there is some sort of orthogonal decomposition.",
            "Now if F. If F, of course, and there's a summit can be represented.",
            "So if you have an orthogonal, if you have an orthogonal decomposition of your space, any F can be represented as two components by project.",
            "So this is my orthogonal decomposition.",
            "These two vectors.",
            "Now if I have a vector like this, it's can be represented as a sum of this device.",
            "Well, really this time.",
            "OK, it's a basic fact from linear algebra, so you don't have to worry about this.",
            "Space is being infinite dimensional.",
            "The intuition basically almost exactly carried through, but sometimes you have to be a little careful, but you can basically.",
            "More often than not, you can rely on your intuition.",
            "OK, now.",
            "What is F perpendicular F perpendicular is a space of functions which are perpendicular to H. But with or before two.",
            "I'm sorry perpendicular to S, but we thought before that F is perpendicular to S if and only if F takes zeros on all data points.",
            "So anything which is in F perpendicular will take zeros on all data points.",
            "So that means that F of X I = F, F of XI because it's FF XI plus F perpendicular vex I.",
            "But this is zero and all data points.",
            "So for any data point F of X = F F of XI.",
            "Oh, OK. Oh, that's claim one.",
            "I hope you believe this.",
            "Now FS of H. Let's compare this to F of H. So there are two vectors and you know this is the usual Pythagorean theorem saying that F ^2 + F perpendicular squared equals F. Squared right, because when you have two vectors, when you sum up the length of this is the sum, the squared length of this is the sum of sqrt 4 right triangle?",
            "Square length of the hypotenuse is the sum square of squared sites.",
            "This is exactly that well, so that just means that the hypotenuse is longer in each of the sites.",
            "That's what this is OK so.",
            "There are two facts.",
            "One is F of X = F. SFS of XI and the other one is FSH is shorter than F. So that's basically enough.",
            "Now what are we trying to show?",
            "We were trying to show that this.",
            "Is minimized by something in FS right?",
            "We were trying to show that the minimize of this lies in this."
        ],
        [
            "Some of Bob functions on data points.",
            "Suppose it's not.",
            "Then there is then this.",
            "So F started the solution to this.",
            "Write it as F S + F perpendicular.",
            "Then for each point F star of XI is equal to FF of XI, write becausw ffxi's with her before has the same value as FF star of XII.",
            "Hope you remember that this fact.",
            "So then the loss function is the same because it only depends on those values.",
            "On the other hand, we make that thing shorter for this.",
            "So this guy is the same for FS and this is at least not longer.",
            "For sure, maybe maybe smaller.",
            "So by going from F star to F star with decreased, this whole sum by leaving this first part to be exactly the same as before and making this possibly shorter.",
            "So OK, so that means that when for this thing it became smaller, but it cannot be a smaller because it's already the minimum.",
            "So that is therefore the minimum Haskell I in.",
            "This space.",
            "In S, so the minimum is actually the sum of bumps.",
            "Again, this is it's.",
            "Important argument, because this is actually what makes us algorithms work."
        ],
        [
            "OK.",
            "So finally OK, this is just basically repeating it F styles and this if styles and S it's a linear combination of the bump functions, yeah?",
            "A form of F so you can.",
            "You can think of this as being Gaussians E to the X I -- X K squared.",
            "Oh so yeah, maybe that's it's a good point.",
            "Thanks so.",
            "Yeah, I should say that very directly.",
            "So for the Gaussian so K. XY equals.",
            "Let me just say ether minus X -- Y.",
            "Square.",
            "OK, and you can just think about it being just numbers.",
            "Or they can be vectors, it doesn't matter.",
            "Let me just make some simple real numbers for make it as simple as possible.",
            "Now what is the norm?",
            "So K. X.",
            "Dot so this is a function of the dot X is fixed.",
            "And Ky dot.",
            "Again, this is a function of the dot, not off.",
            "Why is fixed?",
            "I fixed X and I fixed why this had two functions.",
            "Now I want to compute the inner product in the corresponding space and this is just K of XY.",
            "So this is the definition of the product for this bump functions.",
            "That's what the product is.",
            "And you have to play a little bit with this to make sure that somehow it makes sense, yeah?",
            "Right, so basically with when you have three guys.",
            "What happens is that because they are quite far K of X -- y is close to 0, so there almost octogonal.",
            "Then you just have a three of orthogonal work.",
            "Three are talking about.",
            "Then it's just Pythagorean theorem saying that it's 1 + 1 + 1 square root.",
            "So if you have any bumps and they are quite far from each other, you just get square root of M as you're not.",
            "This is just an intuition.",
            "You have to be, but it's I think it's a good intuition.",
            "OK, so so this is this is a case for the Gaussian kernel.",
            "OK, so.",
            "Let's now talk a little."
        ],
        [
            "A bit about the algorithms, so here is the first algorithm.",
            "Take F to be this more specific thing.",
            "I just take least squares fit, so at each point I just fit the IF of X, y -- Y.",
            "Plus I add this penalty so it's basically the same as before.",
            "I just choose the loss function to be square.",
            "I know that the solution is of this form.",
            "Right, that's what I showed.",
            "That is the representative.",
            "Now, what exactly is offers?",
            "I can actually write your formula so the vector file?",
            "So if So what do you do?",
            "You take your vectors of wise and you left multiply it by K plus Lambda I to the minus one.",
            "So Lambda is N by N matrix and K is also end by a kernel matrix.",
            "So K of IJKXXJ.",
            "So it's just end by N matrix.",
            "And what do I do?",
            "I evaluate my Gaussian.",
            "On pairs of points.",
            "So XI, XJ, Z through the X you can think of this as just being.",
            "The bump centered at.",
            "You know it's a value of the if you center bump at point XY, what is the value of XJ?",
            "That's what this is.",
            "And well, this is now just the matrix multiplication problem, or as a division problem.",
            "So this gives me Alphas and I have a function.",
            "I just send this up.",
            "So this is a very explicit solution and I get something which is.",
            "Quite nice from this.",
            "So this is an algorithm, and here is a demo."
        ],
        [
            "Yeah, just see.",
            "So this is.",
            "This is a nice demo written.",
            "By Mike Rainey.",
            "Who is a student in CS?",
            "OK, so.",
            "What am I going to do?",
            "Let me just pull up, let me just pull up two points to study.",
            "So this class one so it can actually do something more, but I'll.",
            "Talk about that later, not today.",
            "So suppose I just have two points.",
            "Now this is RBF radio such as the radius of that Gaussian.",
            "This is robotically Sigma.",
            "So basically this is right now with 0.05.",
            "And so this scale is 1, so it's about 1:20, so the bump looks like this maybe.",
            "This is somehow how the bond looks like.",
            "The size of the bumper about one point of the width.",
            "So I have two points.",
            "So what's my classifier going to be if I just have two points?",
            "Yeah, any idea?",
            "It's going to be a straight line, right?",
            "I have two bumps and I just have linear combination of two identical bumps and that's going to be a straight line.",
            "Well, let's see.",
            "Hopefully yeah, so that's as expected.",
            "What if I put Fe?",
            "One more point here?",
            "What is that going to be?",
            "Huh?",
            "It's going to be actually sort of like two straight lines.",
            "Because what happens is that you have two Gaussians and the size of the Gaussian is quite small, so it basically the radius of the stone is somehow like the radius of the Gaussian that you have.",
            "So it turns pretty sharp, and so on.",
            "So you can play with the thing it's actually online.",
            "OK, let me just open the file.",
            "OK, so here here are some points.",
            "So red and blue and I would like to do some classification, so I would like to draw a boundary.",
            "I would like to draw a boundary somehow to separate.",
            "Red points from Blue Point, so I would say something like this might be pretty reasonable.",
            "It's not exactly clear where they are, but.",
            "So let me see what happens.",
            "So any idea what will happen?",
            "Put it go like this.",
            "What about this point?",
            "So let me let me put so this is the gamma.",
            "This is the regularization parameter.",
            "Remember how much I weigh that norm thing?",
            "So let me put actually 30 here.",
            "OK, so that's pretty good, right?",
            "That's more or less what you expect.",
            "Um?",
            "So what happens if I make this regularization parameter much smaller?",
            "Remember regularization parameter is."
        ],
        [
            "Regularization parameter is this guy is Lambda.",
            "So what happens when I make it much much smaller?",
            "What happens when I make this guy much smaller is that I don't care about the norm.",
            "I can have very complex boundaries, but I want them to be fit very.",
            "I want I want this thing to fit the data very well.",
            "That's also known as overfitting.",
            "So I want to fit it to data very well and I don't care about the boundary.",
            "So let me let me make it.",
            "01 no.",
            "Why doesn't it work?",
            "OK, now it works.",
            "What is this?",
            "OK, so you see what happens now.",
            "Basically the global boundaries somehow doesn't change too much here, but what happens is that all of these guys, they get their own little boundary.",
            "So now you're fitting every point.",
            "Basically, if it says read that Red has to be correct.",
            "So this is exactly what overfitting is using.",
            "Well, this point is just probably some sort of noise really, here everything is blue.",
            "I don't think you want to classify this as red, that's a mistake and hear the same thing.",
            "And I don't know here.",
            "Maybe, maybe not.",
            "But basically what happens when you put this parameter to be quite low.",
            "You start getting very complex boundary, but every point is classified correctly from your data set.",
            "But there is no reason to think that when you get a new point here, it won't be.",
            "It won't be read it probably in all likelihood will be read so you are not doing very well here.",
            "You were doing much better with the higher parameter gamma, so here you are allowing more complex functions and you get better fit to the existing data.",
            "But you probably will get worse fit to new data points.",
            "OK so this is regularised least squares and OK.",
            "So another thing to emphasize here is that of course this is a very standard matrix inversion problem and you can solve a lot of problems like this.",
            "It's it works very well in many problems like you can use it for digital classification and all sorts of things.",
            "It works very often, works very well.",
            "Yeah.",
            "Which.",
            "Yes.",
            "Why is this a good classifier?",
            "The intuition if.",
            "The intuition is the following.",
            "This thing penalizes function which will basically.",
            "So this is going to produce a function which is pretty nice and yet fits the data well.",
            "That's why it's a good classifier.",
            "So somehow this penalty penalizes functions which change too much.",
            "I don't know if it answers your question or not.",
            "Why is it a good classifier?",
            "Right, well, of course it depends on Lambda, right?",
            "But basically what you want, you want to penalize functions with complicated behavior, and this is what weather does.",
            "Of course, it's a very simplistic algorithm and in general.",
            "Do you often want to do something more sophisticated than that?",
            "But it's a very simple algorithm and it worked quite well over.",
            "A wide variety of tasks, but you.",
            "I mean you have to see that correct data.",
            "Also, if you figure some random stuff, it won't work.",
            "And this is actually very similar to support vector machines.",
            "I'll explain what support vector machines are in the kernel case in a second.",
            "So what?"
        ],
        [
            "Gotta support vector machine, so as you saw before, support vector machines.",
            "So now of course I am in this more general setting.",
            "So suppose vector machines are the following support vector machines is.",
            "Instead of penalizing the squared fit, you're penalized this function, and this is the following function.",
            "If F basically has the right sign.",
            "Ann, it's at least where value is at least one.",
            "Then this doesn't produce any penalty.",
            "So suppose why is plus one then if F is bigger than one, then this guy is going to be bigger than one 1 minus something which is bigger than one is going to be negative.",
            "When I take the + plus positive part of it, it's going to be I mean.",
            "Plus just means thick, zero if it's less than zero and take the same thing if it's positive.",
            "So for if F of X is bigger than one in, why is 1?",
            "Then it's fine by the same talking if F of X is less than one.",
            "And why?",
            "If lesson minus one and why?",
            "I is minus one so why I can only have two values here 1 N minus one?",
            "Well, you can have more, but that's the classification.",
            "Then this is fine.",
            "So basically this doesn't penalize you if you have the correct value, but this value needs to have some margin.",
            "And so this is your loss function and this is.",
            "What is this?",
            "This is the same penalty as before.",
            "And actually, I showed you the rigorously squares because I don't have a demo like that for SPMS, but you would see something very similar in terms of.",
            "In terms of the boundaries and solutions, and then this, this is an algorithm which has been exceedingly popular in the last 10 years.",
            "There are literally thousands of papers on this.",
            "And this was introduced by Wapnick.",
            "OK, so now this is slightly more complicated, optimizing the other one, the other one was just in matrix inversion.",
            "This is some quadratic optimization problem, but it's still it can be done quite fast and this leads to a quadratic programming problem, so you can optimize this guy and you just have to choose alphas correctly to minimize this.",
            "OK, so now what's interesting about this, so this has one big advantage of regularised this squares, and this is sparsity.",
            "So it turns out that."
        ],
        [
            "So you have this somewhere.",
            "Your solution is a sum of bump functions over each data point.",
            "However, maybe you have 100,000 data points.",
            "Then you don't want to actually compute each one of them.",
            "It can be very.",
            "Complicated suppose you want to evaluate this function at a new point and you have to compute each of 100,000 of those guys, which can be quite computationally inefficient, so support vector machines has something called sparsity, which just means that more often, most of the things are zero."
        ],
        [
            "So OK, so here is an example.",
            "You have some blue points and you have some red points.",
            "And.",
            "Again, I want the classification, so this is."
        ],
        [
            "Classification boundary, so I find this by minimizing as before my optimization problem.",
            "So this is my F = 0, so F everything positive is classified as read, everything negative is classified as blue.",
            "Now it turns out that I can consider this now 2 lines.",
            "This is F = 1.",
            "So this is a level one whenever equals one and this is F = -- 1.",
            "OK, now it turns out.",
            "That in this so this expansion of course give me gives me a bump over each data point.",
            "So I have imagined this function is like I have here little Gaussian here, little Gaussian here, little Gaussian, and so on, and I send them with some coefficients.",
            "It turns out for the support vector machines, the only point which are non 0 here.",
            "All this so I hope you can see them there."
        ],
        [
            "Have black circles around them and what are they?",
            "Their points?",
            "We chat somehow within the margin of the boundary.",
            "And this margin is this distance between F being equals to minus one and F = 1.",
            "So this is a boundary of specification.",
            "This is somehow this margin size.",
            "An all of this points have non zero coefficients.",
            "Plus points which are misclassified.",
            "So this that and at this point are clearly misclassified, so they also become support vectors.",
            "So instead of having this expansions over all points, you only get expansion of a point.",
            "Some of the points which are the support vectors and these are like close to the boundary or are misclassified and potentially there could be a lot fewer of these guys then the total number of points.",
            "So."
        ],
        [
            "Here's an interesting thing.",
            "So this actually the sparsity.",
            "So what do I mean by sparsity?",
            "By sparsity I mean how many of the coefficients in that expansion unknown 0.",
            "So it turns out that this is closely related to the classification error of the base optimal classifier.",
            "And remember that the Bayes optimal classifier is the best possible classifier.",
            "Suppose you know everything about the data and you have to draw the boundary somehow.",
            "You still make mistakes because you know the data has noise, so that thing will still maybe get it wrong 10% of the time.",
            "So it turns out that if the Bayes optimal classifier makes errors 10% of the time, then what you get, roughly speaking, you would get that.",
            "Expansion.",
            "10% of this guys are present and the rest is, so the number of this guys is roughly speaking the Bayes error.",
            "It's not quite true because of this margin business, but it's very close to the base error.",
            "So that's an interesting thing about support vector machines, and often they are more efficient than regularised least squares in terms of performance is still there quite similar, sometimes, maybe better, but that's not so the common wisdom are that they work better in terms of performance, but it doesn't seem to be the case.",
            "Um?",
            "But there they are.",
            "They have this nice sparsity property.",
            "OK, so now."
        ],
        [
            "I will mention a few things about the future map.",
            "And this is the way this ideas are often presented in terms of this feature map and what is the feature map.",
            "Remember feature map you map your data to some high dimensional space and you do an ordinary linear classification in that space.",
            "So.",
            "I map accent to H&H is some sort of high dimensional infinite dimensional vector space and then I use linear functions on H. To evaluate to.",
            "Find functions on X to find to fit data on X.",
            "So an if H is very high dimensional then of course the family, the family of linear functions are dementia.",
            "Knowledge of that family is just the dimensionality or the number of parameters of linear functions is the dimensionality of your space 'cause they're controlled by W and.",
            "So if your space, for example is infinite dimensional, well then it's infinitely rich.",
            "Now.",
            "So what is the future map assault?"
        ],
        [
            "Native to the kernel, so one of this if basically the following.",
            "So you can you can have X. Map 28 an so X is your data space in you map it to H&H is as usual, reproducing kernel Hilbert space.",
            "An X goes to KX dot, so for each point X.",
            "Your representation is the actual bump is the function which is above.",
            "So you map from your data space to function space in your image of the what point goes to is actually a function, which is a bump centered at that point.",
            "Now it turns out that you can rewrite this.",
            "Now this is of course a Hilbert space.",
            "The linear space with an inner product now.",
            "This was the problem.",
            "We considered all the time I take squares, but you can take other things, doesn't matter.",
            "So y -- F of XI.",
            "Plus, Lambda times the norm.",
            "Now what is F of XIF of XI?",
            "If you recall the reproducing property, it's just this inner product.",
            "It's in the product key of XI dot NF.",
            "This is the reproducing property and K of excise side.",
            "OK so I can rewrite this at that and this is of course exactly the same thing.",
            "Well, now what is this?",
            "This is ordinary least squares fitting.",
            "This is exactly what this query feeling is.",
            "And when I do this list perfect, so I get the algorithm for regularised least squares.",
            "So consider this if you remember what least squares was before you have linear space you have WS your vector and you have this linear thing which is www.xlw.xbecomes.",
            "This times F right?",
            "And why I is still why I and this is?",
            "Norm F which is so F becomes W. OK, so this is somehow the feature map interpretation, so that's one way people think about kernels that somehow you map your space into an infinite dimensional space and you do some sort of.",
            "Whatever you do, linear regression or linear linear least squares, whatever support vector machines in that space.",
            "OK.",
            "So maybe I'll just very briefly mention this.",
            "OK, so."
        ],
        [
            "So what if?",
            "What is empirical error?",
            "So recall that the empirical error is basically suppose I have a classifier F. Empirical error is how well I do on my training set.",
            "So on my training set on the points which I already have, I might do zero.",
            "I might do whatever.",
            "So this is somehow I take my class.",
            "If I evaluate it on the data I have and then this is I get some number and this is how well I don't the training set now what is generalization error generalization error is basically consider all possible.",
            "Now all the space of all possible data points and how well do I do on that?",
            "So this is expectation over all possible data of how well I don't.",
            "So this is really what I want.",
            "I want this to be as low as possible.",
            "So for classification the Bayes optimal minimizes this quantity.",
            "So basically I want to make sure that my classifier given any possible data point does as well as it can possibly do.",
            "However, I cannot really observe that because I don't know what I suppose I want to distinguish, say, images of seven from images of 9th.",
            "I have a bunch of examples, but I don't know what this is.",
            "I don't know what all possible nines and sevens are.",
            "I have to know that to be able to evaluate that.",
            "So really, what I would like to say, I would like to say that if I have a procedure such that the empirical error is low, so I do quite well on my training set.",
            "And this difference is law.",
            "Then I'm in good shape if the empirical error is low on its own, that means nothing.",
            "I can overfit.",
            "I can have a crazy function which goes which fits my point exactly.",
            "This will be 0.",
            "This can still be huge, so I want to have something which does very well on the training data and doesn't overfit.",
            "That's almost as well everywhere else.",
            "So I really want to estimate this difference.",
            "This is what people call generalization Gap, so here is.",
            "Um?",
            "Assume the theorem."
        ],
        [
            "'cause I call it a theorem in quotes because it's incorrect.",
            "Basically it's saying the following, it's saying.",
            "Again.",
            "Don't don't think this is an actual theme, I just want to show you what sort of statement you can make about this.",
            "As I say them, it's completely wrong.",
            "But the actual statement is much more complicated, so you are F is the minimizer of this guy.",
            "Then what can I say?",
            "About this generalization gap for a function which I which is produced by my favorite kernel method.",
            "It turns out that you can say basically the following, that you can say that it's something like that.",
            "It's something like one over Lambda over square root of N. So for example, if I fix Lambda and N goes to Infinity then this will tend to zero as an square root of N. Now if my Lambda becomes very small then you can see that I have now bound right when Lambda is very small.",
            "This is very large, so this whole thing explodes.",
            "I have nothing.",
            "So that means if my Lambda is very small, that means that I'm overfitting.",
            "Right when Lambda is very, very close to zero, I don't pay any price for making this function complex, so I will fill any data and then I will.",
            "This bound tells me that.",
            "Well, basically I cannot say anything about this difference when Lambda is very large and at the hands it's quite small, but when Lambda is very large, I'm underfitting I'm making a function which is nice, but we will probably have bad empirical error.",
            "So this is a sort of things and you will see them things like that tomorrow.",
            "I think in the day after and.",
            "So this is the sort of theoretical statement which can be made about this.",
            "One of them.",
            "So ultimately what you want to show you want to show that for your method this is small and the difference is small.",
            "Then you're in good shape.",
            "OK, so I think that's."
        ],
        [
            "Pretty much.",
            "What I had to say.",
            "So here are some references.",
            "It's nowhere close to the complete list, but.",
            "This some of this may be useful.",
            "OK, so I think John wanted to make some announcement.",
            "Where is he?",
            "I guess there is no announcement then.",
            "Hey John.",
            "They want to make an announcement."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so before.",
                    "label": 0
                },
                {
                    "sent": "I started the talk.",
                    "label": 0
                },
                {
                    "sent": "There are a few announcements to make so.",
                    "label": 0
                },
                {
                    "sent": "So remember there will be talks in the afternoon here, starting with two o'clock, and there will be some very nice talks so.",
                    "label": 0
                },
                {
                    "sent": "That would be a good place to go to.",
                    "label": 0
                },
                {
                    "sent": "There is also so.",
                    "label": 0
                },
                {
                    "sent": "Talks about.",
                    "label": 0
                },
                {
                    "sent": "I don't remember the it's on the yeah you can go to the website.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "You should come and find out.",
                    "label": 0
                },
                {
                    "sent": "So maybe it would be a good idea to print out the schedule for the afternoon talks.",
                    "label": 0
                },
                {
                    "sent": "So that's one.",
                    "label": 0
                },
                {
                    "sent": "So there are talks in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "Then there will be at.",
                    "label": 0
                },
                {
                    "sent": "Check.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "3:15 and oh, I'm sorry, that's just log in.",
                    "label": 0
                },
                {
                    "sent": "And password.",
                    "label": 0
                },
                {
                    "sent": "If TTI.",
                    "label": 0
                },
                {
                    "sent": "See_0 methods and what?",
                    "label": 0
                },
                {
                    "sent": "What I'm going to talk in the second part of the introduction.",
                    "label": 0
                },
                {
                    "sent": "I will talk more about algorithms, so I will talk about kernel based algorithms and so kernel based algorithms.",
                    "label": 0
                },
                {
                    "sent": "It's it's a large family of algorithms and it has been quite popular in the last perhaps 50.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Years or so, and that's probably was is considered to be one of the biggest developments in machine learning in particular associated with support vector machines.",
                    "label": 0
                },
                {
                    "sent": "But there are.",
                    "label": 0
                },
                {
                    "sent": "Other algorithms like regularised least squares, of which actually I haven't talked quite a bit and kernel principle components analysis.",
                    "label": 0
                },
                {
                    "sent": "And there are many others algorithms, and basically the problems this algorithms address are the usual problems were interested in machine learning, which is classification, regression density estimation.",
                    "label": 0
                },
                {
                    "sent": "And there are various other things like.",
                    "label": 0
                },
                {
                    "sent": "Outlier detection for example, and.",
                    "label": 0
                },
                {
                    "sent": "Other things also.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is algorithms.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "I will mostly talk about regularised least squares and support vector machines.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is the problem that we remind you what the problem is?",
                    "label": 0
                },
                {
                    "sent": "Of course, you just seen it in part.",
                    "label": 0
                },
                {
                    "sent": "Let's talk, but I think it's good to set it again.",
                    "label": 0
                },
                {
                    "sent": "So regression and classification.",
                    "label": 0
                },
                {
                    "sent": "And here is a problem of regression.",
                    "label": 0
                },
                {
                    "sent": "So fundamental problem regression is to estimate a function from some space.",
                    "label": 0
                },
                {
                    "sent": "Let's just say R to the N for simplicity into R. So you have a bunch of.",
                    "label": 0
                },
                {
                    "sent": "They are examples which are XY pairs.",
                    "label": 0
                },
                {
                    "sent": "X is an RNN wise and R and you want to estimate a function from this example.",
                    "label": 0
                },
                {
                    "sent": "So estimator function means that given a new example, you will be able to tell what the value is and there are many.",
                    "label": 0
                },
                {
                    "sent": "Many cases in which this is very are useful.",
                    "label": 0
                },
                {
                    "sent": "For example, you know various.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have a house and you want to estimate the price of a house depending on various parameters of the House and the market and so on.",
                    "label": 0
                },
                {
                    "sent": "So that's one example.",
                    "label": 0
                },
                {
                    "sent": "Then maybe you have a patient and you want to find the probability that they will die depending on various parameters of their health.",
                    "label": 0
                },
                {
                    "sent": "And so if you're an insurance company, are very interested in this questions estimate sale on Jarrett and you know if you're trying to, if a if you have a rocket and you're going into space, maybe you would like to know what your rocket is doing once you find the engine, what is it going to do in say, two months from now and where it's going to end up?",
                    "label": 0
                },
                {
                    "sent": "All that's estimating some sort of four actually position of course, is not one real numbers.",
                    "label": 0
                },
                {
                    "sent": "If three real numbers, but it's basically the same thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so this end classification is very similar, but you have minus one in one labels and typically you want to say separate.",
                    "label": 0
                },
                {
                    "sent": "Images of 1 from images of two or images of digit.",
                    "label": 0
                },
                {
                    "sent": "You know five from digit 9.",
                    "label": 0
                },
                {
                    "sent": "Or perhaps you want to separate grammatical sentences from ungrammatical sentences.",
                    "label": 0
                },
                {
                    "sent": "So these are classification and basically this.",
                    "label": 0
                },
                {
                    "sent": "Sometimes they are quite similar, and what's interesting is that actually nobody really does classification people.",
                    "label": 0
                },
                {
                    "sent": "What people do.",
                    "label": 0
                },
                {
                    "sent": "People typically estimate real valued function.",
                    "label": 0
                },
                {
                    "sent": "And then they threshold that real valued function like this.",
                    "label": 0
                },
                {
                    "sent": "So if basically they say if it's greater than zero then it's class one.",
                    "label": 0
                },
                {
                    "sent": "So this is a two class classification, just two classes.",
                    "label": 0
                },
                {
                    "sent": "In reality you might want.",
                    "label": 0
                },
                {
                    "sent": "To have more than two, you have faith in character recognition.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to tell all characters apart, but this is the simplest case, and perhaps the most fundamental case.",
                    "label": 0
                },
                {
                    "sent": "And so most methods estimate actually real.",
                    "label": 0
                },
                {
                    "sent": "Why real valued function RN to R and then threshold it?",
                    "label": 0
                },
                {
                    "sent": "And that's what produces a classification.",
                    "label": 0
                },
                {
                    "sent": "So you rarely do classification direct, so I will actually concentrate on getting this real valued functions and then doing close.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vacation, so here are some pictures.",
                    "label": 0
                },
                {
                    "sent": "What you want to do is something like this.",
                    "label": 0
                },
                {
                    "sent": "So let me show you this.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is your data.",
                    "label": 0
                },
                {
                    "sent": "So this X values.",
                    "label": 0
                },
                {
                    "sent": "And This is why wireless, right?",
                    "label": 0
                },
                {
                    "sent": "So, Datapoint is an XY pair.",
                    "label": 0
                },
                {
                    "sent": "This is X.",
                    "label": 0
                },
                {
                    "sent": "This is Y.",
                    "label": 0
                },
                {
                    "sent": "And given so, what's your goal?",
                    "label": 0
                },
                {
                    "sent": "Given the new data point, you might want predict the wild.",
                    "label": 1
                },
                {
                    "sent": "Well, as you can see, the data is a little noisy.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a little all over the place, but there is some pattern.",
                    "label": 0
                },
                {
                    "sent": "The pattern is something like this perhaps.",
                    "label": 0
                },
                {
                    "sent": "So in noise is sort of very persistent feature of the data, and I think it's important to understand that noise is everywhere in the data.",
                    "label": 0
                },
                {
                    "sent": "Even think you can say, well, I can always tell an image of seven from an image of nine.",
                    "label": 0
                },
                {
                    "sent": "First, it's not even true.",
                    "label": 0
                },
                {
                    "sent": "You cannot.",
                    "label": 0
                },
                {
                    "sent": "There are some examples where especially like, well, some of them are easier to sell than others, but four and nine can be extremely similar, for example, and sometimes you just don't know what they were trying to write.",
                    "label": 0
                },
                {
                    "sent": "Then suppose, how do you actually know what the labels are?",
                    "label": 0
                },
                {
                    "sent": "Maybe there is some guy in here sits down and tries to label all of the things.",
                    "label": 0
                },
                {
                    "sent": "Well, this guy will just will make a mistake once in awhile no matter what, and so no matter what you have, your data is always noisy.",
                    "label": 0
                },
                {
                    "sent": "Well almost always so you have to deal with the issue of noise and in some sense it's a fundamental issue.",
                    "label": 0
                },
                {
                    "sent": "So what do you want?",
                    "label": 0
                },
                {
                    "sent": "You want to have?",
                    "label": 0
                },
                {
                    "sent": "Say.",
                    "label": 0
                },
                {
                    "sent": "Something like this?",
                    "label": 0
                },
                {
                    "sent": "So this in some sense is a function which fits this data very nicely.",
                    "label": 0
                },
                {
                    "sent": "And so, given a new point, I just read off the value of the function, and that's my predictor.",
                    "label": 0
                },
                {
                    "sent": "Um well.",
                    "label": 0
                },
                {
                    "sent": "In fact, I can tell you the secret, but there's actually data was generated from this distribution.",
                    "label": 0
                },
                {
                    "sent": "That's nice.",
                    "label": 0
                },
                {
                    "sent": "I mean, so this is in some sense at best you can have.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What about this?",
                    "label": 0
                },
                {
                    "sent": "So this is a very nice.",
                    "label": 0
                },
                {
                    "sent": "Well behaved function, but it doesn't seem to fit the data very well.",
                    "label": 0
                },
                {
                    "sent": "This actually is fitting some sort of polynomial.",
                    "label": 0
                },
                {
                    "sent": "Well, so this is something which under fit you are not fitting well enough.",
                    "label": 0
                },
                {
                    "sent": "It's a nice pleasant function to dillis.",
                    "label": 0
                },
                {
                    "sent": "It's almost like a straight line, but unfortunately it doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And now this.",
                    "label": 0
                },
                {
                    "sent": "Is and also not such a nice function?",
                    "label": 0
                },
                {
                    "sent": "Perhaps it fits the data perfectly.",
                    "label": 0
                },
                {
                    "sent": "However, as you can notice, this is some polynomial of high degree which is fit exactly to the data, so it fits the data perfectly.",
                    "label": 0
                },
                {
                    "sent": "However, well, if I have a data point here, do I really think it's going to be out there?",
                    "label": 0
                },
                {
                    "sent": "The value maybe not.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of overfitting.",
                    "label": 0
                },
                {
                    "sent": "You fit the data that you have perfectly, but on a new point they will probably do very badly.",
                    "label": 0
                },
                {
                    "sent": "There, here you will.",
                    "label": 0
                },
                {
                    "sent": "Here you will do.",
                    "label": 0
                },
                {
                    "sent": "Basically as badly on the new point as you do on points here, so it fits pretty badly here, but on a new point it will probably not do much worse.",
                    "label": 0
                },
                {
                    "sent": "So this is an underfit.",
                    "label": 0
                },
                {
                    "sent": "It works consistently but badly.",
                    "label": 0
                },
                {
                    "sent": "This works very well on the data, but will work very badly on the new points.",
                    "label": 0
                },
                {
                    "sent": "And this is perfect fit, so I don't know what is perfect fit.",
                    "label": 0
                },
                {
                    "sent": "It's hard to tell right?",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "I mean in this case this is actually the truth, but.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "You wouldn't know that this is exactly the truth from this data.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I hope this is so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "11 point to be made from all those pictures is you never should try to fit the data exactly.",
                    "label": 0
                },
                {
                    "sent": "You fit the data exactly.",
                    "label": 0
                },
                {
                    "sent": "Fit all the noise which is in the data.",
                    "label": 0
                },
                {
                    "sent": "You will always overfit and in some sense that you have to be careful as part of our thing you have to be careful with maximum margin solutions because they may or if it.",
                    "label": 0
                },
                {
                    "sent": "Alright, so now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will talk about general framework of regularization.",
                    "label": 0
                },
                {
                    "sent": "It's a basic idea, so we have so F before we're trying to estimate some function from a high dimensional space into R and we're giving some data points.",
                    "label": 0
                },
                {
                    "sent": "Examples XII.",
                    "label": 0
                },
                {
                    "sent": "So this might be images, images, a point in a high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "When you take all values of grayscale values at all pixels, and this may be the label.",
                    "label": 0
                },
                {
                    "sent": "Oh maybe this is the position of your spaceship and.",
                    "label": 0
                },
                {
                    "sent": "10 You know 10 days in the future.",
                    "label": 0
                },
                {
                    "sent": "Or maybe the stock price or anything that.",
                    "label": 0
                },
                {
                    "sent": "Now you want to have some sort of hypothesis space, and then what do you want you want to so one can argue that one might want to optimize something like this.",
                    "label": 0
                },
                {
                    "sent": "So what is this?",
                    "label": 0
                },
                {
                    "sent": "This is fit to the data.",
                    "label": 0
                },
                {
                    "sent": "This is the loss function and we see how well this function fits to the data.",
                    "label": 0
                },
                {
                    "sent": "So you want the function to fit pretty well to the data.",
                    "label": 0
                },
                {
                    "sent": "Plus there is this extra term.",
                    "label": 0
                },
                {
                    "sent": "What does this extra term do?",
                    "label": 0
                },
                {
                    "sent": "This extra term tells you.",
                    "label": 0
                },
                {
                    "sent": "That's very complex.",
                    "label": 0
                },
                {
                    "sent": "Functions are not good like the function I showed you before, which goes crazy.",
                    "label": 0
                },
                {
                    "sent": "It fits data perfectly, so this would be 0.",
                    "label": 0
                },
                {
                    "sent": "However, this term presumably should be very high because it's a very complex function.",
                    "label": 0
                },
                {
                    "sent": "It goes crazy, so you want some sort of tradeoff between having good fit to the data and having low complexity.",
                    "label": 0
                },
                {
                    "sent": "So you want something which fits the data pretty well and yet not too complex, so this is in some sense like all comes Razor saying the simpler hypothesis are better.",
                    "label": 0
                },
                {
                    "sent": "You want to explain things in a simple way.",
                    "label": 0
                },
                {
                    "sent": "So and this I mega incorporates our simplicity assumption.",
                    "label": 0
                },
                {
                    "sent": "So there are many ways to do it and I will talk about reproducing kernel Hilbert space, of course, and.",
                    "label": 0
                },
                {
                    "sent": "But it's important to keep in mind that this framework is quite general.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you might want to define a function in terms of, say, computer program.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have a computer program written and saying some computer language in C and this.",
                    "label": 0
                },
                {
                    "sent": "You can have, so this will be the output of your computer program on a data point, so it giving the data point it produces some output and this may be the length of the computer program, so you want to minimize fit to the data plus the length of the computer program, and that's that is known as Kolmogorov complexity, and that's in some sense a very powerful prior.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, you cannot do anything algorithmically with it, so even though that prior might be in theory extremely good, but you cannot, you cannot.",
                    "label": 0
                },
                {
                    "sent": "What is the length of the shortest program, which I'll put something?",
                    "label": 0
                },
                {
                    "sent": "Impossible to tell, so there are various ways to do it, and people have done things like minimum description, length and so on, and I will talk about some quite different.",
                    "label": 0
                },
                {
                    "sent": "Obviously from this idea of simplicity, but what's important to notice is that there are many good ways to define this complexity, but we need something which can be computed efficiently, yeah?",
                    "label": 0
                },
                {
                    "sent": "Arg mean is the minimum if F which minimizes the sum.",
                    "label": 0
                },
                {
                    "sent": "So you have fit to the data.",
                    "label": 0
                },
                {
                    "sent": "This is how close F approximates your data plus some sort of penalty off F. How complex it is, how much it will goes.",
                    "label": 0
                },
                {
                    "sent": "OK, so you want function which has the smallest.",
                    "label": 0
                },
                {
                    "sent": "This total penalties are smallest.",
                    "label": 0
                },
                {
                    "sent": "Given in this class particular class of functions, and this may be linear functions and this may be quadratic fit and this may be the norm and then you get back to your list.",
                    "label": 0
                },
                {
                    "sent": "Penalized regularised least squares, for example.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is regularization.",
                    "label": 0
                },
                {
                    "sent": "This is very general sort of formulation for regularization.",
                    "label": 0
                },
                {
                    "sent": "Now why reproducing kernel Hilbert spaces and kernels and so on?",
                    "label": 0
                },
                {
                    "sent": "So let me just explain.",
                    "label": 0
                },
                {
                    "sent": "Let me draw maybe a picture here.",
                    "label": 0
                },
                {
                    "sent": "Hope it will be visible.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose you have something like this.",
                    "label": 0
                },
                {
                    "sent": "So this is maybe your kernel.",
                    "label": 0
                },
                {
                    "sent": "So suppose I take a Gaussian kernel and I have something like that.",
                    "label": 0
                },
                {
                    "sent": "So this is K, so let's just say E to the minus X squared over Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "So this kernel has, well, it has some norm in this Journal space and maybe the norm of this kernel is 1.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this somehow has normal.",
                    "label": 0
                },
                {
                    "sent": "Now suppose I take a function which is like this.",
                    "label": 0
                },
                {
                    "sent": "So it has three bumps.",
                    "label": 0
                },
                {
                    "sent": "So if this is F norm F. Equals one.",
                    "label": 0
                },
                {
                    "sent": "Now it has three bumps, so I just add up three kernels.",
                    "label": 0
                },
                {
                    "sent": "And I get this new function, so this function is clearly more complex than that function.",
                    "label": 0
                },
                {
                    "sent": "It wiggles a lot more.",
                    "label": 0
                },
                {
                    "sent": "So basically you can.",
                    "label": 0
                },
                {
                    "sent": "It's a simple computation.",
                    "label": 0
                },
                {
                    "sent": "You will see that the norm of this is basically the number well, square root of the number of bumps.",
                    "label": 0
                },
                {
                    "sent": "So norm.",
                    "label": 0
                },
                {
                    "sent": "If this is G. Norm J equals square root of.",
                    "label": 0
                },
                {
                    "sent": "Three approximately.",
                    "label": 0
                },
                {
                    "sent": "So basically, somehow with spaces we will deal is very, very intuitively.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly true.",
                    "label": 0
                },
                {
                    "sent": "They're counting the number of bonds, so the more the function wiggles, the higher norm you will have.",
                    "label": 0
                },
                {
                    "sent": "So for very complex functions we will have high norm in this space for very simple functions you have blown.",
                    "label": 0
                },
                {
                    "sent": "OK, so now here is actually.",
                    "label": 0
                },
                {
                    "sent": "This is now the problem from which you get support vector machines in the kernel.",
                    "label": 0
                },
                {
                    "sent": "The kernel support vector machines and.",
                    "label": 0
                },
                {
                    "sent": "Real.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Colorized kernel regularised least squares, and this is the following problem.",
                    "label": 0
                },
                {
                    "sent": "So you want to fit your data well?",
                    "label": 0
                },
                {
                    "sent": "Using function F such that the norm of the function in an appropriate function space is not too high.",
                    "label": 0
                },
                {
                    "sent": "I'm not too high, meaning the sum has to be small.",
                    "label": 0
                },
                {
                    "sent": "And this space is a reproducing kernel Hilbert space H. And again, you can think of about this quantity as reflecting how much the function wiggles.",
                    "label": 0
                },
                {
                    "sent": "So simple function has small norm, complex functions have high now.",
                    "label": 0
                },
                {
                    "sent": "0509 so this is fit to data plus complexity penalty and complexity is controlled by.",
                    "label": 0
                },
                {
                    "sent": "Hilbert space North.",
                    "label": 0
                },
                {
                    "sent": "So this is the basic problem.",
                    "label": 0
                },
                {
                    "sent": "Now, depending on the nature of this loss function you will get support vector machines.",
                    "label": 0
                },
                {
                    "sent": "You will get regularize least squares you can get.",
                    "label": 0
                },
                {
                    "sent": "Kernel logistic regression.",
                    "label": 0
                },
                {
                    "sent": "You can also get other things, so then this depends on that, but this is an essential element here and this.",
                    "label": 0
                },
                {
                    "sent": "Hilbert space norm is what allows for simple algorithms for this problems.",
                    "label": 0
                },
                {
                    "sent": "So really you can put here.",
                    "label": 0
                },
                {
                    "sent": "You can put some very complicated thing here like like Kolmogorov complexity save how long, how many bits does it take to write a program which writes itself.",
                    "label": 0
                },
                {
                    "sent": "But then you can have no algorithm if you put something like this you get a beauty as you will see in a second you will see you will have a very nice simple algorithm.",
                    "label": 0
                },
                {
                    "sent": "And this algorithm is basically based on the following fact.",
                    "label": 0
                },
                {
                    "sent": "And this is the fact that they represent a theorem and the Rep.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Santa theorem first.",
                    "label": 0
                },
                {
                    "sent": "That suppose now F. So I have a function F in my Hilbert space and this function minimizes this sum to fit to the data, plus reproducing kernel Hilbert space, not.",
                    "label": 1
                },
                {
                    "sent": "So now it turns out that the solution to this is actually of this form.",
                    "label": 0
                },
                {
                    "sent": "F star of X equals sum Alpha IKXIX and XI.",
                    "label": 0
                },
                {
                    "sent": "Of course, I owe data points.",
                    "label": 0
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "It means basically the former.",
                    "label": 0
                },
                {
                    "sent": "Whoops so it means that.",
                    "label": 0
                },
                {
                    "sent": "So you have your point.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The solution to this so you can think about the kernel being the Gaussian kernel, so it's a bump function.",
                    "label": 0
                },
                {
                    "sent": "It's like that.",
                    "label": 0
                },
                {
                    "sent": "So K fxy.",
                    "label": 0
                },
                {
                    "sent": "Equals E they just say X -- y ^2.",
                    "label": 0
                },
                {
                    "sent": "So it kernel just looks like a bump.",
                    "label": 0
                },
                {
                    "sent": "So this is a pretty typical kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I fixed one well, it's a function of two variables of course, but when I fix one of them, it just looks like a bump and other controls where this bond was centered.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "What does it represent?",
                    "label": 0
                },
                {
                    "sent": "A theorem?",
                    "label": 0
                },
                {
                    "sent": "Say it says that your function which is this minimizer, which hopefully is a good function, is going to be a some of those bumps.",
                    "label": 0
                },
                {
                    "sent": "So maybe like this basically efficient?",
                    "label": 0
                },
                {
                    "sent": "Over.",
                    "label": 0
                },
                {
                    "sent": "Your point.",
                    "label": 0
                },
                {
                    "sent": "So you get a function which is a sum of this bump functions over this points.",
                    "label": 0
                },
                {
                    "sent": "And of course, you just have to some finitely many other things, as many as you have data points, so it reduces it.",
                    "label": 0
                },
                {
                    "sent": "Difficult, difficult, difficult.",
                    "label": 0
                },
                {
                    "sent": "I don't know infinite dimensional problem to something which just has N parameters.",
                    "label": 0
                },
                {
                    "sent": "So instead of having to minimize over this very complicated function space or some Hilbert space and stuff like that here, just minimize over this.",
                    "label": 0
                },
                {
                    "sent": "Coefficients of the bumps.",
                    "label": 0
                },
                {
                    "sent": "So Europe, what sort of linear coefficient of this bumps produces a function.",
                    "label": 0
                },
                {
                    "sent": "Which has this minimum and this you can actually compute exactly, it's a.",
                    "label": 0
                },
                {
                    "sent": "Fairly simple.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Reasonably simple problem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "If the key.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithmics fact This is why kernels are actually useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Now I remind you the reproducing property, so reproducing property is the following fact.",
                    "label": 0
                },
                {
                    "sent": "If so, again we have this.",
                    "label": 0
                },
                {
                    "sent": "We have this Hilbert space age.",
                    "label": 0
                },
                {
                    "sent": "And this Hilbert space basically consists of all sorts of linear combinations of this bump functions.",
                    "label": 0
                },
                {
                    "sent": "Really infinite linear combinations.",
                    "label": 0
                },
                {
                    "sent": "Now what is the reproducing property reproducing property of the following?",
                    "label": 0
                },
                {
                    "sent": "Suppose I have a function F in this space.",
                    "label": 0
                },
                {
                    "sent": "This space, of course, is the Hilbert space, meaning it has some inner product.",
                    "label": 0
                },
                {
                    "sent": "An inner product is just well for two vectors.",
                    "label": 0
                },
                {
                    "sent": "It's like you know, like angle well angle times lags.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It turns, I mean that that is somehow almost the definition of edge that F of X if the product K of X and I put the dot here to.",
                    "label": 0
                },
                {
                    "sent": "This is a function of this parameter.",
                    "label": 0
                },
                {
                    "sent": "And F so KX Dot is a function of this open.",
                    "label": 0
                },
                {
                    "sent": "Functional, this can be Y and this is F of Y.",
                    "label": 0
                },
                {
                    "sent": "So there this is a product on two functions KX Dot NF.",
                    "label": 0
                },
                {
                    "sent": "Well, if you wish, you can write that Doc here.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is F of X, so this is really the why.",
                    "label": 0
                },
                {
                    "sent": "The things work and somehow point evaluations are represented by kernels in some sense.",
                    "label": 0
                },
                {
                    "sent": "In terms of freeze representation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now I'm actually going to prove.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Represent Ethereum because it's such a fundamental thing that I think it's important to do the proof, it's.",
                    "label": 0
                },
                {
                    "sent": "A simple proof, but it has to be.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me prove it.",
                    "label": 0
                },
                {
                    "sent": "So let me just remind you what I'm trying to prove.",
                    "label": 0
                },
                {
                    "sent": "I'm minimizing this.",
                    "label": 0
                },
                {
                    "sent": "And I want to show that the solution is the sum of the functions over data points.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So consider as to be the span of the bump functions over data points.",
                    "label": 0
                },
                {
                    "sent": "And I want to show that my solution to the minimizer light so spam in all possible linear combinations of those guys and I want to show that my solution will lie in this space.",
                    "label": 0
                },
                {
                    "sent": "That's what I'm trying to show.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to show that there is some linear combinations of this guy.",
                    "label": 0
                },
                {
                    "sent": "Said that my solution to the minimization problem will lie here.",
                    "label": 0
                },
                {
                    "sent": "So first, let us notice that if F. Is in the Hilbert space an F is perpendicular to S, then that means that F of X I = 0.",
                    "label": 0
                },
                {
                    "sent": "And other ways also true?",
                    "label": 0
                },
                {
                    "sent": "And why is this?",
                    "label": 0
                },
                {
                    "sent": "This is because of the reproducing property.",
                    "label": 0
                },
                {
                    "sent": "Remember this is F of XI.",
                    "label": 0
                },
                {
                    "sent": "This is the reproducing property of the kernel.",
                    "label": 0
                },
                {
                    "sent": "This guy is F of XI.",
                    "label": 0
                },
                {
                    "sent": "If FKXI dot equals zero, that means that the inner product equals zero, therefore they are perpendicular.",
                    "label": 0
                },
                {
                    "sent": "So having this product is 0.",
                    "label": 0
                },
                {
                    "sent": "Is the same as having F of XI being there?",
                    "label": 0
                },
                {
                    "sent": "So they perpendicular F is perpendicular to this guy KXI dot if and only if F of X = 0.",
                    "label": 0
                },
                {
                    "sent": "I hope please ask questions if this is unclear.",
                    "label": 0
                },
                {
                    "sent": "So now suppose F of XI is zero at each data point XI.",
                    "label": 0
                },
                {
                    "sent": "So you have a function which is 0 on each data point.",
                    "label": 1
                },
                {
                    "sent": "Then such a function would have to be perpendicular to each one of these guys K of XI.",
                    "label": 0
                },
                {
                    "sent": "If it is perpendicular to each one of these guys, it is certainly perpendicular to this path.",
                    "label": 0
                },
                {
                    "sent": "Right, if you have a vector which is perpendicular to listen to that, then it will be perpendicular to the whole plane.",
                    "label": 0
                },
                {
                    "sent": "Spain by spanned by this to vectors.",
                    "label": 0
                },
                {
                    "sent": "So on the other hand, if you have.",
                    "label": 0
                },
                {
                    "sent": "Something which is perpendicular to each one of this.",
                    "label": 0
                },
                {
                    "sent": "Then the value is 0.",
                    "label": 0
                },
                {
                    "sent": "So basically F is perpendicular to S if and only if it takes value 0 at each data point.",
                    "label": 0
                },
                {
                    "sent": "So this is claim one.",
                    "label": 0
                },
                {
                    "sent": "This is the first.",
                    "label": 0
                },
                {
                    "sent": "State.",
                    "label": 0
                },
                {
                    "sent": "And again, please ask questions.",
                    "label": 0
                },
                {
                    "sent": "In some sense this is very elementary, but in some other sense it think it takes some time to see it.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is a subspace of H&H, is just some sort of linear space.",
                    "label": 0
                },
                {
                    "sent": "OK, it's infinite dimensional, but doesn't matter so much.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you can find the complement to this, and again this is.",
                    "label": 0
                },
                {
                    "sent": "You'll have to be a bit careful, but it's basically true, so there is.",
                    "label": 0
                },
                {
                    "sent": "You just take this space and you take the orthogonal complement to that.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "So S is remember S are all possible linear combinations of this bump functions.",
                    "label": 0
                },
                {
                    "sent": "And then you just look at what sort of vectors are toggle to that.",
                    "label": 0
                },
                {
                    "sent": "And there is some sort of orthogonal decomposition.",
                    "label": 0
                },
                {
                    "sent": "Now if F. If F, of course, and there's a summit can be represented.",
                    "label": 0
                },
                {
                    "sent": "So if you have an orthogonal, if you have an orthogonal decomposition of your space, any F can be represented as two components by project.",
                    "label": 0
                },
                {
                    "sent": "So this is my orthogonal decomposition.",
                    "label": 0
                },
                {
                    "sent": "These two vectors.",
                    "label": 0
                },
                {
                    "sent": "Now if I have a vector like this, it's can be represented as a sum of this device.",
                    "label": 0
                },
                {
                    "sent": "Well, really this time.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a basic fact from linear algebra, so you don't have to worry about this.",
                    "label": 0
                },
                {
                    "sent": "Space is being infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "The intuition basically almost exactly carried through, but sometimes you have to be a little careful, but you can basically.",
                    "label": 0
                },
                {
                    "sent": "More often than not, you can rely on your intuition.",
                    "label": 0
                },
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "What is F perpendicular F perpendicular is a space of functions which are perpendicular to H. But with or before two.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry perpendicular to S, but we thought before that F is perpendicular to S if and only if F takes zeros on all data points.",
                    "label": 0
                },
                {
                    "sent": "So anything which is in F perpendicular will take zeros on all data points.",
                    "label": 0
                },
                {
                    "sent": "So that means that F of X I = F, F of XI because it's FF XI plus F perpendicular vex I.",
                    "label": 0
                },
                {
                    "sent": "But this is zero and all data points.",
                    "label": 0
                },
                {
                    "sent": "So for any data point F of X = F F of XI.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK. Oh, that's claim one.",
                    "label": 0
                },
                {
                    "sent": "I hope you believe this.",
                    "label": 0
                },
                {
                    "sent": "Now FS of H. Let's compare this to F of H. So there are two vectors and you know this is the usual Pythagorean theorem saying that F ^2 + F perpendicular squared equals F. Squared right, because when you have two vectors, when you sum up the length of this is the sum, the squared length of this is the sum of sqrt 4 right triangle?",
                    "label": 0
                },
                {
                    "sent": "Square length of the hypotenuse is the sum square of squared sites.",
                    "label": 0
                },
                {
                    "sent": "This is exactly that well, so that just means that the hypotenuse is longer in each of the sites.",
                    "label": 0
                },
                {
                    "sent": "That's what this is OK so.",
                    "label": 0
                },
                {
                    "sent": "There are two facts.",
                    "label": 0
                },
                {
                    "sent": "One is F of X = F. SFS of XI and the other one is FSH is shorter than F. So that's basically enough.",
                    "label": 0
                },
                {
                    "sent": "Now what are we trying to show?",
                    "label": 0
                },
                {
                    "sent": "We were trying to show that this.",
                    "label": 0
                },
                {
                    "sent": "Is minimized by something in FS right?",
                    "label": 0
                },
                {
                    "sent": "We were trying to show that the minimize of this lies in this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of Bob functions on data points.",
                    "label": 0
                },
                {
                    "sent": "Suppose it's not.",
                    "label": 0
                },
                {
                    "sent": "Then there is then this.",
                    "label": 0
                },
                {
                    "sent": "So F started the solution to this.",
                    "label": 0
                },
                {
                    "sent": "Write it as F S + F perpendicular.",
                    "label": 0
                },
                {
                    "sent": "Then for each point F star of XI is equal to FF of XI, write becausw ffxi's with her before has the same value as FF star of XII.",
                    "label": 0
                },
                {
                    "sent": "Hope you remember that this fact.",
                    "label": 0
                },
                {
                    "sent": "So then the loss function is the same because it only depends on those values.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we make that thing shorter for this.",
                    "label": 0
                },
                {
                    "sent": "So this guy is the same for FS and this is at least not longer.",
                    "label": 0
                },
                {
                    "sent": "For sure, maybe maybe smaller.",
                    "label": 0
                },
                {
                    "sent": "So by going from F star to F star with decreased, this whole sum by leaving this first part to be exactly the same as before and making this possibly shorter.",
                    "label": 0
                },
                {
                    "sent": "So OK, so that means that when for this thing it became smaller, but it cannot be a smaller because it's already the minimum.",
                    "label": 0
                },
                {
                    "sent": "So that is therefore the minimum Haskell I in.",
                    "label": 0
                },
                {
                    "sent": "This space.",
                    "label": 0
                },
                {
                    "sent": "In S, so the minimum is actually the sum of bumps.",
                    "label": 0
                },
                {
                    "sent": "Again, this is it's.",
                    "label": 0
                },
                {
                    "sent": "Important argument, because this is actually what makes us algorithms work.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So finally OK, this is just basically repeating it F styles and this if styles and S it's a linear combination of the bump functions, yeah?",
                    "label": 0
                },
                {
                    "sent": "A form of F so you can.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as being Gaussians E to the X I -- X K squared.",
                    "label": 0
                },
                {
                    "sent": "Oh so yeah, maybe that's it's a good point.",
                    "label": 0
                },
                {
                    "sent": "Thanks so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I should say that very directly.",
                    "label": 0
                },
                {
                    "sent": "So for the Gaussian so K. XY equals.",
                    "label": 0
                },
                {
                    "sent": "Let me just say ether minus X -- Y.",
                    "label": 0
                },
                {
                    "sent": "Square.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can just think about it being just numbers.",
                    "label": 0
                },
                {
                    "sent": "Or they can be vectors, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Let me just make some simple real numbers for make it as simple as possible.",
                    "label": 0
                },
                {
                    "sent": "Now what is the norm?",
                    "label": 0
                },
                {
                    "sent": "So K. X.",
                    "label": 0
                },
                {
                    "sent": "Dot so this is a function of the dot X is fixed.",
                    "label": 0
                },
                {
                    "sent": "And Ky dot.",
                    "label": 0
                },
                {
                    "sent": "Again, this is a function of the dot, not off.",
                    "label": 0
                },
                {
                    "sent": "Why is fixed?",
                    "label": 0
                },
                {
                    "sent": "I fixed X and I fixed why this had two functions.",
                    "label": 0
                },
                {
                    "sent": "Now I want to compute the inner product in the corresponding space and this is just K of XY.",
                    "label": 0
                },
                {
                    "sent": "So this is the definition of the product for this bump functions.",
                    "label": 0
                },
                {
                    "sent": "That's what the product is.",
                    "label": 0
                },
                {
                    "sent": "And you have to play a little bit with this to make sure that somehow it makes sense, yeah?",
                    "label": 0
                },
                {
                    "sent": "Right, so basically with when you have three guys.",
                    "label": 0
                },
                {
                    "sent": "What happens is that because they are quite far K of X -- y is close to 0, so there almost octogonal.",
                    "label": 0
                },
                {
                    "sent": "Then you just have a three of orthogonal work.",
                    "label": 0
                },
                {
                    "sent": "Three are talking about.",
                    "label": 0
                },
                {
                    "sent": "Then it's just Pythagorean theorem saying that it's 1 + 1 + 1 square root.",
                    "label": 0
                },
                {
                    "sent": "So if you have any bumps and they are quite far from each other, you just get square root of M as you're not.",
                    "label": 0
                },
                {
                    "sent": "This is just an intuition.",
                    "label": 0
                },
                {
                    "sent": "You have to be, but it's I think it's a good intuition.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this is this is a case for the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's now talk a little.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit about the algorithms, so here is the first algorithm.",
                    "label": 0
                },
                {
                    "sent": "Take F to be this more specific thing.",
                    "label": 0
                },
                {
                    "sent": "I just take least squares fit, so at each point I just fit the IF of X, y -- Y.",
                    "label": 0
                },
                {
                    "sent": "Plus I add this penalty so it's basically the same as before.",
                    "label": 0
                },
                {
                    "sent": "I just choose the loss function to be square.",
                    "label": 0
                },
                {
                    "sent": "I know that the solution is of this form.",
                    "label": 0
                },
                {
                    "sent": "Right, that's what I showed.",
                    "label": 0
                },
                {
                    "sent": "That is the representative.",
                    "label": 0
                },
                {
                    "sent": "Now, what exactly is offers?",
                    "label": 0
                },
                {
                    "sent": "I can actually write your formula so the vector file?",
                    "label": 0
                },
                {
                    "sent": "So if So what do you do?",
                    "label": 0
                },
                {
                    "sent": "You take your vectors of wise and you left multiply it by K plus Lambda I to the minus one.",
                    "label": 0
                },
                {
                    "sent": "So Lambda is N by N matrix and K is also end by a kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So K of IJKXXJ.",
                    "label": 0
                },
                {
                    "sent": "So it's just end by N matrix.",
                    "label": 0
                },
                {
                    "sent": "And what do I do?",
                    "label": 0
                },
                {
                    "sent": "I evaluate my Gaussian.",
                    "label": 0
                },
                {
                    "sent": "On pairs of points.",
                    "label": 0
                },
                {
                    "sent": "So XI, XJ, Z through the X you can think of this as just being.",
                    "label": 0
                },
                {
                    "sent": "The bump centered at.",
                    "label": 0
                },
                {
                    "sent": "You know it's a value of the if you center bump at point XY, what is the value of XJ?",
                    "label": 0
                },
                {
                    "sent": "That's what this is.",
                    "label": 0
                },
                {
                    "sent": "And well, this is now just the matrix multiplication problem, or as a division problem.",
                    "label": 0
                },
                {
                    "sent": "So this gives me Alphas and I have a function.",
                    "label": 0
                },
                {
                    "sent": "I just send this up.",
                    "label": 0
                },
                {
                    "sent": "So this is a very explicit solution and I get something which is.",
                    "label": 0
                },
                {
                    "sent": "Quite nice from this.",
                    "label": 0
                },
                {
                    "sent": "So this is an algorithm, and here is a demo.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, just see.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is a nice demo written.",
                    "label": 0
                },
                {
                    "sent": "By Mike Rainey.",
                    "label": 0
                },
                {
                    "sent": "Who is a student in CS?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What am I going to do?",
                    "label": 0
                },
                {
                    "sent": "Let me just pull up, let me just pull up two points to study.",
                    "label": 0
                },
                {
                    "sent": "So this class one so it can actually do something more, but I'll.",
                    "label": 0
                },
                {
                    "sent": "Talk about that later, not today.",
                    "label": 0
                },
                {
                    "sent": "So suppose I just have two points.",
                    "label": 0
                },
                {
                    "sent": "Now this is RBF radio such as the radius of that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "This is robotically Sigma.",
                    "label": 0
                },
                {
                    "sent": "So basically this is right now with 0.05.",
                    "label": 0
                },
                {
                    "sent": "And so this scale is 1, so it's about 1:20, so the bump looks like this maybe.",
                    "label": 0
                },
                {
                    "sent": "This is somehow how the bond looks like.",
                    "label": 0
                },
                {
                    "sent": "The size of the bumper about one point of the width.",
                    "label": 0
                },
                {
                    "sent": "So I have two points.",
                    "label": 0
                },
                {
                    "sent": "So what's my classifier going to be if I just have two points?",
                    "label": 0
                },
                {
                    "sent": "Yeah, any idea?",
                    "label": 0
                },
                {
                    "sent": "It's going to be a straight line, right?",
                    "label": 0
                },
                {
                    "sent": "I have two bumps and I just have linear combination of two identical bumps and that's going to be a straight line.",
                    "label": 0
                },
                {
                    "sent": "Well, let's see.",
                    "label": 0
                },
                {
                    "sent": "Hopefully yeah, so that's as expected.",
                    "label": 0
                },
                {
                    "sent": "What if I put Fe?",
                    "label": 0
                },
                {
                    "sent": "One more point here?",
                    "label": 0
                },
                {
                    "sent": "What is that going to be?",
                    "label": 0
                },
                {
                    "sent": "Huh?",
                    "label": 0
                },
                {
                    "sent": "It's going to be actually sort of like two straight lines.",
                    "label": 0
                },
                {
                    "sent": "Because what happens is that you have two Gaussians and the size of the Gaussian is quite small, so it basically the radius of the stone is somehow like the radius of the Gaussian that you have.",
                    "label": 0
                },
                {
                    "sent": "So it turns pretty sharp, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you can play with the thing it's actually online.",
                    "label": 0
                },
                {
                    "sent": "OK, let me just open the file.",
                    "label": 0
                },
                {
                    "sent": "OK, so here here are some points.",
                    "label": 0
                },
                {
                    "sent": "So red and blue and I would like to do some classification, so I would like to draw a boundary.",
                    "label": 0
                },
                {
                    "sent": "I would like to draw a boundary somehow to separate.",
                    "label": 0
                },
                {
                    "sent": "Red points from Blue Point, so I would say something like this might be pretty reasonable.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly clear where they are, but.",
                    "label": 0
                },
                {
                    "sent": "So let me see what happens.",
                    "label": 0
                },
                {
                    "sent": "So any idea what will happen?",
                    "label": 0
                },
                {
                    "sent": "Put it go like this.",
                    "label": 0
                },
                {
                    "sent": "What about this point?",
                    "label": 0
                },
                {
                    "sent": "So let me let me put so this is the gamma.",
                    "label": 0
                },
                {
                    "sent": "This is the regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "Remember how much I weigh that norm thing?",
                    "label": 0
                },
                {
                    "sent": "So let me put actually 30 here.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's pretty good, right?",
                    "label": 0
                },
                {
                    "sent": "That's more or less what you expect.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So what happens if I make this regularization parameter much smaller?",
                    "label": 0
                },
                {
                    "sent": "Remember regularization parameter is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regularization parameter is this guy is Lambda.",
                    "label": 0
                },
                {
                    "sent": "So what happens when I make it much much smaller?",
                    "label": 0
                },
                {
                    "sent": "What happens when I make this guy much smaller is that I don't care about the norm.",
                    "label": 0
                },
                {
                    "sent": "I can have very complex boundaries, but I want them to be fit very.",
                    "label": 0
                },
                {
                    "sent": "I want I want this thing to fit the data very well.",
                    "label": 0
                },
                {
                    "sent": "That's also known as overfitting.",
                    "label": 0
                },
                {
                    "sent": "So I want to fit it to data very well and I don't care about the boundary.",
                    "label": 0
                },
                {
                    "sent": "So let me let me make it.",
                    "label": 0
                },
                {
                    "sent": "01 no.",
                    "label": 0
                },
                {
                    "sent": "Why doesn't it work?",
                    "label": 0
                },
                {
                    "sent": "OK, now it works.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 0
                },
                {
                    "sent": "OK, so you see what happens now.",
                    "label": 0
                },
                {
                    "sent": "Basically the global boundaries somehow doesn't change too much here, but what happens is that all of these guys, they get their own little boundary.",
                    "label": 0
                },
                {
                    "sent": "So now you're fitting every point.",
                    "label": 0
                },
                {
                    "sent": "Basically, if it says read that Red has to be correct.",
                    "label": 0
                },
                {
                    "sent": "So this is exactly what overfitting is using.",
                    "label": 0
                },
                {
                    "sent": "Well, this point is just probably some sort of noise really, here everything is blue.",
                    "label": 0
                },
                {
                    "sent": "I don't think you want to classify this as red, that's a mistake and hear the same thing.",
                    "label": 0
                },
                {
                    "sent": "And I don't know here.",
                    "label": 0
                },
                {
                    "sent": "Maybe, maybe not.",
                    "label": 0
                },
                {
                    "sent": "But basically what happens when you put this parameter to be quite low.",
                    "label": 0
                },
                {
                    "sent": "You start getting very complex boundary, but every point is classified correctly from your data set.",
                    "label": 0
                },
                {
                    "sent": "But there is no reason to think that when you get a new point here, it won't be.",
                    "label": 0
                },
                {
                    "sent": "It won't be read it probably in all likelihood will be read so you are not doing very well here.",
                    "label": 0
                },
                {
                    "sent": "You were doing much better with the higher parameter gamma, so here you are allowing more complex functions and you get better fit to the existing data.",
                    "label": 0
                },
                {
                    "sent": "But you probably will get worse fit to new data points.",
                    "label": 1
                },
                {
                    "sent": "OK so this is regularised least squares and OK.",
                    "label": 0
                },
                {
                    "sent": "So another thing to emphasize here is that of course this is a very standard matrix inversion problem and you can solve a lot of problems like this.",
                    "label": 0
                },
                {
                    "sent": "It's it works very well in many problems like you can use it for digital classification and all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "It works very often, works very well.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Why is this a good classifier?",
                    "label": 0
                },
                {
                    "sent": "The intuition if.",
                    "label": 0
                },
                {
                    "sent": "The intuition is the following.",
                    "label": 0
                },
                {
                    "sent": "This thing penalizes function which will basically.",
                    "label": 0
                },
                {
                    "sent": "So this is going to produce a function which is pretty nice and yet fits the data well.",
                    "label": 0
                },
                {
                    "sent": "That's why it's a good classifier.",
                    "label": 0
                },
                {
                    "sent": "So somehow this penalty penalizes functions which change too much.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it answers your question or not.",
                    "label": 0
                },
                {
                    "sent": "Why is it a good classifier?",
                    "label": 0
                },
                {
                    "sent": "Right, well, of course it depends on Lambda, right?",
                    "label": 0
                },
                {
                    "sent": "But basically what you want, you want to penalize functions with complicated behavior, and this is what weather does.",
                    "label": 0
                },
                {
                    "sent": "Of course, it's a very simplistic algorithm and in general.",
                    "label": 0
                },
                {
                    "sent": "Do you often want to do something more sophisticated than that?",
                    "label": 0
                },
                {
                    "sent": "But it's a very simple algorithm and it worked quite well over.",
                    "label": 0
                },
                {
                    "sent": "A wide variety of tasks, but you.",
                    "label": 0
                },
                {
                    "sent": "I mean you have to see that correct data.",
                    "label": 0
                },
                {
                    "sent": "Also, if you figure some random stuff, it won't work.",
                    "label": 0
                },
                {
                    "sent": "And this is actually very similar to support vector machines.",
                    "label": 0
                },
                {
                    "sent": "I'll explain what support vector machines are in the kernel case in a second.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Gotta support vector machine, so as you saw before, support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So now of course I am in this more general setting.",
                    "label": 0
                },
                {
                    "sent": "So suppose vector machines are the following support vector machines is.",
                    "label": 1
                },
                {
                    "sent": "Instead of penalizing the squared fit, you're penalized this function, and this is the following function.",
                    "label": 0
                },
                {
                    "sent": "If F basically has the right sign.",
                    "label": 0
                },
                {
                    "sent": "Ann, it's at least where value is at least one.",
                    "label": 0
                },
                {
                    "sent": "Then this doesn't produce any penalty.",
                    "label": 0
                },
                {
                    "sent": "So suppose why is plus one then if F is bigger than one, then this guy is going to be bigger than one 1 minus something which is bigger than one is going to be negative.",
                    "label": 0
                },
                {
                    "sent": "When I take the + plus positive part of it, it's going to be I mean.",
                    "label": 0
                },
                {
                    "sent": "Plus just means thick, zero if it's less than zero and take the same thing if it's positive.",
                    "label": 0
                },
                {
                    "sent": "So for if F of X is bigger than one in, why is 1?",
                    "label": 0
                },
                {
                    "sent": "Then it's fine by the same talking if F of X is less than one.",
                    "label": 0
                },
                {
                    "sent": "And why?",
                    "label": 0
                },
                {
                    "sent": "If lesson minus one and why?",
                    "label": 0
                },
                {
                    "sent": "I is minus one so why I can only have two values here 1 N minus one?",
                    "label": 0
                },
                {
                    "sent": "Well, you can have more, but that's the classification.",
                    "label": 0
                },
                {
                    "sent": "Then this is fine.",
                    "label": 0
                },
                {
                    "sent": "So basically this doesn't penalize you if you have the correct value, but this value needs to have some margin.",
                    "label": 0
                },
                {
                    "sent": "And so this is your loss function and this is.",
                    "label": 0
                },
                {
                    "sent": "What is this?",
                    "label": 1
                },
                {
                    "sent": "This is the same penalty as before.",
                    "label": 0
                },
                {
                    "sent": "And actually, I showed you the rigorously squares because I don't have a demo like that for SPMS, but you would see something very similar in terms of.",
                    "label": 0
                },
                {
                    "sent": "In terms of the boundaries and solutions, and then this, this is an algorithm which has been exceedingly popular in the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "There are literally thousands of papers on this.",
                    "label": 0
                },
                {
                    "sent": "And this was introduced by Wapnick.",
                    "label": 0
                },
                {
                    "sent": "OK, so now this is slightly more complicated, optimizing the other one, the other one was just in matrix inversion.",
                    "label": 0
                },
                {
                    "sent": "This is some quadratic optimization problem, but it's still it can be done quite fast and this leads to a quadratic programming problem, so you can optimize this guy and you just have to choose alphas correctly to minimize this.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what's interesting about this, so this has one big advantage of regularised this squares, and this is sparsity.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you have this somewhere.",
                    "label": 0
                },
                {
                    "sent": "Your solution is a sum of bump functions over each data point.",
                    "label": 0
                },
                {
                    "sent": "However, maybe you have 100,000 data points.",
                    "label": 0
                },
                {
                    "sent": "Then you don't want to actually compute each one of them.",
                    "label": 0
                },
                {
                    "sent": "It can be very.",
                    "label": 0
                },
                {
                    "sent": "Complicated suppose you want to evaluate this function at a new point and you have to compute each of 100,000 of those guys, which can be quite computationally inefficient, so support vector machines has something called sparsity, which just means that more often, most of the things are zero.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so here is an example.",
                    "label": 0
                },
                {
                    "sent": "You have some blue points and you have some red points.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Again, I want the classification, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification boundary, so I find this by minimizing as before my optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So this is my F = 0, so F everything positive is classified as read, everything negative is classified as blue.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that I can consider this now 2 lines.",
                    "label": 0
                },
                {
                    "sent": "This is F = 1.",
                    "label": 0
                },
                {
                    "sent": "So this is a level one whenever equals one and this is F = -- 1.",
                    "label": 0
                },
                {
                    "sent": "OK, now it turns out.",
                    "label": 0
                },
                {
                    "sent": "That in this so this expansion of course give me gives me a bump over each data point.",
                    "label": 0
                },
                {
                    "sent": "So I have imagined this function is like I have here little Gaussian here, little Gaussian here, little Gaussian, and so on, and I send them with some coefficients.",
                    "label": 0
                },
                {
                    "sent": "It turns out for the support vector machines, the only point which are non 0 here.",
                    "label": 1
                },
                {
                    "sent": "All this so I hope you can see them there.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have black circles around them and what are they?",
                    "label": 0
                },
                {
                    "sent": "Their points?",
                    "label": 0
                },
                {
                    "sent": "We chat somehow within the margin of the boundary.",
                    "label": 0
                },
                {
                    "sent": "And this margin is this distance between F being equals to minus one and F = 1.",
                    "label": 0
                },
                {
                    "sent": "So this is a boundary of specification.",
                    "label": 0
                },
                {
                    "sent": "This is somehow this margin size.",
                    "label": 0
                },
                {
                    "sent": "An all of this points have non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "Plus points which are misclassified.",
                    "label": 0
                },
                {
                    "sent": "So this that and at this point are clearly misclassified, so they also become support vectors.",
                    "label": 0
                },
                {
                    "sent": "So instead of having this expansions over all points, you only get expansion of a point.",
                    "label": 0
                },
                {
                    "sent": "Some of the points which are the support vectors and these are like close to the boundary or are misclassified and potentially there could be a lot fewer of these guys then the total number of points.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an interesting thing.",
                    "label": 0
                },
                {
                    "sent": "So this actually the sparsity.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by sparsity?",
                    "label": 0
                },
                {
                    "sent": "By sparsity I mean how many of the coefficients in that expansion unknown 0.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this is closely related to the classification error of the base optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "And remember that the Bayes optimal classifier is the best possible classifier.",
                    "label": 1
                },
                {
                    "sent": "Suppose you know everything about the data and you have to draw the boundary somehow.",
                    "label": 0
                },
                {
                    "sent": "You still make mistakes because you know the data has noise, so that thing will still maybe get it wrong 10% of the time.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that if the Bayes optimal classifier makes errors 10% of the time, then what you get, roughly speaking, you would get that.",
                    "label": 0
                },
                {
                    "sent": "Expansion.",
                    "label": 0
                },
                {
                    "sent": "10% of this guys are present and the rest is, so the number of this guys is roughly speaking the Bayes error.",
                    "label": 0
                },
                {
                    "sent": "It's not quite true because of this margin business, but it's very close to the base error.",
                    "label": 1
                },
                {
                    "sent": "So that's an interesting thing about support vector machines, and often they are more efficient than regularised least squares in terms of performance is still there quite similar, sometimes, maybe better, but that's not so the common wisdom are that they work better in terms of performance, but it doesn't seem to be the case.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But there they are.",
                    "label": 0
                },
                {
                    "sent": "They have this nice sparsity property.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will mention a few things about the future map.",
                    "label": 0
                },
                {
                    "sent": "And this is the way this ideas are often presented in terms of this feature map and what is the feature map.",
                    "label": 0
                },
                {
                    "sent": "Remember feature map you map your data to some high dimensional space and you do an ordinary linear classification in that space.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I map accent to H&H is some sort of high dimensional infinite dimensional vector space and then I use linear functions on H. To evaluate to.",
                    "label": 0
                },
                {
                    "sent": "Find functions on X to find to fit data on X.",
                    "label": 0
                },
                {
                    "sent": "So an if H is very high dimensional then of course the family, the family of linear functions are dementia.",
                    "label": 0
                },
                {
                    "sent": "Knowledge of that family is just the dimensionality or the number of parameters of linear functions is the dimensionality of your space 'cause they're controlled by W and.",
                    "label": 0
                },
                {
                    "sent": "So if your space, for example is infinite dimensional, well then it's infinitely rich.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So what is the future map assault?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Native to the kernel, so one of this if basically the following.",
                    "label": 0
                },
                {
                    "sent": "So you can you can have X. Map 28 an so X is your data space in you map it to H&H is as usual, reproducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "An X goes to KX dot, so for each point X.",
                    "label": 0
                },
                {
                    "sent": "Your representation is the actual bump is the function which is above.",
                    "label": 0
                },
                {
                    "sent": "So you map from your data space to function space in your image of the what point goes to is actually a function, which is a bump centered at that point.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that you can rewrite this.",
                    "label": 0
                },
                {
                    "sent": "Now this is of course a Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "The linear space with an inner product now.",
                    "label": 0
                },
                {
                    "sent": "This was the problem.",
                    "label": 0
                },
                {
                    "sent": "We considered all the time I take squares, but you can take other things, doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So y -- F of XI.",
                    "label": 0
                },
                {
                    "sent": "Plus, Lambda times the norm.",
                    "label": 0
                },
                {
                    "sent": "Now what is F of XIF of XI?",
                    "label": 0
                },
                {
                    "sent": "If you recall the reproducing property, it's just this inner product.",
                    "label": 0
                },
                {
                    "sent": "It's in the product key of XI dot NF.",
                    "label": 0
                },
                {
                    "sent": "This is the reproducing property and K of excise side.",
                    "label": 0
                },
                {
                    "sent": "OK so I can rewrite this at that and this is of course exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "Well, now what is this?",
                    "label": 0
                },
                {
                    "sent": "This is ordinary least squares fitting.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what this query feeling is.",
                    "label": 0
                },
                {
                    "sent": "And when I do this list perfect, so I get the algorithm for regularised least squares.",
                    "label": 0
                },
                {
                    "sent": "So consider this if you remember what least squares was before you have linear space you have WS your vector and you have this linear thing which is www.xlw.xbecomes.",
                    "label": 0
                },
                {
                    "sent": "This times F right?",
                    "label": 0
                },
                {
                    "sent": "And why I is still why I and this is?",
                    "label": 0
                },
                {
                    "sent": "Norm F which is so F becomes W. OK, so this is somehow the feature map interpretation, so that's one way people think about kernels that somehow you map your space into an infinite dimensional space and you do some sort of.",
                    "label": 0
                },
                {
                    "sent": "Whatever you do, linear regression or linear linear least squares, whatever support vector machines in that space.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So maybe I'll just very briefly mention this.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what if?",
                    "label": 0
                },
                {
                    "sent": "What is empirical error?",
                    "label": 0
                },
                {
                    "sent": "So recall that the empirical error is basically suppose I have a classifier F. Empirical error is how well I do on my training set.",
                    "label": 0
                },
                {
                    "sent": "So on my training set on the points which I already have, I might do zero.",
                    "label": 0
                },
                {
                    "sent": "I might do whatever.",
                    "label": 0
                },
                {
                    "sent": "So this is somehow I take my class.",
                    "label": 0
                },
                {
                    "sent": "If I evaluate it on the data I have and then this is I get some number and this is how well I don't the training set now what is generalization error generalization error is basically consider all possible.",
                    "label": 0
                },
                {
                    "sent": "Now all the space of all possible data points and how well do I do on that?",
                    "label": 0
                },
                {
                    "sent": "So this is expectation over all possible data of how well I don't.",
                    "label": 0
                },
                {
                    "sent": "So this is really what I want.",
                    "label": 0
                },
                {
                    "sent": "I want this to be as low as possible.",
                    "label": 0
                },
                {
                    "sent": "So for classification the Bayes optimal minimizes this quantity.",
                    "label": 0
                },
                {
                    "sent": "So basically I want to make sure that my classifier given any possible data point does as well as it can possibly do.",
                    "label": 1
                },
                {
                    "sent": "However, I cannot really observe that because I don't know what I suppose I want to distinguish, say, images of seven from images of 9th.",
                    "label": 0
                },
                {
                    "sent": "I have a bunch of examples, but I don't know what this is.",
                    "label": 0
                },
                {
                    "sent": "I don't know what all possible nines and sevens are.",
                    "label": 0
                },
                {
                    "sent": "I have to know that to be able to evaluate that.",
                    "label": 0
                },
                {
                    "sent": "So really, what I would like to say, I would like to say that if I have a procedure such that the empirical error is low, so I do quite well on my training set.",
                    "label": 0
                },
                {
                    "sent": "And this difference is law.",
                    "label": 0
                },
                {
                    "sent": "Then I'm in good shape if the empirical error is low on its own, that means nothing.",
                    "label": 0
                },
                {
                    "sent": "I can overfit.",
                    "label": 0
                },
                {
                    "sent": "I can have a crazy function which goes which fits my point exactly.",
                    "label": 0
                },
                {
                    "sent": "This will be 0.",
                    "label": 0
                },
                {
                    "sent": "This can still be huge, so I want to have something which does very well on the training data and doesn't overfit.",
                    "label": 0
                },
                {
                    "sent": "That's almost as well everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So I really want to estimate this difference.",
                    "label": 0
                },
                {
                    "sent": "This is what people call generalization Gap, so here is.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Assume the theorem.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "'cause I call it a theorem in quotes because it's incorrect.",
                    "label": 0
                },
                {
                    "sent": "Basically it's saying the following, it's saying.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "Don't don't think this is an actual theme, I just want to show you what sort of statement you can make about this.",
                    "label": 0
                },
                {
                    "sent": "As I say them, it's completely wrong.",
                    "label": 0
                },
                {
                    "sent": "But the actual statement is much more complicated, so you are F is the minimizer of this guy.",
                    "label": 0
                },
                {
                    "sent": "Then what can I say?",
                    "label": 1
                },
                {
                    "sent": "About this generalization gap for a function which I which is produced by my favorite kernel method.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can say basically the following, that you can say that it's something like that.",
                    "label": 0
                },
                {
                    "sent": "It's something like one over Lambda over square root of N. So for example, if I fix Lambda and N goes to Infinity then this will tend to zero as an square root of N. Now if my Lambda becomes very small then you can see that I have now bound right when Lambda is very small.",
                    "label": 0
                },
                {
                    "sent": "This is very large, so this whole thing explodes.",
                    "label": 0
                },
                {
                    "sent": "I have nothing.",
                    "label": 0
                },
                {
                    "sent": "So that means if my Lambda is very small, that means that I'm overfitting.",
                    "label": 0
                },
                {
                    "sent": "Right when Lambda is very, very close to zero, I don't pay any price for making this function complex, so I will fill any data and then I will.",
                    "label": 0
                },
                {
                    "sent": "This bound tells me that.",
                    "label": 0
                },
                {
                    "sent": "Well, basically I cannot say anything about this difference when Lambda is very large and at the hands it's quite small, but when Lambda is very large, I'm underfitting I'm making a function which is nice, but we will probably have bad empirical error.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of things and you will see them things like that tomorrow.",
                    "label": 0
                },
                {
                    "sent": "I think in the day after and.",
                    "label": 0
                },
                {
                    "sent": "So this is the sort of theoretical statement which can be made about this.",
                    "label": 0
                },
                {
                    "sent": "One of them.",
                    "label": 0
                },
                {
                    "sent": "So ultimately what you want to show you want to show that for your method this is small and the difference is small.",
                    "label": 1
                },
                {
                    "sent": "Then you're in good shape.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think that's.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty much.",
                    "label": 0
                },
                {
                    "sent": "What I had to say.",
                    "label": 0
                },
                {
                    "sent": "So here are some references.",
                    "label": 0
                },
                {
                    "sent": "It's nowhere close to the complete list, but.",
                    "label": 0
                },
                {
                    "sent": "This some of this may be useful.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think John wanted to make some announcement.",
                    "label": 0
                },
                {
                    "sent": "Where is he?",
                    "label": 0
                },
                {
                    "sent": "I guess there is no announcement then.",
                    "label": 0
                },
                {
                    "sent": "Hey John.",
                    "label": 0
                },
                {
                    "sent": "They want to make an announcement.",
                    "label": 0
                }
            ]
        }
    }
}