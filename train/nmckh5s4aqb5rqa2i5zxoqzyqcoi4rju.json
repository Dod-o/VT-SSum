{
    "id": "nmckh5s4aqb5rqa2i5zxoqzyqcoi4rju",
    "title": "Spatial Bayesian Nonparametrics for Natural Image Segmentation",
    "info": {
        "author": [
            "Erik Sudderth, Brown University"
        ],
        "published": "Jan. 24, 2012",
        "recorded": "December 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2011_sudderth_segmentation/",
    "segmentation": [
        [
            "Alright, so I'm going to be talking about image segmentation.",
            "And."
        ],
        [
            "Our goal, sort of the long term goal behind this kind of work is to try to do in an automatic way what a person has done here, right?",
            "So they've taken images.",
            "They've labeled the sort of outline the objects in the image.",
            "That's called segmentation.",
            "In this case, they've also assigned some sort of semantic labels to what they are.",
            "OK, so the."
        ],
        [
            "Widely sort of.",
            "If you look at contemporary papers, the most sort of common way people look at this is in terms of supervised learning problem.",
            "So people define some small number of categories.",
            "They're interested in sort of get some labeled data where those are labeled, and then try to sort of build a classifier.",
            "This is a pretty.",
            "This is a sample result.",
            "This is from about four years ago of what I think is actually very good work.",
            "They got all the technical details right, but the final story was a little disappointing where they had a very simple sort of classifier that basically looked at little patches of the image and classified them independently.",
            "Tawanna 20 classes that got 74% accuracy, then they added a Markov random field.",
            "Prior.",
            "Did loopy belief propagation training and inference and so forth, and it went from 74 to 78%.",
            "And you can see that basically it's you know it's just kind of smoothing over a little bit of the noise.",
            "It's not really correcting any major errors, so this is disappointing.",
            "For building what we'd like to be a sort of fancy spatial model, and so we'd like to try to.",
            "I would.",
            "I'm interested in understanding, so why don't we get more benefit out of these things?",
            "And one way of understanding why you don't get more benefit out of this is to say, well, what's happening is that my sort of performance of my spatial model being propped up by having a good classifier to start with, and so maybe to try to understand why it's good and bad I can."
        ],
        [
            "Go to sort of something where I have less supervision and this is come to the problem of not object recognition, but segmentation and so this is.",
            "These are examples of where you these are.",
            "Each row is multiple people taking this image and their instructions or something like just outline the objects that are interested in the scene out on the things that are interesting.",
            "And of course there's variability from person to person, but what's interesting is that there's also a remarkable amount of agreement between people in terms of what they think are the sort of salient boundaries, so this is actually a problem you can study in its own right.",
            "This is taking a natural image, sort of identifying the."
        ],
        [
            "Object key objects there, and I think there's been a data set set up on and sort of a whole series of methods and benchmarks and so on, so I'm going to be thinking about this sort of space of image segmentation problems, but from a nonparametric Bayesian perspective."
        ],
        [
            "Why'd Asian nonparametrics for segmentation?",
            "Well, I think it's a really nice fit, so segmentation is essentially a partitioning problem.",
            "We don't know how many regions the image contains and what their sizes are.",
            "We have a unit area image and we need to split it up right so priors on partitions are one of the sort of building blocks for Bayesian non parametrics.",
            "They fit right in here.",
            "Also, if you talk to sort of applied applied vision, people who want to use segmentation methods, they realized that this is sort of fundamentally ambiguous problem.",
            "There's no way I'm ever going to get a segmentation method that's perfect, so actually they would really like to have multiple segmentations, maybe ranked by probability from a low level method that could be used in a high level way, so that's a really sort of nice way sort of argument for Bayesian methods.",
            "There's also huge variability from image to image in terms of.",
            "There might be 2 objects here, a dozen there, so I can't just assume I know how many regions are clusters they're going to be OK, so that's kind of an argument.",
            "I think, for Nonparametrics.",
            "So great, so I don't know what's going to be so applied."
        ],
        [
            "These infinite models right?",
            "And so I've got a whole bunch of infinite models.",
            "There's been a whole series of these, and so so there's the hype aspect of this workshop.",
            "I think to some extent were partially to blame for this, right?",
            "You know, we come up with the infinite fancy widget, and as time goes on you have to add more words after infinite to get a new model that hasn't been done before, but you know, we're creative.",
            "We can keep doing that.",
            "But you know, I think there hasn't been enough understanding of OK for a particular data set.",
            "How can I understand, you know, is the way in which I'm doing my infinite model useful out of the?",
            "In fact, dozens of ways I could have chosen to make sort of a large flexible model.",
            "Which one should I have picked?",
            "So I'm going to sort of treat segmentation as."
        ],
        [
            "Case study for going through some of the issues that show up in that kind of problem.",
            "OK, so I'm going to talk about a model I've worked on.",
            "This is the model itself is now a few years old.",
            "It's based on it, makes dependent manual processes using the Gaussian process.",
            "Then I'm going to talk about some more recent work on inference and learning, as well as some results on image segmentation."
        ],
        [
            "OK.",
            "So there's lots of ways of thinking about the Pitman Yor process for the purposes of this talk, will think of it as a distribution on infinite discrete measures or partitions, and so one way of representing this with this stick breaking construction.",
            "You've probably all seen too many times where I have unit listica probability mass, I take a random proportion.",
            "That's the first part of the partition.",
            "Then I take a random proportion of what's left and a random proportion of what's left, and so on down the chain for the Pitman yor process.",
            "These are sampled from a sequence of beta distributions.",
            "There's two free parameters A&B.",
            "If I set one of those parameters to zero, I get the famous nearest late process.",
            "This is kind of a larger family that includes the Dirichlet process, special case."
        ],
        [
            "In Pitman, Yor indirectly process all of those stick proportions.",
            "I'm sampling to come from the same distribution and Pitman Yor.",
            "They vary so as I move down the chain from the 1st to the 10th of the 20th stick component, I go from this blue curve to the red to green.",
            "So actually they're getting smaller and smaller on average, so the DT is already in some sense sort of heavy tailed prime partitions, and the pit maneuver has heavier tails."
        ],
        [
            "Now to look at sort of whether this might be useful in understanding the segmentation problem, we looked at the data set of human segmentations, so this is a a site called label me set up by some colleagues at MIT where there's a web interface in which people can use create Polygon outlines of objects.",
            "They can also assign semantic labels.",
            "I won't be talking about that here.",
            "I looked at a subset of this that's about 29,000 segments in about 2700 images, so fairly large collection of data and you can ask things."
        ],
        [
            "About the basic statistics.",
            "So for example, how many objects or more generally, just how many of these regions are there that are above some size threshold, right?",
            "Say larger than 50 pixels?",
            "So there's an empirical histogram.",
            "That's that black curve in the data, the red and green are predictions of that.",
            "Pin your orders they process models would give where I've sort of optimized the hyperparameters of those models to be as close to this data as possible, so you can see you get and not awful fit from the Dirichlet process and are quite good match with pit manure.",
            "Also, we can look at the sizes of these things and what's interesting is if I take a histogram of object sizes and I'm going to put this on a log log scale, you get this sort of nearly straight line behavior.",
            "This is what people sometimes call a power law distribution where there's a few large objects and then lots of sort of very small objects, and this is just a sort of characteristic property of the world we live in."
        ],
        [
            "So why Pittman your this is Jim Pitman and Mark your they were interested in this process as sort of something that generalizes the Dursley process in lots of ways and unifies lots of things in probability.",
            "But for statistical applications, this sort of driving thing is that it gives us power law distributions.",
            "There's two complementary ways to see this.",
            "One is that I can say if I have in unique in observations, how many unique clusters do I expect to have seen a priore under the DP that grows logarithmically under Pitman yor that grows polynomially so this is.",
            "A is some number between zero and one.",
            "This is actually sort of a classic observation, and the text literature.",
            "This has a name, heaps law that if I just look at words and say how many unique vocabulary words do I sign as I read English text, it has this kind of growth.",
            "I can also say if I just take my sort of clusters or segments and I sort them in size from largest to smallest, how quickly does that decay for the rest?",
            "The process that's exponential for Pitman Yor that's polynomial and that's connected to zips along, so there's been some very nice work.",
            "Sort of pointing out that there's nice links between Pitman Yor.",
            "And natural language statistics and building sort of models based on top of that.",
            "The plots I just showed suggest that this is also a good fit for images."
        ],
        [
            "As an aside, you know so if you look at this sort of toy datasets, people like to use a machine learning papers, right?",
            "You know where these sort of like points in the plane and they show their clustering algorithm.",
            "They tend to have this flavor of, you know, there's a small number of clusters, and they're all roughly equal in size, and I think this people just kind of like symmetry.",
            "I think most real data doesn't really look anything like this, right?",
            "If you think about these power distributions, are there's some huge clusters in some very small clusters, right?",
            "And so the segmentation algorithms like normalized cut spectral clustering, has a bias that you want everything to be equal in size.",
            "Works great here.",
            "Works terribly for sort of image type things.",
            "So anyway, I think this is maybe some of our sort of opera intuition about what all the data we're going to find in the world looks like is not necessarily well calibrated.",
            "When we set up these toy validations."
        ],
        [
            "OK, so how am I going to apply this to images?",
            "Well, I'm going to extract some features.",
            "I'm going to sort of a lot of kind of machinery I can build on what we're going to do to reduce dimensionality is we're going to partition the image into about 1000 super pixels.",
            "You can think of these things as these little blob shape regions, and I try to do this in a way such that I hopefully don't merge anything real in the world.",
            "OK, and then I'm going to compute some descriptor of each of these, so we're doing something very basic sort of tried and true sort of texture and color histograms, about 100 bins for each, and so you're just counting the sort of some histogram represent."
        ],
        [
            "And of what the color and what the texture?",
            "So texture sort of an edginess measure how much energy there is at various orientations.",
            "OK, and so then the."
        ],
        [
            "Another way to sort of take that Pittman your model and apply it here would be a Pitman your mixture model, in which you can I can I can draw this as a graphical model, so imagine this is just as an example where I have just four pixels or 4 super pixels at the bottom.",
            "I have my observed features.",
            "These are going to be color and texture their histograms, so they'll be sampled from multinomial distributions.",
            "Each segment is going to have its own multinomial distributions.",
            "I'll put the standard conjugate directly prior just to keep things simple.",
            "Then each feature is going to be sampled to the segment in the basic mixture model, there's an independent assignment variable Z, that sort of that sample independently for each of my features, and then so this all just looks like any old mixture model where this sort of nonparametrics come in is that I allow myself to have an unbounded number of these potential segments, and then I put this prior on their sizes, which is going to be this from the sequence of beta random variables.",
            "So you could apply this to images.",
            "You won't be surprised to find that results aren't stellar because you're totally ignoring the spatial structure of the senior, ignoring the fact that you expect things that are next to each other spatially should be more likely to be in the same segment, so we'd like to build that in.",
            "One of the more conventional ways to think about doing this would be through a dependent DP or Pittman your mixture and so roughly speaking these models say OK. Well now I'm going to give myself more parameters so everyone of my pixels is going to have its own segment distribution, so I've got a distribution at each of these super pixels and.",
            "Then I'm going to have some prior that couples these things together and has marginals that are either sort of deer, say process, Pittman, your, or maybe have similar properties, right?",
            "And so you get a whole bunch of papers.",
            "I mean that I wrote a few here and there's like dozens more by picking different ways of how you create these things and how you induce dependence.",
            "So what's happening is the idea is that you know if the probability of 1 segment is high here.",
            "It will also be high at neighboring things, and that will give me some spatial context."
        ],
        [
            "Here's a model in that flavor.",
            "An example of what sort of samples from the prior is.",
            "So this is a parametric model that looks a lot like those where what I do is I take a set of Gaussian processes.",
            "I pass them through a softmax.",
            "I exponentiated normalize and then I take those probabilities at each pixel and sample and So what you see you get some spatial coherence, but they're very samples are very noisy, right?",
            "And so?",
            "This is actually this much papers have done this.",
            "You can try to work around it, but it's it's very noisy.",
            "There's also no global model of that, there being some large and some small clusters.",
            "Everything is kind of all roughly similar in size, and I think this is."
        ],
        [
            "Works in a fundamental problem of once I say I'm going to have a lot of distributions and then independent samples from those.",
            "It's very hard to get strong spatial coherence and things, so I'd like to sort of look at some other approaches."
        ],
        [
            "You may be familiar with icing orpaz Markov random field models, right?",
            "So these are undirected graphical models in which you for every pair of nodes have a potential that says it's better to be in the same segment then in different segments seems very natural.",
            "There's been some previous success at using these things for things, basically in domains where you have a lot of supervision, but when you take that supervision away, the results people haven't had as good of success.",
            "One way of understand."
        ],
        [
            "I was looking at samples from this Markov random field prior, so here I'm there's a one parameter in that prior here I'm showing 5 samples when I set that parameter to 1.4, two, 1.44 and 1.46.",
            "So you have noise, one giant cluster kind of a mess in between.",
            "There's no setting that gives you anything like, looks like a sensible image partition, and this is because the icing and Potts model was the sort of 1st idealization of phase transition behavior in physics, right?",
            "So it's great for phase transitions, not so good for.",
            "For images and I actually I would argue not so good for spatial statistics, even though it's been used there.",
            "I."
        ],
        [
            "Now I'm going to look at it.",
            "I've been looking at a different approach where there's a single set of probabilities that are going to have this Pittman.",
            "Your flavor and the coupling comes down here at the assignment level in terms of how I choose to assign pixels to there, and it's easiest to see in pictures, so to generate a partition like that one in the lower left, what I'm going to do is first I'm going to sample a smooth function, so I'm going to do a Gaussian process just because that's analytically convenient.",
            "The key property I need is smoothness, and I'm going to cut that with a threshold and everything above the threshold is going to be assigned to that dark blue region, everything below.",
            "Is going to be not and I'm going to sample a second second smooth function.",
            "I cut it with a threshold and I assign everything that's that's.",
            "All pixels that are above that threshold and were not already assigned to the first region, so that will be that green area and so on.",
            "So you can continue this down the chain."
        ],
        [
            "Here's what another sample is where I sample smooth functions.",
            "A sequence of them, and I cut them with thresholds.",
            "OK, so what's really interesting is that."
        ],
        [
            "If you do things right, so if you give the right random distribution on these thresholds, you retain a Pitman your marginal distribution on the sizes of these segments, while also getting spatial clustering.",
            "That's flexibly parameterized by these Gaussian processes.",
            "And it turns out there's I don't have time to go into it, but there's a deep connection between the fact that I have a sequence of functions and the stick breaking representation of of the pit maneuver process."
        ],
        [
            "OK, and so you can draw this as a graphical Model 2 where I have this whole set of Gaussian processes, they don't have to be Markov, that's that's important, and modeling I'm going to have this same Pittman your prior over segment sizes and then finally I'm going to have look at sort of given my Gaussian process realization zanmai sort of realized segment sizes.",
            "These are deterministic, they're just looking at the first function to cross the threshold and the threshold locations.",
            "The where are those set up.",
            "It turns out that the normal CDF shows up in doing this.",
            "And this is very much like copula models.",
            "So actually for those who saw that yesterday, the ideas of what's going on here to give me the right marginals is very much the constructions people using copula models alright."
        ],
        [
            "So this is a couple pictures of what samples from that prior look like compared to say the marker random field so they're not perfect, but they're getting a lot closer to what we want real image partitions to have.",
            "There's some that have a few large regions, some that have more small regions.",
            "When I have small regions, they tend to be little blobs or not a bunch of dust.",
            "OK, so this is going to be my starting point, yeah?",
            "Those squared exponential this credential kernel look like with sort of more.",
            "Like a Brownian motion?",
            "Or would they just be silk patches but with more jaggedy edges?",
            "Or yeah, that wouldn't give you smooth function, so you would get more noise around the boundaries.",
            "But you still get nice solid patches.",
            "Yeah, So what I'm going?"
        ],
        [
            "Port is learning that kernel essentially.",
            "One thing that would seem like we have here is that then there's also especially a model of occlusion that's happening, and that's tightly coupled with the stick breaking process.",
            "Presumably the thing that tends to get the most mass is also the thing that tends to be in front.",
            "Yes, yes, so yes, there's all this nice connections between the stick breaking induces.",
            "This sequence of assignments, which turns out basically look like occlusion under an orthographic projection model in computer vision.",
            "So it's just kind of beautiful coincidence of ideas from the two fields, at least from my perspective.",
            "OK, so now I'm going to sort of go sort of a little more quickly through how we can think about doing inference and learning.",
            "These models we started a couple of minutes late."
        ],
        [
            "So the 1st way I looked about doing inference was with a mean field variational method.",
            "I was I'm interested in sort of scaling this to big datasets and and so on, and I was a little worried about about MCMC, so variational method for those who haven't seen them essentially look at sort of convexity based bounds on the marginal likelihood of data, so they end up having two terms.",
            "There's a sort of expected log probability of my hidden variables, as well as an entropy term, and so then you optimize this over the space of all sort of some.",
            "Distributions of your hidden variables that live in some tractable family.",
            "And so you start by doing this you do some truncation.",
            "You say I had this nice infinite model I'm going to.",
            "I'm going to allow myself at most 20 segments per image or something like that, and then in a given image will find some subset of that in a data driven way, you put Gaussian sort of parameters on all these things.",
            "You have to optimize them.",
            "I've done a lot of variational methods.",
            "This is by far the trickiest nastiest model I've ever worked with.",
            "It's kind of fun, but it was a lot.",
            "There's a lot of ugly math that goes into these.",
            "It turns out that you can.",
            "I do get tractable updates because if I have, for example a Gaussian posterior on the location of a threshold in the Gaussian on the location of a function, I can compute the probability that one Gaussian lies above another in closed form from the normal CDF.",
            "So some of these identities go in there.",
            "So you go to a lot of work to build this method."
        ],
        [
            "I really doesn't work well, so this is showing on a single image from lots of different random initializations.",
            "That sort of trace plots of running this variational method, and this is only about a dozen 10 to 20 iterations and it gets stuck, so wanders up to some mode and it gets stuck and the differences among these modes is something like a factor of, you know that's about 1000 in one to 2000 and log probability, and this is the difference between a really interesting interpretation and just I'll show an example images.",
            "Garbage interpretatione useless segmentation of the image.",
            "OK, so Maurice."
        ],
        [
            "I've been looking at an alternative method that has a slightly different flavor, so under those variational posteriors it turns out that allowing yourself soft assignments doesn't really help you, because typically there's not much uncertainty when once you have all these variables factorized.",
            "So I'm just going to get hard assignments of super pixels to layers.",
            "I'm going to be considering hard partitions and searching over a space of hard partitions.",
            "This is also ultimately what I want.",
            "I have to output a partition that's the result of my segmentation algorithm.",
            "Given a hard partition, I can integrate all of my likelihood parameters analytically, right with no approximations.",
            "I can integrate all those out.",
            "And it turns out I can also do very accurate marginalization of these latent functions.",
            "So you're saying out of all continuous functions, I could have sampled what fraction of those would have given me this particular partition.",
            "It turns out that using the expectation propagation method, which is a higher order variational method, you get something that's very accurate here.",
            "The reason that's a big win is that mean field with Gaussian variables completely does the wrong thing with the variances.",
            "It doesn't capture uncertainty at all, and it gives you sort of terrible estimates EP fixes that there's a lot of literature validating that.",
            "And the nice thing is now if I'm going to look at hard partitions in search, I don't have to do any finite conservative model truncation.",
            "So I can change the partition and I just have to integrate over the set of functions that have at least one superpixel assigned to them, but infinite tail of other functions.",
            "I can just sort of marginalized out analytically, so there's no need to sort of approximate the model.",
            "We have"
        ],
        [
            "Various search moves.",
            "This is kind of something that you know at this point is not very elegant, it's just more practical.",
            "You can say that you know we basically have ways of splitting and merging regions, removing layers, shifting superpixel single super pixels so these don't get you between all modes.",
            "But one way I like to think about it is that essentially the mean field method.",
            "All it can do is shift single super pixels.",
            "It has a hard time making any larger changes."
        ],
        [
            "And so here is just sort of a practical example.",
            "Here's ten runs of mean field.",
            "Here's ten runs of EP in terms of the likelihoods, they get too.",
            "We have huge variability.",
            "Small variability.",
            "This is the best and worst mode found by EP across these ten runs, so they're not the same, but they're pretty close, right?",
            "There's one difference at the bottom.",
            "Here is the best and worst mode run by mean field across these things, so you can see that this one has just has tons in this huge difference, right?",
            "So if I had the computation the run mean field?",
            "1000 times or 10,000 times or something like that.",
            "It might be OK.",
            "Even the modes that prefers or not as good that you get better answers.",
            "I think out of EP by better approximating that variance.",
            "Is that mean field does, yeah.",
            "Are the vertical axes comfortable?",
            "And yeah, so that's another.",
            "That's another point.",
            "Is that mean field is producing a lower bound on the likelihoods, which is very pessimistic.",
            "So you see that these range between minus 9500 -- 8000, and these are all clustered around minus 6500, and so the gap between minus 8000 -- 600 is sort of the difference between the pessimistic bound and the more tight approximation of the marginal likelihood.",
            "That EP gives you.",
            "Yeah."
        ],
        [
            "Here's another example of sort of practical settings.",
            "This is sort of like if I run with the number of initializations.",
            "That's kind of not crazy in terms of computation.",
            "This is just an example of what you might get from the mean field in the EP, and so it's just a lot more robust.",
            "I can sort of trust it more.",
            "I could hope to give somebody the CP code and have them be able to use it reliably, and it's just the mean field.",
            "I wasn't able to trust the results."
        ],
        [
            "OK, I'm going to say a few brief things about."
        ],
        [
            "Learning, so some of the parameters like this sort of Pitman, your hyperparameters, and these Dursley priors are very easy to sort of sort of set from we have a data set with some human segmentations, and so it's easy to sort of set those using standard likelihood based web methods.",
            "What's a little more interesting is how I'm going to set this covariance kernel, and the intuition is that there's this one to one mapping between the covariance of two super pixels and the probability that the features that those locations are in the same segment.",
            "Right high positive correlation means you're likely to be the same segment.",
            "Strong negative correlation means you're unlikely, and so I'm going to kind of divide my image cues into two pieces.",
            "I'm going to have color and texture histogram models of what's going on within each region.",
            "I'm going to treat that in the standard generative way, and then I'm also going to look at sort of contour based cues.",
            "So if I look at pairs of things and look at a straight line, I say is there are strong contour between them.",
            "This is something I'm going to sort of capture in a more conditional way you can think of this thing.",
            "In this thing, and so we're going to look at a set of."
        ],
        [
            "You can't see the pictures, but we're going to look at a set of human segmentations.",
            "And so the point here is that you know in the segmentation problem I can expect to have labeled examples every category I care about, so I can't just sort of learn a model of a bunch of categories.",
            "But what I can do is learn basically a binary binary classifier and actually I need probabilities.",
            "So I'm going to use logistic regression that says OK if I have this superpixel here in this superpixel here, what's the probability that they're in the same segment in a kind of category independent way?",
            "And it turns out you can actually learn that in a reasonable way.",
            "If I have a bunch of human things I could just take a bunch of pairs.",
            "Have super pixels in those human things?",
            "Train a classifier.",
            "Standard features."
        ],
        [
            "OK, so now I have these estimates of probability of being in the same things and now I can ask what's given these probabilities of being in the same region.",
            "What's the?",
            "What's the correspondence between some choice of the correlation of two super pixels and the probability that they'll be in the same region, right?",
            "And you could be in the same region by both being assigned to layer one or both being assigned to layer two, or both being assigned to layer three.",
            "So you know there's a little probability, and you can do some low dimensional numerical integration and you get this mapping.",
            "So this tells me for the correlation varying between negative one and one and my Gaussian processes are going process with unit diagonal, the probability of being in the same region and so.",
            "As correlation goes to one, that's probability goes to one as correlation goes to as correlation at zero I get the probability I would have with a conventional sort of Chinese restaurant process model as it goes to negative one, I get something that is small but nonzero.",
            "Depends on the hyperparameters.",
            "And then as a."
        ],
        [
            "Final step now if you take your sort of have for every pair of super pixels you have an estimate of what this correlation should be.",
            "Those may not be consistent with each other, right?",
            "If I take a covariance matrix, I independently estimate the entries.",
            "I may get something that's not positive definite, so we have to do a final projection.",
            "We use a sort of.",
            "We do a low rank projection, which gives us computational efficiency using the projected gradient method.",
            "Is actually an interesting sort of geometric thing.",
            "Is that the way that I get inconsistency is that I have Contour accused that say this guy should is highly correlated with him and he's highly card with him, but those are not highly correlated, right?",
            "'cause that boundary passes outside, so there's actually loss of sort of transitivity that comes from non convex geometry."
        ],
        [
            "And it works.",
            "So this is a scatter plot, so this is sort of showing this learned sort of model versus a log likelihood under a model that sort of had heuristically set parameters.",
            "The best I could do with tweaking by hand over a couple of months.",
            "So you can see below the line means to learn models basically uniformly across almost every image.",
            "We're doing a much better job of predicting that you know what's likely in terms of human segmentations.",
            "I can also look at sort of correspondence of the learn likelihoods.",
            "This is a sort of scatter plot of log likelihood with a Rand index, which is a quantitative evaluation for segmentation.",
            "They're not perfectly correlated, but there's definitely a sort of sort of positive relationship we'd like there.",
            "So we think what the model is capturing some interesting things."
        ],
        [
            "I think I'll skip that and."
        ],
        [
            "I have about one or two minutes and I will just show a few."
        ],
        [
            "Results are going to be hard to see, unfortunately, but so.",
            "This is showing you know, these four columns are results with different sort of.",
            "Sort of, the most conventional widely used, popular segmentation methods that kind of the right combination of good performance and being not too difficult and so on in the vision literature, most they don't really have anything to do with Asian methods.",
            "This is the output of our method, and if you kind of the hard part about making a good segmentation is essentially this inference issue.",
            "I was talking bout this reliability that there's lots of methods that you tweak the parameters.",
            "You can get it to do well on a couple images, but then you have an image which which there's a lot more objects or a lot fewer objects.",
            "It will give a bad result, right?",
            "And so it's kind of getting something that varies that will produce lots of regions.",
            "Here were only produced two regions here.",
            "Realize it's just a snake on a background and not much else going on.",
            "That's what we need out of a good segmentation method, so I should emphasize that when I'm showing you know multiple partitions that have various numbers of regions, this is all the same model, right?",
            "It's the exact same learn model that feeds into all of this.",
            "It's just marginal likelihood that's driving."
        ],
        [
            "I terms of quantitative results were sort of.",
            "There's this Berkeley segmentation data set.",
            "There's a bunch of numbers 'cause there's a bunch of ways to measure segmentation accuracy.",
            "Roughly speaking, where comparable to most methods and worse than this, this one sort of highly tuned method called GPB on the Berkeley segmentation.",
            "If we go to a different data set were actually a little bit better than the GP method.",
            "This is a case where in this Berkeley segmentation data set, people been playing with the same test data for six or seven years now, so there's maybe a little overfitting going on.",
            "I think there's a lot of room for improvement of our model, so we'd like to run that search longer, which is basically in a matter of improving some aspects of our implementation.",
            "I didn't talk at all about likelihoods, and turns out this histogram thing I'm doing is pretty simple minded.",
            "We could do better, and also we're just I'm just taking essentially finding the most probable segmentation we find, and that's actually not a good idea for a lot of these error measures like the Rand index.",
            "If you want to hedge your bets, you should predict an over segmentation that will, on average, give you lower loss.",
            "But we have in fact a whole bunch."
        ],
        [
            "Different modes, so now we're not.",
            "We not only have the most probable segmentation for each image, we have lots of segmentations, so these are showing three modes ranked by probability.",
            "They vary from each other in interesting ways, so for that you see for this bottom one down here I have one in which the couple modes kind of just have the shark.",
            "This bottom one.",
            "There's a bunch of fish, little fish swimming around in the scene.",
            "It started to pick those out, so that's less likely, but that is also picked out so you can combine information across these.",
            "I think that would probably give you better."
        ],
        [
            "Results here's other examples of showing.",
            "For the woman down here on the bottom, it's really easy to see the sort of range of partitions of sort of different resolutions.",
            "These are not nested in a single tree, like some hierarchical clustering methods give you, and in fact it's really.",
            "You can't properly capture the uncertainty and image partitions with a single tree of partitions."
        ],
        [
            "And this is just, you know.",
            "Regardless of the numbers, this is just showing a bunch of results.",
            "I don't really care so much whether I'm the second best or the best segmentation method is something that I think is producing very interesting results and really kind of is very accurate for this problem and is doing the things I like in that.",
            "It's sort of varying the resolution across images in a sensible way so."
        ],
        [
            "Oh I think you know I'm.",
            "I'm pretty sort of optimistic about being able to do successful Asian American modeling, but I think it requires a lot more than tossing out a new model, right?",
            "So you have a lot of work here.",
            "Went into studying model assumptions, match the statistics model comparisons.",
            "I actually compared about 10 models before I picked that.",
            "First one is the best one for this domain.",
            "We need sort of reliable, consistent inference algorithms.",
            "The basic mean field and basic sampling algorithms don't cut it, but I think we'll hear more about that later today.",
            "And we in the learning problem.",
            "I think maybe you know we need to think a little more.",
            "There's a lot of cases where if you want to have a clustering method, we just can't hope to do everything based on marginal likelihood of unlabeled data.",
            "There's so many ways to cluster data, so if we want clusterings that are consistent with something, we might think about.",
            "If there are smart ways to get side information for part of our thing, so that's kind of what I did with my had a set of human segmentations, I couldn't learn about everything I could learn about a few aspects and that was really important for this problem.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm going to be talking about image segmentation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our goal, sort of the long term goal behind this kind of work is to try to do in an automatic way what a person has done here, right?",
                    "label": 0
                },
                {
                    "sent": "So they've taken images.",
                    "label": 0
                },
                {
                    "sent": "They've labeled the sort of outline the objects in the image.",
                    "label": 0
                },
                {
                    "sent": "That's called segmentation.",
                    "label": 0
                },
                {
                    "sent": "In this case, they've also assigned some sort of semantic labels to what they are.",
                    "label": 0
                },
                {
                    "sent": "OK, so the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Widely sort of.",
                    "label": 0
                },
                {
                    "sent": "If you look at contemporary papers, the most sort of common way people look at this is in terms of supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "So people define some small number of categories.",
                    "label": 0
                },
                {
                    "sent": "They're interested in sort of get some labeled data where those are labeled, and then try to sort of build a classifier.",
                    "label": 0
                },
                {
                    "sent": "This is a pretty.",
                    "label": 0
                },
                {
                    "sent": "This is a sample result.",
                    "label": 0
                },
                {
                    "sent": "This is from about four years ago of what I think is actually very good work.",
                    "label": 0
                },
                {
                    "sent": "They got all the technical details right, but the final story was a little disappointing where they had a very simple sort of classifier that basically looked at little patches of the image and classified them independently.",
                    "label": 0
                },
                {
                    "sent": "Tawanna 20 classes that got 74% accuracy, then they added a Markov random field.",
                    "label": 0
                },
                {
                    "sent": "Prior.",
                    "label": 0
                },
                {
                    "sent": "Did loopy belief propagation training and inference and so forth, and it went from 74 to 78%.",
                    "label": 0
                },
                {
                    "sent": "And you can see that basically it's you know it's just kind of smoothing over a little bit of the noise.",
                    "label": 0
                },
                {
                    "sent": "It's not really correcting any major errors, so this is disappointing.",
                    "label": 0
                },
                {
                    "sent": "For building what we'd like to be a sort of fancy spatial model, and so we'd like to try to.",
                    "label": 0
                },
                {
                    "sent": "I would.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in understanding, so why don't we get more benefit out of these things?",
                    "label": 0
                },
                {
                    "sent": "And one way of understanding why you don't get more benefit out of this is to say, well, what's happening is that my sort of performance of my spatial model being propped up by having a good classifier to start with, and so maybe to try to understand why it's good and bad I can.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to sort of something where I have less supervision and this is come to the problem of not object recognition, but segmentation and so this is.",
                    "label": 0
                },
                {
                    "sent": "These are examples of where you these are.",
                    "label": 0
                },
                {
                    "sent": "Each row is multiple people taking this image and their instructions or something like just outline the objects that are interested in the scene out on the things that are interesting.",
                    "label": 0
                },
                {
                    "sent": "And of course there's variability from person to person, but what's interesting is that there's also a remarkable amount of agreement between people in terms of what they think are the sort of salient boundaries, so this is actually a problem you can study in its own right.",
                    "label": 0
                },
                {
                    "sent": "This is taking a natural image, sort of identifying the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Object key objects there, and I think there's been a data set set up on and sort of a whole series of methods and benchmarks and so on, so I'm going to be thinking about this sort of space of image segmentation problems, but from a nonparametric Bayesian perspective.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why'd Asian nonparametrics for segmentation?",
                    "label": 0
                },
                {
                    "sent": "Well, I think it's a really nice fit, so segmentation is essentially a partitioning problem.",
                    "label": 0
                },
                {
                    "sent": "We don't know how many regions the image contains and what their sizes are.",
                    "label": 1
                },
                {
                    "sent": "We have a unit area image and we need to split it up right so priors on partitions are one of the sort of building blocks for Bayesian non parametrics.",
                    "label": 0
                },
                {
                    "sent": "They fit right in here.",
                    "label": 0
                },
                {
                    "sent": "Also, if you talk to sort of applied applied vision, people who want to use segmentation methods, they realized that this is sort of fundamentally ambiguous problem.",
                    "label": 0
                },
                {
                    "sent": "There's no way I'm ever going to get a segmentation method that's perfect, so actually they would really like to have multiple segmentations, maybe ranked by probability from a low level method that could be used in a high level way, so that's a really sort of nice way sort of argument for Bayesian methods.",
                    "label": 1
                },
                {
                    "sent": "There's also huge variability from image to image in terms of.",
                    "label": 0
                },
                {
                    "sent": "There might be 2 objects here, a dozen there, so I can't just assume I know how many regions are clusters they're going to be OK, so that's kind of an argument.",
                    "label": 0
                },
                {
                    "sent": "I think, for Nonparametrics.",
                    "label": 0
                },
                {
                    "sent": "So great, so I don't know what's going to be so applied.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These infinite models right?",
                    "label": 0
                },
                {
                    "sent": "And so I've got a whole bunch of infinite models.",
                    "label": 0
                },
                {
                    "sent": "There's been a whole series of these, and so so there's the hype aspect of this workshop.",
                    "label": 0
                },
                {
                    "sent": "I think to some extent were partially to blame for this, right?",
                    "label": 0
                },
                {
                    "sent": "You know, we come up with the infinite fancy widget, and as time goes on you have to add more words after infinite to get a new model that hasn't been done before, but you know, we're creative.",
                    "label": 0
                },
                {
                    "sent": "We can keep doing that.",
                    "label": 0
                },
                {
                    "sent": "But you know, I think there hasn't been enough understanding of OK for a particular data set.",
                    "label": 0
                },
                {
                    "sent": "How can I understand, you know, is the way in which I'm doing my infinite model useful out of the?",
                    "label": 0
                },
                {
                    "sent": "In fact, dozens of ways I could have chosen to make sort of a large flexible model.",
                    "label": 0
                },
                {
                    "sent": "Which one should I have picked?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to sort of treat segmentation as.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case study for going through some of the issues that show up in that kind of problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about a model I've worked on.",
                    "label": 0
                },
                {
                    "sent": "This is the model itself is now a few years old.",
                    "label": 0
                },
                {
                    "sent": "It's based on it, makes dependent manual processes using the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about some more recent work on inference and learning, as well as some results on image segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of ways of thinking about the Pitman Yor process for the purposes of this talk, will think of it as a distribution on infinite discrete measures or partitions, and so one way of representing this with this stick breaking construction.",
                    "label": 1
                },
                {
                    "sent": "You've probably all seen too many times where I have unit listica probability mass, I take a random proportion.",
                    "label": 0
                },
                {
                    "sent": "That's the first part of the partition.",
                    "label": 0
                },
                {
                    "sent": "Then I take a random proportion of what's left and a random proportion of what's left, and so on down the chain for the Pitman yor process.",
                    "label": 0
                },
                {
                    "sent": "These are sampled from a sequence of beta distributions.",
                    "label": 0
                },
                {
                    "sent": "There's two free parameters A&B.",
                    "label": 0
                },
                {
                    "sent": "If I set one of those parameters to zero, I get the famous nearest late process.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a larger family that includes the Dirichlet process, special case.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In Pitman, Yor indirectly process all of those stick proportions.",
                    "label": 0
                },
                {
                    "sent": "I'm sampling to come from the same distribution and Pitman Yor.",
                    "label": 0
                },
                {
                    "sent": "They vary so as I move down the chain from the 1st to the 10th of the 20th stick component, I go from this blue curve to the red to green.",
                    "label": 0
                },
                {
                    "sent": "So actually they're getting smaller and smaller on average, so the DT is already in some sense sort of heavy tailed prime partitions, and the pit maneuver has heavier tails.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now to look at sort of whether this might be useful in understanding the segmentation problem, we looked at the data set of human segmentations, so this is a a site called label me set up by some colleagues at MIT where there's a web interface in which people can use create Polygon outlines of objects.",
                    "label": 0
                },
                {
                    "sent": "They can also assign semantic labels.",
                    "label": 0
                },
                {
                    "sent": "I won't be talking about that here.",
                    "label": 0
                },
                {
                    "sent": "I looked at a subset of this that's about 29,000 segments in about 2700 images, so fairly large collection of data and you can ask things.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the basic statistics.",
                    "label": 0
                },
                {
                    "sent": "So for example, how many objects or more generally, just how many of these regions are there that are above some size threshold, right?",
                    "label": 1
                },
                {
                    "sent": "Say larger than 50 pixels?",
                    "label": 0
                },
                {
                    "sent": "So there's an empirical histogram.",
                    "label": 0
                },
                {
                    "sent": "That's that black curve in the data, the red and green are predictions of that.",
                    "label": 0
                },
                {
                    "sent": "Pin your orders they process models would give where I've sort of optimized the hyperparameters of those models to be as close to this data as possible, so you can see you get and not awful fit from the Dirichlet process and are quite good match with pit manure.",
                    "label": 0
                },
                {
                    "sent": "Also, we can look at the sizes of these things and what's interesting is if I take a histogram of object sizes and I'm going to put this on a log log scale, you get this sort of nearly straight line behavior.",
                    "label": 0
                },
                {
                    "sent": "This is what people sometimes call a power law distribution where there's a few large objects and then lots of sort of very small objects, and this is just a sort of characteristic property of the world we live in.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why Pittman your this is Jim Pitman and Mark your they were interested in this process as sort of something that generalizes the Dursley process in lots of ways and unifies lots of things in probability.",
                    "label": 1
                },
                {
                    "sent": "But for statistical applications, this sort of driving thing is that it gives us power law distributions.",
                    "label": 1
                },
                {
                    "sent": "There's two complementary ways to see this.",
                    "label": 0
                },
                {
                    "sent": "One is that I can say if I have in unique in observations, how many unique clusters do I expect to have seen a priore under the DP that grows logarithmically under Pitman yor that grows polynomially so this is.",
                    "label": 0
                },
                {
                    "sent": "A is some number between zero and one.",
                    "label": 0
                },
                {
                    "sent": "This is actually sort of a classic observation, and the text literature.",
                    "label": 0
                },
                {
                    "sent": "This has a name, heaps law that if I just look at words and say how many unique vocabulary words do I sign as I read English text, it has this kind of growth.",
                    "label": 0
                },
                {
                    "sent": "I can also say if I just take my sort of clusters or segments and I sort them in size from largest to smallest, how quickly does that decay for the rest?",
                    "label": 0
                },
                {
                    "sent": "The process that's exponential for Pitman Yor that's polynomial and that's connected to zips along, so there's been some very nice work.",
                    "label": 0
                },
                {
                    "sent": "Sort of pointing out that there's nice links between Pitman Yor.",
                    "label": 0
                },
                {
                    "sent": "And natural language statistics and building sort of models based on top of that.",
                    "label": 1
                },
                {
                    "sent": "The plots I just showed suggest that this is also a good fit for images.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As an aside, you know so if you look at this sort of toy datasets, people like to use a machine learning papers, right?",
                    "label": 1
                },
                {
                    "sent": "You know where these sort of like points in the plane and they show their clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "They tend to have this flavor of, you know, there's a small number of clusters, and they're all roughly equal in size, and I think this people just kind of like symmetry.",
                    "label": 0
                },
                {
                    "sent": "I think most real data doesn't really look anything like this, right?",
                    "label": 0
                },
                {
                    "sent": "If you think about these power distributions, are there's some huge clusters in some very small clusters, right?",
                    "label": 0
                },
                {
                    "sent": "And so the segmentation algorithms like normalized cut spectral clustering, has a bias that you want everything to be equal in size.",
                    "label": 0
                },
                {
                    "sent": "Works great here.",
                    "label": 0
                },
                {
                    "sent": "Works terribly for sort of image type things.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I think this is maybe some of our sort of opera intuition about what all the data we're going to find in the world looks like is not necessarily well calibrated.",
                    "label": 0
                },
                {
                    "sent": "When we set up these toy validations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how am I going to apply this to images?",
                    "label": 0
                },
                {
                    "sent": "Well, I'm going to extract some features.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of a lot of kind of machinery I can build on what we're going to do to reduce dimensionality is we're going to partition the image into about 1000 super pixels.",
                    "label": 0
                },
                {
                    "sent": "You can think of these things as these little blob shape regions, and I try to do this in a way such that I hopefully don't merge anything real in the world.",
                    "label": 0
                },
                {
                    "sent": "OK, and then I'm going to compute some descriptor of each of these, so we're doing something very basic sort of tried and true sort of texture and color histograms, about 100 bins for each, and so you're just counting the sort of some histogram represent.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of what the color and what the texture?",
                    "label": 0
                },
                {
                    "sent": "So texture sort of an edginess measure how much energy there is at various orientations.",
                    "label": 0
                },
                {
                    "sent": "OK, and so then the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another way to sort of take that Pittman your model and apply it here would be a Pitman your mixture model, in which you can I can I can draw this as a graphical model, so imagine this is just as an example where I have just four pixels or 4 super pixels at the bottom.",
                    "label": 0
                },
                {
                    "sent": "I have my observed features.",
                    "label": 0
                },
                {
                    "sent": "These are going to be color and texture their histograms, so they'll be sampled from multinomial distributions.",
                    "label": 0
                },
                {
                    "sent": "Each segment is going to have its own multinomial distributions.",
                    "label": 0
                },
                {
                    "sent": "I'll put the standard conjugate directly prior just to keep things simple.",
                    "label": 0
                },
                {
                    "sent": "Then each feature is going to be sampled to the segment in the basic mixture model, there's an independent assignment variable Z, that sort of that sample independently for each of my features, and then so this all just looks like any old mixture model where this sort of nonparametrics come in is that I allow myself to have an unbounded number of these potential segments, and then I put this prior on their sizes, which is going to be this from the sequence of beta random variables.",
                    "label": 0
                },
                {
                    "sent": "So you could apply this to images.",
                    "label": 0
                },
                {
                    "sent": "You won't be surprised to find that results aren't stellar because you're totally ignoring the spatial structure of the senior, ignoring the fact that you expect things that are next to each other spatially should be more likely to be in the same segment, so we'd like to build that in.",
                    "label": 0
                },
                {
                    "sent": "One of the more conventional ways to think about doing this would be through a dependent DP or Pittman your mixture and so roughly speaking these models say OK. Well now I'm going to give myself more parameters so everyone of my pixels is going to have its own segment distribution, so I've got a distribution at each of these super pixels and.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to have some prior that couples these things together and has marginals that are either sort of deer, say process, Pittman, your, or maybe have similar properties, right?",
                    "label": 0
                },
                {
                    "sent": "And so you get a whole bunch of papers.",
                    "label": 0
                },
                {
                    "sent": "I mean that I wrote a few here and there's like dozens more by picking different ways of how you create these things and how you induce dependence.",
                    "label": 0
                },
                {
                    "sent": "So what's happening is the idea is that you know if the probability of 1 segment is high here.",
                    "label": 0
                },
                {
                    "sent": "It will also be high at neighboring things, and that will give me some spatial context.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a model in that flavor.",
                    "label": 0
                },
                {
                    "sent": "An example of what sort of samples from the prior is.",
                    "label": 0
                },
                {
                    "sent": "So this is a parametric model that looks a lot like those where what I do is I take a set of Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "I pass them through a softmax.",
                    "label": 0
                },
                {
                    "sent": "I exponentiated normalize and then I take those probabilities at each pixel and sample and So what you see you get some spatial coherence, but they're very samples are very noisy, right?",
                    "label": 0
                },
                {
                    "sent": "And so?",
                    "label": 0
                },
                {
                    "sent": "This is actually this much papers have done this.",
                    "label": 0
                },
                {
                    "sent": "You can try to work around it, but it's it's very noisy.",
                    "label": 0
                },
                {
                    "sent": "There's also no global model of that, there being some large and some small clusters.",
                    "label": 0
                },
                {
                    "sent": "Everything is kind of all roughly similar in size, and I think this is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works in a fundamental problem of once I say I'm going to have a lot of distributions and then independent samples from those.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to get strong spatial coherence and things, so I'd like to sort of look at some other approaches.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You may be familiar with icing orpaz Markov random field models, right?",
                    "label": 0
                },
                {
                    "sent": "So these are undirected graphical models in which you for every pair of nodes have a potential that says it's better to be in the same segment then in different segments seems very natural.",
                    "label": 0
                },
                {
                    "sent": "There's been some previous success at using these things for things, basically in domains where you have a lot of supervision, but when you take that supervision away, the results people haven't had as good of success.",
                    "label": 0
                },
                {
                    "sent": "One way of understand.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I was looking at samples from this Markov random field prior, so here I'm there's a one parameter in that prior here I'm showing 5 samples when I set that parameter to 1.4, two, 1.44 and 1.46.",
                    "label": 0
                },
                {
                    "sent": "So you have noise, one giant cluster kind of a mess in between.",
                    "label": 0
                },
                {
                    "sent": "There's no setting that gives you anything like, looks like a sensible image partition, and this is because the icing and Potts model was the sort of 1st idealization of phase transition behavior in physics, right?",
                    "label": 0
                },
                {
                    "sent": "So it's great for phase transitions, not so good for.",
                    "label": 1
                },
                {
                    "sent": "For images and I actually I would argue not so good for spatial statistics, even though it's been used there.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to look at it.",
                    "label": 0
                },
                {
                    "sent": "I've been looking at a different approach where there's a single set of probabilities that are going to have this Pittman.",
                    "label": 0
                },
                {
                    "sent": "Your flavor and the coupling comes down here at the assignment level in terms of how I choose to assign pixels to there, and it's easiest to see in pictures, so to generate a partition like that one in the lower left, what I'm going to do is first I'm going to sample a smooth function, so I'm going to do a Gaussian process just because that's analytically convenient.",
                    "label": 0
                },
                {
                    "sent": "The key property I need is smoothness, and I'm going to cut that with a threshold and everything above the threshold is going to be assigned to that dark blue region, everything below.",
                    "label": 0
                },
                {
                    "sent": "Is going to be not and I'm going to sample a second second smooth function.",
                    "label": 0
                },
                {
                    "sent": "I cut it with a threshold and I assign everything that's that's.",
                    "label": 0
                },
                {
                    "sent": "All pixels that are above that threshold and were not already assigned to the first region, so that will be that green area and so on.",
                    "label": 1
                },
                {
                    "sent": "So you can continue this down the chain.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's what another sample is where I sample smooth functions.",
                    "label": 0
                },
                {
                    "sent": "A sequence of them, and I cut them with thresholds.",
                    "label": 0
                },
                {
                    "sent": "OK, so what's really interesting is that.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you do things right, so if you give the right random distribution on these thresholds, you retain a Pitman your marginal distribution on the sizes of these segments, while also getting spatial clustering.",
                    "label": 0
                },
                {
                    "sent": "That's flexibly parameterized by these Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "And it turns out there's I don't have time to go into it, but there's a deep connection between the fact that I have a sequence of functions and the stick breaking representation of of the pit maneuver process.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so you can draw this as a graphical Model 2 where I have this whole set of Gaussian processes, they don't have to be Markov, that's that's important, and modeling I'm going to have this same Pittman your prior over segment sizes and then finally I'm going to have look at sort of given my Gaussian process realization zanmai sort of realized segment sizes.",
                    "label": 0
                },
                {
                    "sent": "These are deterministic, they're just looking at the first function to cross the threshold and the threshold locations.",
                    "label": 0
                },
                {
                    "sent": "The where are those set up.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the normal CDF shows up in doing this.",
                    "label": 1
                },
                {
                    "sent": "And this is very much like copula models.",
                    "label": 0
                },
                {
                    "sent": "So actually for those who saw that yesterday, the ideas of what's going on here to give me the right marginals is very much the constructions people using copula models alright.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a couple pictures of what samples from that prior look like compared to say the marker random field so they're not perfect, but they're getting a lot closer to what we want real image partitions to have.",
                    "label": 1
                },
                {
                    "sent": "There's some that have a few large regions, some that have more small regions.",
                    "label": 0
                },
                {
                    "sent": "When I have small regions, they tend to be little blobs or not a bunch of dust.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is going to be my starting point, yeah?",
                    "label": 0
                },
                {
                    "sent": "Those squared exponential this credential kernel look like with sort of more.",
                    "label": 0
                },
                {
                    "sent": "Like a Brownian motion?",
                    "label": 0
                },
                {
                    "sent": "Or would they just be silk patches but with more jaggedy edges?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, that wouldn't give you smooth function, so you would get more noise around the boundaries.",
                    "label": 0
                },
                {
                    "sent": "But you still get nice solid patches.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what I'm going?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Port is learning that kernel essentially.",
                    "label": 0
                },
                {
                    "sent": "One thing that would seem like we have here is that then there's also especially a model of occlusion that's happening, and that's tightly coupled with the stick breaking process.",
                    "label": 0
                },
                {
                    "sent": "Presumably the thing that tends to get the most mass is also the thing that tends to be in front.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so yes, there's all this nice connections between the stick breaking induces.",
                    "label": 0
                },
                {
                    "sent": "This sequence of assignments, which turns out basically look like occlusion under an orthographic projection model in computer vision.",
                    "label": 0
                },
                {
                    "sent": "So it's just kind of beautiful coincidence of ideas from the two fields, at least from my perspective.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I'm going to sort of go sort of a little more quickly through how we can think about doing inference and learning.",
                    "label": 0
                },
                {
                    "sent": "These models we started a couple of minutes late.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the 1st way I looked about doing inference was with a mean field variational method.",
                    "label": 1
                },
                {
                    "sent": "I was I'm interested in sort of scaling this to big datasets and and so on, and I was a little worried about about MCMC, so variational method for those who haven't seen them essentially look at sort of convexity based bounds on the marginal likelihood of data, so they end up having two terms.",
                    "label": 0
                },
                {
                    "sent": "There's a sort of expected log probability of my hidden variables, as well as an entropy term, and so then you optimize this over the space of all sort of some.",
                    "label": 0
                },
                {
                    "sent": "Distributions of your hidden variables that live in some tractable family.",
                    "label": 0
                },
                {
                    "sent": "And so you start by doing this you do some truncation.",
                    "label": 0
                },
                {
                    "sent": "You say I had this nice infinite model I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to allow myself at most 20 segments per image or something like that, and then in a given image will find some subset of that in a data driven way, you put Gaussian sort of parameters on all these things.",
                    "label": 0
                },
                {
                    "sent": "You have to optimize them.",
                    "label": 0
                },
                {
                    "sent": "I've done a lot of variational methods.",
                    "label": 0
                },
                {
                    "sent": "This is by far the trickiest nastiest model I've ever worked with.",
                    "label": 0
                },
                {
                    "sent": "It's kind of fun, but it was a lot.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of ugly math that goes into these.",
                    "label": 0
                },
                {
                    "sent": "It turns out that you can.",
                    "label": 0
                },
                {
                    "sent": "I do get tractable updates because if I have, for example a Gaussian posterior on the location of a threshold in the Gaussian on the location of a function, I can compute the probability that one Gaussian lies above another in closed form from the normal CDF.",
                    "label": 0
                },
                {
                    "sent": "So some of these identities go in there.",
                    "label": 0
                },
                {
                    "sent": "So you go to a lot of work to build this method.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I really doesn't work well, so this is showing on a single image from lots of different random initializations.",
                    "label": 1
                },
                {
                    "sent": "That sort of trace plots of running this variational method, and this is only about a dozen 10 to 20 iterations and it gets stuck, so wanders up to some mode and it gets stuck and the differences among these modes is something like a factor of, you know that's about 1000 in one to 2000 and log probability, and this is the difference between a really interesting interpretation and just I'll show an example images.",
                    "label": 0
                },
                {
                    "sent": "Garbage interpretatione useless segmentation of the image.",
                    "label": 0
                },
                {
                    "sent": "OK, so Maurice.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've been looking at an alternative method that has a slightly different flavor, so under those variational posteriors it turns out that allowing yourself soft assignments doesn't really help you, because typically there's not much uncertainty when once you have all these variables factorized.",
                    "label": 0
                },
                {
                    "sent": "So I'm just going to get hard assignments of super pixels to layers.",
                    "label": 1
                },
                {
                    "sent": "I'm going to be considering hard partitions and searching over a space of hard partitions.",
                    "label": 0
                },
                {
                    "sent": "This is also ultimately what I want.",
                    "label": 0
                },
                {
                    "sent": "I have to output a partition that's the result of my segmentation algorithm.",
                    "label": 1
                },
                {
                    "sent": "Given a hard partition, I can integrate all of my likelihood parameters analytically, right with no approximations.",
                    "label": 0
                },
                {
                    "sent": "I can integrate all those out.",
                    "label": 0
                },
                {
                    "sent": "And it turns out I can also do very accurate marginalization of these latent functions.",
                    "label": 0
                },
                {
                    "sent": "So you're saying out of all continuous functions, I could have sampled what fraction of those would have given me this particular partition.",
                    "label": 0
                },
                {
                    "sent": "It turns out that using the expectation propagation method, which is a higher order variational method, you get something that's very accurate here.",
                    "label": 0
                },
                {
                    "sent": "The reason that's a big win is that mean field with Gaussian variables completely does the wrong thing with the variances.",
                    "label": 0
                },
                {
                    "sent": "It doesn't capture uncertainty at all, and it gives you sort of terrible estimates EP fixes that there's a lot of literature validating that.",
                    "label": 1
                },
                {
                    "sent": "And the nice thing is now if I'm going to look at hard partitions in search, I don't have to do any finite conservative model truncation.",
                    "label": 0
                },
                {
                    "sent": "So I can change the partition and I just have to integrate over the set of functions that have at least one superpixel assigned to them, but infinite tail of other functions.",
                    "label": 0
                },
                {
                    "sent": "I can just sort of marginalized out analytically, so there's no need to sort of approximate the model.",
                    "label": 0
                },
                {
                    "sent": "We have",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Various search moves.",
                    "label": 0
                },
                {
                    "sent": "This is kind of something that you know at this point is not very elegant, it's just more practical.",
                    "label": 0
                },
                {
                    "sent": "You can say that you know we basically have ways of splitting and merging regions, removing layers, shifting superpixel single super pixels so these don't get you between all modes.",
                    "label": 0
                },
                {
                    "sent": "But one way I like to think about it is that essentially the mean field method.",
                    "label": 0
                },
                {
                    "sent": "All it can do is shift single super pixels.",
                    "label": 0
                },
                {
                    "sent": "It has a hard time making any larger changes.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so here is just sort of a practical example.",
                    "label": 0
                },
                {
                    "sent": "Here's ten runs of mean field.",
                    "label": 1
                },
                {
                    "sent": "Here's ten runs of EP in terms of the likelihoods, they get too.",
                    "label": 0
                },
                {
                    "sent": "We have huge variability.",
                    "label": 0
                },
                {
                    "sent": "Small variability.",
                    "label": 0
                },
                {
                    "sent": "This is the best and worst mode found by EP across these ten runs, so they're not the same, but they're pretty close, right?",
                    "label": 0
                },
                {
                    "sent": "There's one difference at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Here is the best and worst mode run by mean field across these things, so you can see that this one has just has tons in this huge difference, right?",
                    "label": 0
                },
                {
                    "sent": "So if I had the computation the run mean field?",
                    "label": 0
                },
                {
                    "sent": "1000 times or 10,000 times or something like that.",
                    "label": 0
                },
                {
                    "sent": "It might be OK.",
                    "label": 0
                },
                {
                    "sent": "Even the modes that prefers or not as good that you get better answers.",
                    "label": 0
                },
                {
                    "sent": "I think out of EP by better approximating that variance.",
                    "label": 0
                },
                {
                    "sent": "Is that mean field does, yeah.",
                    "label": 0
                },
                {
                    "sent": "Are the vertical axes comfortable?",
                    "label": 0
                },
                {
                    "sent": "And yeah, so that's another.",
                    "label": 0
                },
                {
                    "sent": "That's another point.",
                    "label": 0
                },
                {
                    "sent": "Is that mean field is producing a lower bound on the likelihoods, which is very pessimistic.",
                    "label": 0
                },
                {
                    "sent": "So you see that these range between minus 9500 -- 8000, and these are all clustered around minus 6500, and so the gap between minus 8000 -- 600 is sort of the difference between the pessimistic bound and the more tight approximation of the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "That EP gives you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's another example of sort of practical settings.",
                    "label": 0
                },
                {
                    "sent": "This is sort of like if I run with the number of initializations.",
                    "label": 0
                },
                {
                    "sent": "That's kind of not crazy in terms of computation.",
                    "label": 0
                },
                {
                    "sent": "This is just an example of what you might get from the mean field in the EP, and so it's just a lot more robust.",
                    "label": 0
                },
                {
                    "sent": "I can sort of trust it more.",
                    "label": 0
                },
                {
                    "sent": "I could hope to give somebody the CP code and have them be able to use it reliably, and it's just the mean field.",
                    "label": 0
                },
                {
                    "sent": "I wasn't able to trust the results.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'm going to say a few brief things about.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning, so some of the parameters like this sort of Pitman, your hyperparameters, and these Dursley priors are very easy to sort of sort of set from we have a data set with some human segmentations, and so it's easy to sort of set those using standard likelihood based web methods.",
                    "label": 0
                },
                {
                    "sent": "What's a little more interesting is how I'm going to set this covariance kernel, and the intuition is that there's this one to one mapping between the covariance of two super pixels and the probability that the features that those locations are in the same segment.",
                    "label": 1
                },
                {
                    "sent": "Right high positive correlation means you're likely to be the same segment.",
                    "label": 0
                },
                {
                    "sent": "Strong negative correlation means you're unlikely, and so I'm going to kind of divide my image cues into two pieces.",
                    "label": 1
                },
                {
                    "sent": "I'm going to have color and texture histogram models of what's going on within each region.",
                    "label": 0
                },
                {
                    "sent": "I'm going to treat that in the standard generative way, and then I'm also going to look at sort of contour based cues.",
                    "label": 0
                },
                {
                    "sent": "So if I look at pairs of things and look at a straight line, I say is there are strong contour between them.",
                    "label": 0
                },
                {
                    "sent": "This is something I'm going to sort of capture in a more conditional way you can think of this thing.",
                    "label": 0
                },
                {
                    "sent": "In this thing, and so we're going to look at a set of.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can't see the pictures, but we're going to look at a set of human segmentations.",
                    "label": 0
                },
                {
                    "sent": "And so the point here is that you know in the segmentation problem I can expect to have labeled examples every category I care about, so I can't just sort of learn a model of a bunch of categories.",
                    "label": 0
                },
                {
                    "sent": "But what I can do is learn basically a binary binary classifier and actually I need probabilities.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use logistic regression that says OK if I have this superpixel here in this superpixel here, what's the probability that they're in the same segment in a kind of category independent way?",
                    "label": 1
                },
                {
                    "sent": "And it turns out you can actually learn that in a reasonable way.",
                    "label": 0
                },
                {
                    "sent": "If I have a bunch of human things I could just take a bunch of pairs.",
                    "label": 0
                },
                {
                    "sent": "Have super pixels in those human things?",
                    "label": 0
                },
                {
                    "sent": "Train a classifier.",
                    "label": 0
                },
                {
                    "sent": "Standard features.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I have these estimates of probability of being in the same things and now I can ask what's given these probabilities of being in the same region.",
                    "label": 0
                },
                {
                    "sent": "What's the?",
                    "label": 0
                },
                {
                    "sent": "What's the correspondence between some choice of the correlation of two super pixels and the probability that they'll be in the same region, right?",
                    "label": 1
                },
                {
                    "sent": "And you could be in the same region by both being assigned to layer one or both being assigned to layer two, or both being assigned to layer three.",
                    "label": 0
                },
                {
                    "sent": "So you know there's a little probability, and you can do some low dimensional numerical integration and you get this mapping.",
                    "label": 0
                },
                {
                    "sent": "So this tells me for the correlation varying between negative one and one and my Gaussian processes are going process with unit diagonal, the probability of being in the same region and so.",
                    "label": 0
                },
                {
                    "sent": "As correlation goes to one, that's probability goes to one as correlation goes to as correlation at zero I get the probability I would have with a conventional sort of Chinese restaurant process model as it goes to negative one, I get something that is small but nonzero.",
                    "label": 0
                },
                {
                    "sent": "Depends on the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And then as a.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Final step now if you take your sort of have for every pair of super pixels you have an estimate of what this correlation should be.",
                    "label": 0
                },
                {
                    "sent": "Those may not be consistent with each other, right?",
                    "label": 1
                },
                {
                    "sent": "If I take a covariance matrix, I independently estimate the entries.",
                    "label": 0
                },
                {
                    "sent": "I may get something that's not positive definite, so we have to do a final projection.",
                    "label": 0
                },
                {
                    "sent": "We use a sort of.",
                    "label": 0
                },
                {
                    "sent": "We do a low rank projection, which gives us computational efficiency using the projected gradient method.",
                    "label": 1
                },
                {
                    "sent": "Is actually an interesting sort of geometric thing.",
                    "label": 0
                },
                {
                    "sent": "Is that the way that I get inconsistency is that I have Contour accused that say this guy should is highly correlated with him and he's highly card with him, but those are not highly correlated, right?",
                    "label": 0
                },
                {
                    "sent": "'cause that boundary passes outside, so there's actually loss of sort of transitivity that comes from non convex geometry.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it works.",
                    "label": 0
                },
                {
                    "sent": "So this is a scatter plot, so this is sort of showing this learned sort of model versus a log likelihood under a model that sort of had heuristically set parameters.",
                    "label": 0
                },
                {
                    "sent": "The best I could do with tweaking by hand over a couple of months.",
                    "label": 0
                },
                {
                    "sent": "So you can see below the line means to learn models basically uniformly across almost every image.",
                    "label": 0
                },
                {
                    "sent": "We're doing a much better job of predicting that you know what's likely in terms of human segmentations.",
                    "label": 0
                },
                {
                    "sent": "I can also look at sort of correspondence of the learn likelihoods.",
                    "label": 0
                },
                {
                    "sent": "This is a sort of scatter plot of log likelihood with a Rand index, which is a quantitative evaluation for segmentation.",
                    "label": 1
                },
                {
                    "sent": "They're not perfectly correlated, but there's definitely a sort of sort of positive relationship we'd like there.",
                    "label": 0
                },
                {
                    "sent": "So we think what the model is capturing some interesting things.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'll skip that and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have about one or two minutes and I will just show a few.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results are going to be hard to see, unfortunately, but so.",
                    "label": 0
                },
                {
                    "sent": "This is showing you know, these four columns are results with different sort of.",
                    "label": 0
                },
                {
                    "sent": "Sort of, the most conventional widely used, popular segmentation methods that kind of the right combination of good performance and being not too difficult and so on in the vision literature, most they don't really have anything to do with Asian methods.",
                    "label": 0
                },
                {
                    "sent": "This is the output of our method, and if you kind of the hard part about making a good segmentation is essentially this inference issue.",
                    "label": 0
                },
                {
                    "sent": "I was talking bout this reliability that there's lots of methods that you tweak the parameters.",
                    "label": 0
                },
                {
                    "sent": "You can get it to do well on a couple images, but then you have an image which which there's a lot more objects or a lot fewer objects.",
                    "label": 0
                },
                {
                    "sent": "It will give a bad result, right?",
                    "label": 0
                },
                {
                    "sent": "And so it's kind of getting something that varies that will produce lots of regions.",
                    "label": 0
                },
                {
                    "sent": "Here were only produced two regions here.",
                    "label": 0
                },
                {
                    "sent": "Realize it's just a snake on a background and not much else going on.",
                    "label": 0
                },
                {
                    "sent": "That's what we need out of a good segmentation method, so I should emphasize that when I'm showing you know multiple partitions that have various numbers of regions, this is all the same model, right?",
                    "label": 0
                },
                {
                    "sent": "It's the exact same learn model that feeds into all of this.",
                    "label": 0
                },
                {
                    "sent": "It's just marginal likelihood that's driving.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I terms of quantitative results were sort of.",
                    "label": 0
                },
                {
                    "sent": "There's this Berkeley segmentation data set.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of numbers 'cause there's a bunch of ways to measure segmentation accuracy.",
                    "label": 0
                },
                {
                    "sent": "Roughly speaking, where comparable to most methods and worse than this, this one sort of highly tuned method called GPB on the Berkeley segmentation.",
                    "label": 1
                },
                {
                    "sent": "If we go to a different data set were actually a little bit better than the GP method.",
                    "label": 0
                },
                {
                    "sent": "This is a case where in this Berkeley segmentation data set, people been playing with the same test data for six or seven years now, so there's maybe a little overfitting going on.",
                    "label": 0
                },
                {
                    "sent": "I think there's a lot of room for improvement of our model, so we'd like to run that search longer, which is basically in a matter of improving some aspects of our implementation.",
                    "label": 0
                },
                {
                    "sent": "I didn't talk at all about likelihoods, and turns out this histogram thing I'm doing is pretty simple minded.",
                    "label": 0
                },
                {
                    "sent": "We could do better, and also we're just I'm just taking essentially finding the most probable segmentation we find, and that's actually not a good idea for a lot of these error measures like the Rand index.",
                    "label": 0
                },
                {
                    "sent": "If you want to hedge your bets, you should predict an over segmentation that will, on average, give you lower loss.",
                    "label": 0
                },
                {
                    "sent": "But we have in fact a whole bunch.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different modes, so now we're not.",
                    "label": 0
                },
                {
                    "sent": "We not only have the most probable segmentation for each image, we have lots of segmentations, so these are showing three modes ranked by probability.",
                    "label": 1
                },
                {
                    "sent": "They vary from each other in interesting ways, so for that you see for this bottom one down here I have one in which the couple modes kind of just have the shark.",
                    "label": 0
                },
                {
                    "sent": "This bottom one.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of fish, little fish swimming around in the scene.",
                    "label": 0
                },
                {
                    "sent": "It started to pick those out, so that's less likely, but that is also picked out so you can combine information across these.",
                    "label": 0
                },
                {
                    "sent": "I think that would probably give you better.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results here's other examples of showing.",
                    "label": 0
                },
                {
                    "sent": "For the woman down here on the bottom, it's really easy to see the sort of range of partitions of sort of different resolutions.",
                    "label": 0
                },
                {
                    "sent": "These are not nested in a single tree, like some hierarchical clustering methods give you, and in fact it's really.",
                    "label": 0
                },
                {
                    "sent": "You can't properly capture the uncertainty and image partitions with a single tree of partitions.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just, you know.",
                    "label": 0
                },
                {
                    "sent": "Regardless of the numbers, this is just showing a bunch of results.",
                    "label": 0
                },
                {
                    "sent": "I don't really care so much whether I'm the second best or the best segmentation method is something that I think is producing very interesting results and really kind of is very accurate for this problem and is doing the things I like in that.",
                    "label": 0
                },
                {
                    "sent": "It's sort of varying the resolution across images in a sensible way so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh I think you know I'm.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sort of optimistic about being able to do successful Asian American modeling, but I think it requires a lot more than tossing out a new model, right?",
                    "label": 0
                },
                {
                    "sent": "So you have a lot of work here.",
                    "label": 0
                },
                {
                    "sent": "Went into studying model assumptions, match the statistics model comparisons.",
                    "label": 1
                },
                {
                    "sent": "I actually compared about 10 models before I picked that.",
                    "label": 0
                },
                {
                    "sent": "First one is the best one for this domain.",
                    "label": 1
                },
                {
                    "sent": "We need sort of reliable, consistent inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "The basic mean field and basic sampling algorithms don't cut it, but I think we'll hear more about that later today.",
                    "label": 0
                },
                {
                    "sent": "And we in the learning problem.",
                    "label": 0
                },
                {
                    "sent": "I think maybe you know we need to think a little more.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of cases where if you want to have a clustering method, we just can't hope to do everything based on marginal likelihood of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "There's so many ways to cluster data, so if we want clusterings that are consistent with something, we might think about.",
                    "label": 0
                },
                {
                    "sent": "If there are smart ways to get side information for part of our thing, so that's kind of what I did with my had a set of human segmentations, I couldn't learn about everything I could learn about a few aspects and that was really important for this problem.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}