{
    "id": "6emqdojtowsb7mwiwilipef5lf5q2gsb",
    "title": "A Corpus-based Evaluation Method for Distributional Semantic Models",
    "info": {
        "author": [
            "Abdellah Fourtassi, Laboratoire de Sciences Cognitives et Psycholinguistique, \u00c9cole Normale Sup\u00e9rieure (ENS)"
        ],
        "published": "Oct. 2, 2013",
        "recorded": "August 2013",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/acl2013_fourtassi_models/",
    "segmentation": [
        [
            "Thank you for being here.",
            "So I'm going to talk about a corpus based evaluation methods for distributional semantic models.",
            "And in order to understand why we needed such evaluation method, let's start from the big."
        ],
        [
            "In what we are mainly interested here is the similarities between words and if we look in the literature we can find.",
            "Generally to be classes of algorithms so we can find the thesaurus based algorithms that make use of relationship talk talk.",
            "So monocle relationships between words using an online thesaurus like word Nets.",
            "But we're not going to focus on this class of algorithm, because unfortunately we don't have such thesauri or C services for all the languages that we are interested in.",
            "So we are going to focus on distributional semantic models like latent semantic analysis or latent Dursley allocations.",
            "And these models compute the similarities between words based on their distribution in large textual corpora.",
            "And the question we are asking here is.",
            "How can we evaluate these models?",
            "Or how can we evaluate these kind of models?",
            "How can we be sure that this similarities they are giving us reflects?",
            "The human intuition so."
        ],
        [
            "If we want to somehow classify the evaluation methods out there, we can propose the following or so.",
            "We have the intrinsic extrinsic methods and the intrinsic methods.",
            "The intrinsic methods are the ones that use some external resources or human expertise.",
            "And we can do this either quantitatively by comparing the results of the model to human generated databases or benchmark.",
            "Um, but if we don't have these resources as it is.",
            "Mostly the case we can do it qualitatively by relying on the research or intuition and the way this is classically learn is by selecting high frequency words and examining their nearest neighbors and see if the fit the intuition.",
            "But what if the researcher doesn't speak these languages?",
            "And actually, this is what motivated the work, because I was working on language acquisition, I was not doing this kind of work.",
            "I was trying to understand how children learn phonetic categories, and I wanted to see if semantic information could help.",
            "So I wanted to use these semantic models, but I couldn't find any extrinsic methods to systematically evaluate and compare these models on different languages.",
            "And This is why we need."
        ],
        [
            "Intrinsic methods and in the literature the most famous one is the hell out likelihood, where you have a subset of the data and you compute the probability of the model generating this subset that he has never seen, it has never seen before.",
            "However, this method has some problems.",
            "It could not be applied to non probabilistic models like latent semantic analysis.",
            "It could be costly computationally at most.",
            "Importantly, it does not work as it was shown by chunk in a NIPS paper in 2009.",
            "He shows that likelihood probabilities does not do not correlate with human judgments."
        ],
        [
            "So we need our method to be intrinsic, but at the same time to predict human judgments and to be easy to implement.",
            "So be."
        ],
        [
            "Or explaining the method.",
            "Let's just have a small reminder of.",
            "Distributional semantic models and, more precisely latent semantic analysis.",
            "Because this is the model we're going to use, so we have we have.",
            "Big corporate divided into.",
            "The corpus divided into documents and we model each word by a vector, where each entry represent the frequency by which this word occurs in given documents and we end up with matrix big matrix and we perform some additional transformation of this matrix to reduce noises and arrive semantic space and then eventually we can compare 2 words by measuring the angle between their vectors and then taking the cosine of this angle so.",
            "For example, I presented here we have, we see that cats and dogs have similar distributions in the corpus.",
            "This is why they will end up having high semantic similarity, whereas 2 words like dog and Apple, which have different distribution are not going to be to have high similar semantics."
        ],
        [
            "So this is maybe the most important slide.",
            "In this presentation we called our committed SDT or for reason that would clear soon.",
            "So this is how it works.",
            "So we take corpus.",
            "We select a small set of words and we replace each instance of this word.",
            "These words by one of two lexicon varieties.",
            "So, for example, arts would become part 1/2 of the time and are two of the time and the same thing for dog, and so forth.",
            "And we call this the pseudo synonym Corpus.",
            "Then we derive we apply Latin semantic analysis and we derive semantic space.",
            "So intuitively, if the model is doing a good job, then it should give semantic similarity between the pseudo synonyms art one or two.",
            "For example, that is higher than a semantic similarity between random peer.",
            "For example here art, one dog one.",
            "So we quantified, quantify this intuition by using a simple binary classifier that performs a same different detection task.",
            "And the results are computed by the standard area under the curve that we called all here and it gives us the probability that given two pairs of words, wanna sort of synonyms and the other one random, fear, the probability that these two pairs are going to be correctly classified just based on the."
        ],
        [
            "Cosine similarity.",
            "OK, so let's see how we're doing on our checklist.",
            "So now we have an interesting, committed and we need to prove that it works.",
            "Actually, that it predicts human judgments, and we are going to."
        ],
        [
            "Use some benchmarks, some human generated database.",
            "So the first one is where the social norms collected by Nelson and colleagues and this way the created this database.",
            "They presented people with proper words like book Young and we ask people to give the first word that comes to mind and the rank these associate's based on the frequency by which they were named by people.",
            "And we're going to apply this to our model.",
            "So first we generate a semantic model from.",
            "Wiki Corpus, which is a corpus composed of artikkel randomly selected from Wikipedia and.",
            "We took the overlap of the vocabulary of the corpus with the norms, and then we computed the similarities of all the pairs in the overlap and then we looked at the rank of the three first asociates as given by LSA.",
            "So for example, for the first associate, we compute the median rank, the meeting of the second Associate, and the median rank of the Third Associate.",
            "So if the model is doing a good job, then it should predict the human ranking of the associate's right, and this is what's."
        ],
        [
            "It happens, so if we plot the median rank, the medium ranks of different associate's and we vary the semantic dimensions, we see that the associate's or associate one is always better than the it's.",
            "It doesn't overlap with the other associate's and so forth.",
            "So what this plot shows is that LSA is actually a good model.",
            "It could predict human judgments.",
            "But this is not what we are interested in.",
            "We are mostly interested in.",
            "The optimal value of the model when we vary the parameters and if we if you see here for example, the global trend, you will find that as we increase the number of semantic dimensions, the overall performance of the model gets better and the question we are asking here is whether or not our method which is DT whole, can predict these threads.",
            "In other words, does median rank correlates with our measure right?"
        ],
        [
            "So this is some results if when we vary the diamond dimensions on the corpus size, we see that our method which is on the right predicts quite well the global effects of dimension and also the corpus size."
        ],
        [
            "And this is the correlation for document lengths.",
            "Our method predicts that document of that that has 10 sentences is psychologically optimal."
        ],
        [
            "And this is the overall correlation between the median rank and the STT hole, and we see that we have quite a good correlation, right?",
            "But just."
        ],
        [
            "To make sure we use another database human generated database, which is that awful synonym test that was used by the inventors of LSA actually.",
            "And this database consists of on words, probe words and for alternative answers, only one of which is defined as the correct one.",
            "And As for the median rank we want to see if.",
            "The Institute whole correlates with the percentage of correct answers, and here is the."
        ],
        [
            "Roll correlation Anne works quite well, right?"
        ],
        [
            "So let's see how we're doing normal check checklist.",
            "So intrinsic method.",
            "Yes, predict human judgment."
        ],
        [
            "So now the question is, how can we use this method in practice, right?",
            "I mean the correlation I presented were computed over the overlap between the vocabulary of the corporate of the corpus and the norms and of the norms of total data set.",
            "But as the main idea idea is to apply this method.",
            "Corpora and languages that for which there is no such external resources.",
            "So we are interested in knowing how stable is the predictive power of our measure when we now just select random words, and we want to know with what are the properties of these random words in terms of number, the number, and the frequency that makes that gives high correlations.",
            "So we compute it again.",
            "The correlation of the table with the median rank.",
            "And here is the results.",
            "So let's start from the right.",
            "This is what we did previously.",
            "This is the correlation with the overlap and it gives the most the highest correlation value we can obtain.",
            "And this is when we take all the words in the corpus, right?",
            "All the weather in corpus, and so it gives a value that's not as high as the overlap, but still it's get.",
            "It gives a significant correlation value.",
            "And then we we varied.",
            "We took random words and we vary the size of the set from 100 words to 1000 words and the frequency of these words and what these results say is that small sets of random words.",
            "In this case, they represent less than 3% of the total vocabulary of the corpus.",
            "The if they are chosen in a mid frequency range then they could give results as high as the ones obtained with the overlap, and this is no."
        ],
        [
            "Big surprise because we know since learned that mid frequency words have more discriminating power."
        ],
        [
            "So I think I we completed our checklist here and as a conclusion we provide here an evaluation method that could be used as a proxy when no human generating data is available, it could be used to set the parameters like the semantic dimensions, document length or corpus size.",
            "However, this method just give global and rough measure of the semantic quality, but we are working on a word level variant of the method that could hopefully enable us to examine some more fine grained properties like.",
            "Difference between abstract concrete words and the distribution of different linguistic categories."
        ],
        [
            "That thank you.",
            "It seems like this would be very good for being able to compare in a situation where the models are based on the same parameters, right?",
            "So similar context and window size and so on.",
            "Do you have any intuition as to how?",
            "If you could start comparing words with different parameters within the space?",
            "Yes, so were derived from different models with different selection of parameters.",
            "Actually we didn't test this kind of possibilities and no.",
            "I have no intuition about this.",
            "Any other questions 'cause I have one more that I'll go ahead and ask.",
            "So you said that you started this.",
            "Sorry that you started this mainly because of your previous your other work that you did my PhD thesis exactly, which I guess it doesn't seem necessarily related to this, but how do you apply them this to that work?",
            "Oh OK, so right?",
            "So I'm not sure I can explain this in 5 minutes.",
            "I mean what we what we're trying to do is to see whether so we're trying to learn phonetic categories, right?",
            "How children could reduce allophony too.",
            "Allophonic variant phonemic categories.",
            "So for example, we can have the word cat and cat, so it's either localized or with T, and children should know how to classify cluster these two variants in the same category, and we wanted to see whether information in the level at the level of the Mexican could help cluster these two.",
            "So allophone so for example if we have cat and care.",
            "Ann from some distribution information we know that Cat and Cat happens on the same.",
            "Occur in the same context so they will have similar similar semantic distance, so children may use this information to cluster cat and care as minimum pyron.",
            "See OK, this is a minimum here.",
            "This is a phonetic variant of one another, so T and glottal stop are just telephoning, so this is the same work.",
            "This is kind of idea."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thank you for being here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about a corpus based evaluation methods for distributional semantic models.",
                    "label": 1
                },
                {
                    "sent": "And in order to understand why we needed such evaluation method, let's start from the big.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In what we are mainly interested here is the similarities between words and if we look in the literature we can find.",
                    "label": 0
                },
                {
                    "sent": "Generally to be classes of algorithms so we can find the thesaurus based algorithms that make use of relationship talk talk.",
                    "label": 1
                },
                {
                    "sent": "So monocle relationships between words using an online thesaurus like word Nets.",
                    "label": 0
                },
                {
                    "sent": "But we're not going to focus on this class of algorithm, because unfortunately we don't have such thesauri or C services for all the languages that we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So we are going to focus on distributional semantic models like latent semantic analysis or latent Dursley allocations.",
                    "label": 0
                },
                {
                    "sent": "And these models compute the similarities between words based on their distribution in large textual corpora.",
                    "label": 0
                },
                {
                    "sent": "And the question we are asking here is.",
                    "label": 0
                },
                {
                    "sent": "How can we evaluate these models?",
                    "label": 0
                },
                {
                    "sent": "Or how can we evaluate these kind of models?",
                    "label": 0
                },
                {
                    "sent": "How can we be sure that this similarities they are giving us reflects?",
                    "label": 0
                },
                {
                    "sent": "The human intuition so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we want to somehow classify the evaluation methods out there, we can propose the following or so.",
                    "label": 0
                },
                {
                    "sent": "We have the intrinsic extrinsic methods and the intrinsic methods.",
                    "label": 1
                },
                {
                    "sent": "The intrinsic methods are the ones that use some external resources or human expertise.",
                    "label": 0
                },
                {
                    "sent": "And we can do this either quantitatively by comparing the results of the model to human generated databases or benchmark.",
                    "label": 0
                },
                {
                    "sent": "Um, but if we don't have these resources as it is.",
                    "label": 0
                },
                {
                    "sent": "Mostly the case we can do it qualitatively by relying on the research or intuition and the way this is classically learn is by selecting high frequency words and examining their nearest neighbors and see if the fit the intuition.",
                    "label": 0
                },
                {
                    "sent": "But what if the researcher doesn't speak these languages?",
                    "label": 1
                },
                {
                    "sent": "And actually, this is what motivated the work, because I was working on language acquisition, I was not doing this kind of work.",
                    "label": 0
                },
                {
                    "sent": "I was trying to understand how children learn phonetic categories, and I wanted to see if semantic information could help.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to use these semantic models, but I couldn't find any extrinsic methods to systematically evaluate and compare these models on different languages.",
                    "label": 0
                },
                {
                    "sent": "And This is why we need.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intrinsic methods and in the literature the most famous one is the hell out likelihood, where you have a subset of the data and you compute the probability of the model generating this subset that he has never seen, it has never seen before.",
                    "label": 0
                },
                {
                    "sent": "However, this method has some problems.",
                    "label": 0
                },
                {
                    "sent": "It could not be applied to non probabilistic models like latent semantic analysis.",
                    "label": 0
                },
                {
                    "sent": "It could be costly computationally at most.",
                    "label": 1
                },
                {
                    "sent": "Importantly, it does not work as it was shown by chunk in a NIPS paper in 2009.",
                    "label": 1
                },
                {
                    "sent": "He shows that likelihood probabilities does not do not correlate with human judgments.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need our method to be intrinsic, but at the same time to predict human judgments and to be easy to implement.",
                    "label": 0
                },
                {
                    "sent": "So be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or explaining the method.",
                    "label": 0
                },
                {
                    "sent": "Let's just have a small reminder of.",
                    "label": 0
                },
                {
                    "sent": "Distributional semantic models and, more precisely latent semantic analysis.",
                    "label": 1
                },
                {
                    "sent": "Because this is the model we're going to use, so we have we have.",
                    "label": 0
                },
                {
                    "sent": "Big corporate divided into.",
                    "label": 0
                },
                {
                    "sent": "The corpus divided into documents and we model each word by a vector, where each entry represent the frequency by which this word occurs in given documents and we end up with matrix big matrix and we perform some additional transformation of this matrix to reduce noises and arrive semantic space and then eventually we can compare 2 words by measuring the angle between their vectors and then taking the cosine of this angle so.",
                    "label": 0
                },
                {
                    "sent": "For example, I presented here we have, we see that cats and dogs have similar distributions in the corpus.",
                    "label": 0
                },
                {
                    "sent": "This is why they will end up having high semantic similarity, whereas 2 words like dog and Apple, which have different distribution are not going to be to have high similar semantics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is maybe the most important slide.",
                    "label": 0
                },
                {
                    "sent": "In this presentation we called our committed SDT or for reason that would clear soon.",
                    "label": 0
                },
                {
                    "sent": "So this is how it works.",
                    "label": 0
                },
                {
                    "sent": "So we take corpus.",
                    "label": 0
                },
                {
                    "sent": "We select a small set of words and we replace each instance of this word.",
                    "label": 0
                },
                {
                    "sent": "These words by one of two lexicon varieties.",
                    "label": 0
                },
                {
                    "sent": "So, for example, arts would become part 1/2 of the time and are two of the time and the same thing for dog, and so forth.",
                    "label": 0
                },
                {
                    "sent": "And we call this the pseudo synonym Corpus.",
                    "label": 0
                },
                {
                    "sent": "Then we derive we apply Latin semantic analysis and we derive semantic space.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, if the model is doing a good job, then it should give semantic similarity between the pseudo synonyms art one or two.",
                    "label": 0
                },
                {
                    "sent": "For example, that is higher than a semantic similarity between random peer.",
                    "label": 0
                },
                {
                    "sent": "For example here art, one dog one.",
                    "label": 0
                },
                {
                    "sent": "So we quantified, quantify this intuition by using a simple binary classifier that performs a same different detection task.",
                    "label": 0
                },
                {
                    "sent": "And the results are computed by the standard area under the curve that we called all here and it gives us the probability that given two pairs of words, wanna sort of synonyms and the other one random, fear, the probability that these two pairs are going to be correctly classified just based on the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's see how we're doing on our checklist.",
                    "label": 0
                },
                {
                    "sent": "So now we have an interesting, committed and we need to prove that it works.",
                    "label": 0
                },
                {
                    "sent": "Actually, that it predicts human judgments, and we are going to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use some benchmarks, some human generated database.",
                    "label": 0
                },
                {
                    "sent": "So the first one is where the social norms collected by Nelson and colleagues and this way the created this database.",
                    "label": 0
                },
                {
                    "sent": "They presented people with proper words like book Young and we ask people to give the first word that comes to mind and the rank these associate's based on the frequency by which they were named by people.",
                    "label": 0
                },
                {
                    "sent": "And we're going to apply this to our model.",
                    "label": 0
                },
                {
                    "sent": "So first we generate a semantic model from.",
                    "label": 0
                },
                {
                    "sent": "Wiki Corpus, which is a corpus composed of artikkel randomly selected from Wikipedia and.",
                    "label": 0
                },
                {
                    "sent": "We took the overlap of the vocabulary of the corpus with the norms, and then we computed the similarities of all the pairs in the overlap and then we looked at the rank of the three first asociates as given by LSA.",
                    "label": 0
                },
                {
                    "sent": "So for example, for the first associate, we compute the median rank, the meeting of the second Associate, and the median rank of the Third Associate.",
                    "label": 1
                },
                {
                    "sent": "So if the model is doing a good job, then it should predict the human ranking of the associate's right, and this is what's.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It happens, so if we plot the median rank, the medium ranks of different associate's and we vary the semantic dimensions, we see that the associate's or associate one is always better than the it's.",
                    "label": 0
                },
                {
                    "sent": "It doesn't overlap with the other associate's and so forth.",
                    "label": 0
                },
                {
                    "sent": "So what this plot shows is that LSA is actually a good model.",
                    "label": 0
                },
                {
                    "sent": "It could predict human judgments.",
                    "label": 0
                },
                {
                    "sent": "But this is not what we are interested in.",
                    "label": 0
                },
                {
                    "sent": "We are mostly interested in.",
                    "label": 0
                },
                {
                    "sent": "The optimal value of the model when we vary the parameters and if we if you see here for example, the global trend, you will find that as we increase the number of semantic dimensions, the overall performance of the model gets better and the question we are asking here is whether or not our method which is DT whole, can predict these threads.",
                    "label": 0
                },
                {
                    "sent": "In other words, does median rank correlates with our measure right?",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is some results if when we vary the diamond dimensions on the corpus size, we see that our method which is on the right predicts quite well the global effects of dimension and also the corpus size.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the correlation for document lengths.",
                    "label": 0
                },
                {
                    "sent": "Our method predicts that document of that that has 10 sentences is psychologically optimal.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the overall correlation between the median rank and the STT hole, and we see that we have quite a good correlation, right?",
                    "label": 0
                },
                {
                    "sent": "But just.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make sure we use another database human generated database, which is that awful synonym test that was used by the inventors of LSA actually.",
                    "label": 0
                },
                {
                    "sent": "And this database consists of on words, probe words and for alternative answers, only one of which is defined as the correct one.",
                    "label": 0
                },
                {
                    "sent": "And As for the median rank we want to see if.",
                    "label": 0
                },
                {
                    "sent": "The Institute whole correlates with the percentage of correct answers, and here is the.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roll correlation Anne works quite well, right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's see how we're doing normal check checklist.",
                    "label": 0
                },
                {
                    "sent": "So intrinsic method.",
                    "label": 0
                },
                {
                    "sent": "Yes, predict human judgment.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the question is, how can we use this method in practice, right?",
                    "label": 1
                },
                {
                    "sent": "I mean the correlation I presented were computed over the overlap between the vocabulary of the corporate of the corpus and the norms and of the norms of total data set.",
                    "label": 0
                },
                {
                    "sent": "But as the main idea idea is to apply this method.",
                    "label": 0
                },
                {
                    "sent": "Corpora and languages that for which there is no such external resources.",
                    "label": 0
                },
                {
                    "sent": "So we are interested in knowing how stable is the predictive power of our measure when we now just select random words, and we want to know with what are the properties of these random words in terms of number, the number, and the frequency that makes that gives high correlations.",
                    "label": 0
                },
                {
                    "sent": "So we compute it again.",
                    "label": 0
                },
                {
                    "sent": "The correlation of the table with the median rank.",
                    "label": 1
                },
                {
                    "sent": "And here is the results.",
                    "label": 0
                },
                {
                    "sent": "So let's start from the right.",
                    "label": 0
                },
                {
                    "sent": "This is what we did previously.",
                    "label": 0
                },
                {
                    "sent": "This is the correlation with the overlap and it gives the most the highest correlation value we can obtain.",
                    "label": 1
                },
                {
                    "sent": "And this is when we take all the words in the corpus, right?",
                    "label": 0
                },
                {
                    "sent": "All the weather in corpus, and so it gives a value that's not as high as the overlap, but still it's get.",
                    "label": 0
                },
                {
                    "sent": "It gives a significant correlation value.",
                    "label": 0
                },
                {
                    "sent": "And then we we varied.",
                    "label": 1
                },
                {
                    "sent": "We took random words and we vary the size of the set from 100 words to 1000 words and the frequency of these words and what these results say is that small sets of random words.",
                    "label": 0
                },
                {
                    "sent": "In this case, they represent less than 3% of the total vocabulary of the corpus.",
                    "label": 0
                },
                {
                    "sent": "The if they are chosen in a mid frequency range then they could give results as high as the ones obtained with the overlap, and this is no.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Big surprise because we know since learned that mid frequency words have more discriminating power.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think I we completed our checklist here and as a conclusion we provide here an evaluation method that could be used as a proxy when no human generating data is available, it could be used to set the parameters like the semantic dimensions, document length or corpus size.",
                    "label": 1
                },
                {
                    "sent": "However, this method just give global and rough measure of the semantic quality, but we are working on a word level variant of the method that could hopefully enable us to examine some more fine grained properties like.",
                    "label": 0
                },
                {
                    "sent": "Difference between abstract concrete words and the distribution of different linguistic categories.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That thank you.",
                    "label": 0
                },
                {
                    "sent": "It seems like this would be very good for being able to compare in a situation where the models are based on the same parameters, right?",
                    "label": 0
                },
                {
                    "sent": "So similar context and window size and so on.",
                    "label": 0
                },
                {
                    "sent": "Do you have any intuition as to how?",
                    "label": 0
                },
                {
                    "sent": "If you could start comparing words with different parameters within the space?",
                    "label": 0
                },
                {
                    "sent": "Yes, so were derived from different models with different selection of parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually we didn't test this kind of possibilities and no.",
                    "label": 0
                },
                {
                    "sent": "I have no intuition about this.",
                    "label": 0
                },
                {
                    "sent": "Any other questions 'cause I have one more that I'll go ahead and ask.",
                    "label": 0
                },
                {
                    "sent": "So you said that you started this.",
                    "label": 0
                },
                {
                    "sent": "Sorry that you started this mainly because of your previous your other work that you did my PhD thesis exactly, which I guess it doesn't seem necessarily related to this, but how do you apply them this to that work?",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not sure I can explain this in 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "I mean what we what we're trying to do is to see whether so we're trying to learn phonetic categories, right?",
                    "label": 0
                },
                {
                    "sent": "How children could reduce allophony too.",
                    "label": 0
                },
                {
                    "sent": "Allophonic variant phonemic categories.",
                    "label": 0
                },
                {
                    "sent": "So for example, we can have the word cat and cat, so it's either localized or with T, and children should know how to classify cluster these two variants in the same category, and we wanted to see whether information in the level at the level of the Mexican could help cluster these two.",
                    "label": 0
                },
                {
                    "sent": "So allophone so for example if we have cat and care.",
                    "label": 0
                },
                {
                    "sent": "Ann from some distribution information we know that Cat and Cat happens on the same.",
                    "label": 0
                },
                {
                    "sent": "Occur in the same context so they will have similar similar semantic distance, so children may use this information to cluster cat and care as minimum pyron.",
                    "label": 0
                },
                {
                    "sent": "See OK, this is a minimum here.",
                    "label": 0
                },
                {
                    "sent": "This is a phonetic variant of one another, so T and glottal stop are just telephoning, so this is the same work.",
                    "label": 0
                },
                {
                    "sent": "This is kind of idea.",
                    "label": 0
                }
            ]
        }
    }
}