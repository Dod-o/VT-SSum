{
    "id": "nlztpts7n2kwreyij7k5s7wsldx5hg45",
    "title": "How classifieres can be use to solve any reasonable loss",
    "info": {
        "author": [
            "John Langford, Microsoft Research"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mcslw04_langford_hccus/",
    "segmentation": [
        [
            "Speaker of the base.",
            "Speak about how classifier can be used to solve.",
            "Any nothing at all.",
            "Alright thanks.",
            "OK, so this is joint work with a bunch of other people.",
            "And the number is growing.",
            "If you have questions where I'm talking, you should feel."
        ],
        [
            "Need to ask him then OK so start out with sort of what is a dream of machine learning.",
            "So this is sort of a communal dream of a large number of people and the communal hope of larger people is it's possible to create some sort of pretty universal prediction device right?",
            "So you feed in a bunch of data and then it starts making predictions for you and you're getting the answer.",
            "So.",
            "A basic question is, can you do this?",
            "And you can say both know and yes, depending on exactly how strong you expect this to be.",
            "So the obvious answer is no, you can't do this because.",
            "Well, if you're familiar with encryption, people are trying very hard and quite successfully to make things not predictable.",
            "Another way of saying this is that.",
            "There are certain learning problems.",
            "OK, so for any learning problem for any prediction problem, if you know the answer in advance then what you want this box to be is just the thing which uses.",
            "The correct prediction method.",
            "But you don't know that and you can't know that if you're going to have some sort of universal device, so you're always going to lose in comparison to someone who actually knows the correct answer.",
            "So incription this incription, this would be the person who knows the secret key.",
            "It is however, possible as far as we know.",
            "To do as well as humans can do it right so humans can solve a diverse number of prediction problems, sometimes more or less successfully.",
            "And it may be feasible to create a machine which can do something like that.",
            "And people think this is possible.",
            "If you think it's not possible.",
            "Yeah, so I think it's possible to build a box which is as good as a human.",
            "For solving things across all domains that human could solve.",
            "Not in our lifetime.",
            "If it's a safe answer.",
            "I'm not certain, actually.",
            "I think it might be possible, maybe even in our lifetimes.",
            "So there's a question of just how strong you want this this universality to be.",
            "And maybe there are are weak enough notions that it's possible, and.",
            "And.",
            "And there's a question about how you might go about doing that.",
            "OK, so there's two variations that you find."
        ],
        [
            "One of them is.",
            "Sort of.",
            "For every new different type of prediction problem, you create a new algorithm.",
            "So there's classification and there's regression.",
            "There's reinforcement learning, and there's ranking, and this causes classification and the structured prediction.",
            "So.",
            "That's one way, and then another way is you have some engine which is in between some sort of data transformation layer and some other prediction transformation layer.",
            "Right, so maybe if you're solving a classification problem, you have some method for turning the classification into whatever you're going to feed into your core engine, and then you have some method of getting out the answer in the core engine.",
            "You can do this for.",
            "For many different learning problems.",
            "So this is what this talk is about.",
            "We're going to be talking about methods for taking.",
            "Pretty arbitrary problems transforming actually this engine here will just be a classifier.",
            "And we're going to take these problems are going to transform them into classification problems.",
            "We're going to try to learn to predict correctly the individual classification problems, and then we're going to take the predictions.",
            "And we're going to transform them to solve the original problem.",
            "So there's."
        ],
        [
            "Very basic questions that you should be asking.",
            "One of them is, can this work mathematically?",
            "In the answer is yes, there is a way to think about this mathematically.",
            "To prove that you have a good transformation an.",
            "And then there's a question about does this work empirically, right?",
            "So it may be that although you can create this good transformation mathematically, it doesn't actually work in practice, and the answer is actually, it does work fairly well fairly often in practice.",
            "And then another question is just how do you reason about this process of transforming the data and transforming the prediction?",
            "That pair of processes.",
            "And then there's sort of a new theorem style which goes along with this.",
            "So this is this is.",
            "Unlike the kinds of theorems that you're used to, where you assume independence of the data, it's unlike online learning where you don't assume independence data.",
            "But the question that you're answering is different.",
            "So this is kind of a different kind of theorem.",
            "Sorry, I don't mean to quote Wolfram."
        ],
        [
            "Right, actually I had a conversation with him.",
            "When something like this I said.",
            "You know, I think that science is useful because it can predict things.",
            "And he said something like.",
            "Science is about much more than predicting things.",
            "It's also about explaining.",
            "He said he, well, you get to into predicting things.",
            "Talk to me.",
            "OK, so.",
            "There's some baggage which goes along with this, which is sort of a new style to trying to solve learning problems and what I want to do is I want to sort of show you this style and along the way I'll show you a theorem so there's several different reductions that we've worked out a sufficient number so that I'm comfortable.",
            "Saying reasonable.",
            "So what I did was I picked one which is neither particularly difficult or particularly easy, just sort of representative.",
            "An will just kind of go through how you try to solve the problem using this approach.",
            "So.",
            "Right, so the first step is going to be.",
            "You want to try to define the real problem you know, try to define it very crisply.",
            "The second step is going to be creating this transform pair.",
            "The third step is going to be analyzing this created transform pair just mathematically.",
            "And then if we're happy with the analysis, then we just use it in.",
            "Otherwise we go to here and we hope that the math actually helps us do this better.",
            "Turns out often it does.",
            "OK, So what is the learning problem?",
            "So I'm going to think of learning problem as a set of things and.",
            "One of these OK, so let's see we're familiar with features, right?",
            "So the X is just features.",
            "And with."
        ],
        [
            "Here with the prediction space, it's very common in all formulations, so this is like 0 one or some real value differing regression or whatever.",
            "OK, so the prediction space and the advice space are separated here.",
            "And it's actually important for a lot of learning problems.",
            "So what does this mean?",
            "So at training time, what's going to happen is you're going to get.",
            "X cross K samples.",
            "Any going to use these samples to make a prediction prediction device from X to Y?",
            "Yeah, yeah.",
            "And then your loss is just going to be advice, cross whatever prediction you make map to some positive number.",
            "Sometimes called an action space in testicles.",
            "Right, right, right.",
            "The prediction.",
            "The prediction is the action.",
            "Because pay was the case with the.",
            "Just say what you called it.",
            "Were you saying that wise decision for K?",
            "Which is the learner generating why it's generating?",
            "A predictor going from X to Y?",
            "OK yeah, so pay pay which would normally be the the training data values of the true.",
            "It is for some problems it is essential to have the extra generality that it can be something other than just the prediction space.",
            "Get back.",
            "Intermediate, but you get some parameters like a state space and then offensive prediction.",
            "Case intermediate between.",
            "So it will turn out that.",
            "Examples.",
            "It's not exactly a parameter value, because it doesn't necessarily parameterise the process generating the features are like that.",
            "So actually Haeussler uses generalization.",
            "David Haussler is one of the early learning theory people.",
            "Right, OK?",
            "K is just advice, so we're going to have some algorithm which is going to take it as input X cross.",
            "K star is going to output.",
            "Exxaro why?",
            "Right?",
            "An no no.",
            "K could be why so used to K being why?",
            "Right, so it's just information.",
            "It's information.",
            "It's advice about what.",
            "Why should be which is not available at testing time.",
            "So K could be the labels could be something else too, but could be labels.",
            "I mean in other fields it might be cool to states, but some representation of build code.",
            "State yeah OK so.",
            "Right, let's go.",
            "Here.",
            "So there's this KDD conference and.",
            "They have little championships, right so?",
            "The championship is judged in some way according to some criterion, right?",
            "So there are actually two different problems associated with championship and one of the things you want to do is try to maximize the zero inaccuracy.",
            "That's the one we're very familiar with.",
            "Another one was maximizing the area under the Arosi curve, so this is some sort of complicated thing.",
            "Little bit tricky to describe.",
            "The other one was minimizing the cross entropy, so there's some for the formula for area.",
            "There are seeker there.",
            "It's kind of if you order everything from most 022 most one.",
            "How many times do you have to?",
            "Do a binary swap in order to.",
            "Reorder to get the all zeros and then all ones.",
            "There's cross entropy, which is.",
            "Write the average probability of the truth according to some.",
            "Method of predicting probabilities.",
            "There's this Q score which is totally crazy.",
            "There is OK, so this is not quite a standard classification problem, this is.",
            "A problem where you.",
            "Are trying to predict if a bunch of different approaching sequences are Houma logs.",
            "I think of some other protein sequence based upon some features.",
            "So top one says if the one that I ranked most likely to be the same is actually the truth, then I get one.",
            "Otherwise they get 0.",
            "There's a root mean squared error.",
            "This is another one we're familiar with.",
            "This is just typical regression criterion.",
            "This is mean average precision.",
            "Note that so the point is that the world kind of imposes a bunch of different loss functions on us.",
            "And it's actually sometimes controversial too.",
            "To say this because a lot of people regard a loss function is a free parameter of a learning system.",
            "But there's another notion of loss, which is the thing which is actually imposed by the real world, and in one sense or another.",
            "The championship here is some sort of real world boss.",
            "So you want to optimize that and you don't have any choice, you can't.",
            "You can't just ignore it.",
            "OK, so there's a lot of different losses here and what I would like to say is, oh, it's easy to just make a reduction for each individual loss and then optimize that, but that's a little bit complicated and when we're playing this with this, we didn't actually have time.",
            "OK, so.",
            "The."
        ],
        [
            "First step is defining the problem and I can tell you a sufficient problem for which I can make predictions.",
            "That kind of hope to handle all of those losses, and that is just doing class probability estimation.",
            "So what happens here is?",
            "You have.",
            "A major D on X cross zero or one.",
            "So this is just standard classification.",
            "But now your predictor is going to predict into the space 021, so it's going to make some sort of probabilistic prediction.",
            "And we're going to have his input just a standard classification data set.",
            "I'm going to find some probabilistic classifier which optimizes the squared error.",
            "OK, so the claim is that if you have this CPU is something saying you can do with your predictions too.",
            "Try to solve our area to the RC curve and top one and I don't know what else.",
            "All the rest of yours.",
            "Even the slack queue thing, which is really weird.",
            "So this is the first step in trying to solve things in sort of a reduction style.",
            "Try to isolate some particular problem.",
            "And then the next step is trying to come up with."
        ],
        [
            "Some sort of algorithm for solving this class probability estimation given our core engine so.",
            "Me coming from kind of a learning theory background.",
            "My core engine is always the classifier.",
            "There's nothing particularly compelling about a classifier over other possibilities, but it's something reasonable that we've studied a good bit and we have a lot of.",
            "Classification algorithms?",
            "OK, so how do we come up with some sort of?",
            "Algorithm.",
            "This is kind of how does imagination work, but we can make some observations.",
            "A classifier.",
            "Just normal classifiers is correct.",
            "Then what's really saying is the probability that y = 1 given the input features greater than .5.",
            "Right, so it's giving you some information about the conditional probability of Y given X whenever the classifier is correct.",
            "And then what you can observe is that you can sort of.",
            "You can fiddle with things so that.",
            "This threshold changes.",
            "Instead of it being .5, it's going to be something else.",
            "So in particular, if we map.",
            "Original example to this.",
            "So what does this mean?",
            "So this is an important weighted classification example.",
            "So.",
            "I need define importance with classification.",
            "So an important weighted classification.",
            "So in normal classification, which you care about is minimizing the error classifier.",
            "Which is just the expectation oversamples.",
            "Of indicator functions.",
            "Of C of X.",
            "Doesn't equal why, right?",
            "An importance weighted classification which you want to optimize.",
            "It's just this.",
            "So some examples are more important than other examples, and because they're more important, you want to make sure that you get those ones correct.",
            "Just I and which is also it's the third number.",
            "Yes, the major of importance.",
            "It's just this.",
            "OK, so one of the reasons why.",
            "We did this is because we already had a reduction from importance weighted classification to binary classification around right?",
            "So we knew that.",
            "But we could easily handle these importance weights and we could handle it pretty well.",
            "OK, so.",
            "When you do this, the claim is that if the classifier is perfect, then when the classifier predicts one, that's really saying that the conditional probability that y = 1 given executed in P. And this is a simple proof.",
            "If the prediction is easier than expected importance.",
            "Let's see how does it go.",
            "So sort of a balancing equation here, right?",
            "So there's.",
            "We have some probability that y = 1 given X.",
            "And.",
            "Then we have some prediction.",
            "And we're going to pay some expected importance opposite of to whatever prediction we make.",
            "So if we predict zero, then we'll be paying.",
            "When the truth turns out to be a one.",
            "If we predict one, then we'll be paying when the truth to be a 0.",
            "In turns up, these equations are symmetric under substitution of P&D and swapping, so that means that P is supposed to equal D. This is the expected records given X at this threshold value.",
            "Yes yeah yeah.",
            "So the claim is that.",
            "These are equal in P = D. And then you need to get the sign right?",
            "Alright, it's complicated, whatever, it's simple.",
            "OK, so that motive."
        ],
        [
            "In our rhythm, this is the algorithm.",
            "What you do that?",
            "And you make up a bunch of peace between zero and one.",
            "This gives you a bunch of importance weighted datasets.",
            "And then you apply your learning algorithm.",
            "And then you get a classifier.",
            "You get a bunch of classifiers and then you have some feature that you want to predict on.",
            "So then you get a bunch of predictions.",
            "You know 110000.",
            "And then you just predict.",
            "Whatever the threshold is.",
            "So this.",
            "In the typical case, what you'll get is ones and zeros, or at least.",
            "In the optimal case you get ones and zeros so.",
            "We know from this logic back here for that.",
            "The point where you switch from zero to one is the point where P = D. This is the reduction from binary.",
            "Yes, probabilistic binary.",
            "So we can just look at whatever the P was where we switched over.",
            "So it's somewhere between point 1.5, so I just output .3.",
            "OK, so.",
            "Yeah, kind of handy.",
            "There's a few details which are important to deal with.",
            "Like for example, you may not actually have one one, all ones, and then all zeros you might have.",
            "You know 110100 and the way to handle that is to just.",
            "Sort."
        ],
        [
            "Right, so you just kind of salty have all ones and then all zeros.",
            "You pretend that was what the classifiers actually told you.",
            "You need to just treat it.",
            "Then how do you?",
            "So after you sort your going to get all ones and then all zeros and then you just pick the threshold in the same way.",
            "OK, so.",
            "Let's say we get 11101010000.",
            "So instead what we do is we sort this and then we get 11111.",
            "000000.",
            "Spectre this is hospital see.",
            "No, we should sort with respect to these predictions.",
            "Which one?",
            "Smallest number of swaps?",
            "Yeah yeah yeah.",
            "So.",
            "I.",
            "Here this one goes here.",
            "In that zero goes there, then sorted right?",
            "And then what I do is just pretend that this is what the classifiers told me.",
            "For this threshold algorithm.",
            "Yeah, the count the ones in this is the place where I am.",
            "It'll turn out.",
            "There's some very nice math saying this is a good idea.",
            "What was the goal?",
            "Binary prediction classification problem right?",
            "Right in the final.",
            "Addiction solution.",
            "Prediction solution.",
            "Yeah, the problem is you're converting the way I'm converting.",
            "You can think about it either way.",
            "Where do the weights come from?",
            "Where do the weights come from, oyes?",
            "That's right.",
            "How do you choose the weights?",
            "And the answer is, you can use a uniform grid, or if you're clever in the transductive setting, you can try to just do it on demand.",
            "So.",
            "What that means is.",
            "We have an interval between zero and one.",
            "We want to try to classify some particular what went wrong with the probabilistic prediction for some particular feature.",
            "So we can first just use this, which is equivalent to normal classification.",
            "And maybe this will say that it's over here.",
            "And then what you can try to do is.",
            "This one this P OK and then try to do that P and then.",
            "That being so forth.",
            "I think that this is not stable enough because this is not.",
            "This is not robust to individual errors, but we have a lot of test examples and you discover that you need sort of more resolution in this region because most things are not very likely to be a 0.",
            "Then you can just be handy.",
            "Yeah.",
            "Yeah.",
            "OK.",
            "Right, so so that's an algorithm, and now we'd kind of like to know this works well, right?",
            "Is it a heuristic or not?",
            "That's a great question."
        ],
        [
            "Uh.",
            "So so before we do the analysis, there's sort of 1 one slight shift of viewpoint which is very helpful.",
            "This is we can think about.",
            "Uh.",
            "Actually, only learning one classifier.",
            "The trick here is that we can just embed the threshold into the feature space.",
            "And then we can make this definition hold.",
            "And we can take our datasets in.",
            "Take a union over datasets and that will give us the correct measure on.",
            "On the individual.",
            "Classifier, The one classifier we learn.",
            "So that's very helpful there.",
            "In the original paper on this, we didn't do this, and it makes the math significantly more messy.",
            "Uh.",
            "OK, so this is a slightly different algorithm than from.",
            "We're analyzing then from what we're proposing, but.",
            "It's obviously very similar and we haven't actually experimented with with this piece of math.",
            "Yeah so.",
            "Yeah.",
            "Yeah, but it's not a really very good code.",
            "It's just strange.",
            "For everything you want, you don't get much traffic.",
            "So if you can do it on demand and then it's a more efficient process.",
            "True that.",
            "Actually get up.",
            "Binary encoding supposed to streaming amount.",
            "So if you have a single test example, if you need to.",
            "Make a probabilistic prediction for.",
            "Then this can be done by in some sort of on-demand process.",
            "And active learning all that if you have many test examples then what will end up happening is you.",
            "Divisions.",
            "Yeah, yeah.",
            "Treatment of binary expansion of the subdivision of the line.",
            "It is possible.",
            "It requires more cleverness than I want to go into.",
            "It requires active learning kinds of cleverness.",
            "OK, so this is very nice theoretically.",
            "Without this slight modification works well in practice.",
            "We haven't tested yet.",
            "Uh.",
            "But what this means is that you can think about.",
            "Drawing.",
            "An example of.",
            "That you feed into.",
            "The the importance weighted learning algorithm.",
            "By drawing from the original distribution D and then drawing P uniformly from zero to 1.",
            "And then just putting them together using.",
            "This formula.",
            "OK, so.",
            "What I'm going to do is I'm going to make the process which does that.",
            "I'm going to call that process probing of D, right?",
            "So that there will be this is this is some major.",
            "On binary classification problems, which is induced by the original major binary pacification problems via this process of drawing D, drawing P and then using our little algorithm.",
            "Condition.",
            "No.",
            "It's not a class conditional, it is a joint on X cross, a new Y.",
            "Right?",
            "OK, so.",
            "So now we can actually state the theorem.",
            "So one queued up survey Shun is that we can think about this is related to your counting point."
        ],
        [
            "We can think about the output of the probing algorithm depending upon some classifier, C is just predicting.",
            "It now what is the probability that?",
            "OK, so in infinite resolution the output is the probability with respect to the random P from zero to 1.",
            "That the classifier outputs A1.",
            "So we're just counting the proportion of the time that we actually get a one, and that is exactly.",
            "That threshold.",
            "OK, So what does the theorem say the theorem says for every classifier for every probabilistic classifier?",
            "Or rather for everybody classifier which takes his P and its feature space.",
            "For every measure on X cross 01.",
            "This holds so the expected squared error.",
            "And the probabilistic prediction.",
            "Is bounded by.",
            "This is the.",
            "Error rate of the classifier on the induced distribution.",
            "Manage the minimum possible error rate.",
            "So we really want to subtract this because we're creating kind of a hard classification problem.",
            "By this process.",
            "It might be that there's just some fundamental noise.",
            "D is not actually deterministic, and so we really want to subtract this operates.",
            "But the results will look too good.",
            "Over all possible classifiers, this is the base error.",
            "He is.",
            "This.",
            "Yeah, use the smallest possible error rate.",
            "OK, so.",
            "So does this say so?",
            "This is kind of a strange theorem, because it says that.",
            "Even though we're never actually told.",
            "What the probability of Y given X is just optimizing 01 loss.",
            "Means that will actually be.",
            "Finding the probability ynx.",
            "Right, if we optimize this, well, this will go to zero and then this will go to 0.",
            "And we must be predicting the correct probabilities.",
            "This says that any.",
            "Binary classification algorithm can be thought of as a class probability estimation algorithm.",
            "It also says.",
            "Because this is, this is squared loss and there's a slight variation that you can make any classifier do regression.",
            "This holds for every C. For every D there's no assumption of independence here, and that means that all of the familiar bounds you love in terms of classification loss.",
            "Will work.",
            "Is also is sufficiently primitive that it will compose with online learning.",
            "Yeah.",
            "Check this minimum possible expected error.",
            "Yeah here.",
            "If you have any.",
            "In general, you know hope that they come to the cheap.",
            "Faraldi, that's right.",
            "But for individual D, it might happen.",
            "For some for the day that you care about, it might happen.",
            "So what does this say for those types of binary classifier?",
            "Stay it says that.",
            "Yeah.",
            "And exactly what that set is is related to.",
            "The structure of this process.",
            "If they can get kind of close to the optimal classifier with free to get there, does that somehow.",
            "You said they also get close to the truth.",
            "Yeah, 'cause if this is close to this then this is also near to.",
            "I mean this is bounded by the distance between.",
            "How close you are to.",
            "I see there's two kinds of clothes there is this close to the optimal senior set of C and is close to the optimal.",
            "See in all C. Yeah, close in the first sense doesn't help close in the second sense helps.",
            "Choosing no CC is the classifier.",
            "Coffee.",
            "And yeah, so E is just measuring the 01 loss and then I'm writing out the.",
            "The squared error explicitly here.",
            "Posted on the right hand side.",
            "They both cost for different problems that cost for.",
            "Prediction, binary prediction and this one is the constable probabilistic prediction, right?",
            "This is.",
            "How much you lose for binary prediction on the induced problem.",
            "This is how much you lose for probabilistic prediction on the original problem.",
            "Yeah.",
            "Hulu something about.",
            "Considering the recent.",
            "This is right.",
            "You get this from the usual statistical way of dismissing one, right, yeah?",
            "Yeah.",
            "Using this method, do I use this?",
            "It's a good question to ask, I think.",
            "In general the answer is you can lose, but for your specific example, I think you don't lose.",
            "Because the roughly speaking, the sample complexity required to estimate a threshold online is equivalent to the same complexity required to estimate the error rate of any classifier.",
            "That just has to do something has something to do with the structure of a line.",
            "And threshold functions on the line.",
            "OK. To me this is the style of reduction theorem.",
            "It's very primitive.",
            "We have strong quantifiers and relate the loss from the original problem to loss on our induced problem.",
            "This D doesn't have to be the that the training set is drawn from.",
            "There's no assumption that the training set and test set are drawn from the same distribution's.",
            "It could be that the drug from entirely different distributions and then you just plug in this D and what easily happens in that case.",
            "Is this just happens to be much larger than this?",
            "And then the theorem is true, but not very interesting.",
            "OK, so the proof of this is reasonably simple, so I'll just show it to you.",
            "I will tell you in advance it's only two pages so.",
            "You can pay attention or go to sleep for the appropriate period of time.",
            "OK, so the first point is that the expected importance is 1/2.",
            "And the reason why the expected importance is 1/2 is because you have this interval between zero and one.",
            "We're drawing AP uniformly from this interval.",
            "And then we're looking at important to just magnitude of Y -- P, which is either going to be this function or this function.",
            "When you integrate, you get a half.",
            "Just yeah.",
            "OK, so we start with the.",
            "Induced loss.",
            "Minus the minimum possible loss, and you can just write it out.",
            "This is just writing out what the formula is except.",
            "Now I've.",
            "So this is just the loss for whatever see you have.",
            "Then this is the Bayes optimal classifier.",
            "So I'm just writing out.",
            "What the Bayes optimal classifier is losses here?",
            "So there's some probability of 1 given X.",
            "In the last.",
            "In that case, it's going to be 1 -- P. In this probability.",
            "Some probability of zero given X and the last lesson that case is going to be P 'cause there be probability of actually having a one.",
            "So then we do some algebra.",
            "Actually, no, we don't even do algebra.",
            "What we do is we shuffle the expectation around a little bit.",
            "And then you think about what this means.",
            "And.",
            "With X&P fixed, either C is going to predict the same as the Bayes optimal or opposite of it.",
            "What that means is that this difference is either going to be 0 or it's going to be this.",
            "'cause you just subtract that from that you get this.",
            "OK. And now let's go back and think about that a minute.",
            "So this is an absolute value around.",
            "Some truth right so?",
            "Let's say this is D of 1 given X.",
            "This is an absolute value and that says that if we're at this P and we screw up, then the month Roblox we suffer is this, and for here then it's actually there is some sort of line like this.",
            "Which describes how much loss.",
            "We suffer if this is the P it was predicted correctly.",
            "So now we need to think about we have an adversary.",
            "There is going to choose a classifier and distribution chart and messes up as much as possible is going to have some budget of epsilon binary errors going to have some total loss.",
            "You can suffer.",
            "He's going to try to distort the probabilistic prediction as much as possible.",
            "So there's two observations which really help.",
            "One of them is.",
            "If the adversary has if this is correct.",
            "And this is correct, and the adversary can make either this one error that one error.",
            "Then he always prefers to make this one error because he pays less and has the same effect after the sorting.",
            "So this is where the sorting is, not a heuristic.",
            "Because of this fact that we sort things screwing up here is equivalent to screwing up here.",
            "It has the same effect, the same impact on the probability prediction that we make.",
            "And since screwing up here pays a larger loss.",
            "We want to screw up in places with smaller loss.",
            "So that's the second point.",
            "The first point is that.",
            "You never an adversary who's on a budget is never going to actually screw up over here and over here.",
            "Because when you sort these two cancel each other.",
            "So there again, this sort is helping us.",
            "So these two observations together say that.",
            "The way the adversary is going to try to distort your public prediction as much as possible given his budget, he's going to choose a wedge like this.",
            "It's just going to make all these guys air.",
            "And nobody else will care.",
            "And then a probabilistic prediction will be over here, and we'll pay this kind of squared error.",
            "So we just need to calculate how much.",
            "How much volume is in this in order to know what the budget is?",
            "OK, so we're thinking up here that we just have a budget of epsilon, so this volume is an integral must be just epsilon.",
            "So you have the integral from the truth to the truth plus some Delta.",
            "Of this absolute value, just get Delta squared.",
            "OK, so that's a proof, right?",
            "That double squared is going to be plugged into.",
            "Here.",
            "And it just becomes this.",
            "Alright, so.",
            "Yeah, how long did this work?",
            "So this is the results from the Championship this 2004 right?",
            "We actually entered.",
            "We were actually coming over the program.",
            "We did it.",
            "And at first I was disappointed.",
            "Because we didn't win.",
            "But nevertheless, we turned out that we did pretty well.",
            "And you see this when you realize there were 60 some entries, and I guess we were 6th or something here on this one and we were.",
            "6th over here.",
            "And again I get my 67 entries.",
            "And you realize this more when you go and you actually go to this Kitty.",
            "Conference.",
            "He discovered that the people who won.",
            "Basically, use the humanistic learning algorithm.",
            "Humans are great learning algorithms.",
            "Stop listening.",
            "Yeah, yeah.",
            "We can't beat humans yet.",
            "But you know, if you go and you look at the teams that actually work on both.",
            "Problems we were second.",
            "This there is the winner and there was us for a second team.",
            "In ours, we didn't actually look at the data.",
            "This was completely automatic.",
            "We mean we came up with the theory.",
            "We tested decision trees in logistic regression.",
            "We ended up using decision trees.",
            "And that was it.",
            "So the approach we used is entirely automatic.",
            "And press requires less time for the next problem then then this.",
            "To do so, so you use, for example integration.",
            "So you start with the program we just treated as 01 classifier.",
            "That was substantially better.",
            "Yeah.",
            "Yeah, turns out it works much better.",
            "OK, but then you re converted back again.",
            "So yeah yeah, yeah.",
            "I think we also tried to support vector machines and they were about the same as logistic regression.",
            "So.",
            "So I think this is sort of the state of the art in.",
            "In trying to make that box that I talked about at the beginning.",
            "We can't meet humans yet, but if you take a look, we're actually not that far off.",
            "And I mean, we're only paying like a percent or two on each of these different metrics.",
            "I guess that's the worst one.",
            "Ooh, cross entropy is really fun.",
            "You gotta look at these guys down here.",
            "Right, right?",
            "E 73 E 73.",
            "About 1/3 of the entries had some sort of infinite cross entropy because they predicted zero when it was a one or something like that.",
            "So it did something.",
            "Sandwiches did something saying for each of these possibilities, and it wasn't even really optimized.",
            "I mean, if I was really going to try to do the reductionist approach which didn't have time for entirely, then it would have had a different layer data transforming prediction transform for each of these different metrics, and you know we were just starting in 2004 on this kind of stuff, so we didn't have time to try to do that.",
            "OK. Is proving to be most.",
            "Squared error yeah.",
            "I mean there's the theorem says.",
            "In particular, it doesn't say anything about cross entropy, and it can't.",
            "Because cross entropy has infinite loss anytime you have infinite loss, it's not reasonable.",
            "Yeah.",
            "OK.",
            "So again, so the Z 73's are reasonable because so.",
            "Yes, I'm surprising that they occur.",
            "That was surprised that it occurred for 1/3 of the injuries though.",
            "That says something about the state of a random machine learning researcher I guess.",
            "Or something like that.",
            "Alright, so."
        ],
        [
            "I guess I should finish soon.",
            "Right, so learning reductions are a lot of different things.",
            "An they can be pretty useful from the theory point of view.",
            "I think I'll skip all and just focus on this one.",
            "So if you go out and you talk to the various applied learning people.",
            "He tried to understand what they're doing in a lot of cases, I think that they're actually doing is this, but they're doing it intuitively innocent, intuitive level.",
            "But if you go and look at the algorithms that they're actually using, it turns out a lot of them have reasonable reduction transforms.",
            "As if they were thinking about this math.",
            "So this is some formalization of the intuitions which.",
            "Which a number of applied learning people have.",
            "So maybe give some examples.",
            "But first what I wanted to do is."
        ],
        [
            "Give you some ammunition.",
            "So there are limitations to this, so one thing which is important to understand is that this is not a complete story.",
            "It is not the case that if you have a good reduction transform, you have a good learning algorithm because of course we're not even talking about learning algorithm is an second of all because.",
            "There are examples of reductions which are nonsense.",
            "So for example, you could have I could have done the program reduction, but instead I could have just encrypted the features.",
            "And no, learning out of them.",
            "No sane learning algorithm would actually succeed there.",
            "Uh.",
            "Nothing which is important is the computational requirements.",
            "So you were talking about this here.",
            "Maybe we pay a factor of 10, or maybe a factor of 100 more.",
            "It's not exponential, but it is important to think about effective trainer effect of 100.",
            "Um?",
            "And maybe there's some ways to speed this up if we can actually get this one classifier trick working in practice.",
            "And the last thing is that there's no guarantees that created problems are easy.",
            "So this is sort of.",
            "The kind of reasoning that you're doing is something like.",
            "If I could solve all subproblems equally easily, then which subproblems would I solve?",
            "And then you're hoping the ones you create or not too hard.",
            "OK, so."
        ],
        [
            "Let me show you some pretty pictures.",
            "There's actually two kinds of reductions that we've been working on.",
            "One of them is air limiting reductions, which I think are sort of the roughest form of reduction.",
            "So there what you do is you just say how does the error rate in the original problem.",
            "How is that bounded by the area on the creative side problems?",
            "And several of these.",
            "Algorithms are not by us.",
            "So error correcting output codes are by Tom Dietrich.",
            "And it'll boost is by rupture.",
            "Each of these can be thought of as.",
            "As an error transform reduction, there's some new algorithms here.",
            "So this is for Constance declassification.",
            "So This is why I put the reason any reasonable loss into the title.",
            "So in constant classification, what happens is.",
            "Your distribution your so your distribution is just over X.",
            "Cross.",
            "A set of numbers.",
            "To the.",
            "I don't know.",
            "They're predicting T classes, so to the T. So then all we try to do is you try to minimize.",
            "The expected.",
            "Cost, so this is the cost and constant to classification.",
            "So we draw an example of this form.",
            "A predictor index is some element in this vector.",
            "We try to minimize whatever element it indexes.",
            "That's sufficiently general to describe any loss function.",
            "And the theorem works well.",
            "Whenever the losses are reasonably bounded.",
            "Did you try like a direct comparison on the first?",
            "Take a classifier?",
            "Try your method in the like plot.",
            "Just kind of yeah, yeah.",
            "Little big yeah so so concentrate the comparison only reduction yeah yeah.",
            "Sure.",
            "So we did this and it is in the program paper and it turns out that the probing method works a little bit better than the plot method was predicting machine.",
            "Yes.",
            "Yes, and it works a little bit better than a logistic.",
            "There are reasons why you expect it to work a little bit better, because there are probabilistic services which are describable by this reduction that are just not describable by.",
            "By the process of fitting logistic to some.",
            "Linear threshold.",
            "OK, so there's there's some example."
        ],
        [
            "As of this happening in practice."
        ],
        [
            "And then this is the other kind of reduction.",
            "This is the kind that I showed you still regret transform reduction, so you're looking at the regret.",
            "Which is the difference between how well you did and how well you could have done.",
            "On the original problem and on the creative problem, you want this one to bound that one.",
            "So again, we have a set of different reductions.",
            "This one was just published, it Colt, and it handles any custom classification problem.",
            "So this is a modification of the operating output codes which Tom Dietrich worked on.",
            "The one you told us about this classification.",
            "That right here."
        ],
        [
            "Alright, so again, there's various ways that this is useful."
        ],
        [
            "OK, so this is the last there's several.",
            "Big open questions I guess.",
            "So one of them is about the relationship between reinforcement learning and.",
            "And just classification.",
            "So there's a lot of people in nice email that are fond of.",
            "Saying that reinforcement learning is different classification.",
            "And this is reductions actually give you a way to think about how similar to learning problems are.",
            "'cause if there's a reduction this way in this reduction that way, and they're both very tight, then they are almost the same problem in some sense.",
            "But if there isn't a reduction that's tight, then they're not so.",
            "This is sort of.",
            "This is sort of an objective criterion for.",
            "For building up a structure of overlearning problems over and learning problems.",
            "Maybe we can understand what's harder and what's easier.",
            "Anyway, that's this one we don't really understand the limits of how well we can do with these reductions yet.",
            "Money Criminous is playing with this a little bit.",
            "And then there's other.",
            "Observation that you know reductions.",
            "It's possible to design if you're trying, you can design A reduction, which will be nonsense in practice, but it gives you great mathematics.",
            "So what we'd like is some other.",
            "Form of reductions analysis.",
            "Something other than air transforms or regret.",
            "Transforms which is more robust too.",
            "This kind of thing we want that we want to be the case that whenever you do good mathematics, you do good learning algorithm.",
            "Right now it's just when you do good mathematics and you're not trying to be evil.",
            "It seems like you do a good learning algorithm.",
            "OK, so thank you.",
            "Yeah.",
            "Beginning, he said.",
            "Holy Grail is this universal.",
            "She.",
            "And then we said that, baby, that here in mind was the standard measure that we should try to achieve."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Speaker of the base.",
                    "label": 0
                },
                {
                    "sent": "Speak about how classifier can be used to solve.",
                    "label": 0
                },
                {
                    "sent": "Any nothing at all.",
                    "label": 0
                },
                {
                    "sent": "Alright thanks.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is joint work with a bunch of other people.",
                    "label": 0
                },
                {
                    "sent": "And the number is growing.",
                    "label": 0
                },
                {
                    "sent": "If you have questions where I'm talking, you should feel.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Need to ask him then OK so start out with sort of what is a dream of machine learning.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a communal dream of a large number of people and the communal hope of larger people is it's possible to create some sort of pretty universal prediction device right?",
                    "label": 1
                },
                {
                    "sent": "So you feed in a bunch of data and then it starts making predictions for you and you're getting the answer.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A basic question is, can you do this?",
                    "label": 0
                },
                {
                    "sent": "And you can say both know and yes, depending on exactly how strong you expect this to be.",
                    "label": 0
                },
                {
                    "sent": "So the obvious answer is no, you can't do this because.",
                    "label": 0
                },
                {
                    "sent": "Well, if you're familiar with encryption, people are trying very hard and quite successfully to make things not predictable.",
                    "label": 0
                },
                {
                    "sent": "Another way of saying this is that.",
                    "label": 0
                },
                {
                    "sent": "There are certain learning problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so for any learning problem for any prediction problem, if you know the answer in advance then what you want this box to be is just the thing which uses.",
                    "label": 0
                },
                {
                    "sent": "The correct prediction method.",
                    "label": 0
                },
                {
                    "sent": "But you don't know that and you can't know that if you're going to have some sort of universal device, so you're always going to lose in comparison to someone who actually knows the correct answer.",
                    "label": 0
                },
                {
                    "sent": "So incription this incription, this would be the person who knows the secret key.",
                    "label": 0
                },
                {
                    "sent": "It is however, possible as far as we know.",
                    "label": 0
                },
                {
                    "sent": "To do as well as humans can do it right so humans can solve a diverse number of prediction problems, sometimes more or less successfully.",
                    "label": 0
                },
                {
                    "sent": "And it may be feasible to create a machine which can do something like that.",
                    "label": 0
                },
                {
                    "sent": "And people think this is possible.",
                    "label": 0
                },
                {
                    "sent": "If you think it's not possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think it's possible to build a box which is as good as a human.",
                    "label": 0
                },
                {
                    "sent": "For solving things across all domains that human could solve.",
                    "label": 0
                },
                {
                    "sent": "Not in our lifetime.",
                    "label": 0
                },
                {
                    "sent": "If it's a safe answer.",
                    "label": 0
                },
                {
                    "sent": "I'm not certain, actually.",
                    "label": 0
                },
                {
                    "sent": "I think it might be possible, maybe even in our lifetimes.",
                    "label": 0
                },
                {
                    "sent": "So there's a question of just how strong you want this this universality to be.",
                    "label": 0
                },
                {
                    "sent": "And maybe there are are weak enough notions that it's possible, and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And there's a question about how you might go about doing that.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's two variations that you find.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of them is.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "For every new different type of prediction problem, you create a new algorithm.",
                    "label": 0
                },
                {
                    "sent": "So there's classification and there's regression.",
                    "label": 0
                },
                {
                    "sent": "There's reinforcement learning, and there's ranking, and this causes classification and the structured prediction.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's one way, and then another way is you have some engine which is in between some sort of data transformation layer and some other prediction transformation layer.",
                    "label": 0
                },
                {
                    "sent": "Right, so maybe if you're solving a classification problem, you have some method for turning the classification into whatever you're going to feed into your core engine, and then you have some method of getting out the answer in the core engine.",
                    "label": 0
                },
                {
                    "sent": "You can do this for.",
                    "label": 0
                },
                {
                    "sent": "For many different learning problems.",
                    "label": 0
                },
                {
                    "sent": "So this is what this talk is about.",
                    "label": 0
                },
                {
                    "sent": "We're going to be talking about methods for taking.",
                    "label": 0
                },
                {
                    "sent": "Pretty arbitrary problems transforming actually this engine here will just be a classifier.",
                    "label": 0
                },
                {
                    "sent": "And we're going to take these problems are going to transform them into classification problems.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to learn to predict correctly the individual classification problems, and then we're going to take the predictions.",
                    "label": 0
                },
                {
                    "sent": "And we're going to transform them to solve the original problem.",
                    "label": 0
                },
                {
                    "sent": "So there's.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very basic questions that you should be asking.",
                    "label": 0
                },
                {
                    "sent": "One of them is, can this work mathematically?",
                    "label": 0
                },
                {
                    "sent": "In the answer is yes, there is a way to think about this mathematically.",
                    "label": 0
                },
                {
                    "sent": "To prove that you have a good transformation an.",
                    "label": 0
                },
                {
                    "sent": "And then there's a question about does this work empirically, right?",
                    "label": 0
                },
                {
                    "sent": "So it may be that although you can create this good transformation mathematically, it doesn't actually work in practice, and the answer is actually, it does work fairly well fairly often in practice.",
                    "label": 0
                },
                {
                    "sent": "And then another question is just how do you reason about this process of transforming the data and transforming the prediction?",
                    "label": 0
                },
                {
                    "sent": "That pair of processes.",
                    "label": 0
                },
                {
                    "sent": "And then there's sort of a new theorem style which goes along with this.",
                    "label": 0
                },
                {
                    "sent": "So this is this is.",
                    "label": 0
                },
                {
                    "sent": "Unlike the kinds of theorems that you're used to, where you assume independence of the data, it's unlike online learning where you don't assume independence data.",
                    "label": 0
                },
                {
                    "sent": "But the question that you're answering is different.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a different kind of theorem.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I don't mean to quote Wolfram.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, actually I had a conversation with him.",
                    "label": 0
                },
                {
                    "sent": "When something like this I said.",
                    "label": 0
                },
                {
                    "sent": "You know, I think that science is useful because it can predict things.",
                    "label": 0
                },
                {
                    "sent": "And he said something like.",
                    "label": 0
                },
                {
                    "sent": "Science is about much more than predicting things.",
                    "label": 0
                },
                {
                    "sent": "It's also about explaining.",
                    "label": 0
                },
                {
                    "sent": "He said he, well, you get to into predicting things.",
                    "label": 0
                },
                {
                    "sent": "Talk to me.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "There's some baggage which goes along with this, which is sort of a new style to trying to solve learning problems and what I want to do is I want to sort of show you this style and along the way I'll show you a theorem so there's several different reductions that we've worked out a sufficient number so that I'm comfortable.",
                    "label": 0
                },
                {
                    "sent": "Saying reasonable.",
                    "label": 0
                },
                {
                    "sent": "So what I did was I picked one which is neither particularly difficult or particularly easy, just sort of representative.",
                    "label": 0
                },
                {
                    "sent": "An will just kind of go through how you try to solve the problem using this approach.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Right, so the first step is going to be.",
                    "label": 0
                },
                {
                    "sent": "You want to try to define the real problem you know, try to define it very crisply.",
                    "label": 0
                },
                {
                    "sent": "The second step is going to be creating this transform pair.",
                    "label": 0
                },
                {
                    "sent": "The third step is going to be analyzing this created transform pair just mathematically.",
                    "label": 0
                },
                {
                    "sent": "And then if we're happy with the analysis, then we just use it in.",
                    "label": 0
                },
                {
                    "sent": "Otherwise we go to here and we hope that the math actually helps us do this better.",
                    "label": 0
                },
                {
                    "sent": "Turns out often it does.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the learning problem?",
                    "label": 0
                },
                {
                    "sent": "So I'm going to think of learning problem as a set of things and.",
                    "label": 0
                },
                {
                    "sent": "One of these OK, so let's see we're familiar with features, right?",
                    "label": 0
                },
                {
                    "sent": "So the X is just features.",
                    "label": 0
                },
                {
                    "sent": "And with.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here with the prediction space, it's very common in all formulations, so this is like 0 one or some real value differing regression or whatever.",
                    "label": 0
                },
                {
                    "sent": "OK, so the prediction space and the advice space are separated here.",
                    "label": 0
                },
                {
                    "sent": "And it's actually important for a lot of learning problems.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "So at training time, what's going to happen is you're going to get.",
                    "label": 0
                },
                {
                    "sent": "X cross K samples.",
                    "label": 0
                },
                {
                    "sent": "Any going to use these samples to make a prediction prediction device from X to Y?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "And then your loss is just going to be advice, cross whatever prediction you make map to some positive number.",
                    "label": 0
                },
                {
                    "sent": "Sometimes called an action space in testicles.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right.",
                    "label": 0
                },
                {
                    "sent": "The prediction.",
                    "label": 0
                },
                {
                    "sent": "The prediction is the action.",
                    "label": 0
                },
                {
                    "sent": "Because pay was the case with the.",
                    "label": 0
                },
                {
                    "sent": "Just say what you called it.",
                    "label": 0
                },
                {
                    "sent": "Were you saying that wise decision for K?",
                    "label": 0
                },
                {
                    "sent": "Which is the learner generating why it's generating?",
                    "label": 0
                },
                {
                    "sent": "A predictor going from X to Y?",
                    "label": 0
                },
                {
                    "sent": "OK yeah, so pay pay which would normally be the the training data values of the true.",
                    "label": 0
                },
                {
                    "sent": "It is for some problems it is essential to have the extra generality that it can be something other than just the prediction space.",
                    "label": 0
                },
                {
                    "sent": "Get back.",
                    "label": 0
                },
                {
                    "sent": "Intermediate, but you get some parameters like a state space and then offensive prediction.",
                    "label": 0
                },
                {
                    "sent": "Case intermediate between.",
                    "label": 0
                },
                {
                    "sent": "So it will turn out that.",
                    "label": 0
                },
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "It's not exactly a parameter value, because it doesn't necessarily parameterise the process generating the features are like that.",
                    "label": 0
                },
                {
                    "sent": "So actually Haeussler uses generalization.",
                    "label": 0
                },
                {
                    "sent": "David Haussler is one of the early learning theory people.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                },
                {
                    "sent": "K is just advice, so we're going to have some algorithm which is going to take it as input X cross.",
                    "label": 0
                },
                {
                    "sent": "K star is going to output.",
                    "label": 0
                },
                {
                    "sent": "Exxaro why?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "An no no.",
                    "label": 0
                },
                {
                    "sent": "K could be why so used to K being why?",
                    "label": 0
                },
                {
                    "sent": "Right, so it's just information.",
                    "label": 0
                },
                {
                    "sent": "It's information.",
                    "label": 0
                },
                {
                    "sent": "It's advice about what.",
                    "label": 0
                },
                {
                    "sent": "Why should be which is not available at testing time.",
                    "label": 0
                },
                {
                    "sent": "So K could be the labels could be something else too, but could be labels.",
                    "label": 0
                },
                {
                    "sent": "I mean in other fields it might be cool to states, but some representation of build code.",
                    "label": 0
                },
                {
                    "sent": "State yeah OK so.",
                    "label": 0
                },
                {
                    "sent": "Right, let's go.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So there's this KDD conference and.",
                    "label": 0
                },
                {
                    "sent": "They have little championships, right so?",
                    "label": 0
                },
                {
                    "sent": "The championship is judged in some way according to some criterion, right?",
                    "label": 0
                },
                {
                    "sent": "So there are actually two different problems associated with championship and one of the things you want to do is try to maximize the zero inaccuracy.",
                    "label": 0
                },
                {
                    "sent": "That's the one we're very familiar with.",
                    "label": 0
                },
                {
                    "sent": "Another one was maximizing the area under the Arosi curve, so this is some sort of complicated thing.",
                    "label": 0
                },
                {
                    "sent": "Little bit tricky to describe.",
                    "label": 0
                },
                {
                    "sent": "The other one was minimizing the cross entropy, so there's some for the formula for area.",
                    "label": 0
                },
                {
                    "sent": "There are seeker there.",
                    "label": 0
                },
                {
                    "sent": "It's kind of if you order everything from most 022 most one.",
                    "label": 0
                },
                {
                    "sent": "How many times do you have to?",
                    "label": 0
                },
                {
                    "sent": "Do a binary swap in order to.",
                    "label": 0
                },
                {
                    "sent": "Reorder to get the all zeros and then all ones.",
                    "label": 0
                },
                {
                    "sent": "There's cross entropy, which is.",
                    "label": 0
                },
                {
                    "sent": "Write the average probability of the truth according to some.",
                    "label": 0
                },
                {
                    "sent": "Method of predicting probabilities.",
                    "label": 0
                },
                {
                    "sent": "There's this Q score which is totally crazy.",
                    "label": 0
                },
                {
                    "sent": "There is OK, so this is not quite a standard classification problem, this is.",
                    "label": 0
                },
                {
                    "sent": "A problem where you.",
                    "label": 0
                },
                {
                    "sent": "Are trying to predict if a bunch of different approaching sequences are Houma logs.",
                    "label": 0
                },
                {
                    "sent": "I think of some other protein sequence based upon some features.",
                    "label": 0
                },
                {
                    "sent": "So top one says if the one that I ranked most likely to be the same is actually the truth, then I get one.",
                    "label": 0
                },
                {
                    "sent": "Otherwise they get 0.",
                    "label": 0
                },
                {
                    "sent": "There's a root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "This is another one we're familiar with.",
                    "label": 0
                },
                {
                    "sent": "This is just typical regression criterion.",
                    "label": 0
                },
                {
                    "sent": "This is mean average precision.",
                    "label": 0
                },
                {
                    "sent": "Note that so the point is that the world kind of imposes a bunch of different loss functions on us.",
                    "label": 0
                },
                {
                    "sent": "And it's actually sometimes controversial too.",
                    "label": 0
                },
                {
                    "sent": "To say this because a lot of people regard a loss function is a free parameter of a learning system.",
                    "label": 0
                },
                {
                    "sent": "But there's another notion of loss, which is the thing which is actually imposed by the real world, and in one sense or another.",
                    "label": 0
                },
                {
                    "sent": "The championship here is some sort of real world boss.",
                    "label": 0
                },
                {
                    "sent": "So you want to optimize that and you don't have any choice, you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't just ignore it.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a lot of different losses here and what I would like to say is, oh, it's easy to just make a reduction for each individual loss and then optimize that, but that's a little bit complicated and when we're playing this with this, we didn't actually have time.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First step is defining the problem and I can tell you a sufficient problem for which I can make predictions.",
                    "label": 0
                },
                {
                    "sent": "That kind of hope to handle all of those losses, and that is just doing class probability estimation.",
                    "label": 0
                },
                {
                    "sent": "So what happens here is?",
                    "label": 0
                },
                {
                    "sent": "You have.",
                    "label": 0
                },
                {
                    "sent": "A major D on X cross zero or one.",
                    "label": 0
                },
                {
                    "sent": "So this is just standard classification.",
                    "label": 0
                },
                {
                    "sent": "But now your predictor is going to predict into the space 021, so it's going to make some sort of probabilistic prediction.",
                    "label": 0
                },
                {
                    "sent": "And we're going to have his input just a standard classification data set.",
                    "label": 0
                },
                {
                    "sent": "I'm going to find some probabilistic classifier which optimizes the squared error.",
                    "label": 0
                },
                {
                    "sent": "OK, so the claim is that if you have this CPU is something saying you can do with your predictions too.",
                    "label": 0
                },
                {
                    "sent": "Try to solve our area to the RC curve and top one and I don't know what else.",
                    "label": 0
                },
                {
                    "sent": "All the rest of yours.",
                    "label": 0
                },
                {
                    "sent": "Even the slack queue thing, which is really weird.",
                    "label": 0
                },
                {
                    "sent": "So this is the first step in trying to solve things in sort of a reduction style.",
                    "label": 0
                },
                {
                    "sent": "Try to isolate some particular problem.",
                    "label": 0
                },
                {
                    "sent": "And then the next step is trying to come up with.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some sort of algorithm for solving this class probability estimation given our core engine so.",
                    "label": 0
                },
                {
                    "sent": "Me coming from kind of a learning theory background.",
                    "label": 0
                },
                {
                    "sent": "My core engine is always the classifier.",
                    "label": 0
                },
                {
                    "sent": "There's nothing particularly compelling about a classifier over other possibilities, but it's something reasonable that we've studied a good bit and we have a lot of.",
                    "label": 0
                },
                {
                    "sent": "Classification algorithms?",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we come up with some sort of?",
                    "label": 0
                },
                {
                    "sent": "Algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is kind of how does imagination work, but we can make some observations.",
                    "label": 0
                },
                {
                    "sent": "A classifier.",
                    "label": 0
                },
                {
                    "sent": "Just normal classifiers is correct.",
                    "label": 0
                },
                {
                    "sent": "Then what's really saying is the probability that y = 1 given the input features greater than .5.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's giving you some information about the conditional probability of Y given X whenever the classifier is correct.",
                    "label": 0
                },
                {
                    "sent": "And then what you can observe is that you can sort of.",
                    "label": 0
                },
                {
                    "sent": "You can fiddle with things so that.",
                    "label": 0
                },
                {
                    "sent": "This threshold changes.",
                    "label": 0
                },
                {
                    "sent": "Instead of it being .5, it's going to be something else.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if we map.",
                    "label": 0
                },
                {
                    "sent": "Original example to this.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "So this is an important weighted classification example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I need define importance with classification.",
                    "label": 0
                },
                {
                    "sent": "So an important weighted classification.",
                    "label": 0
                },
                {
                    "sent": "So in normal classification, which you care about is minimizing the error classifier.",
                    "label": 0
                },
                {
                    "sent": "Which is just the expectation oversamples.",
                    "label": 0
                },
                {
                    "sent": "Of indicator functions.",
                    "label": 0
                },
                {
                    "sent": "Of C of X.",
                    "label": 0
                },
                {
                    "sent": "Doesn't equal why, right?",
                    "label": 0
                },
                {
                    "sent": "An importance weighted classification which you want to optimize.",
                    "label": 0
                },
                {
                    "sent": "It's just this.",
                    "label": 0
                },
                {
                    "sent": "So some examples are more important than other examples, and because they're more important, you want to make sure that you get those ones correct.",
                    "label": 0
                },
                {
                    "sent": "Just I and which is also it's the third number.",
                    "label": 0
                },
                {
                    "sent": "Yes, the major of importance.",
                    "label": 0
                },
                {
                    "sent": "It's just this.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the reasons why.",
                    "label": 0
                },
                {
                    "sent": "We did this is because we already had a reduction from importance weighted classification to binary classification around right?",
                    "label": 0
                },
                {
                    "sent": "So we knew that.",
                    "label": 0
                },
                {
                    "sent": "But we could easily handle these importance weights and we could handle it pretty well.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "When you do this, the claim is that if the classifier is perfect, then when the classifier predicts one, that's really saying that the conditional probability that y = 1 given executed in P. And this is a simple proof.",
                    "label": 0
                },
                {
                    "sent": "If the prediction is easier than expected importance.",
                    "label": 0
                },
                {
                    "sent": "Let's see how does it go.",
                    "label": 0
                },
                {
                    "sent": "So sort of a balancing equation here, right?",
                    "label": 0
                },
                {
                    "sent": "So there's.",
                    "label": 0
                },
                {
                    "sent": "We have some probability that y = 1 given X.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Then we have some prediction.",
                    "label": 0
                },
                {
                    "sent": "And we're going to pay some expected importance opposite of to whatever prediction we make.",
                    "label": 0
                },
                {
                    "sent": "So if we predict zero, then we'll be paying.",
                    "label": 0
                },
                {
                    "sent": "When the truth turns out to be a one.",
                    "label": 0
                },
                {
                    "sent": "If we predict one, then we'll be paying when the truth to be a 0.",
                    "label": 0
                },
                {
                    "sent": "In turns up, these equations are symmetric under substitution of P&D and swapping, so that means that P is supposed to equal D. This is the expected records given X at this threshold value.",
                    "label": 0
                },
                {
                    "sent": "Yes yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So the claim is that.",
                    "label": 0
                },
                {
                    "sent": "These are equal in P = D. And then you need to get the sign right?",
                    "label": 0
                },
                {
                    "sent": "Alright, it's complicated, whatever, it's simple.",
                    "label": 0
                },
                {
                    "sent": "OK, so that motive.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our rhythm, this is the algorithm.",
                    "label": 0
                },
                {
                    "sent": "What you do that?",
                    "label": 0
                },
                {
                    "sent": "And you make up a bunch of peace between zero and one.",
                    "label": 0
                },
                {
                    "sent": "This gives you a bunch of importance weighted datasets.",
                    "label": 0
                },
                {
                    "sent": "And then you apply your learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then you get a classifier.",
                    "label": 0
                },
                {
                    "sent": "You get a bunch of classifiers and then you have some feature that you want to predict on.",
                    "label": 0
                },
                {
                    "sent": "So then you get a bunch of predictions.",
                    "label": 0
                },
                {
                    "sent": "You know 110000.",
                    "label": 0
                },
                {
                    "sent": "And then you just predict.",
                    "label": 0
                },
                {
                    "sent": "Whatever the threshold is.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                },
                {
                    "sent": "In the typical case, what you'll get is ones and zeros, or at least.",
                    "label": 0
                },
                {
                    "sent": "In the optimal case you get ones and zeros so.",
                    "label": 0
                },
                {
                    "sent": "We know from this logic back here for that.",
                    "label": 0
                },
                {
                    "sent": "The point where you switch from zero to one is the point where P = D. This is the reduction from binary.",
                    "label": 0
                },
                {
                    "sent": "Yes, probabilistic binary.",
                    "label": 0
                },
                {
                    "sent": "So we can just look at whatever the P was where we switched over.",
                    "label": 0
                },
                {
                    "sent": "So it's somewhere between point 1.5, so I just output .3.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, kind of handy.",
                    "label": 0
                },
                {
                    "sent": "There's a few details which are important to deal with.",
                    "label": 0
                },
                {
                    "sent": "Like for example, you may not actually have one one, all ones, and then all zeros you might have.",
                    "label": 0
                },
                {
                    "sent": "You know 110100 and the way to handle that is to just.",
                    "label": 0
                },
                {
                    "sent": "Sort.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so you just kind of salty have all ones and then all zeros.",
                    "label": 0
                },
                {
                    "sent": "You pretend that was what the classifiers actually told you.",
                    "label": 0
                },
                {
                    "sent": "You need to just treat it.",
                    "label": 0
                },
                {
                    "sent": "Then how do you?",
                    "label": 0
                },
                {
                    "sent": "So after you sort your going to get all ones and then all zeros and then you just pick the threshold in the same way.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Let's say we get 11101010000.",
                    "label": 0
                },
                {
                    "sent": "So instead what we do is we sort this and then we get 11111.",
                    "label": 0
                },
                {
                    "sent": "000000.",
                    "label": 0
                },
                {
                    "sent": "Spectre this is hospital see.",
                    "label": 0
                },
                {
                    "sent": "No, we should sort with respect to these predictions.",
                    "label": 0
                },
                {
                    "sent": "Which one?",
                    "label": 0
                },
                {
                    "sent": "Smallest number of swaps?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Here this one goes here.",
                    "label": 0
                },
                {
                    "sent": "In that zero goes there, then sorted right?",
                    "label": 0
                },
                {
                    "sent": "And then what I do is just pretend that this is what the classifiers told me.",
                    "label": 0
                },
                {
                    "sent": "For this threshold algorithm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the count the ones in this is the place where I am.",
                    "label": 0
                },
                {
                    "sent": "It'll turn out.",
                    "label": 0
                },
                {
                    "sent": "There's some very nice math saying this is a good idea.",
                    "label": 0
                },
                {
                    "sent": "What was the goal?",
                    "label": 0
                },
                {
                    "sent": "Binary prediction classification problem right?",
                    "label": 0
                },
                {
                    "sent": "Right in the final.",
                    "label": 0
                },
                {
                    "sent": "Addiction solution.",
                    "label": 0
                },
                {
                    "sent": "Prediction solution.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the problem is you're converting the way I'm converting.",
                    "label": 0
                },
                {
                    "sent": "You can think about it either way.",
                    "label": 0
                },
                {
                    "sent": "Where do the weights come from?",
                    "label": 0
                },
                {
                    "sent": "Where do the weights come from, oyes?",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "How do you choose the weights?",
                    "label": 0
                },
                {
                    "sent": "And the answer is, you can use a uniform grid, or if you're clever in the transductive setting, you can try to just do it on demand.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What that means is.",
                    "label": 0
                },
                {
                    "sent": "We have an interval between zero and one.",
                    "label": 0
                },
                {
                    "sent": "We want to try to classify some particular what went wrong with the probabilistic prediction for some particular feature.",
                    "label": 0
                },
                {
                    "sent": "So we can first just use this, which is equivalent to normal classification.",
                    "label": 0
                },
                {
                    "sent": "And maybe this will say that it's over here.",
                    "label": 0
                },
                {
                    "sent": "And then what you can try to do is.",
                    "label": 0
                },
                {
                    "sent": "This one this P OK and then try to do that P and then.",
                    "label": 0
                },
                {
                    "sent": "That being so forth.",
                    "label": 0
                },
                {
                    "sent": "I think that this is not stable enough because this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not robust to individual errors, but we have a lot of test examples and you discover that you need sort of more resolution in this region because most things are not very likely to be a 0.",
                    "label": 0
                },
                {
                    "sent": "Then you can just be handy.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so so that's an algorithm, and now we'd kind of like to know this works well, right?",
                    "label": 0
                },
                {
                    "sent": "Is it a heuristic or not?",
                    "label": 0
                },
                {
                    "sent": "That's a great question.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "So so before we do the analysis, there's sort of 1 one slight shift of viewpoint which is very helpful.",
                    "label": 0
                },
                {
                    "sent": "This is we can think about.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Actually, only learning one classifier.",
                    "label": 0
                },
                {
                    "sent": "The trick here is that we can just embed the threshold into the feature space.",
                    "label": 0
                },
                {
                    "sent": "And then we can make this definition hold.",
                    "label": 0
                },
                {
                    "sent": "And we can take our datasets in.",
                    "label": 0
                },
                {
                    "sent": "Take a union over datasets and that will give us the correct measure on.",
                    "label": 0
                },
                {
                    "sent": "On the individual.",
                    "label": 0
                },
                {
                    "sent": "Classifier, The one classifier we learn.",
                    "label": 0
                },
                {
                    "sent": "So that's very helpful there.",
                    "label": 0
                },
                {
                    "sent": "In the original paper on this, we didn't do this, and it makes the math significantly more messy.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a slightly different algorithm than from.",
                    "label": 0
                },
                {
                    "sent": "We're analyzing then from what we're proposing, but.",
                    "label": 0
                },
                {
                    "sent": "It's obviously very similar and we haven't actually experimented with with this piece of math.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but it's not a really very good code.",
                    "label": 0
                },
                {
                    "sent": "It's just strange.",
                    "label": 0
                },
                {
                    "sent": "For everything you want, you don't get much traffic.",
                    "label": 0
                },
                {
                    "sent": "So if you can do it on demand and then it's a more efficient process.",
                    "label": 0
                },
                {
                    "sent": "True that.",
                    "label": 0
                },
                {
                    "sent": "Actually get up.",
                    "label": 0
                },
                {
                    "sent": "Binary encoding supposed to streaming amount.",
                    "label": 0
                },
                {
                    "sent": "So if you have a single test example, if you need to.",
                    "label": 0
                },
                {
                    "sent": "Make a probabilistic prediction for.",
                    "label": 0
                },
                {
                    "sent": "Then this can be done by in some sort of on-demand process.",
                    "label": 0
                },
                {
                    "sent": "And active learning all that if you have many test examples then what will end up happening is you.",
                    "label": 0
                },
                {
                    "sent": "Divisions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Treatment of binary expansion of the subdivision of the line.",
                    "label": 0
                },
                {
                    "sent": "It is possible.",
                    "label": 0
                },
                {
                    "sent": "It requires more cleverness than I want to go into.",
                    "label": 0
                },
                {
                    "sent": "It requires active learning kinds of cleverness.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very nice theoretically.",
                    "label": 0
                },
                {
                    "sent": "Without this slight modification works well in practice.",
                    "label": 0
                },
                {
                    "sent": "We haven't tested yet.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "But what this means is that you can think about.",
                    "label": 0
                },
                {
                    "sent": "Drawing.",
                    "label": 0
                },
                {
                    "sent": "An example of.",
                    "label": 0
                },
                {
                    "sent": "That you feed into.",
                    "label": 0
                },
                {
                    "sent": "The the importance weighted learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "By drawing from the original distribution D and then drawing P uniformly from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "And then just putting them together using.",
                    "label": 0
                },
                {
                    "sent": "This formula.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is I'm going to make the process which does that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to call that process probing of D, right?",
                    "label": 0
                },
                {
                    "sent": "So that there will be this is this is some major.",
                    "label": 0
                },
                {
                    "sent": "On binary classification problems, which is induced by the original major binary pacification problems via this process of drawing D, drawing P and then using our little algorithm.",
                    "label": 0
                },
                {
                    "sent": "Condition.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "It's not a class conditional, it is a joint on X cross, a new Y.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So now we can actually state the theorem.",
                    "label": 0
                },
                {
                    "sent": "So one queued up survey Shun is that we can think about this is related to your counting point.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can think about the output of the probing algorithm depending upon some classifier, C is just predicting.",
                    "label": 0
                },
                {
                    "sent": "It now what is the probability that?",
                    "label": 0
                },
                {
                    "sent": "OK, so in infinite resolution the output is the probability with respect to the random P from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "That the classifier outputs A1.",
                    "label": 0
                },
                {
                    "sent": "So we're just counting the proportion of the time that we actually get a one, and that is exactly.",
                    "label": 0
                },
                {
                    "sent": "That threshold.",
                    "label": 0
                },
                {
                    "sent": "OK, So what does the theorem say the theorem says for every classifier for every probabilistic classifier?",
                    "label": 0
                },
                {
                    "sent": "Or rather for everybody classifier which takes his P and its feature space.",
                    "label": 0
                },
                {
                    "sent": "For every measure on X cross 01.",
                    "label": 0
                },
                {
                    "sent": "This holds so the expected squared error.",
                    "label": 0
                },
                {
                    "sent": "And the probabilistic prediction.",
                    "label": 0
                },
                {
                    "sent": "Is bounded by.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Error rate of the classifier on the induced distribution.",
                    "label": 0
                },
                {
                    "sent": "Manage the minimum possible error rate.",
                    "label": 0
                },
                {
                    "sent": "So we really want to subtract this because we're creating kind of a hard classification problem.",
                    "label": 0
                },
                {
                    "sent": "By this process.",
                    "label": 0
                },
                {
                    "sent": "It might be that there's just some fundamental noise.",
                    "label": 0
                },
                {
                    "sent": "D is not actually deterministic, and so we really want to subtract this operates.",
                    "label": 0
                },
                {
                    "sent": "But the results will look too good.",
                    "label": 0
                },
                {
                    "sent": "Over all possible classifiers, this is the base error.",
                    "label": 0
                },
                {
                    "sent": "He is.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Yeah, use the smallest possible error rate.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So does this say so?",
                    "label": 0
                },
                {
                    "sent": "This is kind of a strange theorem, because it says that.",
                    "label": 0
                },
                {
                    "sent": "Even though we're never actually told.",
                    "label": 0
                },
                {
                    "sent": "What the probability of Y given X is just optimizing 01 loss.",
                    "label": 0
                },
                {
                    "sent": "Means that will actually be.",
                    "label": 0
                },
                {
                    "sent": "Finding the probability ynx.",
                    "label": 0
                },
                {
                    "sent": "Right, if we optimize this, well, this will go to zero and then this will go to 0.",
                    "label": 0
                },
                {
                    "sent": "And we must be predicting the correct probabilities.",
                    "label": 0
                },
                {
                    "sent": "This says that any.",
                    "label": 0
                },
                {
                    "sent": "Binary classification algorithm can be thought of as a class probability estimation algorithm.",
                    "label": 0
                },
                {
                    "sent": "It also says.",
                    "label": 0
                },
                {
                    "sent": "Because this is, this is squared loss and there's a slight variation that you can make any classifier do regression.",
                    "label": 0
                },
                {
                    "sent": "This holds for every C. For every D there's no assumption of independence here, and that means that all of the familiar bounds you love in terms of classification loss.",
                    "label": 0
                },
                {
                    "sent": "Will work.",
                    "label": 0
                },
                {
                    "sent": "Is also is sufficiently primitive that it will compose with online learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Check this minimum possible expected error.",
                    "label": 0
                },
                {
                    "sent": "Yeah here.",
                    "label": 0
                },
                {
                    "sent": "If you have any.",
                    "label": 0
                },
                {
                    "sent": "In general, you know hope that they come to the cheap.",
                    "label": 0
                },
                {
                    "sent": "Faraldi, that's right.",
                    "label": 0
                },
                {
                    "sent": "But for individual D, it might happen.",
                    "label": 0
                },
                {
                    "sent": "For some for the day that you care about, it might happen.",
                    "label": 0
                },
                {
                    "sent": "So what does this say for those types of binary classifier?",
                    "label": 0
                },
                {
                    "sent": "Stay it says that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And exactly what that set is is related to.",
                    "label": 0
                },
                {
                    "sent": "The structure of this process.",
                    "label": 0
                },
                {
                    "sent": "If they can get kind of close to the optimal classifier with free to get there, does that somehow.",
                    "label": 0
                },
                {
                    "sent": "You said they also get close to the truth.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 'cause if this is close to this then this is also near to.",
                    "label": 0
                },
                {
                    "sent": "I mean this is bounded by the distance between.",
                    "label": 0
                },
                {
                    "sent": "How close you are to.",
                    "label": 0
                },
                {
                    "sent": "I see there's two kinds of clothes there is this close to the optimal senior set of C and is close to the optimal.",
                    "label": 0
                },
                {
                    "sent": "See in all C. Yeah, close in the first sense doesn't help close in the second sense helps.",
                    "label": 0
                },
                {
                    "sent": "Choosing no CC is the classifier.",
                    "label": 0
                },
                {
                    "sent": "Coffee.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so E is just measuring the 01 loss and then I'm writing out the.",
                    "label": 0
                },
                {
                    "sent": "The squared error explicitly here.",
                    "label": 0
                },
                {
                    "sent": "Posted on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "They both cost for different problems that cost for.",
                    "label": 0
                },
                {
                    "sent": "Prediction, binary prediction and this one is the constable probabilistic prediction, right?",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "How much you lose for binary prediction on the induced problem.",
                    "label": 0
                },
                {
                    "sent": "This is how much you lose for probabilistic prediction on the original problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Hulu something about.",
                    "label": 0
                },
                {
                    "sent": "Considering the recent.",
                    "label": 0
                },
                {
                    "sent": "This is right.",
                    "label": 0
                },
                {
                    "sent": "You get this from the usual statistical way of dismissing one, right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Using this method, do I use this?",
                    "label": 0
                },
                {
                    "sent": "It's a good question to ask, I think.",
                    "label": 0
                },
                {
                    "sent": "In general the answer is you can lose, but for your specific example, I think you don't lose.",
                    "label": 0
                },
                {
                    "sent": "Because the roughly speaking, the sample complexity required to estimate a threshold online is equivalent to the same complexity required to estimate the error rate of any classifier.",
                    "label": 0
                },
                {
                    "sent": "That just has to do something has something to do with the structure of a line.",
                    "label": 0
                },
                {
                    "sent": "And threshold functions on the line.",
                    "label": 0
                },
                {
                    "sent": "OK. To me this is the style of reduction theorem.",
                    "label": 0
                },
                {
                    "sent": "It's very primitive.",
                    "label": 0
                },
                {
                    "sent": "We have strong quantifiers and relate the loss from the original problem to loss on our induced problem.",
                    "label": 0
                },
                {
                    "sent": "This D doesn't have to be the that the training set is drawn from.",
                    "label": 0
                },
                {
                    "sent": "There's no assumption that the training set and test set are drawn from the same distribution's.",
                    "label": 0
                },
                {
                    "sent": "It could be that the drug from entirely different distributions and then you just plug in this D and what easily happens in that case.",
                    "label": 0
                },
                {
                    "sent": "Is this just happens to be much larger than this?",
                    "label": 0
                },
                {
                    "sent": "And then the theorem is true, but not very interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, so the proof of this is reasonably simple, so I'll just show it to you.",
                    "label": 0
                },
                {
                    "sent": "I will tell you in advance it's only two pages so.",
                    "label": 0
                },
                {
                    "sent": "You can pay attention or go to sleep for the appropriate period of time.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first point is that the expected importance is 1/2.",
                    "label": 0
                },
                {
                    "sent": "And the reason why the expected importance is 1/2 is because you have this interval between zero and one.",
                    "label": 0
                },
                {
                    "sent": "We're drawing AP uniformly from this interval.",
                    "label": 0
                },
                {
                    "sent": "And then we're looking at important to just magnitude of Y -- P, which is either going to be this function or this function.",
                    "label": 0
                },
                {
                    "sent": "When you integrate, you get a half.",
                    "label": 0
                },
                {
                    "sent": "Just yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so we start with the.",
                    "label": 0
                },
                {
                    "sent": "Induced loss.",
                    "label": 0
                },
                {
                    "sent": "Minus the minimum possible loss, and you can just write it out.",
                    "label": 0
                },
                {
                    "sent": "This is just writing out what the formula is except.",
                    "label": 0
                },
                {
                    "sent": "Now I've.",
                    "label": 0
                },
                {
                    "sent": "So this is just the loss for whatever see you have.",
                    "label": 0
                },
                {
                    "sent": "Then this is the Bayes optimal classifier.",
                    "label": 0
                },
                {
                    "sent": "So I'm just writing out.",
                    "label": 0
                },
                {
                    "sent": "What the Bayes optimal classifier is losses here?",
                    "label": 0
                },
                {
                    "sent": "So there's some probability of 1 given X.",
                    "label": 0
                },
                {
                    "sent": "In the last.",
                    "label": 0
                },
                {
                    "sent": "In that case, it's going to be 1 -- P. In this probability.",
                    "label": 0
                },
                {
                    "sent": "Some probability of zero given X and the last lesson that case is going to be P 'cause there be probability of actually having a one.",
                    "label": 0
                },
                {
                    "sent": "So then we do some algebra.",
                    "label": 0
                },
                {
                    "sent": "Actually, no, we don't even do algebra.",
                    "label": 0
                },
                {
                    "sent": "What we do is we shuffle the expectation around a little bit.",
                    "label": 0
                },
                {
                    "sent": "And then you think about what this means.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "With X&P fixed, either C is going to predict the same as the Bayes optimal or opposite of it.",
                    "label": 0
                },
                {
                    "sent": "What that means is that this difference is either going to be 0 or it's going to be this.",
                    "label": 0
                },
                {
                    "sent": "'cause you just subtract that from that you get this.",
                    "label": 0
                },
                {
                    "sent": "OK. And now let's go back and think about that a minute.",
                    "label": 0
                },
                {
                    "sent": "So this is an absolute value around.",
                    "label": 0
                },
                {
                    "sent": "Some truth right so?",
                    "label": 0
                },
                {
                    "sent": "Let's say this is D of 1 given X.",
                    "label": 0
                },
                {
                    "sent": "This is an absolute value and that says that if we're at this P and we screw up, then the month Roblox we suffer is this, and for here then it's actually there is some sort of line like this.",
                    "label": 0
                },
                {
                    "sent": "Which describes how much loss.",
                    "label": 0
                },
                {
                    "sent": "We suffer if this is the P it was predicted correctly.",
                    "label": 0
                },
                {
                    "sent": "So now we need to think about we have an adversary.",
                    "label": 0
                },
                {
                    "sent": "There is going to choose a classifier and distribution chart and messes up as much as possible is going to have some budget of epsilon binary errors going to have some total loss.",
                    "label": 0
                },
                {
                    "sent": "You can suffer.",
                    "label": 0
                },
                {
                    "sent": "He's going to try to distort the probabilistic prediction as much as possible.",
                    "label": 0
                },
                {
                    "sent": "So there's two observations which really help.",
                    "label": 0
                },
                {
                    "sent": "One of them is.",
                    "label": 0
                },
                {
                    "sent": "If the adversary has if this is correct.",
                    "label": 0
                },
                {
                    "sent": "And this is correct, and the adversary can make either this one error that one error.",
                    "label": 0
                },
                {
                    "sent": "Then he always prefers to make this one error because he pays less and has the same effect after the sorting.",
                    "label": 0
                },
                {
                    "sent": "So this is where the sorting is, not a heuristic.",
                    "label": 0
                },
                {
                    "sent": "Because of this fact that we sort things screwing up here is equivalent to screwing up here.",
                    "label": 0
                },
                {
                    "sent": "It has the same effect, the same impact on the probability prediction that we make.",
                    "label": 0
                },
                {
                    "sent": "And since screwing up here pays a larger loss.",
                    "label": 0
                },
                {
                    "sent": "We want to screw up in places with smaller loss.",
                    "label": 0
                },
                {
                    "sent": "So that's the second point.",
                    "label": 0
                },
                {
                    "sent": "The first point is that.",
                    "label": 0
                },
                {
                    "sent": "You never an adversary who's on a budget is never going to actually screw up over here and over here.",
                    "label": 0
                },
                {
                    "sent": "Because when you sort these two cancel each other.",
                    "label": 0
                },
                {
                    "sent": "So there again, this sort is helping us.",
                    "label": 0
                },
                {
                    "sent": "So these two observations together say that.",
                    "label": 0
                },
                {
                    "sent": "The way the adversary is going to try to distort your public prediction as much as possible given his budget, he's going to choose a wedge like this.",
                    "label": 0
                },
                {
                    "sent": "It's just going to make all these guys air.",
                    "label": 0
                },
                {
                    "sent": "And nobody else will care.",
                    "label": 0
                },
                {
                    "sent": "And then a probabilistic prediction will be over here, and we'll pay this kind of squared error.",
                    "label": 0
                },
                {
                    "sent": "So we just need to calculate how much.",
                    "label": 0
                },
                {
                    "sent": "How much volume is in this in order to know what the budget is?",
                    "label": 0
                },
                {
                    "sent": "OK, so we're thinking up here that we just have a budget of epsilon, so this volume is an integral must be just epsilon.",
                    "label": 0
                },
                {
                    "sent": "So you have the integral from the truth to the truth plus some Delta.",
                    "label": 0
                },
                {
                    "sent": "Of this absolute value, just get Delta squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a proof, right?",
                    "label": 0
                },
                {
                    "sent": "That double squared is going to be plugged into.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And it just becomes this.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how long did this work?",
                    "label": 0
                },
                {
                    "sent": "So this is the results from the Championship this 2004 right?",
                    "label": 0
                },
                {
                    "sent": "We actually entered.",
                    "label": 0
                },
                {
                    "sent": "We were actually coming over the program.",
                    "label": 0
                },
                {
                    "sent": "We did it.",
                    "label": 0
                },
                {
                    "sent": "And at first I was disappointed.",
                    "label": 0
                },
                {
                    "sent": "Because we didn't win.",
                    "label": 0
                },
                {
                    "sent": "But nevertheless, we turned out that we did pretty well.",
                    "label": 0
                },
                {
                    "sent": "And you see this when you realize there were 60 some entries, and I guess we were 6th or something here on this one and we were.",
                    "label": 0
                },
                {
                    "sent": "6th over here.",
                    "label": 0
                },
                {
                    "sent": "And again I get my 67 entries.",
                    "label": 0
                },
                {
                    "sent": "And you realize this more when you go and you actually go to this Kitty.",
                    "label": 0
                },
                {
                    "sent": "Conference.",
                    "label": 0
                },
                {
                    "sent": "He discovered that the people who won.",
                    "label": 0
                },
                {
                    "sent": "Basically, use the humanistic learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Humans are great learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Stop listening.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "We can't beat humans yet.",
                    "label": 0
                },
                {
                    "sent": "But you know, if you go and you look at the teams that actually work on both.",
                    "label": 0
                },
                {
                    "sent": "Problems we were second.",
                    "label": 0
                },
                {
                    "sent": "This there is the winner and there was us for a second team.",
                    "label": 0
                },
                {
                    "sent": "In ours, we didn't actually look at the data.",
                    "label": 0
                },
                {
                    "sent": "This was completely automatic.",
                    "label": 0
                },
                {
                    "sent": "We mean we came up with the theory.",
                    "label": 0
                },
                {
                    "sent": "We tested decision trees in logistic regression.",
                    "label": 0
                },
                {
                    "sent": "We ended up using decision trees.",
                    "label": 0
                },
                {
                    "sent": "And that was it.",
                    "label": 0
                },
                {
                    "sent": "So the approach we used is entirely automatic.",
                    "label": 0
                },
                {
                    "sent": "And press requires less time for the next problem then then this.",
                    "label": 0
                },
                {
                    "sent": "To do so, so you use, for example integration.",
                    "label": 0
                },
                {
                    "sent": "So you start with the program we just treated as 01 classifier.",
                    "label": 0
                },
                {
                    "sent": "That was substantially better.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, turns out it works much better.",
                    "label": 0
                },
                {
                    "sent": "OK, but then you re converted back again.",
                    "label": 0
                },
                {
                    "sent": "So yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "I think we also tried to support vector machines and they were about the same as logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I think this is sort of the state of the art in.",
                    "label": 0
                },
                {
                    "sent": "In trying to make that box that I talked about at the beginning.",
                    "label": 0
                },
                {
                    "sent": "We can't meet humans yet, but if you take a look, we're actually not that far off.",
                    "label": 0
                },
                {
                    "sent": "And I mean, we're only paying like a percent or two on each of these different metrics.",
                    "label": 0
                },
                {
                    "sent": "I guess that's the worst one.",
                    "label": 0
                },
                {
                    "sent": "Ooh, cross entropy is really fun.",
                    "label": 0
                },
                {
                    "sent": "You gotta look at these guys down here.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "E 73 E 73.",
                    "label": 0
                },
                {
                    "sent": "About 1/3 of the entries had some sort of infinite cross entropy because they predicted zero when it was a one or something like that.",
                    "label": 0
                },
                {
                    "sent": "So it did something.",
                    "label": 0
                },
                {
                    "sent": "Sandwiches did something saying for each of these possibilities, and it wasn't even really optimized.",
                    "label": 0
                },
                {
                    "sent": "I mean, if I was really going to try to do the reductionist approach which didn't have time for entirely, then it would have had a different layer data transforming prediction transform for each of these different metrics, and you know we were just starting in 2004 on this kind of stuff, so we didn't have time to try to do that.",
                    "label": 0
                },
                {
                    "sent": "OK. Is proving to be most.",
                    "label": 0
                },
                {
                    "sent": "Squared error yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean there's the theorem says.",
                    "label": 0
                },
                {
                    "sent": "In particular, it doesn't say anything about cross entropy, and it can't.",
                    "label": 0
                },
                {
                    "sent": "Because cross entropy has infinite loss anytime you have infinite loss, it's not reasonable.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So again, so the Z 73's are reasonable because so.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm surprising that they occur.",
                    "label": 0
                },
                {
                    "sent": "That was surprised that it occurred for 1/3 of the injuries though.",
                    "label": 0
                },
                {
                    "sent": "That says something about the state of a random machine learning researcher I guess.",
                    "label": 0
                },
                {
                    "sent": "Or something like that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess I should finish soon.",
                    "label": 0
                },
                {
                    "sent": "Right, so learning reductions are a lot of different things.",
                    "label": 0
                },
                {
                    "sent": "An they can be pretty useful from the theory point of view.",
                    "label": 0
                },
                {
                    "sent": "I think I'll skip all and just focus on this one.",
                    "label": 0
                },
                {
                    "sent": "So if you go out and you talk to the various applied learning people.",
                    "label": 0
                },
                {
                    "sent": "He tried to understand what they're doing in a lot of cases, I think that they're actually doing is this, but they're doing it intuitively innocent, intuitive level.",
                    "label": 0
                },
                {
                    "sent": "But if you go and look at the algorithms that they're actually using, it turns out a lot of them have reasonable reduction transforms.",
                    "label": 0
                },
                {
                    "sent": "As if they were thinking about this math.",
                    "label": 0
                },
                {
                    "sent": "So this is some formalization of the intuitions which.",
                    "label": 0
                },
                {
                    "sent": "Which a number of applied learning people have.",
                    "label": 0
                },
                {
                    "sent": "So maybe give some examples.",
                    "label": 0
                },
                {
                    "sent": "But first what I wanted to do is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give you some ammunition.",
                    "label": 0
                },
                {
                    "sent": "So there are limitations to this, so one thing which is important to understand is that this is not a complete story.",
                    "label": 0
                },
                {
                    "sent": "It is not the case that if you have a good reduction transform, you have a good learning algorithm because of course we're not even talking about learning algorithm is an second of all because.",
                    "label": 0
                },
                {
                    "sent": "There are examples of reductions which are nonsense.",
                    "label": 0
                },
                {
                    "sent": "So for example, you could have I could have done the program reduction, but instead I could have just encrypted the features.",
                    "label": 0
                },
                {
                    "sent": "And no, learning out of them.",
                    "label": 0
                },
                {
                    "sent": "No sane learning algorithm would actually succeed there.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Nothing which is important is the computational requirements.",
                    "label": 0
                },
                {
                    "sent": "So you were talking about this here.",
                    "label": 0
                },
                {
                    "sent": "Maybe we pay a factor of 10, or maybe a factor of 100 more.",
                    "label": 0
                },
                {
                    "sent": "It's not exponential, but it is important to think about effective trainer effect of 100.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And maybe there's some ways to speed this up if we can actually get this one classifier trick working in practice.",
                    "label": 0
                },
                {
                    "sent": "And the last thing is that there's no guarantees that created problems are easy.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of.",
                    "label": 0
                },
                {
                    "sent": "The kind of reasoning that you're doing is something like.",
                    "label": 0
                },
                {
                    "sent": "If I could solve all subproblems equally easily, then which subproblems would I solve?",
                    "label": 0
                },
                {
                    "sent": "And then you're hoping the ones you create or not too hard.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show you some pretty pictures.",
                    "label": 0
                },
                {
                    "sent": "There's actually two kinds of reductions that we've been working on.",
                    "label": 0
                },
                {
                    "sent": "One of them is air limiting reductions, which I think are sort of the roughest form of reduction.",
                    "label": 0
                },
                {
                    "sent": "So there what you do is you just say how does the error rate in the original problem.",
                    "label": 0
                },
                {
                    "sent": "How is that bounded by the area on the creative side problems?",
                    "label": 0
                },
                {
                    "sent": "And several of these.",
                    "label": 0
                },
                {
                    "sent": "Algorithms are not by us.",
                    "label": 0
                },
                {
                    "sent": "So error correcting output codes are by Tom Dietrich.",
                    "label": 0
                },
                {
                    "sent": "And it'll boost is by rupture.",
                    "label": 0
                },
                {
                    "sent": "Each of these can be thought of as.",
                    "label": 0
                },
                {
                    "sent": "As an error transform reduction, there's some new algorithms here.",
                    "label": 0
                },
                {
                    "sent": "So this is for Constance declassification.",
                    "label": 0
                },
                {
                    "sent": "So This is why I put the reason any reasonable loss into the title.",
                    "label": 0
                },
                {
                    "sent": "So in constant classification, what happens is.",
                    "label": 0
                },
                {
                    "sent": "Your distribution your so your distribution is just over X.",
                    "label": 0
                },
                {
                    "sent": "Cross.",
                    "label": 0
                },
                {
                    "sent": "A set of numbers.",
                    "label": 0
                },
                {
                    "sent": "To the.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "They're predicting T classes, so to the T. So then all we try to do is you try to minimize.",
                    "label": 0
                },
                {
                    "sent": "The expected.",
                    "label": 0
                },
                {
                    "sent": "Cost, so this is the cost and constant to classification.",
                    "label": 0
                },
                {
                    "sent": "So we draw an example of this form.",
                    "label": 0
                },
                {
                    "sent": "A predictor index is some element in this vector.",
                    "label": 0
                },
                {
                    "sent": "We try to minimize whatever element it indexes.",
                    "label": 0
                },
                {
                    "sent": "That's sufficiently general to describe any loss function.",
                    "label": 0
                },
                {
                    "sent": "And the theorem works well.",
                    "label": 0
                },
                {
                    "sent": "Whenever the losses are reasonably bounded.",
                    "label": 0
                },
                {
                    "sent": "Did you try like a direct comparison on the first?",
                    "label": 0
                },
                {
                    "sent": "Take a classifier?",
                    "label": 0
                },
                {
                    "sent": "Try your method in the like plot.",
                    "label": 0
                },
                {
                    "sent": "Just kind of yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Little big yeah so so concentrate the comparison only reduction yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "So we did this and it is in the program paper and it turns out that the probing method works a little bit better than the plot method was predicting machine.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, and it works a little bit better than a logistic.",
                    "label": 0
                },
                {
                    "sent": "There are reasons why you expect it to work a little bit better, because there are probabilistic services which are describable by this reduction that are just not describable by.",
                    "label": 0
                },
                {
                    "sent": "By the process of fitting logistic to some.",
                    "label": 0
                },
                {
                    "sent": "Linear threshold.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's there's some example.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As of this happening in practice.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is the other kind of reduction.",
                    "label": 0
                },
                {
                    "sent": "This is the kind that I showed you still regret transform reduction, so you're looking at the regret.",
                    "label": 0
                },
                {
                    "sent": "Which is the difference between how well you did and how well you could have done.",
                    "label": 0
                },
                {
                    "sent": "On the original problem and on the creative problem, you want this one to bound that one.",
                    "label": 0
                },
                {
                    "sent": "So again, we have a set of different reductions.",
                    "label": 0
                },
                {
                    "sent": "This one was just published, it Colt, and it handles any custom classification problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a modification of the operating output codes which Tom Dietrich worked on.",
                    "label": 0
                },
                {
                    "sent": "The one you told us about this classification.",
                    "label": 0
                },
                {
                    "sent": "That right here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so again, there's various ways that this is useful.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the last there's several.",
                    "label": 0
                },
                {
                    "sent": "Big open questions I guess.",
                    "label": 0
                },
                {
                    "sent": "So one of them is about the relationship between reinforcement learning and.",
                    "label": 0
                },
                {
                    "sent": "And just classification.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of people in nice email that are fond of.",
                    "label": 0
                },
                {
                    "sent": "Saying that reinforcement learning is different classification.",
                    "label": 0
                },
                {
                    "sent": "And this is reductions actually give you a way to think about how similar to learning problems are.",
                    "label": 0
                },
                {
                    "sent": "'cause if there's a reduction this way in this reduction that way, and they're both very tight, then they are almost the same problem in some sense.",
                    "label": 0
                },
                {
                    "sent": "But if there isn't a reduction that's tight, then they're not so.",
                    "label": 0
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                },
                {
                    "sent": "This is sort of an objective criterion for.",
                    "label": 0
                },
                {
                    "sent": "For building up a structure of overlearning problems over and learning problems.",
                    "label": 0
                },
                {
                    "sent": "Maybe we can understand what's harder and what's easier.",
                    "label": 0
                },
                {
                    "sent": "Anyway, that's this one we don't really understand the limits of how well we can do with these reductions yet.",
                    "label": 0
                },
                {
                    "sent": "Money Criminous is playing with this a little bit.",
                    "label": 0
                },
                {
                    "sent": "And then there's other.",
                    "label": 0
                },
                {
                    "sent": "Observation that you know reductions.",
                    "label": 0
                },
                {
                    "sent": "It's possible to design if you're trying, you can design A reduction, which will be nonsense in practice, but it gives you great mathematics.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like is some other.",
                    "label": 0
                },
                {
                    "sent": "Form of reductions analysis.",
                    "label": 0
                },
                {
                    "sent": "Something other than air transforms or regret.",
                    "label": 0
                },
                {
                    "sent": "Transforms which is more robust too.",
                    "label": 0
                },
                {
                    "sent": "This kind of thing we want that we want to be the case that whenever you do good mathematics, you do good learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Right now it's just when you do good mathematics and you're not trying to be evil.",
                    "label": 0
                },
                {
                    "sent": "It seems like you do a good learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Beginning, he said.",
                    "label": 0
                },
                {
                    "sent": "Holy Grail is this universal.",
                    "label": 0
                },
                {
                    "sent": "She.",
                    "label": 0
                },
                {
                    "sent": "And then we said that, baby, that here in mind was the standard measure that we should try to achieve.",
                    "label": 0
                }
            ]
        }
    }
}