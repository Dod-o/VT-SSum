{
    "id": "nrvggedm3zrol6j63gokuki7wl2jaqik",
    "title": "No-Regret Learning in Convex Games",
    "info": {
        "author": [
            "Geoffrey J. Gordon, Machine Learning Department, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 7, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_gordon_nrl/",
    "segmentation": [
        [
            "Alright, so thank you."
        ],
        [
            "Much.",
            "So the background for this talk is there's been a lot of work in the past, on no regret algorithms, and the interest in them is basically that they can learn well even if you're in a situation where your training data is coming from an adversary, an adversary is choosing the worst possible order to present your training data in or is having concept drift so that the hypothesis you're trying to learn is changing, and these algorithms are still able to guarantee that they can learn relatively well.",
            "And this property seems like it would be useful for learning in a multi agent setting if you're trying to learn in the context of a multi agent decision problem.",
            "And in fact there have been some results in that direction in the past, so there's a well known result that says no regret learners when you play 2.",
            "No regret learners against one another in a 0 sum game.",
            "They're guaranteed to reach a mini Max equilibrium.",
            "And by reach I mean they're guaranteed to achieve the same pay offs the mini Max pay offs as they would get in an equilibrium, and they're guaranteed to achieve the same.",
            "Marginal frequency of play.",
            "And then there's a similar more recent result saying that in a general sum game, if you have this slightly stronger, no regret property called no internal regret, you reach a correlated equilibrium.",
            "And again, that means that you reach the play frequencies and the pay offs.",
            "And again this is weaker than a guarantee of convergence.",
            "Your play frequencies don't converge to a point, but they converge to the set of possible play frequencies for a correlated equilibrium."
        ],
        [
            "And so the picture looks kind of like this.",
            "We have two different kinds of regret.",
            "The ordinary no regret, which is sometimes called no external regret.",
            "And then this stronger form called no internal.",
            "And we have two kinds of equilibrium mini Max equilibrium or the generalization of mini Max equilibrium to general.",
            "Some games, which is called a course correlated equilibrium.",
            "It's a very weak type of equilibrium and then you have correlated equilibrium, which is the thing which would really like to get in a general sum game."
        ],
        [
            "So in this talk, we're going to take that picture and we're going to run with it.",
            "We're going to try and extend it to games with more structure.",
            "And what do I mean by more structure?",
            "I mean, for example, extensive form games in which you have hidden information and you have your decisions taken overtime or games in which the agents are trying to do classification or regression problems.",
            "For example, opponent modeling by using a support vector machine or a linear regression, or something like that.",
            "And in general, we're going to try and do convex games.",
            "Which include both of those examples of special cases."
        ],
        [
            "So in convex games, the picture that was known before this paper looks a lot like the picture in ordinary normal form games, so we have these two types of regret, external regret, and it's actually the case that internal regret doesn't make much sense in convex games, but there's a generalization called swap regret, and so there's the external on one end and the swap on the other, the weaker, and the stronger notion of regret.",
            "And then there are these two notions of equilibrium course, correlated equilibrium and correlated equilibrium.",
            "And they're the same as before.",
            "An ones weaker in one stronger and the contribution of this paper."
        ],
        [
            "Is to fill in that space in the middle with a few interesting forms of regret and equilibrium.",
            "And so we're going to go through these examples in a little bit more detail in the talk, but.",
            "Basically what we have is 3 new forms of regret.",
            "We have a continuum of forms of regret that we talk about in the paper, but we single out three points along the continuum that we think are interesting.",
            "One of 'em is what we call no extensive form regret, and that happens to be interesting because it corresponds to extensive form correlated equilibrium, which is perhaps the most natural notion of equilibrium in extensive form games.",
            "The next one is linear regret, and that corresponds to a type of equilibrium that people don't talk about much called a linear equilibrium.",
            "And then finally finite element regret.",
            "So I've abbreviated them EF&FY, and hopefully that doesn't cause any confusion and that corresponds to a type of equilibrium which we're going to call finite element equilibrium, which nobody else has talked about before.",
            "But we're interested in it because it turns out we can prove some interesting things about."
        ],
        [
            "So 1 interesting thing is that all of these types of regret are exactly the same.",
            "If you're in a matrix game, so you only get the interesting differences when you go to the more structured setting of convex games, and in fact we prove that these regret types are distinct in the paper."
        ],
        [
            "And then the other thing is that in our paper we provide efficient algorithms for all of them except for the, except for the most flexible.",
            "And you'll see later on why it's difficult to provide an efficient algorithm for the for the swap regret."
        ],
        [
            "However, this is the reason we're interested in finite element equilibrium.",
            "It turns out that finite element equilibrium provides us with an efficient way to achieve a subclass of correlated equilibrium, which is in some cases payoff equivalent.",
            "So you can, if you have a correlated equilibrium, it might be very complicated, but you can find one that has the same payoff to all players in the class that we can get."
        ],
        [
            "Alright, so just briefly an outline of the talk I'm going to start off by giving background on convex games and online convex programs as well as different kinds of regret.",
            "Then I'm going to describe the algorithm that we're going to use, and in fact it's actually just an algorithm schema, and so you instantiate it for different types of regret and the algorithm improve for very simple, so hopefully I'll be able to give a an intuition for why I think that it's a nice algorithm.",
            "Then I'm going to talk about making it fast in practice, which is always a good thing.",
            "And finally, I'll summarize."
        ],
        [
            "So convex games were generalization of normal form games and also have extensive form games and also of Markov decision processes and also if a well about a billion other interesting learning problems that we want including classification and regression.",
            "The basic idea is that there are end players.",
            "Each player has a convex set from which they choose their action.",
            "So for example, if one of these convex sets corresponds to the state action frequencies in a Markov decision process, then that player is essentially choosing a path in a Markov decision processes their play.",
            "And I figure I have to mention that one, since this is the room that all of the reinforcement learning sessions have been in so far.",
            "So OK, we have our convex action sets.",
            "Everybody simultaneously chooses an action from their action set and then the payoff to the I player is a linear function of the players action and that linear function depends in some arbitrary way on the way the other players have chosen their actions and so another more maybe a simpler example is if you choose as your convex set a simplex in D dimensions, then that corresponds to a normal form game with the actions."
        ],
        [
            "Online convex programs are sort of the single agents view of what's happening in a repeated convex game, so the agent just focusing down on a single agent.",
            "I have my own feasible region from which I'm choosing my actions, and I alternately choose an action, and then I observe a cost vector that was determined by the actions of all of the other players, and then it goes back and forth.",
            "Action, cost, vector action, cost vector an.",
            "I have a total cost, which is the sum of the dot product between the actions at time T and the cost vector at time T. And I want to make that as low as possible and again if A is a simplex, this corresponds to something that sort of known from the literature is the problem of learning from expert advice.",
            "You have D experts, each of whom is making a prediction, and you want to agree with the one who turns out to be right in the end."
        ],
        [
            "To measure performance in a in a experts problem, the typical measure is what's called external regret.",
            "And So what the external regret looks like is you have you have the actual sum of costs that you occur in curd in nature, and then you have this sort of estimate of how well you could have done if you had known what the sequence was ahead of time, right?",
            "So you take the best action out of your entire set, and you say what?",
            "Would have been the total cost of the best action.",
            "Alright, we're going to use."
        ],
        [
            "Slightly more general definition of regret and before we give that definition, I want to talk about this.",
            "Necessary concept called an action transformation.",
            "So the idea of an action transformation is a function that takes your feasible region and Maps it into itself.",
            "So, for example, if our feasible region is this unit square that's on the slide, then we can take."
        ],
        [
            "A linear transformation with weights that are, you know, some their absolute values sum to one across each row, and we get what's essentially a rotation or skewing or translation of the.",
            "Of the square that's guaranteed to remain inside the square.",
            "Alright."
        ],
        [
            "So using these action transformations we can find a generalization of external regret, which is called fear regret, and it's essentially the same definition we have our actual observed cost that we actually in curd when we played the game.",
            "And then this time we're going to minimize over our action transformations.",
            "How much we would have gotten if we had taken each of our actions and transformed them and played that transformed action instead.",
            "So one reason that you might think that this.",
            "Notion of regret is interesting is because it's a strict generalization of external regret.",
            "So, for example, if we look at the constant transformations, there's a constant transformation that takes your no matter.",
            "It ignores what action it was that you actually did and says play this action instead, right?",
            "And in that case, what we'd be doing is we'd be minimizing over our set of actions, and we'd be minimizing the sum of the cost of having played that action instead, and we'd get back the original notion of external regret.",
            "But if you have the flexibility of doing transformations.",
            "Going back and messing with your actual sequence."
        ],
        [
            "Can be more flexible and so for example, we've already seen the external transformations if we have all measurable action transformations, then we get what's known as swap regret.",
            "That's the most flexible and you can see why that might be rather difficult to learn because the set of all measurable transformations is very large, and I also mentioned the set of linear transformations."
        ],
        [
            "I.",
            "The two sort of novel types of transformations that we talk about in this paper have to do with what we call finite element regret and extensive form regret.",
            "So let's take a look at the finite element regret.",
            "So here we have a polygonal feasible region.",
            "It happens to be a Pentagon with the corners labeled and what we've done is we've made a finite element mesh that connects the vertices of this feasible region, and so the finite element mesh means that we've divided into mutually exclusive and exhaustive triangles.",
            "An entire dimensions, they'd be simplices, and then we're allowed to put each one of the corners wherever we want, and then the triangles follow along according to the corresponding linear mapping.",
            "And So what I've shown here is 1 particular finite element transformation of the Pentagon into itself.",
            "Which is, you know, maybe not depending on may not be particularly interesting, but you can do this with much higher dimensional things that don't fit on the 2 dimensional screen here."
        ],
        [
            "And then if you want to hear about the extensive form transformations, I'm not going to have time to talk about them in the talk, but I encourage you to come to the poster and ask me."
        ],
        [
            "About them"
        ],
        [
            "Alright, so we have this idea of no fee regret which corresponds to this sort of flexible notion of performance measurement in online convex programs, and we want to design an algorithm which achieves No Fear regret, and we show in the paper that achieving no fee regret means that you all of the agents do it.",
            "You have a fee equilibrium in the end, so achieving No Fear.",
            "Regret is good and the idea of the algorithm is that we're going to do is the typical thing that people do when confronted with.",
            "Problem they don't know how to solve.",
            "We're going to reduce it to a problem that we do know how to solve and what we're going to do.",
            "In particular is reduce the No Fear regret problem for the set a tuineau external regret problem where there are plenty of algorithms for a more complicated convex feasible region."
        ],
        [
            "And in particular, we're going to take the set fee itself and consider that set fee as a convex subset or the convex Hull of fee is a convex subset of some vector space, and we're going to do no external regret on that feasible region."
        ],
        [
            "Little bit more detail so where we previously had a sequence of actions and cost vectors in our original lower dimensional space."
        ],
        [
            "What we're going to do now is we're going to have a sequence of fictitious actions and cost vectors that our reduction is going to produce that are in this higher dimensional space that corresponds to the transformations.",
            "And what?"
        ],
        [
            "Going to do is instead of directly learning on the lower dimensional actions and cost vectors every time we get a low dimensional cost vector, we're going to transform it to a high dimensional cost vector which corresponds to a function on our transformations, and then each one of these transformations we're going to pass to a new external regret learner, and that no external regret learner is going to give us a new hypothesis, which is a transformation, and we're going to take that transformation and transform it back to an action on the original space.",
            "Play that action and continue.",
            "So the question then is how are we going to do the two transformations and those are on this slide.",
            "There are the two lines in this.",
            "Algorithm summary so when we have a transformation fighty which was an action of the higher level, no regret algorithm, what we're going to do is find a fixed point of that transformation and call it a T and play that action.",
            "Not obvious why you'd want to do it, but maybe it will become obvious when I show the proof.",
            "And then to go the other way when we have a cost vector and our previous action, what we're going to do is construct a loss function which acts on transformations and the way we're going to do it is sort of the obvious way we take the transformation, and we apply it to the action and we see how much that transformed action would have cost.",
            "Alright, so this is a fairly simple algorithm summary.",
            "It may not be obvious why we made the particular choices we did, but at the very least it it's easy to implement as long as you have implementations of the two subroutines."
        ],
        [
            "So the two subroutines are this.",
            "No external regret algorithm that operates on this higher dimensional space and this fixed point Finder and these are the only ways that this algorithm depends on the actual set capital fee.",
            "Which means that if we can implement those two subroutines efficiently, this particular algorithm will be an efficient algorithm for achieving.",
            "No, see regret."
        ],
        [
            "Fact in the paper we prove that the algorithm does achieve No Fear, regret, and that it runs in polynomial time.",
            "If its subroutines do."
        ],
        [
            "Alright, so I just want I'm not going to go through this proof, I just want to put it on the slide to show that it is in fact the three line proof that once you have this algorithm you achieve No Fear regret.",
            "And if you want to know what the three lines are, come to the poster."
        ],
        [
            "But now I'm going to talk about how to make it fast in practice.",
            "So.",
            "The problem is that the set fee of transformations might be a really complicated set.",
            "For example, the set of finite element transformations, right?",
            "There's a lot of corners.",
            "You have a lot of degrees of freedom, and so it could be expensive to achieve no external regret on this high dimensional set with a lot of corners.",
            "And So what we're going to do is we're going to use half of the kernel trick to get an efficient linear representation that we can then implement and actually use this algae."
        ],
        [
            "Rhythm and the half of the kernel trick that we're going to use is the fact that you can take the take nonlinear functions and represent them as the composition of a fixed nonlinear part and an adjustable linear part.",
            "The other half of the kernel trick being that you never have to work in the high dimensional space, and we're going to actually work in the high dimensional space for some of these algorithms, because it turns out to be sufficiently small that we can still do that."
        ],
        [
            "And it turns out that if you have a kernelized fee, you can make this algorithm fast.",
            "You know just as soon as there's a general way to make this algorithm fast.",
            "If you have a kernelized fee and that's basically to observe that the loss functions in the high dimensional space are actually linear, and that the set of feasible loss functions as a convex set, and so you basically just do your ordinary Euclidean, no external regret algorithms on that on that convex set."
        ],
        [
            "Alright, one more theorem I want to mention.",
            "If we play only vertices of our finite element mesh.",
            "This is why we're interested in the finite element.",
            "Sort of regret if we play only vertices of the mesh and if we achieve no finite element regret, we also have no swap regret.",
            "Right, and so that means that in addition to being in a finite element equilibrium, where in a correlated equilibrium as well.",
            "And it turns out that we can extend our algorithm and the way to do that is written in the paper, but loosely what happens is that when the algorithm wants to play something that wasn't one of the vertices of the mesh, we warp its play to one of the nearby vertices of the mesh, and we show how to do that without altering its payoff and while preserving the regret guarantees and So what that means is that.",
            "Because we don't even alter the payoff, so we wind up getting for any correlated equilibrium.",
            "There's a payoff equivalent correlated equilibrium, which plays only vertices of the finite element mesh, and so if we have everybody learning on one of these finite element meshes, we get an actual correlated equilibrium in the class that we selected from can achieve all of the possible pay offs in the game."
        ],
        [
            "So just to summarize.",
            "I've shown you this.",
            "Sort of panoply of different regret types and the corresponding equilibrium types, and given you efficient algorithms to do most of them, and an efficient way to take the sort of most complicated one of them and push it a little bit further so that we can actually learn real correlated equilibrium."
        ],
        [
            "There are two of these that are particularly interesting.",
            "This is the first efficient learner that is guaranteed to converge to extensive form correlated equilibrium when you play it on extensive form games.",
            "So that's."
        ],
        [
            "Nice and this sort of transformation between finite element and swap gives us an exponentially more efficient algorithm than previously available ones.",
            "To learn correlated equilibrium, which is also a useful thing.",
            "And then in the last 30."
        ],
        [
            "Seconds I want to talk about.",
            "Some related work, so this builds on a long line of interesting related work.",
            "There's a paper by of Rimmon Ishai, which uses a very similar fixed point trick.",
            "In the context of normal form games, there's still some Lugosi's paper, which was the first to talk about fear, regret in the context of online convex programs, and they do an algorithm, not a very efficient one, but the first algorithm for swap."
        ],
        [
            "And then, finally, there's an interesting paper by Hazan and Cal.",
            "Satya.",
            "Do you know how to say his last name, Callie Cal.",
            "Anyway, Hazan in Sathyan.",
            "Who proposed a non kernelized version of the same algorithm, simultaneously with us?",
            "They published it in this past NIPS and they have a nice computational equivalence between finding fixed points and achieving no few regret."
        ],
        [
            "So thank you very much.",
            "Questions, yeah.",
            "Six points of those transformations.",
            "It depends very much on how complicated your set is.",
            "So if you have a in the case where you're using this kernel trick, what you have is a linear transformation, and so you can find fixed points of linear transformations using eigenvalue tricks, but you have to be careful because there might be a null space with multiple dimensions, and so you have to find that null space and then solve for a fixed point that's actually in your feasible region.",
            "So it's Poly time, but it could get kind of expensive.",
            "Yes.",
            "Arm.",
            "So yes, in order to, I imagine this is not stuff we've done, but I imagine that one could do it based on some parameter other than dimensionality.",
            "But no, we haven't.",
            "We haven't actually used any infinite dimensional kernels.",
            "Any other questions?",
            "Alright thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so thank you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much.",
                    "label": 0
                },
                {
                    "sent": "So the background for this talk is there's been a lot of work in the past, on no regret algorithms, and the interest in them is basically that they can learn well even if you're in a situation where your training data is coming from an adversary, an adversary is choosing the worst possible order to present your training data in or is having concept drift so that the hypothesis you're trying to learn is changing, and these algorithms are still able to guarantee that they can learn relatively well.",
                    "label": 0
                },
                {
                    "sent": "And this property seems like it would be useful for learning in a multi agent setting if you're trying to learn in the context of a multi agent decision problem.",
                    "label": 1
                },
                {
                    "sent": "And in fact there have been some results in that direction in the past, so there's a well known result that says no regret learners when you play 2.",
                    "label": 0
                },
                {
                    "sent": "No regret learners against one another in a 0 sum game.",
                    "label": 0
                },
                {
                    "sent": "They're guaranteed to reach a mini Max equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And by reach I mean they're guaranteed to achieve the same pay offs the mini Max pay offs as they would get in an equilibrium, and they're guaranteed to achieve the same.",
                    "label": 0
                },
                {
                    "sent": "Marginal frequency of play.",
                    "label": 1
                },
                {
                    "sent": "And then there's a similar more recent result saying that in a general sum game, if you have this slightly stronger, no regret property called no internal regret, you reach a correlated equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And again, that means that you reach the play frequencies and the pay offs.",
                    "label": 0
                },
                {
                    "sent": "And again this is weaker than a guarantee of convergence.",
                    "label": 0
                },
                {
                    "sent": "Your play frequencies don't converge to a point, but they converge to the set of possible play frequencies for a correlated equilibrium.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the picture looks kind of like this.",
                    "label": 0
                },
                {
                    "sent": "We have two different kinds of regret.",
                    "label": 0
                },
                {
                    "sent": "The ordinary no regret, which is sometimes called no external regret.",
                    "label": 1
                },
                {
                    "sent": "And then this stronger form called no internal.",
                    "label": 1
                },
                {
                    "sent": "And we have two kinds of equilibrium mini Max equilibrium or the generalization of mini Max equilibrium to general.",
                    "label": 0
                },
                {
                    "sent": "Some games, which is called a course correlated equilibrium.",
                    "label": 0
                },
                {
                    "sent": "It's a very weak type of equilibrium and then you have correlated equilibrium, which is the thing which would really like to get in a general sum game.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this talk, we're going to take that picture and we're going to run with it.",
                    "label": 0
                },
                {
                    "sent": "We're going to try and extend it to games with more structure.",
                    "label": 1
                },
                {
                    "sent": "And what do I mean by more structure?",
                    "label": 0
                },
                {
                    "sent": "I mean, for example, extensive form games in which you have hidden information and you have your decisions taken overtime or games in which the agents are trying to do classification or regression problems.",
                    "label": 0
                },
                {
                    "sent": "For example, opponent modeling by using a support vector machine or a linear regression, or something like that.",
                    "label": 1
                },
                {
                    "sent": "And in general, we're going to try and do convex games.",
                    "label": 0
                },
                {
                    "sent": "Which include both of those examples of special cases.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in convex games, the picture that was known before this paper looks a lot like the picture in ordinary normal form games, so we have these two types of regret, external regret, and it's actually the case that internal regret doesn't make much sense in convex games, but there's a generalization called swap regret, and so there's the external on one end and the swap on the other, the weaker, and the stronger notion of regret.",
                    "label": 1
                },
                {
                    "sent": "And then there are these two notions of equilibrium course, correlated equilibrium and correlated equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And they're the same as before.",
                    "label": 0
                },
                {
                    "sent": "An ones weaker in one stronger and the contribution of this paper.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to fill in that space in the middle with a few interesting forms of regret and equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to go through these examples in a little bit more detail in the talk, but.",
                    "label": 0
                },
                {
                    "sent": "Basically what we have is 3 new forms of regret.",
                    "label": 0
                },
                {
                    "sent": "We have a continuum of forms of regret that we talk about in the paper, but we single out three points along the continuum that we think are interesting.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is what we call no extensive form regret, and that happens to be interesting because it corresponds to extensive form correlated equilibrium, which is perhaps the most natural notion of equilibrium in extensive form games.",
                    "label": 0
                },
                {
                    "sent": "The next one is linear regret, and that corresponds to a type of equilibrium that people don't talk about much called a linear equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And then finally finite element regret.",
                    "label": 0
                },
                {
                    "sent": "So I've abbreviated them EF&FY, and hopefully that doesn't cause any confusion and that corresponds to a type of equilibrium which we're going to call finite element equilibrium, which nobody else has talked about before.",
                    "label": 0
                },
                {
                    "sent": "But we're interested in it because it turns out we can prove some interesting things about.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So 1 interesting thing is that all of these types of regret are exactly the same.",
                    "label": 0
                },
                {
                    "sent": "If you're in a matrix game, so you only get the interesting differences when you go to the more structured setting of convex games, and in fact we prove that these regret types are distinct in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the other thing is that in our paper we provide efficient algorithms for all of them except for the, except for the most flexible.",
                    "label": 0
                },
                {
                    "sent": "And you'll see later on why it's difficult to provide an efficient algorithm for the for the swap regret.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, this is the reason we're interested in finite element equilibrium.",
                    "label": 0
                },
                {
                    "sent": "It turns out that finite element equilibrium provides us with an efficient way to achieve a subclass of correlated equilibrium, which is in some cases payoff equivalent.",
                    "label": 0
                },
                {
                    "sent": "So you can, if you have a correlated equilibrium, it might be very complicated, but you can find one that has the same payoff to all players in the class that we can get.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so just briefly an outline of the talk I'm going to start off by giving background on convex games and online convex programs as well as different kinds of regret.",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to describe the algorithm that we're going to use, and in fact it's actually just an algorithm schema, and so you instantiate it for different types of regret and the algorithm improve for very simple, so hopefully I'll be able to give a an intuition for why I think that it's a nice algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about making it fast in practice, which is always a good thing.",
                    "label": 1
                },
                {
                    "sent": "And finally, I'll summarize.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So convex games were generalization of normal form games and also have extensive form games and also of Markov decision processes and also if a well about a billion other interesting learning problems that we want including classification and regression.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that there are end players.",
                    "label": 0
                },
                {
                    "sent": "Each player has a convex set from which they choose their action.",
                    "label": 0
                },
                {
                    "sent": "So for example, if one of these convex sets corresponds to the state action frequencies in a Markov decision process, then that player is essentially choosing a path in a Markov decision processes their play.",
                    "label": 0
                },
                {
                    "sent": "And I figure I have to mention that one, since this is the room that all of the reinforcement learning sessions have been in so far.",
                    "label": 0
                },
                {
                    "sent": "So OK, we have our convex action sets.",
                    "label": 1
                },
                {
                    "sent": "Everybody simultaneously chooses an action from their action set and then the payoff to the I player is a linear function of the players action and that linear function depends in some arbitrary way on the way the other players have chosen their actions and so another more maybe a simpler example is if you choose as your convex set a simplex in D dimensions, then that corresponds to a normal form game with the actions.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Online convex programs are sort of the single agents view of what's happening in a repeated convex game, so the agent just focusing down on a single agent.",
                    "label": 1
                },
                {
                    "sent": "I have my own feasible region from which I'm choosing my actions, and I alternately choose an action, and then I observe a cost vector that was determined by the actions of all of the other players, and then it goes back and forth.",
                    "label": 0
                },
                {
                    "sent": "Action, cost, vector action, cost vector an.",
                    "label": 0
                },
                {
                    "sent": "I have a total cost, which is the sum of the dot product between the actions at time T and the cost vector at time T. And I want to make that as low as possible and again if A is a simplex, this corresponds to something that sort of known from the literature is the problem of learning from expert advice.",
                    "label": 0
                },
                {
                    "sent": "You have D experts, each of whom is making a prediction, and you want to agree with the one who turns out to be right in the end.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To measure performance in a in a experts problem, the typical measure is what's called external regret.",
                    "label": 1
                },
                {
                    "sent": "And So what the external regret looks like is you have you have the actual sum of costs that you occur in curd in nature, and then you have this sort of estimate of how well you could have done if you had known what the sequence was ahead of time, right?",
                    "label": 0
                },
                {
                    "sent": "So you take the best action out of your entire set, and you say what?",
                    "label": 0
                },
                {
                    "sent": "Would have been the total cost of the best action.",
                    "label": 1
                },
                {
                    "sent": "Alright, we're going to use.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slightly more general definition of regret and before we give that definition, I want to talk about this.",
                    "label": 0
                },
                {
                    "sent": "Necessary concept called an action transformation.",
                    "label": 0
                },
                {
                    "sent": "So the idea of an action transformation is a function that takes your feasible region and Maps it into itself.",
                    "label": 1
                },
                {
                    "sent": "So, for example, if our feasible region is this unit square that's on the slide, then we can take.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A linear transformation with weights that are, you know, some their absolute values sum to one across each row, and we get what's essentially a rotation or skewing or translation of the.",
                    "label": 0
                },
                {
                    "sent": "Of the square that's guaranteed to remain inside the square.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So using these action transformations we can find a generalization of external regret, which is called fear regret, and it's essentially the same definition we have our actual observed cost that we actually in curd when we played the game.",
                    "label": 0
                },
                {
                    "sent": "And then this time we're going to minimize over our action transformations.",
                    "label": 0
                },
                {
                    "sent": "How much we would have gotten if we had taken each of our actions and transformed them and played that transformed action instead.",
                    "label": 0
                },
                {
                    "sent": "So one reason that you might think that this.",
                    "label": 0
                },
                {
                    "sent": "Notion of regret is interesting is because it's a strict generalization of external regret.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if we look at the constant transformations, there's a constant transformation that takes your no matter.",
                    "label": 0
                },
                {
                    "sent": "It ignores what action it was that you actually did and says play this action instead, right?",
                    "label": 0
                },
                {
                    "sent": "And in that case, what we'd be doing is we'd be minimizing over our set of actions, and we'd be minimizing the sum of the cost of having played that action instead, and we'd get back the original notion of external regret.",
                    "label": 0
                },
                {
                    "sent": "But if you have the flexibility of doing transformations.",
                    "label": 0
                },
                {
                    "sent": "Going back and messing with your actual sequence.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can be more flexible and so for example, we've already seen the external transformations if we have all measurable action transformations, then we get what's known as swap regret.",
                    "label": 0
                },
                {
                    "sent": "That's the most flexible and you can see why that might be rather difficult to learn because the set of all measurable transformations is very large, and I also mentioned the set of linear transformations.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The two sort of novel types of transformations that we talk about in this paper have to do with what we call finite element regret and extensive form regret.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at the finite element regret.",
                    "label": 1
                },
                {
                    "sent": "So here we have a polygonal feasible region.",
                    "label": 0
                },
                {
                    "sent": "It happens to be a Pentagon with the corners labeled and what we've done is we've made a finite element mesh that connects the vertices of this feasible region, and so the finite element mesh means that we've divided into mutually exclusive and exhaustive triangles.",
                    "label": 0
                },
                {
                    "sent": "An entire dimensions, they'd be simplices, and then we're allowed to put each one of the corners wherever we want, and then the triangles follow along according to the corresponding linear mapping.",
                    "label": 0
                },
                {
                    "sent": "And So what I've shown here is 1 particular finite element transformation of the Pentagon into itself.",
                    "label": 0
                },
                {
                    "sent": "Which is, you know, maybe not depending on may not be particularly interesting, but you can do this with much higher dimensional things that don't fit on the 2 dimensional screen here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if you want to hear about the extensive form transformations, I'm not going to have time to talk about them in the talk, but I encourage you to come to the poster and ask me.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About them",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so we have this idea of no fee regret which corresponds to this sort of flexible notion of performance measurement in online convex programs, and we want to design an algorithm which achieves No Fear regret, and we show in the paper that achieving no fee regret means that you all of the agents do it.",
                    "label": 0
                },
                {
                    "sent": "You have a fee equilibrium in the end, so achieving No Fear.",
                    "label": 0
                },
                {
                    "sent": "Regret is good and the idea of the algorithm is that we're going to do is the typical thing that people do when confronted with.",
                    "label": 0
                },
                {
                    "sent": "Problem they don't know how to solve.",
                    "label": 0
                },
                {
                    "sent": "We're going to reduce it to a problem that we do know how to solve and what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "In particular is reduce the No Fear regret problem for the set a tuineau external regret problem where there are plenty of algorithms for a more complicated convex feasible region.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in particular, we're going to take the set fee itself and consider that set fee as a convex subset or the convex Hull of fee is a convex subset of some vector space, and we're going to do no external regret on that feasible region.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit more detail so where we previously had a sequence of actions and cost vectors in our original lower dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're going to do now is we're going to have a sequence of fictitious actions and cost vectors that our reduction is going to produce that are in this higher dimensional space that corresponds to the transformations.",
                    "label": 0
                },
                {
                    "sent": "And what?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to do is instead of directly learning on the lower dimensional actions and cost vectors every time we get a low dimensional cost vector, we're going to transform it to a high dimensional cost vector which corresponds to a function on our transformations, and then each one of these transformations we're going to pass to a new external regret learner, and that no external regret learner is going to give us a new hypothesis, which is a transformation, and we're going to take that transformation and transform it back to an action on the original space.",
                    "label": 0
                },
                {
                    "sent": "Play that action and continue.",
                    "label": 0
                },
                {
                    "sent": "So the question then is how are we going to do the two transformations and those are on this slide.",
                    "label": 0
                },
                {
                    "sent": "There are the two lines in this.",
                    "label": 0
                },
                {
                    "sent": "Algorithm summary so when we have a transformation fighty which was an action of the higher level, no regret algorithm, what we're going to do is find a fixed point of that transformation and call it a T and play that action.",
                    "label": 0
                },
                {
                    "sent": "Not obvious why you'd want to do it, but maybe it will become obvious when I show the proof.",
                    "label": 0
                },
                {
                    "sent": "And then to go the other way when we have a cost vector and our previous action, what we're going to do is construct a loss function which acts on transformations and the way we're going to do it is sort of the obvious way we take the transformation, and we apply it to the action and we see how much that transformed action would have cost.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is a fairly simple algorithm summary.",
                    "label": 0
                },
                {
                    "sent": "It may not be obvious why we made the particular choices we did, but at the very least it it's easy to implement as long as you have implementations of the two subroutines.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the two subroutines are this.",
                    "label": 0
                },
                {
                    "sent": "No external regret algorithm that operates on this higher dimensional space and this fixed point Finder and these are the only ways that this algorithm depends on the actual set capital fee.",
                    "label": 0
                },
                {
                    "sent": "Which means that if we can implement those two subroutines efficiently, this particular algorithm will be an efficient algorithm for achieving.",
                    "label": 0
                },
                {
                    "sent": "No, see regret.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fact in the paper we prove that the algorithm does achieve No Fear, regret, and that it runs in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "If its subroutines do.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I just want I'm not going to go through this proof, I just want to put it on the slide to show that it is in fact the three line proof that once you have this algorithm you achieve No Fear regret.",
                    "label": 0
                },
                {
                    "sent": "And if you want to know what the three lines are, come to the poster.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But now I'm going to talk about how to make it fast in practice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The problem is that the set fee of transformations might be a really complicated set.",
                    "label": 0
                },
                {
                    "sent": "For example, the set of finite element transformations, right?",
                    "label": 0
                },
                {
                    "sent": "There's a lot of corners.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of degrees of freedom, and so it could be expensive to achieve no external regret on this high dimensional set with a lot of corners.",
                    "label": 1
                },
                {
                    "sent": "And So what we're going to do is we're going to use half of the kernel trick to get an efficient linear representation that we can then implement and actually use this algae.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rhythm and the half of the kernel trick that we're going to use is the fact that you can take the take nonlinear functions and represent them as the composition of a fixed nonlinear part and an adjustable linear part.",
                    "label": 0
                },
                {
                    "sent": "The other half of the kernel trick being that you never have to work in the high dimensional space, and we're going to actually work in the high dimensional space for some of these algorithms, because it turns out to be sufficiently small that we can still do that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that if you have a kernelized fee, you can make this algorithm fast.",
                    "label": 0
                },
                {
                    "sent": "You know just as soon as there's a general way to make this algorithm fast.",
                    "label": 0
                },
                {
                    "sent": "If you have a kernelized fee and that's basically to observe that the loss functions in the high dimensional space are actually linear, and that the set of feasible loss functions as a convex set, and so you basically just do your ordinary Euclidean, no external regret algorithms on that on that convex set.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, one more theorem I want to mention.",
                    "label": 0
                },
                {
                    "sent": "If we play only vertices of our finite element mesh.",
                    "label": 0
                },
                {
                    "sent": "This is why we're interested in the finite element.",
                    "label": 0
                },
                {
                    "sent": "Sort of regret if we play only vertices of the mesh and if we achieve no finite element regret, we also have no swap regret.",
                    "label": 1
                },
                {
                    "sent": "Right, and so that means that in addition to being in a finite element equilibrium, where in a correlated equilibrium as well.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that we can extend our algorithm and the way to do that is written in the paper, but loosely what happens is that when the algorithm wants to play something that wasn't one of the vertices of the mesh, we warp its play to one of the nearby vertices of the mesh, and we show how to do that without altering its payoff and while preserving the regret guarantees and So what that means is that.",
                    "label": 0
                },
                {
                    "sent": "Because we don't even alter the payoff, so we wind up getting for any correlated equilibrium.",
                    "label": 0
                },
                {
                    "sent": "There's a payoff equivalent correlated equilibrium, which plays only vertices of the finite element mesh, and so if we have everybody learning on one of these finite element meshes, we get an actual correlated equilibrium in the class that we selected from can achieve all of the possible pay offs in the game.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to summarize.",
                    "label": 0
                },
                {
                    "sent": "I've shown you this.",
                    "label": 0
                },
                {
                    "sent": "Sort of panoply of different regret types and the corresponding equilibrium types, and given you efficient algorithms to do most of them, and an efficient way to take the sort of most complicated one of them and push it a little bit further so that we can actually learn real correlated equilibrium.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are two of these that are particularly interesting.",
                    "label": 0
                },
                {
                    "sent": "This is the first efficient learner that is guaranteed to converge to extensive form correlated equilibrium when you play it on extensive form games.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice and this sort of transformation between finite element and swap gives us an exponentially more efficient algorithm than previously available ones.",
                    "label": 0
                },
                {
                    "sent": "To learn correlated equilibrium, which is also a useful thing.",
                    "label": 0
                },
                {
                    "sent": "And then in the last 30.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seconds I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "Some related work, so this builds on a long line of interesting related work.",
                    "label": 1
                },
                {
                    "sent": "There's a paper by of Rimmon Ishai, which uses a very similar fixed point trick.",
                    "label": 0
                },
                {
                    "sent": "In the context of normal form games, there's still some Lugosi's paper, which was the first to talk about fear, regret in the context of online convex programs, and they do an algorithm, not a very efficient one, but the first algorithm for swap.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then, finally, there's an interesting paper by Hazan and Cal.",
                    "label": 0
                },
                {
                    "sent": "Satya.",
                    "label": 0
                },
                {
                    "sent": "Do you know how to say his last name, Callie Cal.",
                    "label": 0
                },
                {
                    "sent": "Anyway, Hazan in Sathyan.",
                    "label": 0
                },
                {
                    "sent": "Who proposed a non kernelized version of the same algorithm, simultaneously with us?",
                    "label": 0
                },
                {
                    "sent": "They published it in this past NIPS and they have a nice computational equivalence between finding fixed points and achieving no few regret.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Questions, yeah.",
                    "label": 0
                },
                {
                    "sent": "Six points of those transformations.",
                    "label": 0
                },
                {
                    "sent": "It depends very much on how complicated your set is.",
                    "label": 0
                },
                {
                    "sent": "So if you have a in the case where you're using this kernel trick, what you have is a linear transformation, and so you can find fixed points of linear transformations using eigenvalue tricks, but you have to be careful because there might be a null space with multiple dimensions, and so you have to find that null space and then solve for a fixed point that's actually in your feasible region.",
                    "label": 0
                },
                {
                    "sent": "So it's Poly time, but it could get kind of expensive.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Arm.",
                    "label": 0
                },
                {
                    "sent": "So yes, in order to, I imagine this is not stuff we've done, but I imagine that one could do it based on some parameter other than dimensionality.",
                    "label": 0
                },
                {
                    "sent": "But no, we haven't.",
                    "label": 0
                },
                {
                    "sent": "We haven't actually used any infinite dimensional kernels.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright thanks.",
                    "label": 0
                }
            ]
        }
    }
}