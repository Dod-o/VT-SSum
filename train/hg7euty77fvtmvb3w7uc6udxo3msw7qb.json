{
    "id": "hg7euty77fvtmvb3w7uc6udxo3msw7qb",
    "title": "Generalized Expansion Dimension",
    "info": {
        "author": [
            "Michael Nett, National Institute of Informatics"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Data Mining->Applications"
        ]
    },
    "url": "http://videolectures.net/ptdm2012_nett_expansion_dimension/",
    "segmentation": [
        [
            "Thank you for the opportunity to giving this talk today."
        ],
        [
            "For the sake of basically being brief, I'll be brief, so if there are any questions, feel free to ask.",
            "And even if you can't read anything because of the lighting, or if I'm not speaking loud enough, please tell me that.",
            "Well, the outline for this talk is start with talking a little bit about the motivation of why you would want to actually consider dimensionality as an issue.",
            "I'm going to talk about the way that people have modeled intrinsic dimensionality and the way we want to do it.",
            "And this all is pretty much application driven, so I'm basically bound to talk about applications in the end and conclude the whole thing by talking about the challenges that are still open."
        ],
        [
            "Alright, so one of the I think most typical situations where that people think of if you talk about dimensionality is it's a way to characterize the difficulty of your data.",
            "So you can imagine a very frequent situation being someone asking another person, you know how you have this data in this machine learning or data mining problem contexts.",
            "How difficult is it?",
            "And like, the answer is more often than it should be, probably something that is.",
            "Phrased in terms of the representation of dimensionality of the data, so the number of features that you have.",
            "And.",
            "Recently it turned out more and more that this is not very appropriate and.",
            "We can we can better express these things in terms of something called intrinsic dimensionality.",
            "And, well, the question is then of course, why should we care in practice about about the difference between these things and, well, the first thing that you can do is you can usually analyze the cost of the performance of fundamental operations like search in terms of intrinsic dimensionality.",
            "And there are a few examples that I mentioned later."
        ],
        [
            "Knowledge about intrinsic dimensionality.",
            "Intrinsic dimensionality can actually help us to design efficient heuristics.",
            "And something that's really cool.",
            "I'll explain a little bit later.",
            "It can actually help us to support certain algorithmic decisions that the program has to make at runtime.",
            "Alright, so.",
            "As I said, counting the number of features of your data is usually not very appropriate and you can imagine having types of data like graphs, for example, where it's not really clear what what are number of features is because it depends on the featurization, right?",
            "So you get a lot of different issues if you just want to talk about numbers of features.",
            "However, with respect to intrinsic dimensionality, is there have been a number of things proposed?",
            "Arranging from things like the aspect ratio or fractal dimensions which are completely in terms of dimensionality.",
            "Two things like covering dimensions disorder in equalities or expansion dimension and the last one is particularly relatively important to this talk because our model is can be seen as kind of a generalization of this thing.",
            "Well, what is the core idea of our model?",
            "The idea is if you look around here, if you have some reference point and you look around you see a certain distribution of your data.",
            "So your data has a certain density, right?",
            "You could have more points in a smaller radius, or you could have a greater number of points.",
            "Sorry, smaller number of points in a larger area so that tells you something about the density and what we're going to do in our model is we're going to try to figure out which dimensionality actually explains the density that you observe the most or the best."
        ],
        [
            "So the key observation that we use formally speaking is that the volume of a neighborhood.",
            "So you take a point, you take a radius and you consider the basically the surrounding.",
            "And the volume of this neighborhood is determined by the radius that you pick.",
            "And by the dimensionality of your data set.",
            "Or your space rather, and there might be a few other constants in there, but we don't really need to worry too much about that.",
            "Well, formula that you see quite often in the introductory courses to data mining is relating to the volume of hyper sphere in Euclidean space.",
            "So if you have Euclidean space and you have M dimensions, then this basically tells you what the volume of this neighborhood is.",
            "So.",
            "And well, we can now go ahead and match."
        ],
        [
            "Someone we would be in a situation where we know we have Euclidean space but someone would keep this M hidden from us.",
            "So and we have to guess it or find out what the M is and the general way how we want to do that is by basically measuring neighborhood volumes according to certain choices of Radii.",
            "And then try to solve these results.",
            "These measurements that we get in order to extract the dimensionality.",
            "And in case of the Euclidean.",
            "Euclidean space.",
            "What you can just do is you can consider the ratio of.",
            "The volume of 2 two neighborhoods and all these mess with the gamma function and the powers of Pi cancels out, which is very convenient, and it allows you to basically solve these this expression.",
            "Directly for the dimensionality.",
            "And it turns out that if you use other spaces where you have generally a distance that is induced by the by an LP norm, not not necessarily Euclidean.",
            "And you might even have weightings for the different features.",
            "This result still holds.",
            "However, if you go to other domains like spaces with vector angle distances as you see for documents or Hemming distance for categorical data, you get other expressions so.",
            "What this kind of solution is actually depends on the space that you're working with."
        ],
        [
            "Alright, so however, just considering a complete a full space is not very interesting because usually work with fixed data points, so it's kind of you can think of this as samples from from a space, right?",
            "And what we do in order to transfer this idea is instead of instead of thinking of volumes of neighborhoods, we try to estimate these volumes by simply counting the number of points of our data set that fall into this particular area of space.",
            "And by doing that, instead of having volume, so we basically get number of points K1 and K2 that correspond to a certain radius in our data set.",
            "So R1 and R2.",
            "And while we gain locality awareness because you can imagine if you move your center of neighborhoods around in your data set, these numbers that you see will change, so the estimates of the dimensionality that you get at these points may actually be different.",
            "And in fact this is something interesting.",
            "But they're also like, well, it's more problems that you get, and one of them is that the number of points that you capture within a certain neighborhood actually does not grow smoothly when you increase the radius of that neighborhood.",
            "And, well, you can imagine there are different ways of dealing with this, something that we decided to do for the sake of having a very stable approach is basically we use medians of medians.",
            "So we have two neighborhoods in this case, and we basically we fixed the neighborhood size of the first one and we scale over the range of the second one.",
            "And from all these values that we then get, we take the medians.",
            "Alright.",
            "Well, for."
        ],
        [
            "Even as I'm basically, I want to skip.",
            "This is just the formalization of this media."
        ],
        [
            "Of medians idea.",
            "Then if you put this into an actual program, I hope the colors are visible from back there.",
            "I'm not sure because of the lighting.",
            "Anyway, if you put this into a program and use like a small data set in this case I just put a plane into a 3 dimensional space, used Euclidean distance, and I cut out a few pieces to make it a little bit more interesting and you go to each of the points where you have a data point and you run this approach and compute the dimensionality estimate using this idea.",
            "Well, you get these numbers so you can see.",
            "Yeah, in the areas where you have mostly in the two dimensional plane, you see values that are around 2:00 and if you go to something that kind of.",
            "Or basically looks more like a 1 dimensional line.",
            "You see less values.",
            "Sorry, smaller values actually."
        ],
        [
            "OK, so how does this actually relate to applications and the big picture?",
            "If you think of the formula we had before is that this model of dimensionality combines different kinds of information and the first one is on top is the dimensionality.",
            "I call it M. And then you have contributions from distance, various distance values and rank values.",
            "And the idea is now, if you know enough of these things.",
            "Then you can actually put bounds on the other one, so you can actually infer the other ones.",
            "And we can we can exploit this in different ways in."
        ],
        [
            "Different applications.",
            "The first one I want to talk about a little bit is pruning techniques so you know while there's there things around like metric, you can use the triangle inequality for pruning.",
            "But we want to do something different and what we do is we assume we are in a situation where you have any index basically which is which is based on a separator hierarchy.",
            "So at any point in the search you have like a collection of separator structures that could be planes or spheres or whatever.",
            "And what the only thing that we basically assume is that given the query point, you can compute the distances of the minimal distance between the query and any of these separators efficiently in some way.",
            "So if you can do that, then this scenario is basically matching.",
            "And what we say is you start exploring the space and at some point you will search with progress until you hit a separator and then you are.",
            "Basically you are at the point where you have to decide.",
            "Do I look behind the separator?",
            "So do I open another branch in my index structure?",
            "And usually you don't want to do that, because this is one way how you can accelerate search by pruning these parts off if possible.",
            "And So what do we know?",
            "At certain point, if we for example search for K nearest neighbors, we have.",
            "Let's assume we have K candidates.",
            "We know the distance of our furthest candidate, which could be somewhere on here.",
            "For example in this distance.",
            "And we can.",
            "As I said, we we have to be able to compute the distance to the most restrictive separator.",
            "And since we have our candidate set in memory somewhere, we can also of course check how many points we find within these distance bonds, right?",
            "So what we have is we have two radii and we have two ranking values, so.",
            "You can see where the application actually comes in.",
            "The thing is, if we put these into the into the dimensional framework that we had before.",
            "The one point that we actually need to care about is we have seen K points within this.",
            "Basically this radius, right?",
            "But we didn't open other sites of separators, so we don't know if there are more points in here or in that area or at that point.",
            "So it's.",
            "Kind of the outer circle, so to say.",
            "The inner one has a K1, points.",
            "The outer one may have K points that we've seen so far, plus epsilon points that we have not discovered yet.",
            "And.",
            "Of course, if we can figure if you can prove so in some way that epsilon is 0, then it's safe to cut off these paths and simply terminate the search.",
            "So."
        ],
        [
            "Our model tells us in if we know something about how the dimensionality of our data looks like.",
            "So for example, if we know the maximum.",
            "We can we know that the maximum I called item X in here is basically an upper bound for this expression that we had on the slides.",
            "Before.",
            "You can reshape this to give you the inequality that you say, see below and at any point in the search when this is satisfied, you can simply terminate and you have 100% accuracy.",
            "And.",
            "Well, of course you can aim for 100% accuracy, but you could also go ahead and say you want to have, for example, a ustic values.",
            "So you want to be even faster because this might be not the most restrictive pruning condition.",
            "So you could go ahead and and.",
            "You don't choose the maximum value, but you basically choose like a percentile.",
            "So you could say I want to have a value of dimensionality that works for 90% of my data."
        ],
        [
            "So.",
            "Another problem that comes in here is basically that but computing the dimensionality is very expensive, as you can imagine, there are a lot of neighborhoods that needs to be computed, and a lot of possibilities, so it's like a combinatorial problem.",
            "And this is exposed a little bit more.",
            "In the paper we have solution.",
            "You can basically approximate this simply by sampling and while we get estimates that concentrate very sharply around the true value so.",
            "In this sense, you don't lose much accuracy by doing sampling.",
            "And by by using estimates rather than the real values, which are very."
        ],
        [
            "Pensive I'm going to skip this one just a few kind of advertisement.",
            "This is another application of the model and it's going to be presented in all the Clustering 2 section on ICM.",
            "So if you're interested in learning more about this, I recommend.",
            "Listening to the talk.",
            "Um?"
        ],
        [
            "Alright, so yeah, just to make it even more interesting for this problem of multi step similarity search.",
            "There was an algorithm, well rather older ready from 1998 from Xylan Kriegel and they actually showed that if you don't have any knowledge about the future distribution of distance.",
            "While you do this multi step search then this solution is optimal and what we show in the talk that's going to be in the clustering to set session is basically if you have kind of knowledge and in this case it's.",
            "That means, in terms of the dimensionality of your data, then you can actually do much better than this.",
            "Alright."
        ],
        [
            "That's a belongs there.",
            "Then there is a few things actually that are just to give you an impression of how well developed these ideas are.",
            "There is already a lot of work.",
            "There was already a lot of work done, basically which either use dimensionality or different models of dimensionality so as to being able to analyze the performance of methods.",
            "One very well, the first example is from cargo and rule or the cover tree.",
            "So the first 2 for example.",
            "Basically, people developed a measure.",
            "In these cases the expansion.",
            "I mentioned the classical one and they could use this to show that's the second example that you can do similarity search N log N time, but you get.",
            "Basically you get penalized by other constant terms which are large.",
            "If your dimensionality is large, so.",
            "This is pretty neat.",
            "We have one thing in the pipeline right now it's not published yet, but there is a technical report available, so if you Google if you Google that, you can actually download it for free I think.",
            "And.",
            "What we basically do is we show that you can trade off between data set size and the impact that the dimensionality actually gives you a little bit better and you get things that are in practice actually much better than the results that.",
            "Have been used here then there was an application on basically outlier detection two years ago and I CDM, which won the best paper award.",
            "Which also uses this model in order to speed up the process and give you certain guarantees in certain situations.",
            "All these things.",
            "And then there is a second talk, which is later at CDM this year."
        ],
        [
            "OK, what are the challenges?",
            "Very shortly.",
            "I talked a little about Euclidean space but we have gone to a few other domains but there are still like very interesting things open.",
            "For example, what about set lattices or even even like the big Data style scale free networks that everyone is talking about?",
            "How can you bring this model to these domains?",
            "We are thinking about having a more general model for the complexity of better, and in particular this is like kind of a case study that would be very interesting is if you actually go ahead and if you compare empirically, compare this model of dimensionality with other models like PCA for example.",
            "What can we learn there?",
            "I'm.",
            "Yeah, current work and future work, so we have the dimensional pruning which is currently in the pipeline.",
            "So we at the experimentation stage right now we have someone working on anomaly detection using measures of dimensionality as a density estimates someone is working on feature selection and we are currently working on using in an algorithm.",
            "The search algorithm using on the fly measurements of dimensionality for cost or accuracy, rebalancing.",
            "Alright, so time is up actually, so thank you very much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for the opportunity to giving this talk today.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the sake of basically being brief, I'll be brief, so if there are any questions, feel free to ask.",
                    "label": 0
                },
                {
                    "sent": "And even if you can't read anything because of the lighting, or if I'm not speaking loud enough, please tell me that.",
                    "label": 0
                },
                {
                    "sent": "Well, the outline for this talk is start with talking a little bit about the motivation of why you would want to actually consider dimensionality as an issue.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about the way that people have modeled intrinsic dimensionality and the way we want to do it.",
                    "label": 0
                },
                {
                    "sent": "And this all is pretty much application driven, so I'm basically bound to talk about applications in the end and conclude the whole thing by talking about the challenges that are still open.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so one of the I think most typical situations where that people think of if you talk about dimensionality is it's a way to characterize the difficulty of your data.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine a very frequent situation being someone asking another person, you know how you have this data in this machine learning or data mining problem contexts.",
                    "label": 0
                },
                {
                    "sent": "How difficult is it?",
                    "label": 0
                },
                {
                    "sent": "And like, the answer is more often than it should be, probably something that is.",
                    "label": 0
                },
                {
                    "sent": "Phrased in terms of the representation of dimensionality of the data, so the number of features that you have.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Recently it turned out more and more that this is not very appropriate and.",
                    "label": 0
                },
                {
                    "sent": "We can we can better express these things in terms of something called intrinsic dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And, well, the question is then of course, why should we care in practice about about the difference between these things and, well, the first thing that you can do is you can usually analyze the cost of the performance of fundamental operations like search in terms of intrinsic dimensionality.",
                    "label": 1
                },
                {
                    "sent": "And there are a few examples that I mentioned later.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Knowledge about intrinsic dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Intrinsic dimensionality can actually help us to design efficient heuristics.",
                    "label": 1
                },
                {
                    "sent": "And something that's really cool.",
                    "label": 0
                },
                {
                    "sent": "I'll explain a little bit later.",
                    "label": 0
                },
                {
                    "sent": "It can actually help us to support certain algorithmic decisions that the program has to make at runtime.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "As I said, counting the number of features of your data is usually not very appropriate and you can imagine having types of data like graphs, for example, where it's not really clear what what are number of features is because it depends on the featurization, right?",
                    "label": 1
                },
                {
                    "sent": "So you get a lot of different issues if you just want to talk about numbers of features.",
                    "label": 1
                },
                {
                    "sent": "However, with respect to intrinsic dimensionality, is there have been a number of things proposed?",
                    "label": 0
                },
                {
                    "sent": "Arranging from things like the aspect ratio or fractal dimensions which are completely in terms of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "Two things like covering dimensions disorder in equalities or expansion dimension and the last one is particularly relatively important to this talk because our model is can be seen as kind of a generalization of this thing.",
                    "label": 0
                },
                {
                    "sent": "Well, what is the core idea of our model?",
                    "label": 0
                },
                {
                    "sent": "The idea is if you look around here, if you have some reference point and you look around you see a certain distribution of your data.",
                    "label": 0
                },
                {
                    "sent": "So your data has a certain density, right?",
                    "label": 0
                },
                {
                    "sent": "You could have more points in a smaller radius, or you could have a greater number of points.",
                    "label": 0
                },
                {
                    "sent": "Sorry, smaller number of points in a larger area so that tells you something about the density and what we're going to do in our model is we're going to try to figure out which dimensionality actually explains the density that you observe the most or the best.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key observation that we use formally speaking is that the volume of a neighborhood.",
                    "label": 1
                },
                {
                    "sent": "So you take a point, you take a radius and you consider the basically the surrounding.",
                    "label": 0
                },
                {
                    "sent": "And the volume of this neighborhood is determined by the radius that you pick.",
                    "label": 1
                },
                {
                    "sent": "And by the dimensionality of your data set.",
                    "label": 0
                },
                {
                    "sent": "Or your space rather, and there might be a few other constants in there, but we don't really need to worry too much about that.",
                    "label": 1
                },
                {
                    "sent": "Well, formula that you see quite often in the introductory courses to data mining is relating to the volume of hyper sphere in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "So if you have Euclidean space and you have M dimensions, then this basically tells you what the volume of this neighborhood is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And well, we can now go ahead and match.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Someone we would be in a situation where we know we have Euclidean space but someone would keep this M hidden from us.",
                    "label": 0
                },
                {
                    "sent": "So and we have to guess it or find out what the M is and the general way how we want to do that is by basically measuring neighborhood volumes according to certain choices of Radii.",
                    "label": 0
                },
                {
                    "sent": "And then try to solve these results.",
                    "label": 0
                },
                {
                    "sent": "These measurements that we get in order to extract the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And in case of the Euclidean.",
                    "label": 0
                },
                {
                    "sent": "Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "What you can just do is you can consider the ratio of.",
                    "label": 0
                },
                {
                    "sent": "The volume of 2 two neighborhoods and all these mess with the gamma function and the powers of Pi cancels out, which is very convenient, and it allows you to basically solve these this expression.",
                    "label": 0
                },
                {
                    "sent": "Directly for the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if you use other spaces where you have generally a distance that is induced by the by an LP norm, not not necessarily Euclidean.",
                    "label": 0
                },
                {
                    "sent": "And you might even have weightings for the different features.",
                    "label": 0
                },
                {
                    "sent": "This result still holds.",
                    "label": 0
                },
                {
                    "sent": "However, if you go to other domains like spaces with vector angle distances as you see for documents or Hemming distance for categorical data, you get other expressions so.",
                    "label": 0
                },
                {
                    "sent": "What this kind of solution is actually depends on the space that you're working with.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so however, just considering a complete a full space is not very interesting because usually work with fixed data points, so it's kind of you can think of this as samples from from a space, right?",
                    "label": 0
                },
                {
                    "sent": "And what we do in order to transfer this idea is instead of instead of thinking of volumes of neighborhoods, we try to estimate these volumes by simply counting the number of points of our data set that fall into this particular area of space.",
                    "label": 0
                },
                {
                    "sent": "And by doing that, instead of having volume, so we basically get number of points K1 and K2 that correspond to a certain radius in our data set.",
                    "label": 0
                },
                {
                    "sent": "So R1 and R2.",
                    "label": 0
                },
                {
                    "sent": "And while we gain locality awareness because you can imagine if you move your center of neighborhoods around in your data set, these numbers that you see will change, so the estimates of the dimensionality that you get at these points may actually be different.",
                    "label": 0
                },
                {
                    "sent": "And in fact this is something interesting.",
                    "label": 0
                },
                {
                    "sent": "But they're also like, well, it's more problems that you get, and one of them is that the number of points that you capture within a certain neighborhood actually does not grow smoothly when you increase the radius of that neighborhood.",
                    "label": 1
                },
                {
                    "sent": "And, well, you can imagine there are different ways of dealing with this, something that we decided to do for the sake of having a very stable approach is basically we use medians of medians.",
                    "label": 1
                },
                {
                    "sent": "So we have two neighborhoods in this case, and we basically we fixed the neighborhood size of the first one and we scale over the range of the second one.",
                    "label": 0
                },
                {
                    "sent": "And from all these values that we then get, we take the medians.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Well, for.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even as I'm basically, I want to skip.",
                    "label": 0
                },
                {
                    "sent": "This is just the formalization of this media.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of medians idea.",
                    "label": 0
                },
                {
                    "sent": "Then if you put this into an actual program, I hope the colors are visible from back there.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure because of the lighting.",
                    "label": 0
                },
                {
                    "sent": "Anyway, if you put this into a program and use like a small data set in this case I just put a plane into a 3 dimensional space, used Euclidean distance, and I cut out a few pieces to make it a little bit more interesting and you go to each of the points where you have a data point and you run this approach and compute the dimensionality estimate using this idea.",
                    "label": 0
                },
                {
                    "sent": "Well, you get these numbers so you can see.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the areas where you have mostly in the two dimensional plane, you see values that are around 2:00 and if you go to something that kind of.",
                    "label": 0
                },
                {
                    "sent": "Or basically looks more like a 1 dimensional line.",
                    "label": 0
                },
                {
                    "sent": "You see less values.",
                    "label": 0
                },
                {
                    "sent": "Sorry, smaller values actually.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how does this actually relate to applications and the big picture?",
                    "label": 1
                },
                {
                    "sent": "If you think of the formula we had before is that this model of dimensionality combines different kinds of information and the first one is on top is the dimensionality.",
                    "label": 0
                },
                {
                    "sent": "I call it M. And then you have contributions from distance, various distance values and rank values.",
                    "label": 0
                },
                {
                    "sent": "And the idea is now, if you know enough of these things.",
                    "label": 0
                },
                {
                    "sent": "Then you can actually put bounds on the other one, so you can actually infer the other ones.",
                    "label": 1
                },
                {
                    "sent": "And we can we can exploit this in different ways in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different applications.",
                    "label": 0
                },
                {
                    "sent": "The first one I want to talk about a little bit is pruning techniques so you know while there's there things around like metric, you can use the triangle inequality for pruning.",
                    "label": 0
                },
                {
                    "sent": "But we want to do something different and what we do is we assume we are in a situation where you have any index basically which is which is based on a separator hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So at any point in the search you have like a collection of separator structures that could be planes or spheres or whatever.",
                    "label": 0
                },
                {
                    "sent": "And what the only thing that we basically assume is that given the query point, you can compute the distances of the minimal distance between the query and any of these separators efficiently in some way.",
                    "label": 0
                },
                {
                    "sent": "So if you can do that, then this scenario is basically matching.",
                    "label": 0
                },
                {
                    "sent": "And what we say is you start exploring the space and at some point you will search with progress until you hit a separator and then you are.",
                    "label": 0
                },
                {
                    "sent": "Basically you are at the point where you have to decide.",
                    "label": 0
                },
                {
                    "sent": "Do I look behind the separator?",
                    "label": 0
                },
                {
                    "sent": "So do I open another branch in my index structure?",
                    "label": 0
                },
                {
                    "sent": "And usually you don't want to do that, because this is one way how you can accelerate search by pruning these parts off if possible.",
                    "label": 0
                },
                {
                    "sent": "And So what do we know?",
                    "label": 0
                },
                {
                    "sent": "At certain point, if we for example search for K nearest neighbors, we have.",
                    "label": 0
                },
                {
                    "sent": "Let's assume we have K candidates.",
                    "label": 0
                },
                {
                    "sent": "We know the distance of our furthest candidate, which could be somewhere on here.",
                    "label": 0
                },
                {
                    "sent": "For example in this distance.",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "As I said, we we have to be able to compute the distance to the most restrictive separator.",
                    "label": 0
                },
                {
                    "sent": "And since we have our candidate set in memory somewhere, we can also of course check how many points we find within these distance bonds, right?",
                    "label": 0
                },
                {
                    "sent": "So what we have is we have two radii and we have two ranking values, so.",
                    "label": 0
                },
                {
                    "sent": "You can see where the application actually comes in.",
                    "label": 0
                },
                {
                    "sent": "The thing is, if we put these into the into the dimensional framework that we had before.",
                    "label": 0
                },
                {
                    "sent": "The one point that we actually need to care about is we have seen K points within this.",
                    "label": 0
                },
                {
                    "sent": "Basically this radius, right?",
                    "label": 0
                },
                {
                    "sent": "But we didn't open other sites of separators, so we don't know if there are more points in here or in that area or at that point.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "Kind of the outer circle, so to say.",
                    "label": 0
                },
                {
                    "sent": "The inner one has a K1, points.",
                    "label": 0
                },
                {
                    "sent": "The outer one may have K points that we've seen so far, plus epsilon points that we have not discovered yet.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Of course, if we can figure if you can prove so in some way that epsilon is 0, then it's safe to cut off these paths and simply terminate the search.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our model tells us in if we know something about how the dimensionality of our data looks like.",
                    "label": 0
                },
                {
                    "sent": "So for example, if we know the maximum.",
                    "label": 0
                },
                {
                    "sent": "We can we know that the maximum I called item X in here is basically an upper bound for this expression that we had on the slides.",
                    "label": 0
                },
                {
                    "sent": "Before.",
                    "label": 0
                },
                {
                    "sent": "You can reshape this to give you the inequality that you say, see below and at any point in the search when this is satisfied, you can simply terminate and you have 100% accuracy.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, of course you can aim for 100% accuracy, but you could also go ahead and say you want to have, for example, a ustic values.",
                    "label": 0
                },
                {
                    "sent": "So you want to be even faster because this might be not the most restrictive pruning condition.",
                    "label": 0
                },
                {
                    "sent": "So you could go ahead and and.",
                    "label": 0
                },
                {
                    "sent": "You don't choose the maximum value, but you basically choose like a percentile.",
                    "label": 0
                },
                {
                    "sent": "So you could say I want to have a value of dimensionality that works for 90% of my data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Another problem that comes in here is basically that but computing the dimensionality is very expensive, as you can imagine, there are a lot of neighborhoods that needs to be computed, and a lot of possibilities, so it's like a combinatorial problem.",
                    "label": 0
                },
                {
                    "sent": "And this is exposed a little bit more.",
                    "label": 0
                },
                {
                    "sent": "In the paper we have solution.",
                    "label": 0
                },
                {
                    "sent": "You can basically approximate this simply by sampling and while we get estimates that concentrate very sharply around the true value so.",
                    "label": 1
                },
                {
                    "sent": "In this sense, you don't lose much accuracy by doing sampling.",
                    "label": 0
                },
                {
                    "sent": "And by by using estimates rather than the real values, which are very.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pensive I'm going to skip this one just a few kind of advertisement.",
                    "label": 0
                },
                {
                    "sent": "This is another application of the model and it's going to be presented in all the Clustering 2 section on ICM.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in learning more about this, I recommend.",
                    "label": 0
                },
                {
                    "sent": "Listening to the talk.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so yeah, just to make it even more interesting for this problem of multi step similarity search.",
                    "label": 0
                },
                {
                    "sent": "There was an algorithm, well rather older ready from 1998 from Xylan Kriegel and they actually showed that if you don't have any knowledge about the future distribution of distance.",
                    "label": 0
                },
                {
                    "sent": "While you do this multi step search then this solution is optimal and what we show in the talk that's going to be in the clustering to set session is basically if you have kind of knowledge and in this case it's.",
                    "label": 0
                },
                {
                    "sent": "That means, in terms of the dimensionality of your data, then you can actually do much better than this.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a belongs there.",
                    "label": 0
                },
                {
                    "sent": "Then there is a few things actually that are just to give you an impression of how well developed these ideas are.",
                    "label": 0
                },
                {
                    "sent": "There is already a lot of work.",
                    "label": 0
                },
                {
                    "sent": "There was already a lot of work done, basically which either use dimensionality or different models of dimensionality so as to being able to analyze the performance of methods.",
                    "label": 0
                },
                {
                    "sent": "One very well, the first example is from cargo and rule or the cover tree.",
                    "label": 0
                },
                {
                    "sent": "So the first 2 for example.",
                    "label": 0
                },
                {
                    "sent": "Basically, people developed a measure.",
                    "label": 0
                },
                {
                    "sent": "In these cases the expansion.",
                    "label": 0
                },
                {
                    "sent": "I mentioned the classical one and they could use this to show that's the second example that you can do similarity search N log N time, but you get.",
                    "label": 0
                },
                {
                    "sent": "Basically you get penalized by other constant terms which are large.",
                    "label": 0
                },
                {
                    "sent": "If your dimensionality is large, so.",
                    "label": 0
                },
                {
                    "sent": "This is pretty neat.",
                    "label": 0
                },
                {
                    "sent": "We have one thing in the pipeline right now it's not published yet, but there is a technical report available, so if you Google if you Google that, you can actually download it for free I think.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we basically do is we show that you can trade off between data set size and the impact that the dimensionality actually gives you a little bit better and you get things that are in practice actually much better than the results that.",
                    "label": 0
                },
                {
                    "sent": "Have been used here then there was an application on basically outlier detection two years ago and I CDM, which won the best paper award.",
                    "label": 0
                },
                {
                    "sent": "Which also uses this model in order to speed up the process and give you certain guarantees in certain situations.",
                    "label": 0
                },
                {
                    "sent": "All these things.",
                    "label": 0
                },
                {
                    "sent": "And then there is a second talk, which is later at CDM this year.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what are the challenges?",
                    "label": 0
                },
                {
                    "sent": "Very shortly.",
                    "label": 0
                },
                {
                    "sent": "I talked a little about Euclidean space but we have gone to a few other domains but there are still like very interesting things open.",
                    "label": 0
                },
                {
                    "sent": "For example, what about set lattices or even even like the big Data style scale free networks that everyone is talking about?",
                    "label": 0
                },
                {
                    "sent": "How can you bring this model to these domains?",
                    "label": 0
                },
                {
                    "sent": "We are thinking about having a more general model for the complexity of better, and in particular this is like kind of a case study that would be very interesting is if you actually go ahead and if you compare empirically, compare this model of dimensionality with other models like PCA for example.",
                    "label": 1
                },
                {
                    "sent": "What can we learn there?",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, current work and future work, so we have the dimensional pruning which is currently in the pipeline.",
                    "label": 1
                },
                {
                    "sent": "So we at the experimentation stage right now we have someone working on anomaly detection using measures of dimensionality as a density estimates someone is working on feature selection and we are currently working on using in an algorithm.",
                    "label": 0
                },
                {
                    "sent": "The search algorithm using on the fly measurements of dimensionality for cost or accuracy, rebalancing.",
                    "label": 0
                },
                {
                    "sent": "Alright, so time is up actually, so thank you very much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}