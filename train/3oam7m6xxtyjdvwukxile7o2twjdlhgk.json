{
    "id": "3oam7m6xxtyjdvwukxile7o2twjdlhgk",
    "title": "Generalizations of the Theory of Linearly Solvable MDPs",
    "info": {
        "author": [
            "Krishnamurthy Dvijotham, Department of Computer Science and Engineering, University of Washington"
        ],
        "published": "Oct. 16, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Robotics",
            "Top->Physics->Statistical Physics",
            "Top->Mathematics->Control Theory"
        ]
    },
    "url": "http://videolectures.net/cyberstat2012_dvijotham_linearly_solvable_mdp/",
    "segmentation": [
        [
            "I'll be talking about couple of projects that I worked on as extensions to Mo theory of linearly solvable MDP's and the idea is just to sort of provide a high level overview of the results.",
            "I won't be really going into the details, but we can obviously talk later if someone is interested."
        ],
        [
            "So first I'll be talking about an extension of linearly solvable MDP's to the game theoretic setting where you have an adversary in addition to the controller.",
            "So I'll describe certain properties of these games and illustrate the effect of the adversary through some simple numerical examples.",
            "Then I'll talk a little bit about policy gradients in linearly solvable MDP's.",
            "So this is basically very similar to the idea of Authentical control, where you have some parameterization of your policy, and you want you, you simulate your system with these policy para meters an just based on these simulations you want to compute some iterative improvements to your policy, so that's what that part will be apart."
        ],
        [
            "So let me begin by introducing Markov games.",
            "These are also known as stochastic games.",
            "There in the context of control, they are generally used in a setting where you have some kind of model uncertainty, so you have some uncertain para meters in your dynamics.",
            "There's something about your dynamics that you don't quite understand.",
            "And so one convenient way to model these is as some kind of adversary.",
            "And the adversary can can use control to affect your system, the same way that you controller can.",
            "And usually the adversary is.",
            "Controls are chosen so as to maximize costs.",
            "So mathematically, you can treat this as a 0 sum, 2 player 0 sum game where you have cost function that depends on your state.",
            "The controllers control input and the adversarial control input.",
            "And like in standard control problems this is accumulated over some time horizon.",
            "You have stochastic dynamics that are function of both control inputs.",
            "And so the controller is trying to minimize these costs while the adversary is trying to maximize them.",
            "And shapely in the early 1950s came up with the solution to this problem, and the solution is through dynamic programming type recursion, very similar to the Bellman equation, except that you have this additional Max term over the adversarial input.",
            "And in games of this form, you say that you have a saddle point equilibrium.",
            "If first of all a solution exists and the solution doesn't change if you interchange the min and Max.",
            "Can I ask questions?",
            "Yeah, why would it be more logical or under what circumstances would be more logical to assume that the unknown dynamics are your enemy rather than say some average of what they are?",
            "I mean, I would have thought it was more natural to replace them by some expected, not probably well.",
            "I mean in the context of robust control, I thought that's what's done.",
            "Traditionally, like you have some some unmodeled dynamics and you want your controller to work despite the worst case.",
            "In in that.",
            "Smart idea OK I see.",
            "Well yeah.",
            "I mean I don't have very good answer to that.",
            "What happened here about that?",
            "Yeah, so I. I'm not really sure.",
            "I guess the idea is just to be sort of paranoid here.",
            "You want to be robust to the worst case."
        ],
        [
            "So I like linear equations and linear linearly solvable MVP's are great that way because you can transform the Bellman equation to a linear equation through this exponential transformation.",
            "So you have in standard linearly solvable MPs, you have a state costan.",
            "This scale divergences between the control and control dynamics, and this cost structure allows you to make certain manipulations to the Bellman equation that make it linear.",
            "So if you sort of try try to naively extend this to the game theoretic setting.",
            "And you just plug in cable costs for both the controller and the adversary.",
            "It turns out that the resulting shapely equation is is not linear anymore.",
            "So the motivation for this project was how do you generalize this cost structure to the game theoretic setting so that you still get a linearly solvable Bellman equation so?"
        ],
        [
            "So for that you need to introduce this concept of any divergences.",
            "Renny came up with these in the early 1960s by basically looking at the axiomatic characterization of Shannon entropy and relaxing one of the assumptions, and this allows you to define a more general class of empty entropies.",
            "That includes there any entropies as a special case.",
            "So, so the definition is given here.",
            "It sort of looks like the KL divergent, where instead of the log you have P or Q to the Alpha.",
            "And you can recover the KL Divergent as a special case.",
            "In the limit where Alpha goes to one."
        ],
        [
            "So certain properties of these divergences they are known to be non negative.",
            "If Alpha is greater than zero, then there's zero forcing in the sense that they are equal to 0 only when the two arguments are equal, they are increasing in the argument Alpha and also continuous in Alpha.",
            "So here is a plot of the divergent versus Alpha for a particular set of distributions."
        ],
        [
            "I.",
            "So here is the structure of these linearly solvable Markov games.",
            "You start with your system in a state X at at a certain time T. If neither the controller nor the adversary intervened, you would have had hypothetically transition to a new state based on this uncontrolled or passive dynamics.",
            "But in actually the controller and the adversary, both of them are going to intervene so.",
            "So it turns out that for this structure to work out.",
            "The dynamics is such that the what the adversary chooses is actually what happens is what the controller chooses is actually what happens.",
            "So if you remember from most AKA couple of days back in these sort of problems you need to work with controls as probability distributions.",
            "So when you when I say you see is the control input, here it's actually the distribution resulting from a particular control input.",
            "And So what happens is that the controller and the adversary pick their respective distributions.",
            "What ultimately happens is is governed just by what the controller pics, but the adversary still has an effect on the problem through through this cost function.",
            "So the natural way to think about this process is you have some uncontrolled dynamics.",
            "The controller does some intervention to change it, so the adversary does some intervention to change it to some other dynamics.",
            "And the controller further does some other change right?",
            "And?",
            "And there's a control cost associated with each of these transformations, which is proportional to the divergences between the change dynamics and the original dynamics.",
            "So here you have this rainy divergance between the uncontrolled dynamics and the dynamics picked by the adversary, and then further on top of that you have a control cost for the controller, which is the divergent between what the controller pics and what the adversary picks.",
            "And it turns out that with this particular cost structure.",
            "You can do some manipulations to the shapely equation that.",
            "Sorry about that.",
            "So you can do some manipulations to the shapely equation that let it become linearly solvable again."
        ],
        [
            "So it turns out that as you can see here, you'll need some restrictions on Alpha in order to guarantee a solution.",
            "So if the solution exists, you can.",
            "You can solve the problem for any Alpha and it still has a meaningful solution, but you can only guarantee that the solution exists for Alpha between zero and.",
            "Is against every game in which EU and the nature is playing a game over choose probability distribution.",
            "Yes, yeah.",
            "So there's there's some natural dynamics.",
            "The adversary pick something else and the controller picks some further change about that, and both the controller and the adversary are penalized for the change they cost from what was there before.",
            "Right, but everything is expected values.",
            "Yes, yes, it's not expected.",
            "OK, so.",
            "So, so here's the main result.",
            "So when Alpha lies between zero and one, you can guarantee that linearly solver Markov game has a saddle point equilibrium and the solution is given by the following linear Bellman equation.",
            "So the only difference between this cennamo standard case of expected costs is the Alpha showing up here.",
            "So as such the Bellman equation has a very similar structure.",
            "You can solve it using similar methods.",
            "But the control law is is defined in in terms of the Z scale to the one minus Alpha, which is which is again different from.",
            "The standard linearly solvent control.",
            "So you can see that this framework has the property that as far as Alpha increases, the adversary gets stronger.",
            "So if you."
        ],
        [
            "Go back to this previous equation I have there any divergent of the order one over Alpha and as Alpha increases this we know that it's monotone in Alpha in the order.",
            "So basically the control costs for the adversary decreases as Alpha increases and in the limit Alpha goes to Infinity.",
            "This control cost becomes high enough that it's always optimal for the adversary to not change the dynamics.",
            "So you go back to the standard setting.",
            "Here in this case for you to place to casting.",
            "Policy.",
            "Why is that?",
            "Deterministic, but I'm not in system.",
            "You wouldn't expect the controller to be random.",
            "But but there's there's an adversary, right?",
            "I understand, but just think about driving your car or something like that.",
            "You wouldn't expect the driver to use a stochastic policy in a deterministic situation.",
            "Well think, think, think.",
            "Think of you being chased by a copper or a guy firing a gun.",
            "You were, you would wonder like make your policy stochastic so that he can't guess what you're doing, OK?",
            "You didn't really have a priority probability distribution on the cop or the gunfire.",
            "I mean, if you know that there's someone for system, you have family of systems.",
            "You said you had a system.",
            "No, I said I had a system with parametric uncertainty in my model.",
            "I insist, OK, OK, I see.",
            "OK, well then this probably makes more sense for for the case where you have a family system, then you want a single policy that that works across all of them."
        ],
        [
            "OK."
        ],
        [
            "You restricted your offer free between zero and one, and the limit offer goes to one is a standard clear yes but but but basic."
        ],
        [
            "The that the limit of Alpha goes to one will give you a Cal control cost for the adversary as well, but I mean the special case of linearly solver them DPS is obtained by getting rid of the adversary, so that is the limit where Alpha goes to Infinity.",
            "Basically where the?",
            "Sorry, I say I said which is the limit where Alpha goes to zero.",
            "Yeah so so then this becomes the infinite order versions which is high enough that."
        ],
        [
            "OK so you.",
            "I mean you can see that by plugging in Alpha equal to 0 here you get back the standard equations for the linearly solver."
        ],
        [
            "OK, so here are some simple illustrations to illustrate the effect of the adversary or so the general effect of the adversary is, as you increase Alpha, your control policy is going to become more and more risk averse.",
            "So consider this simple problem of driving your car up a Hill and you have two Hills here and you.",
            "Your cost is basically monotonically decreasing as height increases, so you're trying to get up as far as high as possible.",
            "So there are two Hills here and the first one is like really narrow and thin and the second one is a little shorter, but it's a little broader.",
            "And so you."
        ],
        [
            "And see that as Alpha changes between being risk and negative values where which corresponds to actually a cooperative game.",
            "You have when Alpha is negative, you basically choose the risky solution an when A0 you sort of player bets evenly between both of them and well, first positive you you focus on the risk of our solution."
        ],
        [
            "Similar problem here.",
            "This is actually similar to birds example of the drunken spider.",
            "So you have a start state and a goal state and you want to get get there as quickly as possible, but you have these obstacles in between and if you get pushed into one of the obstacles you incur a very high cost so.",
            "So what you will end up."
        ],
        [
            "Doing is that when when when you're not sufficiently discovers, you tend to choose a solution that goes straight through the middle, but when the risk aversion is increases to a sufficient value, you have some kind of phase transition in this behavior and you choose paths that go around."
        ],
        [
            "Is the final example of a simple system with an unstable equilibrium at the origin.",
            "And your goal is to sort of push to keep the system at unstable equilibrium."
        ],
        [
            "And so here are the control loss for three different values of the risk sensitive perimetre.",
            "And so when you are not sufficiently discovers, you basically employ very strong controls and you effectively cancel out the natural dynamics of the system.",
            "But as you become more and more risk averse, you tend to play it safe and you trade off performance for lower control gains."
        ],
        [
            "How much time do I have left?",
            "I think 5 six minutes OK?",
            "So I'll talk a little bit about policy gradients and this idea of computing iterative improvements to your control strategy, given simulations of the system.",
            "So this sort of concludes the previous part of the talk, and now I'm going back to the standard expected cost case without without the adversary.",
            "I.",
            "So the set."
        ],
        [
            "King is the following, so you can.",
            "This is sort of an alternative way of thinking of these learners.",
            "Solvable MDP's you can think of yourself as having a deterministic system with a stochastic policy, which gives you the probability of actions given your state.",
            "If you assume that your dynamics are such that they are invertible.",
            "So given the current state and the next state, you can infer the controls that were applied.",
            "Then you can re parameterized your policy simply in terms of probability transition matrix from state X to state X prime.",
            "And you have this cost function which I allow to depend both on the current state in the next state and plus the KL divergent's cost.",
            "So it turns out."
        ],
        [
            "But for this class of problems.",
            "The policy gradient has a nice natural interpretation, so if I consider a trajectory XI can compute the probability of that trajectory under my policy and.",
            "And then I have state cost plus the KL divergent control cost.",
            "So this is my expected cost function and if I just simply differentiate this with respect to my policy para meters theater, I get this sort of simple expression for the policy gradient.",
            "And as with the previous talk and evangelists talk yesterday, this is expressed as an expectation over your current policy and so you can basically simply do rollouts of your system under your current policy, compute estimates of this and average to get an unbiased estimate of your gradient.",
            "And it turns out that this algorithm can actually actually includes a lot of the previous algorithms as."
        ],
        [
            "Actual cases, so this approach was actually pioneered by Williams in 1992, where he used the reinforce algorithm.",
            "But in our case, because we have this nice structure of like deterministic dynamics and stochastic policies, we can basically reduce our sampling to just sampling states rather than sampling both States and controls.",
            "And it turns out that the PI2 algorithm, the iterative authentical control algorithm that Evangelos talked about, and there was also talked about in the first talk today can be recovered as a special case by applying this algorithm to the case where you have a risk seeking objective.",
            "So rather than the expected cost you have expected exponentiated cost.",
            "And if it policy parametrization that corresponds to this kind of controller, fine system, ax plus BXU, where US parameterized as some function of X and Theta, you can show that the policy gradient updates that should arrive here exactly correspond to the updates that you get in the iterative by two algorithm.",
            "I.",
            "If you take the infinite horizon limit, you recover certain results derived by ammo in a previous paper where you basically use something called the policy gradient theorem to.",
            "To represent the policy gradient in an infinite Horizon average cost problem."
        ],
        [
            "So this is sort of just iterating what I said just now.",
            "Another additional advantage of this."
        ],
        [
            "Approach is that it lets you derive 2nd order information as well.",
            "So if you choose this log linear policy parametrization, you can define this.",
            "Function of your trajectory, which is basically just the features accumulated overtime and it turns out that the policy action has a very simple expression in terms of this.",
            "It's so you have your control dynamics and you define a new distribution by multiplying your control dynamics by the cost.",
            "OK, and you take the covariance of the features under this shifted dynamics.",
            "Well, minus the covariance under your control dynamics, and so if you imagine that the cost is sort of small in a in a very small region of the state space, and it blows up otherwise, then.",
            "Sort of intuitive that the covariance of of of of your random variable is going to be smaller.",
            "Dir under this distribution rather than this one.",
            "So in that case you would have a convex optimization problem."
        ],
        [
            "So in summary, I I presented a sort of extension to the theory of linear solid control, and I talked about some efficient policy gradient methods."
        ],
        [
            "For future work, interesting questions are how do you solve the Bellman equation using some kind of approximation scheme for nonlinear systems?",
            "And there is also an interesting theoretical question of like what's the most general class of linearly solvable control problems.",
            "And I also plan to investigate the conditions under which you can actually so show that this policy gradient method is convex."
        ],
        [
            "So these are some references for my work.",
            "Thanks for listening.",
            "Can take any questions.",
            "Is this nice and observation?",
            "Essentially gives some global.",
            "The reason for using it?",
            "I wonder if there is any other what is the form of the large deviation title?",
            "Guarantee for the rating of urgencies.",
            "I am not aware of.",
            "Include some sort of coding theorems within.",
            "OK, so no, not feel assumption for the early OK without this summer finally today.",
            "I see I. Yeah, I don't.",
            "I don't know about a loss of vision.",
            "Go further anyways.",
            "Miracle.",
            "OK, but this thing goes Peter again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll be talking about couple of projects that I worked on as extensions to Mo theory of linearly solvable MDP's and the idea is just to sort of provide a high level overview of the results.",
                    "label": 0
                },
                {
                    "sent": "I won't be really going into the details, but we can obviously talk later if someone is interested.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first I'll be talking about an extension of linearly solvable MDP's to the game theoretic setting where you have an adversary in addition to the controller.",
                    "label": 0
                },
                {
                    "sent": "So I'll describe certain properties of these games and illustrate the effect of the adversary through some simple numerical examples.",
                    "label": 0
                },
                {
                    "sent": "Then I'll talk a little bit about policy gradients in linearly solvable MDP's.",
                    "label": 1
                },
                {
                    "sent": "So this is basically very similar to the idea of Authentical control, where you have some parameterization of your policy, and you want you, you simulate your system with these policy para meters an just based on these simulations you want to compute some iterative improvements to your policy, so that's what that part will be apart.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me begin by introducing Markov games.",
                    "label": 1
                },
                {
                    "sent": "These are also known as stochastic games.",
                    "label": 0
                },
                {
                    "sent": "There in the context of control, they are generally used in a setting where you have some kind of model uncertainty, so you have some uncertain para meters in your dynamics.",
                    "label": 0
                },
                {
                    "sent": "There's something about your dynamics that you don't quite understand.",
                    "label": 0
                },
                {
                    "sent": "And so one convenient way to model these is as some kind of adversary.",
                    "label": 0
                },
                {
                    "sent": "And the adversary can can use control to affect your system, the same way that you controller can.",
                    "label": 0
                },
                {
                    "sent": "And usually the adversary is.",
                    "label": 0
                },
                {
                    "sent": "Controls are chosen so as to maximize costs.",
                    "label": 1
                },
                {
                    "sent": "So mathematically, you can treat this as a 0 sum, 2 player 0 sum game where you have cost function that depends on your state.",
                    "label": 0
                },
                {
                    "sent": "The controllers control input and the adversarial control input.",
                    "label": 0
                },
                {
                    "sent": "And like in standard control problems this is accumulated over some time horizon.",
                    "label": 0
                },
                {
                    "sent": "You have stochastic dynamics that are function of both control inputs.",
                    "label": 0
                },
                {
                    "sent": "And so the controller is trying to minimize these costs while the adversary is trying to maximize them.",
                    "label": 0
                },
                {
                    "sent": "And shapely in the early 1950s came up with the solution to this problem, and the solution is through dynamic programming type recursion, very similar to the Bellman equation, except that you have this additional Max term over the adversarial input.",
                    "label": 1
                },
                {
                    "sent": "And in games of this form, you say that you have a saddle point equilibrium.",
                    "label": 0
                },
                {
                    "sent": "If first of all a solution exists and the solution doesn't change if you interchange the min and Max.",
                    "label": 0
                },
                {
                    "sent": "Can I ask questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah, why would it be more logical or under what circumstances would be more logical to assume that the unknown dynamics are your enemy rather than say some average of what they are?",
                    "label": 0
                },
                {
                    "sent": "I mean, I would have thought it was more natural to replace them by some expected, not probably well.",
                    "label": 0
                },
                {
                    "sent": "I mean in the context of robust control, I thought that's what's done.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, like you have some some unmodeled dynamics and you want your controller to work despite the worst case.",
                    "label": 0
                },
                {
                    "sent": "In in that.",
                    "label": 0
                },
                {
                    "sent": "Smart idea OK I see.",
                    "label": 0
                },
                {
                    "sent": "Well yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't have very good answer to that.",
                    "label": 0
                },
                {
                    "sent": "What happened here about that?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I. I'm not really sure.",
                    "label": 0
                },
                {
                    "sent": "I guess the idea is just to be sort of paranoid here.",
                    "label": 0
                },
                {
                    "sent": "You want to be robust to the worst case.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I like linear equations and linear linearly solvable MVP's are great that way because you can transform the Bellman equation to a linear equation through this exponential transformation.",
                    "label": 1
                },
                {
                    "sent": "So you have in standard linearly solvable MPs, you have a state costan.",
                    "label": 0
                },
                {
                    "sent": "This scale divergences between the control and control dynamics, and this cost structure allows you to make certain manipulations to the Bellman equation that make it linear.",
                    "label": 0
                },
                {
                    "sent": "So if you sort of try try to naively extend this to the game theoretic setting.",
                    "label": 0
                },
                {
                    "sent": "And you just plug in cable costs for both the controller and the adversary.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the resulting shapely equation is is not linear anymore.",
                    "label": 0
                },
                {
                    "sent": "So the motivation for this project was how do you generalize this cost structure to the game theoretic setting so that you still get a linearly solvable Bellman equation so?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for that you need to introduce this concept of any divergences.",
                    "label": 0
                },
                {
                    "sent": "Renny came up with these in the early 1960s by basically looking at the axiomatic characterization of Shannon entropy and relaxing one of the assumptions, and this allows you to define a more general class of empty entropies.",
                    "label": 0
                },
                {
                    "sent": "That includes there any entropies as a special case.",
                    "label": 0
                },
                {
                    "sent": "So, so the definition is given here.",
                    "label": 0
                },
                {
                    "sent": "It sort of looks like the KL divergent, where instead of the log you have P or Q to the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And you can recover the KL Divergent as a special case.",
                    "label": 0
                },
                {
                    "sent": "In the limit where Alpha goes to one.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So certain properties of these divergences they are known to be non negative.",
                    "label": 0
                },
                {
                    "sent": "If Alpha is greater than zero, then there's zero forcing in the sense that they are equal to 0 only when the two arguments are equal, they are increasing in the argument Alpha and also continuous in Alpha.",
                    "label": 0
                },
                {
                    "sent": "So here is a plot of the divergent versus Alpha for a particular set of distributions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So here is the structure of these linearly solvable Markov games.",
                    "label": 0
                },
                {
                    "sent": "You start with your system in a state X at at a certain time T. If neither the controller nor the adversary intervened, you would have had hypothetically transition to a new state based on this uncontrolled or passive dynamics.",
                    "label": 1
                },
                {
                    "sent": "But in actually the controller and the adversary, both of them are going to intervene so.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that for this structure to work out.",
                    "label": 0
                },
                {
                    "sent": "The dynamics is such that the what the adversary chooses is actually what happens is what the controller chooses is actually what happens.",
                    "label": 0
                },
                {
                    "sent": "So if you remember from most AKA couple of days back in these sort of problems you need to work with controls as probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So when you when I say you see is the control input, here it's actually the distribution resulting from a particular control input.",
                    "label": 0
                },
                {
                    "sent": "And So what happens is that the controller and the adversary pick their respective distributions.",
                    "label": 0
                },
                {
                    "sent": "What ultimately happens is is governed just by what the controller pics, but the adversary still has an effect on the problem through through this cost function.",
                    "label": 0
                },
                {
                    "sent": "So the natural way to think about this process is you have some uncontrolled dynamics.",
                    "label": 0
                },
                {
                    "sent": "The controller does some intervention to change it, so the adversary does some intervention to change it to some other dynamics.",
                    "label": 0
                },
                {
                    "sent": "And the controller further does some other change right?",
                    "label": 0
                },
                {
                    "sent": "And?",
                    "label": 0
                },
                {
                    "sent": "And there's a control cost associated with each of these transformations, which is proportional to the divergences between the change dynamics and the original dynamics.",
                    "label": 0
                },
                {
                    "sent": "So here you have this rainy divergance between the uncontrolled dynamics and the dynamics picked by the adversary, and then further on top of that you have a control cost for the controller, which is the divergent between what the controller pics and what the adversary picks.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that with this particular cost structure.",
                    "label": 0
                },
                {
                    "sent": "You can do some manipulations to the shapely equation that.",
                    "label": 0
                },
                {
                    "sent": "Sorry about that.",
                    "label": 0
                },
                {
                    "sent": "So you can do some manipulations to the shapely equation that let it become linearly solvable again.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that as you can see here, you'll need some restrictions on Alpha in order to guarantee a solution.",
                    "label": 0
                },
                {
                    "sent": "So if the solution exists, you can.",
                    "label": 0
                },
                {
                    "sent": "You can solve the problem for any Alpha and it still has a meaningful solution, but you can only guarantee that the solution exists for Alpha between zero and.",
                    "label": 0
                },
                {
                    "sent": "Is against every game in which EU and the nature is playing a game over choose probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah.",
                    "label": 0
                },
                {
                    "sent": "So there's there's some natural dynamics.",
                    "label": 0
                },
                {
                    "sent": "The adversary pick something else and the controller picks some further change about that, and both the controller and the adversary are penalized for the change they cost from what was there before.",
                    "label": 0
                },
                {
                    "sent": "Right, but everything is expected values.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, it's not expected.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So, so here's the main result.",
                    "label": 0
                },
                {
                    "sent": "So when Alpha lies between zero and one, you can guarantee that linearly solver Markov game has a saddle point equilibrium and the solution is given by the following linear Bellman equation.",
                    "label": 1
                },
                {
                    "sent": "So the only difference between this cennamo standard case of expected costs is the Alpha showing up here.",
                    "label": 1
                },
                {
                    "sent": "So as such the Bellman equation has a very similar structure.",
                    "label": 0
                },
                {
                    "sent": "You can solve it using similar methods.",
                    "label": 0
                },
                {
                    "sent": "But the control law is is defined in in terms of the Z scale to the one minus Alpha, which is which is again different from.",
                    "label": 0
                },
                {
                    "sent": "The standard linearly solvent control.",
                    "label": 1
                },
                {
                    "sent": "So you can see that this framework has the property that as far as Alpha increases, the adversary gets stronger.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go back to this previous equation I have there any divergent of the order one over Alpha and as Alpha increases this we know that it's monotone in Alpha in the order.",
                    "label": 0
                },
                {
                    "sent": "So basically the control costs for the adversary decreases as Alpha increases and in the limit Alpha goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "This control cost becomes high enough that it's always optimal for the adversary to not change the dynamics.",
                    "label": 0
                },
                {
                    "sent": "So you go back to the standard setting.",
                    "label": 0
                },
                {
                    "sent": "Here in this case for you to place to casting.",
                    "label": 0
                },
                {
                    "sent": "Policy.",
                    "label": 0
                },
                {
                    "sent": "Why is that?",
                    "label": 0
                },
                {
                    "sent": "Deterministic, but I'm not in system.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't expect the controller to be random.",
                    "label": 0
                },
                {
                    "sent": "But but there's there's an adversary, right?",
                    "label": 0
                },
                {
                    "sent": "I understand, but just think about driving your car or something like that.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't expect the driver to use a stochastic policy in a deterministic situation.",
                    "label": 0
                },
                {
                    "sent": "Well think, think, think.",
                    "label": 0
                },
                {
                    "sent": "Think of you being chased by a copper or a guy firing a gun.",
                    "label": 0
                },
                {
                    "sent": "You were, you would wonder like make your policy stochastic so that he can't guess what you're doing, OK?",
                    "label": 0
                },
                {
                    "sent": "You didn't really have a priority probability distribution on the cop or the gunfire.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you know that there's someone for system, you have family of systems.",
                    "label": 0
                },
                {
                    "sent": "You said you had a system.",
                    "label": 0
                },
                {
                    "sent": "No, I said I had a system with parametric uncertainty in my model.",
                    "label": 0
                },
                {
                    "sent": "I insist, OK, OK, I see.",
                    "label": 0
                },
                {
                    "sent": "OK, well then this probably makes more sense for for the case where you have a family system, then you want a single policy that that works across all of them.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You restricted your offer free between zero and one, and the limit offer goes to one is a standard clear yes but but but basic.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The that the limit of Alpha goes to one will give you a Cal control cost for the adversary as well, but I mean the special case of linearly solver them DPS is obtained by getting rid of the adversary, so that is the limit where Alpha goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "Basically where the?",
                    "label": 0
                },
                {
                    "sent": "Sorry, I say I said which is the limit where Alpha goes to zero.",
                    "label": 0
                },
                {
                    "sent": "Yeah so so then this becomes the infinite order versions which is high enough that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so you.",
                    "label": 0
                },
                {
                    "sent": "I mean you can see that by plugging in Alpha equal to 0 here you get back the standard equations for the linearly solver.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here are some simple illustrations to illustrate the effect of the adversary or so the general effect of the adversary is, as you increase Alpha, your control policy is going to become more and more risk averse.",
                    "label": 0
                },
                {
                    "sent": "So consider this simple problem of driving your car up a Hill and you have two Hills here and you.",
                    "label": 0
                },
                {
                    "sent": "Your cost is basically monotonically decreasing as height increases, so you're trying to get up as far as high as possible.",
                    "label": 0
                },
                {
                    "sent": "So there are two Hills here and the first one is like really narrow and thin and the second one is a little shorter, but it's a little broader.",
                    "label": 0
                },
                {
                    "sent": "And so you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And see that as Alpha changes between being risk and negative values where which corresponds to actually a cooperative game.",
                    "label": 0
                },
                {
                    "sent": "You have when Alpha is negative, you basically choose the risky solution an when A0 you sort of player bets evenly between both of them and well, first positive you you focus on the risk of our solution.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Similar problem here.",
                    "label": 0
                },
                {
                    "sent": "This is actually similar to birds example of the drunken spider.",
                    "label": 0
                },
                {
                    "sent": "So you have a start state and a goal state and you want to get get there as quickly as possible, but you have these obstacles in between and if you get pushed into one of the obstacles you incur a very high cost so.",
                    "label": 0
                },
                {
                    "sent": "So what you will end up.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing is that when when when you're not sufficiently discovers, you tend to choose a solution that goes straight through the middle, but when the risk aversion is increases to a sufficient value, you have some kind of phase transition in this behavior and you choose paths that go around.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the final example of a simple system with an unstable equilibrium at the origin.",
                    "label": 0
                },
                {
                    "sent": "And your goal is to sort of push to keep the system at unstable equilibrium.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here are the control loss for three different values of the risk sensitive perimetre.",
                    "label": 0
                },
                {
                    "sent": "And so when you are not sufficiently discovers, you basically employ very strong controls and you effectively cancel out the natural dynamics of the system.",
                    "label": 0
                },
                {
                    "sent": "But as you become more and more risk averse, you tend to play it safe and you trade off performance for lower control gains.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How much time do I have left?",
                    "label": 0
                },
                {
                    "sent": "I think 5 six minutes OK?",
                    "label": 0
                },
                {
                    "sent": "So I'll talk a little bit about policy gradients and this idea of computing iterative improvements to your control strategy, given simulations of the system.",
                    "label": 0
                },
                {
                    "sent": "So this sort of concludes the previous part of the talk, and now I'm going back to the standard expected cost case without without the adversary.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So the set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "King is the following, so you can.",
                    "label": 0
                },
                {
                    "sent": "This is sort of an alternative way of thinking of these learners.",
                    "label": 0
                },
                {
                    "sent": "Solvable MDP's you can think of yourself as having a deterministic system with a stochastic policy, which gives you the probability of actions given your state.",
                    "label": 0
                },
                {
                    "sent": "If you assume that your dynamics are such that they are invertible.",
                    "label": 0
                },
                {
                    "sent": "So given the current state and the next state, you can infer the controls that were applied.",
                    "label": 0
                },
                {
                    "sent": "Then you can re parameterized your policy simply in terms of probability transition matrix from state X to state X prime.",
                    "label": 0
                },
                {
                    "sent": "And you have this cost function which I allow to depend both on the current state in the next state and plus the KL divergent's cost.",
                    "label": 0
                },
                {
                    "sent": "So it turns out.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But for this class of problems.",
                    "label": 0
                },
                {
                    "sent": "The policy gradient has a nice natural interpretation, so if I consider a trajectory XI can compute the probability of that trajectory under my policy and.",
                    "label": 0
                },
                {
                    "sent": "And then I have state cost plus the KL divergent control cost.",
                    "label": 1
                },
                {
                    "sent": "So this is my expected cost function and if I just simply differentiate this with respect to my policy para meters theater, I get this sort of simple expression for the policy gradient.",
                    "label": 0
                },
                {
                    "sent": "And as with the previous talk and evangelists talk yesterday, this is expressed as an expectation over your current policy and so you can basically simply do rollouts of your system under your current policy, compute estimates of this and average to get an unbiased estimate of your gradient.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this algorithm can actually actually includes a lot of the previous algorithms as.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actual cases, so this approach was actually pioneered by Williams in 1992, where he used the reinforce algorithm.",
                    "label": 0
                },
                {
                    "sent": "But in our case, because we have this nice structure of like deterministic dynamics and stochastic policies, we can basically reduce our sampling to just sampling states rather than sampling both States and controls.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that the PI2 algorithm, the iterative authentical control algorithm that Evangelos talked about, and there was also talked about in the first talk today can be recovered as a special case by applying this algorithm to the case where you have a risk seeking objective.",
                    "label": 0
                },
                {
                    "sent": "So rather than the expected cost you have expected exponentiated cost.",
                    "label": 0
                },
                {
                    "sent": "And if it policy parametrization that corresponds to this kind of controller, fine system, ax plus BXU, where US parameterized as some function of X and Theta, you can show that the policy gradient updates that should arrive here exactly correspond to the updates that you get in the iterative by two algorithm.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "If you take the infinite horizon limit, you recover certain results derived by ammo in a previous paper where you basically use something called the policy gradient theorem to.",
                    "label": 0
                },
                {
                    "sent": "To represent the policy gradient in an infinite Horizon average cost problem.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is sort of just iterating what I said just now.",
                    "label": 0
                },
                {
                    "sent": "Another additional advantage of this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach is that it lets you derive 2nd order information as well.",
                    "label": 0
                },
                {
                    "sent": "So if you choose this log linear policy parametrization, you can define this.",
                    "label": 0
                },
                {
                    "sent": "Function of your trajectory, which is basically just the features accumulated overtime and it turns out that the policy action has a very simple expression in terms of this.",
                    "label": 0
                },
                {
                    "sent": "It's so you have your control dynamics and you define a new distribution by multiplying your control dynamics by the cost.",
                    "label": 0
                },
                {
                    "sent": "OK, and you take the covariance of the features under this shifted dynamics.",
                    "label": 0
                },
                {
                    "sent": "Well, minus the covariance under your control dynamics, and so if you imagine that the cost is sort of small in a in a very small region of the state space, and it blows up otherwise, then.",
                    "label": 0
                },
                {
                    "sent": "Sort of intuitive that the covariance of of of of your random variable is going to be smaller.",
                    "label": 0
                },
                {
                    "sent": "Dir under this distribution rather than this one.",
                    "label": 0
                },
                {
                    "sent": "So in that case you would have a convex optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary, I I presented a sort of extension to the theory of linear solid control, and I talked about some efficient policy gradient methods.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For future work, interesting questions are how do you solve the Bellman equation using some kind of approximation scheme for nonlinear systems?",
                    "label": 1
                },
                {
                    "sent": "And there is also an interesting theoretical question of like what's the most general class of linearly solvable control problems.",
                    "label": 1
                },
                {
                    "sent": "And I also plan to investigate the conditions under which you can actually so show that this policy gradient method is convex.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some references for my work.",
                    "label": 0
                },
                {
                    "sent": "Thanks for listening.",
                    "label": 0
                },
                {
                    "sent": "Can take any questions.",
                    "label": 0
                },
                {
                    "sent": "Is this nice and observation?",
                    "label": 0
                },
                {
                    "sent": "Essentially gives some global.",
                    "label": 0
                },
                {
                    "sent": "The reason for using it?",
                    "label": 0
                },
                {
                    "sent": "I wonder if there is any other what is the form of the large deviation title?",
                    "label": 0
                },
                {
                    "sent": "Guarantee for the rating of urgencies.",
                    "label": 0
                },
                {
                    "sent": "I am not aware of.",
                    "label": 0
                },
                {
                    "sent": "Include some sort of coding theorems within.",
                    "label": 0
                },
                {
                    "sent": "OK, so no, not feel assumption for the early OK without this summer finally today.",
                    "label": 0
                },
                {
                    "sent": "I see I. Yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't know about a loss of vision.",
                    "label": 0
                },
                {
                    "sent": "Go further anyways.",
                    "label": 0
                },
                {
                    "sent": "Miracle.",
                    "label": 0
                },
                {
                    "sent": "OK, but this thing goes Peter again.",
                    "label": 0
                }
            ]
        }
    }
}