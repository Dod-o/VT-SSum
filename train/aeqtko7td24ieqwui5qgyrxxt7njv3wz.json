{
    "id": "aeqtko7td24ieqwui5qgyrxxt7njv3wz",
    "title": "Automatic Differentiation",
    "info": {
        "author": [
            "Matthew James Johnson, Google, Inc."
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/",
    "segmentation": [
        [
            "Great well thanks Graham for the introduction.",
            "It's really great to be here.",
            "It's an honor to speak to all of you and learn so many awesome things.",
            "I talked about automatic differentiation.",
            "This is more of a.",
            "This is a tutorial talk.",
            "I think it's very important for all of us to like improve our understanding of how automatic differentiation works because not only do we use it all the time and it's important understand our tools, but I think actually in some cases were sort of held back by our tools.",
            "There have been some grumblings today about like, oh maybe, some things are easy to implement in the software that we have now.",
            "Other things are quite difficult.",
            "Maybe learning to learn type things and so you know.",
            "To an extent, the better we can make our software tools for things like automatic differentiation, the more we'll be able to free our minds to think of crazy new research ideas and try out new new things.",
            "So I'm going to be talking about.",
            "So my understanding of Autodesk comes from working on a software package called Autograde started by Google Mclaurin, who is a PhD student.",
            "When I was a postdoc at Harvard and then he was joined by me and David were both postdocs with Ryan Adams.",
            "So a lot of my knowledge of auto diff comes from these people and working with them and learning from them.",
            "So anything super clever you see here probably came from one of them.",
            "Not for me, alright?"
        ],
        [
            "So.",
            "We are in an awesome world right now in terms of the power that we have are at our fingertips because of our software tools, so I'm sure many of you have used at least one.",
            "Maybe use multiple of these frameworks.",
            "This tensor flow theano.",
            "They're also more recent software packages like Pytorch and Mimpi.",
            "These are really amazing because they let us write down essentially a forward model specification for our neural network, or even maybe our probabilistic variational inference problem and then automatic differentiation an oftentimes even optimization and some kinds of inference are done essentially automatically for us.",
            "So it means that we don't have to think so hard about, you know, just have an idea and then spend 90% of our time implementing it.",
            "So this is an amazing time to be working in machine learning I think.",
            "However, there's still some things that these tools don't solve as well as maybe they could.",
            "So in particular, things like loops branching so you're like recursion or lexical closures, data structures like lists and dicts, and this sort of thing.",
            "To varying degrees, these tools are maybe not so good at supporting all of these like language features that we use when we write other kinds of programs.",
            "Also, it can be quite hard to debug these things, especially if you're sort of building things ahead of time.",
            "You know we don't get to use the same sort of debugger tools that we used to, and we write other programs.",
            "There's also in for some of these second compiler or interpreter that we have to learn how to satisfy and go ahead and help that as sort of new mini language to learn on top of what we already know.",
            "So I think that these things solve, you know.",
            "These tools are getting better at solving some of these problems, but I think they are still not quite there in fact."
        ],
        [
            "So I want to tell you about a package.",
            "The package that we've worked on Autograde, which is just an automatic differentiation package.",
            "And it's sort of, I think, provides a good example of how to implement automatic differentiation so that it doesn't get in the way.",
            "In particular, it differentiates native Python code, so you can just write Python in the same way you always do with essentially all of the Python language available to you.",
            "That means that you can use NUM, PY, and SCI py.",
            "You can also write code with recursion and lexical closures.",
            "All these nice things, data structures.",
            "It's fully closed under its own operation.",
            "And so you know, thinking about differentiating a program that might have differentiating, going on inside of it.",
            "Like perhaps you're you have an optimizer that's differentiating a loss function and you want to differentiate with respect to some learning hyperparameters.",
            "That entire procedure?",
            "No problem, alright, we'll do it.",
            "An IT sort of won't.",
            "It won't bug you about it.",
            "It's also nice because there's essentially one function API, I really just there's a grad function that's all you need to know.",
            "If you already know NUM, PY and sci py, that's sort of all you need.",
            "And in some ways that might become more clear later.",
            "It's actually quite a small pure Python system, which means that it's easy to extend.",
            "Its easy to if you want to research in automatic differentiation or extend the system somehow.",
            "Perhaps you want to make it include wrap some other numerical libraries will become more clear later.",
            "Hi autograph I think makes it quite easy to do those things, so I'm going to be basing a lot of this talk on autograde.",
            "It's going to be about auto diff in general and so hopefully by the end of this talk you'll know not only how autograde works, but you'll sort of be able to walk out of this room and implement your own auto diff library that has all of these awesome features.",
            "One last point, I guess it's sort of interesting.",
            "I think some of the tools that have come out recently, like Pytorch and Mimpi.",
            "They are they have some of the support for some of these nice features.",
            "Their design of their auto diff system is actually based on autographs design, so that's why you know, there's Pytorch dot autograde.",
            "That's why it's called Auto grad.",
            "It's sort of inherited.",
            "The design from a torch autograde, which was a port of this Python code to the torch language and system.",
            "So I think Google has had a big impact even though you know we don't.",
            "You know?",
            "Maybe not all of us use autograph, but I think Google's work has been really impactful here, so."
        ],
        [
            "Going on, here's an example of what autograde code looks like.",
            "Who in this room is used Auto Grad Python library autograph?",
            "Oh great.",
            "So I'm telling you something new, so here's an example of what a grade code looks like.",
            "In fact, it's just Python code.",
            "There's almost no auto graphic in here.",
            "We're just writing NUM py, right?",
            "So this is how you might implement some kind of MLP right.",
            "We just have a prediction function in a way of scoring it against targets.",
            "The prediction function just loop through some weights and biases.",
            "Applying, you know Matt, moles and nonlinearities.",
            "This is the only place where Autograde enters.",
            "Is this grad function?",
            "And basically my claim to use that you can use lists and dicts and closures is not a lot of fancy stuff going on in this example you just use grad and it's a higher order function.",
            "You give it a function Python callable and it gives you back another Python callable which evaluates the gradient of this function, in this case with respect to the first positional arguments.",
            "But you can give it different argument numbers or.",
            "Names."
        ],
        [
            "So as I said, it's closed under its own operation, which means that you can do as many grads as you like, so this example might seem a little contrived, but actually I found it extremely useful in my work, not just like going that extra mile and not ever having to think if I do another grad on this thing.",
            "Is the TF while loop going to break or something?",
            "Or is there going to be some strange error?",
            "Some educates you don't have to think about that with autograde, so if you're doing some higher order optimization, maybe with some like Hessian vector products.",
            "Or something really complicated inside, and then you want to do learning to learn.",
            "On top of that, or something to optimize your hyperparameters using gradient based search.",
            "You don't worry about it."
        ],
        [
            "Just briefly, here's some more examples of sort of using autograde.",
            "Here's the implementation of hash, and so I said there's a one function API grad, but actually you might even want Jacobian because Jacobian evaluates it, gives you back a full Jacobian matrix for a vector to vector function.",
            "Here's how we implement hash and its Jacobian Jacobian.",
            "Evaluate Hessian vector products more efficiently will become more clear.",
            "Why if you want passion vector product, this is a much more efficient thing to do.",
            "We just sort of use higher order functions in Python And super easy.",
            "Write down more on this at the end of the talk.",
            "If we get to it."
        ],
        [
            "Here's just one last example.",
            "This is a tweet from Ryan on Black box variational inference.",
            "So what this is doing is essentially sort of variational inference where we're optimizing a Gaussian approximation against some target log probability.",
            "So this is essentially building the elbow.",
            "The evidence lower bound that we're going to optimize.",
            "There's an entropy term.",
            "And then here's an average energy term.",
            "And then we just call grad on it.",
            "The interesting thing here is that this is going to sort of do the reparameterization trick for you, right?",
            "It's just the fact that we are sampling here.",
            "You just called Brad and it'll differentiate this unbiased estimator and give you an unbiased estimator of the gradient.",
            "So Ryan at.",
            "Other people were at Twitter for some time and so this is actually how they do their code checkins.",
            "They don't use GitHub, they actually have to use tweets, so this is an official Twitter check in."
        ],
        [
            "So that's a high level of autograde.",
            "Let me tell you the goals for the rest of this tutorial.",
            "So first the first part is going to be about math and just defining reverse mode and forward mode automatic differentiation.",
            "I guess I should say reverse and forward accumulation of partial derivatives and Jacobian information, so it's going to be math at first.",
            "That's going to tell us you know what are these things.",
            "What is reverse mode or backdrop in terms of math symbols.",
            "The second part is going to be about autographs implementation about carrying out some of the operations that we develop in the first part, and in fact I'm going to show you all of it.",
            "You will like literally be able to understand all of autographs QR code after seeing this because it is short.",
            "I'll be able to go through it pretty quickly.",
            "And then finally, I'm not sure how much time depending on questions.",
            "I'm not sure how far will get into this, but I have a bunch of advanced auto diff stuff that I think is interesting to think about, so we'll see if we get to the sort of things like checkpointing differentiating optimal sort of optimal optimizing points and like fixed points, these sorts of things doing this implicitly, I think there's a lot of exciting new stuff we can do with some of these techniques, so let's see if we can get there.",
            "Starting off with."
        ],
        [
            "Jacobian's, I'm going to set up some mathematical notation just so we're all on the same page.",
            "I'm going to be talking about evaluating the derivative of this function F, so it's from RN to R. Think of this as a loss function on your parameters in our end, so implicitly, there's training data inputs and targets, but that's not being tracked here, it's just F on the parameter vector to some loss value.",
            "That's how you should think of F. So you know, we can also specify F by naming its inputs and outputs, which is going to be very convenient for a lot of certain kind of derivative notation, so I'm going to talk about the input of F being X with this little bolding in the text, referring to the fact that it's a vector in RN, and then why is just a scalar output.",
            "So let's also say that F is a composition of several functions, so all of auto diff is really about function composition and differentiating composition of functions.",
            "So let's breakdown F into a composition of these four functions.",
            "And if we want to name the inputs and outputs, will say that.",
            "Why is F of X and then we can breakdown F as evaluating a DFC of B of A right?",
            "In fact, let's make some names for all the intermediate values as well.",
            "Those will be useful to refer to on the next slide, so in fact we can race.",
            "Imagine like this is a way to spell out the program if we read it from right to left, right?",
            "First, we're going to evaluate Avex into this variable called A.",
            "They're going to be to that and so on.",
            "Any questions on the notation?",
            "Grace so."
        ],
        [
            "Here's our breakdown of how we're evaluating F, and we want to evaluate the derivative so there's.",
            "The women's right to the derivative, at least for this slide, is as this Jacobian matrix.",
            "So we write F prime of X to refer to the function of X that evaluates the.",
            "The Jacobian matrix of F valuated X.",
            "We also use this other notation that looks kind of like fractions where we write partial Y partial X terms of the output.",
            "We're naming the output as well as the input and this is just a matrix of partial derivatives.",
            "In this case, it's just one row, right?",
            "This is a row vector or row matrix and that's because F has a scalar valued output, but a vector input.",
            "So using this decomposition of F as a composition of these four functions, as I'm sure you guys remember from calculus, we have this chain rule which tells us that we can actually write this Jacobian matrix as a product of these other Jacobian matrices.",
            "This sort of fraction notation is kind of handy because it reminds us we think of like kind of canceling.",
            "Intuitively, these fractions that we're getting partial Y partial X expanded out in terms of these intermediate.",
            "And to unpack the notation a little bit more.",
            "We can say that this partial I partial C is none other than the Jacobian matrix of D. That's a D prime evaluated at at sea.",
            "And so on.",
            "So what we're saying is that to get the derivative, this Jacobian matrix of F, which is a one by N matrix, we're going to write that out as a product of these four other matrices, right?",
            "Breaking this down in terms of the Jacobian matrices of our functions we're composing.",
            "And so we can say that this one by N matrix is a product of these other things, with with intermediate sizes in the middle.",
            "So for example, this guy right here.",
            "What is the size of this matrix?",
            "Can anyone tell me?",
            "So it's one by something.",
            "It's not 1 by N necessarily.",
            "It's actually one by whatever the size of C is right.",
            "It's one by the size of C, and this is the size of C by the size of B and the size of the the size of A and the size of a by this.",
            "Then that last one is NI think it was drawing things wrong when I was reading things out, but this is a.",
            "Let's see, this horizontal size should be N on this last one.",
            "So we have a product of these matrices, so you know if we had some way of evaluating these Jacobian matrices, we can.",
            "We can express the derivative thereafter in terms of them, we've reduced our problem.",
            "So we have to multiply these matrices.",
            "Does it matter what order we multiply them in?",
            "Yes, what you know which order would we multiply this in?",
            "Do you think?",
            "Yeah, look there.",
            "Problem of the next matrix chain ordering problem.",
            "That's right, yeah.",
            "Yeah, So what you're talking about is if I have a sequence of matrices and I want to draw parentheses around them so as to say how in what order should I perform the multiplication to minimize the number of flops, or perhaps minimize the memory usage?",
            "There's a dynamic program that can actually solve that on chains, but in fact, even though this example had.",
            "You know this composition was sort of just a chain graph that came out.",
            "In general, we might compose functions in a much more complicated way where you can imagine the composition structure is better represented as a DAG of some kind, and it turns out it turns out that the optimal way to multiply matrices in that form, the ultimate the optimal way to accumulate jacobian's, is actually, I think in NP hard problem in that case.",
            "So there's a dynamic program on the chain, but not for the general problem.",
            "So let's just talk about two simple.",
            "Like 2 extreme cases."
        ],
        [
            "Of how to associate these things the 1st way would be to write all the parentheses, sort of nested on the right.",
            "OK, so the first thing we're going to play, or those two matrices there were on the far right.",
            "That means that we're going to get intermediate results that are of size like this.",
            "So for example, this thing is size of B by size of X, so all of our intermediate values right?",
            "Are these big matrices.",
            "So this is called forward accumulation.",
            "Another way to associate these matrices.",
            "Again, these are two extremes of what we could do.",
            "We could try to optimize this in a more general way.",
            "These are two extremes.",
            "Another way we can associate multiplying out these matrices is sort of from the left and interesting thing, because we had the scalar valued output.",
            "Remember we have the one on the far side of our our matrices like the thing on the far left with just a row vector.",
            "It turns out that these intermediate products are actually themselves just one by intermediate size.",
            "Jacobian's this is called reverse accumulation.",
            "So you can see at least just by looking at the size of the intermediate that we have to accumulate that it seems like for functions that go from RN for some large N, maybe in the millions to R, right, a scalar valued output certainly looks like accumulating, just having to do matrix vector products.",
            "Or I should say vector matrix products all the time and only having to store these vectors seems much more efficient.",
            "If we were evaluating the derivative of a function.",
            "That went from R to RN like maybe it's parameterising a curve in some high dimension space.",
            "Maybe we then we'd want to forward accumulation, but because we had a large vector input and just a scalar output, this reverse accumulation looking much better."
        ],
        [
            "Let me give you another way to describe the exact same thing, and that's just going to be sticking a vector on the end.",
            "So what if I had asked you instead of give me the Jacobian matrix of this thing, give me the Jacobian multiplied by some vector that I will give to you, right?",
            "So Jacobian vector product?",
            "Then if we think of evaluating the Jacobian vector product, we can just write associate and it doesn't matter the size of why, because we're going to have this vector on the end here, so all these intermediate values are going to be vectors themselves, so this is sort of, you know, when we have a vector on the right and we're trying to evaluate a Jacobian vector product forward accumulation makes sense.",
            "In fact, it doesn't matter what the size of the output is in this case.",
            "So I think the best way to think about forward accumulation.",
            "You should associate that in your mind as evaluating Jacobian vector product's OK. Another way to think about this is how would you build an entire Jacobian matrix, right?",
            "If you had access?",
            "If I gave you a function I said oh this thing, you give it X&V and it will give you a Jacobian vector product out and you were like, well, I want the entire Jacobian.",
            "Right would you do well?",
            "You could just feed in a basis.",
            "You could feed in A1 hot basis for V, right?",
            "If you give it the vector 100 you call this function that gives you coming back to product that will reveal to you the first column of the Jacobian, right?",
            "And so on, you give it the next one hot vector.",
            "It will give you the next column.",
            "So another thing to think of when you think of forward mode is that if you wanted to build an entire Jacobian matrix Ford mode, which lets you build it one column vector at a time.",
            "If you had these Jacobian vector product as your primitive.",
            "And then you know another.",
            "Another way to think of this is, you know, if we had an entire matrix, is an identity matrix of just stuck on the end here, right?",
            "There's one hot vectors and we're applying this.",
            "You should think of this as applying to the column vectors in this matrix, so we can write an entire Jacobian matrix by sort of taking basis and then pushing it forward.",
            "We can think of 1 column vector at a time, so forward mode, Jacobian vector, product's build Jacobian matrices, one column at a time.",
            "So."
        ],
        [
            "How about on the other side?",
            "One?",
            "If I asked for a vector Jacobian product, right?",
            "Really a vector transpose Jacobian, but I call this vector Jacobian product right?",
            "Then if we expand this out again, right?",
            "This is what this looks like now again, no matter what the size of the input of the output is makes a lot of sense to just do Association on the left right away, we can build up a vector Jacobian product for F is by evaluating all of these vector Jacobian products one at a time.",
            "So from the output to the input and this is, you know, I think the best way to think about reverse accumulation in the thing that reverse accumulation or reverse mode audit if does efficiently is it gives us access to vector Jacobian products.",
            "Essentially, you know by corresponding argument if you fed in one hot vectors to this vector Jacobian product function.",
            "This would let you build a Jacobian one row at a time.",
            "So you can see why we like reverse mode in the context of optimization and neural networks, right?",
            "It's because we always have these loss functions that go from huge from RN, where N is millions or maybe billions to R, right?",
            "Just like our ends are huge and that means our Jacobian is always going to be a one by N matrix.",
            "So if you have a method that can build the Jacobian one row at a time, in this case you're done in one shot, right?",
            "Or if you have a small number of outputs, you only have to call it a few times.",
            "But if you did forward mode, if you had to build a one by end Jacobian one column at a time, you're just getting one number for every single call.",
            "Another thing that I haven't quite spelled out, but.",
            "You know the high level?",
            "You should just think of each evaluation of a Jacobian, vector, product or vector Jacobian product as comparable to the cost of evaluating the function, at least in terms of FLOPS.",
            "OK, so somehow we have to pay the price if we're using reverse accumulation and we have a one by N Jacobian matrix that we're after, we just have to pay something that looks like the we have to evaluate the function, But then we just have to pay that same function evaluation cost once to get the entire Jacobian matrix because it's a one by hand.",
            "And as before, we can sort of if we like, think of what we were doing before is having a little identity matrix.",
            "In our case, for a scalar output functions one by one matrix or just expanding out the Jacobian by saying start this is like where we start the recursion at a one an identity matrix.",
            "Then we are applying this thinking of applying it one row at a time.",
            "OK, so."
        ],
        [
            "To summarize forward accumulation, I guess even bigger.",
            "Sorry the whole point of thinking about differentiating shooting this way is that we have these big composite functions which are built up out of compositions of similar functions, right?",
            "You write a complicated neural net, but you're building it out of tensor flow primitives, and even underneath that, if you if you wanted to, you could say those are all built out of plus and times right?",
            "I mean computers don't know how to do much more than plus and times so.",
            "High level, we're trying to breakdown evaluation of jacobian's into.",
            "You know, evaluating similar Jacobian, then accumulating them by multiplying those things together?",
            "Ford accumulation is an extreme where we sort of right associate everything you should think of that as giving us access to efficient Jacobian vector product.",
            "So there's some jargon here, which is that we think of like pushing forward.",
            "We sort of have a vector that sits at the input, right?",
            "It's like an input sized vector and we can push it forward by applying all these jacobian's these Jacobian vector products to it, and this builds Jacobian is 1 column at a time.",
            "If you give it one hot vectors.",
            "Where is reverse accumulation?",
            "You should think of vector Jacobian product.",
            "So really vector transpose Jacobian products.",
            "We think this is some jargon in differential geometry where you're pulling back because you sort of have a vector that's the size of your output and you pull it back to get sort of differential information at your input.",
            "If you apply this to build full Jacobian matrices by applying it to one vectors.",
            "Think of this building in Jacobian matrix, one row at a time.",
            "So some details that didn't cover non chain."
        ],
        [
            "In composition, the purpose of this slide is just to tell you that even though we're only talking about chains, it's actually quite straightforward to generalize it to non chains right where we compose functions and sort of we have fan out because we're going to use, we have an output of a function, some temporary variable, and we're going to give it to two different functions applied to.",
            "That's like fan out, or we can also have Fannin where we apply functions of many arguments, which I didn't show you before, so Fannin, that just means that you know we have some function F and it has two.",
            "Inputs this is not surprising that this shouldn't be too scary, because we can always just think of like oh, why not just stack those inputs, right?",
            "Like you say, it's two inputs, but I can always stack them and call them one input.",
            "That's exactly right.",
            "Another way to think of it is just you can unstack F and you can say I have a way of thinking about Jacobian matrices with respect to the first argument and separately with respect to the second argument.",
            "So sort of fan in if you like is handled by just thinking of having separate Jacobian matrices.",
            "Fan Out is a little more subtle, but I hope to convince you with a simple argument when you use when you have a variable you maybe computing along a chain and then you end up sending it to two different functions, right?",
            "So now you have this pan out in the function that you are trying to differentiate.",
            "That sort of looks like function G that takes an input X and just replicates it and gives 2 outputs.",
            "And really, this is a linear function, quite a simple one, right?",
            "It's just like taking this stacked identity matrix and applying to exit seems trivial, right?",
            "But the point of writing this out is that now we can think of what is the Jacobian of G. Well, it's a linear function, so it's really just this linear coefficient matrix, so it's just this guy.",
            "So what is a vector Jacobian product?",
            "So when we're doing back prop and we have to handle fan out, you know we're getting two gradients, so sort of speak two sets of partial derivative information from both of our outputs.",
            "What do we do then?",
            "We just sum them, right?",
            "This is saying the vector Jacobian prodigy is just this guy will just means some those two things, so handing fan out?",
            "Basically means that we're going to be coming as we accumulate.",
            "I derivatives during background.",
            "So.",
            "Any questions on?",
            "Non chain stuff will see this actually more concretely when we talk about the code, which is what we're going to do next."
        ],
        [
            "So that was jacobian's in the chain rule.",
            "That was the math part, yeah?",
            "This matrix view is nice.",
            "Is this unifying?",
            "But in practice, if you have a big graph.",
            "You will end up with huge matrices.",
            "Yeah, would be the best data structure for doing this, right?",
            "So we don't instantiate those matrices.",
            "That's a great point.",
            "So in fact, this is another advantage.",
            "Repeat the question for yeah."
        ],
        [
            "You so the question was like, actually, it seems like if you had to instantiate all these matrices, maybe these would be quite large, right?",
            "In fact, this is like the matrix of your number of inputs like your number parameters of billion by you know how many activations you have in your first output.",
            "So these matrices could be quite large.",
            "Maybe we shouldn't.",
            "You explicitly instantiate them.",
            "This is another advantage of thinking in terms of what we really want.",
            "Our Jacobian, vector product sore back prop.",
            "Jacobian products, that's to say that we don't actually have to instantiate this Jacobian matrix to evaluate the vector Jacobian product, right?",
            "As long as you write a function that evaluates the vector product, we're happy.",
            "So, for example, if you have an elementwise operation, so an elementwise 10 H, or something like that, right?",
            "If you think of the Jacobian matrix for an element wise operation, it's actually a diagonal matrix.",
            "You shouldn't instantiate that whole diagonal matrix, right?",
            "You can just apply it by doing some.",
            "Element wise operation.",
            "That's like applying that.",
            "Basically that's an argument to say that because this Jacobian is diagonal, we can implement the vector Jacobian product operator with an elementwise operation.",
            "So as we become more clear in the next section, we're not in our."
        ],
        [
            "Imitation going to build explicit matrices like explicit Jacobian matrices.",
            "Instead, we're just going to write functions that implement vector Jacobian product operators, and because these matrices are often extremely sparse, or perhaps diagonal and sort of thing, we can be much more efficient that way, both in terms of memory and in terms of compute time.",
            "So I'm going to tell you bout autograde's implementation and seriously, this will be everything you can follow along in the source code if you want, but."
        ],
        [
            "I'll be showing the source the relevant parts of the source on the slides, so.",
            "To do auto diff, we really have to audit is about tracking which of which primitive functions get applied to your input.",
            "And then using some definitions of how to evaluate Jacobian vector products or vector Jacobian products for each one of those primitive things.",
            "So there are basically 2.",
            "Or at least two extreme strategies for implementing audit if one of them is to read and generate source code ahead of time.",
            "So this might mean reading Python is not too hard, right?",
            "Python has an AST module, we have to read Python.",
            "It's very syntactically rich languages.",
            "A lot of features.",
            "And then maybe you could generate Python, right?",
            "What we seem to do more often is define another language that we then like sort of, use Python as a meta programming language.",
            "For right.",
            "We can also say create a computation graph language and will actually write our programs.",
            "The functions F that we want to differentiate in this computation graph language.",
            "This is what tensor flow is doing, right?",
            "So tensor flow is actually the way it does auto diff is.",
            "It looks here Tensorflow program, which is some dataflow representation, you know?",
            "Some graph representation and then it has a way of taking a graph program.",
            "And producing another graph program that evaluates the derivative.",
            "This is 1 implementation option.",
            "It has advantages and disadvantages.",
            "For example, if you have the entire program in a sort of more.",
            "Dataflow representation, maybe you can do more automatic transformations to it, right?",
            "This is why we have ahead of time languages and compilers, right?",
            "C++ compiler's can build your language into a dataflow representation and reason about it to optimize the program and generate source code.",
            "It's sort of knows more about what that program's behavior is going to be.",
            "Python, on the other hand, as a language, not just an implementation, is slow because every line might surprise you, like who knows what the next line is going to do you think it was calling DOT?",
            "But like in an earlier line, I rewrote dot to be like.",
            "Make a HTTP get request on some server or something, so Python is like very surprising so.",
            "Another way to implement audit if that is easier to support in higher level languages is to monitor function execution at runtime.",
            "So this strategy is basically saying instead of implementing something that can read and decide you know Python to the extent that we need to differentiate it.",
            "Instead, let's just do everything as late as possible, right?",
            "Let's not even try to build this graph or like reason about what primitive functions get applied to your input to produce the value of your function.",
            "Let's not reason about that until we're actually doing it at runtime, right?",
            "Let's actually let the Python interpreter let's see Python or whatever, implement Python And just monitor as it's executing what functions get applied to the input.",
            "And then we'll be able to support a lot of the cool features that I was talking about as well.",
            "Become clear.",
            "So I'm going to be talking about this implementation style, which is monitoring function execution or runtime.",
            "This is how autograph is implemented.",
            "This is also how like Py torch in admin PY essentially do things.",
            "So.",
            "I won't be talking too much more about the about how tensor flow works, but maybe if we have questions at the end, I can say a bit more, yeah?",
            "Tensor flow and Sumeet was also talking with Pytorch implementations.",
            "There talking with tape method.",
            "Yeah, Knology is this basically the same same thing?",
            "Yeah, that's right.",
            "So the question was when Smith was talking about the taping method, so I didn't see that talk when he was talking about about taping.",
            "Is this the same thing?",
            "And yes, it is so tape is a jargon from the literature.",
            "That means like fight instead said record function execution at runtime right?",
            "That would be like taping.",
            "That's where they got the term.",
            "I guess they didn't call it Laserdisc ING or something, or like Betamax ING, just because of whatever technology is available, so this stuff is.",
            "I should also say that these things aren't new.",
            "They've been known in the auditor for a long time, though there are some new elements I believe to autograde when it first came out that now exists in some other systems.",
            "I don't like the term tape for, like a very technical reason, but basically when you record the graph, the tape is like a topological sorting of that graph, but in general.",
            "You don't have to be constrained to one particular topological sorting, so I'm just saying more generally you just have to monitor function execution and build a graph data structure of some kind.",
            "So yeah, if it's pending on how many details soon went into, you may have told you a version of this already, but you know, after I give this version, I guess you can compare and see which one you."
        ],
        [
            "More so.",
            "I'm going to tell you auto autograph implementation in three parts.",
            "There are really just three ingredients and their ingredients I've already alluded to.",
            "So one basic thing we have to do is be able to trace the tape, the composition of primitive functions.",
            "That just means when you give me a function and you say I want to differentiate this with respect to input, when we get an input value and give that to the function we want to then record what operations get applied to it.",
            "So the first step is just racing, which doesn't on its own, necessarily have much to do with auto diff.",
            "The second is to define a vector Jacobian product operator, right?",
            "Not necessarily the Jacobian matrix, but some operator that implements a vegetarian product for each of these primitives, right?",
            "And then, once we have sort of a trace of what primitives were applied and a vector decoding product for every primitive, Now we have all the information we need to sort of do back prop to apply these vectors.",
            "Your product's in a sequence and pull back the derivative information from the output to the input.",
            "So first part tracing composition and primitive functions so."
        ],
        [
            "Function.",
            "Should really think of in this context is like a numerical kernel.",
            "OK, so with autograph because we liked working with NUM PY and SCI py, we wrapped empires are numerical kernel library.",
            "You could actually take any other numerical kernel library like pie or even Pytorch and wrap it in terms of autograde.",
            "And it will be able to trace its execution.",
            "Maybe I'll say more about that in a bit, but for now I'm just think of a primitive as a NUM py function, like NUM, py, some or sign or dot.",
            "So for each NUM py function we wrap it.",
            "In a primitive, what that means is that we.",
            "This is why at the top of your file you write import auto grad NUM PY as NP right?",
            "We have a thin wrapper for every of every one of those of those functions and what the purpose of the rapper is is it will see a boxed value coming in sort of node value and this node basically is corresponding to some value a those propagating through your computation.",
            "It has some extra information like what function it came from F and what its parents were.",
            "In this case, maybe just had one parent which is X.",
            "So here's all the data primitive does the first thing it does is it unboxes the value right?",
            "It gets this raw ND array a out of the boxing and it passes it to them by some function, right?",
            "This is to get the result of the forward computation that you're trying to implement.",
            "Then the other thing it does is it boxes up the output right?",
            "So it takes the input unboxes it gives it to the NUM py, primitive or whatever numerical kernel primitive.",
            "It gets the results, which is an RA ND array and then it boxes it up with the information that we need like, oh, by the way, we just apply to some right.",
            "This is like annotating a node in the graph with what function was applied, what the inputs were, and what the output was.",
            "So here is."
        ],
        [
            "The code to implement that of course there's some you know, some lines that I'm hiding, the core maybe 100 or 200 lines, but these are really the operational ones, so this is basically a struct, right?",
            "This is basically like saying.",
            "These are the nodes we're going to build our graph out of, sort of some type information, But basically this recipe is what contains that information I showed you on the previous slide about what function was just applied and what its parents were.",
            "And then the value is the value, so that's it for nodes."
        ],
        [
            "And then I said our primitive wrapper is really just has to unbox any box arguments, call the function and then box up the outputs.",
            "And that's exactly what this code is doing.",
            "So we have a class primitive and it's called method in Python.",
            "Whenever you call one of these things.",
            "Here's what it does.",
            "It just goes through the arguments.",
            "It says.",
            "Hey, if this node is an argument then unbox it right place in the Argylls list the actual value.",
            "So in optimization, if we have if we know ahead of time that this Jacobian is 0, but otherwise just append that to the parents, then this is self fund is the underlying NUM py function, right?",
            "So we unbox the values.",
            "Here we applied self dot fun to the unboxed values to get the result value, and then we're returning a new node for boxing up.",
            "That's it.",
            "There are a few things that I have hidden here which I will now reveal having to do with these progenitors, so this is actually something to do with higher order auto diff.",
            "Turns out that when you're doing higher order audit, if there could be many traces that are happening at once and in the very first version of AutoCAD if you get check out the original autograph like there's some Git command.",
            "I thought I had it on the slide.",
            "Look at the very first check in.",
            "It's a version that seems like it supports higher order audit.",
            "It's only 80 lines actually.",
            "But the way it does it is by sort of nesting the boxing of nodes, and it turns out that there are ways you can break that when you're doing crazy combinations of.",
            "Traces start and stop.",
            "So in fact, I think a better way to do this is we only ever have one graph and then we track these progenitors.",
            "The list of progenitors basically saying for any node why we can ask that node?",
            "Why were you boxed right?",
            "Someone started tracing someone box to value and pass it into a function.",
            "That's the progenitor, like the first thing that got boxed.",
            "Everything else is not a progenitor, and we can look at any node and say why were you boxed.",
            "And with that, let's us do is basically says when we have many iterations of this tracing going on, we can essentially pull out the relevant part Tenniel and trace just by taking sort of looking at the subset of nodes in the graph that have the right projector list.",
            "So it's an optimization that's a high level."
        ],
        [
            "Nothing.",
            "And then here's forward pass.",
            "So this is what we do to trace a function given a function and some Args, inquiries to evaluate it on and the argument number that we want to sort of trace what primitives get applied to that to that argument.",
            "This is all the code.",
            "We just make a new progenitor.",
            "That's like a new node, but then it just marks it as a as a progenitor.",
            "We plug that into argument list.",
            "And then we say you know where that's an active progenitor and then we just evaluate the function with that box value.",
            "So we sort of box up on value and then drop it into the function and then as it's calling NUM PY functions and other things, it's going to be building up this graph.",
            "And then when we get out of the value of the function of the end node, right?",
            "We sort of box the thing and then dropped it in.",
            "We don't know what's going on.",
            "It comes out and it's still a node, and we know that's the sort of start node in the end note of our of the graph of the function evaluated at these arguments.",
            "So."
        ],
        [
            "In pictures, what does this look like?",
            "We would start node X that we box up.",
            "We passed into a function, some user defined function may be the first thing it does, is it calls an umpire function that will call capital A here grace.",
            "Then maybe it prints to the screen and you know there's an.",
            "If some system calls, who knows and then, but the next thing that gets applied to a or any existing nodes is this B function right?",
            "And so on until we pop out of that forward pass and we get this end, no doubt.",
            "That's tracing, it's an entirely of that racing so."
        ],
        [
            "Again, I just drew for you a chain, but this tracing just works with tags, so we can drop in a function, and dropping an ode to a user function and then we get out of trace that might have this kind of DAG structure.",
            "An important thing to note.",
            "The difference between this kind of graph and a tensor flow graph is the tensor flow.",
            "Graphs have control flow in them, right?",
            "What would happen if there's a while loop in user code?",
            "With this implementation.",
            "Any thoughts, any hands?",
            "A loop in the graph, not quite.",
            "So I heard someone say it gets unrolled, right?",
            "So we're actually just tracking the printers that applied.",
            "We don't know what Python syntax you're using.",
            "You could be using exceptions for control flow and whatever.",
            "We don't care, we just care about what functions got applied to produce the output.",
            "So if you add a while loop or any control flow we don't see it.",
            "We just see the effect that that had.",
            "And I guess you know a key here is that we are going to redo this tracing for every Vijay P for every grad that we evaluate.",
            "So once we're done with this graph.",
            "This graph is just representing what functions got applied to your input.",
            "This one evaluation.",
            "But on the next evaluation we're going to produce another graph.",
            "Right, so that's what gives it flexibility."
        ],
        [
            "We don't have to.",
            "Also makes it easy so we don't have to think about differentiating control flow structures.",
            "OK, that was tracing.",
            "Let's move on to defining of Egypt for every primitive.",
            "This one is actually super easy."
        ],
        [
            "Just some review.",
            "The basic problem we have is we have some link in this graph, right?",
            "It looks like we had some temporary variable X, some input to a function and we got an output by applying A to it right and then sort of the recursive thing that we have to solve is we have some derivative information at the output of our function.",
            "That's like the derivative of our loss.",
            "The partial derivative arlos our final output with respect to this temporary a right?",
            "So we have that derivative information with respect to a the output.",
            "We want to pull that back to be derivative information with respect to X, right?",
            "How do we take a DYDA and make out of it DYDX?",
            "This is what we said before.",
            "This is what you know why we sort of use this fraction notation.",
            "What we need to do is take this vector and apply this vector Jacobian product operator.",
            "Alright, so this little guy sort of including the dot, this linear operator that we're applying to this vector.",
            "We're calling a vector Jacobian product operator, and that's the thing we have to know given sort of the partial of our final output with respect to a.",
            "That's the thing that we need to apply to pull that back to be a derivative are output respect to the input to this function.",
            "So this is sort of the recursive thing that we need to solve.",
            "This is Step 2."
        ],
        [
            "And that ingredient list that I told you so here's some examples of GPS in autograde.",
            "So for 10 inch for example, for the hyperbolic tangent, this thing is just saying hey given sort of G is the name for the vector, the derivative information vector that we're trying to pull back.",
            "And here is, by the way, the answer to the function that got applied and the input, which is just X.",
            "Here is the function that evaluates the vector Jacobian product operator right?",
            "It's just G divided by NP.",
            "Coach squared, right?",
            "So you can see this is not instantiating a Jacobian matrix, right?",
            "It's just saying oh to apply this.",
            "This is a diagonal Jacobian.",
            "Is a diagonal operator, and so I'm just going to do some elementwise stuff to G. Questions.",
            "We also have crazier things.",
            "It's really fun to look through the NUM py.",
            "Grads file.",
            "We have things like SVD, you know.",
            "Sorting, basically I think almost all of NUM py we have wrapped so you can use and call to your heart's content.",
            "And so we've defined these primitives for essentially every non function and some subset of sci-fi functions that we've used in our research.",
            "So."
        ],
        [
            "Those are the 1st two ingredients.",
            "Now we just have to compose the JPS backward which."
        ],
        [
            "Symbolically is really simple, right?",
            "So we have this trace.",
            "Of our function evaluation and we know that for every link we have a vector Jacobian product operator.",
            "So how do we pull starting from this partial derivative of the output which will just initialized to one which will be like partial Y partial lie?",
            "How do we pull that back all the way to the input when we do it in steps?",
            "First we want to use the VJP for the function D right to pull this guy back here.",
            "So we applied the BJP for this function and pulled that information back to get us partial partial see right?",
            "And then we apply the JP for C. Pull that back to be this derivative, and so on until we get the derivative of the spec to our input that we were after.",
            "So here's some."
        ],
        [
            "Cool higher order auto diff.",
            "Just works with the stuff I've already shown you arbitrary higher order audit if you don't need anything else.",
            "And the magic is that the backward pass, right when we're applying these Vijay PS because we defined the JPS right, these vector Jacobian products in terms of other primitives that autograph already knows about.",
            "That means that the VJP pass itself is just more applications of NUM py that we can trace."
        ],
        [
            "So for example, when we were doing this back before, maybe someone else is tracing us, right?",
            "Maybe there's some higher order audit if some Hessian vector products in a truncated Newton optimizer.",
            "So basically, while we were doing that backward pass, it could be that everytime were applying of JP, someone else was building a trace of what we were doing right?",
            "And this might you know these things might not be one to one.",
            "Maybe the VJP for the B function is quite complicated."
        ],
        [
            "But basically someone else could have an end node sitting out there and we don't know.",
            "So that's how higher order auto works.",
            "Basically, when you call Grada Brad, this is what's going on.",
            "Let me show."
        ],
        [
            "The code that implements this and then the grad function, which is what we're after, which can be written entirely in terms of what's on this slide and the next slide is grad, so here's a backward pass we had forward pass, which built the trace right, and then we said for every element in the trace we defined of JP.",
            "Now we just have to do that pullback along the graph.",
            "The details aren't too important.",
            "Let me just highlight a few things where the forward pass gave us the start node in the end.",
            "Node G is sort of the grading they were trying to pull back.",
            "Think of is just a one.",
            "For example, we're just doing gradients.",
            "So we have started node.",
            "This line is says for node in Topo sort.",
            "So this just says topless orthograph.",
            "This will be iterating it over, iterating over it in a valid order from sort of the output to the input.",
            "We just sort of grab.",
            "We have a Dictionary of the gradients keyed by nodes.",
            "The important thing is that we just grab sort of the sort of gradient information that's coming in that we want to apply the JP two.",
            "We unpack the recipe and we just say for every input argument to this function, apply the functions VJP with respect to that argument number.",
            "Right, so this is just going to go.",
            "It's going to traverse the graph in the top of sort and just apply all these.",
            "When we did FJP that's what it was setting up.",
            "There's something else you need here that I think other libraries like Pytorch probably don't do.",
            "But if you do indexing for example, into your arrays right?",
            "And you pop out these little sub arrays and pass them on this code actually does some nice like mutating assignment to make all that quite fast.",
            "So we don't instantiate a bunch of extra memory.",
            "So there's some nice tiny elements of the design that I don't have time to go into this backward pass, super simple."
        ],
        [
            "And then here's grad so grad.",
            "It turns out, remember how I said like everything is really just VJS Vectorscope products in reverse mode and we're just about composing VIPs so grad is actually a tiny function.",
            "Extra like you know error checking stuff in the main code, but all this does is it calls this make VJP function and that's really like the key to autograde.",
            "That's like saying from the primitive JPS build up a composite of JP for some function.",
            "And here's make JP make JP just calls forward pass.",
            "If the output is not a node, that means that the output didn't depend on the input, so it's just zero.",
            "Just return 0, but if the output is a node hears VJP we called forward pass to get started and then just call backward pass.",
            "I've already shown you those things, so this is a composite VJP function that apply to any gradient information we want to pull back.",
            "So in particular when we do grad that just gets the JP and then calls BJP on ones.",
            "OK. That's it, that is all."
        ],
        [
            "Grad so just to summarize doors, I showed you the tracing of composition in terms of what primitive functions are applied and basically all you need to know there is that there's a node data type.",
            "There's a primitive, which is a wrapper to any numerical library you want.",
            "In this case NPI, and then this function forward pass, which was you know, six liner or something like that, and this just built a DAG of what operations were applied to the input when you evaluate it at one time.",
            "Then we just defined groups for each primitive, right?",
            "So we covered all of them, pie, and then some fraction of \u03c0.",
            "And then it was just composing JP backward, which meant we had this backward pass and make the JPN grad was written in terms of that, right?",
            "So just racing and then once we have GPS for everything, it's just about walking the graph and applying the GPS, yeah?",
            "Boxing.",
            "What is the?",
            "Yeah, great question.",
            "So the question was how does the so there's some runtime overhead here, right?",
            "That ahead of time system like tensor flow does not have to incur in particular because we have to trace what's going on with the function.",
            "There's some overhead, whereas Tensorflow already knows what's going to happen with the function it inspected it ahead of time, doesn't have to sort of re inspected every time instead of reading source, we're sort of probing the function by just tracing what happens.",
            "What are the overheads associated with that?",
            "So it depends on the workload.",
            "How big those overheads are, the workloads that we write down, I think, are dominated by numerical kernels.",
            "That's why GPU's matter so much right there, not dominated by overheads, were spending all of our time in like mammals and cons, which are basically like mammals.",
            "And those things have the property that the number of flops scales super linearly with the size of the input.",
            "So if we're making our networks bigger, we can always sort of Max out the arithmetic intensity of our machines.",
            "So basically I think that for a lot of common workloads doesn't mean all workloads.",
            "I think for a lot of common workloads we're spending all of our time these numerical kernels.",
            "And overheads, overheads of the boxing unboxing?",
            "I showed you this is actually pretty simple operation like these rappers are quite thin.",
            "Yes, there massaging pointers, but we're sort of never touching the values of these big arrays except through primitive kernels.",
            "Tracing overhead is just, you know, clicking together linking up this graph data structure.",
            "So there is an overhead.",
            "I think you can write programs where that overhead is substantial, but I think for a lot of the common workloads in.",
            "Deep learning, in particular.",
            "We're spending all of our time in numerical kernels and then anytime we're not spending numerical kernels, is probably because of memory bandwidth, not like interpreter overhead.",
            "In fact, I have reason to believe that C Python is actually not gets a bad rap, but it's actually quite fast and competitive with so.",
            "Tensorflow also has interpreter overhead, not for building this runtime data structure for the tracing, but it has other interpreter overheads, and I think you know, I think these things are actually small for common workloads.",
            "But not for everything.",
            "It's still worth thinking about how to make this process faster.",
            "For example.",
            "Maybe?",
            "When we trace, you know there's this flexibility that we're getting with this tracing.",
            "But there are some functions that we don't need to trace over and over, 'cause there is no control flow, right?",
            "Or there's no random number generator being accessed.",
            "So for example, if you just want the gradient of some loss function and you know your predictions came from some inception V3 every time we're tracing that, it's it comes out the same, right?",
            "So I think there's actually a lot of interesting work to be done here in terms of balancing the flexibility, but then minimizing any costs associated with flexibility.",
            "Picking you up.",
            "Who are?",
            "Remember to get.",
            "Yeah, I think there are trade offs, so the question is just, you know, is there more things we can do here to trade off what's done ahead of time or not.",
            "And yeah, this is sort of the most dynamic approach, but I think it's quite interesting to think about.",
            "You know, doing something sort of ahead of time to the extent we can.",
            "Be happy to talk more about that after.",
            "I think there's actually a lot of interesting stuff to do there."
        ],
        [
            "So those autograph let me just highlight some tradeoffs in forward versus reverse mode.",
            "We don't talk as much about forward mode because it's nowhere near as relevant to us doing optimization, especially deep learning kind of optimization.",
            "So reverse mode has to trace the program's execution, right?",
            "That's where we just talked about the tracing and it has to record all of the intermediate values at least direct implementation records.",
            "All the intermediate values.",
            "That means that we have a memory cost for reverse mode.",
            "This is not about autograph this traverse mode.",
            "We have a memory cost that scales like the depth of the program.",
            "OK, there's some tricks that maybe I'll have time to talk about in a couple of minutes that might be able to trade off time in memory, but basically reverse mode because we're recording.",
            "You know what functions got applied and where to linearize them.",
            "It has this memory overhead that scales like the depth of the program.",
            "Forward mode does not have this overhead.",
            "Because essentially I didn't go into the details, but basically as your evaluating the function forward, you can be applying the primitive Jacobian vector product.",
            "The JV PS as you go right, so it has a constant factor memory overhead, essentially doubling the instantaneous memory cost.",
            "Maximum instantaneous memory cost of your of your program.",
            "You don't have to like record anything, you're just evaluating as you go along.",
            "But as we talked about before, the huge cost is that for functions from RN to R it requires end calls to get the gradient.",
            "And that's just really expensive in fact.",
            "Numerical differentiation right where you just add epsilon to your input.",
            "It has this problem as well.",
            "You have to tweak each input separately, so that's actually the problem with numerical differentiation, which Google explained to me one time.",
            "I don't think it's so much that it's like not accurate at machine precision, because maybe we want to do some smoothing, like why you know maybe want to smooth out the gradient over some interval.",
            "The real cost is that numerical differentiation.",
            "Is like forward mode in this property that you have to make any calls to the function and it's too slow.",
            "That way though there are some things you could call it on random vectors and then try to reconstruct the gradient right?",
            "And this is the evolutionary strategies type stuff.",
            "So even more fun is that Autograde has a fully compatible forward mode extension written by Jamie Townsend.",
            "Then get here and that means you can mix together forward mode in reverse mode to your heart's content.",
            "In AutoCAD you can call forward grad of grad of another for grad at as many times as you want, so it's all sort of fully closed and nice.",
            "And this forward mode implementation doesn't have this memory overhead, so you can play games there.",
            "So."
        ],
        [
            "That's pretty much it for the main goals until the advanced stuff.",
            "So I told you the, you know mathematically, what are we trying to do breakdown?",
            "You know jacobian's or derivatives of composite functions into primitive ones, and then I told you about autographs implementation, which is the same strategies employed by Mimpi and Pytorch.",
            "Now I think autographs implementation is still a bit better, specially with respect to higher order out.",
            "If it's really quite simple and it's fully closed tracing object in Python.",
            "This is separate from.",
            "This also separates audited from what numerical kernel library you're using.",
            "You can wrap anything an autograph doesn't know about what, whether the you know, raise your operating or CPU backed or GPU back, or what the kernels are doing right?",
            "It's just at a higher level.",
            "Which is, I think, a nice property, yeah?",
            "This computer's GPU thing.",
            "There isn't any like GPU back end right now for this though right GPU back end for autograde.",
            "Yeah, so the question was, is there a GPU back end?",
            "Well I have a branch where I wrap some coopi primitives.",
            "Just kind of fun and then this is a several months ago.",
            "I think I made an update and then I just sort of got a little annoyed using Coopi which is a numerical thing not auditing.",
            "So a nice property of that is that because autographs, NUM, PY and some of sci-fi.",
            "If you rap package like coopi, you can also wrap the primitives that translate from an ND array to a coupe or a.",
            "And back, and those are actually those for management pair.",
            "So you can actually build a system that has GPU backed kernels for example for your neural net, and then if you want to go onto the go into NUM PY and do CPU back kernels for like some crazy problems.",
            "Graphical models thing you can do that as well.",
            "So the short answer is not right now, it's something that I'm interested in developing and I think several other people are as well.",
            "Basically you can take any GPU backed.",
            "Numerical library an rapid in autograde, basically using the tools that I showed you right.",
            "You just have to wrap everything as a primitive and then define a node type for your tensor.",
            "So there's an array node that we have.",
            "You basically have to make one of those so check out the Coop, I branch and if you email me I can send you a link to it.",
            "So my question was about this differentiation.",
            "So can you like explain a little bit more so?",
            "Like if I wanted to make an RNN?",
            "And not you complete backup time, but you scored another translation.",
            "Would I be like calling some forward mode thing at every time step or I would just work like I do for loop over the time steps and everytime I look forward one time?",
            "So the question was you know how to use forward mode with friends?",
            "Let's talk about it after.",
            "But basically you solve this problem where you have to iterate over all of your parameters.",
            "Actually there's some I've heard you know how we call reverse mode audited backdrop.",
            "I've heard a term for forward mode.",
            "I think it was called real time recurrent learning.",
            "Yeah OK, definitely 80s.",
            "It was in a lecture by Russ Tedrick real time recurrent learning I think is in this context like foreign mode for sort of recurrent type problems.",
            "Fun jargon, it's not efficient, right?",
            "I mean, I think it's not efficient for the same reason, forward mode is not efficient, but it's more biologically possible, perhaps or something?",
            "No.",
            "OK, that was the justification for it.",
            "I heard I see I see I see.",
            "Cool, so let me spend some time with you until 12:30.",
            "Is that right?",
            "Yeah, good question.",
            "Awesome.",
            "10 minutes for questions, maybe spend, so I'll spend 10 or 13 minutes.",
            "Talking about some advanced auto diff.",
            "I'll just I'll go through it in order so check."
        ],
        [
            "Checkpoint is interesting because it allows us to trade off the space requirements that reverse mode auto DIFF has.",
            "And just, you know, drastically reduce the amount of storage required at the cost of some extra flop overhead.",
            "So if you're more memory limited on your GPU, your system.",
            "But you can do a lot of flops.",
            "Maybe this sort of thing makes sense.",
            "So the basic idea is that when we trace out a big, let me see.",
            "So we trace out a big computation graph.",
            "We are naively sort of storing all of the activations.",
            "If you are if you like, but all the values that happened in that in that forward pass.",
            "And the reason we need those values is because we need to tell each primitive JP where it's supposed to be linearized.",
            "This is sort of getting the linearization point the bias points of the system.",
            "That means that our memory storage scales like the depth of this program.",
            "So like the extent of this of this chain.",
            "I you might.",
            "Think so.",
            "Here's the strategy for reducing the total amount of computation.",
            "For example, let's say that these were two."
        ],
        [
            "Calls to the same function, right?",
            "It had a lot of internal state.",
            "Maybe this is actually an optimizer internally or something right?",
            "Or doing something expensive and we don't, you know, collect all of these things.",
            "So why don't we just throw out?",
            "Or rather, why don't we not store those values, right?",
            "So when we trace, the function will start tracing recording all these values, But then when we're inside this purple box, this one function let's just not record the values that are happening will build the graph.",
            "But just like don't don't record the actually excuse me, we won't even build the graph, will just not record these values and then we'll just get the output here we don't have any storage costs associated with what's in this purple box.",
            "And then we go back into the purple box.",
            "And we do the same thing, right?",
            "So we don't want to do any of those things now.",
            "The total memory cost of our program is something like two or three nodes compared to.",
            "I don't know 8 or 10.",
            "Seems great.",
            "What do we do on the back pass?",
            "So now we have this this partial derivative we want to pull it back.",
            "Now we want to pull it back through this purple box.",
            "What do we do?",
            "We don't have these intermediate values, but we can reconstruct them right?",
            "Because we had its input.",
            "Right and sort of in this land everything is functional programming, so we know what the input was.",
            "We can just re run the function forward and build this graph and then back prop through it to get this value and then throw away everything.",
            "Right, so this way we have to incur at most sort of 1 copy of the purple box's memory overhead.",
            "When we hit the other purple box, we do the same thing we re instantiate by running it forward and then pull it back and then throw away stuff so.",
            "That seems like a nice strategy.",
            "Turns out we can implement this in like 4 lines of autograde.",
            "Does anyone have any ideas?",
            "I'll give you, I'll give some hints.",
            "So the property we want this purple box to have is that we don't want to trace into it.",
            "That's like the first thing.",
            "Somehow we want to have a way to mark a function that says don't trace into it.",
            "Instead, stop the tracing, just run it forwards.",
            "And then restart the tracing afterwards.",
            "Does that sound familiar?",
            "Sounds like a primitive right?",
            "That sounds like a permanent function, so in fact."
        ],
        [
            "A way to implement checkpointing is to just wrap the function as a primitive.",
            "Just look at this one line wrapped equals primitive fun and we return wrapped.",
            "Right, what a primitive says, is don't treat this like a numerical kernel implemented in some other language.",
            "You know who knows what?",
            "Don't try to trace into it with your boxed values.",
            "Just give it an unboxed value and just run it forward and don't do any recording.",
            "And then here's the trick to get the VJP to work this function.",
            "We're declaring a primitive, so we're saying don't do any tracing inside our function.",
            "We're going to wrap.",
            "We're going to write it JP as Oh yeah, just rerun the function and then apply that to the gradient, right?",
            "So the property wanted is for no tracing to happen on the forward pass, but then on the backward pass, if anyone asks to backdrop through us, then we're going to rerun, reconstitute our values forward, and then back prop, and then throw everything away.",
            "So this is actually checkpointing you can use this as a decorator.",
            "You just at checkpoint your function and AutoCAD will not race into it.",
            "That's kind of cool, OK?"
        ],
        [
            "Here's another nice auto diff trick.",
            "I actually so checkpointing.",
            "Things except for some some iterations, autograph some iterations on some ideas that were well known in the automotive community.",
            "Checkpointing Super well known.",
            "I don't know of a reference for this idea, but I'm sure someone came up with it, but we just came up with it on the autograde issues tracker like a few weeks ago and it's a neat trick.",
            "So the idea is once you have reverse mode in some sense you can get forward mode for free, or at least almost for free.",
            "Hi and here's the function that does it now.",
            "This is a little confusing.",
            "We're calling make JP and they were calling me JP.",
            "And yeah, I guess so we're calling make VJP twice.",
            "To get forward mode, and then there's this mysterious comment.",
            "V JP JP is just JVP by linearity.",
            "What's going on here?",
            "So here's the idea in terms of sort of a chain graph, think of our function as implementing this mapping from X to Y. OK, this first JVP that we call.",
            "Excuse me, this first VJP that we call make JP is saying oh, I can take a vector and apply this system.",
            "We just describe to get a Jacobian, transpose vector or vector Jacobian product operator.",
            "That's a composition of these things.",
            "I've transposed things and written as Jacobian transpose vector, but it's just a VJP.",
            "So what we want to do is we want to get forward mode which is Jacobian vector product's.",
            "But we've now built this function that evaluates Jacobian transpose vector products.",
            "So if only we had a way, so this blue function is this linear function that implements the adjoint operator of the thing we want, the transpose that we want.",
            "If only we had a thing that when we apply it to linear operators just gives us the transpose.",
            "Right, so it turns out, remember Vector Jacobian products when you evaluate that on a linear function, right?",
            "It's just going to be like taking your vector, transposing and hit by the Jacobian.",
            "So if your function is linear, that's just going to be applying the Agile and this is going to be applying the transpose, so if we now make a VJP from this V, sorry, so let's build a consider the coding of this intermediate function, which is just this J transpose.",
            "If we make a V JP that operates on some vector U.",
            "This is going to apply the operator at JU, which is a Jacobian vector product.",
            "And Moreover, because this function was linear, it doesn't matter what the value of the was.",
            "It doesn't matter where we linearize a linear function, because it's just always the same, right?",
            "So in fact, these links show you that this computation does not depend on any of the values.",
            "Sorry, any of the values in this intermediate line, right?",
            "This view was just a dummy variable.",
            "In fact, here it's zeros.",
            "Doesn't matter, they will depend on the on the forward like where we linearized forward function of course, but not a backward one.",
            "So this will actually build a compositional way of evaluating Jacobian vector products.",
            "That's what this function that gets returned is.",
            "This V. JP, JP could have also called that JVP.",
            "So one problem with this which is quite interesting is that we had to create these dummy values and propagate them.",
            "We had to create zeros, ornans or whatever V and propagate it just to build this Jacobian, transpose vector or this VJP line.",
            "So that we could then transpose it.",
            "It turns out that with tensor flow, because it's in ahead of time system, you don't have to propagate dummy values.",
            "So in fact this trick has no extra costs in tensor flow.",
            "So here it is in tensor flow.",
            "Except for the fact that like there's some things in tensor flow where it doesn't track the provenance perfectly, but once we clean those up, this will be a great implementation.",
            "Afford mode in Tensorflow so intensively you give sort of you, name the inputs and outputs, right?",
            "You refer to those instead of calling grad on a function like an autograt, you refer to the inputs and outputs.",
            "So this is saying like hey, if function the outputs are wise.",
            "The inputs are ex is and then DX is.",
            "That's like the grading information at the input.",
            "That vector that we want to apply a JVP too.",
            "We want to push forward.",
            "And here we're just calling.",
            "We make a dummy variable that's the size of the outputs.",
            "Here I'm just assuming it's a tensor.",
            "Then we call TF gradients twice.",
            "The first call applies to this dumb Evie.",
            "Ann is just TF, gradients and wise indexes.",
            "And then the second call is from G, which is a name for this output.",
            "Here to V get that JP operator and apply it to DX is.",
            "So, but the jarring, maybe the switch back from differentiating functions to think of drivers on the input and output values themselves.",
            "But yeah, this."
        ],
        [
            "Actually, forward mode in tensor flow in principle for free, and there's no like if you made a manual implementation of Ford mode, you would not do anything different.",
            "You would in fact, right this exact same code, and this has the property of low minimal OS memory like we don't have to trace out the whole thing first, everything.",
            "Cool, so that's forward from reverse.",
            "Let me before going to questions.",
            "I'm just going to prime everyone with."
        ],
        [
            "I think an interesting thing going forward that we've seen start to pop up in several papers, including one that I worked on that I'm going to talk about after lunch.",
            "That's the idea of differentiating sort of solutions or Optima, or fixed points of functions.",
            "For example, there's a recent paper opt Nets that was saying like let's put optimization programs inside of our network is layers and then back problem efficiently.",
            "Where efficiently means autograde can backdrop through an optimizer, but it has to trace the entire execution.",
            "How do we not have to trace through the execution?",
            "So here's the question.",
            "If I have, let's say I have some parameters a, I think of those as just some, like exogeneous parameters and then I'm I have some function that I'm optimizing over X, right?",
            "So it could be parameterising optimization problem and then I'm minimizing Exum searching over X like this could be a planning routine or something like that.",
            "I'm defining then this function, so there's some like hand WAVY stuff about.",
            "Maybe there's multiple minima or something.",
            "In fact I don't have notation for it, but the guarantee of this right argument, but I really mean just find a local minimum in this example.",
            "In fact, just the local stationary point, perhaps so extra is going to be the value of X that solves this problem for some parameters day.",
            "Then I can say, well, that's a function, right?",
            "How do we differentiate through it?",
            "As I said, if you implement this in Python auto gradual differentiate through it no problem, but it will trace through the entire computation.",
            "Maybe instead we can use the fact that X star is an optimizer of F. Write and rewrite and somehow compute the Jacobian information.",
            "The sort of joint information that we need.",
            "From F from the properties of F and the fact that there's a local minimum, so here's another version of essentially the same thing.",
            "It's equation solving.",
            "Solve some system equations parameterized by a bit over these variables X for X.",
            "So we could say the text are now is some solution for A and we might ask how do we differentiate this X star?",
            "Which one of these is more general, do you think?",
            "Bottom, Why is that?",
            "The top is a special case of that yeah, so so I guess we can always write down a system of equations that are like maybe.",
            "At least necessary, but maybe even necessary and sufficient conditions for local optimality, right?",
            "We can say, oh, find a stationary point like the gradient equals zero.",
            "That's one equation.",
            "And then we might even say second derivative test or something, so we can always write optimization.",
            "We can reduce it to equation solving if we want solving these conditions for optimality.",
            "For example in the OPNET paper they talk about solving the KKT conditions.",
            "That was saying, like how the optimization problem think about the KKT conditions so."
        ],
        [
            "Here's a general way to think about this.",
            "Again, I'm just priming use of.",
            "The details aren't aren't too important, but let's say let's take the second form.",
            "This equation solving form.",
            "How do we think about differentiating X star of a given this information, assuming the derivative exists and some technical conditions distributed to exist, we can say something that must satisfy by differentiating both sides of this thing with respect to a.",
            "So using the chain rule that looks like this and then we can rewrite this using an inverse.",
            "They're going to assume exists.",
            "With some regularity conditions, basically say, says the derivative survey can be written in terms of derivative information on G. The way to read this is basically we're linearizing our system of equations at our solution, and that linear informit linearized information is, although we really need to do back prop.",
            "However, we do have to set up this linear system solve.",
            "So differentiating solutions and Optima think I can write that as solving linear systems.",
            "They are indeed the linearized versions of the equations.",
            "And Furthermore, if I've already run a solver, solve these equations or optimize my problem, maybe that solver can give me information about how to solve this system and implicit way.",
            "Or maybe there are some values that that solver returns to me that already contains some of this information.",
            "So for example, in the optinet paper, if you use a primal dual solver.",
            "It turns out that the LaGrange multipliers are.",
            "You can interpret them as a sensitivity of the optimal optimal value with respect to some constraints, like if I change the constraints a little bit, how does my cost change that's in the low range multipliers, so maybe we shouldn't be surprised that you know if we have LaGrange multiplier estimates, we can use them in solving this system, so I'm going to stop."
        ],
        [
            "There.",
            "Yeah, but there's all kinds of cool stuff.",
            "Let me just say thanks to everyone who's helped with this and made slides and sort of thing broken.",
            "Jeff Perlmutter in particular had affect on Google, who taught me everything I know about auto diff.",
            "So yeah, thanks for listening."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great well thanks Graham for the introduction.",
                    "label": 0
                },
                {
                    "sent": "It's really great to be here.",
                    "label": 0
                },
                {
                    "sent": "It's an honor to speak to all of you and learn so many awesome things.",
                    "label": 0
                },
                {
                    "sent": "I talked about automatic differentiation.",
                    "label": 1
                },
                {
                    "sent": "This is more of a.",
                    "label": 0
                },
                {
                    "sent": "This is a tutorial talk.",
                    "label": 0
                },
                {
                    "sent": "I think it's very important for all of us to like improve our understanding of how automatic differentiation works because not only do we use it all the time and it's important understand our tools, but I think actually in some cases were sort of held back by our tools.",
                    "label": 0
                },
                {
                    "sent": "There have been some grumblings today about like, oh maybe, some things are easy to implement in the software that we have now.",
                    "label": 0
                },
                {
                    "sent": "Other things are quite difficult.",
                    "label": 0
                },
                {
                    "sent": "Maybe learning to learn type things and so you know.",
                    "label": 0
                },
                {
                    "sent": "To an extent, the better we can make our software tools for things like automatic differentiation, the more we'll be able to free our minds to think of crazy new research ideas and try out new new things.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about.",
                    "label": 0
                },
                {
                    "sent": "So my understanding of Autodesk comes from working on a software package called Autograde started by Google Mclaurin, who is a PhD student.",
                    "label": 0
                },
                {
                    "sent": "When I was a postdoc at Harvard and then he was joined by me and David were both postdocs with Ryan Adams.",
                    "label": 0
                },
                {
                    "sent": "So a lot of my knowledge of auto diff comes from these people and working with them and learning from them.",
                    "label": 0
                },
                {
                    "sent": "So anything super clever you see here probably came from one of them.",
                    "label": 0
                },
                {
                    "sent": "Not for me, alright?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We are in an awesome world right now in terms of the power that we have are at our fingertips because of our software tools, so I'm sure many of you have used at least one.",
                    "label": 0
                },
                {
                    "sent": "Maybe use multiple of these frameworks.",
                    "label": 0
                },
                {
                    "sent": "This tensor flow theano.",
                    "label": 0
                },
                {
                    "sent": "They're also more recent software packages like Pytorch and Mimpi.",
                    "label": 0
                },
                {
                    "sent": "These are really amazing because they let us write down essentially a forward model specification for our neural network, or even maybe our probabilistic variational inference problem and then automatic differentiation an oftentimes even optimization and some kinds of inference are done essentially automatically for us.",
                    "label": 0
                },
                {
                    "sent": "So it means that we don't have to think so hard about, you know, just have an idea and then spend 90% of our time implementing it.",
                    "label": 0
                },
                {
                    "sent": "So this is an amazing time to be working in machine learning I think.",
                    "label": 0
                },
                {
                    "sent": "However, there's still some things that these tools don't solve as well as maybe they could.",
                    "label": 0
                },
                {
                    "sent": "So in particular, things like loops branching so you're like recursion or lexical closures, data structures like lists and dicts, and this sort of thing.",
                    "label": 1
                },
                {
                    "sent": "To varying degrees, these tools are maybe not so good at supporting all of these like language features that we use when we write other kinds of programs.",
                    "label": 0
                },
                {
                    "sent": "Also, it can be quite hard to debug these things, especially if you're sort of building things ahead of time.",
                    "label": 0
                },
                {
                    "sent": "You know we don't get to use the same sort of debugger tools that we used to, and we write other programs.",
                    "label": 0
                },
                {
                    "sent": "There's also in for some of these second compiler or interpreter that we have to learn how to satisfy and go ahead and help that as sort of new mini language to learn on top of what we already know.",
                    "label": 0
                },
                {
                    "sent": "So I think that these things solve, you know.",
                    "label": 0
                },
                {
                    "sent": "These tools are getting better at solving some of these problems, but I think they are still not quite there in fact.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to tell you about a package.",
                    "label": 0
                },
                {
                    "sent": "The package that we've worked on Autograde, which is just an automatic differentiation package.",
                    "label": 0
                },
                {
                    "sent": "And it's sort of, I think, provides a good example of how to implement automatic differentiation so that it doesn't get in the way.",
                    "label": 0
                },
                {
                    "sent": "In particular, it differentiates native Python code, so you can just write Python in the same way you always do with essentially all of the Python language available to you.",
                    "label": 1
                },
                {
                    "sent": "That means that you can use NUM, PY, and SCI py.",
                    "label": 0
                },
                {
                    "sent": "You can also write code with recursion and lexical closures.",
                    "label": 0
                },
                {
                    "sent": "All these nice things, data structures.",
                    "label": 0
                },
                {
                    "sent": "It's fully closed under its own operation.",
                    "label": 0
                },
                {
                    "sent": "And so you know, thinking about differentiating a program that might have differentiating, going on inside of it.",
                    "label": 0
                },
                {
                    "sent": "Like perhaps you're you have an optimizer that's differentiating a loss function and you want to differentiate with respect to some learning hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "That entire procedure?",
                    "label": 0
                },
                {
                    "sent": "No problem, alright, we'll do it.",
                    "label": 0
                },
                {
                    "sent": "An IT sort of won't.",
                    "label": 0
                },
                {
                    "sent": "It won't bug you about it.",
                    "label": 0
                },
                {
                    "sent": "It's also nice because there's essentially one function API, I really just there's a grad function that's all you need to know.",
                    "label": 0
                },
                {
                    "sent": "If you already know NUM, PY and sci py, that's sort of all you need.",
                    "label": 0
                },
                {
                    "sent": "And in some ways that might become more clear later.",
                    "label": 0
                },
                {
                    "sent": "It's actually quite a small pure Python system, which means that it's easy to extend.",
                    "label": 0
                },
                {
                    "sent": "Its easy to if you want to research in automatic differentiation or extend the system somehow.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you want to make it include wrap some other numerical libraries will become more clear later.",
                    "label": 0
                },
                {
                    "sent": "Hi autograph I think makes it quite easy to do those things, so I'm going to be basing a lot of this talk on autograde.",
                    "label": 0
                },
                {
                    "sent": "It's going to be about auto diff in general and so hopefully by the end of this talk you'll know not only how autograde works, but you'll sort of be able to walk out of this room and implement your own auto diff library that has all of these awesome features.",
                    "label": 0
                },
                {
                    "sent": "One last point, I guess it's sort of interesting.",
                    "label": 0
                },
                {
                    "sent": "I think some of the tools that have come out recently, like Pytorch and Mimpi.",
                    "label": 0
                },
                {
                    "sent": "They are they have some of the support for some of these nice features.",
                    "label": 0
                },
                {
                    "sent": "Their design of their auto diff system is actually based on autographs design, so that's why you know, there's Pytorch dot autograde.",
                    "label": 0
                },
                {
                    "sent": "That's why it's called Auto grad.",
                    "label": 0
                },
                {
                    "sent": "It's sort of inherited.",
                    "label": 0
                },
                {
                    "sent": "The design from a torch autograde, which was a port of this Python code to the torch language and system.",
                    "label": 0
                },
                {
                    "sent": "So I think Google has had a big impact even though you know we don't.",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "Maybe not all of us use autograph, but I think Google's work has been really impactful here, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going on, here's an example of what autograde code looks like.",
                    "label": 0
                },
                {
                    "sent": "Who in this room is used Auto Grad Python library autograph?",
                    "label": 0
                },
                {
                    "sent": "Oh great.",
                    "label": 0
                },
                {
                    "sent": "So I'm telling you something new, so here's an example of what a grade code looks like.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's just Python code.",
                    "label": 0
                },
                {
                    "sent": "There's almost no auto graphic in here.",
                    "label": 0
                },
                {
                    "sent": "We're just writing NUM py, right?",
                    "label": 0
                },
                {
                    "sent": "So this is how you might implement some kind of MLP right.",
                    "label": 0
                },
                {
                    "sent": "We just have a prediction function in a way of scoring it against targets.",
                    "label": 0
                },
                {
                    "sent": "The prediction function just loop through some weights and biases.",
                    "label": 0
                },
                {
                    "sent": "Applying, you know Matt, moles and nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "This is the only place where Autograde enters.",
                    "label": 0
                },
                {
                    "sent": "Is this grad function?",
                    "label": 0
                },
                {
                    "sent": "And basically my claim to use that you can use lists and dicts and closures is not a lot of fancy stuff going on in this example you just use grad and it's a higher order function.",
                    "label": 0
                },
                {
                    "sent": "You give it a function Python callable and it gives you back another Python callable which evaluates the gradient of this function, in this case with respect to the first positional arguments.",
                    "label": 0
                },
                {
                    "sent": "But you can give it different argument numbers or.",
                    "label": 0
                },
                {
                    "sent": "Names.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, it's closed under its own operation, which means that you can do as many grads as you like, so this example might seem a little contrived, but actually I found it extremely useful in my work, not just like going that extra mile and not ever having to think if I do another grad on this thing.",
                    "label": 0
                },
                {
                    "sent": "Is the TF while loop going to break or something?",
                    "label": 0
                },
                {
                    "sent": "Or is there going to be some strange error?",
                    "label": 0
                },
                {
                    "sent": "Some educates you don't have to think about that with autograde, so if you're doing some higher order optimization, maybe with some like Hessian vector products.",
                    "label": 0
                },
                {
                    "sent": "Or something really complicated inside, and then you want to do learning to learn.",
                    "label": 0
                },
                {
                    "sent": "On top of that, or something to optimize your hyperparameters using gradient based search.",
                    "label": 0
                },
                {
                    "sent": "You don't worry about it.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just briefly, here's some more examples of sort of using autograde.",
                    "label": 0
                },
                {
                    "sent": "Here's the implementation of hash, and so I said there's a one function API grad, but actually you might even want Jacobian because Jacobian evaluates it, gives you back a full Jacobian matrix for a vector to vector function.",
                    "label": 0
                },
                {
                    "sent": "Here's how we implement hash and its Jacobian Jacobian.",
                    "label": 0
                },
                {
                    "sent": "Evaluate Hessian vector products more efficiently will become more clear.",
                    "label": 0
                },
                {
                    "sent": "Why if you want passion vector product, this is a much more efficient thing to do.",
                    "label": 0
                },
                {
                    "sent": "We just sort of use higher order functions in Python And super easy.",
                    "label": 0
                },
                {
                    "sent": "Write down more on this at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "If we get to it.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's just one last example.",
                    "label": 0
                },
                {
                    "sent": "This is a tweet from Ryan on Black box variational inference.",
                    "label": 1
                },
                {
                    "sent": "So what this is doing is essentially sort of variational inference where we're optimizing a Gaussian approximation against some target log probability.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially building the elbow.",
                    "label": 0
                },
                {
                    "sent": "The evidence lower bound that we're going to optimize.",
                    "label": 0
                },
                {
                    "sent": "There's an entropy term.",
                    "label": 0
                },
                {
                    "sent": "And then here's an average energy term.",
                    "label": 0
                },
                {
                    "sent": "And then we just call grad on it.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing here is that this is going to sort of do the reparameterization trick for you, right?",
                    "label": 0
                },
                {
                    "sent": "It's just the fact that we are sampling here.",
                    "label": 0
                },
                {
                    "sent": "You just called Brad and it'll differentiate this unbiased estimator and give you an unbiased estimator of the gradient.",
                    "label": 0
                },
                {
                    "sent": "So Ryan at.",
                    "label": 0
                },
                {
                    "sent": "Other people were at Twitter for some time and so this is actually how they do their code checkins.",
                    "label": 0
                },
                {
                    "sent": "They don't use GitHub, they actually have to use tweets, so this is an official Twitter check in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's a high level of autograde.",
                    "label": 0
                },
                {
                    "sent": "Let me tell you the goals for the rest of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So first the first part is going to be about math and just defining reverse mode and forward mode automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "I guess I should say reverse and forward accumulation of partial derivatives and Jacobian information, so it's going to be math at first.",
                    "label": 0
                },
                {
                    "sent": "That's going to tell us you know what are these things.",
                    "label": 0
                },
                {
                    "sent": "What is reverse mode or backdrop in terms of math symbols.",
                    "label": 0
                },
                {
                    "sent": "The second part is going to be about autographs implementation about carrying out some of the operations that we develop in the first part, and in fact I'm going to show you all of it.",
                    "label": 0
                },
                {
                    "sent": "You will like literally be able to understand all of autographs QR code after seeing this because it is short.",
                    "label": 0
                },
                {
                    "sent": "I'll be able to go through it pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "And then finally, I'm not sure how much time depending on questions.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how far will get into this, but I have a bunch of advanced auto diff stuff that I think is interesting to think about, so we'll see if we get to the sort of things like checkpointing differentiating optimal sort of optimal optimizing points and like fixed points, these sorts of things doing this implicitly, I think there's a lot of exciting new stuff we can do with some of these techniques, so let's see if we can get there.",
                    "label": 0
                },
                {
                    "sent": "Starting off with.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jacobian's, I'm going to set up some mathematical notation just so we're all on the same page.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be talking about evaluating the derivative of this function F, so it's from RN to R. Think of this as a loss function on your parameters in our end, so implicitly, there's training data inputs and targets, but that's not being tracked here, it's just F on the parameter vector to some loss value.",
                    "label": 0
                },
                {
                    "sent": "That's how you should think of F. So you know, we can also specify F by naming its inputs and outputs, which is going to be very convenient for a lot of certain kind of derivative notation, so I'm going to talk about the input of F being X with this little bolding in the text, referring to the fact that it's a vector in RN, and then why is just a scalar output.",
                    "label": 0
                },
                {
                    "sent": "So let's also say that F is a composition of several functions, so all of auto diff is really about function composition and differentiating composition of functions.",
                    "label": 0
                },
                {
                    "sent": "So let's breakdown F into a composition of these four functions.",
                    "label": 0
                },
                {
                    "sent": "And if we want to name the inputs and outputs, will say that.",
                    "label": 0
                },
                {
                    "sent": "Why is F of X and then we can breakdown F as evaluating a DFC of B of A right?",
                    "label": 0
                },
                {
                    "sent": "In fact, let's make some names for all the intermediate values as well.",
                    "label": 0
                },
                {
                    "sent": "Those will be useful to refer to on the next slide, so in fact we can race.",
                    "label": 0
                },
                {
                    "sent": "Imagine like this is a way to spell out the program if we read it from right to left, right?",
                    "label": 0
                },
                {
                    "sent": "First, we're going to evaluate Avex into this variable called A.",
                    "label": 0
                },
                {
                    "sent": "They're going to be to that and so on.",
                    "label": 0
                },
                {
                    "sent": "Any questions on the notation?",
                    "label": 0
                },
                {
                    "sent": "Grace so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's our breakdown of how we're evaluating F, and we want to evaluate the derivative so there's.",
                    "label": 0
                },
                {
                    "sent": "The women's right to the derivative, at least for this slide, is as this Jacobian matrix.",
                    "label": 0
                },
                {
                    "sent": "So we write F prime of X to refer to the function of X that evaluates the.",
                    "label": 0
                },
                {
                    "sent": "The Jacobian matrix of F valuated X.",
                    "label": 0
                },
                {
                    "sent": "We also use this other notation that looks kind of like fractions where we write partial Y partial X terms of the output.",
                    "label": 0
                },
                {
                    "sent": "We're naming the output as well as the input and this is just a matrix of partial derivatives.",
                    "label": 0
                },
                {
                    "sent": "In this case, it's just one row, right?",
                    "label": 0
                },
                {
                    "sent": "This is a row vector or row matrix and that's because F has a scalar valued output, but a vector input.",
                    "label": 0
                },
                {
                    "sent": "So using this decomposition of F as a composition of these four functions, as I'm sure you guys remember from calculus, we have this chain rule which tells us that we can actually write this Jacobian matrix as a product of these other Jacobian matrices.",
                    "label": 0
                },
                {
                    "sent": "This sort of fraction notation is kind of handy because it reminds us we think of like kind of canceling.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, these fractions that we're getting partial Y partial X expanded out in terms of these intermediate.",
                    "label": 0
                },
                {
                    "sent": "And to unpack the notation a little bit more.",
                    "label": 0
                },
                {
                    "sent": "We can say that this partial I partial C is none other than the Jacobian matrix of D. That's a D prime evaluated at at sea.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "So what we're saying is that to get the derivative, this Jacobian matrix of F, which is a one by N matrix, we're going to write that out as a product of these four other matrices, right?",
                    "label": 0
                },
                {
                    "sent": "Breaking this down in terms of the Jacobian matrices of our functions we're composing.",
                    "label": 0
                },
                {
                    "sent": "And so we can say that this one by N matrix is a product of these other things, with with intermediate sizes in the middle.",
                    "label": 0
                },
                {
                    "sent": "So for example, this guy right here.",
                    "label": 0
                },
                {
                    "sent": "What is the size of this matrix?",
                    "label": 0
                },
                {
                    "sent": "Can anyone tell me?",
                    "label": 0
                },
                {
                    "sent": "So it's one by something.",
                    "label": 0
                },
                {
                    "sent": "It's not 1 by N necessarily.",
                    "label": 0
                },
                {
                    "sent": "It's actually one by whatever the size of C is right.",
                    "label": 0
                },
                {
                    "sent": "It's one by the size of C, and this is the size of C by the size of B and the size of the the size of A and the size of a by this.",
                    "label": 0
                },
                {
                    "sent": "Then that last one is NI think it was drawing things wrong when I was reading things out, but this is a.",
                    "label": 0
                },
                {
                    "sent": "Let's see, this horizontal size should be N on this last one.",
                    "label": 0
                },
                {
                    "sent": "So we have a product of these matrices, so you know if we had some way of evaluating these Jacobian matrices, we can.",
                    "label": 0
                },
                {
                    "sent": "We can express the derivative thereafter in terms of them, we've reduced our problem.",
                    "label": 0
                },
                {
                    "sent": "So we have to multiply these matrices.",
                    "label": 0
                },
                {
                    "sent": "Does it matter what order we multiply them in?",
                    "label": 0
                },
                {
                    "sent": "Yes, what you know which order would we multiply this in?",
                    "label": 0
                },
                {
                    "sent": "Do you think?",
                    "label": 0
                },
                {
                    "sent": "Yeah, look there.",
                    "label": 0
                },
                {
                    "sent": "Problem of the next matrix chain ordering problem.",
                    "label": 0
                },
                {
                    "sent": "That's right, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what you're talking about is if I have a sequence of matrices and I want to draw parentheses around them so as to say how in what order should I perform the multiplication to minimize the number of flops, or perhaps minimize the memory usage?",
                    "label": 0
                },
                {
                    "sent": "There's a dynamic program that can actually solve that on chains, but in fact, even though this example had.",
                    "label": 0
                },
                {
                    "sent": "You know this composition was sort of just a chain graph that came out.",
                    "label": 0
                },
                {
                    "sent": "In general, we might compose functions in a much more complicated way where you can imagine the composition structure is better represented as a DAG of some kind, and it turns out it turns out that the optimal way to multiply matrices in that form, the ultimate the optimal way to accumulate jacobian's, is actually, I think in NP hard problem in that case.",
                    "label": 0
                },
                {
                    "sent": "So there's a dynamic program on the chain, but not for the general problem.",
                    "label": 0
                },
                {
                    "sent": "So let's just talk about two simple.",
                    "label": 0
                },
                {
                    "sent": "Like 2 extreme cases.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of how to associate these things the 1st way would be to write all the parentheses, sort of nested on the right.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing we're going to play, or those two matrices there were on the far right.",
                    "label": 0
                },
                {
                    "sent": "That means that we're going to get intermediate results that are of size like this.",
                    "label": 0
                },
                {
                    "sent": "So for example, this thing is size of B by size of X, so all of our intermediate values right?",
                    "label": 0
                },
                {
                    "sent": "Are these big matrices.",
                    "label": 0
                },
                {
                    "sent": "So this is called forward accumulation.",
                    "label": 0
                },
                {
                    "sent": "Another way to associate these matrices.",
                    "label": 0
                },
                {
                    "sent": "Again, these are two extremes of what we could do.",
                    "label": 0
                },
                {
                    "sent": "We could try to optimize this in a more general way.",
                    "label": 0
                },
                {
                    "sent": "These are two extremes.",
                    "label": 0
                },
                {
                    "sent": "Another way we can associate multiplying out these matrices is sort of from the left and interesting thing, because we had the scalar valued output.",
                    "label": 0
                },
                {
                    "sent": "Remember we have the one on the far side of our our matrices like the thing on the far left with just a row vector.",
                    "label": 0
                },
                {
                    "sent": "It turns out that these intermediate products are actually themselves just one by intermediate size.",
                    "label": 0
                },
                {
                    "sent": "Jacobian's this is called reverse accumulation.",
                    "label": 0
                },
                {
                    "sent": "So you can see at least just by looking at the size of the intermediate that we have to accumulate that it seems like for functions that go from RN for some large N, maybe in the millions to R, right, a scalar valued output certainly looks like accumulating, just having to do matrix vector products.",
                    "label": 0
                },
                {
                    "sent": "Or I should say vector matrix products all the time and only having to store these vectors seems much more efficient.",
                    "label": 0
                },
                {
                    "sent": "If we were evaluating the derivative of a function.",
                    "label": 0
                },
                {
                    "sent": "That went from R to RN like maybe it's parameterising a curve in some high dimension space.",
                    "label": 0
                },
                {
                    "sent": "Maybe we then we'd want to forward accumulation, but because we had a large vector input and just a scalar output, this reverse accumulation looking much better.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me give you another way to describe the exact same thing, and that's just going to be sticking a vector on the end.",
                    "label": 0
                },
                {
                    "sent": "So what if I had asked you instead of give me the Jacobian matrix of this thing, give me the Jacobian multiplied by some vector that I will give to you, right?",
                    "label": 0
                },
                {
                    "sent": "So Jacobian vector product?",
                    "label": 0
                },
                {
                    "sent": "Then if we think of evaluating the Jacobian vector product, we can just write associate and it doesn't matter the size of why, because we're going to have this vector on the end here, so all these intermediate values are going to be vectors themselves, so this is sort of, you know, when we have a vector on the right and we're trying to evaluate a Jacobian vector product forward accumulation makes sense.",
                    "label": 0
                },
                {
                    "sent": "In fact, it doesn't matter what the size of the output is in this case.",
                    "label": 0
                },
                {
                    "sent": "So I think the best way to think about forward accumulation.",
                    "label": 0
                },
                {
                    "sent": "You should associate that in your mind as evaluating Jacobian vector product's OK. Another way to think about this is how would you build an entire Jacobian matrix, right?",
                    "label": 0
                },
                {
                    "sent": "If you had access?",
                    "label": 0
                },
                {
                    "sent": "If I gave you a function I said oh this thing, you give it X&V and it will give you a Jacobian vector product out and you were like, well, I want the entire Jacobian.",
                    "label": 0
                },
                {
                    "sent": "Right would you do well?",
                    "label": 0
                },
                {
                    "sent": "You could just feed in a basis.",
                    "label": 0
                },
                {
                    "sent": "You could feed in A1 hot basis for V, right?",
                    "label": 0
                },
                {
                    "sent": "If you give it the vector 100 you call this function that gives you coming back to product that will reveal to you the first column of the Jacobian, right?",
                    "label": 0
                },
                {
                    "sent": "And so on, you give it the next one hot vector.",
                    "label": 0
                },
                {
                    "sent": "It will give you the next column.",
                    "label": 0
                },
                {
                    "sent": "So another thing to think of when you think of forward mode is that if you wanted to build an entire Jacobian matrix Ford mode, which lets you build it one column vector at a time.",
                    "label": 0
                },
                {
                    "sent": "If you had these Jacobian vector product as your primitive.",
                    "label": 0
                },
                {
                    "sent": "And then you know another.",
                    "label": 0
                },
                {
                    "sent": "Another way to think of this is, you know, if we had an entire matrix, is an identity matrix of just stuck on the end here, right?",
                    "label": 0
                },
                {
                    "sent": "There's one hot vectors and we're applying this.",
                    "label": 0
                },
                {
                    "sent": "You should think of this as applying to the column vectors in this matrix, so we can write an entire Jacobian matrix by sort of taking basis and then pushing it forward.",
                    "label": 0
                },
                {
                    "sent": "We can think of 1 column vector at a time, so forward mode, Jacobian vector, product's build Jacobian matrices, one column at a time.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How about on the other side?",
                    "label": 0
                },
                {
                    "sent": "One?",
                    "label": 0
                },
                {
                    "sent": "If I asked for a vector Jacobian product, right?",
                    "label": 0
                },
                {
                    "sent": "Really a vector transpose Jacobian, but I call this vector Jacobian product right?",
                    "label": 0
                },
                {
                    "sent": "Then if we expand this out again, right?",
                    "label": 0
                },
                {
                    "sent": "This is what this looks like now again, no matter what the size of the input of the output is makes a lot of sense to just do Association on the left right away, we can build up a vector Jacobian product for F is by evaluating all of these vector Jacobian products one at a time.",
                    "label": 0
                },
                {
                    "sent": "So from the output to the input and this is, you know, I think the best way to think about reverse accumulation in the thing that reverse accumulation or reverse mode audit if does efficiently is it gives us access to vector Jacobian products.",
                    "label": 0
                },
                {
                    "sent": "Essentially, you know by corresponding argument if you fed in one hot vectors to this vector Jacobian product function.",
                    "label": 0
                },
                {
                    "sent": "This would let you build a Jacobian one row at a time.",
                    "label": 1
                },
                {
                    "sent": "So you can see why we like reverse mode in the context of optimization and neural networks, right?",
                    "label": 0
                },
                {
                    "sent": "It's because we always have these loss functions that go from huge from RN, where N is millions or maybe billions to R, right?",
                    "label": 0
                },
                {
                    "sent": "Just like our ends are huge and that means our Jacobian is always going to be a one by N matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you have a method that can build the Jacobian one row at a time, in this case you're done in one shot, right?",
                    "label": 0
                },
                {
                    "sent": "Or if you have a small number of outputs, you only have to call it a few times.",
                    "label": 0
                },
                {
                    "sent": "But if you did forward mode, if you had to build a one by end Jacobian one column at a time, you're just getting one number for every single call.",
                    "label": 0
                },
                {
                    "sent": "Another thing that I haven't quite spelled out, but.",
                    "label": 0
                },
                {
                    "sent": "You know the high level?",
                    "label": 0
                },
                {
                    "sent": "You should just think of each evaluation of a Jacobian, vector, product or vector Jacobian product as comparable to the cost of evaluating the function, at least in terms of FLOPS.",
                    "label": 0
                },
                {
                    "sent": "OK, so somehow we have to pay the price if we're using reverse accumulation and we have a one by N Jacobian matrix that we're after, we just have to pay something that looks like the we have to evaluate the function, But then we just have to pay that same function evaluation cost once to get the entire Jacobian matrix because it's a one by hand.",
                    "label": 0
                },
                {
                    "sent": "And as before, we can sort of if we like, think of what we were doing before is having a little identity matrix.",
                    "label": 0
                },
                {
                    "sent": "In our case, for a scalar output functions one by one matrix or just expanding out the Jacobian by saying start this is like where we start the recursion at a one an identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we are applying this thinking of applying it one row at a time.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize forward accumulation, I guess even bigger.",
                    "label": 0
                },
                {
                    "sent": "Sorry the whole point of thinking about differentiating shooting this way is that we have these big composite functions which are built up out of compositions of similar functions, right?",
                    "label": 0
                },
                {
                    "sent": "You write a complicated neural net, but you're building it out of tensor flow primitives, and even underneath that, if you if you wanted to, you could say those are all built out of plus and times right?",
                    "label": 0
                },
                {
                    "sent": "I mean computers don't know how to do much more than plus and times so.",
                    "label": 0
                },
                {
                    "sent": "High level, we're trying to breakdown evaluation of jacobian's into.",
                    "label": 0
                },
                {
                    "sent": "You know, evaluating similar Jacobian, then accumulating them by multiplying those things together?",
                    "label": 0
                },
                {
                    "sent": "Ford accumulation is an extreme where we sort of right associate everything you should think of that as giving us access to efficient Jacobian vector product.",
                    "label": 0
                },
                {
                    "sent": "So there's some jargon here, which is that we think of like pushing forward.",
                    "label": 0
                },
                {
                    "sent": "We sort of have a vector that sits at the input, right?",
                    "label": 0
                },
                {
                    "sent": "It's like an input sized vector and we can push it forward by applying all these jacobian's these Jacobian vector products to it, and this builds Jacobian is 1 column at a time.",
                    "label": 0
                },
                {
                    "sent": "If you give it one hot vectors.",
                    "label": 0
                },
                {
                    "sent": "Where is reverse accumulation?",
                    "label": 0
                },
                {
                    "sent": "You should think of vector Jacobian product.",
                    "label": 0
                },
                {
                    "sent": "So really vector transpose Jacobian products.",
                    "label": 0
                },
                {
                    "sent": "We think this is some jargon in differential geometry where you're pulling back because you sort of have a vector that's the size of your output and you pull it back to get sort of differential information at your input.",
                    "label": 0
                },
                {
                    "sent": "If you apply this to build full Jacobian matrices by applying it to one vectors.",
                    "label": 0
                },
                {
                    "sent": "Think of this building in Jacobian matrix, one row at a time.",
                    "label": 1
                },
                {
                    "sent": "So some details that didn't cover non chain.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In composition, the purpose of this slide is just to tell you that even though we're only talking about chains, it's actually quite straightforward to generalize it to non chains right where we compose functions and sort of we have fan out because we're going to use, we have an output of a function, some temporary variable, and we're going to give it to two different functions applied to.",
                    "label": 0
                },
                {
                    "sent": "That's like fan out, or we can also have Fannin where we apply functions of many arguments, which I didn't show you before, so Fannin, that just means that you know we have some function F and it has two.",
                    "label": 0
                },
                {
                    "sent": "Inputs this is not surprising that this shouldn't be too scary, because we can always just think of like oh, why not just stack those inputs, right?",
                    "label": 0
                },
                {
                    "sent": "Like you say, it's two inputs, but I can always stack them and call them one input.",
                    "label": 0
                },
                {
                    "sent": "That's exactly right.",
                    "label": 0
                },
                {
                    "sent": "Another way to think of it is just you can unstack F and you can say I have a way of thinking about Jacobian matrices with respect to the first argument and separately with respect to the second argument.",
                    "label": 0
                },
                {
                    "sent": "So sort of fan in if you like is handled by just thinking of having separate Jacobian matrices.",
                    "label": 0
                },
                {
                    "sent": "Fan Out is a little more subtle, but I hope to convince you with a simple argument when you use when you have a variable you maybe computing along a chain and then you end up sending it to two different functions, right?",
                    "label": 0
                },
                {
                    "sent": "So now you have this pan out in the function that you are trying to differentiate.",
                    "label": 0
                },
                {
                    "sent": "That sort of looks like function G that takes an input X and just replicates it and gives 2 outputs.",
                    "label": 0
                },
                {
                    "sent": "And really, this is a linear function, quite a simple one, right?",
                    "label": 0
                },
                {
                    "sent": "It's just like taking this stacked identity matrix and applying to exit seems trivial, right?",
                    "label": 0
                },
                {
                    "sent": "But the point of writing this out is that now we can think of what is the Jacobian of G. Well, it's a linear function, so it's really just this linear coefficient matrix, so it's just this guy.",
                    "label": 0
                },
                {
                    "sent": "So what is a vector Jacobian product?",
                    "label": 0
                },
                {
                    "sent": "So when we're doing back prop and we have to handle fan out, you know we're getting two gradients, so sort of speak two sets of partial derivative information from both of our outputs.",
                    "label": 0
                },
                {
                    "sent": "What do we do then?",
                    "label": 0
                },
                {
                    "sent": "We just sum them, right?",
                    "label": 0
                },
                {
                    "sent": "This is saying the vector Jacobian prodigy is just this guy will just means some those two things, so handing fan out?",
                    "label": 0
                },
                {
                    "sent": "Basically means that we're going to be coming as we accumulate.",
                    "label": 0
                },
                {
                    "sent": "I derivatives during background.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Any questions on?",
                    "label": 0
                },
                {
                    "sent": "Non chain stuff will see this actually more concretely when we talk about the code, which is what we're going to do next.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that was jacobian's in the chain rule.",
                    "label": 1
                },
                {
                    "sent": "That was the math part, yeah?",
                    "label": 0
                },
                {
                    "sent": "This matrix view is nice.",
                    "label": 0
                },
                {
                    "sent": "Is this unifying?",
                    "label": 0
                },
                {
                    "sent": "But in practice, if you have a big graph.",
                    "label": 0
                },
                {
                    "sent": "You will end up with huge matrices.",
                    "label": 0
                },
                {
                    "sent": "Yeah, would be the best data structure for doing this, right?",
                    "label": 0
                },
                {
                    "sent": "So we don't instantiate those matrices.",
                    "label": 0
                },
                {
                    "sent": "That's a great point.",
                    "label": 0
                },
                {
                    "sent": "So in fact, this is another advantage.",
                    "label": 0
                },
                {
                    "sent": "Repeat the question for yeah.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You so the question was like, actually, it seems like if you had to instantiate all these matrices, maybe these would be quite large, right?",
                    "label": 0
                },
                {
                    "sent": "In fact, this is like the matrix of your number of inputs like your number parameters of billion by you know how many activations you have in your first output.",
                    "label": 0
                },
                {
                    "sent": "So these matrices could be quite large.",
                    "label": 0
                },
                {
                    "sent": "Maybe we shouldn't.",
                    "label": 0
                },
                {
                    "sent": "You explicitly instantiate them.",
                    "label": 0
                },
                {
                    "sent": "This is another advantage of thinking in terms of what we really want.",
                    "label": 0
                },
                {
                    "sent": "Our Jacobian, vector product sore back prop.",
                    "label": 0
                },
                {
                    "sent": "Jacobian products, that's to say that we don't actually have to instantiate this Jacobian matrix to evaluate the vector Jacobian product, right?",
                    "label": 0
                },
                {
                    "sent": "As long as you write a function that evaluates the vector product, we're happy.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you have an elementwise operation, so an elementwise 10 H, or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "If you think of the Jacobian matrix for an element wise operation, it's actually a diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "You shouldn't instantiate that whole diagonal matrix, right?",
                    "label": 0
                },
                {
                    "sent": "You can just apply it by doing some.",
                    "label": 0
                },
                {
                    "sent": "Element wise operation.",
                    "label": 0
                },
                {
                    "sent": "That's like applying that.",
                    "label": 0
                },
                {
                    "sent": "Basically that's an argument to say that because this Jacobian is diagonal, we can implement the vector Jacobian product operator with an elementwise operation.",
                    "label": 0
                },
                {
                    "sent": "So as we become more clear in the next section, we're not in our.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Imitation going to build explicit matrices like explicit Jacobian matrices.",
                    "label": 0
                },
                {
                    "sent": "Instead, we're just going to write functions that implement vector Jacobian product operators, and because these matrices are often extremely sparse, or perhaps diagonal and sort of thing, we can be much more efficient that way, both in terms of memory and in terms of compute time.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to tell you bout autograde's implementation and seriously, this will be everything you can follow along in the source code if you want, but.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll be showing the source the relevant parts of the source on the slides, so.",
                    "label": 0
                },
                {
                    "sent": "To do auto diff, we really have to audit is about tracking which of which primitive functions get applied to your input.",
                    "label": 0
                },
                {
                    "sent": "And then using some definitions of how to evaluate Jacobian vector products or vector Jacobian products for each one of those primitive things.",
                    "label": 0
                },
                {
                    "sent": "So there are basically 2.",
                    "label": 0
                },
                {
                    "sent": "Or at least two extreme strategies for implementing audit if one of them is to read and generate source code ahead of time.",
                    "label": 1
                },
                {
                    "sent": "So this might mean reading Python is not too hard, right?",
                    "label": 0
                },
                {
                    "sent": "Python has an AST module, we have to read Python.",
                    "label": 0
                },
                {
                    "sent": "It's very syntactically rich languages.",
                    "label": 0
                },
                {
                    "sent": "A lot of features.",
                    "label": 0
                },
                {
                    "sent": "And then maybe you could generate Python, right?",
                    "label": 0
                },
                {
                    "sent": "What we seem to do more often is define another language that we then like sort of, use Python as a meta programming language.",
                    "label": 0
                },
                {
                    "sent": "For right.",
                    "label": 1
                },
                {
                    "sent": "We can also say create a computation graph language and will actually write our programs.",
                    "label": 0
                },
                {
                    "sent": "The functions F that we want to differentiate in this computation graph language.",
                    "label": 0
                },
                {
                    "sent": "This is what tensor flow is doing, right?",
                    "label": 0
                },
                {
                    "sent": "So tensor flow is actually the way it does auto diff is.",
                    "label": 0
                },
                {
                    "sent": "It looks here Tensorflow program, which is some dataflow representation, you know?",
                    "label": 0
                },
                {
                    "sent": "Some graph representation and then it has a way of taking a graph program.",
                    "label": 0
                },
                {
                    "sent": "And producing another graph program that evaluates the derivative.",
                    "label": 0
                },
                {
                    "sent": "This is 1 implementation option.",
                    "label": 0
                },
                {
                    "sent": "It has advantages and disadvantages.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have the entire program in a sort of more.",
                    "label": 0
                },
                {
                    "sent": "Dataflow representation, maybe you can do more automatic transformations to it, right?",
                    "label": 0
                },
                {
                    "sent": "This is why we have ahead of time languages and compilers, right?",
                    "label": 0
                },
                {
                    "sent": "C++ compiler's can build your language into a dataflow representation and reason about it to optimize the program and generate source code.",
                    "label": 0
                },
                {
                    "sent": "It's sort of knows more about what that program's behavior is going to be.",
                    "label": 0
                },
                {
                    "sent": "Python, on the other hand, as a language, not just an implementation, is slow because every line might surprise you, like who knows what the next line is going to do you think it was calling DOT?",
                    "label": 0
                },
                {
                    "sent": "But like in an earlier line, I rewrote dot to be like.",
                    "label": 0
                },
                {
                    "sent": "Make a HTTP get request on some server or something, so Python is like very surprising so.",
                    "label": 0
                },
                {
                    "sent": "Another way to implement audit if that is easier to support in higher level languages is to monitor function execution at runtime.",
                    "label": 1
                },
                {
                    "sent": "So this strategy is basically saying instead of implementing something that can read and decide you know Python to the extent that we need to differentiate it.",
                    "label": 0
                },
                {
                    "sent": "Instead, let's just do everything as late as possible, right?",
                    "label": 0
                },
                {
                    "sent": "Let's not even try to build this graph or like reason about what primitive functions get applied to your input to produce the value of your function.",
                    "label": 0
                },
                {
                    "sent": "Let's not reason about that until we're actually doing it at runtime, right?",
                    "label": 0
                },
                {
                    "sent": "Let's actually let the Python interpreter let's see Python or whatever, implement Python And just monitor as it's executing what functions get applied to the input.",
                    "label": 0
                },
                {
                    "sent": "And then we'll be able to support a lot of the cool features that I was talking about as well.",
                    "label": 0
                },
                {
                    "sent": "Become clear.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to be talking about this implementation style, which is monitoring function execution or runtime.",
                    "label": 0
                },
                {
                    "sent": "This is how autograph is implemented.",
                    "label": 0
                },
                {
                    "sent": "This is also how like Py torch in admin PY essentially do things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I won't be talking too much more about the about how tensor flow works, but maybe if we have questions at the end, I can say a bit more, yeah?",
                    "label": 0
                },
                {
                    "sent": "Tensor flow and Sumeet was also talking with Pytorch implementations.",
                    "label": 0
                },
                {
                    "sent": "There talking with tape method.",
                    "label": 0
                },
                {
                    "sent": "Yeah, Knology is this basically the same same thing?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "So the question was when Smith was talking about the taping method, so I didn't see that talk when he was talking about about taping.",
                    "label": 0
                },
                {
                    "sent": "Is this the same thing?",
                    "label": 0
                },
                {
                    "sent": "And yes, it is so tape is a jargon from the literature.",
                    "label": 0
                },
                {
                    "sent": "That means like fight instead said record function execution at runtime right?",
                    "label": 0
                },
                {
                    "sent": "That would be like taping.",
                    "label": 0
                },
                {
                    "sent": "That's where they got the term.",
                    "label": 0
                },
                {
                    "sent": "I guess they didn't call it Laserdisc ING or something, or like Betamax ING, just because of whatever technology is available, so this stuff is.",
                    "label": 0
                },
                {
                    "sent": "I should also say that these things aren't new.",
                    "label": 0
                },
                {
                    "sent": "They've been known in the auditor for a long time, though there are some new elements I believe to autograde when it first came out that now exists in some other systems.",
                    "label": 0
                },
                {
                    "sent": "I don't like the term tape for, like a very technical reason, but basically when you record the graph, the tape is like a topological sorting of that graph, but in general.",
                    "label": 0
                },
                {
                    "sent": "You don't have to be constrained to one particular topological sorting, so I'm just saying more generally you just have to monitor function execution and build a graph data structure of some kind.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if it's pending on how many details soon went into, you may have told you a version of this already, but you know, after I give this version, I guess you can compare and see which one you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to tell you auto autograph implementation in three parts.",
                    "label": 0
                },
                {
                    "sent": "There are really just three ingredients and their ingredients I've already alluded to.",
                    "label": 0
                },
                {
                    "sent": "So one basic thing we have to do is be able to trace the tape, the composition of primitive functions.",
                    "label": 1
                },
                {
                    "sent": "That just means when you give me a function and you say I want to differentiate this with respect to input, when we get an input value and give that to the function we want to then record what operations get applied to it.",
                    "label": 0
                },
                {
                    "sent": "So the first step is just racing, which doesn't on its own, necessarily have much to do with auto diff.",
                    "label": 0
                },
                {
                    "sent": "The second is to define a vector Jacobian product operator, right?",
                    "label": 0
                },
                {
                    "sent": "Not necessarily the Jacobian matrix, but some operator that implements a vegetarian product for each of these primitives, right?",
                    "label": 0
                },
                {
                    "sent": "And then, once we have sort of a trace of what primitives were applied and a vector decoding product for every primitive, Now we have all the information we need to sort of do back prop to apply these vectors.",
                    "label": 0
                },
                {
                    "sent": "Your product's in a sequence and pull back the derivative information from the output to the input.",
                    "label": 0
                },
                {
                    "sent": "So first part tracing composition and primitive functions so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Should really think of in this context is like a numerical kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so with autograph because we liked working with NUM PY and SCI py, we wrapped empires are numerical kernel library.",
                    "label": 0
                },
                {
                    "sent": "You could actually take any other numerical kernel library like pie or even Pytorch and wrap it in terms of autograde.",
                    "label": 0
                },
                {
                    "sent": "And it will be able to trace its execution.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll say more about that in a bit, but for now I'm just think of a primitive as a NUM py function, like NUM, py, some or sign or dot.",
                    "label": 0
                },
                {
                    "sent": "So for each NUM py function we wrap it.",
                    "label": 0
                },
                {
                    "sent": "In a primitive, what that means is that we.",
                    "label": 0
                },
                {
                    "sent": "This is why at the top of your file you write import auto grad NUM PY as NP right?",
                    "label": 0
                },
                {
                    "sent": "We have a thin wrapper for every of every one of those of those functions and what the purpose of the rapper is is it will see a boxed value coming in sort of node value and this node basically is corresponding to some value a those propagating through your computation.",
                    "label": 0
                },
                {
                    "sent": "It has some extra information like what function it came from F and what its parents were.",
                    "label": 0
                },
                {
                    "sent": "In this case, maybe just had one parent which is X.",
                    "label": 0
                },
                {
                    "sent": "So here's all the data primitive does the first thing it does is it unboxes the value right?",
                    "label": 0
                },
                {
                    "sent": "It gets this raw ND array a out of the boxing and it passes it to them by some function, right?",
                    "label": 0
                },
                {
                    "sent": "This is to get the result of the forward computation that you're trying to implement.",
                    "label": 0
                },
                {
                    "sent": "Then the other thing it does is it boxes up the output right?",
                    "label": 0
                },
                {
                    "sent": "So it takes the input unboxes it gives it to the NUM py, primitive or whatever numerical kernel primitive.",
                    "label": 0
                },
                {
                    "sent": "It gets the results, which is an RA ND array and then it boxes it up with the information that we need like, oh, by the way, we just apply to some right.",
                    "label": 0
                },
                {
                    "sent": "This is like annotating a node in the graph with what function was applied, what the inputs were, and what the output was.",
                    "label": 0
                },
                {
                    "sent": "So here is.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The code to implement that of course there's some you know, some lines that I'm hiding, the core maybe 100 or 200 lines, but these are really the operational ones, so this is basically a struct, right?",
                    "label": 0
                },
                {
                    "sent": "This is basically like saying.",
                    "label": 0
                },
                {
                    "sent": "These are the nodes we're going to build our graph out of, sort of some type information, But basically this recipe is what contains that information I showed you on the previous slide about what function was just applied and what its parents were.",
                    "label": 0
                },
                {
                    "sent": "And then the value is the value, so that's it for nodes.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then I said our primitive wrapper is really just has to unbox any box arguments, call the function and then box up the outputs.",
                    "label": 0
                },
                {
                    "sent": "And that's exactly what this code is doing.",
                    "label": 0
                },
                {
                    "sent": "So we have a class primitive and it's called method in Python.",
                    "label": 0
                },
                {
                    "sent": "Whenever you call one of these things.",
                    "label": 0
                },
                {
                    "sent": "Here's what it does.",
                    "label": 0
                },
                {
                    "sent": "It just goes through the arguments.",
                    "label": 0
                },
                {
                    "sent": "It says.",
                    "label": 0
                },
                {
                    "sent": "Hey, if this node is an argument then unbox it right place in the Argylls list the actual value.",
                    "label": 0
                },
                {
                    "sent": "So in optimization, if we have if we know ahead of time that this Jacobian is 0, but otherwise just append that to the parents, then this is self fund is the underlying NUM py function, right?",
                    "label": 0
                },
                {
                    "sent": "So we unbox the values.",
                    "label": 0
                },
                {
                    "sent": "Here we applied self dot fun to the unboxed values to get the result value, and then we're returning a new node for boxing up.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "There are a few things that I have hidden here which I will now reveal having to do with these progenitors, so this is actually something to do with higher order auto diff.",
                    "label": 0
                },
                {
                    "sent": "Turns out that when you're doing higher order audit, if there could be many traces that are happening at once and in the very first version of AutoCAD if you get check out the original autograph like there's some Git command.",
                    "label": 0
                },
                {
                    "sent": "I thought I had it on the slide.",
                    "label": 0
                },
                {
                    "sent": "Look at the very first check in.",
                    "label": 0
                },
                {
                    "sent": "It's a version that seems like it supports higher order audit.",
                    "label": 0
                },
                {
                    "sent": "It's only 80 lines actually.",
                    "label": 0
                },
                {
                    "sent": "But the way it does it is by sort of nesting the boxing of nodes, and it turns out that there are ways you can break that when you're doing crazy combinations of.",
                    "label": 0
                },
                {
                    "sent": "Traces start and stop.",
                    "label": 0
                },
                {
                    "sent": "So in fact, I think a better way to do this is we only ever have one graph and then we track these progenitors.",
                    "label": 0
                },
                {
                    "sent": "The list of progenitors basically saying for any node why we can ask that node?",
                    "label": 0
                },
                {
                    "sent": "Why were you boxed right?",
                    "label": 0
                },
                {
                    "sent": "Someone started tracing someone box to value and pass it into a function.",
                    "label": 0
                },
                {
                    "sent": "That's the progenitor, like the first thing that got boxed.",
                    "label": 0
                },
                {
                    "sent": "Everything else is not a progenitor, and we can look at any node and say why were you boxed.",
                    "label": 0
                },
                {
                    "sent": "And with that, let's us do is basically says when we have many iterations of this tracing going on, we can essentially pull out the relevant part Tenniel and trace just by taking sort of looking at the subset of nodes in the graph that have the right projector list.",
                    "label": 0
                },
                {
                    "sent": "So it's an optimization that's a high level.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing.",
                    "label": 0
                },
                {
                    "sent": "And then here's forward pass.",
                    "label": 0
                },
                {
                    "sent": "So this is what we do to trace a function given a function and some Args, inquiries to evaluate it on and the argument number that we want to sort of trace what primitives get applied to that to that argument.",
                    "label": 0
                },
                {
                    "sent": "This is all the code.",
                    "label": 0
                },
                {
                    "sent": "We just make a new progenitor.",
                    "label": 0
                },
                {
                    "sent": "That's like a new node, but then it just marks it as a as a progenitor.",
                    "label": 0
                },
                {
                    "sent": "We plug that into argument list.",
                    "label": 0
                },
                {
                    "sent": "And then we say you know where that's an active progenitor and then we just evaluate the function with that box value.",
                    "label": 0
                },
                {
                    "sent": "So we sort of box up on value and then drop it into the function and then as it's calling NUM PY functions and other things, it's going to be building up this graph.",
                    "label": 0
                },
                {
                    "sent": "And then when we get out of the value of the function of the end node, right?",
                    "label": 0
                },
                {
                    "sent": "We sort of box the thing and then dropped it in.",
                    "label": 0
                },
                {
                    "sent": "We don't know what's going on.",
                    "label": 0
                },
                {
                    "sent": "It comes out and it's still a node, and we know that's the sort of start node in the end note of our of the graph of the function evaluated at these arguments.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In pictures, what does this look like?",
                    "label": 0
                },
                {
                    "sent": "We would start node X that we box up.",
                    "label": 0
                },
                {
                    "sent": "We passed into a function, some user defined function may be the first thing it does, is it calls an umpire function that will call capital A here grace.",
                    "label": 0
                },
                {
                    "sent": "Then maybe it prints to the screen and you know there's an.",
                    "label": 0
                },
                {
                    "sent": "If some system calls, who knows and then, but the next thing that gets applied to a or any existing nodes is this B function right?",
                    "label": 0
                },
                {
                    "sent": "And so on until we pop out of that forward pass and we get this end, no doubt.",
                    "label": 0
                },
                {
                    "sent": "That's tracing, it's an entirely of that racing so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, I just drew for you a chain, but this tracing just works with tags, so we can drop in a function, and dropping an ode to a user function and then we get out of trace that might have this kind of DAG structure.",
                    "label": 0
                },
                {
                    "sent": "An important thing to note.",
                    "label": 0
                },
                {
                    "sent": "The difference between this kind of graph and a tensor flow graph is the tensor flow.",
                    "label": 0
                },
                {
                    "sent": "Graphs have control flow in them, right?",
                    "label": 1
                },
                {
                    "sent": "What would happen if there's a while loop in user code?",
                    "label": 0
                },
                {
                    "sent": "With this implementation.",
                    "label": 0
                },
                {
                    "sent": "Any thoughts, any hands?",
                    "label": 0
                },
                {
                    "sent": "A loop in the graph, not quite.",
                    "label": 0
                },
                {
                    "sent": "So I heard someone say it gets unrolled, right?",
                    "label": 0
                },
                {
                    "sent": "So we're actually just tracking the printers that applied.",
                    "label": 0
                },
                {
                    "sent": "We don't know what Python syntax you're using.",
                    "label": 0
                },
                {
                    "sent": "You could be using exceptions for control flow and whatever.",
                    "label": 0
                },
                {
                    "sent": "We don't care, we just care about what functions got applied to produce the output.",
                    "label": 0
                },
                {
                    "sent": "So if you add a while loop or any control flow we don't see it.",
                    "label": 0
                },
                {
                    "sent": "We just see the effect that that had.",
                    "label": 0
                },
                {
                    "sent": "And I guess you know a key here is that we are going to redo this tracing for every Vijay P for every grad that we evaluate.",
                    "label": 0
                },
                {
                    "sent": "So once we're done with this graph.",
                    "label": 0
                },
                {
                    "sent": "This graph is just representing what functions got applied to your input.",
                    "label": 0
                },
                {
                    "sent": "This one evaluation.",
                    "label": 0
                },
                {
                    "sent": "But on the next evaluation we're going to produce another graph.",
                    "label": 0
                },
                {
                    "sent": "Right, so that's what gives it flexibility.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We don't have to.",
                    "label": 0
                },
                {
                    "sent": "Also makes it easy so we don't have to think about differentiating control flow structures.",
                    "label": 0
                },
                {
                    "sent": "OK, that was tracing.",
                    "label": 0
                },
                {
                    "sent": "Let's move on to defining of Egypt for every primitive.",
                    "label": 0
                },
                {
                    "sent": "This one is actually super easy.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just some review.",
                    "label": 0
                },
                {
                    "sent": "The basic problem we have is we have some link in this graph, right?",
                    "label": 0
                },
                {
                    "sent": "It looks like we had some temporary variable X, some input to a function and we got an output by applying A to it right and then sort of the recursive thing that we have to solve is we have some derivative information at the output of our function.",
                    "label": 0
                },
                {
                    "sent": "That's like the derivative of our loss.",
                    "label": 0
                },
                {
                    "sent": "The partial derivative arlos our final output with respect to this temporary a right?",
                    "label": 0
                },
                {
                    "sent": "So we have that derivative information with respect to a the output.",
                    "label": 0
                },
                {
                    "sent": "We want to pull that back to be derivative information with respect to X, right?",
                    "label": 0
                },
                {
                    "sent": "How do we take a DYDA and make out of it DYDX?",
                    "label": 0
                },
                {
                    "sent": "This is what we said before.",
                    "label": 0
                },
                {
                    "sent": "This is what you know why we sort of use this fraction notation.",
                    "label": 0
                },
                {
                    "sent": "What we need to do is take this vector and apply this vector Jacobian product operator.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this little guy sort of including the dot, this linear operator that we're applying to this vector.",
                    "label": 0
                },
                {
                    "sent": "We're calling a vector Jacobian product operator, and that's the thing we have to know given sort of the partial of our final output with respect to a.",
                    "label": 0
                },
                {
                    "sent": "That's the thing that we need to apply to pull that back to be a derivative are output respect to the input to this function.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the recursive thing that we need to solve.",
                    "label": 0
                },
                {
                    "sent": "This is Step 2.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that ingredient list that I told you so here's some examples of GPS in autograde.",
                    "label": 0
                },
                {
                    "sent": "So for 10 inch for example, for the hyperbolic tangent, this thing is just saying hey given sort of G is the name for the vector, the derivative information vector that we're trying to pull back.",
                    "label": 0
                },
                {
                    "sent": "And here is, by the way, the answer to the function that got applied and the input, which is just X.",
                    "label": 0
                },
                {
                    "sent": "Here is the function that evaluates the vector Jacobian product operator right?",
                    "label": 0
                },
                {
                    "sent": "It's just G divided by NP.",
                    "label": 0
                },
                {
                    "sent": "Coach squared, right?",
                    "label": 0
                },
                {
                    "sent": "So you can see this is not instantiating a Jacobian matrix, right?",
                    "label": 0
                },
                {
                    "sent": "It's just saying oh to apply this.",
                    "label": 0
                },
                {
                    "sent": "This is a diagonal Jacobian.",
                    "label": 0
                },
                {
                    "sent": "Is a diagonal operator, and so I'm just going to do some elementwise stuff to G. Questions.",
                    "label": 0
                },
                {
                    "sent": "We also have crazier things.",
                    "label": 0
                },
                {
                    "sent": "It's really fun to look through the NUM py.",
                    "label": 0
                },
                {
                    "sent": "Grads file.",
                    "label": 0
                },
                {
                    "sent": "We have things like SVD, you know.",
                    "label": 0
                },
                {
                    "sent": "Sorting, basically I think almost all of NUM py we have wrapped so you can use and call to your heart's content.",
                    "label": 0
                },
                {
                    "sent": "And so we've defined these primitives for essentially every non function and some subset of sci-fi functions that we've used in our research.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Those are the 1st two ingredients.",
                    "label": 0
                },
                {
                    "sent": "Now we just have to compose the JPS backward which.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Symbolically is really simple, right?",
                    "label": 0
                },
                {
                    "sent": "So we have this trace.",
                    "label": 0
                },
                {
                    "sent": "Of our function evaluation and we know that for every link we have a vector Jacobian product operator.",
                    "label": 0
                },
                {
                    "sent": "So how do we pull starting from this partial derivative of the output which will just initialized to one which will be like partial Y partial lie?",
                    "label": 0
                },
                {
                    "sent": "How do we pull that back all the way to the input when we do it in steps?",
                    "label": 0
                },
                {
                    "sent": "First we want to use the VJP for the function D right to pull this guy back here.",
                    "label": 0
                },
                {
                    "sent": "So we applied the BJP for this function and pulled that information back to get us partial partial see right?",
                    "label": 0
                },
                {
                    "sent": "And then we apply the JP for C. Pull that back to be this derivative, and so on until we get the derivative of the spec to our input that we were after.",
                    "label": 0
                },
                {
                    "sent": "So here's some.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cool higher order auto diff.",
                    "label": 0
                },
                {
                    "sent": "Just works with the stuff I've already shown you arbitrary higher order audit if you don't need anything else.",
                    "label": 1
                },
                {
                    "sent": "And the magic is that the backward pass, right when we're applying these Vijay PS because we defined the JPS right, these vector Jacobian products in terms of other primitives that autograph already knows about.",
                    "label": 1
                },
                {
                    "sent": "That means that the VJP pass itself is just more applications of NUM py that we can trace.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, when we were doing this back before, maybe someone else is tracing us, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe there's some higher order audit if some Hessian vector products in a truncated Newton optimizer.",
                    "label": 0
                },
                {
                    "sent": "So basically, while we were doing that backward pass, it could be that everytime were applying of JP, someone else was building a trace of what we were doing right?",
                    "label": 0
                },
                {
                    "sent": "And this might you know these things might not be one to one.",
                    "label": 0
                },
                {
                    "sent": "Maybe the VJP for the B function is quite complicated.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But basically someone else could have an end node sitting out there and we don't know.",
                    "label": 0
                },
                {
                    "sent": "So that's how higher order auto works.",
                    "label": 0
                },
                {
                    "sent": "Basically, when you call Grada Brad, this is what's going on.",
                    "label": 0
                },
                {
                    "sent": "Let me show.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The code that implements this and then the grad function, which is what we're after, which can be written entirely in terms of what's on this slide and the next slide is grad, so here's a backward pass we had forward pass, which built the trace right, and then we said for every element in the trace we defined of JP.",
                    "label": 0
                },
                {
                    "sent": "Now we just have to do that pullback along the graph.",
                    "label": 0
                },
                {
                    "sent": "The details aren't too important.",
                    "label": 0
                },
                {
                    "sent": "Let me just highlight a few things where the forward pass gave us the start node in the end.",
                    "label": 0
                },
                {
                    "sent": "Node G is sort of the grading they were trying to pull back.",
                    "label": 0
                },
                {
                    "sent": "Think of is just a one.",
                    "label": 0
                },
                {
                    "sent": "For example, we're just doing gradients.",
                    "label": 0
                },
                {
                    "sent": "So we have started node.",
                    "label": 0
                },
                {
                    "sent": "This line is says for node in Topo sort.",
                    "label": 1
                },
                {
                    "sent": "So this just says topless orthograph.",
                    "label": 0
                },
                {
                    "sent": "This will be iterating it over, iterating over it in a valid order from sort of the output to the input.",
                    "label": 0
                },
                {
                    "sent": "We just sort of grab.",
                    "label": 0
                },
                {
                    "sent": "We have a Dictionary of the gradients keyed by nodes.",
                    "label": 0
                },
                {
                    "sent": "The important thing is that we just grab sort of the sort of gradient information that's coming in that we want to apply the JP two.",
                    "label": 0
                },
                {
                    "sent": "We unpack the recipe and we just say for every input argument to this function, apply the functions VJP with respect to that argument number.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is just going to go.",
                    "label": 0
                },
                {
                    "sent": "It's going to traverse the graph in the top of sort and just apply all these.",
                    "label": 0
                },
                {
                    "sent": "When we did FJP that's what it was setting up.",
                    "label": 0
                },
                {
                    "sent": "There's something else you need here that I think other libraries like Pytorch probably don't do.",
                    "label": 0
                },
                {
                    "sent": "But if you do indexing for example, into your arrays right?",
                    "label": 0
                },
                {
                    "sent": "And you pop out these little sub arrays and pass them on this code actually does some nice like mutating assignment to make all that quite fast.",
                    "label": 0
                },
                {
                    "sent": "So we don't instantiate a bunch of extra memory.",
                    "label": 0
                },
                {
                    "sent": "So there's some nice tiny elements of the design that I don't have time to go into this backward pass, super simple.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then here's grad so grad.",
                    "label": 0
                },
                {
                    "sent": "It turns out, remember how I said like everything is really just VJS Vectorscope products in reverse mode and we're just about composing VIPs so grad is actually a tiny function.",
                    "label": 0
                },
                {
                    "sent": "Extra like you know error checking stuff in the main code, but all this does is it calls this make VJP function and that's really like the key to autograde.",
                    "label": 0
                },
                {
                    "sent": "That's like saying from the primitive JPS build up a composite of JP for some function.",
                    "label": 0
                },
                {
                    "sent": "And here's make JP make JP just calls forward pass.",
                    "label": 0
                },
                {
                    "sent": "If the output is not a node, that means that the output didn't depend on the input, so it's just zero.",
                    "label": 0
                },
                {
                    "sent": "Just return 0, but if the output is a node hears VJP we called forward pass to get started and then just call backward pass.",
                    "label": 0
                },
                {
                    "sent": "I've already shown you those things, so this is a composite VJP function that apply to any gradient information we want to pull back.",
                    "label": 0
                },
                {
                    "sent": "So in particular when we do grad that just gets the JP and then calls BJP on ones.",
                    "label": 0
                },
                {
                    "sent": "OK. That's it, that is all.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grad so just to summarize doors, I showed you the tracing of composition in terms of what primitive functions are applied and basically all you need to know there is that there's a node data type.",
                    "label": 0
                },
                {
                    "sent": "There's a primitive, which is a wrapper to any numerical library you want.",
                    "label": 0
                },
                {
                    "sent": "In this case NPI, and then this function forward pass, which was you know, six liner or something like that, and this just built a DAG of what operations were applied to the input when you evaluate it at one time.",
                    "label": 0
                },
                {
                    "sent": "Then we just defined groups for each primitive, right?",
                    "label": 1
                },
                {
                    "sent": "So we covered all of them, pie, and then some fraction of \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And then it was just composing JP backward, which meant we had this backward pass and make the JPN grad was written in terms of that, right?",
                    "label": 0
                },
                {
                    "sent": "So just racing and then once we have GPS for everything, it's just about walking the graph and applying the GPS, yeah?",
                    "label": 0
                },
                {
                    "sent": "Boxing.",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "Yeah, great question.",
                    "label": 0
                },
                {
                    "sent": "So the question was how does the so there's some runtime overhead here, right?",
                    "label": 0
                },
                {
                    "sent": "That ahead of time system like tensor flow does not have to incur in particular because we have to trace what's going on with the function.",
                    "label": 0
                },
                {
                    "sent": "There's some overhead, whereas Tensorflow already knows what's going to happen with the function it inspected it ahead of time, doesn't have to sort of re inspected every time instead of reading source, we're sort of probing the function by just tracing what happens.",
                    "label": 0
                },
                {
                    "sent": "What are the overheads associated with that?",
                    "label": 0
                },
                {
                    "sent": "So it depends on the workload.",
                    "label": 0
                },
                {
                    "sent": "How big those overheads are, the workloads that we write down, I think, are dominated by numerical kernels.",
                    "label": 0
                },
                {
                    "sent": "That's why GPU's matter so much right there, not dominated by overheads, were spending all of our time in like mammals and cons, which are basically like mammals.",
                    "label": 0
                },
                {
                    "sent": "And those things have the property that the number of flops scales super linearly with the size of the input.",
                    "label": 0
                },
                {
                    "sent": "So if we're making our networks bigger, we can always sort of Max out the arithmetic intensity of our machines.",
                    "label": 0
                },
                {
                    "sent": "So basically I think that for a lot of common workloads doesn't mean all workloads.",
                    "label": 0
                },
                {
                    "sent": "I think for a lot of common workloads we're spending all of our time these numerical kernels.",
                    "label": 0
                },
                {
                    "sent": "And overheads, overheads of the boxing unboxing?",
                    "label": 0
                },
                {
                    "sent": "I showed you this is actually pretty simple operation like these rappers are quite thin.",
                    "label": 0
                },
                {
                    "sent": "Yes, there massaging pointers, but we're sort of never touching the values of these big arrays except through primitive kernels.",
                    "label": 0
                },
                {
                    "sent": "Tracing overhead is just, you know, clicking together linking up this graph data structure.",
                    "label": 0
                },
                {
                    "sent": "So there is an overhead.",
                    "label": 0
                },
                {
                    "sent": "I think you can write programs where that overhead is substantial, but I think for a lot of the common workloads in.",
                    "label": 0
                },
                {
                    "sent": "Deep learning, in particular.",
                    "label": 0
                },
                {
                    "sent": "We're spending all of our time in numerical kernels and then anytime we're not spending numerical kernels, is probably because of memory bandwidth, not like interpreter overhead.",
                    "label": 0
                },
                {
                    "sent": "In fact, I have reason to believe that C Python is actually not gets a bad rap, but it's actually quite fast and competitive with so.",
                    "label": 0
                },
                {
                    "sent": "Tensorflow also has interpreter overhead, not for building this runtime data structure for the tracing, but it has other interpreter overheads, and I think you know, I think these things are actually small for common workloads.",
                    "label": 0
                },
                {
                    "sent": "But not for everything.",
                    "label": 0
                },
                {
                    "sent": "It's still worth thinking about how to make this process faster.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "When we trace, you know there's this flexibility that we're getting with this tracing.",
                    "label": 0
                },
                {
                    "sent": "But there are some functions that we don't need to trace over and over, 'cause there is no control flow, right?",
                    "label": 0
                },
                {
                    "sent": "Or there's no random number generator being accessed.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you just want the gradient of some loss function and you know your predictions came from some inception V3 every time we're tracing that, it's it comes out the same, right?",
                    "label": 0
                },
                {
                    "sent": "So I think there's actually a lot of interesting work to be done here in terms of balancing the flexibility, but then minimizing any costs associated with flexibility.",
                    "label": 0
                },
                {
                    "sent": "Picking you up.",
                    "label": 0
                },
                {
                    "sent": "Who are?",
                    "label": 0
                },
                {
                    "sent": "Remember to get.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there are trade offs, so the question is just, you know, is there more things we can do here to trade off what's done ahead of time or not.",
                    "label": 0
                },
                {
                    "sent": "And yeah, this is sort of the most dynamic approach, but I think it's quite interesting to think about.",
                    "label": 0
                },
                {
                    "sent": "You know, doing something sort of ahead of time to the extent we can.",
                    "label": 0
                },
                {
                    "sent": "Be happy to talk more about that after.",
                    "label": 0
                },
                {
                    "sent": "I think there's actually a lot of interesting stuff to do there.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So those autograph let me just highlight some tradeoffs in forward versus reverse mode.",
                    "label": 1
                },
                {
                    "sent": "We don't talk as much about forward mode because it's nowhere near as relevant to us doing optimization, especially deep learning kind of optimization.",
                    "label": 1
                },
                {
                    "sent": "So reverse mode has to trace the program's execution, right?",
                    "label": 0
                },
                {
                    "sent": "That's where we just talked about the tracing and it has to record all of the intermediate values at least direct implementation records.",
                    "label": 0
                },
                {
                    "sent": "All the intermediate values.",
                    "label": 0
                },
                {
                    "sent": "That means that we have a memory cost for reverse mode.",
                    "label": 0
                },
                {
                    "sent": "This is not about autograph this traverse mode.",
                    "label": 0
                },
                {
                    "sent": "We have a memory cost that scales like the depth of the program.",
                    "label": 1
                },
                {
                    "sent": "OK, there's some tricks that maybe I'll have time to talk about in a couple of minutes that might be able to trade off time in memory, but basically reverse mode because we're recording.",
                    "label": 0
                },
                {
                    "sent": "You know what functions got applied and where to linearize them.",
                    "label": 0
                },
                {
                    "sent": "It has this memory overhead that scales like the depth of the program.",
                    "label": 0
                },
                {
                    "sent": "Forward mode does not have this overhead.",
                    "label": 0
                },
                {
                    "sent": "Because essentially I didn't go into the details, but basically as your evaluating the function forward, you can be applying the primitive Jacobian vector product.",
                    "label": 0
                },
                {
                    "sent": "The JV PS as you go right, so it has a constant factor memory overhead, essentially doubling the instantaneous memory cost.",
                    "label": 0
                },
                {
                    "sent": "Maximum instantaneous memory cost of your of your program.",
                    "label": 0
                },
                {
                    "sent": "You don't have to like record anything, you're just evaluating as you go along.",
                    "label": 0
                },
                {
                    "sent": "But as we talked about before, the huge cost is that for functions from RN to R it requires end calls to get the gradient.",
                    "label": 0
                },
                {
                    "sent": "And that's just really expensive in fact.",
                    "label": 0
                },
                {
                    "sent": "Numerical differentiation right where you just add epsilon to your input.",
                    "label": 0
                },
                {
                    "sent": "It has this problem as well.",
                    "label": 0
                },
                {
                    "sent": "You have to tweak each input separately, so that's actually the problem with numerical differentiation, which Google explained to me one time.",
                    "label": 0
                },
                {
                    "sent": "I don't think it's so much that it's like not accurate at machine precision, because maybe we want to do some smoothing, like why you know maybe want to smooth out the gradient over some interval.",
                    "label": 0
                },
                {
                    "sent": "The real cost is that numerical differentiation.",
                    "label": 0
                },
                {
                    "sent": "Is like forward mode in this property that you have to make any calls to the function and it's too slow.",
                    "label": 0
                },
                {
                    "sent": "That way though there are some things you could call it on random vectors and then try to reconstruct the gradient right?",
                    "label": 0
                },
                {
                    "sent": "And this is the evolutionary strategies type stuff.",
                    "label": 0
                },
                {
                    "sent": "So even more fun is that Autograde has a fully compatible forward mode extension written by Jamie Townsend.",
                    "label": 0
                },
                {
                    "sent": "Then get here and that means you can mix together forward mode in reverse mode to your heart's content.",
                    "label": 0
                },
                {
                    "sent": "In AutoCAD you can call forward grad of grad of another for grad at as many times as you want, so it's all sort of fully closed and nice.",
                    "label": 0
                },
                {
                    "sent": "And this forward mode implementation doesn't have this memory overhead, so you can play games there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's pretty much it for the main goals until the advanced stuff.",
                    "label": 0
                },
                {
                    "sent": "So I told you the, you know mathematically, what are we trying to do breakdown?",
                    "label": 0
                },
                {
                    "sent": "You know jacobian's or derivatives of composite functions into primitive ones, and then I told you about autographs implementation, which is the same strategies employed by Mimpi and Pytorch.",
                    "label": 0
                },
                {
                    "sent": "Now I think autographs implementation is still a bit better, specially with respect to higher order out.",
                    "label": 0
                },
                {
                    "sent": "If it's really quite simple and it's fully closed tracing object in Python.",
                    "label": 1
                },
                {
                    "sent": "This is separate from.",
                    "label": 0
                },
                {
                    "sent": "This also separates audited from what numerical kernel library you're using.",
                    "label": 0
                },
                {
                    "sent": "You can wrap anything an autograph doesn't know about what, whether the you know, raise your operating or CPU backed or GPU back, or what the kernels are doing right?",
                    "label": 0
                },
                {
                    "sent": "It's just at a higher level.",
                    "label": 0
                },
                {
                    "sent": "Which is, I think, a nice property, yeah?",
                    "label": 0
                },
                {
                    "sent": "This computer's GPU thing.",
                    "label": 0
                },
                {
                    "sent": "There isn't any like GPU back end right now for this though right GPU back end for autograde.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the question was, is there a GPU back end?",
                    "label": 0
                },
                {
                    "sent": "Well I have a branch where I wrap some coopi primitives.",
                    "label": 0
                },
                {
                    "sent": "Just kind of fun and then this is a several months ago.",
                    "label": 0
                },
                {
                    "sent": "I think I made an update and then I just sort of got a little annoyed using Coopi which is a numerical thing not auditing.",
                    "label": 0
                },
                {
                    "sent": "So a nice property of that is that because autographs, NUM, PY and some of sci-fi.",
                    "label": 0
                },
                {
                    "sent": "If you rap package like coopi, you can also wrap the primitives that translate from an ND array to a coupe or a.",
                    "label": 0
                },
                {
                    "sent": "And back, and those are actually those for management pair.",
                    "label": 0
                },
                {
                    "sent": "So you can actually build a system that has GPU backed kernels for example for your neural net, and then if you want to go onto the go into NUM PY and do CPU back kernels for like some crazy problems.",
                    "label": 0
                },
                {
                    "sent": "Graphical models thing you can do that as well.",
                    "label": 0
                },
                {
                    "sent": "So the short answer is not right now, it's something that I'm interested in developing and I think several other people are as well.",
                    "label": 0
                },
                {
                    "sent": "Basically you can take any GPU backed.",
                    "label": 0
                },
                {
                    "sent": "Numerical library an rapid in autograde, basically using the tools that I showed you right.",
                    "label": 0
                },
                {
                    "sent": "You just have to wrap everything as a primitive and then define a node type for your tensor.",
                    "label": 0
                },
                {
                    "sent": "So there's an array node that we have.",
                    "label": 0
                },
                {
                    "sent": "You basically have to make one of those so check out the Coop, I branch and if you email me I can send you a link to it.",
                    "label": 0
                },
                {
                    "sent": "So my question was about this differentiation.",
                    "label": 0
                },
                {
                    "sent": "So can you like explain a little bit more so?",
                    "label": 0
                },
                {
                    "sent": "Like if I wanted to make an RNN?",
                    "label": 0
                },
                {
                    "sent": "And not you complete backup time, but you scored another translation.",
                    "label": 0
                },
                {
                    "sent": "Would I be like calling some forward mode thing at every time step or I would just work like I do for loop over the time steps and everytime I look forward one time?",
                    "label": 0
                },
                {
                    "sent": "So the question was you know how to use forward mode with friends?",
                    "label": 0
                },
                {
                    "sent": "Let's talk about it after.",
                    "label": 0
                },
                {
                    "sent": "But basically you solve this problem where you have to iterate over all of your parameters.",
                    "label": 0
                },
                {
                    "sent": "Actually there's some I've heard you know how we call reverse mode audited backdrop.",
                    "label": 0
                },
                {
                    "sent": "I've heard a term for forward mode.",
                    "label": 0
                },
                {
                    "sent": "I think it was called real time recurrent learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah OK, definitely 80s.",
                    "label": 0
                },
                {
                    "sent": "It was in a lecture by Russ Tedrick real time recurrent learning I think is in this context like foreign mode for sort of recurrent type problems.",
                    "label": 0
                },
                {
                    "sent": "Fun jargon, it's not efficient, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think it's not efficient for the same reason, forward mode is not efficient, but it's more biologically possible, perhaps or something?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, that was the justification for it.",
                    "label": 0
                },
                {
                    "sent": "I heard I see I see I see.",
                    "label": 0
                },
                {
                    "sent": "Cool, so let me spend some time with you until 12:30.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, good question.",
                    "label": 0
                },
                {
                    "sent": "Awesome.",
                    "label": 0
                },
                {
                    "sent": "10 minutes for questions, maybe spend, so I'll spend 10 or 13 minutes.",
                    "label": 0
                },
                {
                    "sent": "Talking about some advanced auto diff.",
                    "label": 0
                },
                {
                    "sent": "I'll just I'll go through it in order so check.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Checkpoint is interesting because it allows us to trade off the space requirements that reverse mode auto DIFF has.",
                    "label": 0
                },
                {
                    "sent": "And just, you know, drastically reduce the amount of storage required at the cost of some extra flop overhead.",
                    "label": 0
                },
                {
                    "sent": "So if you're more memory limited on your GPU, your system.",
                    "label": 0
                },
                {
                    "sent": "But you can do a lot of flops.",
                    "label": 0
                },
                {
                    "sent": "Maybe this sort of thing makes sense.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that when we trace out a big, let me see.",
                    "label": 0
                },
                {
                    "sent": "So we trace out a big computation graph.",
                    "label": 0
                },
                {
                    "sent": "We are naively sort of storing all of the activations.",
                    "label": 0
                },
                {
                    "sent": "If you are if you like, but all the values that happened in that in that forward pass.",
                    "label": 0
                },
                {
                    "sent": "And the reason we need those values is because we need to tell each primitive JP where it's supposed to be linearized.",
                    "label": 0
                },
                {
                    "sent": "This is sort of getting the linearization point the bias points of the system.",
                    "label": 0
                },
                {
                    "sent": "That means that our memory storage scales like the depth of this program.",
                    "label": 0
                },
                {
                    "sent": "So like the extent of this of this chain.",
                    "label": 0
                },
                {
                    "sent": "I you might.",
                    "label": 0
                },
                {
                    "sent": "Think so.",
                    "label": 0
                },
                {
                    "sent": "Here's the strategy for reducing the total amount of computation.",
                    "label": 0
                },
                {
                    "sent": "For example, let's say that these were two.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calls to the same function, right?",
                    "label": 0
                },
                {
                    "sent": "It had a lot of internal state.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is actually an optimizer internally or something right?",
                    "label": 0
                },
                {
                    "sent": "Or doing something expensive and we don't, you know, collect all of these things.",
                    "label": 0
                },
                {
                    "sent": "So why don't we just throw out?",
                    "label": 0
                },
                {
                    "sent": "Or rather, why don't we not store those values, right?",
                    "label": 0
                },
                {
                    "sent": "So when we trace, the function will start tracing recording all these values, But then when we're inside this purple box, this one function let's just not record the values that are happening will build the graph.",
                    "label": 0
                },
                {
                    "sent": "But just like don't don't record the actually excuse me, we won't even build the graph, will just not record these values and then we'll just get the output here we don't have any storage costs associated with what's in this purple box.",
                    "label": 0
                },
                {
                    "sent": "And then we go back into the purple box.",
                    "label": 0
                },
                {
                    "sent": "And we do the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "So we don't want to do any of those things now.",
                    "label": 0
                },
                {
                    "sent": "The total memory cost of our program is something like two or three nodes compared to.",
                    "label": 0
                },
                {
                    "sent": "I don't know 8 or 10.",
                    "label": 0
                },
                {
                    "sent": "Seems great.",
                    "label": 0
                },
                {
                    "sent": "What do we do on the back pass?",
                    "label": 0
                },
                {
                    "sent": "So now we have this this partial derivative we want to pull it back.",
                    "label": 0
                },
                {
                    "sent": "Now we want to pull it back through this purple box.",
                    "label": 0
                },
                {
                    "sent": "What do we do?",
                    "label": 0
                },
                {
                    "sent": "We don't have these intermediate values, but we can reconstruct them right?",
                    "label": 0
                },
                {
                    "sent": "Because we had its input.",
                    "label": 0
                },
                {
                    "sent": "Right and sort of in this land everything is functional programming, so we know what the input was.",
                    "label": 0
                },
                {
                    "sent": "We can just re run the function forward and build this graph and then back prop through it to get this value and then throw away everything.",
                    "label": 0
                },
                {
                    "sent": "Right, so this way we have to incur at most sort of 1 copy of the purple box's memory overhead.",
                    "label": 0
                },
                {
                    "sent": "When we hit the other purple box, we do the same thing we re instantiate by running it forward and then pull it back and then throw away stuff so.",
                    "label": 0
                },
                {
                    "sent": "That seems like a nice strategy.",
                    "label": 0
                },
                {
                    "sent": "Turns out we can implement this in like 4 lines of autograde.",
                    "label": 0
                },
                {
                    "sent": "Does anyone have any ideas?",
                    "label": 0
                },
                {
                    "sent": "I'll give you, I'll give some hints.",
                    "label": 0
                },
                {
                    "sent": "So the property we want this purple box to have is that we don't want to trace into it.",
                    "label": 0
                },
                {
                    "sent": "That's like the first thing.",
                    "label": 0
                },
                {
                    "sent": "Somehow we want to have a way to mark a function that says don't trace into it.",
                    "label": 0
                },
                {
                    "sent": "Instead, stop the tracing, just run it forwards.",
                    "label": 0
                },
                {
                    "sent": "And then restart the tracing afterwards.",
                    "label": 0
                },
                {
                    "sent": "Does that sound familiar?",
                    "label": 0
                },
                {
                    "sent": "Sounds like a primitive right?",
                    "label": 0
                },
                {
                    "sent": "That sounds like a permanent function, so in fact.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A way to implement checkpointing is to just wrap the function as a primitive.",
                    "label": 0
                },
                {
                    "sent": "Just look at this one line wrapped equals primitive fun and we return wrapped.",
                    "label": 1
                },
                {
                    "sent": "Right, what a primitive says, is don't treat this like a numerical kernel implemented in some other language.",
                    "label": 0
                },
                {
                    "sent": "You know who knows what?",
                    "label": 0
                },
                {
                    "sent": "Don't try to trace into it with your boxed values.",
                    "label": 0
                },
                {
                    "sent": "Just give it an unboxed value and just run it forward and don't do any recording.",
                    "label": 0
                },
                {
                    "sent": "And then here's the trick to get the VJP to work this function.",
                    "label": 0
                },
                {
                    "sent": "We're declaring a primitive, so we're saying don't do any tracing inside our function.",
                    "label": 0
                },
                {
                    "sent": "We're going to wrap.",
                    "label": 0
                },
                {
                    "sent": "We're going to write it JP as Oh yeah, just rerun the function and then apply that to the gradient, right?",
                    "label": 0
                },
                {
                    "sent": "So the property wanted is for no tracing to happen on the forward pass, but then on the backward pass, if anyone asks to backdrop through us, then we're going to rerun, reconstitute our values forward, and then back prop, and then throw everything away.",
                    "label": 1
                },
                {
                    "sent": "So this is actually checkpointing you can use this as a decorator.",
                    "label": 0
                },
                {
                    "sent": "You just at checkpoint your function and AutoCAD will not race into it.",
                    "label": 0
                },
                {
                    "sent": "That's kind of cool, OK?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another nice auto diff trick.",
                    "label": 0
                },
                {
                    "sent": "I actually so checkpointing.",
                    "label": 0
                },
                {
                    "sent": "Things except for some some iterations, autograph some iterations on some ideas that were well known in the automotive community.",
                    "label": 0
                },
                {
                    "sent": "Checkpointing Super well known.",
                    "label": 0
                },
                {
                    "sent": "I don't know of a reference for this idea, but I'm sure someone came up with it, but we just came up with it on the autograde issues tracker like a few weeks ago and it's a neat trick.",
                    "label": 0
                },
                {
                    "sent": "So the idea is once you have reverse mode in some sense you can get forward mode for free, or at least almost for free.",
                    "label": 0
                },
                {
                    "sent": "Hi and here's the function that does it now.",
                    "label": 0
                },
                {
                    "sent": "This is a little confusing.",
                    "label": 0
                },
                {
                    "sent": "We're calling make JP and they were calling me JP.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I guess so we're calling make VJP twice.",
                    "label": 0
                },
                {
                    "sent": "To get forward mode, and then there's this mysterious comment.",
                    "label": 0
                },
                {
                    "sent": "V JP JP is just JVP by linearity.",
                    "label": 1
                },
                {
                    "sent": "What's going on here?",
                    "label": 0
                },
                {
                    "sent": "So here's the idea in terms of sort of a chain graph, think of our function as implementing this mapping from X to Y. OK, this first JVP that we call.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, this first VJP that we call make JP is saying oh, I can take a vector and apply this system.",
                    "label": 0
                },
                {
                    "sent": "We just describe to get a Jacobian, transpose vector or vector Jacobian product operator.",
                    "label": 0
                },
                {
                    "sent": "That's a composition of these things.",
                    "label": 0
                },
                {
                    "sent": "I've transposed things and written as Jacobian transpose vector, but it's just a VJP.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is we want to get forward mode which is Jacobian vector product's.",
                    "label": 0
                },
                {
                    "sent": "But we've now built this function that evaluates Jacobian transpose vector products.",
                    "label": 0
                },
                {
                    "sent": "So if only we had a way, so this blue function is this linear function that implements the adjoint operator of the thing we want, the transpose that we want.",
                    "label": 0
                },
                {
                    "sent": "If only we had a thing that when we apply it to linear operators just gives us the transpose.",
                    "label": 0
                },
                {
                    "sent": "Right, so it turns out, remember Vector Jacobian products when you evaluate that on a linear function, right?",
                    "label": 0
                },
                {
                    "sent": "It's just going to be like taking your vector, transposing and hit by the Jacobian.",
                    "label": 0
                },
                {
                    "sent": "So if your function is linear, that's just going to be applying the Agile and this is going to be applying the transpose, so if we now make a VJP from this V, sorry, so let's build a consider the coding of this intermediate function, which is just this J transpose.",
                    "label": 0
                },
                {
                    "sent": "If we make a V JP that operates on some vector U.",
                    "label": 0
                },
                {
                    "sent": "This is going to apply the operator at JU, which is a Jacobian vector product.",
                    "label": 0
                },
                {
                    "sent": "And Moreover, because this function was linear, it doesn't matter what the value of the was.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter where we linearize a linear function, because it's just always the same, right?",
                    "label": 0
                },
                {
                    "sent": "So in fact, these links show you that this computation does not depend on any of the values.",
                    "label": 0
                },
                {
                    "sent": "Sorry, any of the values in this intermediate line, right?",
                    "label": 0
                },
                {
                    "sent": "This view was just a dummy variable.",
                    "label": 0
                },
                {
                    "sent": "In fact, here it's zeros.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter, they will depend on the on the forward like where we linearized forward function of course, but not a backward one.",
                    "label": 0
                },
                {
                    "sent": "So this will actually build a compositional way of evaluating Jacobian vector products.",
                    "label": 0
                },
                {
                    "sent": "That's what this function that gets returned is.",
                    "label": 0
                },
                {
                    "sent": "This V. JP, JP could have also called that JVP.",
                    "label": 0
                },
                {
                    "sent": "So one problem with this which is quite interesting is that we had to create these dummy values and propagate them.",
                    "label": 0
                },
                {
                    "sent": "We had to create zeros, ornans or whatever V and propagate it just to build this Jacobian, transpose vector or this VJP line.",
                    "label": 0
                },
                {
                    "sent": "So that we could then transpose it.",
                    "label": 0
                },
                {
                    "sent": "It turns out that with tensor flow, because it's in ahead of time system, you don't have to propagate dummy values.",
                    "label": 0
                },
                {
                    "sent": "So in fact this trick has no extra costs in tensor flow.",
                    "label": 0
                },
                {
                    "sent": "So here it is in tensor flow.",
                    "label": 0
                },
                {
                    "sent": "Except for the fact that like there's some things in tensor flow where it doesn't track the provenance perfectly, but once we clean those up, this will be a great implementation.",
                    "label": 0
                },
                {
                    "sent": "Afford mode in Tensorflow so intensively you give sort of you, name the inputs and outputs, right?",
                    "label": 0
                },
                {
                    "sent": "You refer to those instead of calling grad on a function like an autograt, you refer to the inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "So this is saying like hey, if function the outputs are wise.",
                    "label": 0
                },
                {
                    "sent": "The inputs are ex is and then DX is.",
                    "label": 0
                },
                {
                    "sent": "That's like the grading information at the input.",
                    "label": 0
                },
                {
                    "sent": "That vector that we want to apply a JVP too.",
                    "label": 0
                },
                {
                    "sent": "We want to push forward.",
                    "label": 0
                },
                {
                    "sent": "And here we're just calling.",
                    "label": 0
                },
                {
                    "sent": "We make a dummy variable that's the size of the outputs.",
                    "label": 0
                },
                {
                    "sent": "Here I'm just assuming it's a tensor.",
                    "label": 0
                },
                {
                    "sent": "Then we call TF gradients twice.",
                    "label": 0
                },
                {
                    "sent": "The first call applies to this dumb Evie.",
                    "label": 0
                },
                {
                    "sent": "Ann is just TF, gradients and wise indexes.",
                    "label": 0
                },
                {
                    "sent": "And then the second call is from G, which is a name for this output.",
                    "label": 0
                },
                {
                    "sent": "Here to V get that JP operator and apply it to DX is.",
                    "label": 0
                },
                {
                    "sent": "So, but the jarring, maybe the switch back from differentiating functions to think of drivers on the input and output values themselves.",
                    "label": 0
                },
                {
                    "sent": "But yeah, this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, forward mode in tensor flow in principle for free, and there's no like if you made a manual implementation of Ford mode, you would not do anything different.",
                    "label": 0
                },
                {
                    "sent": "You would in fact, right this exact same code, and this has the property of low minimal OS memory like we don't have to trace out the whole thing first, everything.",
                    "label": 0
                },
                {
                    "sent": "Cool, so that's forward from reverse.",
                    "label": 1
                },
                {
                    "sent": "Let me before going to questions.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to prime everyone with.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think an interesting thing going forward that we've seen start to pop up in several papers, including one that I worked on that I'm going to talk about after lunch.",
                    "label": 0
                },
                {
                    "sent": "That's the idea of differentiating sort of solutions or Optima, or fixed points of functions.",
                    "label": 0
                },
                {
                    "sent": "For example, there's a recent paper opt Nets that was saying like let's put optimization programs inside of our network is layers and then back problem efficiently.",
                    "label": 0
                },
                {
                    "sent": "Where efficiently means autograde can backdrop through an optimizer, but it has to trace the entire execution.",
                    "label": 0
                },
                {
                    "sent": "How do we not have to trace through the execution?",
                    "label": 0
                },
                {
                    "sent": "So here's the question.",
                    "label": 0
                },
                {
                    "sent": "If I have, let's say I have some parameters a, I think of those as just some, like exogeneous parameters and then I'm I have some function that I'm optimizing over X, right?",
                    "label": 0
                },
                {
                    "sent": "So it could be parameterising optimization problem and then I'm minimizing Exum searching over X like this could be a planning routine or something like that.",
                    "label": 0
                },
                {
                    "sent": "I'm defining then this function, so there's some like hand WAVY stuff about.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's multiple minima or something.",
                    "label": 0
                },
                {
                    "sent": "In fact I don't have notation for it, but the guarantee of this right argument, but I really mean just find a local minimum in this example.",
                    "label": 0
                },
                {
                    "sent": "In fact, just the local stationary point, perhaps so extra is going to be the value of X that solves this problem for some parameters day.",
                    "label": 0
                },
                {
                    "sent": "Then I can say, well, that's a function, right?",
                    "label": 0
                },
                {
                    "sent": "How do we differentiate through it?",
                    "label": 0
                },
                {
                    "sent": "As I said, if you implement this in Python auto gradual differentiate through it no problem, but it will trace through the entire computation.",
                    "label": 0
                },
                {
                    "sent": "Maybe instead we can use the fact that X star is an optimizer of F. Write and rewrite and somehow compute the Jacobian information.",
                    "label": 0
                },
                {
                    "sent": "The sort of joint information that we need.",
                    "label": 0
                },
                {
                    "sent": "From F from the properties of F and the fact that there's a local minimum, so here's another version of essentially the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's equation solving.",
                    "label": 0
                },
                {
                    "sent": "Solve some system equations parameterized by a bit over these variables X for X.",
                    "label": 1
                },
                {
                    "sent": "So we could say the text are now is some solution for A and we might ask how do we differentiate this X star?",
                    "label": 0
                },
                {
                    "sent": "Which one of these is more general, do you think?",
                    "label": 0
                },
                {
                    "sent": "Bottom, Why is that?",
                    "label": 0
                },
                {
                    "sent": "The top is a special case of that yeah, so so I guess we can always write down a system of equations that are like maybe.",
                    "label": 0
                },
                {
                    "sent": "At least necessary, but maybe even necessary and sufficient conditions for local optimality, right?",
                    "label": 0
                },
                {
                    "sent": "We can say, oh, find a stationary point like the gradient equals zero.",
                    "label": 0
                },
                {
                    "sent": "That's one equation.",
                    "label": 0
                },
                {
                    "sent": "And then we might even say second derivative test or something, so we can always write optimization.",
                    "label": 0
                },
                {
                    "sent": "We can reduce it to equation solving if we want solving these conditions for optimality.",
                    "label": 0
                },
                {
                    "sent": "For example in the OPNET paper they talk about solving the KKT conditions.",
                    "label": 0
                },
                {
                    "sent": "That was saying, like how the optimization problem think about the KKT conditions so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a general way to think about this.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm just priming use of.",
                    "label": 0
                },
                {
                    "sent": "The details aren't aren't too important, but let's say let's take the second form.",
                    "label": 0
                },
                {
                    "sent": "This equation solving form.",
                    "label": 0
                },
                {
                    "sent": "How do we think about differentiating X star of a given this information, assuming the derivative exists and some technical conditions distributed to exist, we can say something that must satisfy by differentiating both sides of this thing with respect to a.",
                    "label": 0
                },
                {
                    "sent": "So using the chain rule that looks like this and then we can rewrite this using an inverse.",
                    "label": 0
                },
                {
                    "sent": "They're going to assume exists.",
                    "label": 0
                },
                {
                    "sent": "With some regularity conditions, basically say, says the derivative survey can be written in terms of derivative information on G. The way to read this is basically we're linearizing our system of equations at our solution, and that linear informit linearized information is, although we really need to do back prop.",
                    "label": 0
                },
                {
                    "sent": "However, we do have to set up this linear system solve.",
                    "label": 0
                },
                {
                    "sent": "So differentiating solutions and Optima think I can write that as solving linear systems.",
                    "label": 0
                },
                {
                    "sent": "They are indeed the linearized versions of the equations.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, if I've already run a solver, solve these equations or optimize my problem, maybe that solver can give me information about how to solve this system and implicit way.",
                    "label": 0
                },
                {
                    "sent": "Or maybe there are some values that that solver returns to me that already contains some of this information.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the optinet paper, if you use a primal dual solver.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the LaGrange multipliers are.",
                    "label": 0
                },
                {
                    "sent": "You can interpret them as a sensitivity of the optimal optimal value with respect to some constraints, like if I change the constraints a little bit, how does my cost change that's in the low range multipliers, so maybe we shouldn't be surprised that you know if we have LaGrange multiplier estimates, we can use them in solving this system, so I'm going to stop.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but there's all kinds of cool stuff.",
                    "label": 0
                },
                {
                    "sent": "Let me just say thanks to everyone who's helped with this and made slides and sort of thing broken.",
                    "label": 0
                },
                {
                    "sent": "Jeff Perlmutter in particular had affect on Google, who taught me everything I know about auto diff.",
                    "label": 0
                },
                {
                    "sent": "So yeah, thanks for listening.",
                    "label": 0
                }
            ]
        }
    }
}