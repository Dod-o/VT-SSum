{
    "id": "yswrn6fsacqo3ggt77pgigca6ndbxxk4",
    "title": "Learning Monotone Nonlinear Models using the Choquet Integral",
    "info": {
        "author": [
            "Weiwei Cheng, Mathematik und Informatik, Philipps-Universit\u00e4t Marburg"
        ],
        "published": "Oct. 3, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd2011_cheng_learning/",
    "segmentation": [
        [
            "This work is a bonbon learning in monotone nonlinear models using the Crockett Integral, so I leave it to you OK. How are you?",
            "Thank you for coming and today I'm going to tell you something about our recent research on learning monotone nonlinear models using Szocki integral.",
            "This is joint work with my colleague Ali foreign tyranny just off chansky and my PhD advisor Alcohol Mile."
        ],
        [
            "Right, so anyone who work with machinery applications here should know that incorporating background knowledge into a learning process is a very important task.",
            "Is often allow you to model your problem better.",
            "In this talk, we will focus on very important type of background knowledge.",
            "The lamley is monotonicity.",
            "So sometimes ensuring multiplicity can not only improve you performance and very often in application it is hard constraint.",
            "For example, if you try to build a model to predict the chance that patients suffer from Len concern, then you probably want to have something like the higher the tobacco consumption and the more likely this guy will have a cancer.",
            "And if you model doesn't deliver this, and probably a medical doctor wouldn't like to trust your model and refuse to use it."
        ],
        [
            "How to ensure multiplicity for a linear model is quite simple because it is directly correspond to the son of your Corporation.",
            "And also now model is very easy to interpret because the influence of each predictor is directly reflected by the corresponding coefficient.",
            "But the real world, the problem of a linear model is in a lot of real world applications.",
            "It is just simply not powerful enough, not flexible enough.",
            "Unknown in our model, on the other hand, is very flexible, but to ensure multiplicity become much more difficult.",
            "So here we consider example, we just Simply put some interaction terms into the linear model.",
            "By the way, this is very often used statistic known in the model using statistics.",
            "Already in this case you can find out the partial derivative on one feature attribute's I know depends on all other attributes.",
            "So in this case it is very difficult to pose a sufficient and necessary condition to ensure multiplicity.",
            "And also we have to mention that generally the nonlinear model is much more difficult to interpret than the linear models.",
            "And all these three points not only host for this particular nonlinear model, but actually hold general for other known numbers as well.",
            "For example, like multilayer neural networks, decision trees and so on."
        ],
        [
            "Now in this work we would like to propose the use of shock into goal.",
            "As a key ingredient in designing monotone class of monotone models.",
            "And by using shocking to go, it can provide us very huge insight into the data.",
            "And as a concrete application of shock integral, we propose an extension of the logistic regression which we call.",
            "It showcased aggression.",
            "In the following, our first very short instruction on nonadditive measures and short interval and then I will talk about this circuit regression and in the end I will tell you something about our experiments or results."
        ],
        [
            "OK.",
            "So let me start with no negative measures, less we consider a set of criteria.",
            "So you want to say M. And there's a metal defined on this set.",
            "For the illustration purpose, we know we can just consider this C binary features.",
            "And we interpret this measure mu A as a weight of the set of a. OK, now for example, you can consider the following skills as speaking Chinese coding Java encoding C. For additive measure that as long as two sets are disjoint.",
            "The weight of a Union B is the same as the weight of a plus the weight of B.",
            "For example.",
            "A guy who speaks Chinese and encoding a Java.",
            "He will have a weight of their .6 is exactly the sum of the singletons.",
            "For an owner manual.",
            "This simply only normalized and monotone.",
            "But not necessarily additive.",
            "So what does that mean?",
            "For example, now you try to find some guy, some Java programmer located in Beijing.",
            "So in this case, anyone who only speaks Chinese only coding in Java or owning coding C is not a candidate, so it has a weight of there.",
            "But even someone who coding in Java and also speaking Chinese then is a perfect candidate that probably has a weight of 1.",
            "Someone who coding in C and speaking Chinese have evaluations or .7 because she is fairly similar to Java.",
            "So this one is also pretty good candidate.",
            "And in this case, if there is anyone who know all these three skills but still have a evaluation of 1, because in this particular case, code in C doesn't provide any additional value.",
            "Right, so this is a fairly reasonable scoring system where you want to hire someone coding in Java in Beijing, but this semantic.",
            "Cannot be expressed using additive manner.",
            "OK, because now the sum is simply not equal to the singletons."
        ],
        [
            "Now let's talk about the importance of a criteria and interactions.",
            "For additive manual, there's no interaction between criteria.",
            "And the importancy of 1 particular criteria can be simply measured but corresponding weight.",
            "Right, this is simple.",
            "And for a non additive measure the things get much more difficult.",
            "So here I would like just to mention 2 already established measures, so one is a sharper value which can measure the importancy of 1 criteria.",
            "The basic idea is to measure the average increase of the importancy when adding this particular criteria into a subset.",
            "This is the shop right index and the corresponding Lee.",
            "We also have interaction index which can model the interactions between features and get aerials.",
            "So I just would like to mention here.",
            "As long as a non editable measure is given you could just compute this indexes according to this two equations so I won't go into details here."
        ],
        [
            "Alright, so now we have no tomorrow.",
            "We're ready to talk about.",
            "Shocking to go.",
            "In the case which before we don't consider this C1 to CM so that error binary features now, but in general the evaluation of the feature FCI can be anywhere you like between zero and one.",
            "In the case of additive manual.",
            "So for now the question is when we have this individual evaluation for each material, how can we compute overall evaluation and mathematically is corresponding to how to compute the integral right?",
            "How can we do the integration for additive measure?",
            "And this is you can just do the integral in a normal way, so namely it is a vertical way, you just compute.",
            "Consider each individual criteria 1 by 1.",
            "And wait then accordingly.",
            "So basically corresponding to a weighted mean.",
            "This is how you usually do integral.",
            "So when I talk about a vertical way to do integration is already probably already occur to you that we also have a horizontal way to integration.",
            "So in this particular case, for example, we can first consider.",
            "So criteria which has a evaluation at least as the one, say for half.",
            "Then we waited them accordingly.",
            "Then we consider the criteria has evaluation.",
            "Is between the relation of C4 and C2 and with them.",
            "Accordingly, and so on and so forth.",
            "This is a horizontal way to do integration.",
            "Now The thing is, when you do the integration in the horizontal way, you can apply this no editable measure in a very natural way, which you cannot do in the vertical case because each criteria is considered independently and 1 by 1.",
            "So this is basically the."
        ],
        [
            "We're behind shock integral, so here for the sake of completeness I put the definition of shopping, shopping, nickel here.",
            "But the basic idea just simply just saying I just explained to you.",
            "So I would like to mention that in the context of learning, so evaluation of 1 particular carrier CI correspond to the value of the variable XI.",
            "OK."
        ],
        [
            "So now we have a shock integral.",
            "The question is how can we use it in learning?",
            "So here is a concrete example that we try to generalize logistic regression using shock integral.",
            "For the people who familiar with location question.",
            "Basically what it does is using a logistic function to model the posterior probability that given instance is a positive right?",
            "So here we have a linear part which is the linear combination of the weight and input variables.",
            "What do we do in shock equation?",
            "Is we replace this linear part with shocking the goal and two model parameters?",
            "As you can show that if you choose appropriate prioritization.",
            "Indeed, this showcase regression is a special case or low discretion special case of the shortest regression.",
            "OK."
        ],
        [
            "To better understand sharkeys regression.",
            "You can consider it as a two stage process.",
            "The first of all the utility of an instant X is computed using shock Angel.",
            "And then the second step, a discrete choice is made by thresholding the utility at the threshold data.",
            "We have another model parameter gamma, so here is an illustration.",
            "When the gamma is going to Infinity, why did the gamma then the decision function become a step function?",
            "So which means one network.",
            "That utility is more than the threshold, then this is the probability that this this instance is positive is 1.",
            "But one is smaller.",
            "Gamma is chosen, the decision function become a smooth function.",
            "Which means even when the utility is more than the threshold, there is still a chance that is instant is not positive.",
            "OK, it might be indeed the case, because it could be the case that the utility function doesn't perfectly capture the nature of the data, or maybe the giving feature didn't doesn't reflect the problem at hand so well.",
            "So indeed, that could be the case.",
            "So in a sense, is the gamma parameter corresponding to the position of your model?",
            "How good your classification model is?"
        ],
        [
            "So there are several features of shock is regression first, so we using this non editable measure to specify the importances of subsets of particular variables.",
            "And because it is non additive now we can model this interaction effects between features.",
            "As I introduced, we have this supply index and actually index.",
            "It gives us huge insights into the data.",
            "So now we can answer the question.",
            "For example, if you have two features that you can say.",
            "If these two features complement each other, or there's a certain kind of redundancy among them.",
            "And of course, multiplicity is also guaranteed because she came to go by design this integral.",
            "So by design it is monitored."
        ],
        [
            "OK, now the question is how do we do the parameter estimation?",
            "So we have the following parameters.",
            "We have the negative review.",
            "Notice threshold beta position parameter gamma.",
            "So at this point I should mention that in the most general form, the Nonadditive Metro has a number of parameters which is exponential.",
            "The number of, uh, tributers, so it brings a kind of computational challenges, but this is not the focus of this talk.",
            "Here I will come back to this point in at the end of the talk.",
            "So we're using maximum likelihood estimation to get the parameters.",
            "And.",
            "During the optimization, we express shock Angel using this mobile transform.",
            "I won't talk about this so much because it just it just equivalent formalization.",
            "So mathematically the same I just bring us some convenience."
        ],
        [
            "OK so here is our optimization formulation.",
            "For people who are familiar with maximal estimation for location question, it looks quite similar.",
            "The difference here is now the linear part noise replaced by Shark Angel.",
            "Now we have constraints on the utility threshold and the position parameter.",
            "We also have a constraint on normalization and modesty of the known analytic measure.",
            "So indeed you can show it is convex problem, so you can use some just some standard technique to solve this problem.",
            "Right?"
        ],
        [
            "OK so here's my big table.",
            "Some experimental results.",
            "So we have 9.",
            "Monotone binary classification data size.",
            "We test on five different methods, so CR is our showcase regression.",
            "Because circuit regression is an extension of logistic questions.",
            "Of course, we also compare with logistic regression.",
            "We also have A2 non monotone classifiers, namely kernelized logistic regression with polynomial kernel and with RBF kernel.",
            "So with polynomial kernel we set the degree to two so we can model the low level interactions between features.",
            "Wild are difficult model, high level interaction between kernels between features.",
            "We also compare one of the state of the art monotone owning a classifier more.",
            "It is a rule based classifier.",
            "As you can see here, from the top to bottom, the training is the learning is done by training, test, speaking.",
            "So from top to bottom we have more and more data used for training.",
            "And all the values are.",
            "See here is averaged value of 100 time repetition and here are the average rank of different methods on different settings.",
            "So there are several messages I want to send out here.",
            "So in general shock is regression performance extremely well, even quite compatible with the classifier more.",
            "And also in general we can see that.",
            "The model from classifiers indeed have advantage or compared with the normal ones, at least for this type of data size.",
            "And if you compare showcase regression and logistic regression, there's some very interesting patterns.",
            "Now, if we have only a limited training data.",
            "Logistic regression performs better.",
            "While the shortest regression is suffer a little bit from the overfitting effect.",
            "But one more and more data available.",
            "Then showcase regression becomes significantly better than logistic regression.",
            "Alright."
        ],
        [
            "Are the result.",
            "I have mentioned that this sharp playing decks and interaction index can provide huge insight into the data.",
            "So here is an illustration on the car evaluation data size.",
            "So basically all this this is the interaction index and this is supplying decks, so all this value are computed according to the formula I just showed you.",
            "Here is the darker the color is and the higher the value is.",
            "I only want to mention one thing to look at this supply index.",
            "So from the supplying desk we can, we can say that.",
            "For a car, the most important feature.",
            "Is the price of the car the buying price and price of maintenance?",
            "A safety of a car is also important.",
            "But not as important as at least it's more important that than how comfortable the carriers.",
            "But safety is not as important as the price of the car.",
            "I know if some of you is working in the car industry, if you do, you probably know it's actually the truth."
        ],
        [
            "OK, so it brings me to a conclusion.",
            "So in this work we propose the use of shock in the goal in machine learning, especially in learning non monotone models.",
            "Especially learning model models.",
            "And.",
            "As a concrete application, we using the shock and a goal 2X to generalize the logistic regression where these two.",
            "This showcase regression model.",
            "I have showed you some experimental results to verify our claims.",
            "And of course there are also some ongoing work I have mentioned that there is still a computational challenge of this approach.",
            "There's a possible solution that is instead of to learn unknown additive measure with full flexibility.",
            "We could restrict ourselves.",
            "To learn along additive measure which only have a complexity up towards certain level.",
            "Not, of course, another question is how can we control this complexity in a very efficient way?",
            "And this concludes my talk.",
            "And if you have any questions, I would like to answer.",
            "OK, we have.",
            "Time for one question.",
            "Yes.",
            "I very nice look.",
            "I've got two questions and the first one is did you?",
            "Did you check any dominance relations between the perimeter of the logistic regression and the parameter of your chocolate stick regression?",
            "I mean, did you check if the parameter corresponds to measure which is dominated by?",
            "The nonnegative measure.",
            "We check it, but when you just showed here.",
            "So basically you can already see it in the perimental result.",
            "Whenever Shakira question has a very large improvement compared with regression, actually the indicator that this kind of relation with the parameter you can see there and location question and question.",
            "They're both monotone.",
            "So from that point of view that this difference in terms of the performance has already explained it by this.",
            "Interaction we model in the shortest regression.",
            "And did you try to make a logistic regression with the with as parameter?",
            "The Shepley index of the non additive measure just to to compare in for it would be interesting to see the the distances between the perimeter of your regression and the Shipley index as it's supposed to be the very center of all the measure which which are dominates the nonnegative measure.",
            "I say point.",
            "So basically the idea is that somehow extend location question by adding some terms into the model, then to compare.",
            "If this extended model compared with soccer showcase regression, right?",
            "Disappointment?",
            "No, no, you take your chocolate stick regression and then you remove the nonnegative measure that you use for the computation of the integral and you just consider the Shepley index which is an additive measure.",
            "So it would be a kind of logistic regression.",
            "But it could be compared to the original 1.",
            "It is a very important, very interesting point we did.",
            "We actually also think about this, but the motivation of this working very beginning.",
            "We want to have our monotone classifier.",
            "So for this particular reason we didn't really do it that way.",
            "But this is an interesting advice."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work is a bonbon learning in monotone nonlinear models using the Crockett Integral, so I leave it to you OK. How are you?",
                    "label": 1
                },
                {
                    "sent": "Thank you for coming and today I'm going to tell you something about our recent research on learning monotone nonlinear models using Szocki integral.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my colleague Ali foreign tyranny just off chansky and my PhD advisor Alcohol Mile.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so anyone who work with machinery applications here should know that incorporating background knowledge into a learning process is a very important task.",
                    "label": 1
                },
                {
                    "sent": "Is often allow you to model your problem better.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we will focus on very important type of background knowledge.",
                    "label": 0
                },
                {
                    "sent": "The lamley is monotonicity.",
                    "label": 0
                },
                {
                    "sent": "So sometimes ensuring multiplicity can not only improve you performance and very often in application it is hard constraint.",
                    "label": 0
                },
                {
                    "sent": "For example, if you try to build a model to predict the chance that patients suffer from Len concern, then you probably want to have something like the higher the tobacco consumption and the more likely this guy will have a cancer.",
                    "label": 1
                },
                {
                    "sent": "And if you model doesn't deliver this, and probably a medical doctor wouldn't like to trust your model and refuse to use it.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How to ensure multiplicity for a linear model is quite simple because it is directly correspond to the son of your Corporation.",
                    "label": 1
                },
                {
                    "sent": "And also now model is very easy to interpret because the influence of each predictor is directly reflected by the corresponding coefficient.",
                    "label": 1
                },
                {
                    "sent": "But the real world, the problem of a linear model is in a lot of real world applications.",
                    "label": 0
                },
                {
                    "sent": "It is just simply not powerful enough, not flexible enough.",
                    "label": 0
                },
                {
                    "sent": "Unknown in our model, on the other hand, is very flexible, but to ensure multiplicity become much more difficult.",
                    "label": 0
                },
                {
                    "sent": "So here we consider example, we just Simply put some interaction terms into the linear model.",
                    "label": 1
                },
                {
                    "sent": "By the way, this is very often used statistic known in the model using statistics.",
                    "label": 1
                },
                {
                    "sent": "Already in this case you can find out the partial derivative on one feature attribute's I know depends on all other attributes.",
                    "label": 0
                },
                {
                    "sent": "So in this case it is very difficult to pose a sufficient and necessary condition to ensure multiplicity.",
                    "label": 0
                },
                {
                    "sent": "And also we have to mention that generally the nonlinear model is much more difficult to interpret than the linear models.",
                    "label": 0
                },
                {
                    "sent": "And all these three points not only host for this particular nonlinear model, but actually hold general for other known numbers as well.",
                    "label": 0
                },
                {
                    "sent": "For example, like multilayer neural networks, decision trees and so on.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in this work we would like to propose the use of shock into goal.",
                    "label": 1
                },
                {
                    "sent": "As a key ingredient in designing monotone class of monotone models.",
                    "label": 1
                },
                {
                    "sent": "And by using shocking to go, it can provide us very huge insight into the data.",
                    "label": 0
                },
                {
                    "sent": "And as a concrete application of shock integral, we propose an extension of the logistic regression which we call.",
                    "label": 1
                },
                {
                    "sent": "It showcased aggression.",
                    "label": 0
                },
                {
                    "sent": "In the following, our first very short instruction on nonadditive measures and short interval and then I will talk about this circuit regression and in the end I will tell you something about our experiments or results.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me start with no negative measures, less we consider a set of criteria.",
                    "label": 0
                },
                {
                    "sent": "So you want to say M. And there's a metal defined on this set.",
                    "label": 0
                },
                {
                    "sent": "For the illustration purpose, we know we can just consider this C binary features.",
                    "label": 0
                },
                {
                    "sent": "And we interpret this measure mu A as a weight of the set of a. OK, now for example, you can consider the following skills as speaking Chinese coding Java encoding C. For additive measure that as long as two sets are disjoint.",
                    "label": 0
                },
                {
                    "sent": "The weight of a Union B is the same as the weight of a plus the weight of B.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "A guy who speaks Chinese and encoding a Java.",
                    "label": 0
                },
                {
                    "sent": "He will have a weight of their .6 is exactly the sum of the singletons.",
                    "label": 0
                },
                {
                    "sent": "For an owner manual.",
                    "label": 0
                },
                {
                    "sent": "This simply only normalized and monotone.",
                    "label": 0
                },
                {
                    "sent": "But not necessarily additive.",
                    "label": 0
                },
                {
                    "sent": "So what does that mean?",
                    "label": 0
                },
                {
                    "sent": "For example, now you try to find some guy, some Java programmer located in Beijing.",
                    "label": 0
                },
                {
                    "sent": "So in this case, anyone who only speaks Chinese only coding in Java or owning coding C is not a candidate, so it has a weight of there.",
                    "label": 0
                },
                {
                    "sent": "But even someone who coding in Java and also speaking Chinese then is a perfect candidate that probably has a weight of 1.",
                    "label": 1
                },
                {
                    "sent": "Someone who coding in C and speaking Chinese have evaluations or .7 because she is fairly similar to Java.",
                    "label": 1
                },
                {
                    "sent": "So this one is also pretty good candidate.",
                    "label": 0
                },
                {
                    "sent": "And in this case, if there is anyone who know all these three skills but still have a evaluation of 1, because in this particular case, code in C doesn't provide any additional value.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is a fairly reasonable scoring system where you want to hire someone coding in Java in Beijing, but this semantic.",
                    "label": 0
                },
                {
                    "sent": "Cannot be expressed using additive manner.",
                    "label": 0
                },
                {
                    "sent": "OK, because now the sum is simply not equal to the singletons.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's talk about the importance of a criteria and interactions.",
                    "label": 1
                },
                {
                    "sent": "For additive manual, there's no interaction between criteria.",
                    "label": 1
                },
                {
                    "sent": "And the importancy of 1 particular criteria can be simply measured but corresponding weight.",
                    "label": 0
                },
                {
                    "sent": "Right, this is simple.",
                    "label": 1
                },
                {
                    "sent": "And for a non additive measure the things get much more difficult.",
                    "label": 0
                },
                {
                    "sent": "So here I would like just to mention 2 already established measures, so one is a sharper value which can measure the importancy of 1 criteria.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is to measure the average increase of the importancy when adding this particular criteria into a subset.",
                    "label": 0
                },
                {
                    "sent": "This is the shop right index and the corresponding Lee.",
                    "label": 0
                },
                {
                    "sent": "We also have interaction index which can model the interactions between features and get aerials.",
                    "label": 0
                },
                {
                    "sent": "So I just would like to mention here.",
                    "label": 0
                },
                {
                    "sent": "As long as a non editable measure is given you could just compute this indexes according to this two equations so I won't go into details here.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now we have no tomorrow.",
                    "label": 0
                },
                {
                    "sent": "We're ready to talk about.",
                    "label": 0
                },
                {
                    "sent": "Shocking to go.",
                    "label": 0
                },
                {
                    "sent": "In the case which before we don't consider this C1 to CM so that error binary features now, but in general the evaluation of the feature FCI can be anywhere you like between zero and one.",
                    "label": 0
                },
                {
                    "sent": "In the case of additive manual.",
                    "label": 0
                },
                {
                    "sent": "So for now the question is when we have this individual evaluation for each material, how can we compute overall evaluation and mathematically is corresponding to how to compute the integral right?",
                    "label": 0
                },
                {
                    "sent": "How can we do the integration for additive measure?",
                    "label": 0
                },
                {
                    "sent": "And this is you can just do the integral in a normal way, so namely it is a vertical way, you just compute.",
                    "label": 0
                },
                {
                    "sent": "Consider each individual criteria 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "And wait then accordingly.",
                    "label": 0
                },
                {
                    "sent": "So basically corresponding to a weighted mean.",
                    "label": 0
                },
                {
                    "sent": "This is how you usually do integral.",
                    "label": 0
                },
                {
                    "sent": "So when I talk about a vertical way to do integration is already probably already occur to you that we also have a horizontal way to integration.",
                    "label": 0
                },
                {
                    "sent": "So in this particular case, for example, we can first consider.",
                    "label": 0
                },
                {
                    "sent": "So criteria which has a evaluation at least as the one, say for half.",
                    "label": 0
                },
                {
                    "sent": "Then we waited them accordingly.",
                    "label": 0
                },
                {
                    "sent": "Then we consider the criteria has evaluation.",
                    "label": 0
                },
                {
                    "sent": "Is between the relation of C4 and C2 and with them.",
                    "label": 0
                },
                {
                    "sent": "Accordingly, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "This is a horizontal way to do integration.",
                    "label": 0
                },
                {
                    "sent": "Now The thing is, when you do the integration in the horizontal way, you can apply this no editable measure in a very natural way, which you cannot do in the vertical case because each criteria is considered independently and 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're behind shock integral, so here for the sake of completeness I put the definition of shopping, shopping, nickel here.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea just simply just saying I just explained to you.",
                    "label": 0
                },
                {
                    "sent": "So I would like to mention that in the context of learning, so evaluation of 1 particular carrier CI correspond to the value of the variable XI.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now we have a shock integral.",
                    "label": 0
                },
                {
                    "sent": "The question is how can we use it in learning?",
                    "label": 0
                },
                {
                    "sent": "So here is a concrete example that we try to generalize logistic regression using shock integral.",
                    "label": 0
                },
                {
                    "sent": "For the people who familiar with location question.",
                    "label": 0
                },
                {
                    "sent": "Basically what it does is using a logistic function to model the posterior probability that given instance is a positive right?",
                    "label": 0
                },
                {
                    "sent": "So here we have a linear part which is the linear combination of the weight and input variables.",
                    "label": 0
                },
                {
                    "sent": "What do we do in shock equation?",
                    "label": 0
                },
                {
                    "sent": "Is we replace this linear part with shocking the goal and two model parameters?",
                    "label": 0
                },
                {
                    "sent": "As you can show that if you choose appropriate prioritization.",
                    "label": 0
                },
                {
                    "sent": "Indeed, this showcase regression is a special case or low discretion special case of the shortest regression.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To better understand sharkeys regression.",
                    "label": 0
                },
                {
                    "sent": "You can consider it as a two stage process.",
                    "label": 0
                },
                {
                    "sent": "The first of all the utility of an instant X is computed using shock Angel.",
                    "label": 0
                },
                {
                    "sent": "And then the second step, a discrete choice is made by thresholding the utility at the threshold data.",
                    "label": 0
                },
                {
                    "sent": "We have another model parameter gamma, so here is an illustration.",
                    "label": 0
                },
                {
                    "sent": "When the gamma is going to Infinity, why did the gamma then the decision function become a step function?",
                    "label": 0
                },
                {
                    "sent": "So which means one network.",
                    "label": 0
                },
                {
                    "sent": "That utility is more than the threshold, then this is the probability that this this instance is positive is 1.",
                    "label": 0
                },
                {
                    "sent": "But one is smaller.",
                    "label": 0
                },
                {
                    "sent": "Gamma is chosen, the decision function become a smooth function.",
                    "label": 1
                },
                {
                    "sent": "Which means even when the utility is more than the threshold, there is still a chance that is instant is not positive.",
                    "label": 0
                },
                {
                    "sent": "OK, it might be indeed the case, because it could be the case that the utility function doesn't perfectly capture the nature of the data, or maybe the giving feature didn't doesn't reflect the problem at hand so well.",
                    "label": 0
                },
                {
                    "sent": "So indeed, that could be the case.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, is the gamma parameter corresponding to the position of your model?",
                    "label": 0
                },
                {
                    "sent": "How good your classification model is?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are several features of shock is regression first, so we using this non editable measure to specify the importances of subsets of particular variables.",
                    "label": 1
                },
                {
                    "sent": "And because it is non additive now we can model this interaction effects between features.",
                    "label": 0
                },
                {
                    "sent": "As I introduced, we have this supply index and actually index.",
                    "label": 0
                },
                {
                    "sent": "It gives us huge insights into the data.",
                    "label": 0
                },
                {
                    "sent": "So now we can answer the question.",
                    "label": 1
                },
                {
                    "sent": "For example, if you have two features that you can say.",
                    "label": 0
                },
                {
                    "sent": "If these two features complement each other, or there's a certain kind of redundancy among them.",
                    "label": 0
                },
                {
                    "sent": "And of course, multiplicity is also guaranteed because she came to go by design this integral.",
                    "label": 0
                },
                {
                    "sent": "So by design it is monitored.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the question is how do we do the parameter estimation?",
                    "label": 0
                },
                {
                    "sent": "So we have the following parameters.",
                    "label": 0
                },
                {
                    "sent": "We have the negative review.",
                    "label": 0
                },
                {
                    "sent": "Notice threshold beta position parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "So at this point I should mention that in the most general form, the Nonadditive Metro has a number of parameters which is exponential.",
                    "label": 1
                },
                {
                    "sent": "The number of, uh, tributers, so it brings a kind of computational challenges, but this is not the focus of this talk.",
                    "label": 0
                },
                {
                    "sent": "Here I will come back to this point in at the end of the talk.",
                    "label": 0
                },
                {
                    "sent": "So we're using maximum likelihood estimation to get the parameters.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "During the optimization, we express shock Angel using this mobile transform.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about this so much because it just it just equivalent formalization.",
                    "label": 0
                },
                {
                    "sent": "So mathematically the same I just bring us some convenience.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so here is our optimization formulation.",
                    "label": 0
                },
                {
                    "sent": "For people who are familiar with maximal estimation for location question, it looks quite similar.",
                    "label": 0
                },
                {
                    "sent": "The difference here is now the linear part noise replaced by Shark Angel.",
                    "label": 0
                },
                {
                    "sent": "Now we have constraints on the utility threshold and the position parameter.",
                    "label": 1
                },
                {
                    "sent": "We also have a constraint on normalization and modesty of the known analytic measure.",
                    "label": 0
                },
                {
                    "sent": "So indeed you can show it is convex problem, so you can use some just some standard technique to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so here's my big table.",
                    "label": 0
                },
                {
                    "sent": "Some experimental results.",
                    "label": 0
                },
                {
                    "sent": "So we have 9.",
                    "label": 0
                },
                {
                    "sent": "Monotone binary classification data size.",
                    "label": 0
                },
                {
                    "sent": "We test on five different methods, so CR is our showcase regression.",
                    "label": 0
                },
                {
                    "sent": "Because circuit regression is an extension of logistic questions.",
                    "label": 0
                },
                {
                    "sent": "Of course, we also compare with logistic regression.",
                    "label": 0
                },
                {
                    "sent": "We also have A2 non monotone classifiers, namely kernelized logistic regression with polynomial kernel and with RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "So with polynomial kernel we set the degree to two so we can model the low level interactions between features.",
                    "label": 0
                },
                {
                    "sent": "Wild are difficult model, high level interaction between kernels between features.",
                    "label": 0
                },
                {
                    "sent": "We also compare one of the state of the art monotone owning a classifier more.",
                    "label": 0
                },
                {
                    "sent": "It is a rule based classifier.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, from the top to bottom, the training is the learning is done by training, test, speaking.",
                    "label": 0
                },
                {
                    "sent": "So from top to bottom we have more and more data used for training.",
                    "label": 0
                },
                {
                    "sent": "And all the values are.",
                    "label": 0
                },
                {
                    "sent": "See here is averaged value of 100 time repetition and here are the average rank of different methods on different settings.",
                    "label": 0
                },
                {
                    "sent": "So there are several messages I want to send out here.",
                    "label": 0
                },
                {
                    "sent": "So in general shock is regression performance extremely well, even quite compatible with the classifier more.",
                    "label": 0
                },
                {
                    "sent": "And also in general we can see that.",
                    "label": 0
                },
                {
                    "sent": "The model from classifiers indeed have advantage or compared with the normal ones, at least for this type of data size.",
                    "label": 0
                },
                {
                    "sent": "And if you compare showcase regression and logistic regression, there's some very interesting patterns.",
                    "label": 0
                },
                {
                    "sent": "Now, if we have only a limited training data.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression performs better.",
                    "label": 0
                },
                {
                    "sent": "While the shortest regression is suffer a little bit from the overfitting effect.",
                    "label": 0
                },
                {
                    "sent": "But one more and more data available.",
                    "label": 0
                },
                {
                    "sent": "Then showcase regression becomes significantly better than logistic regression.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are the result.",
                    "label": 0
                },
                {
                    "sent": "I have mentioned that this sharp playing decks and interaction index can provide huge insight into the data.",
                    "label": 1
                },
                {
                    "sent": "So here is an illustration on the car evaluation data size.",
                    "label": 1
                },
                {
                    "sent": "So basically all this this is the interaction index and this is supplying decks, so all this value are computed according to the formula I just showed you.",
                    "label": 0
                },
                {
                    "sent": "Here is the darker the color is and the higher the value is.",
                    "label": 0
                },
                {
                    "sent": "I only want to mention one thing to look at this supply index.",
                    "label": 0
                },
                {
                    "sent": "So from the supplying desk we can, we can say that.",
                    "label": 0
                },
                {
                    "sent": "For a car, the most important feature.",
                    "label": 0
                },
                {
                    "sent": "Is the price of the car the buying price and price of maintenance?",
                    "label": 0
                },
                {
                    "sent": "A safety of a car is also important.",
                    "label": 0
                },
                {
                    "sent": "But not as important as at least it's more important that than how comfortable the carriers.",
                    "label": 0
                },
                {
                    "sent": "But safety is not as important as the price of the car.",
                    "label": 0
                },
                {
                    "sent": "I know if some of you is working in the car industry, if you do, you probably know it's actually the truth.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it brings me to a conclusion.",
                    "label": 0
                },
                {
                    "sent": "So in this work we propose the use of shock in the goal in machine learning, especially in learning non monotone models.",
                    "label": 1
                },
                {
                    "sent": "Especially learning model models.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "As a concrete application, we using the shock and a goal 2X to generalize the logistic regression where these two.",
                    "label": 0
                },
                {
                    "sent": "This showcase regression model.",
                    "label": 0
                },
                {
                    "sent": "I have showed you some experimental results to verify our claims.",
                    "label": 0
                },
                {
                    "sent": "And of course there are also some ongoing work I have mentioned that there is still a computational challenge of this approach.",
                    "label": 0
                },
                {
                    "sent": "There's a possible solution that is instead of to learn unknown additive measure with full flexibility.",
                    "label": 0
                },
                {
                    "sent": "We could restrict ourselves.",
                    "label": 0
                },
                {
                    "sent": "To learn along additive measure which only have a complexity up towards certain level.",
                    "label": 0
                },
                {
                    "sent": "Not, of course, another question is how can we control this complexity in a very efficient way?",
                    "label": 0
                },
                {
                    "sent": "And this concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions, I would like to answer.",
                    "label": 0
                },
                {
                    "sent": "OK, we have.",
                    "label": 0
                },
                {
                    "sent": "Time for one question.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "I very nice look.",
                    "label": 0
                },
                {
                    "sent": "I've got two questions and the first one is did you?",
                    "label": 0
                },
                {
                    "sent": "Did you check any dominance relations between the perimeter of the logistic regression and the parameter of your chocolate stick regression?",
                    "label": 0
                },
                {
                    "sent": "I mean, did you check if the parameter corresponds to measure which is dominated by?",
                    "label": 0
                },
                {
                    "sent": "The nonnegative measure.",
                    "label": 0
                },
                {
                    "sent": "We check it, but when you just showed here.",
                    "label": 0
                },
                {
                    "sent": "So basically you can already see it in the perimental result.",
                    "label": 0
                },
                {
                    "sent": "Whenever Shakira question has a very large improvement compared with regression, actually the indicator that this kind of relation with the parameter you can see there and location question and question.",
                    "label": 0
                },
                {
                    "sent": "They're both monotone.",
                    "label": 0
                },
                {
                    "sent": "So from that point of view that this difference in terms of the performance has already explained it by this.",
                    "label": 0
                },
                {
                    "sent": "Interaction we model in the shortest regression.",
                    "label": 0
                },
                {
                    "sent": "And did you try to make a logistic regression with the with as parameter?",
                    "label": 0
                },
                {
                    "sent": "The Shepley index of the non additive measure just to to compare in for it would be interesting to see the the distances between the perimeter of your regression and the Shipley index as it's supposed to be the very center of all the measure which which are dominates the nonnegative measure.",
                    "label": 0
                },
                {
                    "sent": "I say point.",
                    "label": 0
                },
                {
                    "sent": "So basically the idea is that somehow extend location question by adding some terms into the model, then to compare.",
                    "label": 0
                },
                {
                    "sent": "If this extended model compared with soccer showcase regression, right?",
                    "label": 0
                },
                {
                    "sent": "Disappointment?",
                    "label": 0
                },
                {
                    "sent": "No, no, you take your chocolate stick regression and then you remove the nonnegative measure that you use for the computation of the integral and you just consider the Shepley index which is an additive measure.",
                    "label": 0
                },
                {
                    "sent": "So it would be a kind of logistic regression.",
                    "label": 0
                },
                {
                    "sent": "But it could be compared to the original 1.",
                    "label": 0
                },
                {
                    "sent": "It is a very important, very interesting point we did.",
                    "label": 0
                },
                {
                    "sent": "We actually also think about this, but the motivation of this working very beginning.",
                    "label": 0
                },
                {
                    "sent": "We want to have our monotone classifier.",
                    "label": 0
                },
                {
                    "sent": "So for this particular reason we didn't really do it that way.",
                    "label": 0
                },
                {
                    "sent": "But this is an interesting advice.",
                    "label": 0
                }
            ]
        }
    }
}