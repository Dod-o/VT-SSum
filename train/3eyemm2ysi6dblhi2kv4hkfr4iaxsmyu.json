{
    "id": "3eyemm2ysi6dblhi2kv4hkfr4iaxsmyu",
    "title": "Natural Language Understanding",
    "info": {
        "author": [
            "Phil Blunsom, Department of Computer Science, University of Oxford"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_blunsom_language_understanding/",
    "segmentation": [
        [
            "OK, well it's a it's a pleasure to be here in Montreal once again for the summer school, so I've been tasked with teaching you natural language processing.",
            "So I'm going to approach this by not trying to cover the whole field of natural language processing, but just focus on some core topics, particularly in this first lecture this morning.",
            "Language modeling.",
            "And a bit of machine translation and then this afternoon that would be more general this afternoon.",
            "Or talk a bit more about some things we're doing at Deep Mind in trying to induce meaning, grounding and structure.",
            "OK, so let's get started."
        ],
        [
            "So I thought I might start with a little bit about sort of what is this field?",
            "Natural language processing?",
            "How does it fit in?",
            "There's lots of different terms you here at conferences where people present natural language work like computational linguistics, natural language processing, and all sorts of other related field like psycholinguistics, corpus linguistics.",
            "So what are all these people doing?",
            "So there's no formal definitions of these things, but I have my own sort of informal.",
            "Why identify which of these fields which so linguistics?",
            "Linguistics is the field that's largely concerned with the scientific study of language.",
            "Trying to understand why language is the way it is this human languages.",
            "Now we understand language how we communicate and trying to drive models of this models of language and lots of these concerns aren't necessarily to do with trying to produce models that are implementable or account for.",
            "Data you might see on the web or text or things like that.",
            "There's lots of other concerns in linguistics so often they don't match up with without concerns in artificial intelligence.",
            "Particularly concern for linguistics that drives a lot of the research is how humans learn language and how do children learn language so quickly.",
            "'cause there's still a lot of mystery about that because we fully don't understand language and we don't understand how we learn it, but we do have a lot of data.",
            "OK, so computational linguistics is really the computational study of language, and this can be from the linguistic side we're trying to produce models which are computationally plausable of language, or you're trying to analyze models of language to say what is their complexity.",
            "If we tried to implement this as an algorithm.",
            "And also to produce tools to study language.",
            "So there's a separate from tools that Google or Facebook or someone might use, but tools specifically to study language like parsing large corpora.",
            "So then you can look at statistics of different constructions and things like this.",
            "Computational linguistics overlaps a lot with what we call natural language processing, and for me natural language processing is really on the engineering side of things, so this is the actual building of tools that people might want to build and sell, and things like empty speech recognition text to speech.",
            "All these sorts of things you're probably not familiar with.",
            "So this is a very sort of informal distinction for me, computational linguistics is really the science side and natural language processing is really the engineering side.",
            "And often these two don't match up.",
            "Also, the natural in natural language processing, there's been some controversy recently about what this means for me.",
            "Natural just means human languages or languages that occur in the world, not formal languages or artificial languages.",
            "It's no statement about quality, it's not saying good language processing, it's just saying we're looking at English or French or something like that, not Python.",
            "OK, so that's where we."
        ],
        [
            "I'll see it and this.",
            "This lecture is principally going to be about language modeling.",
            "A task in natural language processing about machine translation.",
            "The second lecture I'll probably get a bit more onto the computational linguistics side of things and talk a bit about.",
            "What linguistics tells us about language and what we might have some concerns we might have if we want to get to a sort of full artificial intelligence processing of language.",
            "So language modeling, so you've probably seen a bit of this, maybe earlier in Joshua's lecture, but just to be formal, so language modeling in its formal sense is just modeling sequences of discrete symbols.",
            "So we can say that we have some utterance W there, and it comes from Sigma Star, the space of all.",
            "Discrete sequences.",
            "And we want to get a probability distribution from a language model that is consistent over this space of string.",
            "So that is over the space of all possible strings that sums to one.",
            "Noting that there is no bound on length of strings.",
            "So a language model tells us, given an utterance, that we see how probable is that.",
            "And depending on what you trained, the model on probability, it can mean different things.",
            "But often we think about it.",
            "How likely am I to observe this this utterance in the data?",
            "And that might seem like a very sort of simple abstract thing to worry about.",
            "Putting a probability on, but it's extremely useful, so a huge number of applications and tasks rely on this basic ability to put probabilities on utterances from language.",
            "So it means we can compare different utterances and say which is more likely.",
            "And we can compare different choices of word order or different words.",
            "And this is key to things like translation, speech recognition.",
            "You're always trying to make a choice about given these possible outputs, I could produce which one is most likely."
        ],
        [
            "OK, so language modeling goes way back to his way back, but particularly in the war, people like Turing and good here.",
            "We're trying to break German codes and they realized that they could take an information theoretic approach to this.",
            "Before then, people generally employed linguists to do decryption.",
            "But being computer scientists, these guys approached it from an information theoretic point of view and they realized that if they could get a good distribution on German messages, then that would.",
            "That would really help them decode those messages."
        ],
        [
            "So they started to feel Dan."
        ],
        [
            "And really, they also gave us a number of the probabilistic modeling techniques that up until recently we were pretty close to the state of the art.",
            "So.",
            "I've introduced language modeling, just the basic problem of putting a probability on a string, but that string could always include some conditioning context, and if we do that then we can do lots of things like these examples here.",
            "The Top Model machine translation you can just take your French string and your English string and concatenate them together and say if I have a good probability distribution on these strings, then I have a good model of translation.",
            "We can conceptually take that further and say that maybe just question answering is just predicting what will come next.",
            "What will someone say after a question?",
            "Here we really need some context beta, which might be our knowledge of the world.",
            "And then maybe we can even take that to dialogue.",
            "Here, the beater is really rather important, but I'm just hiding it there in the conditioning context.",
            "But the idea here is that language modeling is really general.",
            "Problem language models are very general tools, so we can see a lot of problems just from this in language.",
            "From this point of view of putting probability distributions on strings."
        ],
        [
            "So the basis for pretty much pretty much all effective language models is decomposing this joint probability, which is what we want at the top.",
            "There their utterance W one through N, decomposing its probability according to the chain rule.",
            "So this allows us to sequentially score the probability or calculate the probability.",
            "And it's also exact.",
            "I mean, the key thing here is there's no assumption here.",
            "This is just the rules of probability, right?",
            "We can do this.",
            "So then then it makes our task easier instead of having to model this big joint distribution, we can think of it sequentially.",
            "So now we're thinking about we would call this a generative model.",
            "We're thinking of a way to generate the string.",
            "In sequence models like this, life is a bit easier than generative models.",
            "In images where you don't necessarily have a natural sequence to the model.",
            "But here in language we have a natural sequence.",
            "Left to right ordering."
        ],
        [
            "Well, there are questions that a little bit more in the in the second lecture.",
            "So.",
            "If we can build good language models.",
            "If we can build really good language models at the same sort of level as a human, then that language model really must know quite a bit about language and really must understand it.",
            "So in this basic task of language modeling, there is most of language understanding.",
            "Which is cool because we can get limitless amounts of language data on the web.",
            "You can get as much text as you want.",
            "Trillions of words to train models.",
            "The only limitation is your computation.",
            "But as an example, if we have an utterance like there, she built A and I ask you what comes next, you're going to have a pretty high entropy distribution over what could come next, 'cause you really don't know any sort of noun or adjective.",
            "All sorts of things could come after that utterance.",
            "But if I give you an utterance like, Alice went to the beach.",
            "There she built a.",
            "Then you're going to be thinking, well, there's a much lower entropy distribution.",
            "There is probably going to be a sandcastle, maybe a boat something like that, but there are very few options.",
            "And the reason you know that is because you understand the structure of language.",
            "You understand that here she is referring to Alice.",
            "There is referring to beach.",
            "You understand that at beaches people tend to build sandcastles.",
            "You're doing all this processing as you see that string which is hidden from us.",
            "We'd like to build language models that can do that too.",
            "That can resolve what we call Co references here.",
            "They understand common sense relationships.",
            "We nowhere near that, but you can see how in this simple language modeling objective of just predicting the next word.",
            "There's a huge amount of understanding language."
        ],
        [
            "OK, so the key thing with any modeling task is how we're going to evaluate it.",
            "So the classic evaluation in language modeling is cross entropy or perplexity, depending on whether you exponentiate or not.",
            "And you can think of cross entropy as a sort of measure of information orbits that you need to encode a message, and it tells you something about there's a deep connection between compression and language modeling.",
            "If you have a good language model, then you have a good model for compressing.",
            "Text there's a deep relationship there, so these are perplexity.",
            "In particular, is a metric.",
            "You tend to see people use, so normally you collect your training data.",
            "Hopefully it's very large.",
            "You train your model, then you have a test set which is separate from that and you score the perplexity and you say my model is better than the one that was published last year."
        ],
        [
            "So that's our valuation.",
            "We do have to think carefully about data for language modeling, so language modeling is a time series problem.",
            "We're predicting what comes, what happens in the future, given what we've seen in the past.",
            "So we want to be careful when we create data that we don't mix up the past in the future.",
            "So normally, when you create a language modeling data set, often your data will come from maybe Newswire, or somewhere like that.",
            "It has a natural ordering.",
            "You want to make sure that your training data is in the past and your test data is in there is in the future.",
            "Particularly if you have news or something like that or books you don't want to, you generally want to keep particles together in training or test.",
            "You don't want to split them up, because if you see the first half of an article, it's much easier to predict the second half than if you've never seen that article before.",
            "So you get much lower perplexities, and you may be misled."
        ],
        [
            "About the quality of your model.",
            "I bring these things up because despite how easy it is to collect language modeling data, we still don't actually have terribly good standard language modeling data sets.",
            "The ones that people tend to use most commonly is something called the Penn Treebank data set which Depend Tree Bank is a collection of texts from the Wall Street Journal annotated with grammatical structures.",
            "The structures are being thrown away and people just take the texts being processed a bit further.",
            "It's been lowercased vocabulary is being restricted, punctuation is being thrown away, and the result is not a very useful data set for language modeling, but it's still extremely popular, but it's very small and it's quite a skewed representation of language.",
            "So I discourage people from using that one.",
            "The million word corpus was a data set that was created at Google to try and address some of these problems, but unfortunately it also had some other problems, particularly relating to that what I said in the previous slide about separating.",
            "Keeping articles together and always testing on the future so there they just commuted.",
            "Sentence is from a big data set and the problem is a test data set.",
            "Crip has sentences from articles that are in the training data set so that tells you that for any given test sentence you probably can observe in the training data.",
            "All the other sentence is in the article it came from which makes it a lot easier to predict that sentence.",
            "Then if you kept the article separate recently, some data sets called Wikitext.",
            "In a few different size released.",
            "There still hasn't been a lot of results on these, but they do seem to be better, so they're not a bad place to start for language modeling.",
            "So there's two versions of it.",
            "It's at 3 million.",
            "Someone correct me and 200 million.",
            "There's a small and a big.",
            "But their wiki articles they're nicely separated, so it's a bit more consistent, and we have the original, so you can choose vocabulary and all sorts of things.",
            "OK."
        ],
        [
            "Is there a Clock here so I can see?",
            "Now I'm going.",
            "What time is it now?",
            "Thanks.",
            "OK.",
            "So in this sort of 1st maybe 2/3 of the lecture, I'm just going to go through classic language modeling techniques from the more historical Now N gram language models and different neural network instantiations of language models.",
            "You can sort of think of these together as all taking that chain rule decomposition of the joint probability and approximating the context in different ways.",
            "So in the first set of models ngram models, we just throw away most of the context and only keep the recent words.",
            "In the neural engram models we do the same thing, but we use a neural network to get a bit more parameter efficiency into the models and with recurrent neural net language models rather than truncating the history, we compress it all into a fixed size vector."
        ],
        [
            "OK, so count."
        ],
        [
            "Language model, so this is the classic approach to language modeling, an extremely scalable and simple way of doing language modeling.",
            "And it all comes down to basically just counting the number of times you see different outputs.",
            "So you make the Markov assumption, which is when we do this general decomposition rather than each decision being conditional on all of the decisions that came before.",
            "We're going to say it's only conditional on the previous one word or two words, or three words.",
            "So if it's one word, we call that a bigram Model 2 words, we call that a trigram.",
            "So that's the basic ngram model, and that's most of the sort of modeling idea there is.",
            "So just conditioned on recent history.",
            "So in the bottom example, here is a bigram model."
        ],
        [
            "So there's very simple maximum likelihood estimators for these models, or we need to do so in the trigram case.",
            "We're predicting a third word given the previous two, or we need to do is count up the number of times we saw those three words together, and then divide by the number of times we saw the context, and so this is really simple.",
            "You can do this on trillions of words, it's very easy.",
            "And quick.",
            "On the basis of these models, the problem is."
        ],
        [
            "You're going to want more context to get a better model, so just conditioning on the previous word, that's not going to be very good, so you might think I should condition on the previous two or three, or four or five.",
            "The more words you condition on, the less likely you are to actually have seen that set of words.",
            "And if you haven't seen that sequence of words, your account will be 0 and you'll get a probability of 0.",
            "So that's no good, and this is a endemic problem for these sorts of models.",
            "You don't want lots of zero counts, so here's my sort of silly example.",
            "We may have never seen Montreal Burrito or Montreal beer drinker, but we want to score these trigrams an obvious thing to do is because we haven't seen those trigrams.",
            "Why don't we just look at the bigrams the previous two words and this is the basis of all of the sort of state of the art N gram models.",
            "This idea of backing off from longer context to shorter ones, and mixing these distributions together.",
            "And there's two main approaches.",
            "One is called back off, where if you haven't seen something, you back off to the lower order distribution and the other is interpolation where you just interpolate all of your distributions together."
        ],
        [
            "So here's an example of interpolation.",
            "You just have.",
            "So this is a trigram model and we end up with an interpolate linearly interpolating model between unigram.",
            "So just the probability of producing that word independent of context, bigrams and trigrams.",
            "And obviously your interpolation weights need to sum to 1 to get a valid probability.",
            "And there's all sorts of ways of coming up with these interpolation weights, so I'm not going to go into those in these lecture.",
            "In this lecture, there's people spent.",
            "2030 years coming up with all sorts of convoluted ways of estimating these models.",
            "The standard sort of state of the art for these sort of things are something known as an S and I.",
            "And what it tries to do is it tries to match.",
            "So if we look at language and we look at the frequency of unigrams and bigrams we see a power law known as this law.",
            "So we tend to see a few things very frequently and lots of things very infrequently and this is a key nature of language, which is one of the reasons why natural language processing is hard, because there's lots and lots of infrequent things called the long tail.",
            "Of the distribution, Nelson has a bias or a prior to specifically catch UPS distributions, and that's why it does so well.",
            "There's this great paper from.",
            "Well, almost 20 years ago now.",
            "By Joshua Goodman and Stanley Chin that surveyed all of these techniques.",
            "So if you want to sort of classic history of ngram modeling."
        ],
        [
            "That and see just how many ways there are of doing this.",
            "OK, so that was my quick history of ngram models.",
            "Anne.",
            "Where's that?",
            "Different datasets like like how well those in Purple integrations are confirmed.",
            "So then how important are the?",
            "Differences now with like you know, building word or 30,000,000 document data set compared to this definition 98 for this study.",
            "Depend women, so always the more data you have the better it gets.",
            "So yes as we scale the data it gets better.",
            "One thing we do observe is that all these different smoothing techniques matter less the more data you have, which is make sense because the zero account problem gets less and less as you get to trillion words and something that became popular maybe 10 years ago that came from Google with something called stupid back off, which was a way of doing this.",
            "Back off where you didn't actually bother to normalizing these.",
            "Interpolation weights when you do an ngram look up is costly when you've got very large language models, often on different servers.",
            "So people gave up bothering to normalize properly and have a proper distribution, and still once you get to the sort of trillions of words, you get reasonable outputs.",
            "So as your data set gets bigger, yes, lots of these concerns become less.",
            "So these models are extremely scalable, and I'd say that's still no ones.",
            "No ones running neural language models on the sort of scale that people were running N gram models, so these things are just extremely scalable.",
            "They're also extremely fast to do a look up because you don't necessarily need to normalize over the vocabulary when you're looking up the probability.",
            "Often it's just a table look up or a series of table look ups for the different backoff levels.",
            "Also, the last point there is that the smoothing techniques like Ineson I specifically try and match the distribution of language.",
            "None of the neural models that I'll get to incorporate this sort of bias, so it's interesting to ask, what if they did incorporate this bias would?",
            "Could they do better?",
            "So the disadvantages, lodging, lodging, grams, spouse once you go past five or so in your context history, you're just really not not going to see those phrases.",
            "So you're not going to get much benefit, so you're never going to be able condition on things in the past, distant in the past.",
            "So the sort of example I gave with the sandcastle at the start and end gram models.",
            "Never going to do that.",
            "Never going to learn those sort of dependencies.",
            "It also doesn't learn any relationships between words, so cat dog.",
            "These are just different integers in an N gram language model.",
            "It has no sense of these things, both being animals being furry, cute pets, etc.",
            "Also the same for morphology, so it has no sense that running and jumping both verbs are used in similar contexts.",
            "Syntactically, so none of these things are captured."
        ],
        [
            "So models that do capture them so N gram language models."
        ],
        [
            "Applied by Joshua.",
            "About 15 years ago now.",
            "I just a simple neural network parameterization of the account based model.",
            "So my diagram of a feedforward network.",
            "You have some input at the bottom, a hidden layer and some output.",
            "And."
        ],
        [
            "N gram models.",
            "So imagine we want to build a trigram model.",
            "We assume that we have two inputs.",
            "The previous two words W, N -- 1 and minus two.",
            "Those sort of yellow boxes you imagine being one hot vectors of length of the vocabulary.",
            "Just indexing which words they are.",
            "They feed into a feedforward neural network.",
            "We have a hidden layer.",
            "We have transformations on those.",
            "How exactly you do those transformations varies and can give you all sorts of different instantiations of these models.",
            "But the basic idea is that we have a feedforward network we're feeding in the context.",
            "And predicting a distribution over what comes next.",
            "So the distribution of the top there is going to be a big long vector of the length of the vocabulary of words.",
            "You could predict the output, so normally at least 10s of thousands, maybe hundreds of thousands of different options, and it would be a normalized distribution because we passed it through a softmax.",
            "So.",
            "The cost of these models is all in that output distribution.",
            "OK, so this is the basic idea.",
            "Now the reason this is attractive is because now that we're essentially projecting these input words and the output into a into a vector space and that gives us those relationships I mentioned on the previous slides about N gram models not capturing so dogs versus cats running and jumping, and these sorts of things.",
            "Now we're going to learn because those things occur in similar engrams, they're going to end up with similar embeddings in the space."
        ],
        [
            "And this is the basis for all of the word embedding models and everything else this idea.",
            "So this is what this looks like.",
            "We can sample from these models.",
            "That's my little sort of sideways to order their means to get output, so we have something like we built a.",
            "We can get the distribution over the next word."
        ],
        [
            "And what this looks like we feed in our input our start symbols, sample our output, feed that into the next time step the two conditioning context, and so on and so on, and we can sample strings like this from our distribution."
        ],
        [
            "OK, so you should have seen plenty of backpropagation and training a feedforward networks before, so this should all be very simple to train a model like this, it's just back propagation so.",
            "But the nice thing."
        ],
        [
            "The nice thing that in these models on our sequences, so at the bottom we've got all the different Ingrams in a sequence of forward predictions.",
            "They all break apart nice and independently, so if the Red Arrows they're showing the gradients coming down from the back propagation, F is our objective function at the top as a gradients come down, they just flow down those columns, one corresponding to each engram prediction, and so they're all independent, which means we can calculate them all in parallel.",
            "We can distribute them across different machines.",
            "We can do this extremely.",
            "Quickly and efficiently, so that's one of the main things to think about.",
            "These ngram models is that we're truncating the history to only a few previous words, but that does give us a really great scalability.",
            "So actually it was working on this model that I came up with asynchronous SGD because it was taking too much time to train and.",
            "It could be paralyzed.",
            "Yeah, that's a natural way to do it.",
            "'cause you can.",
            "This idea scales brilliantly.",
            "And this is coming back now.",
            "So there's been a number of recent language models based on convolutions, one from Deep Michael Black Net.",
            "But these are.",
            "You can essentially see as evolutions of the N gram language models that try and get longer range conditioning.",
            "But but still get some of this ability to paralyze really easily and be efficient."
        ],
        [
            "OK, so neural engram models compared to classical count based models.",
            "They have all these advantages of capturing correlations between the outputs and inputs in terms of memory and space are much more efficient because everything is getting compressed into this space.",
            "So in terms of performance versus memory tradeoff, the neural engram models tend to be much better if you're just saying I want the maximum performance I can get with a GB of memory or something like this.",
            "I.",
            "So you don't get this scaling like the count based models where the size of the model scales with the number of N gram counts.",
            "But you still do get some scaling.",
            "Obviously, the more data you have and the more you want to capture, you need bigger hidden layers and that then that does scale quadratically as you make the hidden layer bigger.",
            "So you can't just expect you have a model with 1000 hidden units or works great on a million data points that you can get away with 1000 hidden units on a billion data points.",
            "And my final point there is, we're still.",
            "These models are just optimizing likelihood.",
            "We use things like dropout and things like that which changed it a bit, but mostly just optimizing likelihood.",
            "And we know from count based models that likelihood is actually a bad objective for language.",
            "But that's not not something that."
        ],
        [
            "One's really addressed yet."
        ],
        [
            "So why is it about objective for language?",
            "Because because of that power or nature of language, because if you optimize likelihood, you don't reserve enough probability for those rare events 'cause we know that language has very long tail.",
            "So there's always going to be something we didn't expect.",
            "So with likelihood you tend to just really discount that probability.",
            "Put too much probability on the things you've seen.",
            "OK, so the third in our series of models is recurrent networks.",
            "Am I doing?",
            "So I think you've had one lecture Premier show on recurrent networks and it should be obvious how these apply to language modeling.",
            "So here's my little diagram of a feedforward network and recurrent network.",
            "Now we're feeding the hidden layer from the previous time step into the next time step.",
            "Conceptually very simple."
        ],
        [
            "But has some some complications that creates for modeling.",
            "So now we are sampling a string from our recurrent language model.",
            "Rather than taking the previous inputs and having to feed them into the as the context of the next input, we just take the previous hidden state and that carries across and at each time step we're making our prediction just on the current input and the previous hidden state.",
            "So.",
            "Looking something like that."
        ],
        [
            "OK, so again, training with backpropagation with recurrent networks.",
            "Backpropagation is a little bit more complicated because you get these dependencies across time, sort of signified.",
            "There by little red loop on the right hand bottom hidden layer.",
            "Going to create some sort of cyclic dependency unless we do something about it.",
            "So the classic thing to."
        ],
        [
            "Do is to unroll our network across time and think of it for a given sequence.",
            "Imagine this is our sentence of four words along the bottom.",
            "There think of this as one big neural network that we can back propagate through and so we have our objective function F at the top and we back propagate it along all of those little Red Arrows.",
            "And that's going to give us the right updates for SGD etc.",
            "OK."
        ],
        [
            "Downside of this is that it couples all of the updates.",
            "So now if we look at one of these which are looking at the second hidden layer.",
            "And we look at its gradient objective with respect to that hidden layer.",
            "We're going to get contributions that flow all through the network in the future, because changing that hidden layer changes everything that comes after it.",
            "So This is why we can't now break apart this network into lots of little independent networks and get the same sort of data efficiency that we had or.",
            "Computational efficiency that we had with the neural ngram models.",
            "So this makes scaling more difficult.",
            "It means that you you can't really just take your whole corpus if you have a million words of text and you want to say I'm going to model this with recurrent network, that just starts at word zero and runs through a million time steps.",
            "If you want to back propagate through that exactly, you're going to put that whole million time step sequence into memory and do all the calculations.",
            "So often there's natural ways we can break up our data.",
            "We might break text up into sentences or paragraphs.",
            "Sentence is by far the most common and then back propagate individually on each sentence and treat them as independent.",
            "It's suboptimal because we know that information from previous sentences influences the next one.",
            "So again, the example I gave at the start of the sandcastle, we only get that nice prediction if we know what was in the previous sentence."
        ],
        [
            "OK, an alternative is truncated backpropagation through time and this is just a heuristic where we say every so often some fixed number of time steps, let's say 20, which is going to break the gradient dependencies and pretend like these are separate sequences except we still forward propagate across these breakages.",
            "So that's my little dotted line there as we're doing our forward propagation.",
            "We still forward propagate exactly, but when we're doing the back propagation step, we stop at those truncation points.",
            "Now you can take your million time step sequence of text and just break it up into regular 2015 hundred word segments and load those into memory.",
            "That's also nice because it means that everyone of your sequence is the same length and that leads to very efficient processing on GPS and such architectures.",
            "Whereas if you break things up into sentences, they're all different lengths and you have to worry about how do I put different link sentences into one mini batch to get onto my GPU?"
        ],
        [
            "OK."
        ],
        [
            "So in language modeling, recurrent networks are really the state of the art for a given size of data.",
            "As opposed to the N gram models, rather than truncating the history now what we're doing is saying we're still going to try and keep our infinite history, but we're going to try and compress it into this fixed size hidden layer.",
            "That's a nice idea because you think further things are back in time.",
            "Probably the less influence they have, but I can still keep a trace of them.",
            "Certain things have much more influence than others.",
            "For instance, recurrent networks are good at getting what we might call trigger words or things like that.",
            "If you have a document about a particular politician like Donald Trump.",
            "If you see Donald Trump once in an article, you're very likely to see that name again, so being able to record named entities, especially that you've seen in the past, gives you a much better distribution over what comes next in that document.",
            "Recurrent networks are good at capturing that sort of thing.",
            "They're good at capturing any sort of trigger events in the second lecture.",
            "Today, I'll go a bit more into what they're not good at capturing, and that's really the hierarchical syntactic structure of language.",
            "Recurrent networks are still hard to learn, so it's not trivial to learn these things.",
            "You tend to need a lot of computation.",
            "There's all sorts of problems that I think you went through earlier about gradients disappearing or exploding.",
            "We still have this problem that if we want more memory we have to increase the size of our hidden layer.",
            "So if we increase the size of our hidden layer, we can capture or we can we're doing less compression of the past so we can remember more.",
            "But we have an increasing sort of quadratically increasing number of parameters.",
            "I'm not going to go into this, but memory based models, memory networks, differentiable neural computers and all these sorts of things.",
            "Try and break that dependency by having an increase in memory without the increase in the number of parameters.",
            "And again, we're still training with maximum likelihood."
        ],
        [
            "OK, so there are three basic approaches.",
            "Two language modeling and one way to think of these are as I repeating is different ways of compressing the context, either by throwing things away or trying to compress into a into a fixed size vector or your leg cramps do both.",
            "So the key thing here is that once we get to the recurrent network, then at least conceptually we have a model that could maybe solve that example I gave at the start.",
            "If the model could in its hidden layer, capture.",
            "Who had gone to the beach and what they were doing there?",
            "Then it's possible it could make the right prediction in the future, so it's no longer throwing away that history.",
            "Of course, for it to do that, it has to discover a great deal about the structure of language, how subjects and objects coordinate, and how anaphora like she work, which is quite a large ass, but at least we have a model now that conceptually is able to capture those sorts of things."
        ],
        [
            "So.",
            "All my examples so far, I've just been what we call simple recurrent networks.",
            "There's a huge number of variants of this and I'm not going to go into these.",
            "Besides again, I'm I'm sure you mentioned things like LS teams, another gated recurrent architectures, and if you're actually going to build a language model or anything else on our current network, you're going to use one of these gated architectures, 'cause that's the state of the art.",
            "So if in doubt using STM, it's almost always going to work.",
            "And as I'll show a few slides later, maybe it will work a lot better than than you actually think.",
            "So LSD emser."
        ],
        [
            "So what other things can we do with our current networks?",
            "We can make them deeper."
        ],
        [
            "So we can add extra layers just like a deep."
        ],
        [
            "Feedforward network and relating this back to my comments about memory.",
            "Now we can see if we make a network deeper like this.",
            "We are increasing the amount of memory it has at any given time step 'cause now it has sort of 3 three times the width of H at each time step, but we've only had a linear increase in the number of parameters, so increasing depth like this is one way to increase memory in recurrent networks with only a linear increase in the number of parameters.",
            "Generally, though, for a fixed size number of parameters which I might show in a moment as well.",
            "If you fix a number of parameters, it seems like single layers often do just as well as making it deeper."
        ],
        [
            "Also, if you're ever going to make these things deeper, always include skip connections and things like this, 'cause they work and they make training much faster so that is directly connecting each layer and the input to the output gets around a lot of the problems with gradient propagation."
        ],
        [
            "So an alternative way of getting depth so this statement or a deep LSM is ambiguous.",
            "We can go depth vertically like I showed there."
        ],
        [
            "Or there's also this idea of going horizontal with depth so we can add extra recurrent computations between each input.",
            "So this idea has been pushed recently in the recurrent highway networks paper.",
            "That sort of claiming state of the art language modeling performance.",
            "So this doesn't give you the same.",
            "Memory increase, but it conceptually at least allows you to compute more complex functions at each time step.",
            "'cause that's the way we tend to think about these deep networks is the more layers you have, the more complex the function you can compute on the input.",
            "So that's the other way of going deep.",
            "Normally when people say deep LSD or something like that, they mean the previous kind.",
            "But this is a different sort of of deep network."
        ],
        [
            "OK.",
            "So one of the concerns we have in language modeling and using neural networks which comes up less in other other areas of deep learning is that our output spaces tend to be huge.",
            "So if you collect.",
            "Let's say a billion words of text.",
            "You're probably going to find, maybe.",
            "Half a million a million different unique tokens or word types as we call them.",
            "So if you just take your vocabulary to be all the different words you saw in your output as your model gets bigger, your vocabulary is also going to get big.",
            "And as I said earlier, language model follows a power law, so that tells you no matter how much data you gather, you're going to keep your vocabulary going to keep growing, so it's not going to get to a point where you've seen all of the words.",
            "So this means that we have very big output spaces which are very costly to compute.",
            "If you want to do your softmax on your neural network.",
            "GPU's speed this up because this is what GPU's are great at doing.",
            "Big vector matrix products but they also have their limits.",
            "They also have their limits in terms of memory because these things take up a lot of memory so normally in neural language modeling we tend to have some way of.",
            "Speeding up this output vocabulary computation, so in any large scale model, we're going to have to worry about this.",
            "On lots of small scale test sets, people often have very sort of artificially small vocabularies, but we want to do something on a real real sort of scale.",
            "We're going to worry about this.",
            "So there's all sorts of various different solutions, so an obvious one is some sort of short list where we just keep the most frequent words.",
            "You can make this more interesting.",
            "You can mix up count based N gram models in newer models, so just do your newer model on the frequent words in your account based on the infrequent.",
            "This is suboptimal 'cause the newer models are actually created.",
            "Estimating the infrequent distributions 'cause they get these correlations between the words, but this is a sort of easy starting point.",
            "You can do short list which sort of specific to each mini batch process, so that's also a popular approach.",
            "Grab a mini batch of sentence is 1020 senators computer short list of the vocabulary?",
            "Maybe sample it, maybe have some sort of function to compute it to get a smaller normalization and then do it get a different one for the next mini batch and then if you do that, maybe stochastically you'll get something that gives you a good distribution of the whole vocabulary.",
            "You still have to worry about how you'll actually get a probability at Test time though."
        ],
        [
            "So another approach is to change the objective function.",
            "So rather than optimizing likelihood, we could make we could come up with a different objective function.",
            "Hope that it sort of correlates with what we are interested in, but which is easier to estimate.",
            "So a classic version of this is noise contrastive estimation or other negative vertical negative sampling ideas.",
            "So noise contrastive estimation.",
            "The idea is to try and factor out the normalizer into a hyperparameter.",
            "Another hyperman extra parameter.",
            "Here see so rather than, so we know that C is actually a function of what's W in age, but if we pretend that it's not, maybe we can get something."
        ],
        [
            "Scalable noise contrastive estimation.",
            "Basically boils is problem down to saying at each point rather than estimated rather trying to maximize the probability of the word that I'm going to see next given all of the other words, I'm just going to sample some other words and then treat each sample like a binary decision problem to maximize the probability of seeing the right word an rejecting the wrong word.",
            "So this comes out just like a binary decision problem and it tends to work quite well and give something that that maximizing this also maximizes the likelihood quite nicely.",
            "And it's very efficient.",
            "'cause now you're just doing binary estimation.",
            "Also, you can often get away with very few samples, especially if you're doing large scale list estimations.",
            "OK, here is the number of samples, so we sample them from a noise distribution.",
            "A very effective noise distribution is just a unigram frequency of words.",
            "OK. Again, one of the issues with this sort of approach is it only speeds up training at Test time.",
            "If you want probability, you still have to normalize over the whole vocabulary so it hasn't.",
            "It hasn't helped you.",
            "Some people have suggested that you can get away without normalizing at Test time.",
            "We've played with this a bit and it doesn't really work.",
            "You tend to get lots of variants, so."
        ],
        [
            "It might work one day, but but not the next.",
            "Another approach like this is something that was called important sampling, not necessarily connected to important sampling.",
            "In Monte Carlo techniques, but a similar sort of idea which is very similar to the noise contrastive loss except now.",
            "Rather than doing these binary decisions between the noise and the true output, we collect the noise samples into a multi way softmax between the noise and the output.",
            "OK."
        ],
        [
            "Another approach, and personally my favorite one is to factorize the output vocabulary so we can always make these sort of factorization assumptions.",
            "So if we knew that our vocabulary could be split into different classes or different clusters of words nonoverlapping groups, then we could factorize a distribution in first predicting which class the output is going to belong to, and then given that class predict which of the words in that class the output is.",
            "Now.",
            "Just in the sort of statement there, this is exact.",
            "If we have no.",
            "Ignoring parametric assumptions on the distribution, once we parameterized things, it becomes inexact, but we can always factorize a distribution like this.",
            "The simplest sort of factorization is to just take your vocabulary.",
            "So if you have 10,000 output words, split them into 100 classes of 100 words each.",
            "There's all sorts of algorithms for doing that.",
            "There's something called Brown clustering, which is particularly effective, and that will give you a sort of quadratic speedup.",
            "Which is quite nice."
        ],
        [
            "So the key thing there is you need some way of coming up with the factorization and that does affect the result.",
            "Now you can take this idea further and say well if we can factorize once, why not do it again?",
            "And why not go down to binary decisions rather than any decisions and so then we get a tree.",
            "So we can imagine arranging our vocabulary in a tree where at each branching point is binary and predicting any word comes down to following a path in this tree making binary decisions.",
            "This is very attractive because then we get a log in speed up.",
            "So now we can get really large vocabularies at quite low cost, so that's nice.",
            "The downside is that you have to come up with a tree.",
            "So where does the tree come from?",
            "There's all sorts of ways of doing this, none of them are terribly satisfactory.",
            "And it tends to, at least in my experience, not perform as well as some of the simpler factorizations.",
            "Some of the suggestions like just doing trees based on frequency or Huffman coding really don't work very well.",
            "OK, more recently there's some variance of this from Facebook, so the reference at the bottom there where they get a sort of unbalanced factorization with the aim of making it very efficiently GPU, so this is quite an attractive approach as well.",
            "Trying to tune this to a GPU, the ordinary sort of factorizations don't tend to run simply on GPUs."
        ],
        [
            "OK, here's a sort of summary slide of all these different.",
            "Factorization ideas and a different sort of speedups you get.",
            "So your full softmax is obviously linear in the number of words you've got to predict, both at training time and test time.",
            "If you do a factorization, you can get a square root V speed up.",
            "The worst case at Test time is still V, But you can actually generally do much better than that if you enumerate your classes one of the time.",
            "So this is to sample.",
            "So you want to sample from the distribution.",
            "If you enumerate the classes in order of probability, you'll probably sample something much quicker than having to enumerate all of the classes.",
            "The same is true for the balance tree factorization.",
            "Here you get the best sort of speedup of log V, and again, worst case is linear, but you can generally do much better, and then you have the sort of negative sampling options at the bottom.",
            "Which are very fast at training time 'cause normally you can get away with a very small number of samples K but don't help you much at Test time.",
            "One thing to remember is it as researchers we tend to care a lot about training time 'cause we're training lots of models, But if you're actually trying to deploy something industry you care a lot about test time.",
            "So if that model takes long to train, that's OK because you're going to do this a lot when you actually deploy it, so there's often a mismatch between what researchers care about."
        ],
        [
            "What industry cares about in these problems?",
            "OK, a different way of getting around this vocabulary problem is just predict smaller output granularities.",
            "So an obvious one is let's predict characters relevant words.",
            "So there's not that many characters, at least in English.",
            "In languages like Chinese and Japanese, is a lot more characters, but still only in the order of thousands.",
            "English you've only got maybe 100 different characters to worry about, so you're going to get very fast.",
            "Softmax is if you predict characters.",
            "The downside is that there's a lot more characters in an utterance, so you're going to have a lot more current time steps for these two sort of trade off against each other.",
            "And depending on how big your softmax is, the question is which ones dominating your time.",
            "Is it the recurrent calculation or is it the softmax calculation?",
            "Even to softmax and going to characters might speed things up things up if it's a recurrent computation then it might slow things down.",
            "The other problem is that predicting characters hard, so we know that language, or at least in English, and very strongly in English words tend to group in the language tends to group in nice word discrete units.",
            "We lose that if we go down to characters, and so the model has to discover it for itself.",
            "And it's also going to need a lot more memory to do this.",
            "'cause it's remembering back a lot more time steps just to cover one word or two words in the past.",
            "So generally we need much bigger hidden layers.",
            "So I'd say most of the sort of language modeling work I've seen on big data sets we tend to be able to run them such that increasing the with a big enough hidden layer that increasing it doesn't make things much better.",
            "Once we get the characters were definitely not at that point yet.",
            "So even when we've got 10,000 hidden units or models this big, it's still clear that if we could increase up to 100,000, it would get a lot better.",
            "So we're still very much.",
            "In a sort of saturated.",
            "Regime.",
            "The nice thing about characters is we capture all sorts of word internal structure that we know.",
            "Language has like morphology, relationship between words.",
            "There's not that much of that in English, so it doesn't necessarily make much of a difference in English.",
            "But if you were doing French, this makes a significant significant difference.",
            "If you're doing Turkish or Finnish or some very morphologically inflected language like that, then it can make a huge difference.",
            "So what about out of vocabulary words, which wouldn't?",
            "Yeah, good point.",
            "So the other thing, of course, is that if you're doing things on the word level, you have to assume what your vocabulary is, so you have a fixed number of words, whatever that is.",
            "With characters you have an open vocabulary model.",
            "You can assign a probability to any sequence of characters, whether you've seen that word before or not, and so that's very attractive 'cause you don't have to make this decision about what's in the vocabulary wants out of it, and because words have this nice internal structure, and especially for languages.",
            "With inflection, models are going to be actually able to predict inflections of words they've never seen inflicted in that way before, so it may have seen the root, and it may have seen lots of other words inflected with the right case or whatever it's needed, but never that word, but because things are if the morphologies nice and regular, the model may actually be able to guess what the right inflection of that that word is.",
            "That sort of the Holy Grail of character language models and morphology is to get them to predict inflections they've never seen before.",
            "Almost certainly I'm not familiar with.",
            "Secret modeling, but I would assume that people are using recurrent networks on the show.",
            "Do you know of any examples?",
            "I've seen papers.",
            "Yeah, it's an obvious thing to go for, but yes, there's lots of things that give nice sequential dependencies, even things that aren't very sequential.",
            "So it turns out the recurrent networks actually work very well as generative models of images, so this is a sort of pixel RNN.",
            "Series of work where you just assume an ordering to the pixels in the image and then treat them like a big long sequence and that can work surprisingly well."
        ],
        [
            "OK, what am I up to drop out?",
            "So just quickly I think I've seen plenty of drop out before, but when we get to recurrent networks, there are some extra concerns about how we use dropout.",
            "So dropout is good if we don't use some sort of regularization like dropout, we tend to not get very good results.",
            "But"
        ],
        [
            "If we so my little diagram here, I put these little dropout boxes on the vertical sort of feedforward connections, and I haven't put them on the recurrent connections.",
            "If you put straight normal dropout on recurrent connections there and sample them independently, what's going to happen is that the recurrent networks going to lose its ability to capture long range dependencies.",
            "If you think about any hidden unit and think at each time step you sample maybe from a Bernoulli with.",
            ".5 probability whether to keep it or not given a few time steps, you're almost certainly going to have zero doubt that that hidden unit.",
            "So now you're not going to get any sort of propagation very far back in time, so just putting sort of vanilla dropout in between recurrent connections kills any sort of long range dependencies, so it's a bad idea.",
            "So the first sort of idea that people came up with, or the by far the most common approach, is just to put dropout between the inputs and the outputs, but don't touch the recurrent connections and this works well and you get a nice improvement, but you're still not regularising these recurrent connections, you get some."
        ],
        [
            "Training.",
            "An observation by urine gal.",
            "Was that maybe we could keep these drop out on the recurrent connections if we only sampled the months per sequence.",
            "So if you just sample one mask and use them at every time step, then you don't have this problem of in expectation, every hidden unit being zeroed out.",
            "So that's nice, of course.",
            "The downside is now you're only sampling once per sequence, so you might have to see the sequences more often to get a good sort of stochastic estimate.",
            "Urine justified this from a sort of various variational argument and sort of Asian POV, but it just makes intuitive sense that if you sample at once, you get the right sort of behavior."
        ],
        [
            "OK, so just as a sort of a side or end point to this discussion earlier on I said don't use the Penn treebank to do language modeling.",
            "And I'll give some examples here why not so in Group and deep mind we thought we'd look at this data set and the sorts of results people were predicting and just ask the question of how solid are some of these numbers, particularly how solid the baselines people are using.",
            "So the in every paper where someone claims are sort of new neural architecture, they always compare it to someone celestium baseline from X number of papers ago.",
            "So we thought, how well can we actually do with the baseline.",
            "If we do a big Bayesian optimization of its hyperparameters, so Gabor Melis, one of the guys at DeepMind, did this and found some interesting results.",
            "So there's a double lines in the middle above that result reported from various different papers.",
            "And the state of the art was in on this data sets considered to be sort of the current highway networks or the more recent ones trained with sort of a evolutionary type objective to try different architectures.",
            "What we found though if we do a big high performance search over the sorts of high parameters people haven't worried about all sorts of different things about how the models are trained.",
            "Balances between output parameters versus hidden parameters given a fixed parameter budget, we actually get really rather good results of an ordinary STM.",
            "So what we found is that an ordinary LS gym with one layer, which is like the first line there with 10,000,000 parameters, beats anything anyone's ever published on this data set.",
            "We also did the same sort of optimization for some of the other models, particularly the current highway network, and we can make their results slightly better, but not much better and.",
            "Sort of intuitive thing here is that people tend to do quite a lot of optimization of the model, say there proposing, but it's rare that we go back and try and re optimize the baseline or the benchmark.",
            "And so when we do a big optimization, what we find is that LS teams work really well and they're really quite hard to beat now.",
            "Some of this is almost certainly an artifact of this data set, which is why I say this data set is bad.",
            "It's very small and it's very sensitive to these high parameters.",
            "Very interesting as one of the students in our lab has done something very similar but with.",
            "Learning models.",
            "Hyperparameters he found was sometimes not uniquely optimizes the random seed.",
            "Yeah, so any of that.",
            "I don't think we we may have.",
            "We may have done the sideboards pretty thorough.",
            "But yes, we did make a difference randomly, so one sort of extra features I suppose is not necessarily high frame it randomly zeroing out in truncated backdrop whether you actually forward propagate seems to stabilize training quite a lot.",
            "I don't know if that's sort of something that people do often.",
            "Suggest that if we are optimizing random C. Runs in their in their hyperparameter search.",
            "Is going to get the best results, yes, so well, especially when we have things that are very sensitive parameters.",
            "So my takeaway from this is, well, one else.",
            "Teams are really good and to this data set is extremely sensitive to high parameters and I don't have any proof for it and we were trying and we're sort of trying to get that in some of our experiments, but my feeling is that as you scale the data sets bigger, they get less sensitive to parameters.",
            "So if we're comparing numbers on bigger data sets, we probably wouldn't see this sort of variation.",
            "And we do have numbers on the sort of wikitext ones, and we don't see quite as big.",
            "Jumps on those.",
            "That said, it's also harder to do the high parameter optimization on bigger data sets slower, and we do have an awfully large number of GPU's, so we have an advantage with that.",
            "But so yeah, the take home here.",
            "I'd say two things.",
            "One else teams are good too.",
            "Don't use the pen treebank language modeling data set.",
            "Which was partly my own by making it so hard to beat the baseline that people would stop using this data set.",
            "How do you know it's an application?",
            "Well, that's why I said I can't prove this.",
            "This is my.",
            "My intuition is that these things are less sensitive on the bigger data sets.",
            "Because hopefully the things like the random seed.",
            "All these sort of random elements of the model.",
            "These parameters are just shifting, adding a different sort of randomness, like on this data set.",
            "We know that vanilla SGD works better than any of the other optimizers, like Adam or any of these things, and partly that seems to be just a bit of extra randomness.",
            "That seems to matter less and more data you have, because then the data starts to overwhelm the randomness and you're less likely to just go off in different bad directions.",
            "So that is my intuition that the more data you have, the more the data overwhelms all the high parameters and there's less sensitivity 'cause you end up with the right distribution in the end.",
            "With these small data sets, that's not really the case.",
            "It seems it matters a lot.",
            "We used to.",
            "Just wondering, wondering.",
            "Depth yeah, sorry.",
            "Depends on the model.",
            "So for the STM it's vertical depth for the recurrent highway network, it's depth in time.",
            "3rd Elystan with Gru.",
            "So we so we did, and some of the hyperparameters are.",
            "I mean, there's lots of different variants of LS teams with different choices, and they're actually in our hyper research, so that the hyper research does try tying the the different gates and things like that that give you similarities of GI use, and Gumballs tried various giu things and I don't think there's my feelings are not much difference to those.",
            "But we know with this data set like tying parameters like tying input and output parameters helps quite a bit.",
            "This is something that other people have observed and we know that's not necessarily the case for big data sets, but again, it's that question not having so much data.",
            "Where is that question?",
            "Yeah, OK. For example, for LSD.",
            "Ferrari.",
            "That I couldn't tell you.",
            "Actually, that's a good question about how, how much the optimizer, how long the optimization takes, and it's a good.",
            "Yeah, there's another factor here that we're not really testing, which is how reliant.",
            "How easy is it to get certain results with certain parameters, so you might find that maybe there are current highway network is less sensitive parameters, and whatever you choose you end up with roughly this number, where is the LM, is extremely sensitive and you have to choose just the right ones to get a really good number.",
            "I don't know the answer to that one, we haven't really.",
            "Looked at that closely.",
            "It's not.",
            "We'll put it up at some point, but this is just in internal investigations.",
            "Yeah, first.",
            "About the.",
            "Multiple.",
            "Lucky.",
            "We are leaving the system.",
            "So how do we explain?",
            "So again, my intuition.",
            "I think people have looked at this smaller data set the more you have to worry about the local minima and the more data you have that you do tend to get more, even local minima.",
            "On these data sets is clearly bad.",
            "Now some of the high parameters were changing.",
            "Change the objective function so they changed the architecture of the model or something like that so.",
            "Then getting a different surface, some of them like random seeds or the choice of the optimizer.",
            "The same objective function, just with different optimizers, and we do see that particular optimizers get stuck in much worse.",
            "Local Optima.",
            "So that way their local Optima or they're just very flat regions.",
            "I couldn't say, but yes, there's definitely differences there.",
            "And on these data sets we do see that.",
            "Not on the Penn treebank.",
            "We've done it on things like the other prize data set and that again you've got.",
            "Once you get to characters, you've got more time steps, so in some sense you've got more data and those are more stable.",
            "But we again we do see that generally.",
            "The numbers that people are reporting for an STM as a benchmark and the sorts of things we can get if we heavily optimize it.",
            "There is still a gap and LCMS, as far as we can tell, a pretty much state of the art on most problems.",
            "I think I think on the Hunter price with the current highway network was still slightly better, but then the state of the art number was run for months or something like that, and we haven't actually run something that long.",
            "Not a problem with our.",
            "Running their base is always in DLC and is always used as a baseline.",
            "People just aren't incentivized hyperparameter there.",
            "Is this issue that you really should spend as much time optimizing the baseline as you do, optimizing your own model?",
            "There's also the tendency just to take baseline from papers rather than redoing them in your own conditions.",
            "Sure."
        ],
        [
            "OK, I better move on.",
            "So this is my sort of summary of language modeling that was that was it for language modeling.",
            "So recurrent networks give us this ability to capture long range dependency, which is good.",
            "We can make them deeper and I presented lots of ways of addressing this sort of large vocabulary problem."
        ],
        [
            "So I better move on quickly too."
        ],
        [
            "Just talk a bit about machine translation at the end, so I'm going to talk a bit about machine translation is going to be very shallow coverage of machine translation, neural translation, and at the end I've tacked on a little bit of some work.",
            "One of my students did, which is a bit of a wacky or different way of looking at neural machine translation just for some contrast.",
            "OK, so this is the classic picture of the painting of the Tower of Babel, so we have lots of languages in the world and it's an obvious problem to want to be able to translate between them.",
            "This is not a new thing.",
            "This is something that people notice pretty much at the dawn of computers.",
            "One of the first things they thought about when they come up with the computer was we could use this to translate between languages, because languages are really simple and regular incomputable.",
            "An all you need to do is write down a set of rules and you can translate from French to English or something like that.",
            "Famously, I think this was set as an intern project in the 50s to solve French English translation over the summer.",
            "And it didn't quite workout."
        ],
        [
            "So lots of these problems which we think is just really simple.",
            "It's just a bunch of rules turned out to be a lot harder.",
            "Now we are all interested in this sort of machine learning approach to machine translation, trying to learn these models and the reason we can really do this is because what we call parallel corpora that is the same meaning written in different languages occurs everywhere.",
            "So the classic example of this is the Rosetta Stone.",
            "So way back in ancient times, people like to write out messages in different languages.",
            "Here we've got a Gyptian hieroglyphs, ancient Coptic, an ancient Greek.",
            "I think those two are.",
            "And discovering these scripts is what allowed scholars to decipher hieroglyphs and basically in machine translation we just want to.",
            "We just want to automate this process if we have lots of data showing the same meaning in different languages, can we automatically discover?",
            "The correlations between them and us to learn to translate.",
            "So I think one of the first discovery was like in the hieroglyphs things circled a Royal names and they discovered this early on and discovering these cribs let's you start to."
        ],
        [
            "Code the code, the sequence.",
            "So this machine translation, really statistical machine translation, that the probabilistic approach really took off at Fred Jelinek's Group at IBM in the 80s.",
            "And this was an amazing group.",
            "They they did some of the early work on speech recognition, and they realize that the speech recognition models that were using over called probabilistic noisy channel models could just be applied to machine translation if they could get the data.",
            "It's a very interesting group that included people like Bob Mercer and Peter Brown.",
            "Bob Mercer's been in the news quite a lot recently for his political support of various causes, but those guys back then were computational linguists in Fred's group, and actually the story goes that Fred went away for the summer.",
            "They told Fred that we can do machine translation with this model, and he said that stupid it'll never work, and then he went away for the summer and they did it anyway, and when he came back they said look it works and he took credit for it.",
            "He didn't really very nice going anyway.",
            "He's also known for this famous quote at the time that when he was developing this speech, recognition models every time we got rid of a linguist, which if you were doing speech recognition at the time, you tend to hire lots of linguists in translation thing every time he followed when things got better.",
            "And that's still a sort of sentiment that's that's around today."
        ],
        [
            "Although it should, it should not be so I also like to show this.",
            "You probably can't read this, but so these guys pioneered this approach to statistical machine translation.",
            "At that time, no one was really thinking this way, and this is really the dawn of sort of big machine learning models.",
            "Learn, learn from data and really a transition from rationalism to empiricism.",
            "In this sort of computational research anyway, they submitted it to, uh, this was calling, yeah, it's conference.",
            "Back in around about 1990 I think maybe 89.",
            "Anyway they got this great review which says the validity of statistical information theoretic approach to empty has indeed been recognized and was universally recognized as mistaken by 1950, so it's a waste of time as they quote some textbook.",
            "And then the last sentence is great that the crude force of computers is not science.",
            "The paper is simply beyond the scope of calling, so this was the review they got in the paper was rejected.",
            "This turned out to be quite an influential paper in the end.",
            "So anyway, they got their paper published.",
            "Then all those guys."
        ],
        [
            "Joined Rentech a big.",
            "Hedge fund and forgot all about machine translation, but then the rest of the community picked up on these ideas and run with it, and that's what led to Google Translate and all these sorts of services.",
            "OK, so the core of what these guys proposed back then was called the noisy channel model to us.",
            "We normally think about just as Bayes rule that if we're trying to predict some conditional probability, like the probability of a English sentence given a French sentence input that we want to translate, we can use Bayes rule to decompose it like this.",
            "And the reason we might want to do this rather than just estimating the conditional directly is because we get this PAV term and is PF given any now.",
            "PV is our language model.",
            "That's what we've been talking about for the first half of this lecture.",
            "So by decomposing this we can train a language model separately from our conditional translation model.",
            "And this is very powerful, because one it's much easier to get monolingual data.",
            "That is to get bilingual data.",
            "We might get a best 100 million sentences of bilingual data that we can get trillions of words of monolingual English.",
            "So decoupling this allows us to train very big language models.",
            "And the reverse model separately.",
            "There's some other advantages to reverse model dimension in a moment."
        ],
        [
            "So one way of thinking of this noisy channel model is that we sort of feed in our our French and we get some roughly comperable English but slightly garbled.",
            "And then we feed that through our language model and it sort of ranks.",
            "Which one of these should I choose as my output?"
        ],
        [
            "So I'll put up one of the classic IBM models that came out of IBM in the 90s eighties.",
            "This was the simplest one.",
            "They were considerably more complex ones, but there's a simple generative model of source sentence.",
            "Given a target sentence, and there's some normalizes at the front.",
            "But the key thing is that there's some over something called a, which is an alignment which today we call attention, and we're just taking the product of the words SJ.",
            "Given the target word that they're aligned to, or they tend to.",
            "So this is IBM Model 1, and it's a great little model if you want to just learn alignments between parallel corpora.",
            "This things.",
            "Very fast and scaleable and gives good alignments and it's also basically what now people call hard alignment in that we for the as we are generating the output which using which word in the input it was generated from.",
            "The other thing is that we can train it exactly and it's one of the few unsupervised models that we can train with them that has it is convex.",
            "It has a unique maximum.",
            "It's called a model."
        ],
        [
            "OK, but it doesn't actually work for translation, but it's a nice one.",
            "That's a really good exercise to implement that and run it and align corpora.",
            "So more recently, people realize that.",
            "Deep learning was quite a good way to approach translation problem and or from this point of view of what sort of become known as sequence to sequence models or encoder decoder architectures.",
            "The idea being that we feed in some input.",
            "Here a Chinese sentence.",
            "We generalize it in some way, normally down to some sort of vector space representation, and then from that we want to be able to read out we want to generate our output here.",
            "Our English translation at the top.",
            "And we know that neural networks like recurrent networks are really good at embedding things in vector spaces.",
            "So maybe doing this instead of what people were doing previously would be a good way to approach things.",
            "So this is really."
        ],
        [
            "Taking off an neural empty is now the state of the art.",
            "One very simple idea is to take our recurrent neural network language model and just concatenate our strings and treat this like a language modeling problem where we read in the French we have some delimiter and then we read out the English.",
            "And this was the classic sequence to sequence model proposed a few years back by.",
            "Some guys at Google in the Google Brain team, so this is a really nice idea and some really simple way of looking at this problem.",
            "It doesn't work for translation like this has never come close to state of the art just doing this, but it's a really nice simple way of looking at things and gives us a building block for lots of problems."
        ],
        [
            "One of the reasons that doesn't work as we get this bottleneck here that no matter what sort of length of input we have to boil it down to this vector representation that we're going to then read out our output.",
            "So if we've got 2 words in two words of French or 100 words of friends, we're going to end up embedding in the same space.",
            "So we're going to have to choose a really big hidden layer representation to cope with 100 words, and when we're only doing 2 words were going to waste all of that computation.",
            "The model has to discover these long range dependencies, and then so to get this to work, you then have to do tricks like reversing.",
            "The strings and other sorts of things, but it still won't really get you."
        ],
        [
            "There.",
            "Now there we go, this reversing the strings."
        ],
        [
            "So what does get you there is the idea of attention that came out of the group here at Montreal.",
            "So this says, let's get round that bottleneck problem by instead of boiling everything down to a vector at each time step that we generate something, let's choose what we're what we're going to condition on or align to or attend to in the input.",
            "And this really is just a neural extension of this idea of alignment.",
            "We know that this is a very good bias in translation, especially French to English translation.",
            "We know that roughly French English word is going to be one or two or.",
            "Or a local group of words in the input, which is what we want to condition on to choose the right word.",
            "So this is introducing a structural bias that's very appropriate for this problem.",
            "Another way you can think of it."
        ],
        [
            "Rather than trying to boil everything down to a fixed size representation, now we take our representation to just be the concatenation of the input or that run through some recurrent networks, and that's what we generate from.",
            "So the basic sort of attention model you run."
        ],
        [
            "Recurrent networks in two directions over your input.",
            "So bidirectional recurrent networks.",
            "You concatenate these representations and then."
        ],
        [
            "Start generating output and every time you generate some output you attend to this input matrix.",
            "So you calculate a function.",
            "Sort of similarity or some other function with each of these input time steps.",
            "You then use that as a weight on that input and then you calculate your output softmax and so this is sort of shown here that we calculate a weight on each one of those attention indexes or alignments.",
            "One of them gets a lot of weight, the one that's the appropriate start because we know in English will just start with the noun and then when we translate we produce the appropriate output and we just keep doing this at each time step."
        ],
        [
            "So now at each time step the model gets to choose water conditions on.",
            "So."
        ],
        [
            "I said this is a very appropriate.",
            "Bias to having your translation model, but we know translation behaves like this.",
            "Even in languages like translating from Chinese to English, where there really isn't a one to one correspondence, attention still gives you really the right bias.",
            "The other thing this is doing is it's getting around that vanishing gradient problem in the recurrent network.",
            "So if we have our straight sequence to sequence model, we have a very long path between a given output translation.",
            "Say we're translating 30 word sentence to another 30 word sentence says lots of words in between.",
            "The thing we're producing the output and the thing it's translating in the input.",
            "What attention does it let you short circuit that path and go straight to the thing you're wanting to translate?",
            "That gives you a much shorter.",
            "Path feel gradient to go through.",
            "So training is much easier.",
            "Much easier for the model to discover."
        ],
        [
            "What you want to discover?",
            "OK, in the last few minutes I'm going to quickly talk about a different sort of attention model which tries to implement the classic noisy channel model, but in a sort of modern, neural empty sort of way, and this is something we played with sort of the end of last year at deep mind.",
            "Now there's lots and lots of different variants of neural empty that you might choose from.",
            "I'm not going to claim that this is the best, but I thought I'd talk about this part of this work we did, and also because it gives you a different way of looking at this.",
            "Problem, so the classic conditional neural empty system is just directly estimating the output given the input.",
            "This is nice, especially I've got loads of data, but it has a couple of problems.",
            "One you can't if you have lots of monolingual data you can't easily use that.",
            "There's been various proposals for how to do that, but none of them really scale in the same way that the old noisy channel model did back in the days of the noisy channel model, we knew that improving your translation model might give you sort of 1 blue point better translation.",
            "Doubling the amount of language modeling data you had might give you 10, so industry at least the game, was always to get a bigger language model.",
            "We can't easily do that with these conditional models.",
            "You also get this problem that is related to the explaining away problem that it's easy for the model to forget bits of the input, 'cause it doesn't have to explain the input when it's generating the output.",
            "If there's some word that's very low frequency input, it's quite likely that might just decide not to translate it, especially if it looks good from the sequence.",
            "So things like adjectives and things like that might just get dropped.",
            "'cause if you've got.",
            "The input the the pink dog, and that's not very frequent.",
            "You could just produce at the output the dog, and that's going to look good from the output distribution."
        ],
        [
            "Very easy to drop things.",
            "So.",
            "If we return to this noisy channel idea that gets around a lot of these problems.",
            "Now the Model 1.",
            "You've got this language model that you can train on any amount of monolingual data you have, and it doesn't even have to be a newer model.",
            "It can be one of the classic count models trained on a trillion words that if you're someone like Google, you might have lying around gathering dust.",
            "So, but now you have this reverse model that we call that the channel model that's producing the probability of the input given the output, and this means we have to explain what we observe.",
            "We have to explain the input we observed and now the model can't not explain words in the input by producing the output without incuring quite a penalty.",
            "So probabilistically this is a nice way to structure things.",
            "Now it's very straightforward to train models.",
            "For this we just correct monolingual data.",
            "Trainer.",
            "Recurrent Network is our language model.",
            "We train our normal attention model or whatever it is.",
            "But just with our input and output reversed to get a distribution over X given Y and we have a noisy channel model, so that's good.",
            "Training is easy.",
            "The problem is we can't decode with this model.",
            "So with the direct model, that is why given X, the chain rule allows us to."
        ],
        [
            "Factorize everything nicely so we can produce one word at a time.",
            "Once we're conditioning on the output that we haven't yet produced, we can't do that.",
            "So.",
            "So we need this factorization and that's what I'm going to present here.",
            "How do we actually get a model that we can decode within the noisy channel model?",
            "OK, so."
        ],
        [
            "In the direct model we have this classic sort of chain rule where we can decode greedy decoding.",
            "Works quite well, we can just at each time step produce the wire that's most probable given the the output we produced before and conditioning on the input.",
            "And this is all nice and very easy.",
            "We can make that a beam search if you want slightly better results.",
            "But this is good.",
            "This is the classic neural empty model and it's very easy to code."
        ],
        [
            "If we think about a noisy channel model, we might come up with something like this where we try and produce the output.",
            "The conditional X, adding one at a time.",
            "But of course you can't do that.",
            "That's not, that's not how probability works.",
            "That's not going to work out, so we have this problem now that our model wants to see all of the output before it can tell us anything about the probability of the input.",
            "So your normal sort of attention model is not going to work in this case because you can't produce your output one word at a time.",
            "You have to somehow come across the whole output."
        ],
        [
            "So what we want to do is break this model down.",
            "So what we're going to do is introduce an alignment variable that tells us how much of the input we need to read before we can produce some output.",
            "So we're going to think of this like we're seeing input words one of the time, and at some point the model decides I've seen enough words, I can produce some output now and then produce some output.",
            "Then we'll see some more input than producing more output.",
            "So we're going to introduce alignment variables, Ed.",
            "That's how much of the input that's our alignment.",
            "How much we've currently seen.",
            "So now we have an alignment, except our alignment is sequential.",
            "We're always incrementing this, and so we end up with this factorized distribution.",
            "Here we were predicting zed, and we were predicting whether we should read another word or produce some output for X, and we're going to just use a recurrent network to produce both of these."
        ],
        [
            "OK, so.",
            "And then we have described it.",
            "Now it's just another direct model for translation and we published this last year as a direct model.",
            "It's very similar to a model that Alex Graves published for speech recognition a few years back as well.",
            "It's actually a great paper to look at this 2012 paper 'cause it's really sort of preceding all these sequences sequence type models.",
            "And really Alex was thinking of the same sort of thing.",
            "But this is an online model of translation, so that's nice.",
            "It can actually translate as it's reading input, so it can translate continuously.",
            "The other thing is that for when you do have a fixed links input, we can with this model marginalized out all of these possible alignments.",
            "So in this diagram at the top.",
            "So forgive the exact examples.",
            "Actually, sentence compression rather than translation, but I'm compressing the vertical sentence on the output and we think of producing the output one word at a time by following a path through this grid.",
            "That's the Red Arrows, and every time we take an arrow to the right, we're producing an output word, and every time we take a narrow vertically down, we're reading another input word, so.",
            "By following that path, we're doing this process of reading more input, producing more output, and if we structure our recurrent networks just right, we can marginalized we can sum over all possible paths through this grid so we don't have to resort to reinforcement learning or anything like that.",
            "So that's really nice.",
            "It's a really nice little model.",
            "The other thing to understand about this is because we are always producing the output condition on some prefix of the input.",
            "So this means that we can reverse it in the noisy channel model, because now we know that whenever we see some of the output Y, we can get a distribution over X that we could have produced from that prefix.",
            "So that's what we."
        ],
        [
            "Undertake this model that we were using as a direct model and reverse it and so now this looks like instead of rolling out the columns were rolling out the rows as we as we decode.",
            "So as we see each additional word Y we can get we can fill out this grid for how the probability of producing all of the input.",
            "So we can do this search.",
            "There's one sort of complexity that we haven't got around, which is when you want to predict another word.",
            "You have to enumerate all of the whole vocabulary basically and redo the do a softmax for every word in the vocabulary, which is expensive, so you don't want to do that.",
            "So instead what we do is we have an approximate direct model that gives us a short list and then we just compute the probabilities for yet."
        ],
        [
            "OK, so that was a very quick sort of introduction of that model.",
            "As I said, I'm not sort of saying that this is the state of the art in translation.",
            "We tested this on.",
            "A classic sort of Chinese English data set called FPS and use data set, and it works really nicely.",
            "So the second from bottom line there is basically the noisy channel model.",
            "We have a channel language model.",
            "We also need a bias parameter for technical length reasons, but it works really well as incorporating the language model when we just interpolate a language model with a direct model.",
            "It doesn't work.",
            "You can think of this as if you just interpolate a direct model in the language model there, both trying to explain the output when you do the noisy channel model they're working together.",
            "To explain what's going on so that works really well, we can always also throw in the direct model if we're into that sort of thing, and it even works slightly better, but the point of that is."
        ],
        [
            "To get you thinking about other ways of thinking about these sequence to sequence models so we can use these same ideas like the noisy channel model and such.",
            "OK, hopefully I'm pretty much on time, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, well it's a it's a pleasure to be here in Montreal once again for the summer school, so I've been tasked with teaching you natural language processing.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to approach this by not trying to cover the whole field of natural language processing, but just focus on some core topics, particularly in this first lecture this morning.",
                    "label": 0
                },
                {
                    "sent": "Language modeling.",
                    "label": 1
                },
                {
                    "sent": "And a bit of machine translation and then this afternoon that would be more general this afternoon.",
                    "label": 0
                },
                {
                    "sent": "Or talk a bit more about some things we're doing at Deep Mind in trying to induce meaning, grounding and structure.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I thought I might start with a little bit about sort of what is this field?",
                    "label": 0
                },
                {
                    "sent": "Natural language processing?",
                    "label": 0
                },
                {
                    "sent": "How does it fit in?",
                    "label": 0
                },
                {
                    "sent": "There's lots of different terms you here at conferences where people present natural language work like computational linguistics, natural language processing, and all sorts of other related field like psycholinguistics, corpus linguistics.",
                    "label": 0
                },
                {
                    "sent": "So what are all these people doing?",
                    "label": 0
                },
                {
                    "sent": "So there's no formal definitions of these things, but I have my own sort of informal.",
                    "label": 0
                },
                {
                    "sent": "Why identify which of these fields which so linguistics?",
                    "label": 0
                },
                {
                    "sent": "Linguistics is the field that's largely concerned with the scientific study of language.",
                    "label": 0
                },
                {
                    "sent": "Trying to understand why language is the way it is this human languages.",
                    "label": 0
                },
                {
                    "sent": "Now we understand language how we communicate and trying to drive models of this models of language and lots of these concerns aren't necessarily to do with trying to produce models that are implementable or account for.",
                    "label": 0
                },
                {
                    "sent": "Data you might see on the web or text or things like that.",
                    "label": 0
                },
                {
                    "sent": "There's lots of other concerns in linguistics so often they don't match up with without concerns in artificial intelligence.",
                    "label": 0
                },
                {
                    "sent": "Particularly concern for linguistics that drives a lot of the research is how humans learn language and how do children learn language so quickly.",
                    "label": 1
                },
                {
                    "sent": "'cause there's still a lot of mystery about that because we fully don't understand language and we don't understand how we learn it, but we do have a lot of data.",
                    "label": 0
                },
                {
                    "sent": "OK, so computational linguistics is really the computational study of language, and this can be from the linguistic side we're trying to produce models which are computationally plausable of language, or you're trying to analyze models of language to say what is their complexity.",
                    "label": 0
                },
                {
                    "sent": "If we tried to implement this as an algorithm.",
                    "label": 0
                },
                {
                    "sent": "And also to produce tools to study language.",
                    "label": 0
                },
                {
                    "sent": "So there's a separate from tools that Google or Facebook or someone might use, but tools specifically to study language like parsing large corpora.",
                    "label": 0
                },
                {
                    "sent": "So then you can look at statistics of different constructions and things like this.",
                    "label": 1
                },
                {
                    "sent": "Computational linguistics overlaps a lot with what we call natural language processing, and for me natural language processing is really on the engineering side of things, so this is the actual building of tools that people might want to build and sell, and things like empty speech recognition text to speech.",
                    "label": 0
                },
                {
                    "sent": "All these sorts of things you're probably not familiar with.",
                    "label": 0
                },
                {
                    "sent": "So this is a very sort of informal distinction for me, computational linguistics is really the science side and natural language processing is really the engineering side.",
                    "label": 0
                },
                {
                    "sent": "And often these two don't match up.",
                    "label": 0
                },
                {
                    "sent": "Also, the natural in natural language processing, there's been some controversy recently about what this means for me.",
                    "label": 0
                },
                {
                    "sent": "Natural just means human languages or languages that occur in the world, not formal languages or artificial languages.",
                    "label": 0
                },
                {
                    "sent": "It's no statement about quality, it's not saying good language processing, it's just saying we're looking at English or French or something like that, not Python.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's where we.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll see it and this.",
                    "label": 0
                },
                {
                    "sent": "This lecture is principally going to be about language modeling.",
                    "label": 0
                },
                {
                    "sent": "A task in natural language processing about machine translation.",
                    "label": 0
                },
                {
                    "sent": "The second lecture I'll probably get a bit more onto the computational linguistics side of things and talk a bit about.",
                    "label": 0
                },
                {
                    "sent": "What linguistics tells us about language and what we might have some concerns we might have if we want to get to a sort of full artificial intelligence processing of language.",
                    "label": 0
                },
                {
                    "sent": "So language modeling, so you've probably seen a bit of this, maybe earlier in Joshua's lecture, but just to be formal, so language modeling in its formal sense is just modeling sequences of discrete symbols.",
                    "label": 0
                },
                {
                    "sent": "So we can say that we have some utterance W there, and it comes from Sigma Star, the space of all.",
                    "label": 0
                },
                {
                    "sent": "Discrete sequences.",
                    "label": 0
                },
                {
                    "sent": "And we want to get a probability distribution from a language model that is consistent over this space of string.",
                    "label": 0
                },
                {
                    "sent": "So that is over the space of all possible strings that sums to one.",
                    "label": 0
                },
                {
                    "sent": "Noting that there is no bound on length of strings.",
                    "label": 0
                },
                {
                    "sent": "So a language model tells us, given an utterance, that we see how probable is that.",
                    "label": 1
                },
                {
                    "sent": "And depending on what you trained, the model on probability, it can mean different things.",
                    "label": 0
                },
                {
                    "sent": "But often we think about it.",
                    "label": 0
                },
                {
                    "sent": "How likely am I to observe this this utterance in the data?",
                    "label": 0
                },
                {
                    "sent": "And that might seem like a very sort of simple abstract thing to worry about.",
                    "label": 0
                },
                {
                    "sent": "Putting a probability on, but it's extremely useful, so a huge number of applications and tasks rely on this basic ability to put probabilities on utterances from language.",
                    "label": 0
                },
                {
                    "sent": "So it means we can compare different utterances and say which is more likely.",
                    "label": 1
                },
                {
                    "sent": "And we can compare different choices of word order or different words.",
                    "label": 0
                },
                {
                    "sent": "And this is key to things like translation, speech recognition.",
                    "label": 0
                },
                {
                    "sent": "You're always trying to make a choice about given these possible outputs, I could produce which one is most likely.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so language modeling goes way back to his way back, but particularly in the war, people like Turing and good here.",
                    "label": 0
                },
                {
                    "sent": "We're trying to break German codes and they realized that they could take an information theoretic approach to this.",
                    "label": 0
                },
                {
                    "sent": "Before then, people generally employed linguists to do decryption.",
                    "label": 0
                },
                {
                    "sent": "But being computer scientists, these guys approached it from an information theoretic point of view and they realized that if they could get a good distribution on German messages, then that would.",
                    "label": 0
                },
                {
                    "sent": "That would really help them decode those messages.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they started to feel Dan.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And really, they also gave us a number of the probabilistic modeling techniques that up until recently we were pretty close to the state of the art.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I've introduced language modeling, just the basic problem of putting a probability on a string, but that string could always include some conditioning context, and if we do that then we can do lots of things like these examples here.",
                    "label": 0
                },
                {
                    "sent": "The Top Model machine translation you can just take your French string and your English string and concatenate them together and say if I have a good probability distribution on these strings, then I have a good model of translation.",
                    "label": 0
                },
                {
                    "sent": "We can conceptually take that further and say that maybe just question answering is just predicting what will come next.",
                    "label": 1
                },
                {
                    "sent": "What will someone say after a question?",
                    "label": 0
                },
                {
                    "sent": "Here we really need some context beta, which might be our knowledge of the world.",
                    "label": 0
                },
                {
                    "sent": "And then maybe we can even take that to dialogue.",
                    "label": 0
                },
                {
                    "sent": "Here, the beater is really rather important, but I'm just hiding it there in the conditioning context.",
                    "label": 0
                },
                {
                    "sent": "But the idea here is that language modeling is really general.",
                    "label": 1
                },
                {
                    "sent": "Problem language models are very general tools, so we can see a lot of problems just from this in language.",
                    "label": 0
                },
                {
                    "sent": "From this point of view of putting probability distributions on strings.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basis for pretty much pretty much all effective language models is decomposing this joint probability, which is what we want at the top.",
                    "label": 1
                },
                {
                    "sent": "There their utterance W one through N, decomposing its probability according to the chain rule.",
                    "label": 1
                },
                {
                    "sent": "So this allows us to sequentially score the probability or calculate the probability.",
                    "label": 1
                },
                {
                    "sent": "And it's also exact.",
                    "label": 0
                },
                {
                    "sent": "I mean, the key thing here is there's no assumption here.",
                    "label": 0
                },
                {
                    "sent": "This is just the rules of probability, right?",
                    "label": 0
                },
                {
                    "sent": "We can do this.",
                    "label": 0
                },
                {
                    "sent": "So then then it makes our task easier instead of having to model this big joint distribution, we can think of it sequentially.",
                    "label": 0
                },
                {
                    "sent": "So now we're thinking about we would call this a generative model.",
                    "label": 0
                },
                {
                    "sent": "We're thinking of a way to generate the string.",
                    "label": 0
                },
                {
                    "sent": "In sequence models like this, life is a bit easier than generative models.",
                    "label": 0
                },
                {
                    "sent": "In images where you don't necessarily have a natural sequence to the model.",
                    "label": 0
                },
                {
                    "sent": "But here in language we have a natural sequence.",
                    "label": 0
                },
                {
                    "sent": "Left to right ordering.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, there are questions that a little bit more in the in the second lecture.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we can build good language models.",
                    "label": 0
                },
                {
                    "sent": "If we can build really good language models at the same sort of level as a human, then that language model really must know quite a bit about language and really must understand it.",
                    "label": 0
                },
                {
                    "sent": "So in this basic task of language modeling, there is most of language understanding.",
                    "label": 1
                },
                {
                    "sent": "Which is cool because we can get limitless amounts of language data on the web.",
                    "label": 0
                },
                {
                    "sent": "You can get as much text as you want.",
                    "label": 0
                },
                {
                    "sent": "Trillions of words to train models.",
                    "label": 0
                },
                {
                    "sent": "The only limitation is your computation.",
                    "label": 1
                },
                {
                    "sent": "But as an example, if we have an utterance like there, she built A and I ask you what comes next, you're going to have a pretty high entropy distribution over what could come next, 'cause you really don't know any sort of noun or adjective.",
                    "label": 0
                },
                {
                    "sent": "All sorts of things could come after that utterance.",
                    "label": 0
                },
                {
                    "sent": "But if I give you an utterance like, Alice went to the beach.",
                    "label": 1
                },
                {
                    "sent": "There she built a.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to be thinking, well, there's a much lower entropy distribution.",
                    "label": 0
                },
                {
                    "sent": "There is probably going to be a sandcastle, maybe a boat something like that, but there are very few options.",
                    "label": 0
                },
                {
                    "sent": "And the reason you know that is because you understand the structure of language.",
                    "label": 0
                },
                {
                    "sent": "You understand that here she is referring to Alice.",
                    "label": 0
                },
                {
                    "sent": "There is referring to beach.",
                    "label": 0
                },
                {
                    "sent": "You understand that at beaches people tend to build sandcastles.",
                    "label": 0
                },
                {
                    "sent": "You're doing all this processing as you see that string which is hidden from us.",
                    "label": 0
                },
                {
                    "sent": "We'd like to build language models that can do that too.",
                    "label": 0
                },
                {
                    "sent": "That can resolve what we call Co references here.",
                    "label": 0
                },
                {
                    "sent": "They understand common sense relationships.",
                    "label": 1
                },
                {
                    "sent": "We nowhere near that, but you can see how in this simple language modeling objective of just predicting the next word.",
                    "label": 0
                },
                {
                    "sent": "There's a huge amount of understanding language.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the key thing with any modeling task is how we're going to evaluate it.",
                    "label": 0
                },
                {
                    "sent": "So the classic evaluation in language modeling is cross entropy or perplexity, depending on whether you exponentiate or not.",
                    "label": 0
                },
                {
                    "sent": "And you can think of cross entropy as a sort of measure of information orbits that you need to encode a message, and it tells you something about there's a deep connection between compression and language modeling.",
                    "label": 1
                },
                {
                    "sent": "If you have a good language model, then you have a good model for compressing.",
                    "label": 1
                },
                {
                    "sent": "Text there's a deep relationship there, so these are perplexity.",
                    "label": 0
                },
                {
                    "sent": "In particular, is a metric.",
                    "label": 0
                },
                {
                    "sent": "You tend to see people use, so normally you collect your training data.",
                    "label": 0
                },
                {
                    "sent": "Hopefully it's very large.",
                    "label": 0
                },
                {
                    "sent": "You train your model, then you have a test set which is separate from that and you score the perplexity and you say my model is better than the one that was published last year.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's our valuation.",
                    "label": 0
                },
                {
                    "sent": "We do have to think carefully about data for language modeling, so language modeling is a time series problem.",
                    "label": 1
                },
                {
                    "sent": "We're predicting what comes, what happens in the future, given what we've seen in the past.",
                    "label": 0
                },
                {
                    "sent": "So we want to be careful when we create data that we don't mix up the past in the future.",
                    "label": 0
                },
                {
                    "sent": "So normally, when you create a language modeling data set, often your data will come from maybe Newswire, or somewhere like that.",
                    "label": 0
                },
                {
                    "sent": "It has a natural ordering.",
                    "label": 0
                },
                {
                    "sent": "You want to make sure that your training data is in the past and your test data is in there is in the future.",
                    "label": 1
                },
                {
                    "sent": "Particularly if you have news or something like that or books you don't want to, you generally want to keep particles together in training or test.",
                    "label": 0
                },
                {
                    "sent": "You don't want to split them up, because if you see the first half of an article, it's much easier to predict the second half than if you've never seen that article before.",
                    "label": 0
                },
                {
                    "sent": "So you get much lower perplexities, and you may be misled.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About the quality of your model.",
                    "label": 0
                },
                {
                    "sent": "I bring these things up because despite how easy it is to collect language modeling data, we still don't actually have terribly good standard language modeling data sets.",
                    "label": 0
                },
                {
                    "sent": "The ones that people tend to use most commonly is something called the Penn Treebank data set which Depend Tree Bank is a collection of texts from the Wall Street Journal annotated with grammatical structures.",
                    "label": 1
                },
                {
                    "sent": "The structures are being thrown away and people just take the texts being processed a bit further.",
                    "label": 0
                },
                {
                    "sent": "It's been lowercased vocabulary is being restricted, punctuation is being thrown away, and the result is not a very useful data set for language modeling, but it's still extremely popular, but it's very small and it's quite a skewed representation of language.",
                    "label": 1
                },
                {
                    "sent": "So I discourage people from using that one.",
                    "label": 0
                },
                {
                    "sent": "The million word corpus was a data set that was created at Google to try and address some of these problems, but unfortunately it also had some other problems, particularly relating to that what I said in the previous slide about separating.",
                    "label": 0
                },
                {
                    "sent": "Keeping articles together and always testing on the future so there they just commuted.",
                    "label": 0
                },
                {
                    "sent": "Sentence is from a big data set and the problem is a test data set.",
                    "label": 0
                },
                {
                    "sent": "Crip has sentences from articles that are in the training data set so that tells you that for any given test sentence you probably can observe in the training data.",
                    "label": 0
                },
                {
                    "sent": "All the other sentence is in the article it came from which makes it a lot easier to predict that sentence.",
                    "label": 0
                },
                {
                    "sent": "Then if you kept the article separate recently, some data sets called Wikitext.",
                    "label": 0
                },
                {
                    "sent": "In a few different size released.",
                    "label": 0
                },
                {
                    "sent": "There still hasn't been a lot of results on these, but they do seem to be better, so they're not a bad place to start for language modeling.",
                    "label": 0
                },
                {
                    "sent": "So there's two versions of it.",
                    "label": 0
                },
                {
                    "sent": "It's at 3 million.",
                    "label": 0
                },
                {
                    "sent": "Someone correct me and 200 million.",
                    "label": 0
                },
                {
                    "sent": "There's a small and a big.",
                    "label": 0
                },
                {
                    "sent": "But their wiki articles they're nicely separated, so it's a bit more consistent, and we have the original, so you can choose vocabulary and all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is there a Clock here so I can see?",
                    "label": 0
                },
                {
                    "sent": "Now I'm going.",
                    "label": 0
                },
                {
                    "sent": "What time is it now?",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in this sort of 1st maybe 2/3 of the lecture, I'm just going to go through classic language modeling techniques from the more historical Now N gram language models and different neural network instantiations of language models.",
                    "label": 0
                },
                {
                    "sent": "You can sort of think of these together as all taking that chain rule decomposition of the joint probability and approximating the context in different ways.",
                    "label": 0
                },
                {
                    "sent": "So in the first set of models ngram models, we just throw away most of the context and only keep the recent words.",
                    "label": 0
                },
                {
                    "sent": "In the neural engram models we do the same thing, but we use a neural network to get a bit more parameter efficiency into the models and with recurrent neural net language models rather than truncating the history, we compress it all into a fixed size vector.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so count.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language model, so this is the classic approach to language modeling, an extremely scalable and simple way of doing language modeling.",
                    "label": 0
                },
                {
                    "sent": "And it all comes down to basically just counting the number of times you see different outputs.",
                    "label": 0
                },
                {
                    "sent": "So you make the Markov assumption, which is when we do this general decomposition rather than each decision being conditional on all of the decisions that came before.",
                    "label": 0
                },
                {
                    "sent": "We're going to say it's only conditional on the previous one word or two words, or three words.",
                    "label": 0
                },
                {
                    "sent": "So if it's one word, we call that a bigram Model 2 words, we call that a trigram.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic ngram model, and that's most of the sort of modeling idea there is.",
                    "label": 0
                },
                {
                    "sent": "So just conditioned on recent history.",
                    "label": 0
                },
                {
                    "sent": "So in the bottom example, here is a bigram model.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's very simple maximum likelihood estimators for these models, or we need to do so in the trigram case.",
                    "label": 1
                },
                {
                    "sent": "We're predicting a third word given the previous two, or we need to do is count up the number of times we saw those three words together, and then divide by the number of times we saw the context, and so this is really simple.",
                    "label": 0
                },
                {
                    "sent": "You can do this on trillions of words, it's very easy.",
                    "label": 1
                },
                {
                    "sent": "And quick.",
                    "label": 0
                },
                {
                    "sent": "On the basis of these models, the problem is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're going to want more context to get a better model, so just conditioning on the previous word, that's not going to be very good, so you might think I should condition on the previous two or three, or four or five.",
                    "label": 0
                },
                {
                    "sent": "The more words you condition on, the less likely you are to actually have seen that set of words.",
                    "label": 0
                },
                {
                    "sent": "And if you haven't seen that sequence of words, your account will be 0 and you'll get a probability of 0.",
                    "label": 0
                },
                {
                    "sent": "So that's no good, and this is a endemic problem for these sorts of models.",
                    "label": 0
                },
                {
                    "sent": "You don't want lots of zero counts, so here's my sort of silly example.",
                    "label": 0
                },
                {
                    "sent": "We may have never seen Montreal Burrito or Montreal beer drinker, but we want to score these trigrams an obvious thing to do is because we haven't seen those trigrams.",
                    "label": 1
                },
                {
                    "sent": "Why don't we just look at the bigrams the previous two words and this is the basis of all of the sort of state of the art N gram models.",
                    "label": 0
                },
                {
                    "sent": "This idea of backing off from longer context to shorter ones, and mixing these distributions together.",
                    "label": 0
                },
                {
                    "sent": "And there's two main approaches.",
                    "label": 0
                },
                {
                    "sent": "One is called back off, where if you haven't seen something, you back off to the lower order distribution and the other is interpolation where you just interpolate all of your distributions together.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of interpolation.",
                    "label": 0
                },
                {
                    "sent": "You just have.",
                    "label": 0
                },
                {
                    "sent": "So this is a trigram model and we end up with an interpolate linearly interpolating model between unigram.",
                    "label": 0
                },
                {
                    "sent": "So just the probability of producing that word independent of context, bigrams and trigrams.",
                    "label": 0
                },
                {
                    "sent": "And obviously your interpolation weights need to sum to 1 to get a valid probability.",
                    "label": 0
                },
                {
                    "sent": "And there's all sorts of ways of coming up with these interpolation weights, so I'm not going to go into those in these lecture.",
                    "label": 0
                },
                {
                    "sent": "In this lecture, there's people spent.",
                    "label": 0
                },
                {
                    "sent": "2030 years coming up with all sorts of convoluted ways of estimating these models.",
                    "label": 0
                },
                {
                    "sent": "The standard sort of state of the art for these sort of things are something known as an S and I.",
                    "label": 0
                },
                {
                    "sent": "And what it tries to do is it tries to match.",
                    "label": 0
                },
                {
                    "sent": "So if we look at language and we look at the frequency of unigrams and bigrams we see a power law known as this law.",
                    "label": 0
                },
                {
                    "sent": "So we tend to see a few things very frequently and lots of things very infrequently and this is a key nature of language, which is one of the reasons why natural language processing is hard, because there's lots and lots of infrequent things called the long tail.",
                    "label": 0
                },
                {
                    "sent": "Of the distribution, Nelson has a bias or a prior to specifically catch UPS distributions, and that's why it does so well.",
                    "label": 0
                },
                {
                    "sent": "There's this great paper from.",
                    "label": 0
                },
                {
                    "sent": "Well, almost 20 years ago now.",
                    "label": 0
                },
                {
                    "sent": "By Joshua Goodman and Stanley Chin that surveyed all of these techniques.",
                    "label": 0
                },
                {
                    "sent": "So if you want to sort of classic history of ngram modeling.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That and see just how many ways there are of doing this.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was my quick history of ngram models.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Where's that?",
                    "label": 0
                },
                {
                    "sent": "Different datasets like like how well those in Purple integrations are confirmed.",
                    "label": 0
                },
                {
                    "sent": "So then how important are the?",
                    "label": 0
                },
                {
                    "sent": "Differences now with like you know, building word or 30,000,000 document data set compared to this definition 98 for this study.",
                    "label": 0
                },
                {
                    "sent": "Depend women, so always the more data you have the better it gets.",
                    "label": 0
                },
                {
                    "sent": "So yes as we scale the data it gets better.",
                    "label": 0
                },
                {
                    "sent": "One thing we do observe is that all these different smoothing techniques matter less the more data you have, which is make sense because the zero account problem gets less and less as you get to trillion words and something that became popular maybe 10 years ago that came from Google with something called stupid back off, which was a way of doing this.",
                    "label": 0
                },
                {
                    "sent": "Back off where you didn't actually bother to normalizing these.",
                    "label": 0
                },
                {
                    "sent": "Interpolation weights when you do an ngram look up is costly when you've got very large language models, often on different servers.",
                    "label": 0
                },
                {
                    "sent": "So people gave up bothering to normalize properly and have a proper distribution, and still once you get to the sort of trillions of words, you get reasonable outputs.",
                    "label": 1
                },
                {
                    "sent": "So as your data set gets bigger, yes, lots of these concerns become less.",
                    "label": 0
                },
                {
                    "sent": "So these models are extremely scalable, and I'd say that's still no ones.",
                    "label": 1
                },
                {
                    "sent": "No ones running neural language models on the sort of scale that people were running N gram models, so these things are just extremely scalable.",
                    "label": 0
                },
                {
                    "sent": "They're also extremely fast to do a look up because you don't necessarily need to normalize over the vocabulary when you're looking up the probability.",
                    "label": 0
                },
                {
                    "sent": "Often it's just a table look up or a series of table look ups for the different backoff levels.",
                    "label": 0
                },
                {
                    "sent": "Also, the last point there is that the smoothing techniques like Ineson I specifically try and match the distribution of language.",
                    "label": 1
                },
                {
                    "sent": "None of the neural models that I'll get to incorporate this sort of bias, so it's interesting to ask, what if they did incorporate this bias would?",
                    "label": 0
                },
                {
                    "sent": "Could they do better?",
                    "label": 0
                },
                {
                    "sent": "So the disadvantages, lodging, lodging, grams, spouse once you go past five or so in your context history, you're just really not not going to see those phrases.",
                    "label": 0
                },
                {
                    "sent": "So you're not going to get much benefit, so you're never going to be able condition on things in the past, distant in the past.",
                    "label": 1
                },
                {
                    "sent": "So the sort of example I gave with the sandcastle at the start and end gram models.",
                    "label": 0
                },
                {
                    "sent": "Never going to do that.",
                    "label": 0
                },
                {
                    "sent": "Never going to learn those sort of dependencies.",
                    "label": 0
                },
                {
                    "sent": "It also doesn't learn any relationships between words, so cat dog.",
                    "label": 0
                },
                {
                    "sent": "These are just different integers in an N gram language model.",
                    "label": 0
                },
                {
                    "sent": "It has no sense of these things, both being animals being furry, cute pets, etc.",
                    "label": 0
                },
                {
                    "sent": "Also the same for morphology, so it has no sense that running and jumping both verbs are used in similar contexts.",
                    "label": 0
                },
                {
                    "sent": "Syntactically, so none of these things are captured.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So models that do capture them so N gram language models.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applied by Joshua.",
                    "label": 0
                },
                {
                    "sent": "About 15 years ago now.",
                    "label": 0
                },
                {
                    "sent": "I just a simple neural network parameterization of the account based model.",
                    "label": 0
                },
                {
                    "sent": "So my diagram of a feedforward network.",
                    "label": 0
                },
                {
                    "sent": "You have some input at the bottom, a hidden layer and some output.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "N gram models.",
                    "label": 0
                },
                {
                    "sent": "So imagine we want to build a trigram model.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have two inputs.",
                    "label": 0
                },
                {
                    "sent": "The previous two words W, N -- 1 and minus two.",
                    "label": 0
                },
                {
                    "sent": "Those sort of yellow boxes you imagine being one hot vectors of length of the vocabulary.",
                    "label": 1
                },
                {
                    "sent": "Just indexing which words they are.",
                    "label": 0
                },
                {
                    "sent": "They feed into a feedforward neural network.",
                    "label": 0
                },
                {
                    "sent": "We have a hidden layer.",
                    "label": 0
                },
                {
                    "sent": "We have transformations on those.",
                    "label": 0
                },
                {
                    "sent": "How exactly you do those transformations varies and can give you all sorts of different instantiations of these models.",
                    "label": 1
                },
                {
                    "sent": "But the basic idea is that we have a feedforward network we're feeding in the context.",
                    "label": 0
                },
                {
                    "sent": "And predicting a distribution over what comes next.",
                    "label": 0
                },
                {
                    "sent": "So the distribution of the top there is going to be a big long vector of the length of the vocabulary of words.",
                    "label": 0
                },
                {
                    "sent": "You could predict the output, so normally at least 10s of thousands, maybe hundreds of thousands of different options, and it would be a normalized distribution because we passed it through a softmax.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The cost of these models is all in that output distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the basic idea.",
                    "label": 0
                },
                {
                    "sent": "Now the reason this is attractive is because now that we're essentially projecting these input words and the output into a into a vector space and that gives us those relationships I mentioned on the previous slides about N gram models not capturing so dogs versus cats running and jumping, and these sorts of things.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to learn because those things occur in similar engrams, they're going to end up with similar embeddings in the space.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the basis for all of the word embedding models and everything else this idea.",
                    "label": 0
                },
                {
                    "sent": "So this is what this looks like.",
                    "label": 0
                },
                {
                    "sent": "We can sample from these models.",
                    "label": 0
                },
                {
                    "sent": "That's my little sort of sideways to order their means to get output, so we have something like we built a.",
                    "label": 0
                },
                {
                    "sent": "We can get the distribution over the next word.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what this looks like we feed in our input our start symbols, sample our output, feed that into the next time step the two conditioning context, and so on and so on, and we can sample strings like this from our distribution.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so you should have seen plenty of backpropagation and training a feedforward networks before, so this should all be very simple to train a model like this, it's just back propagation so.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The nice thing that in these models on our sequences, so at the bottom we've got all the different Ingrams in a sequence of forward predictions.",
                    "label": 0
                },
                {
                    "sent": "They all break apart nice and independently, so if the Red Arrows they're showing the gradients coming down from the back propagation, F is our objective function at the top as a gradients come down, they just flow down those columns, one corresponding to each engram prediction, and so they're all independent, which means we can calculate them all in parallel.",
                    "label": 1
                },
                {
                    "sent": "We can distribute them across different machines.",
                    "label": 0
                },
                {
                    "sent": "We can do this extremely.",
                    "label": 0
                },
                {
                    "sent": "Quickly and efficiently, so that's one of the main things to think about.",
                    "label": 0
                },
                {
                    "sent": "These ngram models is that we're truncating the history to only a few previous words, but that does give us a really great scalability.",
                    "label": 0
                },
                {
                    "sent": "So actually it was working on this model that I came up with asynchronous SGD because it was taking too much time to train and.",
                    "label": 0
                },
                {
                    "sent": "It could be paralyzed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a natural way to do it.",
                    "label": 0
                },
                {
                    "sent": "'cause you can.",
                    "label": 0
                },
                {
                    "sent": "This idea scales brilliantly.",
                    "label": 0
                },
                {
                    "sent": "And this is coming back now.",
                    "label": 0
                },
                {
                    "sent": "So there's been a number of recent language models based on convolutions, one from Deep Michael Black Net.",
                    "label": 0
                },
                {
                    "sent": "But these are.",
                    "label": 0
                },
                {
                    "sent": "You can essentially see as evolutions of the N gram language models that try and get longer range conditioning.",
                    "label": 0
                },
                {
                    "sent": "But but still get some of this ability to paralyze really easily and be efficient.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so neural engram models compared to classical count based models.",
                    "label": 0
                },
                {
                    "sent": "They have all these advantages of capturing correlations between the outputs and inputs in terms of memory and space are much more efficient because everything is getting compressed into this space.",
                    "label": 0
                },
                {
                    "sent": "So in terms of performance versus memory tradeoff, the neural engram models tend to be much better if you're just saying I want the maximum performance I can get with a GB of memory or something like this.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So you don't get this scaling like the count based models where the size of the model scales with the number of N gram counts.",
                    "label": 1
                },
                {
                    "sent": "But you still do get some scaling.",
                    "label": 0
                },
                {
                    "sent": "Obviously, the more data you have and the more you want to capture, you need bigger hidden layers and that then that does scale quadratically as you make the hidden layer bigger.",
                    "label": 0
                },
                {
                    "sent": "So you can't just expect you have a model with 1000 hidden units or works great on a million data points that you can get away with 1000 hidden units on a billion data points.",
                    "label": 1
                },
                {
                    "sent": "And my final point there is, we're still.",
                    "label": 0
                },
                {
                    "sent": "These models are just optimizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "We use things like dropout and things like that which changed it a bit, but mostly just optimizing likelihood.",
                    "label": 0
                },
                {
                    "sent": "And we know from count based models that likelihood is actually a bad objective for language.",
                    "label": 0
                },
                {
                    "sent": "But that's not not something that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One's really addressed yet.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is it about objective for language?",
                    "label": 0
                },
                {
                    "sent": "Because because of that power or nature of language, because if you optimize likelihood, you don't reserve enough probability for those rare events 'cause we know that language has very long tail.",
                    "label": 0
                },
                {
                    "sent": "So there's always going to be something we didn't expect.",
                    "label": 0
                },
                {
                    "sent": "So with likelihood you tend to just really discount that probability.",
                    "label": 0
                },
                {
                    "sent": "Put too much probability on the things you've seen.",
                    "label": 0
                },
                {
                    "sent": "OK, so the third in our series of models is recurrent networks.",
                    "label": 0
                },
                {
                    "sent": "Am I doing?",
                    "label": 0
                },
                {
                    "sent": "So I think you've had one lecture Premier show on recurrent networks and it should be obvious how these apply to language modeling.",
                    "label": 0
                },
                {
                    "sent": "So here's my little diagram of a feedforward network and recurrent network.",
                    "label": 1
                },
                {
                    "sent": "Now we're feeding the hidden layer from the previous time step into the next time step.",
                    "label": 0
                },
                {
                    "sent": "Conceptually very simple.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But has some some complications that creates for modeling.",
                    "label": 0
                },
                {
                    "sent": "So now we are sampling a string from our recurrent language model.",
                    "label": 0
                },
                {
                    "sent": "Rather than taking the previous inputs and having to feed them into the as the context of the next input, we just take the previous hidden state and that carries across and at each time step we're making our prediction just on the current input and the previous hidden state.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Looking something like that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so again, training with backpropagation with recurrent networks.",
                    "label": 0
                },
                {
                    "sent": "Backpropagation is a little bit more complicated because you get these dependencies across time, sort of signified.",
                    "label": 0
                },
                {
                    "sent": "There by little red loop on the right hand bottom hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Going to create some sort of cyclic dependency unless we do something about it.",
                    "label": 0
                },
                {
                    "sent": "So the classic thing to.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is to unroll our network across time and think of it for a given sequence.",
                    "label": 0
                },
                {
                    "sent": "Imagine this is our sentence of four words along the bottom.",
                    "label": 0
                },
                {
                    "sent": "There think of this as one big neural network that we can back propagate through and so we have our objective function F at the top and we back propagate it along all of those little Red Arrows.",
                    "label": 0
                },
                {
                    "sent": "And that's going to give us the right updates for SGD etc.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Downside of this is that it couples all of the updates.",
                    "label": 0
                },
                {
                    "sent": "So now if we look at one of these which are looking at the second hidden layer.",
                    "label": 0
                },
                {
                    "sent": "And we look at its gradient objective with respect to that hidden layer.",
                    "label": 0
                },
                {
                    "sent": "We're going to get contributions that flow all through the network in the future, because changing that hidden layer changes everything that comes after it.",
                    "label": 0
                },
                {
                    "sent": "So This is why we can't now break apart this network into lots of little independent networks and get the same sort of data efficiency that we had or.",
                    "label": 0
                },
                {
                    "sent": "Computational efficiency that we had with the neural ngram models.",
                    "label": 0
                },
                {
                    "sent": "So this makes scaling more difficult.",
                    "label": 0
                },
                {
                    "sent": "It means that you you can't really just take your whole corpus if you have a million words of text and you want to say I'm going to model this with recurrent network, that just starts at word zero and runs through a million time steps.",
                    "label": 0
                },
                {
                    "sent": "If you want to back propagate through that exactly, you're going to put that whole million time step sequence into memory and do all the calculations.",
                    "label": 0
                },
                {
                    "sent": "So often there's natural ways we can break up our data.",
                    "label": 0
                },
                {
                    "sent": "We might break text up into sentences or paragraphs.",
                    "label": 0
                },
                {
                    "sent": "Sentence is by far the most common and then back propagate individually on each sentence and treat them as independent.",
                    "label": 0
                },
                {
                    "sent": "It's suboptimal because we know that information from previous sentences influences the next one.",
                    "label": 0
                },
                {
                    "sent": "So again, the example I gave at the start of the sandcastle, we only get that nice prediction if we know what was in the previous sentence.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, an alternative is truncated backpropagation through time and this is just a heuristic where we say every so often some fixed number of time steps, let's say 20, which is going to break the gradient dependencies and pretend like these are separate sequences except we still forward propagate across these breakages.",
                    "label": 1
                },
                {
                    "sent": "So that's my little dotted line there as we're doing our forward propagation.",
                    "label": 1
                },
                {
                    "sent": "We still forward propagate exactly, but when we're doing the back propagation step, we stop at those truncation points.",
                    "label": 0
                },
                {
                    "sent": "Now you can take your million time step sequence of text and just break it up into regular 2015 hundred word segments and load those into memory.",
                    "label": 0
                },
                {
                    "sent": "That's also nice because it means that everyone of your sequence is the same length and that leads to very efficient processing on GPS and such architectures.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you break things up into sentences, they're all different lengths and you have to worry about how do I put different link sentences into one mini batch to get onto my GPU?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in language modeling, recurrent networks are really the state of the art for a given size of data.",
                    "label": 0
                },
                {
                    "sent": "As opposed to the N gram models, rather than truncating the history now what we're doing is saying we're still going to try and keep our infinite history, but we're going to try and compress it into this fixed size hidden layer.",
                    "label": 1
                },
                {
                    "sent": "That's a nice idea because you think further things are back in time.",
                    "label": 0
                },
                {
                    "sent": "Probably the less influence they have, but I can still keep a trace of them.",
                    "label": 0
                },
                {
                    "sent": "Certain things have much more influence than others.",
                    "label": 0
                },
                {
                    "sent": "For instance, recurrent networks are good at getting what we might call trigger words or things like that.",
                    "label": 0
                },
                {
                    "sent": "If you have a document about a particular politician like Donald Trump.",
                    "label": 0
                },
                {
                    "sent": "If you see Donald Trump once in an article, you're very likely to see that name again, so being able to record named entities, especially that you've seen in the past, gives you a much better distribution over what comes next in that document.",
                    "label": 0
                },
                {
                    "sent": "Recurrent networks are good at capturing that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "They're good at capturing any sort of trigger events in the second lecture.",
                    "label": 1
                },
                {
                    "sent": "Today, I'll go a bit more into what they're not good at capturing, and that's really the hierarchical syntactic structure of language.",
                    "label": 0
                },
                {
                    "sent": "Recurrent networks are still hard to learn, so it's not trivial to learn these things.",
                    "label": 1
                },
                {
                    "sent": "You tend to need a lot of computation.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of problems that I think you went through earlier about gradients disappearing or exploding.",
                    "label": 0
                },
                {
                    "sent": "We still have this problem that if we want more memory we have to increase the size of our hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So if we increase the size of our hidden layer, we can capture or we can we're doing less compression of the past so we can remember more.",
                    "label": 1
                },
                {
                    "sent": "But we have an increasing sort of quadratically increasing number of parameters.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into this, but memory based models, memory networks, differentiable neural computers and all these sorts of things.",
                    "label": 1
                },
                {
                    "sent": "Try and break that dependency by having an increase in memory without the increase in the number of parameters.",
                    "label": 1
                },
                {
                    "sent": "And again, we're still training with maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there are three basic approaches.",
                    "label": 0
                },
                {
                    "sent": "Two language modeling and one way to think of these are as I repeating is different ways of compressing the context, either by throwing things away or trying to compress into a into a fixed size vector or your leg cramps do both.",
                    "label": 0
                },
                {
                    "sent": "So the key thing here is that once we get to the recurrent network, then at least conceptually we have a model that could maybe solve that example I gave at the start.",
                    "label": 0
                },
                {
                    "sent": "If the model could in its hidden layer, capture.",
                    "label": 0
                },
                {
                    "sent": "Who had gone to the beach and what they were doing there?",
                    "label": 0
                },
                {
                    "sent": "Then it's possible it could make the right prediction in the future, so it's no longer throwing away that history.",
                    "label": 0
                },
                {
                    "sent": "Of course, for it to do that, it has to discover a great deal about the structure of language, how subjects and objects coordinate, and how anaphora like she work, which is quite a large ass, but at least we have a model now that conceptually is able to capture those sorts of things.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "All my examples so far, I've just been what we call simple recurrent networks.",
                    "label": 0
                },
                {
                    "sent": "There's a huge number of variants of this and I'm not going to go into these.",
                    "label": 0
                },
                {
                    "sent": "Besides again, I'm I'm sure you mentioned things like LS teams, another gated recurrent architectures, and if you're actually going to build a language model or anything else on our current network, you're going to use one of these gated architectures, 'cause that's the state of the art.",
                    "label": 0
                },
                {
                    "sent": "So if in doubt using STM, it's almost always going to work.",
                    "label": 0
                },
                {
                    "sent": "And as I'll show a few slides later, maybe it will work a lot better than than you actually think.",
                    "label": 0
                },
                {
                    "sent": "So LSD emser.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what other things can we do with our current networks?",
                    "label": 0
                },
                {
                    "sent": "We can make them deeper.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can add extra layers just like a deep.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feedforward network and relating this back to my comments about memory.",
                    "label": 0
                },
                {
                    "sent": "Now we can see if we make a network deeper like this.",
                    "label": 0
                },
                {
                    "sent": "We are increasing the amount of memory it has at any given time step 'cause now it has sort of 3 three times the width of H at each time step, but we've only had a linear increase in the number of parameters, so increasing depth like this is one way to increase memory in recurrent networks with only a linear increase in the number of parameters.",
                    "label": 1
                },
                {
                    "sent": "Generally, though, for a fixed size number of parameters which I might show in a moment as well.",
                    "label": 0
                },
                {
                    "sent": "If you fix a number of parameters, it seems like single layers often do just as well as making it deeper.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, if you're ever going to make these things deeper, always include skip connections and things like this, 'cause they work and they make training much faster so that is directly connecting each layer and the input to the output gets around a lot of the problems with gradient propagation.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So an alternative way of getting depth so this statement or a deep LSM is ambiguous.",
                    "label": 0
                },
                {
                    "sent": "We can go depth vertically like I showed there.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or there's also this idea of going horizontal with depth so we can add extra recurrent computations between each input.",
                    "label": 1
                },
                {
                    "sent": "So this idea has been pushed recently in the recurrent highway networks paper.",
                    "label": 1
                },
                {
                    "sent": "That sort of claiming state of the art language modeling performance.",
                    "label": 0
                },
                {
                    "sent": "So this doesn't give you the same.",
                    "label": 0
                },
                {
                    "sent": "Memory increase, but it conceptually at least allows you to compute more complex functions at each time step.",
                    "label": 0
                },
                {
                    "sent": "'cause that's the way we tend to think about these deep networks is the more layers you have, the more complex the function you can compute on the input.",
                    "label": 0
                },
                {
                    "sent": "So that's the other way of going deep.",
                    "label": 0
                },
                {
                    "sent": "Normally when people say deep LSD or something like that, they mean the previous kind.",
                    "label": 0
                },
                {
                    "sent": "But this is a different sort of of deep network.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one of the concerns we have in language modeling and using neural networks which comes up less in other other areas of deep learning is that our output spaces tend to be huge.",
                    "label": 0
                },
                {
                    "sent": "So if you collect.",
                    "label": 0
                },
                {
                    "sent": "Let's say a billion words of text.",
                    "label": 0
                },
                {
                    "sent": "You're probably going to find, maybe.",
                    "label": 0
                },
                {
                    "sent": "Half a million a million different unique tokens or word types as we call them.",
                    "label": 0
                },
                {
                    "sent": "So if you just take your vocabulary to be all the different words you saw in your output as your model gets bigger, your vocabulary is also going to get big.",
                    "label": 0
                },
                {
                    "sent": "And as I said earlier, language model follows a power law, so that tells you no matter how much data you gather, you're going to keep your vocabulary going to keep growing, so it's not going to get to a point where you've seen all of the words.",
                    "label": 1
                },
                {
                    "sent": "So this means that we have very big output spaces which are very costly to compute.",
                    "label": 0
                },
                {
                    "sent": "If you want to do your softmax on your neural network.",
                    "label": 0
                },
                {
                    "sent": "GPU's speed this up because this is what GPU's are great at doing.",
                    "label": 0
                },
                {
                    "sent": "Big vector matrix products but they also have their limits.",
                    "label": 0
                },
                {
                    "sent": "They also have their limits in terms of memory because these things take up a lot of memory so normally in neural language modeling we tend to have some way of.",
                    "label": 0
                },
                {
                    "sent": "Speeding up this output vocabulary computation, so in any large scale model, we're going to have to worry about this.",
                    "label": 0
                },
                {
                    "sent": "On lots of small scale test sets, people often have very sort of artificially small vocabularies, but we want to do something on a real real sort of scale.",
                    "label": 0
                },
                {
                    "sent": "We're going to worry about this.",
                    "label": 0
                },
                {
                    "sent": "So there's all sorts of various different solutions, so an obvious one is some sort of short list where we just keep the most frequent words.",
                    "label": 1
                },
                {
                    "sent": "You can make this more interesting.",
                    "label": 0
                },
                {
                    "sent": "You can mix up count based N gram models in newer models, so just do your newer model on the frequent words in your account based on the infrequent.",
                    "label": 0
                },
                {
                    "sent": "This is suboptimal 'cause the newer models are actually created.",
                    "label": 0
                },
                {
                    "sent": "Estimating the infrequent distributions 'cause they get these correlations between the words, but this is a sort of easy starting point.",
                    "label": 0
                },
                {
                    "sent": "You can do short list which sort of specific to each mini batch process, so that's also a popular approach.",
                    "label": 1
                },
                {
                    "sent": "Grab a mini batch of sentence is 1020 senators computer short list of the vocabulary?",
                    "label": 0
                },
                {
                    "sent": "Maybe sample it, maybe have some sort of function to compute it to get a smaller normalization and then do it get a different one for the next mini batch and then if you do that, maybe stochastically you'll get something that gives you a good distribution of the whole vocabulary.",
                    "label": 0
                },
                {
                    "sent": "You still have to worry about how you'll actually get a probability at Test time though.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another approach is to change the objective function.",
                    "label": 1
                },
                {
                    "sent": "So rather than optimizing likelihood, we could make we could come up with a different objective function.",
                    "label": 0
                },
                {
                    "sent": "Hope that it sort of correlates with what we are interested in, but which is easier to estimate.",
                    "label": 0
                },
                {
                    "sent": "So a classic version of this is noise contrastive estimation or other negative vertical negative sampling ideas.",
                    "label": 0
                },
                {
                    "sent": "So noise contrastive estimation.",
                    "label": 0
                },
                {
                    "sent": "The idea is to try and factor out the normalizer into a hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "Another hyperman extra parameter.",
                    "label": 0
                },
                {
                    "sent": "Here see so rather than, so we know that C is actually a function of what's W in age, but if we pretend that it's not, maybe we can get something.",
                    "label": 1
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scalable noise contrastive estimation.",
                    "label": 0
                },
                {
                    "sent": "Basically boils is problem down to saying at each point rather than estimated rather trying to maximize the probability of the word that I'm going to see next given all of the other words, I'm just going to sample some other words and then treat each sample like a binary decision problem to maximize the probability of seeing the right word an rejecting the wrong word.",
                    "label": 0
                },
                {
                    "sent": "So this comes out just like a binary decision problem and it tends to work quite well and give something that that maximizing this also maximizes the likelihood quite nicely.",
                    "label": 0
                },
                {
                    "sent": "And it's very efficient.",
                    "label": 0
                },
                {
                    "sent": "'cause now you're just doing binary estimation.",
                    "label": 0
                },
                {
                    "sent": "Also, you can often get away with very few samples, especially if you're doing large scale list estimations.",
                    "label": 0
                },
                {
                    "sent": "OK, here is the number of samples, so we sample them from a noise distribution.",
                    "label": 0
                },
                {
                    "sent": "A very effective noise distribution is just a unigram frequency of words.",
                    "label": 1
                },
                {
                    "sent": "OK. Again, one of the issues with this sort of approach is it only speeds up training at Test time.",
                    "label": 1
                },
                {
                    "sent": "If you want probability, you still have to normalize over the whole vocabulary so it hasn't.",
                    "label": 1
                },
                {
                    "sent": "It hasn't helped you.",
                    "label": 0
                },
                {
                    "sent": "Some people have suggested that you can get away without normalizing at Test time.",
                    "label": 0
                },
                {
                    "sent": "We've played with this a bit and it doesn't really work.",
                    "label": 0
                },
                {
                    "sent": "You tend to get lots of variants, so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It might work one day, but but not the next.",
                    "label": 0
                },
                {
                    "sent": "Another approach like this is something that was called important sampling, not necessarily connected to important sampling.",
                    "label": 0
                },
                {
                    "sent": "In Monte Carlo techniques, but a similar sort of idea which is very similar to the noise contrastive loss except now.",
                    "label": 0
                },
                {
                    "sent": "Rather than doing these binary decisions between the noise and the true output, we collect the noise samples into a multi way softmax between the noise and the output.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another approach, and personally my favorite one is to factorize the output vocabulary so we can always make these sort of factorization assumptions.",
                    "label": 1
                },
                {
                    "sent": "So if we knew that our vocabulary could be split into different classes or different clusters of words nonoverlapping groups, then we could factorize a distribution in first predicting which class the output is going to belong to, and then given that class predict which of the words in that class the output is.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Just in the sort of statement there, this is exact.",
                    "label": 0
                },
                {
                    "sent": "If we have no.",
                    "label": 0
                },
                {
                    "sent": "Ignoring parametric assumptions on the distribution, once we parameterized things, it becomes inexact, but we can always factorize a distribution like this.",
                    "label": 0
                },
                {
                    "sent": "The simplest sort of factorization is to just take your vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So if you have 10,000 output words, split them into 100 classes of 100 words each.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of algorithms for doing that.",
                    "label": 1
                },
                {
                    "sent": "There's something called Brown clustering, which is particularly effective, and that will give you a sort of quadratic speedup.",
                    "label": 0
                },
                {
                    "sent": "Which is quite nice.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the key thing there is you need some way of coming up with the factorization and that does affect the result.",
                    "label": 0
                },
                {
                    "sent": "Now you can take this idea further and say well if we can factorize once, why not do it again?",
                    "label": 0
                },
                {
                    "sent": "And why not go down to binary decisions rather than any decisions and so then we get a tree.",
                    "label": 0
                },
                {
                    "sent": "So we can imagine arranging our vocabulary in a tree where at each branching point is binary and predicting any word comes down to following a path in this tree making binary decisions.",
                    "label": 0
                },
                {
                    "sent": "This is very attractive because then we get a log in speed up.",
                    "label": 1
                },
                {
                    "sent": "So now we can get really large vocabularies at quite low cost, so that's nice.",
                    "label": 1
                },
                {
                    "sent": "The downside is that you have to come up with a tree.",
                    "label": 0
                },
                {
                    "sent": "So where does the tree come from?",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of ways of doing this, none of them are terribly satisfactory.",
                    "label": 1
                },
                {
                    "sent": "And it tends to, at least in my experience, not perform as well as some of the simpler factorizations.",
                    "label": 0
                },
                {
                    "sent": "Some of the suggestions like just doing trees based on frequency or Huffman coding really don't work very well.",
                    "label": 0
                },
                {
                    "sent": "OK, more recently there's some variance of this from Facebook, so the reference at the bottom there where they get a sort of unbalanced factorization with the aim of making it very efficiently GPU, so this is quite an attractive approach as well.",
                    "label": 0
                },
                {
                    "sent": "Trying to tune this to a GPU, the ordinary sort of factorizations don't tend to run simply on GPUs.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here's a sort of summary slide of all these different.",
                    "label": 0
                },
                {
                    "sent": "Factorization ideas and a different sort of speedups you get.",
                    "label": 0
                },
                {
                    "sent": "So your full softmax is obviously linear in the number of words you've got to predict, both at training time and test time.",
                    "label": 0
                },
                {
                    "sent": "If you do a factorization, you can get a square root V speed up.",
                    "label": 0
                },
                {
                    "sent": "The worst case at Test time is still V, But you can actually generally do much better than that if you enumerate your classes one of the time.",
                    "label": 0
                },
                {
                    "sent": "So this is to sample.",
                    "label": 0
                },
                {
                    "sent": "So you want to sample from the distribution.",
                    "label": 0
                },
                {
                    "sent": "If you enumerate the classes in order of probability, you'll probably sample something much quicker than having to enumerate all of the classes.",
                    "label": 0
                },
                {
                    "sent": "The same is true for the balance tree factorization.",
                    "label": 0
                },
                {
                    "sent": "Here you get the best sort of speedup of log V, and again, worst case is linear, but you can generally do much better, and then you have the sort of negative sampling options at the bottom.",
                    "label": 0
                },
                {
                    "sent": "Which are very fast at training time 'cause normally you can get away with a very small number of samples K but don't help you much at Test time.",
                    "label": 0
                },
                {
                    "sent": "One thing to remember is it as researchers we tend to care a lot about training time 'cause we're training lots of models, But if you're actually trying to deploy something industry you care a lot about test time.",
                    "label": 0
                },
                {
                    "sent": "So if that model takes long to train, that's OK because you're going to do this a lot when you actually deploy it, so there's often a mismatch between what researchers care about.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What industry cares about in these problems?",
                    "label": 0
                },
                {
                    "sent": "OK, a different way of getting around this vocabulary problem is just predict smaller output granularities.",
                    "label": 0
                },
                {
                    "sent": "So an obvious one is let's predict characters relevant words.",
                    "label": 0
                },
                {
                    "sent": "So there's not that many characters, at least in English.",
                    "label": 0
                },
                {
                    "sent": "In languages like Chinese and Japanese, is a lot more characters, but still only in the order of thousands.",
                    "label": 0
                },
                {
                    "sent": "English you've only got maybe 100 different characters to worry about, so you're going to get very fast.",
                    "label": 0
                },
                {
                    "sent": "Softmax is if you predict characters.",
                    "label": 0
                },
                {
                    "sent": "The downside is that there's a lot more characters in an utterance, so you're going to have a lot more current time steps for these two sort of trade off against each other.",
                    "label": 0
                },
                {
                    "sent": "And depending on how big your softmax is, the question is which ones dominating your time.",
                    "label": 0
                },
                {
                    "sent": "Is it the recurrent calculation or is it the softmax calculation?",
                    "label": 0
                },
                {
                    "sent": "Even to softmax and going to characters might speed things up things up if it's a recurrent computation then it might slow things down.",
                    "label": 0
                },
                {
                    "sent": "The other problem is that predicting characters hard, so we know that language, or at least in English, and very strongly in English words tend to group in the language tends to group in nice word discrete units.",
                    "label": 0
                },
                {
                    "sent": "We lose that if we go down to characters, and so the model has to discover it for itself.",
                    "label": 0
                },
                {
                    "sent": "And it's also going to need a lot more memory to do this.",
                    "label": 0
                },
                {
                    "sent": "'cause it's remembering back a lot more time steps just to cover one word or two words in the past.",
                    "label": 0
                },
                {
                    "sent": "So generally we need much bigger hidden layers.",
                    "label": 0
                },
                {
                    "sent": "So I'd say most of the sort of language modeling work I've seen on big data sets we tend to be able to run them such that increasing the with a big enough hidden layer that increasing it doesn't make things much better.",
                    "label": 0
                },
                {
                    "sent": "Once we get the characters were definitely not at that point yet.",
                    "label": 0
                },
                {
                    "sent": "So even when we've got 10,000 hidden units or models this big, it's still clear that if we could increase up to 100,000, it would get a lot better.",
                    "label": 0
                },
                {
                    "sent": "So we're still very much.",
                    "label": 0
                },
                {
                    "sent": "In a sort of saturated.",
                    "label": 0
                },
                {
                    "sent": "Regime.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about characters is we capture all sorts of word internal structure that we know.",
                    "label": 0
                },
                {
                    "sent": "Language has like morphology, relationship between words.",
                    "label": 0
                },
                {
                    "sent": "There's not that much of that in English, so it doesn't necessarily make much of a difference in English.",
                    "label": 0
                },
                {
                    "sent": "But if you were doing French, this makes a significant significant difference.",
                    "label": 0
                },
                {
                    "sent": "If you're doing Turkish or Finnish or some very morphologically inflected language like that, then it can make a huge difference.",
                    "label": 0
                },
                {
                    "sent": "So what about out of vocabulary words, which wouldn't?",
                    "label": 0
                },
                {
                    "sent": "Yeah, good point.",
                    "label": 0
                },
                {
                    "sent": "So the other thing, of course, is that if you're doing things on the word level, you have to assume what your vocabulary is, so you have a fixed number of words, whatever that is.",
                    "label": 0
                },
                {
                    "sent": "With characters you have an open vocabulary model.",
                    "label": 0
                },
                {
                    "sent": "You can assign a probability to any sequence of characters, whether you've seen that word before or not, and so that's very attractive 'cause you don't have to make this decision about what's in the vocabulary wants out of it, and because words have this nice internal structure, and especially for languages.",
                    "label": 0
                },
                {
                    "sent": "With inflection, models are going to be actually able to predict inflections of words they've never seen inflicted in that way before, so it may have seen the root, and it may have seen lots of other words inflected with the right case or whatever it's needed, but never that word, but because things are if the morphologies nice and regular, the model may actually be able to guess what the right inflection of that that word is.",
                    "label": 0
                },
                {
                    "sent": "That sort of the Holy Grail of character language models and morphology is to get them to predict inflections they've never seen before.",
                    "label": 0
                },
                {
                    "sent": "Almost certainly I'm not familiar with.",
                    "label": 0
                },
                {
                    "sent": "Secret modeling, but I would assume that people are using recurrent networks on the show.",
                    "label": 0
                },
                {
                    "sent": "Do you know of any examples?",
                    "label": 0
                },
                {
                    "sent": "I've seen papers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's an obvious thing to go for, but yes, there's lots of things that give nice sequential dependencies, even things that aren't very sequential.",
                    "label": 0
                },
                {
                    "sent": "So it turns out the recurrent networks actually work very well as generative models of images, so this is a sort of pixel RNN.",
                    "label": 0
                },
                {
                    "sent": "Series of work where you just assume an ordering to the pixels in the image and then treat them like a big long sequence and that can work surprisingly well.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, what am I up to drop out?",
                    "label": 0
                },
                {
                    "sent": "So just quickly I think I've seen plenty of drop out before, but when we get to recurrent networks, there are some extra concerns about how we use dropout.",
                    "label": 0
                },
                {
                    "sent": "So dropout is good if we don't use some sort of regularization like dropout, we tend to not get very good results.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we so my little diagram here, I put these little dropout boxes on the vertical sort of feedforward connections, and I haven't put them on the recurrent connections.",
                    "label": 0
                },
                {
                    "sent": "If you put straight normal dropout on recurrent connections there and sample them independently, what's going to happen is that the recurrent networks going to lose its ability to capture long range dependencies.",
                    "label": 0
                },
                {
                    "sent": "If you think about any hidden unit and think at each time step you sample maybe from a Bernoulli with.",
                    "label": 0
                },
                {
                    "sent": ".5 probability whether to keep it or not given a few time steps, you're almost certainly going to have zero doubt that that hidden unit.",
                    "label": 0
                },
                {
                    "sent": "So now you're not going to get any sort of propagation very far back in time, so just putting sort of vanilla dropout in between recurrent connections kills any sort of long range dependencies, so it's a bad idea.",
                    "label": 0
                },
                {
                    "sent": "So the first sort of idea that people came up with, or the by far the most common approach, is just to put dropout between the inputs and the outputs, but don't touch the recurrent connections and this works well and you get a nice improvement, but you're still not regularising these recurrent connections, you get some.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "An observation by urine gal.",
                    "label": 0
                },
                {
                    "sent": "Was that maybe we could keep these drop out on the recurrent connections if we only sampled the months per sequence.",
                    "label": 0
                },
                {
                    "sent": "So if you just sample one mask and use them at every time step, then you don't have this problem of in expectation, every hidden unit being zeroed out.",
                    "label": 0
                },
                {
                    "sent": "So that's nice, of course.",
                    "label": 0
                },
                {
                    "sent": "The downside is now you're only sampling once per sequence, so you might have to see the sequences more often to get a good sort of stochastic estimate.",
                    "label": 0
                },
                {
                    "sent": "Urine justified this from a sort of various variational argument and sort of Asian POV, but it just makes intuitive sense that if you sample at once, you get the right sort of behavior.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just as a sort of a side or end point to this discussion earlier on I said don't use the Penn treebank to do language modeling.",
                    "label": 0
                },
                {
                    "sent": "And I'll give some examples here why not so in Group and deep mind we thought we'd look at this data set and the sorts of results people were predicting and just ask the question of how solid are some of these numbers, particularly how solid the baselines people are using.",
                    "label": 0
                },
                {
                    "sent": "So the in every paper where someone claims are sort of new neural architecture, they always compare it to someone celestium baseline from X number of papers ago.",
                    "label": 0
                },
                {
                    "sent": "So we thought, how well can we actually do with the baseline.",
                    "label": 0
                },
                {
                    "sent": "If we do a big Bayesian optimization of its hyperparameters, so Gabor Melis, one of the guys at DeepMind, did this and found some interesting results.",
                    "label": 0
                },
                {
                    "sent": "So there's a double lines in the middle above that result reported from various different papers.",
                    "label": 0
                },
                {
                    "sent": "And the state of the art was in on this data sets considered to be sort of the current highway networks or the more recent ones trained with sort of a evolutionary type objective to try different architectures.",
                    "label": 0
                },
                {
                    "sent": "What we found though if we do a big high performance search over the sorts of high parameters people haven't worried about all sorts of different things about how the models are trained.",
                    "label": 0
                },
                {
                    "sent": "Balances between output parameters versus hidden parameters given a fixed parameter budget, we actually get really rather good results of an ordinary STM.",
                    "label": 0
                },
                {
                    "sent": "So what we found is that an ordinary LS gym with one layer, which is like the first line there with 10,000,000 parameters, beats anything anyone's ever published on this data set.",
                    "label": 0
                },
                {
                    "sent": "We also did the same sort of optimization for some of the other models, particularly the current highway network, and we can make their results slightly better, but not much better and.",
                    "label": 0
                },
                {
                    "sent": "Sort of intuitive thing here is that people tend to do quite a lot of optimization of the model, say there proposing, but it's rare that we go back and try and re optimize the baseline or the benchmark.",
                    "label": 0
                },
                {
                    "sent": "And so when we do a big optimization, what we find is that LS teams work really well and they're really quite hard to beat now.",
                    "label": 0
                },
                {
                    "sent": "Some of this is almost certainly an artifact of this data set, which is why I say this data set is bad.",
                    "label": 0
                },
                {
                    "sent": "It's very small and it's very sensitive to these high parameters.",
                    "label": 0
                },
                {
                    "sent": "Very interesting as one of the students in our lab has done something very similar but with.",
                    "label": 0
                },
                {
                    "sent": "Learning models.",
                    "label": 0
                },
                {
                    "sent": "Hyperparameters he found was sometimes not uniquely optimizes the random seed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so any of that.",
                    "label": 0
                },
                {
                    "sent": "I don't think we we may have.",
                    "label": 0
                },
                {
                    "sent": "We may have done the sideboards pretty thorough.",
                    "label": 0
                },
                {
                    "sent": "But yes, we did make a difference randomly, so one sort of extra features I suppose is not necessarily high frame it randomly zeroing out in truncated backdrop whether you actually forward propagate seems to stabilize training quite a lot.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's sort of something that people do often.",
                    "label": 0
                },
                {
                    "sent": "Suggest that if we are optimizing random C. Runs in their in their hyperparameter search.",
                    "label": 0
                },
                {
                    "sent": "Is going to get the best results, yes, so well, especially when we have things that are very sensitive parameters.",
                    "label": 0
                },
                {
                    "sent": "So my takeaway from this is, well, one else.",
                    "label": 0
                },
                {
                    "sent": "Teams are really good and to this data set is extremely sensitive to high parameters and I don't have any proof for it and we were trying and we're sort of trying to get that in some of our experiments, but my feeling is that as you scale the data sets bigger, they get less sensitive to parameters.",
                    "label": 0
                },
                {
                    "sent": "So if we're comparing numbers on bigger data sets, we probably wouldn't see this sort of variation.",
                    "label": 0
                },
                {
                    "sent": "And we do have numbers on the sort of wikitext ones, and we don't see quite as big.",
                    "label": 0
                },
                {
                    "sent": "Jumps on those.",
                    "label": 0
                },
                {
                    "sent": "That said, it's also harder to do the high parameter optimization on bigger data sets slower, and we do have an awfully large number of GPU's, so we have an advantage with that.",
                    "label": 0
                },
                {
                    "sent": "But so yeah, the take home here.",
                    "label": 0
                },
                {
                    "sent": "I'd say two things.",
                    "label": 0
                },
                {
                    "sent": "One else teams are good too.",
                    "label": 0
                },
                {
                    "sent": "Don't use the pen treebank language modeling data set.",
                    "label": 0
                },
                {
                    "sent": "Which was partly my own by making it so hard to beat the baseline that people would stop using this data set.",
                    "label": 0
                },
                {
                    "sent": "How do you know it's an application?",
                    "label": 0
                },
                {
                    "sent": "Well, that's why I said I can't prove this.",
                    "label": 0
                },
                {
                    "sent": "This is my.",
                    "label": 0
                },
                {
                    "sent": "My intuition is that these things are less sensitive on the bigger data sets.",
                    "label": 0
                },
                {
                    "sent": "Because hopefully the things like the random seed.",
                    "label": 0
                },
                {
                    "sent": "All these sort of random elements of the model.",
                    "label": 0
                },
                {
                    "sent": "These parameters are just shifting, adding a different sort of randomness, like on this data set.",
                    "label": 0
                },
                {
                    "sent": "We know that vanilla SGD works better than any of the other optimizers, like Adam or any of these things, and partly that seems to be just a bit of extra randomness.",
                    "label": 0
                },
                {
                    "sent": "That seems to matter less and more data you have, because then the data starts to overwhelm the randomness and you're less likely to just go off in different bad directions.",
                    "label": 0
                },
                {
                    "sent": "So that is my intuition that the more data you have, the more the data overwhelms all the high parameters and there's less sensitivity 'cause you end up with the right distribution in the end.",
                    "label": 0
                },
                {
                    "sent": "With these small data sets, that's not really the case.",
                    "label": 0
                },
                {
                    "sent": "It seems it matters a lot.",
                    "label": 0
                },
                {
                    "sent": "We used to.",
                    "label": 0
                },
                {
                    "sent": "Just wondering, wondering.",
                    "label": 0
                },
                {
                    "sent": "Depth yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "Depends on the model.",
                    "label": 0
                },
                {
                    "sent": "So for the STM it's vertical depth for the recurrent highway network, it's depth in time.",
                    "label": 0
                },
                {
                    "sent": "3rd Elystan with Gru.",
                    "label": 0
                },
                {
                    "sent": "So we so we did, and some of the hyperparameters are.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's lots of different variants of LS teams with different choices, and they're actually in our hyper research, so that the hyper research does try tying the the different gates and things like that that give you similarities of GI use, and Gumballs tried various giu things and I don't think there's my feelings are not much difference to those.",
                    "label": 0
                },
                {
                    "sent": "But we know with this data set like tying parameters like tying input and output parameters helps quite a bit.",
                    "label": 0
                },
                {
                    "sent": "This is something that other people have observed and we know that's not necessarily the case for big data sets, but again, it's that question not having so much data.",
                    "label": 0
                },
                {
                    "sent": "Where is that question?",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK. For example, for LSD.",
                    "label": 0
                },
                {
                    "sent": "Ferrari.",
                    "label": 0
                },
                {
                    "sent": "That I couldn't tell you.",
                    "label": 0
                },
                {
                    "sent": "Actually, that's a good question about how, how much the optimizer, how long the optimization takes, and it's a good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's another factor here that we're not really testing, which is how reliant.",
                    "label": 0
                },
                {
                    "sent": "How easy is it to get certain results with certain parameters, so you might find that maybe there are current highway network is less sensitive parameters, and whatever you choose you end up with roughly this number, where is the LM, is extremely sensitive and you have to choose just the right ones to get a really good number.",
                    "label": 0
                },
                {
                    "sent": "I don't know the answer to that one, we haven't really.",
                    "label": 0
                },
                {
                    "sent": "Looked at that closely.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "We'll put it up at some point, but this is just in internal investigations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, first.",
                    "label": 0
                },
                {
                    "sent": "About the.",
                    "label": 0
                },
                {
                    "sent": "Multiple.",
                    "label": 0
                },
                {
                    "sent": "Lucky.",
                    "label": 0
                },
                {
                    "sent": "We are leaving the system.",
                    "label": 0
                },
                {
                    "sent": "So how do we explain?",
                    "label": 0
                },
                {
                    "sent": "So again, my intuition.",
                    "label": 0
                },
                {
                    "sent": "I think people have looked at this smaller data set the more you have to worry about the local minima and the more data you have that you do tend to get more, even local minima.",
                    "label": 0
                },
                {
                    "sent": "On these data sets is clearly bad.",
                    "label": 0
                },
                {
                    "sent": "Now some of the high parameters were changing.",
                    "label": 0
                },
                {
                    "sent": "Change the objective function so they changed the architecture of the model or something like that so.",
                    "label": 0
                },
                {
                    "sent": "Then getting a different surface, some of them like random seeds or the choice of the optimizer.",
                    "label": 0
                },
                {
                    "sent": "The same objective function, just with different optimizers, and we do see that particular optimizers get stuck in much worse.",
                    "label": 0
                },
                {
                    "sent": "Local Optima.",
                    "label": 0
                },
                {
                    "sent": "So that way their local Optima or they're just very flat regions.",
                    "label": 0
                },
                {
                    "sent": "I couldn't say, but yes, there's definitely differences there.",
                    "label": 0
                },
                {
                    "sent": "And on these data sets we do see that.",
                    "label": 0
                },
                {
                    "sent": "Not on the Penn treebank.",
                    "label": 0
                },
                {
                    "sent": "We've done it on things like the other prize data set and that again you've got.",
                    "label": 0
                },
                {
                    "sent": "Once you get to characters, you've got more time steps, so in some sense you've got more data and those are more stable.",
                    "label": 0
                },
                {
                    "sent": "But we again we do see that generally.",
                    "label": 0
                },
                {
                    "sent": "The numbers that people are reporting for an STM as a benchmark and the sorts of things we can get if we heavily optimize it.",
                    "label": 0
                },
                {
                    "sent": "There is still a gap and LCMS, as far as we can tell, a pretty much state of the art on most problems.",
                    "label": 0
                },
                {
                    "sent": "I think I think on the Hunter price with the current highway network was still slightly better, but then the state of the art number was run for months or something like that, and we haven't actually run something that long.",
                    "label": 0
                },
                {
                    "sent": "Not a problem with our.",
                    "label": 0
                },
                {
                    "sent": "Running their base is always in DLC and is always used as a baseline.",
                    "label": 0
                },
                {
                    "sent": "People just aren't incentivized hyperparameter there.",
                    "label": 0
                },
                {
                    "sent": "Is this issue that you really should spend as much time optimizing the baseline as you do, optimizing your own model?",
                    "label": 0
                },
                {
                    "sent": "There's also the tendency just to take baseline from papers rather than redoing them in your own conditions.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I better move on.",
                    "label": 0
                },
                {
                    "sent": "So this is my sort of summary of language modeling that was that was it for language modeling.",
                    "label": 0
                },
                {
                    "sent": "So recurrent networks give us this ability to capture long range dependency, which is good.",
                    "label": 0
                },
                {
                    "sent": "We can make them deeper and I presented lots of ways of addressing this sort of large vocabulary problem.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I better move on quickly too.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just talk a bit about machine translation at the end, so I'm going to talk a bit about machine translation is going to be very shallow coverage of machine translation, neural translation, and at the end I've tacked on a little bit of some work.",
                    "label": 0
                },
                {
                    "sent": "One of my students did, which is a bit of a wacky or different way of looking at neural machine translation just for some contrast.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the classic picture of the painting of the Tower of Babel, so we have lots of languages in the world and it's an obvious problem to want to be able to translate between them.",
                    "label": 0
                },
                {
                    "sent": "This is not a new thing.",
                    "label": 0
                },
                {
                    "sent": "This is something that people notice pretty much at the dawn of computers.",
                    "label": 0
                },
                {
                    "sent": "One of the first things they thought about when they come up with the computer was we could use this to translate between languages, because languages are really simple and regular incomputable.",
                    "label": 0
                },
                {
                    "sent": "An all you need to do is write down a set of rules and you can translate from French to English or something like that.",
                    "label": 0
                },
                {
                    "sent": "Famously, I think this was set as an intern project in the 50s to solve French English translation over the summer.",
                    "label": 0
                },
                {
                    "sent": "And it didn't quite workout.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So lots of these problems which we think is just really simple.",
                    "label": 0
                },
                {
                    "sent": "It's just a bunch of rules turned out to be a lot harder.",
                    "label": 0
                },
                {
                    "sent": "Now we are all interested in this sort of machine learning approach to machine translation, trying to learn these models and the reason we can really do this is because what we call parallel corpora that is the same meaning written in different languages occurs everywhere.",
                    "label": 0
                },
                {
                    "sent": "So the classic example of this is the Rosetta Stone.",
                    "label": 0
                },
                {
                    "sent": "So way back in ancient times, people like to write out messages in different languages.",
                    "label": 0
                },
                {
                    "sent": "Here we've got a Gyptian hieroglyphs, ancient Coptic, an ancient Greek.",
                    "label": 0
                },
                {
                    "sent": "I think those two are.",
                    "label": 0
                },
                {
                    "sent": "And discovering these scripts is what allowed scholars to decipher hieroglyphs and basically in machine translation we just want to.",
                    "label": 0
                },
                {
                    "sent": "We just want to automate this process if we have lots of data showing the same meaning in different languages, can we automatically discover?",
                    "label": 0
                },
                {
                    "sent": "The correlations between them and us to learn to translate.",
                    "label": 0
                },
                {
                    "sent": "So I think one of the first discovery was like in the hieroglyphs things circled a Royal names and they discovered this early on and discovering these cribs let's you start to.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Code the code, the sequence.",
                    "label": 0
                },
                {
                    "sent": "So this machine translation, really statistical machine translation, that the probabilistic approach really took off at Fred Jelinek's Group at IBM in the 80s.",
                    "label": 1
                },
                {
                    "sent": "And this was an amazing group.",
                    "label": 0
                },
                {
                    "sent": "They they did some of the early work on speech recognition, and they realize that the speech recognition models that were using over called probabilistic noisy channel models could just be applied to machine translation if they could get the data.",
                    "label": 0
                },
                {
                    "sent": "It's a very interesting group that included people like Bob Mercer and Peter Brown.",
                    "label": 0
                },
                {
                    "sent": "Bob Mercer's been in the news quite a lot recently for his political support of various causes, but those guys back then were computational linguists in Fred's group, and actually the story goes that Fred went away for the summer.",
                    "label": 0
                },
                {
                    "sent": "They told Fred that we can do machine translation with this model, and he said that stupid it'll never work, and then he went away for the summer and they did it anyway, and when he came back they said look it works and he took credit for it.",
                    "label": 0
                },
                {
                    "sent": "He didn't really very nice going anyway.",
                    "label": 0
                },
                {
                    "sent": "He's also known for this famous quote at the time that when he was developing this speech, recognition models every time we got rid of a linguist, which if you were doing speech recognition at the time, you tend to hire lots of linguists in translation thing every time he followed when things got better.",
                    "label": 1
                },
                {
                    "sent": "And that's still a sort of sentiment that's that's around today.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Although it should, it should not be so I also like to show this.",
                    "label": 0
                },
                {
                    "sent": "You probably can't read this, but so these guys pioneered this approach to statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "At that time, no one was really thinking this way, and this is really the dawn of sort of big machine learning models.",
                    "label": 0
                },
                {
                    "sent": "Learn, learn from data and really a transition from rationalism to empiricism.",
                    "label": 0
                },
                {
                    "sent": "In this sort of computational research anyway, they submitted it to, uh, this was calling, yeah, it's conference.",
                    "label": 0
                },
                {
                    "sent": "Back in around about 1990 I think maybe 89.",
                    "label": 0
                },
                {
                    "sent": "Anyway they got this great review which says the validity of statistical information theoretic approach to empty has indeed been recognized and was universally recognized as mistaken by 1950, so it's a waste of time as they quote some textbook.",
                    "label": 0
                },
                {
                    "sent": "And then the last sentence is great that the crude force of computers is not science.",
                    "label": 0
                },
                {
                    "sent": "The paper is simply beyond the scope of calling, so this was the review they got in the paper was rejected.",
                    "label": 0
                },
                {
                    "sent": "This turned out to be quite an influential paper in the end.",
                    "label": 0
                },
                {
                    "sent": "So anyway, they got their paper published.",
                    "label": 0
                },
                {
                    "sent": "Then all those guys.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joined Rentech a big.",
                    "label": 0
                },
                {
                    "sent": "Hedge fund and forgot all about machine translation, but then the rest of the community picked up on these ideas and run with it, and that's what led to Google Translate and all these sorts of services.",
                    "label": 0
                },
                {
                    "sent": "OK, so the core of what these guys proposed back then was called the noisy channel model to us.",
                    "label": 1
                },
                {
                    "sent": "We normally think about just as Bayes rule that if we're trying to predict some conditional probability, like the probability of a English sentence given a French sentence input that we want to translate, we can use Bayes rule to decompose it like this.",
                    "label": 0
                },
                {
                    "sent": "And the reason we might want to do this rather than just estimating the conditional directly is because we get this PAV term and is PF given any now.",
                    "label": 0
                },
                {
                    "sent": "PV is our language model.",
                    "label": 0
                },
                {
                    "sent": "That's what we've been talking about for the first half of this lecture.",
                    "label": 0
                },
                {
                    "sent": "So by decomposing this we can train a language model separately from our conditional translation model.",
                    "label": 0
                },
                {
                    "sent": "And this is very powerful, because one it's much easier to get monolingual data.",
                    "label": 0
                },
                {
                    "sent": "That is to get bilingual data.",
                    "label": 0
                },
                {
                    "sent": "We might get a best 100 million sentences of bilingual data that we can get trillions of words of monolingual English.",
                    "label": 0
                },
                {
                    "sent": "So decoupling this allows us to train very big language models.",
                    "label": 0
                },
                {
                    "sent": "And the reverse model separately.",
                    "label": 0
                },
                {
                    "sent": "There's some other advantages to reverse model dimension in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one way of thinking of this noisy channel model is that we sort of feed in our our French and we get some roughly comperable English but slightly garbled.",
                    "label": 1
                },
                {
                    "sent": "And then we feed that through our language model and it sort of ranks.",
                    "label": 0
                },
                {
                    "sent": "Which one of these should I choose as my output?",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll put up one of the classic IBM models that came out of IBM in the 90s eighties.",
                    "label": 0
                },
                {
                    "sent": "This was the simplest one.",
                    "label": 0
                },
                {
                    "sent": "They were considerably more complex ones, but there's a simple generative model of source sentence.",
                    "label": 1
                },
                {
                    "sent": "Given a target sentence, and there's some normalizes at the front.",
                    "label": 0
                },
                {
                    "sent": "But the key thing is that there's some over something called a, which is an alignment which today we call attention, and we're just taking the product of the words SJ.",
                    "label": 0
                },
                {
                    "sent": "Given the target word that they're aligned to, or they tend to.",
                    "label": 1
                },
                {
                    "sent": "So this is IBM Model 1, and it's a great little model if you want to just learn alignments between parallel corpora.",
                    "label": 0
                },
                {
                    "sent": "This things.",
                    "label": 0
                },
                {
                    "sent": "Very fast and scaleable and gives good alignments and it's also basically what now people call hard alignment in that we for the as we are generating the output which using which word in the input it was generated from.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that we can train it exactly and it's one of the few unsupervised models that we can train with them that has it is convex.",
                    "label": 0
                },
                {
                    "sent": "It has a unique maximum.",
                    "label": 0
                },
                {
                    "sent": "It's called a model.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, but it doesn't actually work for translation, but it's a nice one.",
                    "label": 0
                },
                {
                    "sent": "That's a really good exercise to implement that and run it and align corpora.",
                    "label": 0
                },
                {
                    "sent": "So more recently, people realize that.",
                    "label": 0
                },
                {
                    "sent": "Deep learning was quite a good way to approach translation problem and or from this point of view of what sort of become known as sequence to sequence models or encoder decoder architectures.",
                    "label": 1
                },
                {
                    "sent": "The idea being that we feed in some input.",
                    "label": 0
                },
                {
                    "sent": "Here a Chinese sentence.",
                    "label": 0
                },
                {
                    "sent": "We generalize it in some way, normally down to some sort of vector space representation, and then from that we want to be able to read out we want to generate our output here.",
                    "label": 0
                },
                {
                    "sent": "Our English translation at the top.",
                    "label": 1
                },
                {
                    "sent": "And we know that neural networks like recurrent networks are really good at embedding things in vector spaces.",
                    "label": 0
                },
                {
                    "sent": "So maybe doing this instead of what people were doing previously would be a good way to approach things.",
                    "label": 0
                },
                {
                    "sent": "So this is really.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Taking off an neural empty is now the state of the art.",
                    "label": 0
                },
                {
                    "sent": "One very simple idea is to take our recurrent neural network language model and just concatenate our strings and treat this like a language modeling problem where we read in the French we have some delimiter and then we read out the English.",
                    "label": 0
                },
                {
                    "sent": "And this was the classic sequence to sequence model proposed a few years back by.",
                    "label": 0
                },
                {
                    "sent": "Some guys at Google in the Google Brain team, so this is a really nice idea and some really simple way of looking at this problem.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work for translation like this has never come close to state of the art just doing this, but it's a really nice simple way of looking at things and gives us a building block for lots of problems.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of the reasons that doesn't work as we get this bottleneck here that no matter what sort of length of input we have to boil it down to this vector representation that we're going to then read out our output.",
                    "label": 0
                },
                {
                    "sent": "So if we've got 2 words in two words of French or 100 words of friends, we're going to end up embedding in the same space.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to choose a really big hidden layer representation to cope with 100 words, and when we're only doing 2 words were going to waste all of that computation.",
                    "label": 0
                },
                {
                    "sent": "The model has to discover these long range dependencies, and then so to get this to work, you then have to do tricks like reversing.",
                    "label": 0
                },
                {
                    "sent": "The strings and other sorts of things, but it still won't really get you.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "Now there we go, this reversing the strings.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does get you there is the idea of attention that came out of the group here at Montreal.",
                    "label": 0
                },
                {
                    "sent": "So this says, let's get round that bottleneck problem by instead of boiling everything down to a vector at each time step that we generate something, let's choose what we're what we're going to condition on or align to or attend to in the input.",
                    "label": 0
                },
                {
                    "sent": "And this really is just a neural extension of this idea of alignment.",
                    "label": 0
                },
                {
                    "sent": "We know that this is a very good bias in translation, especially French to English translation.",
                    "label": 0
                },
                {
                    "sent": "We know that roughly French English word is going to be one or two or.",
                    "label": 0
                },
                {
                    "sent": "Or a local group of words in the input, which is what we want to condition on to choose the right word.",
                    "label": 0
                },
                {
                    "sent": "So this is introducing a structural bias that's very appropriate for this problem.",
                    "label": 0
                },
                {
                    "sent": "Another way you can think of it.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rather than trying to boil everything down to a fixed size representation, now we take our representation to just be the concatenation of the input or that run through some recurrent networks, and that's what we generate from.",
                    "label": 0
                },
                {
                    "sent": "So the basic sort of attention model you run.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recurrent networks in two directions over your input.",
                    "label": 0
                },
                {
                    "sent": "So bidirectional recurrent networks.",
                    "label": 0
                },
                {
                    "sent": "You concatenate these representations and then.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start generating output and every time you generate some output you attend to this input matrix.",
                    "label": 0
                },
                {
                    "sent": "So you calculate a function.",
                    "label": 0
                },
                {
                    "sent": "Sort of similarity or some other function with each of these input time steps.",
                    "label": 0
                },
                {
                    "sent": "You then use that as a weight on that input and then you calculate your output softmax and so this is sort of shown here that we calculate a weight on each one of those attention indexes or alignments.",
                    "label": 0
                },
                {
                    "sent": "One of them gets a lot of weight, the one that's the appropriate start because we know in English will just start with the noun and then when we translate we produce the appropriate output and we just keep doing this at each time step.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now at each time step the model gets to choose water conditions on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I said this is a very appropriate.",
                    "label": 0
                },
                {
                    "sent": "Bias to having your translation model, but we know translation behaves like this.",
                    "label": 0
                },
                {
                    "sent": "Even in languages like translating from Chinese to English, where there really isn't a one to one correspondence, attention still gives you really the right bias.",
                    "label": 0
                },
                {
                    "sent": "The other thing this is doing is it's getting around that vanishing gradient problem in the recurrent network.",
                    "label": 0
                },
                {
                    "sent": "So if we have our straight sequence to sequence model, we have a very long path between a given output translation.",
                    "label": 0
                },
                {
                    "sent": "Say we're translating 30 word sentence to another 30 word sentence says lots of words in between.",
                    "label": 0
                },
                {
                    "sent": "The thing we're producing the output and the thing it's translating in the input.",
                    "label": 0
                },
                {
                    "sent": "What attention does it let you short circuit that path and go straight to the thing you're wanting to translate?",
                    "label": 0
                },
                {
                    "sent": "That gives you a much shorter.",
                    "label": 0
                },
                {
                    "sent": "Path feel gradient to go through.",
                    "label": 0
                },
                {
                    "sent": "So training is much easier.",
                    "label": 0
                },
                {
                    "sent": "Much easier for the model to discover.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you want to discover?",
                    "label": 0
                },
                {
                    "sent": "OK, in the last few minutes I'm going to quickly talk about a different sort of attention model which tries to implement the classic noisy channel model, but in a sort of modern, neural empty sort of way, and this is something we played with sort of the end of last year at deep mind.",
                    "label": 0
                },
                {
                    "sent": "Now there's lots and lots of different variants of neural empty that you might choose from.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to claim that this is the best, but I thought I'd talk about this part of this work we did, and also because it gives you a different way of looking at this.",
                    "label": 0
                },
                {
                    "sent": "Problem, so the classic conditional neural empty system is just directly estimating the output given the input.",
                    "label": 0
                },
                {
                    "sent": "This is nice, especially I've got loads of data, but it has a couple of problems.",
                    "label": 0
                },
                {
                    "sent": "One you can't if you have lots of monolingual data you can't easily use that.",
                    "label": 1
                },
                {
                    "sent": "There's been various proposals for how to do that, but none of them really scale in the same way that the old noisy channel model did back in the days of the noisy channel model, we knew that improving your translation model might give you sort of 1 blue point better translation.",
                    "label": 1
                },
                {
                    "sent": "Doubling the amount of language modeling data you had might give you 10, so industry at least the game, was always to get a bigger language model.",
                    "label": 0
                },
                {
                    "sent": "We can't easily do that with these conditional models.",
                    "label": 0
                },
                {
                    "sent": "You also get this problem that is related to the explaining away problem that it's easy for the model to forget bits of the input, 'cause it doesn't have to explain the input when it's generating the output.",
                    "label": 1
                },
                {
                    "sent": "If there's some word that's very low frequency input, it's quite likely that might just decide not to translate it, especially if it looks good from the sequence.",
                    "label": 0
                },
                {
                    "sent": "So things like adjectives and things like that might just get dropped.",
                    "label": 0
                },
                {
                    "sent": "'cause if you've got.",
                    "label": 0
                },
                {
                    "sent": "The input the the pink dog, and that's not very frequent.",
                    "label": 0
                },
                {
                    "sent": "You could just produce at the output the dog, and that's going to look good from the output distribution.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very easy to drop things.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we return to this noisy channel idea that gets around a lot of these problems.",
                    "label": 1
                },
                {
                    "sent": "Now the Model 1.",
                    "label": 0
                },
                {
                    "sent": "You've got this language model that you can train on any amount of monolingual data you have, and it doesn't even have to be a newer model.",
                    "label": 0
                },
                {
                    "sent": "It can be one of the classic count models trained on a trillion words that if you're someone like Google, you might have lying around gathering dust.",
                    "label": 0
                },
                {
                    "sent": "So, but now you have this reverse model that we call that the channel model that's producing the probability of the input given the output, and this means we have to explain what we observe.",
                    "label": 0
                },
                {
                    "sent": "We have to explain the input we observed and now the model can't not explain words in the input by producing the output without incuring quite a penalty.",
                    "label": 1
                },
                {
                    "sent": "So probabilistically this is a nice way to structure things.",
                    "label": 0
                },
                {
                    "sent": "Now it's very straightforward to train models.",
                    "label": 0
                },
                {
                    "sent": "For this we just correct monolingual data.",
                    "label": 0
                },
                {
                    "sent": "Trainer.",
                    "label": 0
                },
                {
                    "sent": "Recurrent Network is our language model.",
                    "label": 0
                },
                {
                    "sent": "We train our normal attention model or whatever it is.",
                    "label": 0
                },
                {
                    "sent": "But just with our input and output reversed to get a distribution over X given Y and we have a noisy channel model, so that's good.",
                    "label": 0
                },
                {
                    "sent": "Training is easy.",
                    "label": 0
                },
                {
                    "sent": "The problem is we can't decode with this model.",
                    "label": 0
                },
                {
                    "sent": "So with the direct model, that is why given X, the chain rule allows us to.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Factorize everything nicely so we can produce one word at a time.",
                    "label": 0
                },
                {
                    "sent": "Once we're conditioning on the output that we haven't yet produced, we can't do that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we need this factorization and that's what I'm going to present here.",
                    "label": 1
                },
                {
                    "sent": "How do we actually get a model that we can decode within the noisy channel model?",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the direct model we have this classic sort of chain rule where we can decode greedy decoding.",
                    "label": 1
                },
                {
                    "sent": "Works quite well, we can just at each time step produce the wire that's most probable given the the output we produced before and conditioning on the input.",
                    "label": 0
                },
                {
                    "sent": "And this is all nice and very easy.",
                    "label": 0
                },
                {
                    "sent": "We can make that a beam search if you want slightly better results.",
                    "label": 0
                },
                {
                    "sent": "But this is good.",
                    "label": 0
                },
                {
                    "sent": "This is the classic neural empty model and it's very easy to code.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we think about a noisy channel model, we might come up with something like this where we try and produce the output.",
                    "label": 0
                },
                {
                    "sent": "The conditional X, adding one at a time.",
                    "label": 0
                },
                {
                    "sent": "But of course you can't do that.",
                    "label": 0
                },
                {
                    "sent": "That's not, that's not how probability works.",
                    "label": 1
                },
                {
                    "sent": "That's not going to work out, so we have this problem now that our model wants to see all of the output before it can tell us anything about the probability of the input.",
                    "label": 0
                },
                {
                    "sent": "So your normal sort of attention model is not going to work in this case because you can't produce your output one word at a time.",
                    "label": 0
                },
                {
                    "sent": "You have to somehow come across the whole output.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do is break this model down.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is introduce an alignment variable that tells us how much of the input we need to read before we can produce some output.",
                    "label": 1
                },
                {
                    "sent": "So we're going to think of this like we're seeing input words one of the time, and at some point the model decides I've seen enough words, I can produce some output now and then produce some output.",
                    "label": 0
                },
                {
                    "sent": "Then we'll see some more input than producing more output.",
                    "label": 0
                },
                {
                    "sent": "So we're going to introduce alignment variables, Ed.",
                    "label": 0
                },
                {
                    "sent": "That's how much of the input that's our alignment.",
                    "label": 0
                },
                {
                    "sent": "How much we've currently seen.",
                    "label": 0
                },
                {
                    "sent": "So now we have an alignment, except our alignment is sequential.",
                    "label": 0
                },
                {
                    "sent": "We're always incrementing this, and so we end up with this factorized distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we were predicting zed, and we were predicting whether we should read another word or produce some output for X, and we're going to just use a recurrent network to produce both of these.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "And then we have described it.",
                    "label": 0
                },
                {
                    "sent": "Now it's just another direct model for translation and we published this last year as a direct model.",
                    "label": 1
                },
                {
                    "sent": "It's very similar to a model that Alex Graves published for speech recognition a few years back as well.",
                    "label": 0
                },
                {
                    "sent": "It's actually a great paper to look at this 2012 paper 'cause it's really sort of preceding all these sequences sequence type models.",
                    "label": 0
                },
                {
                    "sent": "And really Alex was thinking of the same sort of thing.",
                    "label": 0
                },
                {
                    "sent": "But this is an online model of translation, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "It can actually translate as it's reading input, so it can translate continuously.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that for when you do have a fixed links input, we can with this model marginalized out all of these possible alignments.",
                    "label": 0
                },
                {
                    "sent": "So in this diagram at the top.",
                    "label": 0
                },
                {
                    "sent": "So forgive the exact examples.",
                    "label": 0
                },
                {
                    "sent": "Actually, sentence compression rather than translation, but I'm compressing the vertical sentence on the output and we think of producing the output one word at a time by following a path through this grid.",
                    "label": 0
                },
                {
                    "sent": "That's the Red Arrows, and every time we take an arrow to the right, we're producing an output word, and every time we take a narrow vertically down, we're reading another input word, so.",
                    "label": 0
                },
                {
                    "sent": "By following that path, we're doing this process of reading more input, producing more output, and if we structure our recurrent networks just right, we can marginalized we can sum over all possible paths through this grid so we don't have to resort to reinforcement learning or anything like that.",
                    "label": 0
                },
                {
                    "sent": "So that's really nice.",
                    "label": 0
                },
                {
                    "sent": "It's a really nice little model.",
                    "label": 0
                },
                {
                    "sent": "The other thing to understand about this is because we are always producing the output condition on some prefix of the input.",
                    "label": 0
                },
                {
                    "sent": "So this means that we can reverse it in the noisy channel model, because now we know that whenever we see some of the output Y, we can get a distribution over X that we could have produced from that prefix.",
                    "label": 0
                },
                {
                    "sent": "So that's what we.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Undertake this model that we were using as a direct model and reverse it and so now this looks like instead of rolling out the columns were rolling out the rows as we as we decode.",
                    "label": 0
                },
                {
                    "sent": "So as we see each additional word Y we can get we can fill out this grid for how the probability of producing all of the input.",
                    "label": 0
                },
                {
                    "sent": "So we can do this search.",
                    "label": 0
                },
                {
                    "sent": "There's one sort of complexity that we haven't got around, which is when you want to predict another word.",
                    "label": 0
                },
                {
                    "sent": "You have to enumerate all of the whole vocabulary basically and redo the do a softmax for every word in the vocabulary, which is expensive, so you don't want to do that.",
                    "label": 1
                },
                {
                    "sent": "So instead what we do is we have an approximate direct model that gives us a short list and then we just compute the probabilities for yet.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was a very quick sort of introduction of that model.",
                    "label": 0
                },
                {
                    "sent": "As I said, I'm not sort of saying that this is the state of the art in translation.",
                    "label": 0
                },
                {
                    "sent": "We tested this on.",
                    "label": 0
                },
                {
                    "sent": "A classic sort of Chinese English data set called FPS and use data set, and it works really nicely.",
                    "label": 0
                },
                {
                    "sent": "So the second from bottom line there is basically the noisy channel model.",
                    "label": 1
                },
                {
                    "sent": "We have a channel language model.",
                    "label": 0
                },
                {
                    "sent": "We also need a bias parameter for technical length reasons, but it works really well as incorporating the language model when we just interpolate a language model with a direct model.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as if you just interpolate a direct model in the language model there, both trying to explain the output when you do the noisy channel model they're working together.",
                    "label": 0
                },
                {
                    "sent": "To explain what's going on so that works really well, we can always also throw in the direct model if we're into that sort of thing, and it even works slightly better, but the point of that is.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get you thinking about other ways of thinking about these sequence to sequence models so we can use these same ideas like the noisy channel model and such.",
                    "label": 0
                },
                {
                    "sent": "OK, hopefully I'm pretty much on time, thank you.",
                    "label": 0
                }
            ]
        }
    }
}