{
    "id": "q76zzdjonexhkfzqt34sng53f3xrjnpm",
    "title": "Apprenticeship Learning Using Linear Programming",
    "info": {
        "author": [
            "Umar Syed, Department of Computer and Information Science, University of Pennsylvania"
        ],
        "published": "Aug. 12, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Optimization Methods->Linear Programming"
        ]
    },
    "url": "http://videolectures.net/icml08_syed_alu/",
    "segmentation": [
        [
            "Hello.",
            "OK, OK thanks.",
            "So this is apprenticeship learning using linear programming, joint work with Michael Bowling and Roger Perry."
        ],
        [
            "So apprenticeship learning is like reinforcement learning, except there are two agents in the environment, an apprentice, an expert.",
            "And the goal is for The Apprentice to learn how to behave in this environment by observing an expert behaving so in our in this car driving example, we have an expert driving on this highway.",
            "He's not doing such a great job.",
            "And our goal is in apprenticeship learning is to devise learning algorithms that take these demonstrations and output a policy for The Apprentice that is at least as good as the experts policy, and if possible, even better.",
            "So in this case, the algorithm has managed to learn a policy that's better than the experts."
        ],
        [
            "So our main contribution is a new apprenticeship learning algorithm that has two main advantages over previous approaches.",
            "The first is that it outputs simpler policies for The Apprentice, and I'll explain what I mean later by simpler and also it is empirically quite a bit faster than previous algorithms.",
            "In our experiments it is sometimes up to two orders of magnitude faster."
        ],
        [
            "So let me first introduce the."
        ],
        [
            "The Ship learning framework, so it's a lot like a Markov decision process with a few key differences.",
            "The first is that we reward function is taken to be unknown.",
            "And instead of the ruler function were given, what I'm going to call K basis reward functions and also a set of demonstrations from the expert.",
            "And the assumption is that this unknown reward function is a weighted combination of the basis reward functions.",
            "So the motivation here is that in many situations it's hard to describe what exactly it is that the expert is optimizing, but it's not hard to describe what factors he's trading off among.",
            "So in our car driving example, we don't know exactly what it is that people optimize when they drive, but we would know the things that they care about.",
            "They care about driving fast, not hitting other cars, staying on the road, and we just don't know how they're trading off.",
            "Among these various factors.",
            "And so the unknown weight vector is the encodes that tradeoff.",
            "And the objective for The Apprentice is 2 is not to learn a policy that's optimal, 'cause in general that will be impossible since the reward function is not given is just to learn a policy whose value is at least as good as the value of the experts policy with respect to the unknown reward function."
        ],
        [
            "So let me just summarize the sum."
        ],
        [
            "The prior work, so the algorithms that I'll be discussing in the talk are all based on sort of the same idea.",
            "The same key idea.",
            "So let me define for each of the basis reward functions.",
            "Let me define a basis value function so the basis value function is the value of the policy with respect to the basis reward.",
            "So just like how there's a true value function, there's now K basis value functions and it's not hard to see that the true value function, whatever it is.",
            "It's going to be this weighted combination of the basis values and this is just a consequence of the linearity of expectation."
        ],
        [
            "And so appealing.",
            "Dang, who introduced the Apprenticeship learning framework?",
            "They took advantage of this observation and to come up with an idea for an algorithm.",
            "So the idea was first estimate the basis values of the experts policy from the data.",
            "That's easy to do because we know the basis rewards, so we can so we can estimate the basic values.",
            "And Secondly, they described an algorithm that can find a policy that matches these basis values.",
            "For the expert, so The Apprentice will have the same basis values as the expert, and if we can do that then we know that the two policies will have the same true value because the true value is just a weighted combination of basis values and the type or the sort of the style of this algorithm was kind of a geometric algorithm at a very geometric kind of flavor."
        ],
        [
            "More recently we we took a different approach.",
            "We made a stronger assumption about the weights.",
            "We assume that they are non negative and they sum to one.",
            "And so what's the idea behind this assumption?",
            "The idea is that we're basically saying that we know a little bit more about the basis rewards.",
            "We don't just know what the true rewards depend on.",
            "We also know sort of the direction of that relationship.",
            "So for example, we know that other things being equal, speed is good.",
            "Other things being equal, collisions are bad, we just don't know how bad and how good.",
            "And so if we make the stronger assumption, then we can get a stronger guarantees.",
            "So the idea again is to estimate the basis values of the experts policy and then we describe an algorithm that finds an apprentice policy that's better than the expert with respect to all of these basis values by as much as possible by a margin that is made as large as possible.",
            "So if we do that then you can see that it's not hard to see that the the value of The Apprentice policy is not only going to be at least as good as the expert, but in some cases it's going to be quite a lot better.",
            "Those cases being with that margin is very large.",
            "And the again the type of the style of the algorithm was a boosting kind of algorithm."
        ],
        [
            "So, so in this paper we.",
            "Our approach is based on the same kind of algorithm idea that we had before, but the style of the algorithm is very different.",
            "It's going to be we're going to formulate.",
            "The problem is just a single linear program, and we're going to hand off this linear program to solver, and that's all we're going to do.",
            "And taking this different approach has a couple of advice."
        ],
        [
            "Edges.",
            "So here they are.",
            "So the previous algorithms that I described, they didn't actually output a stationary single stationary policy.",
            "What they actually output was a distribution over stationary policies like D, so the algorithm would output D and The Apprentice had to sort of at times zero before interacting with the environment.",
            "Choose the stationary policy from this distribution and then execute that policy, and if it did that, then it enjoyed the guarantee that the expected value of this policy was better than the experts in that expectation was taken over that distribution.",
            "D. By contrast, the approach that ought to describe.",
            "It's going to output just a single stationary policy whose value is as good as the experts, and maybe better, and so the advantage is just at the Now The Apprentice policy is a lot simpler and more intuitive rather than being this randomized mixture of stationary policies is just one stationary policy."
        ],
        [
            "Another advantage is that the previous algorithms they so they had a similar structure, they ran for several rounds, several iterations, and in each iteration the algorithm to solve a traditional MDP.",
            "Our approach, the algorithm is just a single linear program and you solve the LP and now we can't say anything formally that this is more efficient.",
            "In fact, in the worst case it's not more efficient, but at least empirically it seems to be a faster approach.",
            "So we sort of can informally conjecture that this is because it kind of solves the problem.",
            "Sort of, quote unquote, all at once, rather than rather than over many rounds.",
            "But we can't say anything more precise than that."
        ],
        [
            "So our approach is based on a concept from DPS called the occupancy measure of a policy, and so let me introduce what exactly the occupancy measure is and also some relevant facts about it.",
            "None of these facts are new, but perhaps they're not at the tip of everyone's tongue.",
            "So let me just go over everything."
        ],
        [
            "OK, So what is the occupancy measure of policy?",
            "Well, it's a sort of a different way of describing how the policy moves through the environment, so the occupancy measure is a vector.",
            "It has length, number of states, times, number of actions, and each component in the vector corresponds to a state action pair.",
            "And the component corresponding to state action pair essay tells you the average number of times this policy visits essay during its interaction with the environment.",
            "With one sort of twist, which is that visits to the pair are suitably discounted in the usual way.",
            "So I think I should just talk through an example to make sure we're on the same page.",
            "So consider a policy Pi and the state action pair S. And suppose that time one the policy is in state action pair essay with certainty and then at the next time step it's in that state action pair with probability 1/2 and then at the third time step is there with probably 1/3 and then after that we know for sure that it never visits that state action pair.",
            "So what I've told you so far is the average number of times this policy visits the state action pair in each of the time steps, and so the occupancy measure is just defined to be the sum of those averages.",
            "The total number of visits, but with the visit suitably discounted.",
            "So in our case the expression is going to look like this thing down here.",
            "So in general this expression will be an infinite sum, but because of the way I define the example, it's a.",
            "It's a finite sum.",
            "Is everyone OK with that means?"
        ],
        [
            "So occupancy measure has a few convenient properties for our purposes.",
            "One is that, given the occupancy measure of a stationary policy, it's easy to recover the stationary policy, and that's given just by this expression so.",
            "On the left hand side I have the probability that a policy takes action A and status and that turns out to be equal to the number of times this policy visits the state action appear as a normalized by the number of times that visits the state S, and so it's intuitively clear that those two things should be equal, and it turns out to be the case that they are.",
            "And for our purposes, the significance is that given as I said, given the occupancy measure of a policy, we can easily convert it into a form where we can act on it right into the usual state action probabilities."
        ],
        [
            "The other convenient fact about this occupancy measures that given it, you can easily compute the value of a policy.",
            "So consider this expression new of X.",
            "That letter is new.",
            "So this is just the sum, the weighted sum of all the occupancy measures for this policy, weighted by the reward of each state action pair.",
            "And it turns out that if X is the occupancy measure of a policy, then new of X is the value of that policy.",
            "And again this should be clear.",
            "Policy earns reward every time visits a state action pair suitably discounted and the occupancy measure tells you how often the policy visits each day to action pair.",
            "And again, for our purposes that this is significant, because this is just a linear expression in X.",
            "So given this, given this occupancy measure X, it's really easy to compute the value of the policy.",
            "We don't have to value duration or anything like that."
        ],
        [
            "And the last thing I'm going to say about this occupancy measure is that.",
            "Every valid occupancy measure must satisfy what are called the bellman flow constraints.",
            "So this is a set of equations that enforces some conditions on any any any vector that is a real valid occupancy measure, and it says something is just a very natural set of constraints.",
            "It simply says that the number of times the policy enters the state has to be equal to the number of times it exits the state.",
            "Naturally.",
            "So this is certainly sort of obviously the necessary condition for any occupancy measure."
        ],
        [
            "But it turns out also to be sufficient.",
            "So these bellman flow constraints sort of completely characterize the set of occupancy measures if X satisfies the bellman flow constraints, indexes the occupancy measure of some policy.",
            "And for again for our purposes.",
            "What's really nice is that these bellman flow constraints are linear in X."
        ],
        [
            "OK, so having reviewed everything, we're not."
        ],
        [
            "To derive our linear program for apprenticeship learning algorithm and it's going to turn out to be really simple Now that we've gone through all the background, so we're going to start with this idea from our earlier paper, right?",
            "So we want to find an apprentice policy that's better than the experts for all the basis values by a larger margin as possible.",
            "So the first step is to replace."
        ],
        [
            "Everywhere we see policy, let's replace it with occupancy measure and we're going to add this constraint to make sure that these are all these are all real occupancy measures and not just some random vectors.",
            "The second step is every time, everywhere we see the value of a function."
        ],
        [
            "The value of a policy or replace it with the occupancy measure of that that of the excuse me, the value of the occupancy measure.",
            "And remember, we argued earlier that these two are equivalent.",
            "And then we're done.",
            "We're just going to get."
        ],
        [
            "Of all this stuff."
        ],
        [
            "And what we're left with is a linear program simply, but because all the stuff that I had it was all linear in X."
        ],
        [
            "And So what we're doing is.",
            "Here's what the program is doing.",
            "It's saying for all valid Occupy."
        ],
        [
            "The measures find one corresponding to a policy that's better than the experts."
        ],
        [
            "By as much as possible."
        ],
        [
            "And the last step is if we want this policy in a form that we can use it.",
            "We're going to convert it back to a stationary policy using the expression that I that I told you about earlier."
        ],
        [
            "And so now the theorem that we have about The Apprentice policy that we learn by solving this LP is the same as the earlier one, and it's going to be at least as good as the experts and possibly better.",
            "And the proof is basically immediate.",
            "Earlier we had to make sort of we had a boosting style algorithm and we had to make a boosting style of argument here.",
            "The proof is really, really straightforward.",
            "And I just want to make one remark, which is that this occupancy measure trick that we just did.",
            "We could have applied it to the algorithm idea from the from the earlier paper and we would have derived linear program version of their algorithm as well."
        ],
        [
            "OK, so ready to show some experiments and some demos."
        ],
        [
            "So there are experiments were in gridworld and we took a sort of a standard grid world and we divide it into a bunch of regions and we defined one basis reward function for each region.",
            "Then we sort of randomly chose a weight vector.",
            "And then we said the expert is is the optimal policy for this randomly chosen reward function.",
            "So each basis reward is sort of an indicator function for each region region.",
            "And if you define everything this way, essentially what you're doing is you're saying some regions in the grid world are better than others.",
            "The weight vector tells you which those regions are, but we don't know the weight vector, so we're going to observe the expert to figure out which regions are better than others.",
            "That's basically what's going on."
        ],
        [
            "OK, and so we're going to try 3 algorithms in this environment.",
            "The original one.",
            "This is the algorithm that had a geometric flavor than our earlier ones.",
            "The boosting style algorithm and then the LP.",
            "And the metric is how long does it take for each of these algorithms to learn a policy that's 95% of optimal?",
            "Which is the value of the experts policy."
        ],
        [
            "So we did it along, sort of two parameters.",
            "We first we kept a number of regions fixed and we varied the size of the grid world and the next slide we I'll show you.",
            "We kept the size of the goodwill fixed and vary the number of regions.",
            "So here you know, here's the running time versus the size of the grid, and notice that the Y axis is a log scale, so it is actually quite a bit faster than the earlier algorithms.",
            "And so."
        ],
        [
            "OK, and so in the next set of experiments we kept the size of the grid rule fixed the change.",
            "The number of regions and what's interesting is that the LP approach actually gets a little bit faster as the number of regions goes up.",
            "And so you know, for us, the LP solver is a bit of a black box, but we think it's because.",
            "As the number of regions increases, the number of constraints, the number of constraints is increasing, and in some cases when you increase the number of constraints, you actually make the problem easier to solve.",
            "You're sort of over constraining it.",
            "So that seems to be what's happening here.",
            "And again, the Y axis here is log scale."
        ],
        [
            "Finally, just some demos.",
            "Applying this to the car driving example, so here's an expert.",
            "This expert is OK.",
            "He's trying to avoid cars, and he's doing an OK job.",
            "And the output of the algorithm is.",
            "A car that also does OK, you know he drives pretty fast and he tries to avoid cars, so this is an example of where we were not able to improve on the behavior of the expert."
        ],
        [
            "Here's an example.",
            "We did actually showed this earlier, so here's the expert driving along not doing too well.",
            "And here's the behavior that we learned here.",
            "So in this example, that margin that we were maximizing that margin turned out to be really big because the experts policy is so poor."
        ],
        [
            "OK, and just a few other topics."
        ],
        [
            "We also, in the course of our experiments, we observed that the earlier algorithm for apprenticeship learning actually seems to do a little bit better than the theory predicts that it should, and so there's sort of a gap gap in our understanding of this algorithm, and we currently have some new results that we're preparing that explains this gap in the current."
        ],
        [
            "Work I guess.",
            "And also I wanted to point out that this sort of this approach is really closely connected to a lot of stuff that's been done in the literature.",
            "Alot of it which actually appears in the our community, not all of it, but a lot of it sort of.",
            "You know, working with these occupancy measures and things like constrained MDP, SRL, multiple rewards, and also some work very closely related to apprenticeship learning called Max margin planning.",
            "They also used a similar trick."
        ],
        [
            "And so, just to recap a new algorithm that does.",
            "Produces similar policies and is empirically faster, thanks.",
            "Questions.",
            "Yeah, um.",
            "I don't know.",
            "I mean, I can say that we've thought about thinking about that.",
            "But I couldn't say anything more, I'm sorry.",
            "Is there any apprenticeship learning stuff that has been extended to the linear function approximation?",
            "Not to my knowledge.",
            "Policies.",
            "Horses or six times.",
            "I remember second edition.",
            "Original.",
            "A different test is different styles, yeah?",
            "Yes.",
            "Creating a style and was trying to get as many options.",
            "Yeah so.",
            "I'm how accepting until experts.",
            "No, you couldn't learn it because you would know that it's dangerous.",
            "So the key difference in our assumptions that we assume that the weights the unknown weights are all positive.",
            "And by assuming there positive, you're saying you know how each of these basis rewards correlate with the true reward.",
            "So you actually know, sort of you don't know the magnitude of the influence, just know the direction of the influence.",
            "That's where the improvements come from.",
            "OK, it's it's Thursday, but there's something I didn't quite get this occupancy measure.",
            "It depends on the discount factor.",
            "Yeah, so the relation that you showed?",
            "Find function.",
            "I can see where it's going, but yeah, the one that keeps the policy.",
            "Can you come out on?",
            "Oh yeah so.",
            "It's not obvious.",
            "I agree it's not obvious, but if you just sort of write it out and work it out, these discount factors basically doesn't matter.",
            "It just works out the way you expect.",
            "It's in the appendix of the paper.",
            "I OK. Is it possible that you could actually perform very well without any expert examples?",
            "Yeah, you could you could, you could just take your prior knowledge about how the basis rewards correlate with that reward and try to do well.",
            "Just extracting that knowledge in the earlier paper, we had some examples of us doing that.",
            "If you haven't wondered at the back, yeah, this all seems that you have a model dynamics that's right.",
            "So there's a paper yesterday that was presented in this room as well, where they also assumed that the reward function was unknown and then.",
            "You're formulating in such a way that they were looking for like pretty much like a convex Hull, or like.",
            "Yeah, yeah.",
            "Wondering the relationship square because it seems that in some cases I guess some of those policies that are purple optimal would end up being the same as the one that you would find that simply maximizing the margin where the market seems to be like a specific direction for maximizing.",
            "Yeah, that's right.",
            "I mean, I think like I mean I could be wrong, but I think the policy we learn is going to be in the set of policies that they learn.",
            "They learn like a set of policies, right?",
            "Yeah, yeah.",
            "So I think it's going to be in that set.",
            "Certainly.",
            "Fashion is so you have this constraints.",
            "They're trying to maximize the management.",
            "If you disappear, the basis functions, for example, and those constraints would change accordingly.",
            "Yeah, so you can imagine playing with weights or.",
            "Do you have any ideas about how to use this or this is a good program that they are going where it seems to me that spot panel said that you are taking a certain direction which is determined by the actual scale of the basis functions.",
            "That yeah, yeah, that's right.",
            "So that's right.",
            "Yeah, it's.",
            "So yeah, if you re scale things, I mean the scale is also.",
            "Perhaps inadvertently, is part of the prior knowledge that you're injecting into the problem.",
            "Yeah, this is the interpretation.",
            "Yeah, right?",
            "I have yet another question.",
            "So you assume that the later are non negative, right?",
            "So what happens if you don't have this knowledge?",
            "Oh well, so one thing you could do is.",
            "I mean, sort of what you can do is just double the number of basis reward functions and for each basis reward replicated with it's negative.",
            "And if you do that, then you're sort of saying, OK, I don't know anything about how the basis awards correlate with the true word.",
            "And still get guarantees, but you'll never do better.",
            "That's the thing, but you always do at least as well.",
            "Which apprenticeship, learning an inverse reinforcement learning yeah.",
            "So I think you know at least the way it was originally posed.",
            "The goal of inverse reinforcement learning was to recover the original reward, right?",
            "So here the goal is not to actually recover the true reward function.",
            "The algorithms they in each round.",
            "They have sort of a candidate reward function, but it has no correlation to the real reward, and it may may not be anything like it.",
            "So the objective is different.",
            "The objective in this case is not to recover the true word.",
            "Alright, no more questions and less time with speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello.",
                    "label": 0
                },
                {
                    "sent": "OK, OK thanks.",
                    "label": 0
                },
                {
                    "sent": "So this is apprenticeship learning using linear programming, joint work with Michael Bowling and Roger Perry.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So apprenticeship learning is like reinforcement learning, except there are two agents in the environment, an apprentice, an expert.",
                    "label": 0
                },
                {
                    "sent": "And the goal is for The Apprentice to learn how to behave in this environment by observing an expert behaving so in our in this car driving example, we have an expert driving on this highway.",
                    "label": 0
                },
                {
                    "sent": "He's not doing such a great job.",
                    "label": 0
                },
                {
                    "sent": "And our goal is in apprenticeship learning is to devise learning algorithms that take these demonstrations and output a policy for The Apprentice that is at least as good as the experts policy, and if possible, even better.",
                    "label": 1
                },
                {
                    "sent": "So in this case, the algorithm has managed to learn a policy that's better than the experts.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our main contribution is a new apprenticeship learning algorithm that has two main advantages over previous approaches.",
                    "label": 1
                },
                {
                    "sent": "The first is that it outputs simpler policies for The Apprentice, and I'll explain what I mean later by simpler and also it is empirically quite a bit faster than previous algorithms.",
                    "label": 0
                },
                {
                    "sent": "In our experiments it is sometimes up to two orders of magnitude faster.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me first introduce the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Ship learning framework, so it's a lot like a Markov decision process with a few key differences.",
                    "label": 0
                },
                {
                    "sent": "The first is that we reward function is taken to be unknown.",
                    "label": 0
                },
                {
                    "sent": "And instead of the ruler function were given, what I'm going to call K basis reward functions and also a set of demonstrations from the expert.",
                    "label": 1
                },
                {
                    "sent": "And the assumption is that this unknown reward function is a weighted combination of the basis reward functions.",
                    "label": 1
                },
                {
                    "sent": "So the motivation here is that in many situations it's hard to describe what exactly it is that the expert is optimizing, but it's not hard to describe what factors he's trading off among.",
                    "label": 0
                },
                {
                    "sent": "So in our car driving example, we don't know exactly what it is that people optimize when they drive, but we would know the things that they care about.",
                    "label": 0
                },
                {
                    "sent": "They care about driving fast, not hitting other cars, staying on the road, and we just don't know how they're trading off.",
                    "label": 0
                },
                {
                    "sent": "Among these various factors.",
                    "label": 0
                },
                {
                    "sent": "And so the unknown weight vector is the encodes that tradeoff.",
                    "label": 0
                },
                {
                    "sent": "And the objective for The Apprentice is 2 is not to learn a policy that's optimal, 'cause in general that will be impossible since the reward function is not given is just to learn a policy whose value is at least as good as the value of the experts policy with respect to the unknown reward function.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me just summarize the sum.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The prior work, so the algorithms that I'll be discussing in the talk are all based on sort of the same idea.",
                    "label": 0
                },
                {
                    "sent": "The same key idea.",
                    "label": 0
                },
                {
                    "sent": "So let me define for each of the basis reward functions.",
                    "label": 0
                },
                {
                    "sent": "Let me define a basis value function so the basis value function is the value of the policy with respect to the basis reward.",
                    "label": 1
                },
                {
                    "sent": "So just like how there's a true value function, there's now K basis value functions and it's not hard to see that the true value function, whatever it is.",
                    "label": 0
                },
                {
                    "sent": "It's going to be this weighted combination of the basis values and this is just a consequence of the linearity of expectation.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so appealing.",
                    "label": 0
                },
                {
                    "sent": "Dang, who introduced the Apprenticeship learning framework?",
                    "label": 1
                },
                {
                    "sent": "They took advantage of this observation and to come up with an idea for an algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the idea was first estimate the basis values of the experts policy from the data.",
                    "label": 0
                },
                {
                    "sent": "That's easy to do because we know the basis rewards, so we can so we can estimate the basic values.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, they described an algorithm that can find a policy that matches these basis values.",
                    "label": 0
                },
                {
                    "sent": "For the expert, so The Apprentice will have the same basis values as the expert, and if we can do that then we know that the two policies will have the same true value because the true value is just a weighted combination of basis values and the type or the sort of the style of this algorithm was kind of a geometric algorithm at a very geometric kind of flavor.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More recently we we took a different approach.",
                    "label": 0
                },
                {
                    "sent": "We made a stronger assumption about the weights.",
                    "label": 0
                },
                {
                    "sent": "We assume that they are non negative and they sum to one.",
                    "label": 1
                },
                {
                    "sent": "And so what's the idea behind this assumption?",
                    "label": 0
                },
                {
                    "sent": "The idea is that we're basically saying that we know a little bit more about the basis rewards.",
                    "label": 0
                },
                {
                    "sent": "We don't just know what the true rewards depend on.",
                    "label": 0
                },
                {
                    "sent": "We also know sort of the direction of that relationship.",
                    "label": 0
                },
                {
                    "sent": "So for example, we know that other things being equal, speed is good.",
                    "label": 0
                },
                {
                    "sent": "Other things being equal, collisions are bad, we just don't know how bad and how good.",
                    "label": 0
                },
                {
                    "sent": "And so if we make the stronger assumption, then we can get a stronger guarantees.",
                    "label": 0
                },
                {
                    "sent": "So the idea again is to estimate the basis values of the experts policy and then we describe an algorithm that finds an apprentice policy that's better than the expert with respect to all of these basis values by as much as possible by a margin that is made as large as possible.",
                    "label": 1
                },
                {
                    "sent": "So if we do that then you can see that it's not hard to see that the the value of The Apprentice policy is not only going to be at least as good as the expert, but in some cases it's going to be quite a lot better.",
                    "label": 0
                },
                {
                    "sent": "Those cases being with that margin is very large.",
                    "label": 0
                },
                {
                    "sent": "And the again the type of the style of the algorithm was a boosting kind of algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so in this paper we.",
                    "label": 0
                },
                {
                    "sent": "Our approach is based on the same kind of algorithm idea that we had before, but the style of the algorithm is very different.",
                    "label": 1
                },
                {
                    "sent": "It's going to be we're going to formulate.",
                    "label": 0
                },
                {
                    "sent": "The problem is just a single linear program, and we're going to hand off this linear program to solver, and that's all we're going to do.",
                    "label": 1
                },
                {
                    "sent": "And taking this different approach has a couple of advice.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Edges.",
                    "label": 0
                },
                {
                    "sent": "So here they are.",
                    "label": 0
                },
                {
                    "sent": "So the previous algorithms that I described, they didn't actually output a stationary single stationary policy.",
                    "label": 1
                },
                {
                    "sent": "What they actually output was a distribution over stationary policies like D, so the algorithm would output D and The Apprentice had to sort of at times zero before interacting with the environment.",
                    "label": 0
                },
                {
                    "sent": "Choose the stationary policy from this distribution and then execute that policy, and if it did that, then it enjoyed the guarantee that the expected value of this policy was better than the experts in that expectation was taken over that distribution.",
                    "label": 0
                },
                {
                    "sent": "D. By contrast, the approach that ought to describe.",
                    "label": 0
                },
                {
                    "sent": "It's going to output just a single stationary policy whose value is as good as the experts, and maybe better, and so the advantage is just at the Now The Apprentice policy is a lot simpler and more intuitive rather than being this randomized mixture of stationary policies is just one stationary policy.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another advantage is that the previous algorithms they so they had a similar structure, they ran for several rounds, several iterations, and in each iteration the algorithm to solve a traditional MDP.",
                    "label": 0
                },
                {
                    "sent": "Our approach, the algorithm is just a single linear program and you solve the LP and now we can't say anything formally that this is more efficient.",
                    "label": 1
                },
                {
                    "sent": "In fact, in the worst case it's not more efficient, but at least empirically it seems to be a faster approach.",
                    "label": 0
                },
                {
                    "sent": "So we sort of can informally conjecture that this is because it kind of solves the problem.",
                    "label": 1
                },
                {
                    "sent": "Sort of, quote unquote, all at once, rather than rather than over many rounds.",
                    "label": 0
                },
                {
                    "sent": "But we can't say anything more precise than that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our approach is based on a concept from DPS called the occupancy measure of a policy, and so let me introduce what exactly the occupancy measure is and also some relevant facts about it.",
                    "label": 1
                },
                {
                    "sent": "None of these facts are new, but perhaps they're not at the tip of everyone's tongue.",
                    "label": 0
                },
                {
                    "sent": "So let me just go over everything.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is the occupancy measure of policy?",
                    "label": 1
                },
                {
                    "sent": "Well, it's a sort of a different way of describing how the policy moves through the environment, so the occupancy measure is a vector.",
                    "label": 1
                },
                {
                    "sent": "It has length, number of states, times, number of actions, and each component in the vector corresponds to a state action pair.",
                    "label": 0
                },
                {
                    "sent": "And the component corresponding to state action pair essay tells you the average number of times this policy visits essay during its interaction with the environment.",
                    "label": 0
                },
                {
                    "sent": "With one sort of twist, which is that visits to the pair are suitably discounted in the usual way.",
                    "label": 0
                },
                {
                    "sent": "So I think I should just talk through an example to make sure we're on the same page.",
                    "label": 0
                },
                {
                    "sent": "So consider a policy Pi and the state action pair S. And suppose that time one the policy is in state action pair essay with certainty and then at the next time step it's in that state action pair with probability 1/2 and then at the third time step is there with probably 1/3 and then after that we know for sure that it never visits that state action pair.",
                    "label": 1
                },
                {
                    "sent": "So what I've told you so far is the average number of times this policy visits the state action pair in each of the time steps, and so the occupancy measure is just defined to be the sum of those averages.",
                    "label": 0
                },
                {
                    "sent": "The total number of visits, but with the visit suitably discounted.",
                    "label": 0
                },
                {
                    "sent": "So in our case the expression is going to look like this thing down here.",
                    "label": 0
                },
                {
                    "sent": "So in general this expression will be an infinite sum, but because of the way I define the example, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a finite sum.",
                    "label": 0
                },
                {
                    "sent": "Is everyone OK with that means?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So occupancy measure has a few convenient properties for our purposes.",
                    "label": 1
                },
                {
                    "sent": "One is that, given the occupancy measure of a stationary policy, it's easy to recover the stationary policy, and that's given just by this expression so.",
                    "label": 1
                },
                {
                    "sent": "On the left hand side I have the probability that a policy takes action A and status and that turns out to be equal to the number of times this policy visits the state action appear as a normalized by the number of times that visits the state S, and so it's intuitively clear that those two things should be equal, and it turns out to be the case that they are.",
                    "label": 0
                },
                {
                    "sent": "And for our purposes, the significance is that given as I said, given the occupancy measure of a policy, we can easily convert it into a form where we can act on it right into the usual state action probabilities.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other convenient fact about this occupancy measures that given it, you can easily compute the value of a policy.",
                    "label": 1
                },
                {
                    "sent": "So consider this expression new of X.",
                    "label": 0
                },
                {
                    "sent": "That letter is new.",
                    "label": 0
                },
                {
                    "sent": "So this is just the sum, the weighted sum of all the occupancy measures for this policy, weighted by the reward of each state action pair.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that if X is the occupancy measure of a policy, then new of X is the value of that policy.",
                    "label": 1
                },
                {
                    "sent": "And again this should be clear.",
                    "label": 1
                },
                {
                    "sent": "Policy earns reward every time visits a state action pair suitably discounted and the occupancy measure tells you how often the policy visits each day to action pair.",
                    "label": 0
                },
                {
                    "sent": "And again, for our purposes that this is significant, because this is just a linear expression in X.",
                    "label": 0
                },
                {
                    "sent": "So given this, given this occupancy measure X, it's really easy to compute the value of the policy.",
                    "label": 0
                },
                {
                    "sent": "We don't have to value duration or anything like that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the last thing I'm going to say about this occupancy measure is that.",
                    "label": 0
                },
                {
                    "sent": "Every valid occupancy measure must satisfy what are called the bellman flow constraints.",
                    "label": 1
                },
                {
                    "sent": "So this is a set of equations that enforces some conditions on any any any vector that is a real valid occupancy measure, and it says something is just a very natural set of constraints.",
                    "label": 1
                },
                {
                    "sent": "It simply says that the number of times the policy enters the state has to be equal to the number of times it exits the state.",
                    "label": 0
                },
                {
                    "sent": "Naturally.",
                    "label": 0
                },
                {
                    "sent": "So this is certainly sort of obviously the necessary condition for any occupancy measure.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But it turns out also to be sufficient.",
                    "label": 0
                },
                {
                    "sent": "So these bellman flow constraints sort of completely characterize the set of occupancy measures if X satisfies the bellman flow constraints, indexes the occupancy measure of some policy.",
                    "label": 1
                },
                {
                    "sent": "And for again for our purposes.",
                    "label": 1
                },
                {
                    "sent": "What's really nice is that these bellman flow constraints are linear in X.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so having reviewed everything, we're not.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To derive our linear program for apprenticeship learning algorithm and it's going to turn out to be really simple Now that we've gone through all the background, so we're going to start with this idea from our earlier paper, right?",
                    "label": 1
                },
                {
                    "sent": "So we want to find an apprentice policy that's better than the experts for all the basis values by a larger margin as possible.",
                    "label": 0
                },
                {
                    "sent": "So the first step is to replace.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everywhere we see policy, let's replace it with occupancy measure and we're going to add this constraint to make sure that these are all these are all real occupancy measures and not just some random vectors.",
                    "label": 0
                },
                {
                    "sent": "The second step is every time, everywhere we see the value of a function.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The value of a policy or replace it with the occupancy measure of that that of the excuse me, the value of the occupancy measure.",
                    "label": 0
                },
                {
                    "sent": "And remember, we argued earlier that these two are equivalent.",
                    "label": 0
                },
                {
                    "sent": "And then we're done.",
                    "label": 0
                },
                {
                    "sent": "We're just going to get.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of all this stuff.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we're left with is a linear program simply, but because all the stuff that I had it was all linear in X.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what we're doing is.",
                    "label": 0
                },
                {
                    "sent": "Here's what the program is doing.",
                    "label": 0
                },
                {
                    "sent": "It's saying for all valid Occupy.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The measures find one corresponding to a policy that's better than the experts.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By as much as possible.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last step is if we want this policy in a form that we can use it.",
                    "label": 0
                },
                {
                    "sent": "We're going to convert it back to a stationary policy using the expression that I that I told you about earlier.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now the theorem that we have about The Apprentice policy that we learn by solving this LP is the same as the earlier one, and it's going to be at least as good as the experts and possibly better.",
                    "label": 0
                },
                {
                    "sent": "And the proof is basically immediate.",
                    "label": 0
                },
                {
                    "sent": "Earlier we had to make sort of we had a boosting style algorithm and we had to make a boosting style of argument here.",
                    "label": 0
                },
                {
                    "sent": "The proof is really, really straightforward.",
                    "label": 0
                },
                {
                    "sent": "And I just want to make one remark, which is that this occupancy measure trick that we just did.",
                    "label": 0
                },
                {
                    "sent": "We could have applied it to the algorithm idea from the from the earlier paper and we would have derived linear program version of their algorithm as well.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so ready to show some experiments and some demos.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are experiments were in gridworld and we took a sort of a standard grid world and we divide it into a bunch of regions and we defined one basis reward function for each region.",
                    "label": 0
                },
                {
                    "sent": "Then we sort of randomly chose a weight vector.",
                    "label": 1
                },
                {
                    "sent": "And then we said the expert is is the optimal policy for this randomly chosen reward function.",
                    "label": 1
                },
                {
                    "sent": "So each basis reward is sort of an indicator function for each region region.",
                    "label": 0
                },
                {
                    "sent": "And if you define everything this way, essentially what you're doing is you're saying some regions in the grid world are better than others.",
                    "label": 0
                },
                {
                    "sent": "The weight vector tells you which those regions are, but we don't know the weight vector, so we're going to observe the expert to figure out which regions are better than others.",
                    "label": 0
                },
                {
                    "sent": "That's basically what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so we're going to try 3 algorithms in this environment.",
                    "label": 0
                },
                {
                    "sent": "The original one.",
                    "label": 0
                },
                {
                    "sent": "This is the algorithm that had a geometric flavor than our earlier ones.",
                    "label": 0
                },
                {
                    "sent": "The boosting style algorithm and then the LP.",
                    "label": 0
                },
                {
                    "sent": "And the metric is how long does it take for each of these algorithms to learn a policy that's 95% of optimal?",
                    "label": 1
                },
                {
                    "sent": "Which is the value of the experts policy.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did it along, sort of two parameters.",
                    "label": 0
                },
                {
                    "sent": "We first we kept a number of regions fixed and we varied the size of the grid world and the next slide we I'll show you.",
                    "label": 0
                },
                {
                    "sent": "We kept the size of the goodwill fixed and vary the number of regions.",
                    "label": 0
                },
                {
                    "sent": "So here you know, here's the running time versus the size of the grid, and notice that the Y axis is a log scale, so it is actually quite a bit faster than the earlier algorithms.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and so in the next set of experiments we kept the size of the grid rule fixed the change.",
                    "label": 0
                },
                {
                    "sent": "The number of regions and what's interesting is that the LP approach actually gets a little bit faster as the number of regions goes up.",
                    "label": 0
                },
                {
                    "sent": "And so you know, for us, the LP solver is a bit of a black box, but we think it's because.",
                    "label": 0
                },
                {
                    "sent": "As the number of regions increases, the number of constraints, the number of constraints is increasing, and in some cases when you increase the number of constraints, you actually make the problem easier to solve.",
                    "label": 0
                },
                {
                    "sent": "You're sort of over constraining it.",
                    "label": 0
                },
                {
                    "sent": "So that seems to be what's happening here.",
                    "label": 0
                },
                {
                    "sent": "And again, the Y axis here is log scale.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, just some demos.",
                    "label": 0
                },
                {
                    "sent": "Applying this to the car driving example, so here's an expert.",
                    "label": 0
                },
                {
                    "sent": "This expert is OK.",
                    "label": 0
                },
                {
                    "sent": "He's trying to avoid cars, and he's doing an OK job.",
                    "label": 0
                },
                {
                    "sent": "And the output of the algorithm is.",
                    "label": 1
                },
                {
                    "sent": "A car that also does OK, you know he drives pretty fast and he tries to avoid cars, so this is an example of where we were not able to improve on the behavior of the expert.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "We did actually showed this earlier, so here's the expert driving along not doing too well.",
                    "label": 1
                },
                {
                    "sent": "And here's the behavior that we learned here.",
                    "label": 0
                },
                {
                    "sent": "So in this example, that margin that we were maximizing that margin turned out to be really big because the experts policy is so poor.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and just a few other topics.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also, in the course of our experiments, we observed that the earlier algorithm for apprenticeship learning actually seems to do a little bit better than the theory predicts that it should, and so there's sort of a gap gap in our understanding of this algorithm, and we currently have some new results that we're preparing that explains this gap in the current.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work I guess.",
                    "label": 0
                },
                {
                    "sent": "And also I wanted to point out that this sort of this approach is really closely connected to a lot of stuff that's been done in the literature.",
                    "label": 0
                },
                {
                    "sent": "Alot of it which actually appears in the our community, not all of it, but a lot of it sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, working with these occupancy measures and things like constrained MDP, SRL, multiple rewards, and also some work very closely related to apprenticeship learning called Max margin planning.",
                    "label": 1
                },
                {
                    "sent": "They also used a similar trick.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so, just to recap a new algorithm that does.",
                    "label": 1
                },
                {
                    "sent": "Produces similar policies and is empirically faster, thanks.",
                    "label": 1
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah, um.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, I can say that we've thought about thinking about that.",
                    "label": 0
                },
                {
                    "sent": "But I couldn't say anything more, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Is there any apprenticeship learning stuff that has been extended to the linear function approximation?",
                    "label": 0
                },
                {
                    "sent": "Not to my knowledge.",
                    "label": 0
                },
                {
                    "sent": "Policies.",
                    "label": 0
                },
                {
                    "sent": "Horses or six times.",
                    "label": 0
                },
                {
                    "sent": "I remember second edition.",
                    "label": 0
                },
                {
                    "sent": "Original.",
                    "label": 0
                },
                {
                    "sent": "A different test is different styles, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Creating a style and was trying to get as many options.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "I'm how accepting until experts.",
                    "label": 0
                },
                {
                    "sent": "No, you couldn't learn it because you would know that it's dangerous.",
                    "label": 0
                },
                {
                    "sent": "So the key difference in our assumptions that we assume that the weights the unknown weights are all positive.",
                    "label": 0
                },
                {
                    "sent": "And by assuming there positive, you're saying you know how each of these basis rewards correlate with the true reward.",
                    "label": 0
                },
                {
                    "sent": "So you actually know, sort of you don't know the magnitude of the influence, just know the direction of the influence.",
                    "label": 0
                },
                {
                    "sent": "That's where the improvements come from.",
                    "label": 0
                },
                {
                    "sent": "OK, it's it's Thursday, but there's something I didn't quite get this occupancy measure.",
                    "label": 0
                },
                {
                    "sent": "It depends on the discount factor.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the relation that you showed?",
                    "label": 0
                },
                {
                    "sent": "Find function.",
                    "label": 0
                },
                {
                    "sent": "I can see where it's going, but yeah, the one that keeps the policy.",
                    "label": 0
                },
                {
                    "sent": "Can you come out on?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah so.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious.",
                    "label": 0
                },
                {
                    "sent": "I agree it's not obvious, but if you just sort of write it out and work it out, these discount factors basically doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "It just works out the way you expect.",
                    "label": 0
                },
                {
                    "sent": "It's in the appendix of the paper.",
                    "label": 0
                },
                {
                    "sent": "I OK. Is it possible that you could actually perform very well without any expert examples?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you could you could, you could just take your prior knowledge about how the basis rewards correlate with that reward and try to do well.",
                    "label": 0
                },
                {
                    "sent": "Just extracting that knowledge in the earlier paper, we had some examples of us doing that.",
                    "label": 0
                },
                {
                    "sent": "If you haven't wondered at the back, yeah, this all seems that you have a model dynamics that's right.",
                    "label": 0
                },
                {
                    "sent": "So there's a paper yesterday that was presented in this room as well, where they also assumed that the reward function was unknown and then.",
                    "label": 0
                },
                {
                    "sent": "You're formulating in such a way that they were looking for like pretty much like a convex Hull, or like.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Wondering the relationship square because it seems that in some cases I guess some of those policies that are purple optimal would end up being the same as the one that you would find that simply maximizing the margin where the market seems to be like a specific direction for maximizing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think like I mean I could be wrong, but I think the policy we learn is going to be in the set of policies that they learn.",
                    "label": 0
                },
                {
                    "sent": "They learn like a set of policies, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "So I think it's going to be in that set.",
                    "label": 0
                },
                {
                    "sent": "Certainly.",
                    "label": 0
                },
                {
                    "sent": "Fashion is so you have this constraints.",
                    "label": 0
                },
                {
                    "sent": "They're trying to maximize the management.",
                    "label": 0
                },
                {
                    "sent": "If you disappear, the basis functions, for example, and those constraints would change accordingly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can imagine playing with weights or.",
                    "label": 0
                },
                {
                    "sent": "Do you have any ideas about how to use this or this is a good program that they are going where it seems to me that spot panel said that you are taking a certain direction which is determined by the actual scale of the basis functions.",
                    "label": 0
                },
                {
                    "sent": "That yeah, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "So that's right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if you re scale things, I mean the scale is also.",
                    "label": 0
                },
                {
                    "sent": "Perhaps inadvertently, is part of the prior knowledge that you're injecting into the problem.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is the interpretation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, right?",
                    "label": 0
                },
                {
                    "sent": "I have yet another question.",
                    "label": 0
                },
                {
                    "sent": "So you assume that the later are non negative, right?",
                    "label": 0
                },
                {
                    "sent": "So what happens if you don't have this knowledge?",
                    "label": 0
                },
                {
                    "sent": "Oh well, so one thing you could do is.",
                    "label": 0
                },
                {
                    "sent": "I mean, sort of what you can do is just double the number of basis reward functions and for each basis reward replicated with it's negative.",
                    "label": 0
                },
                {
                    "sent": "And if you do that, then you're sort of saying, OK, I don't know anything about how the basis awards correlate with the true word.",
                    "label": 0
                },
                {
                    "sent": "And still get guarantees, but you'll never do better.",
                    "label": 0
                },
                {
                    "sent": "That's the thing, but you always do at least as well.",
                    "label": 0
                },
                {
                    "sent": "Which apprenticeship, learning an inverse reinforcement learning yeah.",
                    "label": 0
                },
                {
                    "sent": "So I think you know at least the way it was originally posed.",
                    "label": 0
                },
                {
                    "sent": "The goal of inverse reinforcement learning was to recover the original reward, right?",
                    "label": 0
                },
                {
                    "sent": "So here the goal is not to actually recover the true reward function.",
                    "label": 0
                },
                {
                    "sent": "The algorithms they in each round.",
                    "label": 0
                },
                {
                    "sent": "They have sort of a candidate reward function, but it has no correlation to the real reward, and it may may not be anything like it.",
                    "label": 0
                },
                {
                    "sent": "So the objective is different.",
                    "label": 0
                },
                {
                    "sent": "The objective in this case is not to recover the true word.",
                    "label": 0
                },
                {
                    "sent": "Alright, no more questions and less time with speaker.",
                    "label": 0
                }
            ]
        }
    }
}