{
    "id": "ttlnipscb3nhnk4ytrzcgfajgk734aqh",
    "title": "Efficient MultiClass Maximum Margin Clustering",
    "info": {
        "author": [
            "Bin Zhao, Department of Automation, Tsinghua University"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml08_zhao_emm/",
    "segmentation": [
        [
            "My name is Big John and this is joint work was very long and chance returning to high University.",
            "The topic of this work is called Magic House, maximizing clustering.",
            "Maximizing clustering extends the theory of spot vector machine to the unsupervised scenario and aims at finding a clustering of the datasets.",
            "Into into different clusters with the maximum margin separating hyperplane."
        ],
        [
            "This talk will be dividing into five parts.",
            "I will start with the introduction to the two class maximum margin clustering, including its mathematical formulation and the representative works on it.",
            "Then I'll present the formulation of the multi class Max margin clustering and the cutting plane algorithm we proposed in this paper to solve the problem.",
            "Followed by a theoretical analysis on both the accuracy and time complexity of the algorithm.",
            "Then I'll give some experimental results, including both the two class maximum margin clustering and multiclass maximizing clustering.",
            "Finally, I will conclude this this talk with some summaries."
        ],
        [
            "OK, I'll start with the introduction to the two cloud."
        ],
        [
            "US maximum margin clustering.",
            "As I mentioned before, maximum margin clustering adopts the maximizing thiery hotspot vector machine.",
            "Therefore, we need to 1st review some basics about support vector machine.",
            "For the binary case, given the data set X and their labels Y, SVM finds a hyperplane F. By solving this optimization problem.",
            "You will see that here it is a little bit different from the traditional formulation because I divided the sea by N. This is this means this is better for capturing House C scales with the data set size.",
            "Other things just.",
            "The same with the traditional formulation."
        ],
        [
            "Maximum margin clustering.",
            "Extends the supervector machine theory to the unsupervised scenario where instead of fighting a war, instead of finding the classifying, applying with with the labels known in MMC, the labels of the data set are unknown, so.",
            "In the optimization problem, we have to minimize the objective function with respect to both the hyperplane parameters WB and the labeling variables Y.",
            "So we can see this is a non convex integer programming problem and is thus difficult to solve.",
            "There there have been several works on maximum margin clustering, meaning focusing on the two class maximum margin clustering problem.",
            "Including the techniques they used includes.",
            "Semidefinite programming and alternating optimization, however, the problem with the techniques that it uses.",
            "Semidefinite programming is the high computational complexity.",
            "Well, although that.",
            "The algorithm that uses alternating optimization could solve this problem much efficient.",
            "However, we don't know how fast it will converge.",
            "There is not a theoretical analysis."
        ],
        [
            "So.",
            "In real world applications.",
            "Problems clustering problems with more than two clusters are quite quite common, so we need to extend the two class maximum margin clustering to the multiclass scenario, and at the meantime propose an efficient algorithm to solve the multiclass.",
            "The multiclass maximum margin."
        ],
        [
            "Clustering.",
            "Similar waste two class scenario.",
            "We first present the multiclass support vector machine proposed by criminal singer.",
            "Giving a point stacks and their labels Y multiclass SVM defines a weight vector WP for each for each class P and the classifier sample X by this equation.",
            "So."
        ],
        [
            "Who extend this problem to the?",
            "Unsurprise scenario where we?",
            "Adapt the same formulation of the problem.",
            "Just we also need to minimize the objective function with respect to the labeling variables, so again, this is a non convex integer programming problem.",
            "The major difficulty with this problem is the huge number of variables involved.",
            "Specifically, there are labeling variables Y and unselect variables PSI.",
            "So the basic idea of our algorithm is to reduce.",
            "The number of variables involved."
        ],
        [
            "So we need to reformulate the problem.",
            "This is the first step of our reformulation.",
            "We first need to reduce.",
            "We first need to eliminate the N labeling variables, which take the, which take the integer integer value.",
            "So Problem 5 is equivalent to problem four, and there is a proof in our paper if you are interested, you might refer to the paper.",
            "So after this step of reformulation, the number of variables involved in the optimization problems introduced by N. Moreover, there were no integer variables involved.",
            "However, there were still in Slack variables sigh involved.",
            "So in the next step we will try to eliminate the N select very."
        ],
        [
            "Close so this comes to the problem reformulation Step 2.",
            "Similarly, you can.",
            "We can also prove theoretically that problem 6 is equivalent to problem 5.",
            "The basic idea here is that we reformulate the constraints in problem 6, and each constraint here corresponds to the sum of a subset of constraints from problem 5 and.",
            "The matrix C. Selects the subset of constraints.",
            "And as I mentioned before, we could prove theoretically that we still problems are equivalent."
        ],
        [
            "So after the first after the above 2 step software formulation, the number of variables involved in our optimization problem is reduced by two and minus one.",
            "And however, at the same time the number of constraints increased from 10K to quick pass to the end.",
            "So the major problem left is how to solve this problem with so many constraints and we adapt and adaptation of the cutting plane algorithm where we target to find a small subset of constraints with which the solution of the relax problem fulfills our constraints from problem 6 up to position of epsilon.",
            "That is to say.",
            "The remaining number of remaining exponential, exponential number of constraints are guaranteed to be violated by less than epsilon without the explicit need to adding them up.",
            "Adding, add them into the optimization problem."
        ],
        [
            "So this is the outline of the cutting plane algorithm we used here.",
            "First we start with an empty constrained subset Omega.",
            "Then we compute the optimal the optimal solution to problem 6, subject to the constraints in Omega.",
            "Then we obtain the weight vectors W one through WK.",
            "Anna using this.",
            "With vectors we could calculate liquid, find the most violated constraint in the problem 6, and then we add this constraint into the constraint subset of mega and these two steps just.",
            "Forms a loop and this loop stops when no constraint in six is violated by more than epsilon, and this equation tells us what it means that no constraints in six is valid by more than epsilon.",
            "So basically there are two problems left for us to solve.",
            "The first we need to find we need to compute the optimal solution to problem 6.",
            "Subject to this constraint.",
            "In this constraint subset of mega.",
            "Then we need to find the most violated constraint, so we will solve these two prob."
        ],
        [
            "Homes.",
            "First, let's take a look at how to find the most violated constraint here.",
            "We define the.",
            "As each constraint in problem 6 is represented as a matrix C. And.",
            "The most valid constraints is the one that would result.",
            "In that will result in the largest cosine in all constraints in problem 6, so.",
            "By calculating the matrix C as in question eight, we find the constraint that results in the largest concise, which means we solved.",
            "We get the most violated constraint."
        ],
        [
            "There is one more thing about the previous formulation of the multiclass maximizing clustering that is.",
            "There was some.",
            "There is a trivially optimal solution too.",
            "To the optimization problem six.",
            "That is, if we.",
            "So if I put all the data samples into one cluster and.",
            "The resulting margin will be maximal, so this is a trivially optimized optimal solution.",
            "Moreover, we could always get a larger margin if we eliminate classes clusters.",
            "So to alleviate these trivial solutions, we add this class balance constraint.",
            "Here L is a parameter that controls the class imbalance.",
            "Suppose.",
            "Suppose the current constraints subset is Omega, the multiclass maximum margin clustering could be formulated as problem 9 if we take a look.",
            "If we take a close look at this problem, or you'll see that the objective function is quadratic, and this groups of constraints.",
            "Non convex and.",
            "The class balance constraint is linear, so the major difficulty is this constraint.",
            "However, although this is a non convex constraint, it could be represented as a difference of two convex functions.",
            "And fortune."
        ],
        [
            "Honestly, we got this.",
            "Constrained concave convex procedure.",
            "CCP is optimization methods designed to solve problems with whose objective functions and whose objective function and constraints could be expressed as a difference of two convex functions.",
            "So, here FIS and GI are both convex functions and CII real numbers.",
            "And CCP is designed to solve this problem."
        ],
        [
            "The main slapping CCP is too.",
            "Replace the second convex function with its first other Taylor expansion, at the premise that the present solution.",
            "Specifically, given initial point Z0, the CCP computes Z T + 1 from ZTE by replacing gisy, which is with this first order Taylor expansion and ziti and this procedure just goes on to convergence.",
            "So using CC."
        ],
        [
            "P who could solve this problem?",
            "We just need."
        ],
        [
            "So calculate.",
            "The first other tail."
        ],
        [
            "Expansion of the second part of the convex function.",
            "There's one more detail that this function is now is not smooth, so we need to replace the gradients with the subgradient and which could be calculated at this.",
            "So at this moment we could just substitute the 1st order Taylor expansion into problem line and the procedure of our cutting plane algorithm is complete."
        ],
        [
            "So up to this point, I have presented all details of the of the employee maximum margin clustering algorithm we proposed in this paper.",
            "Next I will present a theoretical analysis on the algorithm."
        ],
        [
            "First I will start always with an analysis of the of the accuracy of the algorithm.",
            "Suppose our algorithm returns appoints W one through WK, cosine, then W one through WK.",
            "Kasai Plus Epsilon is feasible in the original maximum margin clustering problem.",
            "This tells us absolutely indicates how close one wants to be to the optimal separating hyperplane and could be used as the stopping criterion.",
            "In the maximum margin clustering."
        ],
        [
            "Next we will study the time complexity of the algorithm as the algorithm is iterative, so we need to.",
            "First study the time complexity in each step of the iteration.",
            "Then we need to.",
            "Analyze how many iterations are involved in the algorithm.",
            "The first theorem here shows that.",
            "Each in each iteration of the cutting plane, maximum margin clustering algorithm.",
            "Time complexity is OS&K.",
            "Here any represents the size of the data shed and as is the average number of non 0 features in the data set which indicates the sparseness.",
            "All this sparsity and the case, the number of clusters involved, so we know in each iteration of the algorithm the time complexity is OS&K.",
            "Then we need to analyze how many iterations are involved.",
            "Here we prove that for the two class maximum margin clustering problem, the number of.",
            "A number of iterations involved is at most R over epsilon square.",
            "Here R is a constant number in different independent of an ash which is introduced in the in the procedure of the proof of proof of this theorem.",
            "So now we know for the two class maximum margin clustering problem.",
            "And the number of constraints involved is a constant number which is independent of any."
        ],
        [
            "Us so we came to the final theorem that is.",
            "For the cool class, make some margin clustering with its own samples and the sparsity of ash.",
            "The algorithm takes time OSN to converge.",
            "In real world applications such as text categorization.",
            "The data are really sparse, so as.",
            "Cube in much less than N so the time complexity of this algorithm is approximately.",
            "Oh, in that means the computational algorithm, computational complexity, computational time of the algorithm scales roughly linearly with the data set size, which is kind of attractive."
        ],
        [
            "Now I will present some experimental results, although the focus of this paper is the multiclass maximum margin clustering.",
            "In order to better compare with previous works on maximum margin clustering, we also present the results on the two class maximum margin clustering costs because the previous works has mainly focused on the two class."
        ],
        [
            "Problem.",
            "Here we use K means clustering and normalized cards as baselines.",
            "And we also compared to three maximum margin clustering algorithms, namely MMC, GMC and SVR.",
            "MMC and GMC uses semidefinite programming to solve the problem, and SVR uses iterative optimization.",
            "And the last one is our algorithm.",
            "To assess the clustering accuracy, we first remove all labels, labels of all data points in the data set and we run the clustering algorithms.",
            "Then we label each of the resulting cluster with the majority class according to the initial training labels, and then we calculate the misclassifications.",
            "There's also a run index comparison results in our paper if you are.",
            "If you are interested in might refer to that part."
        ],
        [
            "But and this is the multi class comparison results.",
            "So basically account."
        ],
        [
            "Into these two tables.",
            "Our call employee maximizing clustering algorithm achieves.",
            "Similar or even higher performance with previous maximum margin clustering algorithms."
        ],
        [
            "Then we compare the speed.",
            "Of our algorithm with other maximum margin clustering algorithms.",
            "According to this table we see our our algorithm is at least 18 times faster than SVR and about 200 times faster than GMC.",
            "Although as presented in the previous paper, GMC is about 100 times faster than MMC.",
            "So are our algorithm is still faster than mercy by orders of four.",
            "There's one thing I should mention here that all algorithms here are implemented using pure Matlab."
        ],
        [
            "And this is the multi class comparison of the speed as.",
            "As we can see from the table, so can you play maximum margin clustering.",
            "Convergence convergence in some scenarios even faster than K means clustering."
        ],
        [
            "As we stated in the theoretical analysis, in the two class scenario, maximum margin clustering using the cutting plane algorithm CONVERT has a time complexity that scales roughly linearly with the data set size.",
            "Here we present the experimental results to show how the computational time for the multi class maximum margin clustering scales with the number of samples.",
            "As you can see, this is a log log plot and the lines in the log log plots represent polynomial growth of all into the D, and this black line represents ON.",
            "As we can see, the computer computational time of cutting plane, maximum margin clustering scales roughly linearly.",
            "With the data set size."
        ],
        [
            "Finally we study.",
            "How the parameter epsilon affects the accuracy and time complexity of the algorithm first this is the accuracy study.",
            "The horizontal axis represents the value of epsilon and the vertical axis represents the clustering accuracy.",
            "From this these two figures, we see that by setting epsilon equals 2.",
            ".01 it is sufficient to guarantee to guarantee the clustering accuracy."
        ],
        [
            "Next way analysis.",
            "How computational time of can you play maximum margin clustering scale with epsilon?",
            "As we can see from this these two figures.",
            "The computational time of can employ maximum margin clustering decreases as the value of absolute increases.",
            "And the empirical scaling of roughly oh X to the minors.",
            ".5 is much better than the theoretical bounds of all X to the minors too."
        ],
        [
            "Now we come to the conclusions.",
            "So basically the cutting plane make some margin clustering proposing this paper.",
            "Solve the maximum matching clustering problem efficiently and as a result could handle large real world datasets.",
            "Although we made several relaxations in the in this work.",
            "According to the, according to the extreme experimental results, there is no major loss in clustering accuracy.",
            "Moreover, there are some future works.",
            "We could research more.",
            "The first, most important thing is that there was.",
            "Several parameters involved in the algorithm, including L and epsilon.",
            "And the values of these two parameters affect the performance of the algorithm.",
            "So it would be much better if we could design an algorithm that could automatically tune these parameters.",
            "Also, we would would also like to try an even larger datasets which include like.",
            "100,000 data set data samples."
        ],
        [
            "And that's all, thank you.",
            "Please.",
            "Yeah, this classic cutting plane versus like the one of Kelly turning bullish time you have this long tail behavior where it takes quite quick progress and frustrations and apply.",
            "It takes a very long or very large number of iterations until you really improve.",
            "To give some intuition why this is not the case with Gomez.",
            "I think the attritions involved here is is controlled by the value of epsilon."
        ],
        [
            "And according to here to the two figures you might want to set epsilon now to such small value so it will converge much faster.",
            "It will not go to the large part of the.",
            "Of the line.",
            "Of the curve.",
            "OK so I was wondering, could you give a Hanson through some hints into why this is faster than K means?",
            "About kimmings, you mean theoretically?",
            "Yeah, in the stables.",
            "Under under profile.",
            "How?"
        ],
        [
            "Yep, Yep.",
            "Oh well.",
            "I think.",
            "It might be due to Kimmings is also iterative.",
            "And.",
            "Here, like in the text, data sets the number of features are really high.",
            "Yes, but certainly this is dinner with number of features and then certainly the CPM cute.",
            "She is also, but incoming plane maximizing clustering we employ the sparsity sparseness of the data set.",
            "We are in the implementation.",
            "So if this is a sparse later set, our algorithm could converge much faster.",
            "OK, so so my mission is that you are saying this comes from the fact that the initialization of the K means is just unknown.",
            "We use random initializations and this is the average results of.",
            "I don't remember 10 or 20 runs.",
            "And the.",
            "I think the most important reason is that the Community implementation we use perhaps do not.",
            "Take into account the sparseness of that data set, yeah?",
            "Otherwise, I would be very surprised that there is so much worse.",
            "Yeah, maybe.",
            "No, it's not fixed."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Big John and this is joint work was very long and chance returning to high University.",
                    "label": 0
                },
                {
                    "sent": "The topic of this work is called Magic House, maximizing clustering.",
                    "label": 0
                },
                {
                    "sent": "Maximizing clustering extends the theory of spot vector machine to the unsupervised scenario and aims at finding a clustering of the datasets.",
                    "label": 0
                },
                {
                    "sent": "Into into different clusters with the maximum margin separating hyperplane.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This talk will be dividing into five parts.",
                    "label": 0
                },
                {
                    "sent": "I will start with the introduction to the two class maximum margin clustering, including its mathematical formulation and the representative works on it.",
                    "label": 1
                },
                {
                    "sent": "Then I'll present the formulation of the multi class Max margin clustering and the cutting plane algorithm we proposed in this paper to solve the problem.",
                    "label": 1
                },
                {
                    "sent": "Followed by a theoretical analysis on both the accuracy and time complexity of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then I'll give some experimental results, including both the two class maximum margin clustering and multiclass maximizing clustering.",
                    "label": 1
                },
                {
                    "sent": "Finally, I will conclude this this talk with some summaries.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I'll start with the introduction to the two cloud.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "US maximum margin clustering.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned before, maximum margin clustering adopts the maximizing thiery hotspot vector machine.",
                    "label": 1
                },
                {
                    "sent": "Therefore, we need to 1st review some basics about support vector machine.",
                    "label": 0
                },
                {
                    "sent": "For the binary case, given the data set X and their labels Y, SVM finds a hyperplane F. By solving this optimization problem.",
                    "label": 1
                },
                {
                    "sent": "You will see that here it is a little bit different from the traditional formulation because I divided the sea by N. This is this means this is better for capturing House C scales with the data set size.",
                    "label": 0
                },
                {
                    "sent": "Other things just.",
                    "label": 0
                },
                {
                    "sent": "The same with the traditional formulation.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maximum margin clustering.",
                    "label": 0
                },
                {
                    "sent": "Extends the supervector machine theory to the unsupervised scenario where instead of fighting a war, instead of finding the classifying, applying with with the labels known in MMC, the labels of the data set are unknown, so.",
                    "label": 0
                },
                {
                    "sent": "In the optimization problem, we have to minimize the objective function with respect to both the hyperplane parameters WB and the labeling variables Y.",
                    "label": 0
                },
                {
                    "sent": "So we can see this is a non convex integer programming problem and is thus difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "There there have been several works on maximum margin clustering, meaning focusing on the two class maximum margin clustering problem.",
                    "label": 1
                },
                {
                    "sent": "Including the techniques they used includes.",
                    "label": 0
                },
                {
                    "sent": "Semidefinite programming and alternating optimization, however, the problem with the techniques that it uses.",
                    "label": 0
                },
                {
                    "sent": "Semidefinite programming is the high computational complexity.",
                    "label": 0
                },
                {
                    "sent": "Well, although that.",
                    "label": 0
                },
                {
                    "sent": "The algorithm that uses alternating optimization could solve this problem much efficient.",
                    "label": 0
                },
                {
                    "sent": "However, we don't know how fast it will converge.",
                    "label": 1
                },
                {
                    "sent": "There is not a theoretical analysis.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In real world applications.",
                    "label": 0
                },
                {
                    "sent": "Problems clustering problems with more than two clusters are quite quite common, so we need to extend the two class maximum margin clustering to the multiclass scenario, and at the meantime propose an efficient algorithm to solve the multiclass.",
                    "label": 0
                },
                {
                    "sent": "The multiclass maximum margin.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clustering.",
                    "label": 0
                },
                {
                    "sent": "Similar waste two class scenario.",
                    "label": 0
                },
                {
                    "sent": "We first present the multiclass support vector machine proposed by criminal singer.",
                    "label": 0
                },
                {
                    "sent": "Giving a point stacks and their labels Y multiclass SVM defines a weight vector WP for each for each class P and the classifier sample X by this equation.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who extend this problem to the?",
                    "label": 0
                },
                {
                    "sent": "Unsurprise scenario where we?",
                    "label": 0
                },
                {
                    "sent": "Adapt the same formulation of the problem.",
                    "label": 0
                },
                {
                    "sent": "Just we also need to minimize the objective function with respect to the labeling variables, so again, this is a non convex integer programming problem.",
                    "label": 0
                },
                {
                    "sent": "The major difficulty with this problem is the huge number of variables involved.",
                    "label": 0
                },
                {
                    "sent": "Specifically, there are labeling variables Y and unselect variables PSI.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea of our algorithm is to reduce.",
                    "label": 0
                },
                {
                    "sent": "The number of variables involved.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we need to reformulate the problem.",
                    "label": 0
                },
                {
                    "sent": "This is the first step of our reformulation.",
                    "label": 0
                },
                {
                    "sent": "We first need to reduce.",
                    "label": 0
                },
                {
                    "sent": "We first need to eliminate the N labeling variables, which take the, which take the integer integer value.",
                    "label": 0
                },
                {
                    "sent": "So Problem 5 is equivalent to problem four, and there is a proof in our paper if you are interested, you might refer to the paper.",
                    "label": 0
                },
                {
                    "sent": "So after this step of reformulation, the number of variables involved in the optimization problems introduced by N. Moreover, there were no integer variables involved.",
                    "label": 0
                },
                {
                    "sent": "However, there were still in Slack variables sigh involved.",
                    "label": 0
                },
                {
                    "sent": "So in the next step we will try to eliminate the N select very.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Close so this comes to the problem reformulation Step 2.",
                    "label": 0
                },
                {
                    "sent": "Similarly, you can.",
                    "label": 0
                },
                {
                    "sent": "We can also prove theoretically that problem 6 is equivalent to problem 5.",
                    "label": 0
                },
                {
                    "sent": "The basic idea here is that we reformulate the constraints in problem 6, and each constraint here corresponds to the sum of a subset of constraints from problem 5 and.",
                    "label": 1
                },
                {
                    "sent": "The matrix C. Selects the subset of constraints.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned before, we could prove theoretically that we still problems are equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after the first after the above 2 step software formulation, the number of variables involved in our optimization problem is reduced by two and minus one.",
                    "label": 0
                },
                {
                    "sent": "And however, at the same time the number of constraints increased from 10K to quick pass to the end.",
                    "label": 1
                },
                {
                    "sent": "So the major problem left is how to solve this problem with so many constraints and we adapt and adaptation of the cutting plane algorithm where we target to find a small subset of constraints with which the solution of the relax problem fulfills our constraints from problem 6 up to position of epsilon.",
                    "label": 1
                },
                {
                    "sent": "That is to say.",
                    "label": 0
                },
                {
                    "sent": "The remaining number of remaining exponential, exponential number of constraints are guaranteed to be violated by less than epsilon without the explicit need to adding them up.",
                    "label": 0
                },
                {
                    "sent": "Adding, add them into the optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of the cutting plane algorithm we used here.",
                    "label": 1
                },
                {
                    "sent": "First we start with an empty constrained subset Omega.",
                    "label": 1
                },
                {
                    "sent": "Then we compute the optimal the optimal solution to problem 6, subject to the constraints in Omega.",
                    "label": 1
                },
                {
                    "sent": "Then we obtain the weight vectors W one through WK.",
                    "label": 0
                },
                {
                    "sent": "Anna using this.",
                    "label": 1
                },
                {
                    "sent": "With vectors we could calculate liquid, find the most violated constraint in the problem 6, and then we add this constraint into the constraint subset of mega and these two steps just.",
                    "label": 0
                },
                {
                    "sent": "Forms a loop and this loop stops when no constraint in six is violated by more than epsilon, and this equation tells us what it means that no constraints in six is valid by more than epsilon.",
                    "label": 0
                },
                {
                    "sent": "So basically there are two problems left for us to solve.",
                    "label": 0
                },
                {
                    "sent": "The first we need to find we need to compute the optimal solution to problem 6.",
                    "label": 0
                },
                {
                    "sent": "Subject to this constraint.",
                    "label": 0
                },
                {
                    "sent": "In this constraint subset of mega.",
                    "label": 0
                },
                {
                    "sent": "Then we need to find the most violated constraint, so we will solve these two prob.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Homes.",
                    "label": 0
                },
                {
                    "sent": "First, let's take a look at how to find the most violated constraint here.",
                    "label": 1
                },
                {
                    "sent": "We define the.",
                    "label": 0
                },
                {
                    "sent": "As each constraint in problem 6 is represented as a matrix C. And.",
                    "label": 0
                },
                {
                    "sent": "The most valid constraints is the one that would result.",
                    "label": 0
                },
                {
                    "sent": "In that will result in the largest cosine in all constraints in problem 6, so.",
                    "label": 0
                },
                {
                    "sent": "By calculating the matrix C as in question eight, we find the constraint that results in the largest concise, which means we solved.",
                    "label": 0
                },
                {
                    "sent": "We get the most violated constraint.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is one more thing about the previous formulation of the multiclass maximizing clustering that is.",
                    "label": 0
                },
                {
                    "sent": "There was some.",
                    "label": 0
                },
                {
                    "sent": "There is a trivially optimal solution too.",
                    "label": 1
                },
                {
                    "sent": "To the optimization problem six.",
                    "label": 0
                },
                {
                    "sent": "That is, if we.",
                    "label": 0
                },
                {
                    "sent": "So if I put all the data samples into one cluster and.",
                    "label": 0
                },
                {
                    "sent": "The resulting margin will be maximal, so this is a trivially optimized optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we could always get a larger margin if we eliminate classes clusters.",
                    "label": 0
                },
                {
                    "sent": "So to alleviate these trivial solutions, we add this class balance constraint.",
                    "label": 1
                },
                {
                    "sent": "Here L is a parameter that controls the class imbalance.",
                    "label": 0
                },
                {
                    "sent": "Suppose.",
                    "label": 0
                },
                {
                    "sent": "Suppose the current constraints subset is Omega, the multiclass maximum margin clustering could be formulated as problem 9 if we take a look.",
                    "label": 1
                },
                {
                    "sent": "If we take a close look at this problem, or you'll see that the objective function is quadratic, and this groups of constraints.",
                    "label": 0
                },
                {
                    "sent": "Non convex and.",
                    "label": 0
                },
                {
                    "sent": "The class balance constraint is linear, so the major difficulty is this constraint.",
                    "label": 1
                },
                {
                    "sent": "However, although this is a non convex constraint, it could be represented as a difference of two convex functions.",
                    "label": 0
                },
                {
                    "sent": "And fortune.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Honestly, we got this.",
                    "label": 0
                },
                {
                    "sent": "Constrained concave convex procedure.",
                    "label": 0
                },
                {
                    "sent": "CCP is optimization methods designed to solve problems with whose objective functions and whose objective function and constraints could be expressed as a difference of two convex functions.",
                    "label": 1
                },
                {
                    "sent": "So, here FIS and GI are both convex functions and CII real numbers.",
                    "label": 0
                },
                {
                    "sent": "And CCP is designed to solve this problem.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main slapping CCP is too.",
                    "label": 0
                },
                {
                    "sent": "Replace the second convex function with its first other Taylor expansion, at the premise that the present solution.",
                    "label": 0
                },
                {
                    "sent": "Specifically, given initial point Z0, the CCP computes Z T + 1 from ZTE by replacing gisy, which is with this first order Taylor expansion and ziti and this procedure just goes on to convergence.",
                    "label": 1
                },
                {
                    "sent": "So using CC.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "P who could solve this problem?",
                    "label": 0
                },
                {
                    "sent": "We just need.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So calculate.",
                    "label": 0
                },
                {
                    "sent": "The first other tail.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Expansion of the second part of the convex function.",
                    "label": 0
                },
                {
                    "sent": "There's one more detail that this function is now is not smooth, so we need to replace the gradients with the subgradient and which could be calculated at this.",
                    "label": 0
                },
                {
                    "sent": "So at this moment we could just substitute the 1st order Taylor expansion into problem line and the procedure of our cutting plane algorithm is complete.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So up to this point, I have presented all details of the of the employee maximum margin clustering algorithm we proposed in this paper.",
                    "label": 0
                },
                {
                    "sent": "Next I will present a theoretical analysis on the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First I will start always with an analysis of the of the accuracy of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Suppose our algorithm returns appoints W one through WK, cosine, then W one through WK.",
                    "label": 0
                },
                {
                    "sent": "Kasai Plus Epsilon is feasible in the original maximum margin clustering problem.",
                    "label": 1
                },
                {
                    "sent": "This tells us absolutely indicates how close one wants to be to the optimal separating hyperplane and could be used as the stopping criterion.",
                    "label": 1
                },
                {
                    "sent": "In the maximum margin clustering.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we will study the time complexity of the algorithm as the algorithm is iterative, so we need to.",
                    "label": 0
                },
                {
                    "sent": "First study the time complexity in each step of the iteration.",
                    "label": 1
                },
                {
                    "sent": "Then we need to.",
                    "label": 0
                },
                {
                    "sent": "Analyze how many iterations are involved in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "The first theorem here shows that.",
                    "label": 0
                },
                {
                    "sent": "Each in each iteration of the cutting plane, maximum margin clustering algorithm.",
                    "label": 1
                },
                {
                    "sent": "Time complexity is OS&K.",
                    "label": 0
                },
                {
                    "sent": "Here any represents the size of the data shed and as is the average number of non 0 features in the data set which indicates the sparseness.",
                    "label": 0
                },
                {
                    "sent": "All this sparsity and the case, the number of clusters involved, so we know in each iteration of the algorithm the time complexity is OS&K.",
                    "label": 0
                },
                {
                    "sent": "Then we need to analyze how many iterations are involved.",
                    "label": 1
                },
                {
                    "sent": "Here we prove that for the two class maximum margin clustering problem, the number of.",
                    "label": 1
                },
                {
                    "sent": "A number of iterations involved is at most R over epsilon square.",
                    "label": 0
                },
                {
                    "sent": "Here R is a constant number in different independent of an ash which is introduced in the in the procedure of the proof of proof of this theorem.",
                    "label": 1
                },
                {
                    "sent": "So now we know for the two class maximum margin clustering problem.",
                    "label": 0
                },
                {
                    "sent": "And the number of constraints involved is a constant number which is independent of any.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Us so we came to the final theorem that is.",
                    "label": 0
                },
                {
                    "sent": "For the cool class, make some margin clustering with its own samples and the sparsity of ash.",
                    "label": 0
                },
                {
                    "sent": "The algorithm takes time OSN to converge.",
                    "label": 1
                },
                {
                    "sent": "In real world applications such as text categorization.",
                    "label": 0
                },
                {
                    "sent": "The data are really sparse, so as.",
                    "label": 0
                },
                {
                    "sent": "Cube in much less than N so the time complexity of this algorithm is approximately.",
                    "label": 0
                },
                {
                    "sent": "Oh, in that means the computational algorithm, computational complexity, computational time of the algorithm scales roughly linearly with the data set size, which is kind of attractive.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I will present some experimental results, although the focus of this paper is the multiclass maximum margin clustering.",
                    "label": 0
                },
                {
                    "sent": "In order to better compare with previous works on maximum margin clustering, we also present the results on the two class maximum margin clustering costs because the previous works has mainly focused on the two class.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "Here we use K means clustering and normalized cards as baselines.",
                    "label": 0
                },
                {
                    "sent": "And we also compared to three maximum margin clustering algorithms, namely MMC, GMC and SVR.",
                    "label": 0
                },
                {
                    "sent": "MMC and GMC uses semidefinite programming to solve the problem, and SVR uses iterative optimization.",
                    "label": 0
                },
                {
                    "sent": "And the last one is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "To assess the clustering accuracy, we first remove all labels, labels of all data points in the data set and we run the clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then we label each of the resulting cluster with the majority class according to the initial training labels, and then we calculate the misclassifications.",
                    "label": 0
                },
                {
                    "sent": "There's also a run index comparison results in our paper if you are.",
                    "label": 0
                },
                {
                    "sent": "If you are interested in might refer to that part.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But and this is the multi class comparison results.",
                    "label": 0
                },
                {
                    "sent": "So basically account.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into these two tables.",
                    "label": 0
                },
                {
                    "sent": "Our call employee maximizing clustering algorithm achieves.",
                    "label": 0
                },
                {
                    "sent": "Similar or even higher performance with previous maximum margin clustering algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we compare the speed.",
                    "label": 0
                },
                {
                    "sent": "Of our algorithm with other maximum margin clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "According to this table we see our our algorithm is at least 18 times faster than SVR and about 200 times faster than GMC.",
                    "label": 0
                },
                {
                    "sent": "Although as presented in the previous paper, GMC is about 100 times faster than MMC.",
                    "label": 0
                },
                {
                    "sent": "So are our algorithm is still faster than mercy by orders of four.",
                    "label": 0
                },
                {
                    "sent": "There's one thing I should mention here that all algorithms here are implemented using pure Matlab.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is the multi class comparison of the speed as.",
                    "label": 0
                },
                {
                    "sent": "As we can see from the table, so can you play maximum margin clustering.",
                    "label": 0
                },
                {
                    "sent": "Convergence convergence in some scenarios even faster than K means clustering.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we stated in the theoretical analysis, in the two class scenario, maximum margin clustering using the cutting plane algorithm CONVERT has a time complexity that scales roughly linearly with the data set size.",
                    "label": 1
                },
                {
                    "sent": "Here we present the experimental results to show how the computational time for the multi class maximum margin clustering scales with the number of samples.",
                    "label": 1
                },
                {
                    "sent": "As you can see, this is a log log plot and the lines in the log log plots represent polynomial growth of all into the D, and this black line represents ON.",
                    "label": 0
                },
                {
                    "sent": "As we can see, the computer computational time of cutting plane, maximum margin clustering scales roughly linearly.",
                    "label": 0
                },
                {
                    "sent": "With the data set size.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally we study.",
                    "label": 0
                },
                {
                    "sent": "How the parameter epsilon affects the accuracy and time complexity of the algorithm first this is the accuracy study.",
                    "label": 0
                },
                {
                    "sent": "The horizontal axis represents the value of epsilon and the vertical axis represents the clustering accuracy.",
                    "label": 0
                },
                {
                    "sent": "From this these two figures, we see that by setting epsilon equals 2.",
                    "label": 0
                },
                {
                    "sent": ".01 it is sufficient to guarantee to guarantee the clustering accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next way analysis.",
                    "label": 0
                },
                {
                    "sent": "How computational time of can you play maximum margin clustering scale with epsilon?",
                    "label": 1
                },
                {
                    "sent": "As we can see from this these two figures.",
                    "label": 1
                },
                {
                    "sent": "The computational time of can employ maximum margin clustering decreases as the value of absolute increases.",
                    "label": 0
                },
                {
                    "sent": "And the empirical scaling of roughly oh X to the minors.",
                    "label": 0
                },
                {
                    "sent": ".5 is much better than the theoretical bounds of all X to the minors too.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we come to the conclusions.",
                    "label": 0
                },
                {
                    "sent": "So basically the cutting plane make some margin clustering proposing this paper.",
                    "label": 1
                },
                {
                    "sent": "Solve the maximum matching clustering problem efficiently and as a result could handle large real world datasets.",
                    "label": 0
                },
                {
                    "sent": "Although we made several relaxations in the in this work.",
                    "label": 0
                },
                {
                    "sent": "According to the, according to the extreme experimental results, there is no major loss in clustering accuracy.",
                    "label": 1
                },
                {
                    "sent": "Moreover, there are some future works.",
                    "label": 0
                },
                {
                    "sent": "We could research more.",
                    "label": 0
                },
                {
                    "sent": "The first, most important thing is that there was.",
                    "label": 0
                },
                {
                    "sent": "Several parameters involved in the algorithm, including L and epsilon.",
                    "label": 0
                },
                {
                    "sent": "And the values of these two parameters affect the performance of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it would be much better if we could design an algorithm that could automatically tune these parameters.",
                    "label": 0
                },
                {
                    "sent": "Also, we would would also like to try an even larger datasets which include like.",
                    "label": 0
                },
                {
                    "sent": "100,000 data set data samples.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's all, thank you.",
                    "label": 0
                },
                {
                    "sent": "Please.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this classic cutting plane versus like the one of Kelly turning bullish time you have this long tail behavior where it takes quite quick progress and frustrations and apply.",
                    "label": 0
                },
                {
                    "sent": "It takes a very long or very large number of iterations until you really improve.",
                    "label": 0
                },
                {
                    "sent": "To give some intuition why this is not the case with Gomez.",
                    "label": 0
                },
                {
                    "sent": "I think the attritions involved here is is controlled by the value of epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And according to here to the two figures you might want to set epsilon now to such small value so it will converge much faster.",
                    "label": 0
                },
                {
                    "sent": "It will not go to the large part of the.",
                    "label": 0
                },
                {
                    "sent": "Of the line.",
                    "label": 0
                },
                {
                    "sent": "Of the curve.",
                    "label": 0
                },
                {
                    "sent": "OK so I was wondering, could you give a Hanson through some hints into why this is faster than K means?",
                    "label": 0
                },
                {
                    "sent": "About kimmings, you mean theoretically?",
                    "label": 0
                },
                {
                    "sent": "Yeah, in the stables.",
                    "label": 0
                },
                {
                    "sent": "Under under profile.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep, Yep.",
                    "label": 0
                },
                {
                    "sent": "Oh well.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "It might be due to Kimmings is also iterative.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here, like in the text, data sets the number of features are really high.",
                    "label": 0
                },
                {
                    "sent": "Yes, but certainly this is dinner with number of features and then certainly the CPM cute.",
                    "label": 0
                },
                {
                    "sent": "She is also, but incoming plane maximizing clustering we employ the sparsity sparseness of the data set.",
                    "label": 0
                },
                {
                    "sent": "We are in the implementation.",
                    "label": 0
                },
                {
                    "sent": "So if this is a sparse later set, our algorithm could converge much faster.",
                    "label": 0
                },
                {
                    "sent": "OK, so so my mission is that you are saying this comes from the fact that the initialization of the K means is just unknown.",
                    "label": 0
                },
                {
                    "sent": "We use random initializations and this is the average results of.",
                    "label": 0
                },
                {
                    "sent": "I don't remember 10 or 20 runs.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "I think the most important reason is that the Community implementation we use perhaps do not.",
                    "label": 0
                },
                {
                    "sent": "Take into account the sparseness of that data set, yeah?",
                    "label": 0
                },
                {
                    "sent": "Otherwise, I would be very surprised that there is so much worse.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe.",
                    "label": 0
                },
                {
                    "sent": "No, it's not fixed.",
                    "label": 0
                }
            ]
        }
    }
}