{
    "id": "4x5a26u75jpawmdywu7vxjyck6gomf43",
    "title": "Quickly Learning to Make Good Decisions",
    "info": {
        "author": [
            "Emma Brunskill, Computer Science and Artificial Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology, MIT"
        ],
        "published": "July 28, 2015",
        "recorded": "June 2015",
        "category": [
            "Top->Medicine->Neuroscience",
            "Top->Technology->Engineering->Electrical Engineering->Control Engineering",
            "Top->Computer Science->Machine Learning",
            "Top->Computer Science->Artificial Intelligence",
            "Top->Social Sciences->Psychology",
            "Top->Social Sciences->Economics"
        ]
    },
    "url": "http://videolectures.net/rldm2015_brunskill_good_decisions/",
    "segmentation": [
        [
            "Alright, I'm really happy to be here.",
            "I'm going to talk to you today about some of the work we're doing in my lab to quickly learn to make good decisions."
        ],
        [
            "But I'm going to start off and talk about a motivating example for me, which is the matrix who here is seen the matrix?",
            "OK, well there's no matter who here has not seen the matrix.",
            "OK, understandable, so go see it.",
            "It's a good film.",
            "Inside of it there is a character called Trinity, and at one point Trinity has to learn to fly a helicopter.",
            "So what she does is she downloads a program into her brain and approximately 5 seconds later she can fly a helicopter and help go save the world.",
            "I don't believe we're going to accomplish this over the next five years, but I do think that this is a really exciting vision to think about how we could have systems like this, which you could think of is sort of a really accelerated intelligent tutoring system.",
            "Try to enhance human capacity and well being."
        ],
        [
            "And because not surprising given the talk of the conference, I think we can do this using reinforcement learning, or at least that reinforcement learning can be a really important tool in trying to accomplish this."
        ],
        [
            "I think this because the whole idea behind reinforcement learning of thinking about gathering data, trying actions, seeing how they work, refining our models and refining our algorithms is a really powerful one.",
            "It's sort of this general idea of how do we have an evidence based life, and I think that we can use these ideas to try to create these systems to really enhance human capacity."
        ],
        [
            "And well being.",
            "Now."
        ],
        [
            "The stuff that I tend to think about is particularly motivated by problems in education, but more broadly we think about sort of having these reinforcement learning agents interact with people, so to create things like intelligent tutoring systems or medical decision support systems, etc.",
            "And I think when we start to think about reinforcement learning agents that interact with people, it focuses our attention on a particular set of sort of machine learning.",
            "Reinforcement learning challenges that overlap but are a little bit different than some of the ones that we think about in the context of robotics.",
            "And."
        ],
        [
            "This is because when I think about gathering data, every single data point is a person and I think that that raises a couple important things.",
            "The first is that if we make mistakes that affected someone, now, that doesn't mean necessarily that was catastrophic.",
            "Maybe it took us an hour longer to teach someone fractions than it should have, but it doesn't mean that we sort of lost an opportunity.",
            "The second thing is that if it takes us 7 billion people to learn how to teach fractions, not only did we waste a lot of people's time, or you know we didn't enhance their capacity as much as we could have, but we only have 1 billion people left to try to use the results of our wonderful reinforcement learning algorithm.",
            "We don't have infinite people, unlike sort of in the case of educational games or through the game Atari that we saw yesterday.",
            "We can't do infinite numbers of exploration, and so I think that that raises some interesting challenges and constraints, which means that we need to be careful about data.",
            "And we need to create faster learning reinforcement learning agents that can best leverage data as fast as possible in order to make good decisions."
        ],
        [
            "So today we're going to talk about is a couple of particular efforts in my lab around this, and this is kind of bleeding edge research.",
            "I thought it would be fun to talk about some really hot off the presses, stuff that we're doing, and so I'm excited to talk to you guys afterwards about these ideas.",
            "And the first thing I'm going to talk about is sort of optimistic learning and control and partially observable domains.",
            "But before I do that, I'm going to do sort of a really brief overview on Markov decision processes in background, and I know that a number of you guys are experts in this and some of others.",
            "If you probably saw Michael's talk yesterday."
        ],
        [
            "But this is just going to make sure that everyone's got us out the same vocabulary.",
            "So a Markov decision process can be described as a tuple where there is sort of a set of states.",
            "You could think in this example that the state of the ship is its GPS location."
        ],
        [
            "And it has access to a set of actions and that those actions in general can have stochastic outcomes.",
            "You have some probability distribution over the next state."
        ],
        [
            "And that we get some reward.",
            "Maybe there's some spices in the world when there are Pirates, other places, and so we can have a reward associated with states or States and actions, or state actions and next dates.",
            "And in some cases we also have a discount factor that describes how much we care about immediate reward versus future reward."
        ],
        [
            "Now in these situations, instead of just thinking about trajectories of actions, we need to think about policies and mapping of states to actions and the way we value."
        ],
        [
            "Wait, the goodness of a policy is to think about what the expected summer future rewards it.",
            "So we would like to compute the optimal policy that maximizes this expected summer feature rewards given the stochastic city in the environment."
        ],
        [
            "Now, when reinforcement learning, we often don't know what the dynamics are or the reward function in advance.",
            "We essentially don't know how the world works because of that.",
            "This makes the problem of figuring out the optimal policy a lot more challenging."
        ],
        [
            "Now I'm going to talk a couple, but it's sort of some variants of this situation.",
            "One is multi armed bandits which are very important problem in their own right, but you can loosely think about as being essentially RL without state.",
            "So now we just have a set of actions we can take and we can look at some of the expected reward of taking different actions.",
            "And again we want to maximize our expected summer future rewards."
        ],
        [
            "And we can also think about partially observable Markov decision processes, where now we've lost our GPS.",
            "But we can sort of navigate from where the stars are and what the wind is like and use those sort of observations to try to give us a sense of what the underlying hidden state is that we don't have direct access to."
        ],
        [
            "Now, in the context when we're trying to think about reinforcement learning, there's this really fundamental tradeoff that I think is fascinating, which is the exploration exploitation tradeoff.",
            "And exploration you can think about this at least in one way as like trying to understand how the world works.",
            "So in our ship example, we could move all over the world and try to understand the currents and try to learn where there are spices."
        ],
        [
            "But eventually our baskets, frustrated with us and says, alright, I'm glad you've explored the entire world and you can give me a complete map of it if we're all the spices are, but I need you to actually start bringing spices back.",
            "And so in this situation, what we have to do is exploit.",
            "We have to leverage our information about the world in order to make good decisions and try to achieve high reward.",
            "And this fundamental tradeoff is a really hard one, and there's been a lot of really exciting work to think about.",
            "What's the best way to do this exploration exploitation tradeoff?"
        ],
        [
            "So now I'm going to sort of now that everybody is up speed.",
            "I'm going to sort of talk about how we might be able to do, sort of optimistic learning and control when the domain is partially observable.",
            "And I."
        ],
        [
            "1st going to start with why I started thinking about this problem so when I was doing my PhD I thought a lot about planning and partially observable Markov decision processes and the planning problem is to say you're given a specific model, so I know how the world works.",
            "I know what my observation model is and I want to compute a policy.",
            "I want to compute a policy that maximizes my expected summer future rewards.",
            "And so I thought a lot about that, but I was starting to get interested in education and so then when I got to Carnegie Mellon, I was writing a grant with a colleague of mine and we're really excited about bringing these ideas to tutoring systems.",
            "So this is an example of one of the tutors that we have right now and just tried to teach students fractions.",
            "And so the idea of our grant was to say, well, what activities should we give?",
            "Went to the student?",
            "When should we do things like videos?",
            "When should we do things like, you know, learning activities?",
            "When should we get them to self reflect?",
            "And I said, that's great.",
            "We can model.",
            "This is a palm DP.",
            "Will learn a great plan and you know, will optimize student learning.",
            "And so my colleague asked me a very reasonable question.",
            "He's a learning scientist.",
            "He's not a machine learning guy and he said, alright, great.",
            "How much data do you need?",
            "How much data do you need to learn a good palm DP model that then you can optimize and try to get better learning.",
            "And I thought back out of the methods that I knew from grad school and like expectation maximization, and I was like ha, I don't know.",
            "And that seems like a really basic question in this sort of experimental Sciences, it's very common to do things like power analysis to say how much data do you need so that you can actually confirm or disprove your hypothesis.",
            "And it seemed really funny to me that I couldn't say how much data that I would need to learn good enough models so that we could actually create Palm DP's for this example, But it turns out that sort of standard techniques like expectation maximization only give you local Optima and they don't give you any guarantees on the performance of the Palace of the parameters you get out.",
            "You just don't know how well you know them."
        ],
        [
            "Now, why is this important?",
            "Why should we model model uncertainty?",
            "Well, I think there's at least two reasons why modeling model uncertainty is important.",
            "The first is is that if we understand how good our models are, that can help us understand how good our policy will be, because there are well known techniques to take uncertainty over the model and propagate that into uncertainty over the value function or uncertainty over the policy.",
            "So that's one really important reason.",
            "The other important reason is that it can actually help us learn.",
            "So there's a lot of great work in the fully observable case of saying, given uncertainty over how we think the world works, but we can use that to try to balance the exploration exploitation tradeoff.",
            "So we wanted to be able to do this in sort of partially observable domains."
        ],
        [
            "Now, excitingly over the last like sort of five to 10 years, there's been a lot of excitement over predictive state representations and spectral learning and method of moments, and these new forms to sort of do.",
            "Clustering and predictive models and dynamical systems, and there's been a lot of great work by people throughout the machine learning community, and a number of people here.",
            "People like Michael Littman's tender seeing Byron Boots, Jeff Gordon, Joelle Pineau.",
            "There's been a lot of people who've been starting to think about these methods and how we can start to propagate them into control.",
            "But I was particularly excited by one sort of subclass of these type of methods."
        ],
        [
            "And that's called method of moments.",
            "So raise your hand if you learn about method of moments in grad school.",
            "So a couple people, so I certainly didn't, and I think a lot of people still don't.",
            "So method of moments is a way to do latent variable estimation, and it's actually been around for a really long time.",
            "It's been around for at least 100 years and the essential idea is that if you think about a distribution, we can describe it by its moments.",
            "We can describe it by its meaning and its variants and its kurtosis and its Q.",
            "And then if you could write out all the infinite number of moments of a distribution, you would completely characterize it.",
            "So that we can use that for estimation if we have a whole bunch of data and then we could find some parameters that match all the moments of that data.",
            "But if we had enough data, we would be confident that we found the distribution that describes the generating distribution.",
            "So that's the essential idea of method of moments is that we can try to match the moments inside of the data.",
            "So within itself, that's sort of a nice idea, but the more exciting thing to me is that we can use."
        ],
        [
            "It to get formal bounds on how good those parameters are, so work that was led by people like Sham, Kakade, Ann, Daniel Hsu and other people has been really making some pioneering work on how can we use method of moments to get finite sample guarantees for latent variable models.",
            "So what this means is we can take data and we can do things like tensor decomposition and then we can get out parameters with uncertainties.",
            "We can say hey for this mixture model this is how well we know the means or for this hidden Markov model.",
            "This is how well we know the observation models and how well we know the transition models.",
            "So we can start to get this model of model uncertainty."
        ],
        [
            "And so to me that was super exciting, because that means that we could then take those models which we now have uncertainty over and then do a control algorithm that actually considers that model uncertainty.",
            "And then we would generate more data and then we could close this loop.",
            "And I think this is important for a couple of reasons.",
            "The biggest one is that it could allow us to start to tackle these partially observable domains and use this type of idea to try to guide exploration exploitation."
        ],
        [
            "So I'm just going to talk about two particular examples from our lab where we've been doing this, but it's definitely a general big effort, so the first is for partially observable RL and so in partially observable RL.",
            "Remember, we're going to be taking actions with the environment in the environment is going to give us back observations.",
            "And then in the paradigm I provoked proposing, we're going to build a model, and then we're going to be using that model to generate control, and then this loop will be closed.",
            "And this was the."
        ],
        [
            "Ample where I was inspired by, so we sort of have an intelligent tutoring system which is our agents and it's making actions.",
            "It's sort of giving problems and other activities to the student, and the student is responding and you can see how long they watched the video or whether they got a problem correct and we can use that data to build up these models.",
            "Now it turns out that partially bzero Laurel is a really hard problem.",
            "For those of you that learned about Palm, DP's in grad schools, those are already considered pretty intractable.",
            "And then now we don't even know the parameters.",
            "But there's been some really nice Bayesian work on trying to tackle this problem, and what the Bayesian approach does essentially is it says alright, we've got uncertainty over the state.",
            "We've got uncertainty over the model.",
            "We could try to exactly optimize the expected reward, given that uncertainty kind of makes this mega continuous.",
            "State palm DP.",
            "Now I love the Bayesian ideal.",
            "I think that if we could accomplish this, this would exactly optimize the exploration exploitation tradeoff.",
            "So it's really, really elegant.",
            "The problem is, it's almost completely computationally intractable.",
            "While there's been some really nice empirical demonstrations of this, it means because we have to reduce our computation so much to make it or reduce the problem.",
            "So much to make it computationally tractable.",
            "It typically means that we end up with no guarantees on the performance.",
            "OK, so how could we maybe address this?",
            "Well, let's think about."
        ],
        [
            "Fully observable MDP's this problem, intractability is not inherent just to partially observable settings.",
            "So in the fully observable case we would also like to maximize expected reward optimally from the get go, but that's often really hard and so one thing that came out starting for the late 90s till now, which I think was really exciting, is that we could think about sort of an alternative criteria that's a little bit looser, but is much more computationally tractable to compute, and still how it gives us some hard guarantees.",
            "And that's probably approximately correct IRL, so the idea is to say, well, let's think about sort of the number of mistakes the algorithm might make.",
            "And here we're quantifying mistakes by the number of times we might take an action whose value is not close to optimal.",
            "And what we would like to say is that an algorithm is pack if the number of mistakes it makes is a polynomial function of the MDP parameters.",
            "So the MDP parameters are things like the size of the state space, the size of the action space and sort of the range of the rewards, and so this could be large.",
            "This could be a large number of mistakes that were making, but it still bounded, so we're not going to have asymptotic guarantees.",
            "We can say you know we're only going to make a bounded number of mistakes.",
            "We're not going to say how badly we're going to do on those, but eventually we're going to have to be doing pretty well most of the time."
        ],
        [
            "And the cool thing is that there are all these sort of simple and computationally cheap algorithms, many of which use sort of an idea of optimism under uncertainty that are pack.",
            "So they have this nice property that the number of mistakes that they're going to make is only polynomial function of the MDP parameters.",
            "And essentially, this sort of optimism under uncertainty idea is that we take our uncertainty over the model and we act optimistically with it.",
            "We sort of dreamed that were in the best possible world that's allowable given the data that we have, and then that guides us to do more exploration.",
            "And this has been very effective in practice.",
            "So."
        ],
        [
            "How could we do this in the partially observable setting?",
            "Well, what we could do is we could sort of if we could have some way to estimate these palm DP model parameters, an uncertainty over those, then we could act optimistically with respect to that uncertainty.",
            "We could generate more data and we could close the loop.",
            "And our hope of doing this is it might be much more computationally tractable than other Bayesian approaches and allow us to get some more formal guarantees."
        ],
        [
            "So in particular, we wanted to find what our objective was, which is we're defining pack palm DP algorithms and a pack palm DP algorithm is very similar to a pack MDP algorithm, except for the idea is that the number of mistakes we make is going to be a polynomial function of the palm DP instead of an MDP.",
            "Now that sounds like that might be pretty trivial, but in reality all the prior results that we're aware of were at least exponential.",
            "And intuitively, you can think about the fact that we could take a palm DP and we could make the state space, the histories, the sort of sequence of actions and observations, and then that effectively forms an MDP.",
            "But if you do that, that makes the size of the state space grow exponentially with the horizon, and so it means that all the previous guarantees would effectively still be exponential with the problem parameters.",
            "So the key idea we're saying here is that, like you can't be exponential, you have to be polynomial."
        ],
        [
            "So this store knowledge is the first pack palm DP algorithm and the idea is to build and extend on these method of moments approaches.",
            "These new latent variable estimate techniques to try to get formal sample complexity bounds that both integrate the uncertainty of the model and how that propagates into the value function so that we can say that this is the amount of data that we need in order to get a good palm DP model in order to get good palm DP policies so that I can answer my colleague and say, you know, maybe I need 2 million data points.",
            "With students, in order to get good results."
        ],
        [
            "Now, in practice method of moments have some really lovely guarantees, but they're not the most sample efficient.",
            "So in reality I'm not arguing that yet.",
            "We should use these, but certainly a lot of colleagues that are making these better and better.",
            "But in practice things like expectation maximization often give you better estimates.",
            "So since we were also interested in this general idea of whether optimism under uncertainty could help us, we also made sort of a practical algorithm where we computed pseudo confidence intervals and propagated that uncertainty when we're computing a value function.",
            "And this will."
        ],
        [
            "A couple of interesting things to us essentially about how optimistic is good, how optimism optimistic should we be when we're acting?",
            "So this is a really simple domain.",
            "It's called two room.",
            "We made it up.",
            "There's just two states in one room.",
            "You can get a large amount of reward and then the other one.",
            "You can't, but it's hard to go from one room to the other, and because it's partially observable, you're not quite sure which room you're in.",
            "And in this sort of world, an agent has to be persistent.",
            "The agent has to try really hard to explore because it takes a long time to try to get to the other room, and so if it's reviews is a small amount of data, it will tend to learn that it's never worthwhile to try to transition.",
            "It'll just stay there.",
            "So what I'm showing here is a couple of Bayesian approaches that sort of vary in how optimistic they are.",
            "Ours is in the middle, and what you can see here is that our algorithm can learn well, and it can find the optimal policy.",
            "But one algorithm, BBD one, which is based on some work from Zico culture, extended to the Palm DP case.",
            "It is extremely optimistic, and it does very well, and then another one that's a little bit more tempered doesn't do as well, but if."
        ],
        [
            "Think about another domain like Tiger.",
            "I think this shows us the price of false optimism.",
            "So for those of you that don't know, Tiger is another very small sort of classic domain where you can open one of two doors and one of them has a Tiger behind it, and then there's reward behind the other and you can listen to see where the Tiger might be once you open a door, the world resets and you start again.",
            "So in this domain, if you are too optimistic then you open a lot of Tigers.",
            "So in this case it's really painful to sort of open too many bad doors, and so being overly optimistic, which is the BBB algorithm tends to do very poorly.",
            "Now I think this really raises this interesting issue of how optimistic should we be when we're acting."
        ],
        [
            "And in particular, if you think about there being sort of two forms of uncertainty, there's state uncertainty, and there's model uncertainty.",
            "So we know from the palm DP literature that if you assume all the model uncertain, well, even without that, if you assume all the model uncertainty gets resolved on the next step and then you act optimally given that new model, then that has to be an upper bound.",
            "That's what this first line here says.",
            "So if you say, well, I don't know what model I'm in now, but on the next step all my model uncertainty will get resolved.",
            "That's probably an upper bound, but even more optimistic than that is to say, well, not only is it going to get resolved, but actually just get to pick which model I'm in, so I get to imagine that you know the Tiger is definitely behind door 2, and so I get to open door.",
            "Why?",
            "Then I'll get lots of reward, so that is definitely a higher upper bound than this one.",
            "And the interesting thing is that in planning we almost always want tight bounds, but in learning it's not clear that's the case, and that's what the examples I just showed you indicate.",
            "And so I think there's some really interesting questions now about if we think that this sort of optimism under uncertainty works for partially observable domains, how should we best temper this in a sort of computationally efficient way so that we can start to get good empirical performance as part of this, we're collaborating with some colleagues over in civil engineering where they think about this for windmill maintenance, so they think about modeling a windmill, which is a partially observable system, and thinking about when to do maintenance or when to do repairs.",
            "This setting it's really interesting to think about how optimistic or pessimistic should one be."
        ],
        [
            "OK, so I just want to.",
            "I'm not going to go through this part in detail, but these type of ideas also apply to multitask and transfer learning."
        ],
        [
            "And in the multitask transfer learning setting, we've been thinking about the fact that you often want to do a series of sequential decision making tasks you want to teach a series of students, for example.",
            "But that if we want to model all."
        ],
        [
            "Task is being totally different then we're not going to share any information, so we've been thinking about a common model, which is to assume that there's sort of a finite number of tasks underneath, and it turns out that there's a lot of examples of this where we do, sort of like intent, recognition, or sort of user modeling where this is a reasonable approximation."
        ],
        [
            "And so we've been thinking about how we can extend the ideas of these method of moments approaches in order for us to learn these type of models and then leverage them to get better performance.",
            "So we're doing this for things like news personalization so we can try to quickly identify what type of news that you like, what type of news reader you are."
        ],
        [
            "Also, thinking about this more broadly for the underlying theoretical ideas.",
            "So I student Daniel Gale will be talking about some other work for concurrent transfer tomorrow and then my colleague Lihong Li will talk about our transfer work tomorrow."
        ],
        [
            "Alright, so I just want to spend the rest of the time talking about how we can use the past to fake the future.",
            "I think that again, if you're interested like I am in how we can sort of best optimize the decisions we can make using the limited amount of data.",
            "I think this is really important.",
            "So what do I mean?"
        ],
        [
            "By that I mean that in many, many cases, now we're starting to have data about the series of decisions and their outcomes, so we're starting to generate all this data from intelligent tutoring systems.",
            "We have electronic medical record systems.",
            "We have tons of data about how marketing on the web, etc.",
            "And we want to be able to leverage that in order to make better policies going forward.",
            "So what does this mean?",
            "So you could think of is just mean trajectories.",
            "So let's imagine the students in some state, and then we give them an activity and then based on that state we ask them to interact with the person.",
            "And then we ask them to read something and then we ask them to interact with the person again and we observe their post test score.",
            "And we can do this many times with different types of policies or even the same type of policies with different individuals.",
            "And the question is, how can we use this data that we have about decisions that were made in the past in order to make better decisions going forward?"
        ],
        [
            "So one situation is the offline stationary policy evaluation case.",
            "So we have some particular policy in this case that we want to evaluate.",
            "We want to say if the student is sleepy, give them an activity.",
            "If the student is alert, make them read.",
            "And we want to know how well that means they will do on their post test at the end of the semester.",
            "Now this is something that I thought about a lot and there's a number of people that have thought about this a lot."
        ],
        [
            "And we want to be able to sort of generally compare different fixed policies on this offline data.",
            "But I think that one of the really most powerful things about machine learning and reinforcement learning is that they are not static, that they can change in response to data and that they can learn overtime."
        ],
        [
            "And So what I think we really want to be doing in these cases is sort of offline nonstationary policy evaluation, which I think we've briefly alerted to alluded to just now.",
            "Instead of learning to learn.",
            "In the previous talk.",
            "So in this situation, we don't have a fixed policy.",
            "We have an algorithm and what we want to do is to be able to simulate how that algorithm would do if we were going to run in the future.",
            "Now, these algorithms in general are going to be adaptive, so it's a non stationary policy."
        ],
        [
            "So one natural question that you might think about is like, well, why not just build a simulator so you have all this data.",
            "You could just estimate some model from that data and then you could use your use that as a simulator to evaluate your algorithm.",
            "Unfortunately, there's a couple of problems with simulators, particularly when we think about people.",
            "So I think in robotics often we can get some really great simulators, and there's some very nice work that supports that by Peter Stone and others.",
            "But in when we start to think about user modeling, it becomes very hard.",
            "So the first thing is that it often requires an enormous amount of data in order to build a good simulator, and we often just don't have access to that levels of data, and particularly sort of the randomization that we might want.",
            "The second is that if we use our model and pretend that it's perfect, that we can often get this issue that David Silver was talking about yesterday, which is that our errors compound overtime.",
            "Yesterday he showed a beautiful Atari simulator, but yet it wasn't good enough.",
            "In order to do control.",
            "The same thing happens here, and in fact there is very nice work from Drew Bagnell and his student Stephen Ross, showing that these errors can compound quadratically overtime.",
            "So I think that's a really good reason to think about how we could maybe direct."
        ],
        [
            "We use data instead to evaluate algorithms.",
            "Instead of building a simulator."
        ],
        [
            "Alright, so I'm going to talk about sort of one way.",
            "We did this for multi arm bandits and then briefly talk about some work we're doing in reinforcement learning.",
            "And I will highlight the work that we're doing is really sort of standing on top of some really lovely work by Lihong Li and John Langford on doing these ideas for contextual multi armed bandits.",
            "So let's think about doing non stationary policy evaluation for a multi arm bandit where we had sort of a series of arms.",
            "We pulled an outcomes."
        ],
        [
            "So what we could do is we could rearrange this data into cues which essentially just said when we took this action.",
            "This is what the outcomes were.",
            "We took this action.",
            "This is what the rewards where we received.",
            "And now what we want to do is to use this to evaluate how well a multi arm bandit might work in the future."
        ],
        [
            "OK."
        ],
        [
            "So let's say we have some algorithm.",
            "First.",
            "What it does, it wants to choose ARM H2 so we can look into our Q and we can see if we have any data for pulling our May 2 we do and we got a reward of .7 four.",
            "So we take that and we provide it to our algorithm or algorithm, then does whatever it does.",
            "Maybe it updates sort of confidence intervals, maybe updates estimates, and then it picks another arm."
        ],
        [
            "So this time it Pixar one we again go to the Q and then we can just sample one of the outcomes.",
            "Whenever we sample an outcome from the queue, we have to remove this."
        ],
        [
            "And we repeat this."
        ],
        [
            "Until we run out of."
        ],
        [
            "Yeah."
        ],
        [
            "OK, so now what?"
        ],
        [
            "Done as we've simulated a run of this algorithm.",
            "And what this gave us is it gave us a simulated sort of sum of rewards we could get from running this algorithm in the real world.",
            "Well, you will have to sort of look at.",
            "You have to ask me to check that this is actually an unbiased estimator, but we can prove in certain cases that this is an unbiased estimator on some different under some different conditions.",
            "So this gives US1 sort of run of the algorithm, and then you can imagine using this to compare different algorithms."
        ],
        [
            "So we could.",
            "This is 1 algorithm and this gives US1 estimate over 5 steps.",
            "But we could have run other algorithms and then we could compare different algorithms using this data set."
        ],
        [
            "So we did this for a game called Treefrog Treasure.",
            "I think it's been played with for at least 50,000 students so far.",
            "The fractions game and the idea is that the student has to figure out we're on a number line to place a fraction.",
            "And there's a number of different parameters for these."
        ],
        [
            "Games that are often nontrivial to figure out how to set in advance, so there's things like do we give them additional motivation?",
            "And do we have the music on or off?",
            "How do we represent the fraction, etc.",
            "And so we can treat all of these as being parameters that end up describing one arm of a multi armed bandit."
        ],
        [
            "So we have this existing data set and we wanted to be able to try running different algorithms on it and we were curious about how well our Q based evaluator compared to a previous approach that was proposed for contextual multi armed bandits by Liang Lee and John Langford and the others.",
            "And what you can see here is on the Y axis is the average episode length.",
            "Essentially that means how many steps could we evaluate our algorithm on until we ran out of data that it was OK to keep evaluating?",
            "And what you can see is that our approach, which is SDB about, is generally doing much better than the rejection sampling approach from before.",
            "And what that means is that we're better able to use our existing data in order to evaluate algorithms.",
            "In general.",
            "We want as much data efficiency as possible.",
            "If we can only use one data point from our private previous data set, we're not going to get a very good estimate of our algorithm.",
            "And so we want more.",
            "We want to be able to leverage as much as our old data is possible to inform which algorithm we should use going forward."
        ],
        [
            "It also allows us to compare different algorithms.",
            "In this case, we were looking at predictive.",
            "We're looking at predictive state reinforcement learning as well as some deterministic algorithms, and allowed us to show that there's some really nice benefits of using the sort of post posterior sampling approach.",
            "Right, that's what I should do."
        ],
        [
            "For that one.",
            "So more recently we've been looking at non stationary valuation for reinforcement learning, not just bandits in particular.",
            "In this case we've been comparing a number of different offline evaluators we could use based on data."
        ],
        [
            "So one again, in this situation we're going to have sort of trajectories of states, actions and rewards, and we're going to be wanting to use that to do our L algorithm evaluation.",
            "And to our knowledge, is sort of the first of these type of forms of evaluators that exist.",
            "These sort of directly data driven approaches."
        ],
        [
            "So one idea is to sort of take build on the queue based approach where now we can have cues that instead of her based on just states, there's based on state action pairs and then we can have next couples which tell us what was the reward you would have gotten as well as the next state."
        ],
        [
            "And so in this case we can do something very similar to what we had before, which is, we have an algorithm that starts in some state.",
            "It's like some action we go into our Q.",
            "We see if we have any outcomes from doing that."
        ],
        [
            "And then we pull from that queue."
        ],
        [
            "We update our algorithm and we repeat."
        ],
        [
            "Now, another approach that we looked at involves per state rejection sampling.",
            "So in this case is going to reorganize our data a little bit into sort of streams of outcomes for each state, where we say for this state, this is the action we took.",
            "The reward we got in the next state.",
            "And this requires us to have knowledge of what the sampling distribution was."
        ],
        [
            "And so the way this works in this case is that our algorithm is always going to have some current behavior policy."
        ],
        [
            "And if we're it's upstate, we're going to pop a sample from the queue.",
            "So we're going to say, well, here's an action you could have taken.",
            "This is the reward you would have gotten in the outcome."
        ],
        [
            "And then we can sample a number uniformly and we can see whether or not that number is smaller than a particular ratio, which is the ratio of the probability that we would have selected that action given the policy we're trying to evaluate, divided by the probability we would have selected that action under our sampling distribution, normalized.",
            "And this is exactly what rejection sampling does, and so it allows us to truly draw samples from our actual behavior policy.",
            "There's another variant of this that we can also consider that involves actually trying to do this whole thing over episodes."
        ],
        [
            "So again, we wanted to look at this for Treefrog treasure, but in this case we wanted to think about sort of the full RL problem, which is, you know what sequence of activities and fractions should we be giving to the student in order to maximize learning and engagement.",
            "And what we?"
        ],
        [
            "Wanted to be doing in this case is really to compare between different evaluators because what we would love to have is to be able to say for practitioners look if you've got some data set.",
            "This is the evaluator you should run so you can figure out which algorithm to deploy going forward.",
            "And there are a lot of different properties we would like in these types of evaluators.",
            "We would like them to be data efficient so that can use all of our old data, but we'd also like them to be able to give us unbiased estimates.",
            "There's also this really interesting thing called revealed randomness, so in many PRL algorithms they involve taking stochastic policies.",
            "Some of them do that explicitly.",
            "In some of them do it implicitly.",
            "Turns out that if you can represent that uncertainty explicitly, that can give you a lot more sample efficiency.",
            "Now you might ask if an algorithm has some randomness, why would it hide it?",
            "Well, it turns out in things like Bayesian approaches.",
            "It might be really easy to sample from your posterior, but it might be computationally expensive to fully represent the posterior.",
            "So that's the situation where your algorithm might be random, but you only get to see a deterministic example.",
            "So why might that matter?",
            "Well so."
        ],
        [
            "Here's a situation where we have no revealed randomness.",
            "OK, we're treating our algorithm like it's always selecting a deterministic policy.",
            "On the X axis here is showing sort of how many episodes we could evaluate our algorithms for on the Y axis is showing this percentage for friends, so in this Case No matter which evaluator we picked, almost all of them could only value about 0 to 500 runs in this situation."
        ],
        [
            "But if we could use a different evaluator that can leverage revealed randomness, we could use the data that we had to evaluate our algorithms.",
            "For many many, many more time steps.",
            "So up to around 2000 episodes instead.",
            "So this just shows that if you are actually executing another stochastic policy that some of these evaluators can make much better use of that information and give you much better estimates of how well the algorithm would do."
        ],
        [
            "Alright, so my student, there's lots of details more to talk about that and my student has a poster tonight.",
            "Travis Mendelsohn encourage you to come by and look at it.",
            "But I'm sure that everybody is dying for the coffee break.",
            "So let me just wrap."
        ],
        [
            "So I talked today about sort of optimistic learning control in partially observable domains, as well as offline policy evaluation."
        ],
        [
            "And I think that really these are part of our broader goal of trying to create these systems that really enhance human capacity and well being, and that trying to create these faster learning agents is an important tool to trying to get there, thanks.",
            "So you said that this estimator is unbiased under certain conditions, but what I'm really interested in for off policy evaluation is with a finite sample.",
            "How close is my evaluation going to be to its performance in the real environment?",
            "Yeah, so for I think everyone heard that.",
            "But the question is if you have a finite amount of data, how close is your estimate to its true performance?",
            "I don't think that we have those yet.",
            "The only ones I guarantees I know it for that is if you were to have a static policy or evaluating.",
            "So Phil Thomas has some really nice work on that as well As for the contextual bandit case Lihong Li does, but I think that's still an open question of how can we actually get finite sample guarantees saying we know you'll be within this when you execute.",
            "Thank you so much for your talk.",
            "I want to ask about.",
            "The pack complexity part the pack algorithms because I'm concerned that that that criteria the criteria is going to be 2 two week to really be helpful.",
            "You know we get polynomial in the number of states, but typically the number of states will be exponential in the number of state variables like the number of pixels in our cameras and stuff.",
            "Yeah, so I think that's a great question.",
            "I think in general we need represent we need models that can scale up to much larger state spaces.",
            "I think in general the pack guarantees are already way to lose even for discrete state and action spaces.",
            "There's something on the order of 10 to the nine samples that we need, but I think that in practice they often work pretty well when we sort of tune that parameter.",
            "I also think that there are ways that we can leverage the factored representations.",
            "We've done some work on that and others so you can get much smaller sample complexity bounds.",
            "OK, thank you again Emma."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, I'm really happy to be here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk to you today about some of the work we're doing in my lab to quickly learn to make good decisions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I'm going to start off and talk about a motivating example for me, which is the matrix who here is seen the matrix?",
                    "label": 0
                },
                {
                    "sent": "OK, well there's no matter who here has not seen the matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, understandable, so go see it.",
                    "label": 0
                },
                {
                    "sent": "It's a good film.",
                    "label": 0
                },
                {
                    "sent": "Inside of it there is a character called Trinity, and at one point Trinity has to learn to fly a helicopter.",
                    "label": 0
                },
                {
                    "sent": "So what she does is she downloads a program into her brain and approximately 5 seconds later she can fly a helicopter and help go save the world.",
                    "label": 0
                },
                {
                    "sent": "I don't believe we're going to accomplish this over the next five years, but I do think that this is a really exciting vision to think about how we could have systems like this, which you could think of is sort of a really accelerated intelligent tutoring system.",
                    "label": 0
                },
                {
                    "sent": "Try to enhance human capacity and well being.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And because not surprising given the talk of the conference, I think we can do this using reinforcement learning, or at least that reinforcement learning can be a really important tool in trying to accomplish this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think this because the whole idea behind reinforcement learning of thinking about gathering data, trying actions, seeing how they work, refining our models and refining our algorithms is a really powerful one.",
                    "label": 0
                },
                {
                    "sent": "It's sort of this general idea of how do we have an evidence based life, and I think that we can use these ideas to try to create these systems to really enhance human capacity.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And well being.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The stuff that I tend to think about is particularly motivated by problems in education, but more broadly we think about sort of having these reinforcement learning agents interact with people, so to create things like intelligent tutoring systems or medical decision support systems, etc.",
                    "label": 0
                },
                {
                    "sent": "And I think when we start to think about reinforcement learning agents that interact with people, it focuses our attention on a particular set of sort of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning challenges that overlap but are a little bit different than some of the ones that we think about in the context of robotics.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is because when I think about gathering data, every single data point is a person and I think that that raises a couple important things.",
                    "label": 0
                },
                {
                    "sent": "The first is that if we make mistakes that affected someone, now, that doesn't mean necessarily that was catastrophic.",
                    "label": 0
                },
                {
                    "sent": "Maybe it took us an hour longer to teach someone fractions than it should have, but it doesn't mean that we sort of lost an opportunity.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that if it takes us 7 billion people to learn how to teach fractions, not only did we waste a lot of people's time, or you know we didn't enhance their capacity as much as we could have, but we only have 1 billion people left to try to use the results of our wonderful reinforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "We don't have infinite people, unlike sort of in the case of educational games or through the game Atari that we saw yesterday.",
                    "label": 0
                },
                {
                    "sent": "We can't do infinite numbers of exploration, and so I think that that raises some interesting challenges and constraints, which means that we need to be careful about data.",
                    "label": 0
                },
                {
                    "sent": "And we need to create faster learning reinforcement learning agents that can best leverage data as fast as possible in order to make good decisions.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today we're going to talk about is a couple of particular efforts in my lab around this, and this is kind of bleeding edge research.",
                    "label": 0
                },
                {
                    "sent": "I thought it would be fun to talk about some really hot off the presses, stuff that we're doing, and so I'm excited to talk to you guys afterwards about these ideas.",
                    "label": 0
                },
                {
                    "sent": "And the first thing I'm going to talk about is sort of optimistic learning and control and partially observable domains.",
                    "label": 0
                },
                {
                    "sent": "But before I do that, I'm going to do sort of a really brief overview on Markov decision processes in background, and I know that a number of you guys are experts in this and some of others.",
                    "label": 0
                },
                {
                    "sent": "If you probably saw Michael's talk yesterday.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is just going to make sure that everyone's got us out the same vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So a Markov decision process can be described as a tuple where there is sort of a set of states.",
                    "label": 0
                },
                {
                    "sent": "You could think in this example that the state of the ship is its GPS location.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it has access to a set of actions and that those actions in general can have stochastic outcomes.",
                    "label": 0
                },
                {
                    "sent": "You have some probability distribution over the next state.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that we get some reward.",
                    "label": 0
                },
                {
                    "sent": "Maybe there's some spices in the world when there are Pirates, other places, and so we can have a reward associated with states or States and actions, or state actions and next dates.",
                    "label": 0
                },
                {
                    "sent": "And in some cases we also have a discount factor that describes how much we care about immediate reward versus future reward.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now in these situations, instead of just thinking about trajectories of actions, we need to think about policies and mapping of states to actions and the way we value.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, the goodness of a policy is to think about what the expected summer future rewards it.",
                    "label": 0
                },
                {
                    "sent": "So we would like to compute the optimal policy that maximizes this expected summer feature rewards given the stochastic city in the environment.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, when reinforcement learning, we often don't know what the dynamics are or the reward function in advance.",
                    "label": 0
                },
                {
                    "sent": "We essentially don't know how the world works because of that.",
                    "label": 0
                },
                {
                    "sent": "This makes the problem of figuring out the optimal policy a lot more challenging.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to talk a couple, but it's sort of some variants of this situation.",
                    "label": 0
                },
                {
                    "sent": "One is multi armed bandits which are very important problem in their own right, but you can loosely think about as being essentially RL without state.",
                    "label": 0
                },
                {
                    "sent": "So now we just have a set of actions we can take and we can look at some of the expected reward of taking different actions.",
                    "label": 0
                },
                {
                    "sent": "And again we want to maximize our expected summer future rewards.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can also think about partially observable Markov decision processes, where now we've lost our GPS.",
                    "label": 0
                },
                {
                    "sent": "But we can sort of navigate from where the stars are and what the wind is like and use those sort of observations to try to give us a sense of what the underlying hidden state is that we don't have direct access to.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in the context when we're trying to think about reinforcement learning, there's this really fundamental tradeoff that I think is fascinating, which is the exploration exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "And exploration you can think about this at least in one way as like trying to understand how the world works.",
                    "label": 0
                },
                {
                    "sent": "So in our ship example, we could move all over the world and try to understand the currents and try to learn where there are spices.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But eventually our baskets, frustrated with us and says, alright, I'm glad you've explored the entire world and you can give me a complete map of it if we're all the spices are, but I need you to actually start bringing spices back.",
                    "label": 0
                },
                {
                    "sent": "And so in this situation, what we have to do is exploit.",
                    "label": 0
                },
                {
                    "sent": "We have to leverage our information about the world in order to make good decisions and try to achieve high reward.",
                    "label": 0
                },
                {
                    "sent": "And this fundamental tradeoff is a really hard one, and there's been a lot of really exciting work to think about.",
                    "label": 0
                },
                {
                    "sent": "What's the best way to do this exploration exploitation tradeoff?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'm going to sort of now that everybody is up speed.",
                    "label": 0
                },
                {
                    "sent": "I'm going to sort of talk about how we might be able to do, sort of optimistic learning and control when the domain is partially observable.",
                    "label": 0
                },
                {
                    "sent": "And I.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st going to start with why I started thinking about this problem so when I was doing my PhD I thought a lot about planning and partially observable Markov decision processes and the planning problem is to say you're given a specific model, so I know how the world works.",
                    "label": 0
                },
                {
                    "sent": "I know what my observation model is and I want to compute a policy.",
                    "label": 0
                },
                {
                    "sent": "I want to compute a policy that maximizes my expected summer future rewards.",
                    "label": 0
                },
                {
                    "sent": "And so I thought a lot about that, but I was starting to get interested in education and so then when I got to Carnegie Mellon, I was writing a grant with a colleague of mine and we're really excited about bringing these ideas to tutoring systems.",
                    "label": 0
                },
                {
                    "sent": "So this is an example of one of the tutors that we have right now and just tried to teach students fractions.",
                    "label": 0
                },
                {
                    "sent": "And so the idea of our grant was to say, well, what activities should we give?",
                    "label": 0
                },
                {
                    "sent": "Went to the student?",
                    "label": 0
                },
                {
                    "sent": "When should we do things like videos?",
                    "label": 0
                },
                {
                    "sent": "When should we do things like, you know, learning activities?",
                    "label": 0
                },
                {
                    "sent": "When should we get them to self reflect?",
                    "label": 0
                },
                {
                    "sent": "And I said, that's great.",
                    "label": 0
                },
                {
                    "sent": "We can model.",
                    "label": 0
                },
                {
                    "sent": "This is a palm DP.",
                    "label": 0
                },
                {
                    "sent": "Will learn a great plan and you know, will optimize student learning.",
                    "label": 0
                },
                {
                    "sent": "And so my colleague asked me a very reasonable question.",
                    "label": 0
                },
                {
                    "sent": "He's a learning scientist.",
                    "label": 0
                },
                {
                    "sent": "He's not a machine learning guy and he said, alright, great.",
                    "label": 0
                },
                {
                    "sent": "How much data do you need?",
                    "label": 0
                },
                {
                    "sent": "How much data do you need to learn a good palm DP model that then you can optimize and try to get better learning.",
                    "label": 0
                },
                {
                    "sent": "And I thought back out of the methods that I knew from grad school and like expectation maximization, and I was like ha, I don't know.",
                    "label": 0
                },
                {
                    "sent": "And that seems like a really basic question in this sort of experimental Sciences, it's very common to do things like power analysis to say how much data do you need so that you can actually confirm or disprove your hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And it seemed really funny to me that I couldn't say how much data that I would need to learn good enough models so that we could actually create Palm DP's for this example, But it turns out that sort of standard techniques like expectation maximization only give you local Optima and they don't give you any guarantees on the performance of the Palace of the parameters you get out.",
                    "label": 0
                },
                {
                    "sent": "You just don't know how well you know them.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, why is this important?",
                    "label": 0
                },
                {
                    "sent": "Why should we model model uncertainty?",
                    "label": 0
                },
                {
                    "sent": "Well, I think there's at least two reasons why modeling model uncertainty is important.",
                    "label": 0
                },
                {
                    "sent": "The first is is that if we understand how good our models are, that can help us understand how good our policy will be, because there are well known techniques to take uncertainty over the model and propagate that into uncertainty over the value function or uncertainty over the policy.",
                    "label": 0
                },
                {
                    "sent": "So that's one really important reason.",
                    "label": 0
                },
                {
                    "sent": "The other important reason is that it can actually help us learn.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of great work in the fully observable case of saying, given uncertainty over how we think the world works, but we can use that to try to balance the exploration exploitation tradeoff.",
                    "label": 1
                },
                {
                    "sent": "So we wanted to be able to do this in sort of partially observable domains.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, excitingly over the last like sort of five to 10 years, there's been a lot of excitement over predictive state representations and spectral learning and method of moments, and these new forms to sort of do.",
                    "label": 0
                },
                {
                    "sent": "Clustering and predictive models and dynamical systems, and there's been a lot of great work by people throughout the machine learning community, and a number of people here.",
                    "label": 0
                },
                {
                    "sent": "People like Michael Littman's tender seeing Byron Boots, Jeff Gordon, Joelle Pineau.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of people who've been starting to think about these methods and how we can start to propagate them into control.",
                    "label": 0
                },
                {
                    "sent": "But I was particularly excited by one sort of subclass of these type of methods.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's called method of moments.",
                    "label": 0
                },
                {
                    "sent": "So raise your hand if you learn about method of moments in grad school.",
                    "label": 0
                },
                {
                    "sent": "So a couple people, so I certainly didn't, and I think a lot of people still don't.",
                    "label": 0
                },
                {
                    "sent": "So method of moments is a way to do latent variable estimation, and it's actually been around for a really long time.",
                    "label": 0
                },
                {
                    "sent": "It's been around for at least 100 years and the essential idea is that if you think about a distribution, we can describe it by its moments.",
                    "label": 0
                },
                {
                    "sent": "We can describe it by its meaning and its variants and its kurtosis and its Q.",
                    "label": 0
                },
                {
                    "sent": "And then if you could write out all the infinite number of moments of a distribution, you would completely characterize it.",
                    "label": 0
                },
                {
                    "sent": "So that we can use that for estimation if we have a whole bunch of data and then we could find some parameters that match all the moments of that data.",
                    "label": 0
                },
                {
                    "sent": "But if we had enough data, we would be confident that we found the distribution that describes the generating distribution.",
                    "label": 0
                },
                {
                    "sent": "So that's the essential idea of method of moments is that we can try to match the moments inside of the data.",
                    "label": 0
                },
                {
                    "sent": "So within itself, that's sort of a nice idea, but the more exciting thing to me is that we can use.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It to get formal bounds on how good those parameters are, so work that was led by people like Sham, Kakade, Ann, Daniel Hsu and other people has been really making some pioneering work on how can we use method of moments to get finite sample guarantees for latent variable models.",
                    "label": 0
                },
                {
                    "sent": "So what this means is we can take data and we can do things like tensor decomposition and then we can get out parameters with uncertainties.",
                    "label": 1
                },
                {
                    "sent": "We can say hey for this mixture model this is how well we know the means or for this hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "This is how well we know the observation models and how well we know the transition models.",
                    "label": 0
                },
                {
                    "sent": "So we can start to get this model of model uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so to me that was super exciting, because that means that we could then take those models which we now have uncertainty over and then do a control algorithm that actually considers that model uncertainty.",
                    "label": 0
                },
                {
                    "sent": "And then we would generate more data and then we could close this loop.",
                    "label": 0
                },
                {
                    "sent": "And I think this is important for a couple of reasons.",
                    "label": 0
                },
                {
                    "sent": "The biggest one is that it could allow us to start to tackle these partially observable domains and use this type of idea to try to guide exploration exploitation.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just going to talk about two particular examples from our lab where we've been doing this, but it's definitely a general big effort, so the first is for partially observable RL and so in partially observable RL.",
                    "label": 0
                },
                {
                    "sent": "Remember, we're going to be taking actions with the environment in the environment is going to give us back observations.",
                    "label": 0
                },
                {
                    "sent": "And then in the paradigm I provoked proposing, we're going to build a model, and then we're going to be using that model to generate control, and then this loop will be closed.",
                    "label": 0
                },
                {
                    "sent": "And this was the.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ample where I was inspired by, so we sort of have an intelligent tutoring system which is our agents and it's making actions.",
                    "label": 0
                },
                {
                    "sent": "It's sort of giving problems and other activities to the student, and the student is responding and you can see how long they watched the video or whether they got a problem correct and we can use that data to build up these models.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out that partially bzero Laurel is a really hard problem.",
                    "label": 0
                },
                {
                    "sent": "For those of you that learned about Palm, DP's in grad schools, those are already considered pretty intractable.",
                    "label": 0
                },
                {
                    "sent": "And then now we don't even know the parameters.",
                    "label": 0
                },
                {
                    "sent": "But there's been some really nice Bayesian work on trying to tackle this problem, and what the Bayesian approach does essentially is it says alright, we've got uncertainty over the state.",
                    "label": 0
                },
                {
                    "sent": "We've got uncertainty over the model.",
                    "label": 1
                },
                {
                    "sent": "We could try to exactly optimize the expected reward, given that uncertainty kind of makes this mega continuous.",
                    "label": 0
                },
                {
                    "sent": "State palm DP.",
                    "label": 0
                },
                {
                    "sent": "Now I love the Bayesian ideal.",
                    "label": 0
                },
                {
                    "sent": "I think that if we could accomplish this, this would exactly optimize the exploration exploitation tradeoff.",
                    "label": 0
                },
                {
                    "sent": "So it's really, really elegant.",
                    "label": 0
                },
                {
                    "sent": "The problem is, it's almost completely computationally intractable.",
                    "label": 0
                },
                {
                    "sent": "While there's been some really nice empirical demonstrations of this, it means because we have to reduce our computation so much to make it or reduce the problem.",
                    "label": 0
                },
                {
                    "sent": "So much to make it computationally tractable.",
                    "label": 1
                },
                {
                    "sent": "It typically means that we end up with no guarantees on the performance.",
                    "label": 0
                },
                {
                    "sent": "OK, so how could we maybe address this?",
                    "label": 0
                },
                {
                    "sent": "Well, let's think about.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fully observable MDP's this problem, intractability is not inherent just to partially observable settings.",
                    "label": 0
                },
                {
                    "sent": "So in the fully observable case we would also like to maximize expected reward optimally from the get go, but that's often really hard and so one thing that came out starting for the late 90s till now, which I think was really exciting, is that we could think about sort of an alternative criteria that's a little bit looser, but is much more computationally tractable to compute, and still how it gives us some hard guarantees.",
                    "label": 0
                },
                {
                    "sent": "And that's probably approximately correct IRL, so the idea is to say, well, let's think about sort of the number of mistakes the algorithm might make.",
                    "label": 0
                },
                {
                    "sent": "And here we're quantifying mistakes by the number of times we might take an action whose value is not close to optimal.",
                    "label": 0
                },
                {
                    "sent": "And what we would like to say is that an algorithm is pack if the number of mistakes it makes is a polynomial function of the MDP parameters.",
                    "label": 0
                },
                {
                    "sent": "So the MDP parameters are things like the size of the state space, the size of the action space and sort of the range of the rewards, and so this could be large.",
                    "label": 0
                },
                {
                    "sent": "This could be a large number of mistakes that were making, but it still bounded, so we're not going to have asymptotic guarantees.",
                    "label": 0
                },
                {
                    "sent": "We can say you know we're only going to make a bounded number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "We're not going to say how badly we're going to do on those, but eventually we're going to have to be doing pretty well most of the time.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the cool thing is that there are all these sort of simple and computationally cheap algorithms, many of which use sort of an idea of optimism under uncertainty that are pack.",
                    "label": 0
                },
                {
                    "sent": "So they have this nice property that the number of mistakes that they're going to make is only polynomial function of the MDP parameters.",
                    "label": 0
                },
                {
                    "sent": "And essentially, this sort of optimism under uncertainty idea is that we take our uncertainty over the model and we act optimistically with it.",
                    "label": 1
                },
                {
                    "sent": "We sort of dreamed that were in the best possible world that's allowable given the data that we have, and then that guides us to do more exploration.",
                    "label": 0
                },
                {
                    "sent": "And this has been very effective in practice.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How could we do this in the partially observable setting?",
                    "label": 0
                },
                {
                    "sent": "Well, what we could do is we could sort of if we could have some way to estimate these palm DP model parameters, an uncertainty over those, then we could act optimistically with respect to that uncertainty.",
                    "label": 1
                },
                {
                    "sent": "We could generate more data and we could close the loop.",
                    "label": 0
                },
                {
                    "sent": "And our hope of doing this is it might be much more computationally tractable than other Bayesian approaches and allow us to get some more formal guarantees.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, we wanted to find what our objective was, which is we're defining pack palm DP algorithms and a pack palm DP algorithm is very similar to a pack MDP algorithm, except for the idea is that the number of mistakes we make is going to be a polynomial function of the palm DP instead of an MDP.",
                    "label": 0
                },
                {
                    "sent": "Now that sounds like that might be pretty trivial, but in reality all the prior results that we're aware of were at least exponential.",
                    "label": 0
                },
                {
                    "sent": "And intuitively, you can think about the fact that we could take a palm DP and we could make the state space, the histories, the sort of sequence of actions and observations, and then that effectively forms an MDP.",
                    "label": 0
                },
                {
                    "sent": "But if you do that, that makes the size of the state space grow exponentially with the horizon, and so it means that all the previous guarantees would effectively still be exponential with the problem parameters.",
                    "label": 0
                },
                {
                    "sent": "So the key idea we're saying here is that, like you can't be exponential, you have to be polynomial.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this store knowledge is the first pack palm DP algorithm and the idea is to build and extend on these method of moments approaches.",
                    "label": 1
                },
                {
                    "sent": "These new latent variable estimate techniques to try to get formal sample complexity bounds that both integrate the uncertainty of the model and how that propagates into the value function so that we can say that this is the amount of data that we need in order to get a good palm DP model in order to get good palm DP policies so that I can answer my colleague and say, you know, maybe I need 2 million data points.",
                    "label": 0
                },
                {
                    "sent": "With students, in order to get good results.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in practice method of moments have some really lovely guarantees, but they're not the most sample efficient.",
                    "label": 0
                },
                {
                    "sent": "So in reality I'm not arguing that yet.",
                    "label": 0
                },
                {
                    "sent": "We should use these, but certainly a lot of colleagues that are making these better and better.",
                    "label": 0
                },
                {
                    "sent": "But in practice things like expectation maximization often give you better estimates.",
                    "label": 0
                },
                {
                    "sent": "So since we were also interested in this general idea of whether optimism under uncertainty could help us, we also made sort of a practical algorithm where we computed pseudo confidence intervals and propagated that uncertainty when we're computing a value function.",
                    "label": 0
                },
                {
                    "sent": "And this will.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A couple of interesting things to us essentially about how optimistic is good, how optimism optimistic should we be when we're acting?",
                    "label": 0
                },
                {
                    "sent": "So this is a really simple domain.",
                    "label": 0
                },
                {
                    "sent": "It's called two room.",
                    "label": 0
                },
                {
                    "sent": "We made it up.",
                    "label": 0
                },
                {
                    "sent": "There's just two states in one room.",
                    "label": 0
                },
                {
                    "sent": "You can get a large amount of reward and then the other one.",
                    "label": 0
                },
                {
                    "sent": "You can't, but it's hard to go from one room to the other, and because it's partially observable, you're not quite sure which room you're in.",
                    "label": 0
                },
                {
                    "sent": "And in this sort of world, an agent has to be persistent.",
                    "label": 0
                },
                {
                    "sent": "The agent has to try really hard to explore because it takes a long time to try to get to the other room, and so if it's reviews is a small amount of data, it will tend to learn that it's never worthwhile to try to transition.",
                    "label": 0
                },
                {
                    "sent": "It'll just stay there.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here is a couple of Bayesian approaches that sort of vary in how optimistic they are.",
                    "label": 0
                },
                {
                    "sent": "Ours is in the middle, and what you can see here is that our algorithm can learn well, and it can find the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "But one algorithm, BBD one, which is based on some work from Zico culture, extended to the Palm DP case.",
                    "label": 0
                },
                {
                    "sent": "It is extremely optimistic, and it does very well, and then another one that's a little bit more tempered doesn't do as well, but if.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Think about another domain like Tiger.",
                    "label": 0
                },
                {
                    "sent": "I think this shows us the price of false optimism.",
                    "label": 0
                },
                {
                    "sent": "So for those of you that don't know, Tiger is another very small sort of classic domain where you can open one of two doors and one of them has a Tiger behind it, and then there's reward behind the other and you can listen to see where the Tiger might be once you open a door, the world resets and you start again.",
                    "label": 0
                },
                {
                    "sent": "So in this domain, if you are too optimistic then you open a lot of Tigers.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's really painful to sort of open too many bad doors, and so being overly optimistic, which is the BBB algorithm tends to do very poorly.",
                    "label": 0
                },
                {
                    "sent": "Now I think this really raises this interesting issue of how optimistic should we be when we're acting.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in particular, if you think about there being sort of two forms of uncertainty, there's state uncertainty, and there's model uncertainty.",
                    "label": 0
                },
                {
                    "sent": "So we know from the palm DP literature that if you assume all the model uncertain, well, even without that, if you assume all the model uncertainty gets resolved on the next step and then you act optimally given that new model, then that has to be an upper bound.",
                    "label": 0
                },
                {
                    "sent": "That's what this first line here says.",
                    "label": 0
                },
                {
                    "sent": "So if you say, well, I don't know what model I'm in now, but on the next step all my model uncertainty will get resolved.",
                    "label": 0
                },
                {
                    "sent": "That's probably an upper bound, but even more optimistic than that is to say, well, not only is it going to get resolved, but actually just get to pick which model I'm in, so I get to imagine that you know the Tiger is definitely behind door 2, and so I get to open door.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Then I'll get lots of reward, so that is definitely a higher upper bound than this one.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that in planning we almost always want tight bounds, but in learning it's not clear that's the case, and that's what the examples I just showed you indicate.",
                    "label": 0
                },
                {
                    "sent": "And so I think there's some really interesting questions now about if we think that this sort of optimism under uncertainty works for partially observable domains, how should we best temper this in a sort of computationally efficient way so that we can start to get good empirical performance as part of this, we're collaborating with some colleagues over in civil engineering where they think about this for windmill maintenance, so they think about modeling a windmill, which is a partially observable system, and thinking about when to do maintenance or when to do repairs.",
                    "label": 0
                },
                {
                    "sent": "This setting it's really interesting to think about how optimistic or pessimistic should one be.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I just want to.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go through this part in detail, but these type of ideas also apply to multitask and transfer learning.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in the multitask transfer learning setting, we've been thinking about the fact that you often want to do a series of sequential decision making tasks you want to teach a series of students, for example.",
                    "label": 0
                },
                {
                    "sent": "But that if we want to model all.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Task is being totally different then we're not going to share any information, so we've been thinking about a common model, which is to assume that there's sort of a finite number of tasks underneath, and it turns out that there's a lot of examples of this where we do, sort of like intent, recognition, or sort of user modeling where this is a reasonable approximation.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we've been thinking about how we can extend the ideas of these method of moments approaches in order for us to learn these type of models and then leverage them to get better performance.",
                    "label": 0
                },
                {
                    "sent": "So we're doing this for things like news personalization so we can try to quickly identify what type of news that you like, what type of news reader you are.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, thinking about this more broadly for the underlying theoretical ideas.",
                    "label": 0
                },
                {
                    "sent": "So I student Daniel Gale will be talking about some other work for concurrent transfer tomorrow and then my colleague Lihong Li will talk about our transfer work tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I just want to spend the rest of the time talking about how we can use the past to fake the future.",
                    "label": 0
                },
                {
                    "sent": "I think that again, if you're interested like I am in how we can sort of best optimize the decisions we can make using the limited amount of data.",
                    "label": 0
                },
                {
                    "sent": "I think this is really important.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By that I mean that in many, many cases, now we're starting to have data about the series of decisions and their outcomes, so we're starting to generate all this data from intelligent tutoring systems.",
                    "label": 0
                },
                {
                    "sent": "We have electronic medical record systems.",
                    "label": 0
                },
                {
                    "sent": "We have tons of data about how marketing on the web, etc.",
                    "label": 0
                },
                {
                    "sent": "And we want to be able to leverage that in order to make better policies going forward.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "So you could think of is just mean trajectories.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine the students in some state, and then we give them an activity and then based on that state we ask them to interact with the person.",
                    "label": 0
                },
                {
                    "sent": "And then we ask them to read something and then we ask them to interact with the person again and we observe their post test score.",
                    "label": 0
                },
                {
                    "sent": "And we can do this many times with different types of policies or even the same type of policies with different individuals.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how can we use this data that we have about decisions that were made in the past in order to make better decisions going forward?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one situation is the offline stationary policy evaluation case.",
                    "label": 1
                },
                {
                    "sent": "So we have some particular policy in this case that we want to evaluate.",
                    "label": 0
                },
                {
                    "sent": "We want to say if the student is sleepy, give them an activity.",
                    "label": 0
                },
                {
                    "sent": "If the student is alert, make them read.",
                    "label": 0
                },
                {
                    "sent": "And we want to know how well that means they will do on their post test at the end of the semester.",
                    "label": 0
                },
                {
                    "sent": "Now this is something that I thought about a lot and there's a number of people that have thought about this a lot.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we want to be able to sort of generally compare different fixed policies on this offline data.",
                    "label": 0
                },
                {
                    "sent": "But I think that one of the really most powerful things about machine learning and reinforcement learning is that they are not static, that they can change in response to data and that they can learn overtime.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And So what I think we really want to be doing in these cases is sort of offline nonstationary policy evaluation, which I think we've briefly alerted to alluded to just now.",
                    "label": 1
                },
                {
                    "sent": "Instead of learning to learn.",
                    "label": 0
                },
                {
                    "sent": "In the previous talk.",
                    "label": 0
                },
                {
                    "sent": "So in this situation, we don't have a fixed policy.",
                    "label": 0
                },
                {
                    "sent": "We have an algorithm and what we want to do is to be able to simulate how that algorithm would do if we were going to run in the future.",
                    "label": 0
                },
                {
                    "sent": "Now, these algorithms in general are going to be adaptive, so it's a non stationary policy.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one natural question that you might think about is like, well, why not just build a simulator so you have all this data.",
                    "label": 0
                },
                {
                    "sent": "You could just estimate some model from that data and then you could use your use that as a simulator to evaluate your algorithm.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there's a couple of problems with simulators, particularly when we think about people.",
                    "label": 0
                },
                {
                    "sent": "So I think in robotics often we can get some really great simulators, and there's some very nice work that supports that by Peter Stone and others.",
                    "label": 0
                },
                {
                    "sent": "But in when we start to think about user modeling, it becomes very hard.",
                    "label": 0
                },
                {
                    "sent": "So the first thing is that it often requires an enormous amount of data in order to build a good simulator, and we often just don't have access to that levels of data, and particularly sort of the randomization that we might want.",
                    "label": 0
                },
                {
                    "sent": "The second is that if we use our model and pretend that it's perfect, that we can often get this issue that David Silver was talking about yesterday, which is that our errors compound overtime.",
                    "label": 0
                },
                {
                    "sent": "Yesterday he showed a beautiful Atari simulator, but yet it wasn't good enough.",
                    "label": 0
                },
                {
                    "sent": "In order to do control.",
                    "label": 0
                },
                {
                    "sent": "The same thing happens here, and in fact there is very nice work from Drew Bagnell and his student Stephen Ross, showing that these errors can compound quadratically overtime.",
                    "label": 1
                },
                {
                    "sent": "So I think that's a really good reason to think about how we could maybe direct.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use data instead to evaluate algorithms.",
                    "label": 0
                },
                {
                    "sent": "Instead of building a simulator.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so I'm going to talk about sort of one way.",
                    "label": 0
                },
                {
                    "sent": "We did this for multi arm bandits and then briefly talk about some work we're doing in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And I will highlight the work that we're doing is really sort of standing on top of some really lovely work by Lihong Li and John Langford on doing these ideas for contextual multi armed bandits.",
                    "label": 0
                },
                {
                    "sent": "So let's think about doing non stationary policy evaluation for a multi arm bandit where we had sort of a series of arms.",
                    "label": 0
                },
                {
                    "sent": "We pulled an outcomes.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we could do is we could rearrange this data into cues which essentially just said when we took this action.",
                    "label": 0
                },
                {
                    "sent": "This is what the outcomes were.",
                    "label": 0
                },
                {
                    "sent": "We took this action.",
                    "label": 0
                },
                {
                    "sent": "This is what the rewards where we received.",
                    "label": 0
                },
                {
                    "sent": "And now what we want to do is to use this to evaluate how well a multi arm bandit might work in the future.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say we have some algorithm.",
                    "label": 0
                },
                {
                    "sent": "First.",
                    "label": 0
                },
                {
                    "sent": "What it does, it wants to choose ARM H2 so we can look into our Q and we can see if we have any data for pulling our May 2 we do and we got a reward of .7 four.",
                    "label": 0
                },
                {
                    "sent": "So we take that and we provide it to our algorithm or algorithm, then does whatever it does.",
                    "label": 0
                },
                {
                    "sent": "Maybe it updates sort of confidence intervals, maybe updates estimates, and then it picks another arm.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this time it Pixar one we again go to the Q and then we can just sample one of the outcomes.",
                    "label": 0
                },
                {
                    "sent": "Whenever we sample an outcome from the queue, we have to remove this.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we repeat this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until we run out of.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now what?",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done as we've simulated a run of this algorithm.",
                    "label": 0
                },
                {
                    "sent": "And what this gave us is it gave us a simulated sort of sum of rewards we could get from running this algorithm in the real world.",
                    "label": 0
                },
                {
                    "sent": "Well, you will have to sort of look at.",
                    "label": 0
                },
                {
                    "sent": "You have to ask me to check that this is actually an unbiased estimator, but we can prove in certain cases that this is an unbiased estimator on some different under some different conditions.",
                    "label": 0
                },
                {
                    "sent": "So this gives US1 sort of run of the algorithm, and then you can imagine using this to compare different algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we could.",
                    "label": 0
                },
                {
                    "sent": "This is 1 algorithm and this gives US1 estimate over 5 steps.",
                    "label": 0
                },
                {
                    "sent": "But we could have run other algorithms and then we could compare different algorithms using this data set.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we did this for a game called Treefrog Treasure.",
                    "label": 0
                },
                {
                    "sent": "I think it's been played with for at least 50,000 students so far.",
                    "label": 0
                },
                {
                    "sent": "The fractions game and the idea is that the student has to figure out we're on a number line to place a fraction.",
                    "label": 0
                },
                {
                    "sent": "And there's a number of different parameters for these.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Games that are often nontrivial to figure out how to set in advance, so there's things like do we give them additional motivation?",
                    "label": 0
                },
                {
                    "sent": "And do we have the music on or off?",
                    "label": 0
                },
                {
                    "sent": "How do we represent the fraction, etc.",
                    "label": 0
                },
                {
                    "sent": "And so we can treat all of these as being parameters that end up describing one arm of a multi armed bandit.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have this existing data set and we wanted to be able to try running different algorithms on it and we were curious about how well our Q based evaluator compared to a previous approach that was proposed for contextual multi armed bandits by Liang Lee and John Langford and the others.",
                    "label": 0
                },
                {
                    "sent": "And what you can see here is on the Y axis is the average episode length.",
                    "label": 0
                },
                {
                    "sent": "Essentially that means how many steps could we evaluate our algorithm on until we ran out of data that it was OK to keep evaluating?",
                    "label": 0
                },
                {
                    "sent": "And what you can see is that our approach, which is SDB about, is generally doing much better than the rejection sampling approach from before.",
                    "label": 0
                },
                {
                    "sent": "And what that means is that we're better able to use our existing data in order to evaluate algorithms.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "We want as much data efficiency as possible.",
                    "label": 0
                },
                {
                    "sent": "If we can only use one data point from our private previous data set, we're not going to get a very good estimate of our algorithm.",
                    "label": 0
                },
                {
                    "sent": "And so we want more.",
                    "label": 0
                },
                {
                    "sent": "We want to be able to leverage as much as our old data is possible to inform which algorithm we should use going forward.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It also allows us to compare different algorithms.",
                    "label": 0
                },
                {
                    "sent": "In this case, we were looking at predictive.",
                    "label": 0
                },
                {
                    "sent": "We're looking at predictive state reinforcement learning as well as some deterministic algorithms, and allowed us to show that there's some really nice benefits of using the sort of post posterior sampling approach.",
                    "label": 0
                },
                {
                    "sent": "Right, that's what I should do.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For that one.",
                    "label": 0
                },
                {
                    "sent": "So more recently we've been looking at non stationary valuation for reinforcement learning, not just bandits in particular.",
                    "label": 1
                },
                {
                    "sent": "In this case we've been comparing a number of different offline evaluators we could use based on data.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one again, in this situation we're going to have sort of trajectories of states, actions and rewards, and we're going to be wanting to use that to do our L algorithm evaluation.",
                    "label": 0
                },
                {
                    "sent": "And to our knowledge, is sort of the first of these type of forms of evaluators that exist.",
                    "label": 0
                },
                {
                    "sent": "These sort of directly data driven approaches.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one idea is to sort of take build on the queue based approach where now we can have cues that instead of her based on just states, there's based on state action pairs and then we can have next couples which tell us what was the reward you would have gotten as well as the next state.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so in this case we can do something very similar to what we had before, which is, we have an algorithm that starts in some state.",
                    "label": 0
                },
                {
                    "sent": "It's like some action we go into our Q.",
                    "label": 0
                },
                {
                    "sent": "We see if we have any outcomes from doing that.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we pull from that queue.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We update our algorithm and we repeat.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, another approach that we looked at involves per state rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "So in this case is going to reorganize our data a little bit into sort of streams of outcomes for each state, where we say for this state, this is the action we took.",
                    "label": 0
                },
                {
                    "sent": "The reward we got in the next state.",
                    "label": 0
                },
                {
                    "sent": "And this requires us to have knowledge of what the sampling distribution was.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the way this works in this case is that our algorithm is always going to have some current behavior policy.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we're it's upstate, we're going to pop a sample from the queue.",
                    "label": 0
                },
                {
                    "sent": "So we're going to say, well, here's an action you could have taken.",
                    "label": 0
                },
                {
                    "sent": "This is the reward you would have gotten in the outcome.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can sample a number uniformly and we can see whether or not that number is smaller than a particular ratio, which is the ratio of the probability that we would have selected that action given the policy we're trying to evaluate, divided by the probability we would have selected that action under our sampling distribution, normalized.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what rejection sampling does, and so it allows us to truly draw samples from our actual behavior policy.",
                    "label": 0
                },
                {
                    "sent": "There's another variant of this that we can also consider that involves actually trying to do this whole thing over episodes.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, we wanted to look at this for Treefrog treasure, but in this case we wanted to think about sort of the full RL problem, which is, you know what sequence of activities and fractions should we be giving to the student in order to maximize learning and engagement.",
                    "label": 0
                },
                {
                    "sent": "And what we?",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanted to be doing in this case is really to compare between different evaluators because what we would love to have is to be able to say for practitioners look if you've got some data set.",
                    "label": 0
                },
                {
                    "sent": "This is the evaluator you should run so you can figure out which algorithm to deploy going forward.",
                    "label": 0
                },
                {
                    "sent": "And there are a lot of different properties we would like in these types of evaluators.",
                    "label": 0
                },
                {
                    "sent": "We would like them to be data efficient so that can use all of our old data, but we'd also like them to be able to give us unbiased estimates.",
                    "label": 0
                },
                {
                    "sent": "There's also this really interesting thing called revealed randomness, so in many PRL algorithms they involve taking stochastic policies.",
                    "label": 0
                },
                {
                    "sent": "Some of them do that explicitly.",
                    "label": 0
                },
                {
                    "sent": "In some of them do it implicitly.",
                    "label": 0
                },
                {
                    "sent": "Turns out that if you can represent that uncertainty explicitly, that can give you a lot more sample efficiency.",
                    "label": 0
                },
                {
                    "sent": "Now you might ask if an algorithm has some randomness, why would it hide it?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out in things like Bayesian approaches.",
                    "label": 0
                },
                {
                    "sent": "It might be really easy to sample from your posterior, but it might be computationally expensive to fully represent the posterior.",
                    "label": 0
                },
                {
                    "sent": "So that's the situation where your algorithm might be random, but you only get to see a deterministic example.",
                    "label": 0
                },
                {
                    "sent": "So why might that matter?",
                    "label": 0
                },
                {
                    "sent": "Well so.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a situation where we have no revealed randomness.",
                    "label": 0
                },
                {
                    "sent": "OK, we're treating our algorithm like it's always selecting a deterministic policy.",
                    "label": 0
                },
                {
                    "sent": "On the X axis here is showing sort of how many episodes we could evaluate our algorithms for on the Y axis is showing this percentage for friends, so in this Case No matter which evaluator we picked, almost all of them could only value about 0 to 500 runs in this situation.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if we could use a different evaluator that can leverage revealed randomness, we could use the data that we had to evaluate our algorithms.",
                    "label": 0
                },
                {
                    "sent": "For many many, many more time steps.",
                    "label": 0
                },
                {
                    "sent": "So up to around 2000 episodes instead.",
                    "label": 0
                },
                {
                    "sent": "So this just shows that if you are actually executing another stochastic policy that some of these evaluators can make much better use of that information and give you much better estimates of how well the algorithm would do.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so my student, there's lots of details more to talk about that and my student has a poster tonight.",
                    "label": 0
                },
                {
                    "sent": "Travis Mendelsohn encourage you to come by and look at it.",
                    "label": 0
                },
                {
                    "sent": "But I'm sure that everybody is dying for the coffee break.",
                    "label": 0
                },
                {
                    "sent": "So let me just wrap.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I talked today about sort of optimistic learning control in partially observable domains, as well as offline policy evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think that really these are part of our broader goal of trying to create these systems that really enhance human capacity and well being, and that trying to create these faster learning agents is an important tool to trying to get there, thanks.",
                    "label": 0
                },
                {
                    "sent": "So you said that this estimator is unbiased under certain conditions, but what I'm really interested in for off policy evaluation is with a finite sample.",
                    "label": 0
                },
                {
                    "sent": "How close is my evaluation going to be to its performance in the real environment?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so for I think everyone heard that.",
                    "label": 0
                },
                {
                    "sent": "But the question is if you have a finite amount of data, how close is your estimate to its true performance?",
                    "label": 0
                },
                {
                    "sent": "I don't think that we have those yet.",
                    "label": 0
                },
                {
                    "sent": "The only ones I guarantees I know it for that is if you were to have a static policy or evaluating.",
                    "label": 0
                },
                {
                    "sent": "So Phil Thomas has some really nice work on that as well As for the contextual bandit case Lihong Li does, but I think that's still an open question of how can we actually get finite sample guarantees saying we know you'll be within this when you execute.",
                    "label": 0
                },
                {
                    "sent": "Thank you so much for your talk.",
                    "label": 0
                },
                {
                    "sent": "I want to ask about.",
                    "label": 0
                },
                {
                    "sent": "The pack complexity part the pack algorithms because I'm concerned that that that criteria the criteria is going to be 2 two week to really be helpful.",
                    "label": 0
                },
                {
                    "sent": "You know we get polynomial in the number of states, but typically the number of states will be exponential in the number of state variables like the number of pixels in our cameras and stuff.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think that's a great question.",
                    "label": 0
                },
                {
                    "sent": "I think in general we need represent we need models that can scale up to much larger state spaces.",
                    "label": 0
                },
                {
                    "sent": "I think in general the pack guarantees are already way to lose even for discrete state and action spaces.",
                    "label": 0
                },
                {
                    "sent": "There's something on the order of 10 to the nine samples that we need, but I think that in practice they often work pretty well when we sort of tune that parameter.",
                    "label": 0
                },
                {
                    "sent": "I also think that there are ways that we can leverage the factored representations.",
                    "label": 0
                },
                {
                    "sent": "We've done some work on that and others so you can get much smaller sample complexity bounds.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you again Emma.",
                    "label": 0
                }
            ]
        }
    }
}