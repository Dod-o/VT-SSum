{
    "id": "iddq7x2ijxel6j5crbt2niq5d5zr7gr7",
    "title": "Stochastic Search Using the Natural Gradient",
    "info": {
        "author": [
            "Sun Yi, IDSIA"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Optimization Methods->Stochastic Optimization"
        ]
    },
    "url": "http://videolectures.net/icml09_sun_ssu/",
    "segmentation": [
        [
            "The thing I'm going to talk about this stochastic search using the natural gradient and the algorithm we're presenting is called efficient natural evolution strategy, so it's kind of the same thing and the work is done by myself downward store, Tom Show and you're going to meet Huber, and we're from each year.",
            "Oh"
        ],
        [
            "This is really bad.",
            "So the goal is basically what we're trying to solve.",
            "The problem is a black box optimization problem.",
            "I can you give an unknown function and you want to optimize it, and the function is usually defining AD dimensional real space.",
            "And here's an example.",
            "It's a quite complex surface."
        ],
        [
            "And what is the challenge?"
        ],
        [
            "The challenge the first one is that we have really complex fitness landscapes and."
        ],
        [
            "So there's a lot of local minimum set of points with which is basically everything.",
            "Destroy gradient decent or new."
        ],
        [
            "Method and then we have a highly non isotropic Oreo shaped local behavior that means on some direction the fitness change very rapidly and other directions that the fitness sort of stayed the same.",
            "So in this kind of thing if you use things like a Hill climbing they will get stuck some."
        ],
        [
            "Point and then we have a correlation between all dimensions.",
            "That means maybe your fitness surface is rotated so all the dimensions are created and that is also a big problem for PS or other."
        ],
        [
            "Local search.",
            "Hill climbing algorithms.",
            "And the second problem is we have very expensive in his valuations.",
            "So to evaluate each fitness is very expensive.",
            "That means maybe you have to build a new plane or something like that."
        ],
        [
            "And then we have high dimensionality, so the dimension maybe go up too high."
        ],
        [
            "Andrews or something?",
            "Yeah, for.",
            "For everyone."
        ],
        [
            "Be able to see this whole thing.",
            "I guess I just.",
            "Yeah.",
            "Sorry bout that.",
            "So the conclusion is we need powerful, powerful methods to solve this kind of problem."
        ],
        [
            "And the basic idea of stochastic search is we optimize optimize this problem by using a population or."
        ],
        [
            "Collection of samples and here's the basic workflow.",
            "We after the initialization, we first sample from the search distribution and then we evaluate the fitness of the samples.",
            "Then we update the search distribution using the samples and here's a nice picture.",
            "At the beginning we."
        ],
        [
            "Have a search distribution in the blue circle and we have a bunch of samples.",
            "After evaluating them and some of them are good, so we update this search distribution to here.",
            "And then we go again."
        ],
        [
            "And again and again.",
            "Eventually it will reach the."
        ],
        [
            "Area was good solutions."
        ],
        [
            "So."
        ],
        [
            "The algorithm we use is."
        ],
        [
            "Quote stochastic gradient descent, we use let P be the search distribution and what we want to do is to update the parimeter so the expected fitness of of the batch of collection of samples is maximized.",
            "So so the."
        ],
        [
            "The most natural way of doing this is by just using gradient descent.",
            "So we compute the gradient to the parameter Theta and then use it to update the."
        ],
        [
            "The parimeter so the vanilla ingredients, or just the gradient can be computed as this, and the trick is here, because here we are basically a multiplier PC and divided by PC.",
            "So these two things come together to become a log.",
            "And then we have this PC.",
            "You have to here.",
            "So what we have is the expectation of the fitness value.",
            "Somewhere here and multiply with this one with the gradient of the log probability.",
            "And so the nice thing is, this gradient is also expectation, so we."
        ],
        [
            "And basically do it by using Monte Carlo, so this expectation can be estimated using Monte Carlo, and the G&F is basically in matrix form."
        ],
        [
            "So because the this F is the fitness value, we already know that.",
            "So now the problem is to compute this gradient of the log probability this part, and if we're using Goshen or other kind of distribution from the exponential family, we can have a close form derivation because this log when you have some exponential here, which really will simplify things.",
            "So yeah, in this paper will compute from.",
            "For the Gaussian distribution, so this thing.",
            "In Goshen distribution."
        ],
        [
            "Like this?",
            "Anne."
        ],
        [
            "First thing is we have to choose the parimeter set because for the Goshen distribution with mean X and covariance matrix C, If you use them directly then there's a lot of redundant perimetre because she's essentially symmetric.",
            "So what we use is we use the Cholesky decomposition, which is basically an upper triangular matrix, and when you multiply this together, you get back your."
        ],
        [
            "Covariance matrix.",
            "And the reason you to use this is then there's no redundancy in these parameters."
        ],
        [
            "Set theater and after choosing this parameter, set the gradient for X an for for the Cholesky decomposition, a can be computed in closed form like this."
        ],
        [
            "And then this vanilla ingredient can be computed from from this value and multiply that with the fitness.",
            "Then we have sorry.",
            "What is the exact figure?",
            "Up uh, which one?",
            "This and this is the Goshen distribution.",
            "What is what does it mean?",
            "Are the search distribution is basically at each generation you have to sample from some distribution and evaluate them.",
            "That distribution will call it search distribution.",
            "OK. C minus is the inverse.",
            "Yeah, the more discussion we have a search distribution, that's the center of this.",
            "Search or.",
            "No, no no.",
            "I mean the the fitness function is not caution.",
            "It's just that we use a Goshen search distribution to generate a bunch of samples around some point that evaluate the fitness, and then we go somewhere."
        ],
        [
            "And here's another nice picture to give an idea.",
            "At the beginning.",
            "We have this distribu."
        ],
        [
            "And here.",
            "Then we compute the gradient and update is to."
        ],
        [
            "Here."
        ],
        [
            "And then we do it like this.",
            "So the difference between the normal, the usual stochastic search.",
            "Because here we use a gradient to update it and there is also a lot of other ways to do it."
        ],
        [
            "OK."
        ],
        [
            "The novel idea in this efficient natural evolution strategy is the first one is we use a natural gradient."
        ],
        [
            "Instead of the menu."
        ],
        [
            "1.",
            "And the natural gradient is computed exact an efficient way.",
            "So we have a we come up with a."
        ],
        [
            "Nice way of computing it and then we introduce a mathematic trick called important mixing to reuse the previously eval."
        ],
        [
            "Examples.",
            "And then we introduce a optimal fitness baseline to reduce the variance of the gradient estimation."
        ],
        [
            "So the first one."
        ],
        [
            "While using natural gradient, because the vanilla gradient, we just compute it doesn't work and."
        ],
        [
            "There's several reasons because it's have or aggressive steps on ridges, so once."
        ],
        [
            "Stoneridge is is falls off.",
            "And another reason is it has two small steps on plateaus, so basic."
        ],
        [
            "Stuck there.",
            "So this leads to a slow or premature convergence, especially the premature convergence means you converge before you reach the optimal point, which is something we really."
        ],
        [
            "Don't want so?",
            "Here's an example, and this is a it's so.",
            "It's basically a EO shape.",
            "The surface because on One Direction the fitness varies really fast and this is what the vanilla gradient will do."
        ],
        [
            "At first go somewhere there."
        ],
        [
            "And the exact."
        ],
        [
            "So this is really not something we want."
        ],
        [
            "And the basic idea of natural gradient is."
        ],
        [
            "The first is."
        ],
        [
            "This steepest ascent direction when you consider the correlation between all the elements in theater."
        ],
        [
            "And then what we do is basically re weight agreed and elements according to their uncertainties or variance or normalize."
        ],
        [
            "Variance?",
            "And this will give us an isotropic convergence onyo shaped surface.",
            "So basically what we have will be on this on."
        ],
        [
            "At Eclipse we will converge just like it's a.",
            "It's a sphere or just like it's normal.",
            "Well behaved quadratic function.",
            "So here's an example.",
            "Suppose you're on reach and around some small area you evaluate the gradient and you get this kind of thing and each blue line here is a gradient."
        ],
        [
            "Of your sample.",
            "So if you estimate your vanilla gradient, you just basically average over them, so you get the red one.",
            "So it's quite likely you will fall off that."
        ],
        [
            "And here's it what you get when you use net."
        ],
        [
            "Radiant because the natural gradient will increase the value where all the ingredient elements agree.",
            "So on this Y direction or the gradient arguments agree, and on the app."
        ],
        [
            "Struction, all of them sort of disagree because there's different directions, so in this direction the gradient value is decreased, so the result is you can actually follow the reach.",
            "And if you do it in the ideal situation in this ellipse, it will just."
        ],
        [
            "Go to the circle."
        ],
        [
            "OK, the the formulation."
        ],
        [
            "Because we're using search distributions, so the.",
            "With defined divergance between two adjacent distribution parameter by Theta Answer plus Delta, Theta is defined by their KL divergences.",
            "So in this case after some derivation, the natural gradient.",
            "Is this some gradient?",
            "There's a tail down there and this natural gradient is given essentially by this function."
        ],
        [
            "And here F is the Fisher information matrix and intuitively this F is basically the normalized covariance of the gradient, because if you take a small area, this gradient of log becomes gradient of P / P. So it's basically a normalized version of the."
        ],
        [
            "Covariance and the problem is F may not be invertible.",
            "So in this case we get a set of natural gradient and if you follow any one of them you get the ascent.",
            "But this is not really something we want we."
        ],
        [
            "Want to compute it exactly so if this F is invertible, we can compute the natural gradient or estimate the gradient from sample just by putting F inverse here.",
            "So this is something we want because we can compute.",
            "Otherwise it's become really massive.",
            "We have a set of gradient."
        ],
        [
            "So the second part is we can compute this thing.",
            "Efficiency and 1st.",
            "We have several lucky findings about after really plug in the parameters and."
        ],
        [
            "Compute the natural gradient so the first one is, in our case F is indeed invertible.",
            "So we can put a F minus there."
        ],
        [
            "And the second one is F is blocked outgoing matrix and the structure is like this."
        ],
        [
            "And then.",
            "This CR inverse of C is the Fisher Information matrix for the mean."
        ],
        [
            "And each sub block is a feature information matrix for the non zero elements in the correct key decomposition.",
            "So for each row there is N -- 1 -- 2 elements in a non zero elements in A and each sub block from F1 to FD is the Fisher information matrix for those subsets of permit."
        ],
        [
            "So this theme suggests a natural grouping of the elements in theater in our case, and then between groups, there's orthogonal which."
        ],
        [
            "With each other.",
            "And the third finding is we can actually compute it.",
            "So FK has the special form of this one.",
            "Plus decay decay is the is the submatrix located as at the lower right corner of the."
        ],
        [
            "The inverse of C. So the meaning of this is this special form permits a iterative algorithm to compute this inverse of this sub block from.",
            "FK plus one with complexity like this so."
        ],
        [
            "Yeah, when we compute the inverse of the Fisher information matrix."
        ],
        [
            "If we do it naively because F is a matrix of size D squared, so we need."
        ],
        [
            "D to the power of 6 and if we only consider its block diagonal, then computing requires D to the power of 4."
        ],
        [
            "And then we take into account the special structure so that we get a D to the power of three, which is basically best we can get if we're going to use natural gradient."
        ],
        [
            "And then the estimated natural gradient is simply given by this formulation and we plug in the inverse of F there, and this natural gradient can be computed with complexity D to the power of 3."
        ],
        [
            "And then there's a nice trick."
        ],
        [
            "Called important mixing.",
            "We used basically.",
            "The problem is install gastic search in each cycle we have to evaluate a new fitness samples like this.",
            "The Green Line is the search distribution and the triangles are the sample."
        ],
        [
            "Us and it is quite common.",
            "The updated 1 version of the search gradient.",
            "The Red one is quite close to the green."
        ],
        [
            "So.",
            "If we again sample from the green distribution from the red distribution as simple, a lot of samples generate a lot of samples.",
            "Then it's quite likely the newly generated samples will fall into the same area where you already generated or the area you already explored.",
            "So in this case is kind of."
        ],
        [
            "Waste of samples.",
            "So what we do is basically we say we don't want to generate samples or we want to generally get samples in the area where we already know and then we have to keep the updated batch of samples conform to the new search distribution and the benefit."
        ],
        [
            "Of doing this is we need fewer fitness evaluations, which is supposed to be really expense."
        ],
        [
            "So here's how it works.",
            "There's double rejection, sampling's, and then the green part is the distribution we have and the other one is the distribution we want to update."
        ],
        [
            "So the first rejection sampling is just to take the part in the middle and."
        ],
        [
            "The second step is just we generate new samples to fill in the red area so you can see.",
            "Here is the fitness evaluation in the green area can be reused and all we have to do is to evaluate the fitness in the red area, which is relatively much lower.",
            "And the final thing."
        ],
        [
            "It is about the optimal."
        ],
        [
            "This baseline.",
            "A typical problem for for this policy gradient or natural gradient is that the variance of the gradient estimation is really big like this.",
            "So what we want to do is to somehow reduce the gradient.",
            "So we transform this gradient estimation and put a zero item here because it's a.",
            "Basically a gradient of constant.",
            "And we will put in this thing.",
            "Here we have this gradient estimation.",
            "Having this form."
        ],
        [
            "So adding yeah, here we call this be fitness baseline and adding this baseline won't affect the expectation of the gradient, so it's non biased estimation."
        ],
        [
            "Again, and but it will reduce the variance of the estimation.",
            "So for natural gradient the gradient of the variance of the gradient estimation is a nice quadratic form of B.",
            "So given by this and U NVR just given by these two formula because it's a, it's a."
        ],
        [
            "Getting form we can actually compute the yeah the single global optimal point given by this and because we only have samples, so we use the Monte Carlo estimation to estimate the UV inner product between you, me and you and you so we get.",
            "This estimation formula."
        ],
        [
            "And then plug in this fitness baseline.",
            "The natural gradient is then computed by this formula, so nothing changed except the.",
            "We have to extract the fitness baseline here and this will reduce the variance.",
            "And."
        ],
        [
            "So we get a better idea because from the structure of the Fisher Information matrix, we already know that para meters are grouped together, so.",
            "What we can do is that we compute for each Group A single fitness baseline that will reduce the."
        ],
        [
            "Variance even more so, this is what we propose.",
            "A rock fitness baseline.",
            "So we compute for each group of para meters a single value and."
        ],
        [
            "That worked nicely, so the putting everything together.",
            "The algorithm works like this after initialization."
        ],
        [
            "We use the fitness.",
            "Important sampling to mixing to evaluate important."
        ],
        [
            "Mixing to generate samples and evaluate."
        ],
        [
            "Samples and compute."
        ],
        [
            "Baseline, the natural gradient and updated."
        ],
        [
            "So here's some picture about the experiment and you can see it converges exponentially to the optimum.",
            "There's no premature converge."
        ],
        [
            "This.",
            "And this baseline actually help.",
            "An important mixing actually."
        ],
        [
            "By a factor of three to four."
        ],
        [
            "And here."
        ],
        [
            "Is a nice picture."
        ],
        [
            "About"
        ],
        [
            "The ability to."
        ],
        [
            "Jump over local."
        ],
        [
            "Mum.",
            "And here is the double pole balancing task because we perform really well compared to other things about the same number.",
            "Here is is the old version of CMA, but we're pretty state of the art because this is a non Markovian pole balancing task and double prevents."
        ],
        [
            "Tells me it's quite hard."
        ],
        [
            "And in summary, we have a."
        ],
        [
            "Clear black box optimization."
        ],
        [
            "Prism not too much para meters and we have a."
        ],
        [
            "Clear the division and we have any fish."
        ],
        [
            "Way to compute it.",
            "And we introduced."
        ],
        [
            "Importance mixing and fitness baseline to basically reduce the variance and save some fitness evaluations and."
        ],
        [
            "The result is quite competitive."
        ],
        [
            "OK, if you want to try it out, you can just download it.",
            "It's part of the pipe brain."
        ],
        [
            "Project.",
            "And thank you."
        ],
        [
            "Executed.",
            "Differentiation from the bag of tricks, but if you restrict yourself to Gaussian.",
            "Play devil's advocate and say that you FaceTime was dead for measure, sorry.",
            "Is that the measure of God?",
            "Yes.",
            "Yeah.",
            "It's not that easy because we already tried the mixture of caution case.",
            "It doesn't work that nicely, because for mixed regression.",
            "Sampling, you have to somehow estimate the parameters.",
            "Yeah, you have to.",
            "Science accounts and you take this some outside.",
            "Yeah I need to go for customers account and it's all analytical.",
            "Yeah it's all in electrical it's the problem is just.",
            "Can you actually model that fitness surface using the Gaussian mixture?",
            "Sometimes it's pretty hard if you have some value between one and 1 million somewhere.",
            "I mean the the fitness space I or benchmark we use here is quite eel shaped.",
            "It's very you shape actually."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The thing I'm going to talk about this stochastic search using the natural gradient and the algorithm we're presenting is called efficient natural evolution strategy, so it's kind of the same thing and the work is done by myself downward store, Tom Show and you're going to meet Huber, and we're from each year.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is really bad.",
                    "label": 0
                },
                {
                    "sent": "So the goal is basically what we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "The problem is a black box optimization problem.",
                    "label": 0
                },
                {
                    "sent": "I can you give an unknown function and you want to optimize it, and the function is usually defining AD dimensional real space.",
                    "label": 0
                },
                {
                    "sent": "And here's an example.",
                    "label": 0
                },
                {
                    "sent": "It's a quite complex surface.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what is the challenge?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The challenge the first one is that we have really complex fitness landscapes and.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's a lot of local minimum set of points with which is basically everything.",
                    "label": 0
                },
                {
                    "sent": "Destroy gradient decent or new.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Method and then we have a highly non isotropic Oreo shaped local behavior that means on some direction the fitness change very rapidly and other directions that the fitness sort of stayed the same.",
                    "label": 0
                },
                {
                    "sent": "So in this kind of thing if you use things like a Hill climbing they will get stuck some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point and then we have a correlation between all dimensions.",
                    "label": 0
                },
                {
                    "sent": "That means maybe your fitness surface is rotated so all the dimensions are created and that is also a big problem for PS or other.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Local search.",
                    "label": 0
                },
                {
                    "sent": "Hill climbing algorithms.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is we have very expensive in his valuations.",
                    "label": 0
                },
                {
                    "sent": "So to evaluate each fitness is very expensive.",
                    "label": 0
                },
                {
                    "sent": "That means maybe you have to build a new plane or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we have high dimensionality, so the dimension maybe go up too high.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Andrews or something?",
                    "label": 0
                },
                {
                    "sent": "Yeah, for.",
                    "label": 0
                },
                {
                    "sent": "For everyone.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be able to see this whole thing.",
                    "label": 0
                },
                {
                    "sent": "I guess I just.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "So the conclusion is we need powerful, powerful methods to solve this kind of problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the basic idea of stochastic search is we optimize optimize this problem by using a population or.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Collection of samples and here's the basic workflow.",
                    "label": 1
                },
                {
                    "sent": "We after the initialization, we first sample from the search distribution and then we evaluate the fitness of the samples.",
                    "label": 1
                },
                {
                    "sent": "Then we update the search distribution using the samples and here's a nice picture.",
                    "label": 0
                },
                {
                    "sent": "At the beginning we.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have a search distribution in the blue circle and we have a bunch of samples.",
                    "label": 1
                },
                {
                    "sent": "After evaluating them and some of them are good, so we update this search distribution to here.",
                    "label": 0
                },
                {
                    "sent": "And then we go again.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again and again.",
                    "label": 0
                },
                {
                    "sent": "Eventually it will reach the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Area was good solutions.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm we use is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quote stochastic gradient descent, we use let P be the search distribution and what we want to do is to update the parimeter so the expected fitness of of the batch of collection of samples is maximized.",
                    "label": 0
                },
                {
                    "sent": "So so the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The most natural way of doing this is by just using gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So we compute the gradient to the parameter Theta and then use it to update the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The parimeter so the vanilla ingredients, or just the gradient can be computed as this, and the trick is here, because here we are basically a multiplier PC and divided by PC.",
                    "label": 0
                },
                {
                    "sent": "So these two things come together to become a log.",
                    "label": 0
                },
                {
                    "sent": "And then we have this PC.",
                    "label": 0
                },
                {
                    "sent": "You have to here.",
                    "label": 0
                },
                {
                    "sent": "So what we have is the expectation of the fitness value.",
                    "label": 0
                },
                {
                    "sent": "Somewhere here and multiply with this one with the gradient of the log probability.",
                    "label": 0
                },
                {
                    "sent": "And so the nice thing is, this gradient is also expectation, so we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically do it by using Monte Carlo, so this expectation can be estimated using Monte Carlo, and the G&F is basically in matrix form.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So because the this F is the fitness value, we already know that.",
                    "label": 0
                },
                {
                    "sent": "So now the problem is to compute this gradient of the log probability this part, and if we're using Goshen or other kind of distribution from the exponential family, we can have a close form derivation because this log when you have some exponential here, which really will simplify things.",
                    "label": 0
                },
                {
                    "sent": "So yeah, in this paper will compute from.",
                    "label": 0
                },
                {
                    "sent": "For the Gaussian distribution, so this thing.",
                    "label": 0
                },
                {
                    "sent": "In Goshen distribution.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First thing is we have to choose the parimeter set because for the Goshen distribution with mean X and covariance matrix C, If you use them directly then there's a lot of redundant perimetre because she's essentially symmetric.",
                    "label": 0
                },
                {
                    "sent": "So what we use is we use the Cholesky decomposition, which is basically an upper triangular matrix, and when you multiply this together, you get back your.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And the reason you to use this is then there's no redundancy in these parameters.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set theater and after choosing this parameter, set the gradient for X an for for the Cholesky decomposition, a can be computed in closed form like this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then this vanilla ingredient can be computed from from this value and multiply that with the fitness.",
                    "label": 1
                },
                {
                    "sent": "Then we have sorry.",
                    "label": 0
                },
                {
                    "sent": "What is the exact figure?",
                    "label": 0
                },
                {
                    "sent": "Up uh, which one?",
                    "label": 0
                },
                {
                    "sent": "This and this is the Goshen distribution.",
                    "label": 0
                },
                {
                    "sent": "What is what does it mean?",
                    "label": 1
                },
                {
                    "sent": "Are the search distribution is basically at each generation you have to sample from some distribution and evaluate them.",
                    "label": 0
                },
                {
                    "sent": "That distribution will call it search distribution.",
                    "label": 0
                },
                {
                    "sent": "OK. C minus is the inverse.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the more discussion we have a search distribution, that's the center of this.",
                    "label": 0
                },
                {
                    "sent": "Search or.",
                    "label": 0
                },
                {
                    "sent": "No, no no.",
                    "label": 1
                },
                {
                    "sent": "I mean the the fitness function is not caution.",
                    "label": 0
                },
                {
                    "sent": "It's just that we use a Goshen search distribution to generate a bunch of samples around some point that evaluate the fitness, and then we go somewhere.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's another nice picture to give an idea.",
                    "label": 0
                },
                {
                    "sent": "At the beginning.",
                    "label": 0
                },
                {
                    "sent": "We have this distribu.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here.",
                    "label": 0
                },
                {
                    "sent": "Then we compute the gradient and update is to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we do it like this.",
                    "label": 0
                },
                {
                    "sent": "So the difference between the normal, the usual stochastic search.",
                    "label": 0
                },
                {
                    "sent": "Because here we use a gradient to update it and there is also a lot of other ways to do it.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The novel idea in this efficient natural evolution strategy is the first one is we use a natural gradient.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Instead of the menu.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "And the natural gradient is computed exact an efficient way.",
                    "label": 1
                },
                {
                    "sent": "So we have a we come up with a.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice way of computing it and then we introduce a mathematic trick called important mixing to reuse the previously eval.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Examples.",
                    "label": 0
                },
                {
                    "sent": "And then we introduce a optimal fitness baseline to reduce the variance of the gradient estimation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first one.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "While using natural gradient, because the vanilla gradient, we just compute it doesn't work and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's several reasons because it's have or aggressive steps on ridges, so once.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stoneridge is is falls off.",
                    "label": 0
                },
                {
                    "sent": "And another reason is it has two small steps on plateaus, so basic.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stuck there.",
                    "label": 0
                },
                {
                    "sent": "So this leads to a slow or premature convergence, especially the premature convergence means you converge before you reach the optimal point, which is something we really.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't want so?",
                    "label": 0
                },
                {
                    "sent": "Here's an example, and this is a it's so.",
                    "label": 0
                },
                {
                    "sent": "It's basically a EO shape.",
                    "label": 0
                },
                {
                    "sent": "The surface because on One Direction the fitness varies really fast and this is what the vanilla gradient will do.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At first go somewhere there.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the exact.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is really not something we want.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the basic idea of natural gradient is.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first is.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This steepest ascent direction when you consider the correlation between all the elements in theater.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then what we do is basically re weight agreed and elements according to their uncertainties or variance or normalize.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance?",
                    "label": 0
                },
                {
                    "sent": "And this will give us an isotropic convergence onyo shaped surface.",
                    "label": 0
                },
                {
                    "sent": "So basically what we have will be on this on.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At Eclipse we will converge just like it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a sphere or just like it's normal.",
                    "label": 0
                },
                {
                    "sent": "Well behaved quadratic function.",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose you're on reach and around some small area you evaluate the gradient and you get this kind of thing and each blue line here is a gradient.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of your sample.",
                    "label": 0
                },
                {
                    "sent": "So if you estimate your vanilla gradient, you just basically average over them, so you get the red one.",
                    "label": 0
                },
                {
                    "sent": "So it's quite likely you will fall off that.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's it what you get when you use net.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radiant because the natural gradient will increase the value where all the ingredient elements agree.",
                    "label": 0
                },
                {
                    "sent": "So on this Y direction or the gradient arguments agree, and on the app.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Struction, all of them sort of disagree because there's different directions, so in this direction the gradient value is decreased, so the result is you can actually follow the reach.",
                    "label": 0
                },
                {
                    "sent": "And if you do it in the ideal situation in this ellipse, it will just.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go to the circle.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, the the formulation.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because we're using search distributions, so the.",
                    "label": 0
                },
                {
                    "sent": "With defined divergance between two adjacent distribution parameter by Theta Answer plus Delta, Theta is defined by their KL divergences.",
                    "label": 1
                },
                {
                    "sent": "So in this case after some derivation, the natural gradient.",
                    "label": 1
                },
                {
                    "sent": "Is this some gradient?",
                    "label": 0
                },
                {
                    "sent": "There's a tail down there and this natural gradient is given essentially by this function.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here F is the Fisher information matrix and intuitively this F is basically the normalized covariance of the gradient, because if you take a small area, this gradient of log becomes gradient of P / P. So it's basically a normalized version of the.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Covariance and the problem is F may not be invertible.",
                    "label": 1
                },
                {
                    "sent": "So in this case we get a set of natural gradient and if you follow any one of them you get the ascent.",
                    "label": 0
                },
                {
                    "sent": "But this is not really something we want we.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Want to compute it exactly so if this F is invertible, we can compute the natural gradient or estimate the gradient from sample just by putting F inverse here.",
                    "label": 1
                },
                {
                    "sent": "So this is something we want because we can compute.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's become really massive.",
                    "label": 0
                },
                {
                    "sent": "We have a set of gradient.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second part is we can compute this thing.",
                    "label": 0
                },
                {
                    "sent": "Efficiency and 1st.",
                    "label": 0
                },
                {
                    "sent": "We have several lucky findings about after really plug in the parameters and.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Compute the natural gradient so the first one is, in our case F is indeed invertible.",
                    "label": 0
                },
                {
                    "sent": "So we can put a F minus there.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second one is F is blocked outgoing matrix and the structure is like this.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "This CR inverse of C is the Fisher Information matrix for the mean.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And each sub block is a feature information matrix for the non zero elements in the correct key decomposition.",
                    "label": 0
                },
                {
                    "sent": "So for each row there is N -- 1 -- 2 elements in a non zero elements in A and each sub block from F1 to FD is the Fisher information matrix for those subsets of permit.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this theme suggests a natural grouping of the elements in theater in our case, and then between groups, there's orthogonal which.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With each other.",
                    "label": 0
                },
                {
                    "sent": "And the third finding is we can actually compute it.",
                    "label": 0
                },
                {
                    "sent": "So FK has the special form of this one.",
                    "label": 1
                },
                {
                    "sent": "Plus decay decay is the is the submatrix located as at the lower right corner of the.",
                    "label": 1
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The inverse of C. So the meaning of this is this special form permits a iterative algorithm to compute this inverse of this sub block from.",
                    "label": 0
                },
                {
                    "sent": "FK plus one with complexity like this so.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, when we compute the inverse of the Fisher information matrix.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we do it naively because F is a matrix of size D squared, so we need.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "D to the power of 6 and if we only consider its block diagonal, then computing requires D to the power of 4.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we take into account the special structure so that we get a D to the power of three, which is basically best we can get if we're going to use natural gradient.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the estimated natural gradient is simply given by this formulation and we plug in the inverse of F there, and this natural gradient can be computed with complexity D to the power of 3.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's a nice trick.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Called important mixing.",
                    "label": 0
                },
                {
                    "sent": "We used basically.",
                    "label": 0
                },
                {
                    "sent": "The problem is install gastic search in each cycle we have to evaluate a new fitness samples like this.",
                    "label": 1
                },
                {
                    "sent": "The Green Line is the search distribution and the triangles are the sample.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Us and it is quite common.",
                    "label": 1
                },
                {
                    "sent": "The updated 1 version of the search gradient.",
                    "label": 0
                },
                {
                    "sent": "The Red one is quite close to the green.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we again sample from the green distribution from the red distribution as simple, a lot of samples generate a lot of samples.",
                    "label": 0
                },
                {
                    "sent": "Then it's quite likely the newly generated samples will fall into the same area where you already generated or the area you already explored.",
                    "label": 0
                },
                {
                    "sent": "So in this case is kind of.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Waste of samples.",
                    "label": 0
                },
                {
                    "sent": "So what we do is basically we say we don't want to generate samples or we want to generally get samples in the area where we already know and then we have to keep the updated batch of samples conform to the new search distribution and the benefit.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of doing this is we need fewer fitness evaluations, which is supposed to be really expense.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's how it works.",
                    "label": 0
                },
                {
                    "sent": "There's double rejection, sampling's, and then the green part is the distribution we have and the other one is the distribution we want to update.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the first rejection sampling is just to take the part in the middle and.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second step is just we generate new samples to fill in the red area so you can see.",
                    "label": 0
                },
                {
                    "sent": "Here is the fitness evaluation in the green area can be reused and all we have to do is to evaluate the fitness in the red area, which is relatively much lower.",
                    "label": 0
                },
                {
                    "sent": "And the final thing.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It is about the optimal.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This baseline.",
                    "label": 0
                },
                {
                    "sent": "A typical problem for for this policy gradient or natural gradient is that the variance of the gradient estimation is really big like this.",
                    "label": 1
                },
                {
                    "sent": "So what we want to do is to somehow reduce the gradient.",
                    "label": 0
                },
                {
                    "sent": "So we transform this gradient estimation and put a zero item here because it's a.",
                    "label": 0
                },
                {
                    "sent": "Basically a gradient of constant.",
                    "label": 0
                },
                {
                    "sent": "And we will put in this thing.",
                    "label": 0
                },
                {
                    "sent": "Here we have this gradient estimation.",
                    "label": 0
                },
                {
                    "sent": "Having this form.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So adding yeah, here we call this be fitness baseline and adding this baseline won't affect the expectation of the gradient, so it's non biased estimation.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, and but it will reduce the variance of the estimation.",
                    "label": 1
                },
                {
                    "sent": "So for natural gradient the gradient of the variance of the gradient estimation is a nice quadratic form of B.",
                    "label": 1
                },
                {
                    "sent": "So given by this and U NVR just given by these two formula because it's a, it's a.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getting form we can actually compute the yeah the single global optimal point given by this and because we only have samples, so we use the Monte Carlo estimation to estimate the UV inner product between you, me and you and you so we get.",
                    "label": 0
                },
                {
                    "sent": "This estimation formula.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then plug in this fitness baseline.",
                    "label": 0
                },
                {
                    "sent": "The natural gradient is then computed by this formula, so nothing changed except the.",
                    "label": 1
                },
                {
                    "sent": "We have to extract the fitness baseline here and this will reduce the variance.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we get a better idea because from the structure of the Fisher Information matrix, we already know that para meters are grouped together, so.",
                    "label": 0
                },
                {
                    "sent": "What we can do is that we compute for each Group A single fitness baseline that will reduce the.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Variance even more so, this is what we propose.",
                    "label": 0
                },
                {
                    "sent": "A rock fitness baseline.",
                    "label": 0
                },
                {
                    "sent": "So we compute for each group of para meters a single value and.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That worked nicely, so the putting everything together.",
                    "label": 0
                },
                {
                    "sent": "The algorithm works like this after initialization.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use the fitness.",
                    "label": 0
                },
                {
                    "sent": "Important sampling to mixing to evaluate important.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mixing to generate samples and evaluate.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Samples and compute.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Baseline, the natural gradient and updated.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's some picture about the experiment and you can see it converges exponentially to the optimum.",
                    "label": 0
                },
                {
                    "sent": "There's no premature converge.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "And this baseline actually help.",
                    "label": 0
                },
                {
                    "sent": "An important mixing actually.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By a factor of three to four.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a nice picture.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The ability to.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jump over local.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mum.",
                    "label": 0
                },
                {
                    "sent": "And here is the double pole balancing task because we perform really well compared to other things about the same number.",
                    "label": 0
                },
                {
                    "sent": "Here is is the old version of CMA, but we're pretty state of the art because this is a non Markovian pole balancing task and double prevents.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tells me it's quite hard.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And in summary, we have a.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clear black box optimization.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prism not too much para meters and we have a.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clear the division and we have any fish.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way to compute it.",
                    "label": 0
                },
                {
                    "sent": "And we introduced.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Importance mixing and fitness baseline to basically reduce the variance and save some fitness evaluations and.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The result is quite competitive.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, if you want to try it out, you can just download it.",
                    "label": 0
                },
                {
                    "sent": "It's part of the pipe brain.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Project.",
                    "label": 0
                },
                {
                    "sent": "And thank you.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Executed.",
                    "label": 0
                },
                {
                    "sent": "Differentiation from the bag of tricks, but if you restrict yourself to Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Play devil's advocate and say that you FaceTime was dead for measure, sorry.",
                    "label": 0
                },
                {
                    "sent": "Is that the measure of God?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "It's not that easy because we already tried the mixture of caution case.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work that nicely, because for mixed regression.",
                    "label": 0
                },
                {
                    "sent": "Sampling, you have to somehow estimate the parameters.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you have to.",
                    "label": 0
                },
                {
                    "sent": "Science accounts and you take this some outside.",
                    "label": 0
                },
                {
                    "sent": "Yeah I need to go for customers account and it's all analytical.",
                    "label": 0
                },
                {
                    "sent": "Yeah it's all in electrical it's the problem is just.",
                    "label": 0
                },
                {
                    "sent": "Can you actually model that fitness surface using the Gaussian mixture?",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's pretty hard if you have some value between one and 1 million somewhere.",
                    "label": 0
                },
                {
                    "sent": "I mean the the fitness space I or benchmark we use here is quite eel shaped.",
                    "label": 0
                },
                {
                    "sent": "It's very you shape actually.",
                    "label": 0
                }
            ]
        }
    }
}