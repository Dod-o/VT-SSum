{
    "id": "4lujxfasc5xg7ltreyeea4zrje5aikh3",
    "title": "DoSeR - A Knowledge-Base-Agnostic Framework for Entity Disambiguation Using Semantic Embeddings",
    "info": {
        "author": [
            "Stefan Zwicklbauer, University of Passau"
        ],
        "published": "July 28, 2016",
        "recorded": "June 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_zwicklbauer_semantic_embeddings/",
    "segmentation": [
        [
            "As already said, my name is Steven speaking power and I'm a PhD student from the University of Tulsa and I'm very proud to present those are today.",
            "Those are is a knowledge base agnostic framework for entities ambiguation that uses semantic embeddings."
        ],
        [
            "Well, the task entity disambiguation has been very well researched so far, especially in the semantic web community, so it is probably well known.",
            "So yeah, well, entity disambiguation is the task of linking a surface form which has been previously recognized by an entity recognizer, for instance, to its appropriate entity in a knowledge base.",
            "By resolving the surface forms ambiguity.",
            "So in this very specific example, we would like to link the surface Form 3.",
            "Highlighted.",
            "As a yellow snippet to the.",
            "To one of the entities in the knowledge base on the right side.",
            "So in this example knowledge knowledge base, we have different entities that might refer to the word and three.",
            "For instance, we have the plant tree, the tree in the graph theory topic.",
            "The tree is a data structure and so on.",
            "Finally, the disambiguation algorithm has to decide which entity is the correct one given a surface form."
        ],
        [
            "Well, this work.",
            "We focus on those are those are is the abbreviation of disambiguation of semantic resources and initially.",
            "My goal was to create a simple but effective knowledge base agnostic disambiguation approach knowledge base agnostic in this context means that my approach should be able to use the knowledge from entity annotated document knowledge bases.",
            "For instance, Wikipedia is entity document entity, annotated document knowledge base, or we can use the knowledge from RDF knowledge basis.",
            "For instance, YAGO three or two.",
            "Or DB pedia.",
            "So in the following I will abbreviate the term entity annotated document knowledge base as a D knowledge base, because it simplifies the pronunciation.",
            "So my.",
            "My work Dozer can be subdivided in two crucial steps.",
            "In the first step we create.",
            "It is a one time.",
            "And pre processing step here we create and disambiguation index which will contain all entities which should be disambig weighted by our.",
            "Disambiguation approach and every entity contains 3 specific kinds of information.",
            "It's a set of entity labels.",
            "The prior probability of this entity and an entity embedding.",
            "And in the second step, yeah, we make use of this constructed disambiguation index and.",
            "Perform the main disambiguation on it."
        ],
        [
            "As I said before, we store 3 different kinds of entity information in our disambiguation index.",
            "The first one is a set of entity labels.",
            "We collect these entity labels from our underlying knowledge basis.",
            "For instance in terms of adknowledge basis, we simply collect the surface forms which have already been annotated to an entity.",
            "So far, and if no eaid knowledge base is available, we just use the RDF's label attribute.",
            "In our RDF knowledge base.",
            "The second kind of information will averages the prior probability of an entity, while the prior probability just describes the likelihood that an entity or cures in an arbitrary or document we compute compute this prior probability by counting the number of occurrences of each entity in our AAD knowledge base.",
            "So we just count the occurrences of each entity and compute the prior probability, and if noadiah knowledge base is available.",
            "We count the number of in and out going edges of the entity in a graph based knowledge base or in RDF knowledge base, and the third type of information or information we use.",
            "It is the most important one.",
            "Our semantic entity embeddings.",
            "So what are entity embeddings?",
            "Entity embeddings are basically end dimensional vectors which describe the semantic relationship.",
            "Between entities via the cosine similarity.",
            "In other words, we can compute the semantic relatedness between an entity pair by computing the cosine similarity of the entities vectors.",
            "So the question arises, how do we create these entity embeddings?",
            "OK, here we use vert avec vert.",
            "Avec is very popular group of unsupervised algorithms to create word vectors or word embeddings from unlabeled documents.",
            "Well about a week usually accept texture or natural language text in an input.",
            "Corpora and trains the embeddings based on the word occurrences in this natural language text.",
            "So in this work we use virtual work more or less as a black black box to train our embeddings.",
            "But we more we focus on the creation of our virtual VAC input corpora cause since we do, we would like to create entity embeddings instead of word embeddings.",
            "We have to create.",
            "Quick input corpora that contain entities only instead of instead of words.",
            "So in the following I will present two algorithms to create such virtual input corpora, the one.",
            "Uses the knowledge of RDF knowledge basis and the other one uses the knowledge of the knowledge basis."
        ],
        [
            "OK, to create a word to take input corpus out of an egg knowledge base, we perform a simple two step approach or first of all we just iterate over all over all documents in our knowledge base.",
            "So when we take Wikipedia as an ID knowledge base, we first iterate over oil.",
            "Documents in it.",
            "Then we replace all entity hyperlinks which occur in a document.",
            "So let's have a look at the example.",
            "Here we have a short Wikipedia sentence that TS has been a tourist attraction in New York City for over a century.",
            "We have 4 surface phones in it.",
            "TS, Tourist, attraction, NYC and century and each surface form is already linked to another entity, or Wikipedia or tickling.",
            "So TS is linked through the Wikipedia document.",
            "We could time square central century on the other side as link to the Wikipedia or Arctic wiki century.",
            "So here we replace the surface forms with the entities identifiers and the basically the second step we further remove all non entity words and punctuations.",
            "So as a result we obtain an entity list which is in the same order or with entities which are in the same order as given in the original Wikipedia document.",
            "So the location of the entities and the document is remains the same.",
            "And."
        ],
        [
            "Create.",
            "I've worked with input corpus out of an RDF knowledge base.",
            "We regard the RDF knowledge base as an undirected graph.",
            "This is very important on this undirected graph.",
            "We perform a random walk which randomly moves from one entity to another entity.",
            "Here, via it's connecting relations.",
            "So let's have an example.",
            "First of all, we determine an obituary entity in our graph which serves as a starting point.",
            "Here we select the entity.",
            "Berlin is starting point at the same time we write.",
            "The entity Berlin in two hour.",
            "Still empty where to wake input corpus in the next step we randomly choose an idiot sent entity of Berlin.",
            "This is Germany.",
            "Both entities are connected via the capital of relation.",
            "In our RDF knowledge base, so we proceed and randomly choose another entity.",
            "Here, Angela Merkel.",
            "Angela Merkel is connected to Germany via the leader relationship.",
            "So basically we can continue this procedure.",
            "Many times, but to prevent getting stuck somewhere in the graph we Additionally integrate chump probability which allows us to jump from any node in the graph to any other node.",
            "So here in this example we randomly jump to the former German actor, wanted blue from Angela Merkel.",
            "We assume here that both entities are not related and are located somewhere else in the graph and from here on we continue as previously explained.",
            "We select another entity influencer.",
            "So here I have to note that we have presented two algorithms that generate word two VEC input corpora, and finally the generated corpora can be concatenated.",
            "Becausw both corpora contain the same, basically the same entities on the one hand we have Peter entities as output and on the other hand we have Wikipedia entities output and we all know that both kinds of entities entities can be matched to each other.",
            "So overall we generate one huge training corpus for where to Vic.",
            "The."
        ],
        [
            "This brings me to our second contribution.",
            "In my work, the entity disambiguation algorithm.",
            "The algorithm accepts a document with one or multiple surface forms in it, which should be disseminated.",
            "And of course, we use the previously constructed disambiguation index and this example.",
            "Also used a short text fragment, the TS has been in New York attraction for over a century.",
            "Here we have two surface forms which should be disintegrated.",
            "Yeah, well, the first step is to create a disambiguation graph.",
            "And here we first have to generate a set of candidate entities for each surface form.",
            "We do this by matching the surface form with the stored entity labels in our disambiguation index, and if there is an exact matching between them, we use the set of entities that exactly match the surface form as candidate set.",
            "And if no.",
            "Or if no candidate entities are available.",
            "After the exact matching step, we use the trigram similarity approach that is used in the distace disambiguation system.",
            "And the second step.",
            "We create a complete directed keipert it graph which has the constraint that we remove the edges between those entities that do not belong to the same surface form.",
            "So here we removed the entity at the edges between New York City and New York State because both entities belong to the same surface form New York."
        ],
        [
            "And the edges are weighted.",
            "And here we use the normalized semantic relatedness score between an entity pair.",
            "So we can see here the transition probabilities between an entity pair and this third step we Additionally use our.",
            "Prior probability for each entity as a chunk probability which allows us to jump from any node to any other node in the graph.",
            "And here we perform a random chump.",
            "With a fixed parameter, Alpha equals 0.1, which was suggested in the original page rank with priors paper and last but not least, we remove 25% of low probability edges in this graph.",
            "This is cause we would like to optimize the disambiguation step later on.",
            "So here we remove the edge from the entity Time Square to New York State, cause this edge has the lowest transition probability.",
            "So given this disambiguation graph, we apply the page rank with priors algorithm algorithm which computes the Pagerank score for each entity and the entities that provide the highest.",
            "Patriots score denote the final disambiguation results.",
            "So here we disambiguate it time square for surface form TS, and the entity in New York City for New York.",
            "Of course."
        ],
        [
            "We performed an in depth evaluation to compare those are with other state of the art approaches and for that purpose we used cable in its basic version it is integrable, is an easy to use platform to evaluate and compare the outcomes of.",
            "Any disambiguation algorithms on a set of data sets of all we performed two crucial experiments in the first one, we directly compared.",
            "Those are two activities.",
            "This is a recently proposed state of the art entity disambiguation system that.",
            "Is knowledge base agnostic in terms of RDF knowledge bases, so it can use yoga.",
            "Two Jago three or DB PDO, and so on.",
            "As underlying knowledge base.",
            "And since Actis Wars optimized to disambiguate named entities only, we focus on named entity disambiguation to provide a fair comparison between both approaches, and in the second experiment.",
            "We Additionally leverage the knowledge located in Wikipedia and compare our approach to other state of the art approaches that also make use of Wikipedia knowledge.",
            "So."
        ],
        [
            "In this table we can see the microphone values on seven different datasets.",
            "Of my approach and the approach of those are.",
            "The first line shows the result attained when we use DPR, is underlying knowledge base and the second line shows the result when we use Jago three, his knowledge base and finally we evaluated activities on DB pedia only.",
            "The red bold highlighted values in the table denote the best F1 values on this data set.",
            "And as we can see, those outperforms agdistis on 6 out of seven datasets by about four to five MW one percentage points.",
            "In this context, we can say that our work to make embeddings can capture the word to capture the relations in our DB pedia perfectly.",
            "And further, we assume that the entity embeddings are robust against noisy knowledge base information.",
            "And the second experiment."
        ],
        [
            "We compare those are to the following four state of the art dissimulation approaches Wiki Fire Aida, DB Pedia, Spotlight and Wat.",
            "On the first line we show the results achieved on the DB Pedia and in the second line we show the results when we Additionally use Wikipedia's knowledge base so we can significantly increase the F1 values by up to 15 percentage points on average across all datasets.",
            "And I'm happy to say that those are significantly outperforms others.",
            "State of the art approaches on many datasets."
        ],
        [
            "So to conclude, and this worker presented, those are a new state of the art collective entity disambiguation system, and it is a knowledge base agnostic in terms of entity annotated document knowledge bases and RDF knowledge base is it is a simple graph based approach that, based on semantic embeddings.",
            "And our second contribution rules that we presented how these semantic embeddings can be generated on different knowledge bases.",
            "For instance, DB Pedia and Wikipedia at the same time and in the near future we would like to integrate the textual context of surface forms to further increase the disintegration results.",
            "Finally, the code of those are.",
            "Can be seen and downloaded from our GitHub repository.",
            "Thank you for your."
        ],
        [
            "Attention.",
            "If questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As already said, my name is Steven speaking power and I'm a PhD student from the University of Tulsa and I'm very proud to present those are today.",
                    "label": 0
                },
                {
                    "sent": "Those are is a knowledge base agnostic framework for entities ambiguation that uses semantic embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, the task entity disambiguation has been very well researched so far, especially in the semantic web community, so it is probably well known.",
                    "label": 0
                },
                {
                    "sent": "So yeah, well, entity disambiguation is the task of linking a surface form which has been previously recognized by an entity recognizer, for instance, to its appropriate entity in a knowledge base.",
                    "label": 1
                },
                {
                    "sent": "By resolving the surface forms ambiguity.",
                    "label": 0
                },
                {
                    "sent": "So in this very specific example, we would like to link the surface Form 3.",
                    "label": 0
                },
                {
                    "sent": "Highlighted.",
                    "label": 0
                },
                {
                    "sent": "As a yellow snippet to the.",
                    "label": 1
                },
                {
                    "sent": "To one of the entities in the knowledge base on the right side.",
                    "label": 1
                },
                {
                    "sent": "So in this example knowledge knowledge base, we have different entities that might refer to the word and three.",
                    "label": 0
                },
                {
                    "sent": "For instance, we have the plant tree, the tree in the graph theory topic.",
                    "label": 0
                },
                {
                    "sent": "The tree is a data structure and so on.",
                    "label": 0
                },
                {
                    "sent": "Finally, the disambiguation algorithm has to decide which entity is the correct one given a surface form.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, this work.",
                    "label": 0
                },
                {
                    "sent": "We focus on those are those are is the abbreviation of disambiguation of semantic resources and initially.",
                    "label": 1
                },
                {
                    "sent": "My goal was to create a simple but effective knowledge base agnostic disambiguation approach knowledge base agnostic in this context means that my approach should be able to use the knowledge from entity annotated document knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "For instance, Wikipedia is entity document entity, annotated document knowledge base, or we can use the knowledge from RDF knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "For instance, YAGO three or two.",
                    "label": 0
                },
                {
                    "sent": "Or DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So in the following I will abbreviate the term entity annotated document knowledge base as a D knowledge base, because it simplifies the pronunciation.",
                    "label": 0
                },
                {
                    "sent": "So my.",
                    "label": 0
                },
                {
                    "sent": "My work Dozer can be subdivided in two crucial steps.",
                    "label": 0
                },
                {
                    "sent": "In the first step we create.",
                    "label": 0
                },
                {
                    "sent": "It is a one time.",
                    "label": 0
                },
                {
                    "sent": "And pre processing step here we create and disambiguation index which will contain all entities which should be disambig weighted by our.",
                    "label": 1
                },
                {
                    "sent": "Disambiguation approach and every entity contains 3 specific kinds of information.",
                    "label": 0
                },
                {
                    "sent": "It's a set of entity labels.",
                    "label": 0
                },
                {
                    "sent": "The prior probability of this entity and an entity embedding.",
                    "label": 0
                },
                {
                    "sent": "And in the second step, yeah, we make use of this constructed disambiguation index and.",
                    "label": 0
                },
                {
                    "sent": "Perform the main disambiguation on it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said before, we store 3 different kinds of entity information in our disambiguation index.",
                    "label": 0
                },
                {
                    "sent": "The first one is a set of entity labels.",
                    "label": 0
                },
                {
                    "sent": "We collect these entity labels from our underlying knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "For instance in terms of adknowledge basis, we simply collect the surface forms which have already been annotated to an entity.",
                    "label": 0
                },
                {
                    "sent": "So far, and if no eaid knowledge base is available, we just use the RDF's label attribute.",
                    "label": 0
                },
                {
                    "sent": "In our RDF knowledge base.",
                    "label": 0
                },
                {
                    "sent": "The second kind of information will averages the prior probability of an entity, while the prior probability just describes the likelihood that an entity or cures in an arbitrary or document we compute compute this prior probability by counting the number of occurrences of each entity in our AAD knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So we just count the occurrences of each entity and compute the prior probability, and if noadiah knowledge base is available.",
                    "label": 0
                },
                {
                    "sent": "We count the number of in and out going edges of the entity in a graph based knowledge base or in RDF knowledge base, and the third type of information or information we use.",
                    "label": 0
                },
                {
                    "sent": "It is the most important one.",
                    "label": 0
                },
                {
                    "sent": "Our semantic entity embeddings.",
                    "label": 0
                },
                {
                    "sent": "So what are entity embeddings?",
                    "label": 0
                },
                {
                    "sent": "Entity embeddings are basically end dimensional vectors which describe the semantic relationship.",
                    "label": 0
                },
                {
                    "sent": "Between entities via the cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "In other words, we can compute the semantic relatedness between an entity pair by computing the cosine similarity of the entities vectors.",
                    "label": 0
                },
                {
                    "sent": "So the question arises, how do we create these entity embeddings?",
                    "label": 0
                },
                {
                    "sent": "OK, here we use vert avec vert.",
                    "label": 0
                },
                {
                    "sent": "Avec is very popular group of unsupervised algorithms to create word vectors or word embeddings from unlabeled documents.",
                    "label": 0
                },
                {
                    "sent": "Well about a week usually accept texture or natural language text in an input.",
                    "label": 0
                },
                {
                    "sent": "Corpora and trains the embeddings based on the word occurrences in this natural language text.",
                    "label": 1
                },
                {
                    "sent": "So in this work we use virtual work more or less as a black black box to train our embeddings.",
                    "label": 0
                },
                {
                    "sent": "But we more we focus on the creation of our virtual VAC input corpora cause since we do, we would like to create entity embeddings instead of word embeddings.",
                    "label": 1
                },
                {
                    "sent": "We have to create.",
                    "label": 0
                },
                {
                    "sent": "Quick input corpora that contain entities only instead of instead of words.",
                    "label": 0
                },
                {
                    "sent": "So in the following I will present two algorithms to create such virtual input corpora, the one.",
                    "label": 0
                },
                {
                    "sent": "Uses the knowledge of RDF knowledge basis and the other one uses the knowledge of the knowledge basis.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, to create a word to take input corpus out of an egg knowledge base, we perform a simple two step approach or first of all we just iterate over all over all documents in our knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So when we take Wikipedia as an ID knowledge base, we first iterate over oil.",
                    "label": 0
                },
                {
                    "sent": "Documents in it.",
                    "label": 0
                },
                {
                    "sent": "Then we replace all entity hyperlinks which occur in a document.",
                    "label": 1
                },
                {
                    "sent": "So let's have a look at the example.",
                    "label": 0
                },
                {
                    "sent": "Here we have a short Wikipedia sentence that TS has been a tourist attraction in New York City for over a century.",
                    "label": 1
                },
                {
                    "sent": "We have 4 surface phones in it.",
                    "label": 0
                },
                {
                    "sent": "TS, Tourist, attraction, NYC and century and each surface form is already linked to another entity, or Wikipedia or tickling.",
                    "label": 0
                },
                {
                    "sent": "So TS is linked through the Wikipedia document.",
                    "label": 0
                },
                {
                    "sent": "We could time square central century on the other side as link to the Wikipedia or Arctic wiki century.",
                    "label": 0
                },
                {
                    "sent": "So here we replace the surface forms with the entities identifiers and the basically the second step we further remove all non entity words and punctuations.",
                    "label": 0
                },
                {
                    "sent": "So as a result we obtain an entity list which is in the same order or with entities which are in the same order as given in the original Wikipedia document.",
                    "label": 0
                },
                {
                    "sent": "So the location of the entities and the document is remains the same.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Create.",
                    "label": 0
                },
                {
                    "sent": "I've worked with input corpus out of an RDF knowledge base.",
                    "label": 1
                },
                {
                    "sent": "We regard the RDF knowledge base as an undirected graph.",
                    "label": 1
                },
                {
                    "sent": "This is very important on this undirected graph.",
                    "label": 1
                },
                {
                    "sent": "We perform a random walk which randomly moves from one entity to another entity.",
                    "label": 0
                },
                {
                    "sent": "Here, via it's connecting relations.",
                    "label": 0
                },
                {
                    "sent": "So let's have an example.",
                    "label": 0
                },
                {
                    "sent": "First of all, we determine an obituary entity in our graph which serves as a starting point.",
                    "label": 0
                },
                {
                    "sent": "Here we select the entity.",
                    "label": 0
                },
                {
                    "sent": "Berlin is starting point at the same time we write.",
                    "label": 0
                },
                {
                    "sent": "The entity Berlin in two hour.",
                    "label": 0
                },
                {
                    "sent": "Still empty where to wake input corpus in the next step we randomly choose an idiot sent entity of Berlin.",
                    "label": 0
                },
                {
                    "sent": "This is Germany.",
                    "label": 0
                },
                {
                    "sent": "Both entities are connected via the capital of relation.",
                    "label": 0
                },
                {
                    "sent": "In our RDF knowledge base, so we proceed and randomly choose another entity.",
                    "label": 0
                },
                {
                    "sent": "Here, Angela Merkel.",
                    "label": 0
                },
                {
                    "sent": "Angela Merkel is connected to Germany via the leader relationship.",
                    "label": 1
                },
                {
                    "sent": "So basically we can continue this procedure.",
                    "label": 0
                },
                {
                    "sent": "Many times, but to prevent getting stuck somewhere in the graph we Additionally integrate chump probability which allows us to jump from any node in the graph to any other node.",
                    "label": 0
                },
                {
                    "sent": "So here in this example we randomly jump to the former German actor, wanted blue from Angela Merkel.",
                    "label": 0
                },
                {
                    "sent": "We assume here that both entities are not related and are located somewhere else in the graph and from here on we continue as previously explained.",
                    "label": 0
                },
                {
                    "sent": "We select another entity influencer.",
                    "label": 0
                },
                {
                    "sent": "So here I have to note that we have presented two algorithms that generate word two VEC input corpora, and finally the generated corpora can be concatenated.",
                    "label": 0
                },
                {
                    "sent": "Becausw both corpora contain the same, basically the same entities on the one hand we have Peter entities as output and on the other hand we have Wikipedia entities output and we all know that both kinds of entities entities can be matched to each other.",
                    "label": 0
                },
                {
                    "sent": "So overall we generate one huge training corpus for where to Vic.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This brings me to our second contribution.",
                    "label": 0
                },
                {
                    "sent": "In my work, the entity disambiguation algorithm.",
                    "label": 0
                },
                {
                    "sent": "The algorithm accepts a document with one or multiple surface forms in it, which should be disseminated.",
                    "label": 0
                },
                {
                    "sent": "And of course, we use the previously constructed disambiguation index and this example.",
                    "label": 0
                },
                {
                    "sent": "Also used a short text fragment, the TS has been in New York attraction for over a century.",
                    "label": 1
                },
                {
                    "sent": "Here we have two surface forms which should be disintegrated.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, the first step is to create a disambiguation graph.",
                    "label": 0
                },
                {
                    "sent": "And here we first have to generate a set of candidate entities for each surface form.",
                    "label": 0
                },
                {
                    "sent": "We do this by matching the surface form with the stored entity labels in our disambiguation index, and if there is an exact matching between them, we use the set of entities that exactly match the surface form as candidate set.",
                    "label": 0
                },
                {
                    "sent": "And if no.",
                    "label": 0
                },
                {
                    "sent": "Or if no candidate entities are available.",
                    "label": 0
                },
                {
                    "sent": "After the exact matching step, we use the trigram similarity approach that is used in the distace disambiguation system.",
                    "label": 0
                },
                {
                    "sent": "And the second step.",
                    "label": 0
                },
                {
                    "sent": "We create a complete directed keipert it graph which has the constraint that we remove the edges between those entities that do not belong to the same surface form.",
                    "label": 1
                },
                {
                    "sent": "So here we removed the entity at the edges between New York City and New York State because both entities belong to the same surface form New York.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the edges are weighted.",
                    "label": 0
                },
                {
                    "sent": "And here we use the normalized semantic relatedness score between an entity pair.",
                    "label": 0
                },
                {
                    "sent": "So we can see here the transition probabilities between an entity pair and this third step we Additionally use our.",
                    "label": 0
                },
                {
                    "sent": "Prior probability for each entity as a chunk probability which allows us to jump from any node to any other node in the graph.",
                    "label": 0
                },
                {
                    "sent": "And here we perform a random chump.",
                    "label": 0
                },
                {
                    "sent": "With a fixed parameter, Alpha equals 0.1, which was suggested in the original page rank with priors paper and last but not least, we remove 25% of low probability edges in this graph.",
                    "label": 0
                },
                {
                    "sent": "This is cause we would like to optimize the disambiguation step later on.",
                    "label": 0
                },
                {
                    "sent": "So here we remove the edge from the entity Time Square to New York State, cause this edge has the lowest transition probability.",
                    "label": 0
                },
                {
                    "sent": "So given this disambiguation graph, we apply the page rank with priors algorithm algorithm which computes the Pagerank score for each entity and the entities that provide the highest.",
                    "label": 0
                },
                {
                    "sent": "Patriots score denote the final disambiguation results.",
                    "label": 0
                },
                {
                    "sent": "So here we disambiguate it time square for surface form TS, and the entity in New York City for New York.",
                    "label": 1
                },
                {
                    "sent": "Of course.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We performed an in depth evaluation to compare those are with other state of the art approaches and for that purpose we used cable in its basic version it is integrable, is an easy to use platform to evaluate and compare the outcomes of.",
                    "label": 1
                },
                {
                    "sent": "Any disambiguation algorithms on a set of data sets of all we performed two crucial experiments in the first one, we directly compared.",
                    "label": 0
                },
                {
                    "sent": "Those are two activities.",
                    "label": 0
                },
                {
                    "sent": "This is a recently proposed state of the art entity disambiguation system that.",
                    "label": 0
                },
                {
                    "sent": "Is knowledge base agnostic in terms of RDF knowledge bases, so it can use yoga.",
                    "label": 0
                },
                {
                    "sent": "Two Jago three or DB PDO, and so on.",
                    "label": 1
                },
                {
                    "sent": "As underlying knowledge base.",
                    "label": 0
                },
                {
                    "sent": "And since Actis Wars optimized to disambiguate named entities only, we focus on named entity disambiguation to provide a fair comparison between both approaches, and in the second experiment.",
                    "label": 1
                },
                {
                    "sent": "We Additionally leverage the knowledge located in Wikipedia and compare our approach to other state of the art approaches that also make use of Wikipedia knowledge.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this table we can see the microphone values on seven different datasets.",
                    "label": 0
                },
                {
                    "sent": "Of my approach and the approach of those are.",
                    "label": 0
                },
                {
                    "sent": "The first line shows the result attained when we use DPR, is underlying knowledge base and the second line shows the result when we use Jago three, his knowledge base and finally we evaluated activities on DB pedia only.",
                    "label": 0
                },
                {
                    "sent": "The red bold highlighted values in the table denote the best F1 values on this data set.",
                    "label": 1
                },
                {
                    "sent": "And as we can see, those outperforms agdistis on 6 out of seven datasets by about four to five MW one percentage points.",
                    "label": 1
                },
                {
                    "sent": "In this context, we can say that our work to make embeddings can capture the word to capture the relations in our DB pedia perfectly.",
                    "label": 1
                },
                {
                    "sent": "And further, we assume that the entity embeddings are robust against noisy knowledge base information.",
                    "label": 0
                },
                {
                    "sent": "And the second experiment.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compare those are to the following four state of the art dissimulation approaches Wiki Fire Aida, DB Pedia, Spotlight and Wat.",
                    "label": 0
                },
                {
                    "sent": "On the first line we show the results achieved on the DB Pedia and in the second line we show the results when we Additionally use Wikipedia's knowledge base so we can significantly increase the F1 values by up to 15 percentage points on average across all datasets.",
                    "label": 1
                },
                {
                    "sent": "And I'm happy to say that those are significantly outperforms others.",
                    "label": 0
                },
                {
                    "sent": "State of the art approaches on many datasets.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude, and this worker presented, those are a new state of the art collective entity disambiguation system, and it is a knowledge base agnostic in terms of entity annotated document knowledge bases and RDF knowledge base is it is a simple graph based approach that, based on semantic embeddings.",
                    "label": 1
                },
                {
                    "sent": "And our second contribution rules that we presented how these semantic embeddings can be generated on different knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "For instance, DB Pedia and Wikipedia at the same time and in the near future we would like to integrate the textual context of surface forms to further increase the disintegration results.",
                    "label": 0
                },
                {
                    "sent": "Finally, the code of those are.",
                    "label": 0
                },
                {
                    "sent": "Can be seen and downloaded from our GitHub repository.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention.",
                    "label": 0
                },
                {
                    "sent": "If questions.",
                    "label": 0
                }
            ]
        }
    }
}