{
    "id": "oiyo3sy2qhpg44uo4gdjykdef45wkorz",
    "title": "Kernel Based Methods",
    "info": {
        "author": [
            "Colin Campbell, University of Bristol"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_campbell_kbm/",
    "segmentation": [
        [
            "Yeah so.",
            "I.",
            "Yes, good choice will come.",
            "Well, the kernel parameter I come onto in this talk here.",
            "OK, now there has been some work on choosing the kernels.",
            "In fact, if you go to thekernelmachines.org website, come across these papers.",
            "My experience is it's best to have an idea of what type of kernel should be used.",
            "If you know your data is to be separable, OK, then I would just simply use a linear kernel.",
            "In other words, you don't map to high dimensional spaces simply XI dot XJ.",
            "Now I often deal with gene expression datasets.",
            "I'll talk about that later on.",
            "I know that these extremely high dimensional datasets you've got 30 patients, 30,000 gene measurements stream Lee, high dimensional input space.",
            "Intuitively, I know because you got relatively small data, number of data points in a very high dimensional space.",
            "The linear kernel work, so I would use a linear kernel if you're going to use Linux kernel, you may then think will perhaps it could be not separable.",
            "I just don't know this well.",
            "What would happen when you train the SVM is you.",
            "Would wouldn't get to 0 training error if you choose a linear kernel and you get a zero training error.",
            "And I always have code checked, I get zero training error then.",
            "I know it's an appropriate choice if I suspect it's a linear kernel an I found that the IT can't actually get zero training error.",
            "OK then that would indicate to me it's not separable and I should start thinking about other kernels.",
            "If I start thinking about other kernels then my normal choices to go actually for the Gaussian kernel OK?",
            "Is very well behaved.",
            "Works in practice fine, and that's any other kernel I've ever really used in my life.",
            "Having said that, I think I discussed all the kernels, but I would go for Gauss income.",
            "However, of course, depending on the type of problem, it maybe I wanted to text strings.",
            "OK, in that case I obviously would do an edit kernel or graphs.",
            "In this case, obviously do diffusion kernel.",
            "OK, so in practice I think from experience I would probably know what kernel to use anyway.",
            "Oh yeah.",
            "The funny thing I should mention when when I first started with SVM's I made a mistake in which I ran a program with all the Alpha is equal to 1.",
            "OK, now by choosing my kernel parameter this was with the Gaussian kernel.",
            "I could actually get a reasonable type of test error.",
            "OK, in other words it's giving me what would be called a parsing Windows type solution OK?",
            "I then went on experiment with family era and change my alphas and got a little bit better solution.",
            "But there is a partial Windows solution.",
            "If you got the right value of Alpha will give you a test error which is you know.",
            "Frame will beat it, but it's not far from the SVM solution.",
            "OK, so the parser Windows solution with a Gaussian kernel would be equivalent to what you mentioned are sort of sort of just a hyperplane.",
            "In this high dimensional space suffering two points.",
            "I don't know how it does it, but it's sort of.",
            "Yeah, maybe that answers your question.",
            "Well, anyway, I'll I'll.",
            "Well, I just want to come to the second part and what I want to do in the second lecture.",
            "Is the following.",
            "At first lecture everything I did was really quadratic programming.",
            "OK, what I want to do here is really show you can use other parts of optimization theory.",
            "Indeed we can formulate classifiers using linear programming or nonlinear optimization and semidefinite programming.",
            "Indeed, you can really use a nearly all of optimization theory.",
            "Combine it with kernel methods to come up with all sorts of weird and wonderful things so.",
            "So what we want to show you is a broader class of kernel methods.",
            "And indeed.",
            "The opposite of observation to make is that when I did the SVM, I heard my data in the form of a dot product OK, and then I use the kernel trick.",
            "Indeed, there's a whole range of non SVM things where I come across my data is in the form of a dot product and I concur nallies.",
            "In other words, there's alot of things I can Colonel eyes.",
            "I can take PCA and I can do kernel PCA or independent components analysis.",
            "ICA kernel eyes that.",
            "Guys, I see a perceptron.",
            "You probably come across Perceptron.",
            "I can do the kernelized perceptron, a very crude algorithm I did years ago was something called added for an algorithm which is just a fast variant on the perceptron.",
            "The data appears in the form of a dot product XI.",
            "XJ kernelized.",
            "It came up with the kernel out of Tron.",
            "You can actually try it this afternoon.",
            "I give you the code for it.",
            "So 40 line thing has a Colonel in there so you can handle non separable data.",
            "Put a soft margin if you want.",
            "But it is a kernel algorithm.",
            "The kernel out of Tron is, which you can try.",
            "This afternoon is a good example of something which is not an SVM, but you can use a kernel kernels with it.",
            "So I'm really saying now that in fact the SVM which introduces this earlier is 1 example for much bigger class of kernel based methods, OK?",
            "So I can use other other methods from optimiza."
        ],
        [
            "Theory and.",
            "I can do other things apart from classification of Gresham.",
            "OK, to illustrate that I'll be doing novelty detection.",
            "OK, some of some of the topics I mentioned in the second talk were actually research stuff that I did in the past.",
            "The novel detector was a projected with Christine Bennett and.",
            "There's also other things I sort of kernel machines which I miss out.",
            "There is a thing called the Bayes Point Machine which I did with Ralf Herbrich Centora Grapel some years ago, which is actually better than a support vector machine.",
            "I might just say something about it right at the end.",
            "It's rather slow and awkward to train.",
            "It's a problem, but it consistently outperform a support vector machine, so I say here all men are different ideas from optimization theory can be used, and there's a huge range other things."
        ],
        [
            "Apart from classification regression and I want to comment also on a lot more about actually how you do the learning model selection.",
            "I've introduced the kernel in some cases or kernel parameter like in accounting kernel.",
            "But what value should you choose for that kernel parameter?",
            "So I want to do that.",
            "Then again a very interesting but little bit more recent errors research is composite kernels OK in other words?",
            "I may have different types of data, and each of those different types of data is encoded as a kernel OK, and what I want to do is classification using multiple types of data.",
            "OK, give an example of this.",
            "One of my collaborators, marjana me, was very interested in the problem of protein folds.",
            "OK, it's a sort of classification type problem.",
            "You had six different types of data.",
            "OK, some could be encoded as linear kernels, Gaussian kernels.",
            "I think one was a graph kernel.",
            "Really quite different types of data now.",
            "He was therefore able to derive a composite kernel data Fusion and he.",
            "Did the learning machine using the composite kernel.",
            "And it was 11 percentage points better than the best type of data he could use.",
            "Of those six classes.",
            "So there are six types of data.",
            "If you chose that data set, which gave you the best performance found out what is test error is then using the composite Colonel was 11 percentage points better?",
            "OK, so obviously if you've got multiple types of data OK then using composite kernel is better than just using one type of data.",
            "Only recently I've been I said at the beginning I work in cancer informatics.",
            "What we are interested there is actually using multiple types of data to improve prediction on patients.",
            "OK, so we have many types of measurements from the patient and we should use all of these to try and do the best prediction possible."
        ],
        [
            "So I'll talk about linear programming, nonlinear programming, novel detection, and then again, I want to talk about very interesting variants.",
            "Don't forget all of this very interesting variants on kernel methods.",
            "In particular what I did up to now is what you called passive learning.",
            "OK.",
            "Passive learning would be.",
            "I could present some data to you.",
            "You're learning Machine, learns this data and then tries to predict on new data points.",
            "Active learning is.",
            "This is a project that did with Yellow and Alex Mueller out of learning.",
            "Is the learning machine actually poses queries to to you to try and optimally learn a task OK?",
            "Other words, perhaps I'm trying to teach arithmetic to you.",
            "OK, I would say 2 + 2 equals four 3 + 3 = 6.",
            "You learn these.",
            "You set up your learning machine and then you hope to generalize to new instances.",
            "I might give you arithmetic.",
            "Active learning would be that I say 2 + 2 = 4 and then somebody says, well, OK, What's the answer to 3 + 7?",
            "I said 10 and somebody else sticks up their hand, now opposing maximally informed queries.",
            "You can learn the rule more efficiently now that you can easily do it in the context of kernel methods are illustrated later on is projected with Alex and Mellow.",
            "You may think what real life applications I'll show you briefly at drug application, where if you like what the learning machine is, presents a molecule.",
            "An the experimenter says yes, it's biologically active or no, it's not, an it can optionally maximally find the drug using active learning."
        ],
        [
            "Right, so we hope to cover all of these and other things as well, right?",
            "Let's briefly do some of these."
        ],
        [
            "20 minutes, right?",
            "So a linear programming approach, so rather than quadratic programming, perhaps you don't know quadratic programming, but you do know linear programming.",
            "Can you do that?",
            "This approach was really evolved by Ovi Mangasarian.",
            "Anne Christine Bennett rents are Polytechnic Institute Ann.",
            "Just say, here Colonel substitutions not restricted to that SVM framework.",
            "I gave earlier.",
            "Large range of methods can be kernelized.",
            "OK, when suddenly someone day somebody realizing kernelized everything."
        ],
        [
            "Huge explosion of papers where people kernelized everything right?",
            "So you can kernelized simple neural network learning algorithms such as perceptron, kernel, perceptron, kernel, min over kernel and it ran.",
            "I did that right back in 98 and they can handle non inseparable.",
            "They sets.",
            "You can try to call out from this afternoon principle kernel, PCA, kernel, ICA, Colonel, fish and chips.",
            "You know sort of goes on them so you can kernelized other types of learning machine based."
        ],
        [
            "Machine somebody can ask me about that later.",
            "I can member it works so.",
            "Sample of linear programming.",
            "This is due to Christine Bennett and I did this this L or W for memory.",
            "Somehow it slightly wrong, but you minimize someone.",
            "I Alpha now linear and Alpha and this term, so this looks rather similar.",
            "Well sort of sort of expressions you find before, but importantly, it's linear programming type problem.",
            "You can do both classification and regression."
        ],
        [
            "Using linear programming and that linear programming task is subject to constraints that give the constraints here.",
            "OK subject to this alphas, positive stack variables previous OK.",
            "I've experimented this, I think with this some time ago and slightly slower than the QPS VM, but it's quite robust and gave similar sorts of performance OK. Yeah.",
            "Well, it's not so.",
            "It's not so clear where this comes from.",
            "Is.",
            "Well, it probably was clear to Christina.",
            "Probably need to look back through our papers.",
            "But the one novel detection which I did with Christina, she comes up shortly again, there's no form.",
            "There's no primal formulation, straighter dual formulation.",
            "OK, so I can't justify it."
        ],
        [
            "That's why I say about that.",
            "OK, so you can do linear programming, classification and linear programming regression OK. You can do other things apart from classification regression.",
            "Now show you novelty detection or what you might call a one class classifier.",
            "Indeed, the datasets are used.",
            "One was condition monitoring of machinery and I use the data set by Keith Ward.",
            "New might be around today or he was around at the coffee break.",
            "So explain where this problem comes from and then I'll show you a scheme I did with Christine Bennett for novelty detection.",
            "I'll give you.",
            "A medical example of this.",
            "In many real world problems, the task is not to classify but for detect, but to detect novel or abnormal instances.",
            "I'll give you an actual problem which was done in Bristol University, not by myself but by PhD student in computer science he was handling a data set for something called an acoustic neuroma, which is a type of tumor pops up in the brain, largely benign.",
            "It unfortunately puts pressure on the brain and can kill the person, and it's relatively rare.",
            "In the UK Puranam, there's about 150 cases of this particular type of tumor.",
            "Now they were interested in building.",
            "Typically you detect it by doing NMR scan of the brain and then you try and spot it.",
            "They wanted to build a classifier that would actually find obviously the presence of this acoustic neuroma map even had a case where the neuron was present in Bristol Royal Infirmary wasn't spotted while spotted too late and the patient died.",
            "So you want to.",
            "Detect that rumor, certainly, and it attaches to cranial nerve so it tends to introduce an asymmetry in your NMR scan.",
            "OK, so here's a head cranial nerve sits on one side.",
            "OK, so of course the first point to do was to segment your image OK, you got your regions in the image, perhaps derived through descriptors or whatever.",
            "Use these to actually build a classifier that would say, not a neuroma not in your OMA.",
            "Yes, a new Roman.",
            "Now that's fine.",
            "It sort of worked, but you could exploit the fact that when it was present in interesting asymmetry, so symmetry took you along way.",
            "But unfortunately I said just now in the UK that's around about 150 cases per annum.",
            "Is quite easy, therefore to have a shape for tumor, which won't be anything similar to anything.",
            "Your training set.",
            "OK, it's an unusual shape and how it's growing, it's really quite dissimilar to anything you've seen before, and your classifier is going to miss it.",
            "OK, and you must be able to spot it well.",
            "This introduces the novel detector.",
            "In the novel, detector really tries to spot something which diverges from normality.",
            "That's it OK.",
            "In other words, what they developed a system was your first passes to try and classify everything in the image.",
            "However, your second passes to use anomaly detector, which will highlight the vergence from normality.",
            "So your classifier may not be may be unable to classify an unusual shape, but your normal detector will still say it's novel.",
            "It's abnormal and that second stage will pick up something which could be.",
            "Your medic will look at and probably recognize this abnormality.",
            "Now there are many applications that enable detection.",
            "Of course, in medical diagnosis you're trying to really spot divergent from normality in a patient all the time.",
            "But you'll be perhaps interesting novelty detection for condition monitoring you have your machine.",
            "This is a project we did with Berkley Power Station.",
            "You have a shaft of a nuclear power station.",
            "You have a lot of data for the normal operation of the shaft.",
            "You want to spot an error coming up, not abnormal behavior or novel behavior, because if it comes up and your shaft goes down in nuclear power station, you lose 1,000,000 pounds every 10 minutes or something.",
            "So you want to be able to do your normal detection very well.",
            "OK, so that's the most."
        ],
        [
            "Patient detection, so there's actually two approaches you could have to double detection one.",
            "Yeah.",
            "They can.",
            "It's a big storage, normal detection like I only give you an impression here.",
            "There's two ways of doing it, which I'm going to give you.",
            "In fact, if I did really did Noble detection, I wouldn't do it.",
            "Using the scheme about to give you, I'd actually use probably some ideas that Mark Geronimi head again so he actually has used it in practice.",
            "There's actually two ways in which you might approach not detection.",
            "One is to estimate the support of a distribution, which is I want to give you now.",
            "The 2nd is a probabilist.",
            "The way I would actually do it is a probabilistic sort of density estimator.",
            "Now what do I mean by all of that in plain English?",
            "Um?",
            "The way I give you now is a Noble detector, which it did with Christine Bennett.",
            "Anna really, I'm just trying to illustrate.",
            "You can do things beyond classification, regression, but estimating the support is this is your data OK?",
            "Estimating support is you find a boundary round your data and everything in here is regarded as normal and anything outside is regardless abnormal.",
            "That's estimating the support.",
            "Alternatively, what you do is here's your data, perhaps looking at it sideways and you try and estimate a probability function which goes over that data.",
            "OK, so here is the probability is decreasing away and say it's more more abnormal that is harder to do than this OK?",
            "So those schemes are more modular schemes of what I would actually go for in practice, but I'm just wanting to illustrate here quite quickly so you can do this sort of novelty detection, and there's actually two schemes using quadratic programming burner, short cough and tax, and urine, and I just give you the LP wave."
        ],
        [
            "Doing it, which I did with Christine.",
            "Now this one, rather like Christine's linear programming.",
            "You actually formulate it directly in the feature space.",
            "You don't have this picture in input space."
        ],
        [
            "A map across I'll show you the essential ideas.",
            "In fact, it uses a Gaussian kernel.",
            "And I want to note one thing about the Gaussian kernel.",
            "Before I say something about this picture, I give here on this slide.",
            "Now Garrison kernel for getting the kernel parameter looks a bit like this.",
            "OK, well, perhaps I put the kernel parameter in.",
            "OK, that was it.",
            "Just a straight Gaussian.",
            "Now this was my K OK and that's simply equal to 5 X .5 X prime OK. Now let X equal X prime OK. Then obviously this will be easy to zero with just the one.",
            "OK, so if I make this choice OK, 5X dot 5X will be one.",
            "What I'm saying to you is if I make the choice of a Gaussian kernel.",
            "Then actually what happens is you map your data to infinite dimensional space in which every point actually sits on the surface of a hypersphere.",
            "OK, that's it's how it's represented in this feature space.",
            "Here is this hyper sphere OK?",
            "And indeed, if I were to give you a bunch of data where I'm wanted to estimate the support, those data points will sit somewhere on that hyper sphere.",
            "OK, of unit radius.",
            "Here they are a little blobs.",
            "The way to estimate the support of the distribution, but I did with Christine Bennett in the NIPS paper was we basically suck the hyperplane onto the data.",
            "So it sort of impales itself.",
            "And what we call the support objects OK. And we got nips paper out of it, but I reckon now the scheme was bit crew."
        ],
        [
            "So anyway, how do you do this?",
            "Well, it's a cheap.",
            "By minimizing this, OK, you notice linear and Alpha."
        ],
        [
            "Subject to constraints and the constraints are as given OK.",
            "So it's just."
        ],
        [
            "A linear programming constraint.",
            "Linear programming problem OK.",
            "I don't really, I won't say."
        ],
        [
            "About the bars, you can introduce off margins.",
            "Here is a very simple example of what's going on.",
            "I've sprinkled data as data points here and here.",
            "Let's put a boundary around the data OK, and so anything here would be viewed as abnormal.",
            "But if you're inside this region here, it views it as normal OK?",
            "Now."
        ],
        [
            "That's a little bit of a crude illustration.",
            "If you introduce a soft margin, you can allow for some points to be outside the boundary OK, after all, you have to be aware that if I'm trying to.",
            "If I'm trying to learn abnormality OK, and presumably therefore I'm trying to train off normal data and then on new instances I try and list them as normal or abnormal.",
            "I do unfortunately have the possibility that my normal data may contain some abnormalities I may not know that, in which case I may want to lessen the influence of such points and these might be abnormal points in my training data, so I'm wanting to build a model of the normal to try and spot the abnormal.",
            "Unfortunately, training data for my normal."
        ],
        [
            "We have problems with it now.",
            "This was a better type of kernel to use and which would put a much better sort of boundary around the data.",
            "You can make the."
        ],
        [
            "Scheme a lot more elaborate by actually make it probabilistic and have a problem.",
            "See going away from this boundary.",
            "OK then it makes a much better sort of scheme which makes a lot more sense.",
            "OK. Well, that's all I'm going to say about."
        ],
        [
            "No detection, I really want to illustrate there are other things apart from classification regression.",
            "Right, moving on not my next topic.",
            "I want to come to is passive learning versus active learning.",
            "A relatively little bill described in textbooks on kernel methods, but it's interesting little topic in itself that I did with Alex Smolen hello.",
            "So I actually define is just now.",
            "I said there's two ways in which you might learn a problem.",
            "Up to now, everything I've given you has been passive.",
            "Learning the learning machine takes us are most learns from these types of generalized new instances active learning the learning machine poses queries or questions.",
            "The Oracle or source of information.",
            "So if I have an efficient query learner, then it will learn the."
        ],
        [
            "Problem much more efficiently now.",
            "Actually I want to.",
            "First of all, point out why.",
            "Why am I introducing this in the context of kernel methods?",
            "Well, let me go back to this scheme here.",
            "I said that these particular data points here were non support vectors.",
            "When I build the final hypothesis, they don't appear to have Alpha equals zero in a hypothesis.",
            "In other words, I never need to know about them.",
            "I don't need to learn the non support vectors, only these guys are support vectors will actually be influential.",
            "In other words, if I have unlabeled data, OK?",
            "It costs me to label the data.",
            "Then if I have an efficient learning scheme, an active learner, it will try and go for the support vectors and never bother learning the labels of these guys here.",
            "OK, because they're not going to have any influence in a final final hypothesis, they don't need to know to know them, OK?",
            "Now indeed, query learning active learning the various different approaches and subject.",
            "There are several strategies possible, and what the first one I give here called membership queries.",
            "And if you like in this case, the algorithm selects unlabeled examples for the human expert to label human example, this could be I wanted to spot Postal codes.",
            "OK, now every the every digit written down by person on an envelope would have meant something OK and your human expert has to come along and label alot of these and then you try and learn the learning machine learning season.",
            "Hopefully you have."
        ],
        [
            "3 zip codes or Postal code switch is a nine.",
            "OK, that's a 9 very badly written.",
            "OK, but it has a meaning, whereas if I create a query I may ask what does that mean and of course has a meaning.",
            "So creating queries is a different task for membership queries.",
            "I'll be doing mem."
        ],
        [
            "Queries here.",
            "OK, we did a theorem that you can prove in the worst case that.",
            "The number of queries you are asked actually equals a sample size.",
            "In other words, you don't make any gain from using active learning but such."
        ],
        [
            "This is don't appear in much in practice.",
            "OK, I'll show you what the general there for active learning is better than passive learning.",
            "The theorem we proved is at that point the best membership queries.",
            "The best point to request the query is."
        ],
        [
            "Point line closest current."
        ],
        [
            "I'm playing OK, I'll show you what exactly I mean.",
            "This was the story I gave you for the support vector machine at the beginning.",
            "OK, and I said actually I really don't need to ever ask the for the labels of water going to turn out to be."
        ],
        [
            "Support vectors.",
            "Anne.",
            "The IT turns out we proved a theorem that the best points to query are those points which are closest to current hyperplane.",
            "They're maximally ambiguous.",
            "I think I could probably summarize this story with."
        ],
        [
            "Following picture OK now.",
            "This is some sort of intermediate stage of my algorithm, in which I've have some points which are labeled.",
            "I've asked what Sis and etc plus and what's this and it said a minus OK and those circles.",
            "Oh, circles are unlabeled data points, setting off power.",
            "What would that be due to?",
            "Short intermission technical interlude what happened?",
            "Did it?",
            "Just clear the ball burns down.",
            "Take time.",
            "I'm really doing this table one.",
            "OK, right?",
            "OK, so.",
            "So this was it, yeah?",
            "Well yeah, but what are sort of commenting was if I did this separable data these guys were turn up with our free equals zero.",
            "OK there are non support vectors, they don't influence where the hyperlink should be.",
            "If you like I might never need to either learn that point or even know its label.",
            "Yes, yes, I'm going to sort of answer that point shortly.",
            "Yeah, I'll answer it.",
            "Actually when I come on to some plots now, this is an intermediate situation.",
            "These are positive.",
            "Negative an unlabeled so I can ask for label these guys.",
            "OK, what the theorem we did establish with the best point to query is that unlabeled point, which is closer to current hyperplane.",
            "Here it is OK.",
            "This will be the one I'll go for an ask for this label.",
            "In fact again is saying something pretty obvious.",
            "These points are maximally ambiguous.",
            "OK, so go for them.",
            "They're going to influence with a high place, should be so often when you prove this dreadfully long theorems, you arrive at something which is perfectly clear from the beginning, right?",
            "So?",
            "So that's what the theorem said.",
            "Indeed, what the theorem says it was always worth querying these points within the margin band.",
            "However it was actually inadvisable to query these points here.",
            "You asked just now the reason why is 'cause.",
            "If this is negative, it's OK.",
            "It's going to have no effect.",
            "It fits in this supposed to be negatives here fits in with the story, but in effect I did not need to know it was negative.",
            "OK if it's negative.",
            "And these are negatives.",
            "It has no influence.",
            "So you know, don't make any game from that.",
            "Suppose that was a positive.",
            "Then it's going to have some sort of disastrous impact.",
            "And this whole thing, because these are all positives.",
            "This is a positive.",
            "These are negatives within this 2 dimensional domain I've got looks a bit of a mess in a high dimensional domain, it's going to really push my hyperplane into some totally different orientation.",
            "So actually I've got two outcomes for these guys.",
            "I'd have no influence or be they are most likely to be outliers because they would be totally wrongly positioned according to all the other data.",
            "OK, So what the theorem proved was yes query these guys.",
            "Once you empty the margin band stop because it's most likely it's a high probability that the other points are going to query are going to be uninformative or wrong, in fact."
        ],
        [
            "I'll show you that with some plots coming up OK."
        ],
        [
            "I'll just give you a silly toy example.",
            "OK, now this was a thing called the majority rule.",
            "Majority rule is a quite random zeros and ones, and the label is a one.",
            "If there are more ones and zeros and it would be a -- 1 if there more zeros and one little trivial rule.",
            "OK, majority rule.",
            "Now your learning algorithm does not know what the rule is.",
            "It simply has examples presented to it and I have the two instances.",
            "First of all, I have a random xamples presented to it, OK. Alternatively, it can actually ask what's the label of given examples.",
            "OK, now if you do passive learning eventually after run about 200 examples.",
            "It's relatively simple rule to learn OK. Apps had 10 inputs after Turner's amples it's found it with random passive learning.",
            "However, after 60 well posed queries OK, then it's got the role perfectly.",
            "OK, so that's a good example of passive active learning winning over passive learning."
        ],
        [
            "Now.",
            "The question was posed, how do we know when to stop asking queries?",
            "There's no use doing query learning, I just go on and on several several criteria use.",
            "If you have noise."
        ],
        [
            "Status, so you'd have to sort of know that days is noiseless.",
            "Then the simple criterion to use is that I predict ahead what the label should be, and if I'm finding my prediction always agrees with what that label was, then I've learned it and I notice stop in fact, on my noiseless majority rule example I just gave you is making mistakes some of the time it thinks is going to be a plus, but it's actually a minus, so it makes mistakes when it's got to 60.",
            "Is learn the role perfectly.",
            "It then will predict A plus one and it is a plus one or minus.",
            "One is a -- 1.",
            "It makes no further mistakes so it knows to stop at that point.",
            "OK, so you do have a criterion in the noiseless case that you know when you should stop.",
            "OK, I unfortunately don't give the noisy case here.",
            "The noisy case that we established was the theorem I just explained just now.",
            "It's in fact we did it.",
            "I did it on a number of examples.",
            "And.",
            "Basically, I keep asking queries until the marching band is empty and then I stop if you actually go on off asking queries.",
            "It turns out you Start learning some outliers and typically you go through a minimum when the margin band is empty.",
            "OK, we did this with the heart dates from UCI various other datasets, all noisy and actually the minimum.",
            "The test error was actually at that point when the margin band empted OK.",
            "The Y axis is is going to be number of errors.",
            "OK, so it makes when it's learned those 60 OK it had made 25 or 24 it had predicted ahead what the label should be.",
            "An it was wrong.",
            "OK, so it knew had to go on learning OK. Well, it's got to.",
            "Here is no longer making errors.",
            "OK, it's got.",
            "It's got its rule OK, predicts ahead and the answer of the unlabeled point agrees with what it should be.",
            "Yeah it is.",
            "It's accumulative.",
            "It's accumulative error OK?",
            "Um?",
            "Well, I think it's the way it actually searches the space.",
            "I don't really know why this is a noiseless case.",
            "I think if we do a noisy case you might get something bit more exponential, OK?",
            "This one was original.",
            "At Test error OK.",
            "Zero test error here and so on so.",
            "Yeah, yeah, I don't know why for this noise it's probably to do with its noiseless problem.",
            "So I don't really say much about."
        ],
        [
            "What you do when you have noise just give you a little example from drug discovery.",
            "Each compound described by vector 139 features groups.",
            "At two types of sperm."
        ],
        [
            "Friends round one and Round Zero Round zero datasets over 1000 chemically diverse examples 39 are positive.",
            "Anne."
        ],
        [
            "And or active.",
            "And if you do the passive learning, you have the black line and the red line.",
            "If you have active learning.",
            "So in this case it is that the learning algorithm pick a molecule it knows something about its characteristics and says this is biologically active or not, and it rapidly closes in on the 39 active compounds.",
            "You can see what's going on if I use passive learning then my hypothesis.",
            "My current hypothesis is not informing me.",
            "If I'm using active learning, what's happening is I learn something.",
            "I update my hypothesis.",
            "My hypothesis is a little bit better, so the query at last it's going to be a little bit more accurate, so it gains from the new information hypothesis improves a little bit more, and then it posts is even better query.",
            "So it sort of is running away from the active learning hypothesis.",
            "The passive learning hypothesis, which is never altered Journal learning process just simply learn some stuff.",
            "OK, so that's a drug discovery example.",
            "This was done by Gunnar Rage, not me.",
            "That's."
        ],
        [
            "At least one site bounded right?",
            "So that's active learning versus passive learning.",
            "Now I want to say a little bit about training SVM's.",
            "So we've done linear."
        ],
        [
            "Programming quadratic programming.",
            "There's a lot of quadratic programming routines out there.",
            "You can get packages like Minasan local Aku.",
            "You can also download some mathematicians have done very nice implementations of quadratic programming.",
            "Classic algorithm could use as conjugate gradient.",
            "Now there is one problem.",
            "If user kernel methods in practice let me suppose I have a big data set.",
            "This is my kernel KIJ and I ranges one up to M as oppose.",
            "I've got a million.",
            "Samples, that's going to be a whacking great big kernel.",
            "So what do I do about that?",
            "I might have a problem loading the kernel into the.",
            "Into the loading the data into a kernel OK.",
            "There are actually several ways to handle such a problem."
        ],
        [
            "One is the kernel components are valuated, discarded during learning the kernel auditor.",
            "I gave you give you this afternoon.",
            "You can do that, but it's slower than SVM and it's not quite as accurate as a less relevant thing, but it can do that.",
            "You can handle arbitrary sized datasets.",
            "Another one is the working set method in which you haven't involving subset of data.",
            "If you like it.",
            "It's a subset.",
            "Here it is.",
            "OK, it learns it's found these are support vectors.",
            "These guys are non support vectors.",
            "It chucks out it then brings in some new data having kept the original support vectors only and then learns the new data chucks out the non support vectors takes in your next chunk.",
            "OK that is a working set method in which you have a evolving subset of data.",
            "Unfortunately still won't help you in certain cases.",
            "The next one to use this algorithms which explicitly exploit the structure of the problem OK. And I just want to."
        ],
        [
            "Actually lead through to something called SMO.",
            "OK, now chunky and decomposition are really to do with that evolving subset of data which I just gave you just now.",
            "So I see here the support vectors are found and retained and all other data points are discarded.",
            "OK, so that's this chunking method or folding subset?"
        ],
        [
            "Of data OK. Anne."
        ],
        [
            "That's what I'm going to say about that."
        ],
        [
            "Anne.",
            "Right, they're going to want to come onto is decomposition and small algorithm now.",
            "This was done by John Platt at Microsoft in Redmond and it's a limited case of decomposition.",
            "Indeed, when I said here you can do chunking an evolving subset of the data.",
            "OK, so you never saturate, you never have a kernel which is so big you can't put it into memory.",
            "So the limiting case of.",
            "Chunking or decomposition is actually just have a subset of two.",
            "OK, I can't go below 2 samples because in fact I can't do the learning task, but I can go down to two.",
            "In other words, just to choose two samples at a time, and my kernel is only simply 2 by 2.",
            "The reason why I inclined to go for two OK is that I can actually count have an analytic expression for telling me what the Alpha should be when I have a subsets of two, so the small set of parameters which can be optimized with each iterations is 2.",
            "That's because of this constraint OK?",
            "So I have a very minimalist kernel and this is the idea behind small algorithm which is."
        ],
        [
            "Very good algorithm to use in practice.",
            "OK, whoops.",
            "I say here possible derive an analytic expression with relatively few numerical operations and.",
            "But I shouldn't seem to say much more about SMO algorithm itself than than that it can be applied to classification, regression, song and novel detection even.",
            "John Platt is at Microsoft.",
            "He can't release code as a result, so but I think somewhere on the web you can get somebody was implemented SMO in C, But.",
            "It's essentially involves the following sort of idea.",
            "At each stage, only consider tiny little two by two.",
            "Kernel solved this problem and it's within a bigger loop.",
            "OK, so the solution of the Alpha for this sub problem is very very quick and it uses a sort of fact will be able to handle data sets of any size OK.",
            "Right, that's why I say about SMO.",
            "I think I might skip further algorithms and."
        ],
        [
            "Do do model selection.",
            "I might also skip kernel massage ING, right?",
            "So there's one further top to further topics I'm covering my last moments.",
            "One is model selection now this is important.",
            "An important issue is a choice of the kernel parameter.",
            "Indeed, I had a kernel kernel on the board just now an I had a kernel parameter which is Sigma in that kernel or D at the polynomial kernel.",
            "What actually should be the values of those kernel parameters?",
            "If it's poorly chosen, then I'll overfit the problem, leading to poor generalization OK?"
        ],
        [
            "I think I illustrate that in the next slide, yes, this is a mirror symmetry problem.",
            "A noiseless problem, bit like the majority rule.",
            "If I make one choice of my poor choice of my Sigma and getting a rather bad test error 11% and make the best choice of my Sigma and get 5% error which looks quite good.",
            "If I go in the opposite direction it takes actually longer to overfit than sort of under fit.",
            "If I go off in this direction then my Gaussians or my Gaussian kernel of a broad things by Sigma is very broad.",
            "And it has a relatively hard task finding good solution with a broad Gaussians.",
            "OK, so I have a sort of point like Gaussians of parsing window type solution.",
            "Here really doesn't work very well here.",
            "Very broad Gaussians has a problem, less so than here, but there's this optimum value of Sigma that I really want to know."
        ],
        [
            "Now the best value of that parameter could be found using cross validation and practice.",
            "I think I would use cross validation, however, cross validations wasteful of data given example this which I will show you in my last minute, which is Wilms tumor cancer which predict relapse versus non relapse.",
            "So children's tumor we only had 29 examples so I can't be wasteful with data.",
            "OK so I can't do a validation study so in certain cases.",
            "If I can do cross validation, I would do it.",
            "If I can't do it for some reason then I would fall back on these theoretical results which I'm about to present.",
            "Yeah.",
            "Well yeah, but leave one out is really a test test statistic I'm saying.",
            "Well, I could I could do the following for 29.",
            "I got a really horrible thing that.",
            "I have training data validation data defined my Sigma and then finally a set of data which is going to be my test performance.",
            "I can.",
            "Yeah I can do with 29 I could do cross leave one out so I'd have 28 plus one with the 28.",
            "I've gotta do a cross validation within that as well.",
            "It's a little bit awkward.",
            "You could do it, but it's sort of awkward.",
            "Also leave one out, you have the bias variance sort of issue coming in, so you might be tempted."
        ],
        [
            "But to do that I want to show you that a number of schemes have been presented to actually try and find the kernel parameter from the data without recourse to cross validation.",
            "OK, I wanted to point out that these."
        ],
        [
            "This now there are a number of these.",
            "The span rule of fat Nick and there's a nice scheme by Peter Sallick, one of the simplest ones in practice is tossed in Jochims theorem which is in acnl surround.",
            "About 2000, OK?",
            "The theorem is the following.",
            "Now I'm going to cut down the theorem A little bit by doing the following OK, the theorems formally stated here.",
            "B squared is an upper bound on the kernel.",
            "OK, now I'm going to choose a Gaussian kernel, a Gaussian kernel for getting the stuff at the front is E to the whatever, and so the maximum I could have is going to be one.",
            "OK, it's like this Gaussian kernel I'm going to say is 1.",
            "Forget it getting the normalization OK.",
            "So let's suppose it's a one.",
            "OK, that's this.",
            "So be I'm going to set to one.",
            "I'll also do the hard margin case, which is going to be XI.",
            "The slack variable is going to be zero.",
            "OK, just to cut it down.",
            "In that case the theorem would be the number of points such that 2A is greater than or equal to 1 over the sample size.",
            "OK, so how to use this theorem in practice?",
            "Would be for getting us off margin is I would learn I would choose a Sigma value.",
            "I would run my SVM.",
            "Can be done quite quickly.",
            "I would find and in fact say 29 of these data points of the Alphas are.",
            "Two hour is going to be greater than one out of a data set of hundreds, say and then change my Sigma slightly rerun my SVM, find my new alphas.",
            "How many two hours was greater than wild?",
            "Perhaps is 28 / 100 and then I just run it and I've done this in practice and roughly the Sigma value is approximate in the region it should be OK.",
            "So it does work OK in practice.",
            "I'm just saying to you if you can do cross validation, I would do it that way, But if you're a little bit desperate then you can use these bounds OK.",
            "The rough in the right region they looked a bit like the following when I did these simulations.",
            "Here was the when you did this sort of validation study, it went a bit like this, but like my plot previously and I would find that the result coming out of this theorem be a bit like that, it would be off the actual minimum, but.",
            "Not too far off, OK?",
            "Right, so that's the recourse of the desperate.",
            "If you've got too little data.",
            "Right, so there are other theorems I'm going to Miss Kernel massage ING and semidefinite programming and.",
            "Oh"
        ],
        [
            "Do data Fusion briefly?",
            "I talked about these different types of kernels, but wrong.",
            "I think string kernels, graph kernels, etc.",
            "And here's an example.",
            "So here we are.",
            "Data Fusion actually introduced this subject.",
            "In fact, this is one of the big nice things about kernel methods is that they are capable of handling multiple types of data using a composite kernel.",
            "OK, in this case I need to find the beta as well as the Alpha OK.",
            "The first schemes are know about was Steve Garland.",
            "Yes, scandal in 2002.",
            "There's around about 8 methods or six or eight methods and we got one ourselves, not publish where you find the coefficients in composite kernels.",
            "Very nice ones.",
            "Marshalla needed a Bayesian way of doing this.",
            "OK, relative recent paper data integration is in the title in NIPS 2007 or 2006.",
            "Another similar nice schemes as a semidefinite programming way of doing it, which I think I described.",
            "Now.",
            "There's also a paper called multiple kernels by for large problems in general machine learning research around about 2006 burner short cough and Sonnenburg I think was the first author, so there are a number of schemes for doing composite kernels, and it's not even the only way of doing multiple datasets, right?",
            "So I just really."
        ],
        [
            "Again, do it rather briefly.",
            "Kurt Lancret by Informatica no sorry Journal machine learning research.",
            "Protein functional classification using yeast protein."
        ],
        [
            "Five types of data, so this is it.",
            "You got very different data.",
            "Amino acid sequences in a product kernel graph kernel for protein protein graph kernel for jetting interactions as a graph.",
            "We contract another graph.",
            "Kernel expression data.",
            "Real valued numbers."
        ],
        [
            "We use probably linear kernel or Gaussian kernel I think told to be actually did these experiments.",
            "You find that 5 using all this data is used better than using a single kernel.",
            "OK, complex story, but I think what the improvement was from the error test.",
            "Error was from 71.",
            "Maybe I'm doing classification?",
            "So it went from 71 to 85 and so must be a classification error."
        ],
        [
            "Test error so definite improvement from.",
            "From the comment from using composite kernels actually know both the people have been doing composite kernels and I can say one thing that the results presented in their papers look quite good.",
            "I mentioned 11% just now, so I do know that there's a lot of data sets out there where they got frustrated and told me about their frustration.",
            "It's quite common in datasets, particularly bioinformatics.",
            "If I'm using gene expression data.",
            "In fact, within that data set, it really has the information about the graph, so that implicitly in it.",
            "Alternatively, one data set can be a whacking great size and other data can be quite small.",
            "They did find quite a number of sort of cases where what tends to happen is the dominant data set, wins out and forces the other data set.",
            "Just just gets rid of it, but if the data is implicit, one type of data is implicit in another.",
            "OK, so there's a.",
            "It's not a totally rosy stories when I'm sort of saying there so."
        ],
        [
            "There are instances where it's definitely worked.",
            "I sort of gives some schemes here, semidefinite programming, but this is slightly out."
        ],
        [
            "Update by the way, right so?",
            "So as I said, Mark Johnny's done some very nice."
        ],
        [
            "In this area by final things, just want to point out and I'm sure getting hungry.",
            "Just pointing out a few applications.",
            "Indeed, just one of."
        ],
        [
            "Phone and some other customer applications can't possibly summarize them, I know.",
            "Isabelle Guyon and she has a web page where she tried to click together all the applications of SVM's and then she ran out of time 'cause there's so much so many applications, but.",
            "Just give you applications for my own research.",
            "In fact, I sort of moved on to probabilistic graphical models have been doing a lot in the error cancer formatics which is applying machine learning to cancer type problems.",
            "So the most interesting stuff I can't tell you about today is not connected to SPMS, but we've actually identified knockdown targets with these guys in super Cancer Research and actually found.",
            "That means to kill off cancer cells in specific contexts, but here's an example I did with Richard Williams, Kathy Pritchard Jones and others win super Cancer Research, London, published in this Journal here what we wanted to do was predict relapse versus non relapse for something called Wilms tumor from microdata, is a tumor affect children and young adults and you wish therefore to predict ahead."
        ],
        [
            "Will a patient relapse or not OK, I won't tell you much about the technologies MIC array.",
            "I think many of you might have heard of microarray technology by now, but essentially each of these probes on the substrate is for the activity of a given gene."
        ],
        [
            "And I think my here.",
            "Here's what you sort of get out.",
            "This is an illustration only, but you get out this and that could be normal operation and green an red could be overexpressing under expressing genes, so you'd be looking for genes which."
        ],
        [
            "Abnormal operating OK.",
            "In our particular sample, this is rather out of date.",
            "Example typically have many more pros, but this had 17,800 features.",
            "You couldn't use a neural network on that with 17,000 inputs, but what you realize is that big number.",
            "The 17,000 gets absorbed in this kernel matrix.",
            "The kernel matrix actually depends on the smaller number.",
            "The 29 examples or samples that you have.",
            "OK, user support vector machine with a linear kernel.",
            "Big high dimensional input space.",
            "Relatively small number of samples.",
            "You therefore guess you can use a linear kernel."
        ],
        [
            "And what turns out is that relatively few."
        ],
        [
            "Do genes are relevant to the problem?",
            "There's various things you have to do.",
            "You have to do leave without testing.",
            "Well turned out was if you used all the jeans or even the top 100 we had a means of selecting which were relevant features, then your test error 29 roughly balanced relapse versus non relapse was really quite poor.",
            "14 errors out of 29 you really not got a predictor there at all.",
            "However this particular cancer.",
            "Whether you have relapse or non relapse is rather critically dependent on a small number of genes.",
            "In fact, it's now being found that the primary cause of this particular cancer is due to duplication of the DNA on chromosome one.",
            "Predominantly OK, so relatively small number of genes are duplicated.",
            "They simply overexpressed 'cause they've been copied several times.",
            "So if you get down to this relatively small set of genes, roughly 10 of them they're using leave one out prediction at the 29 you get down to just one or two errors.",
            "OK, in other words, you have a predictor which is around about 90% accurate.",
            "We've managed to repeat this and totally novel data."
        ],
        [
            "Recently we just writing the paper at the moment, but.",
            "But you do still get away with about 90% accuracy.",
            "Big long story, especially with this.",
            "With this, just show a second example of a quickly from important one, just to show you the nice things PHMSA do.",
            "This is due to burn, a short cough and.",
            "Yep, and yellow Coonan other peoples in Red Bank, New York.",
            "They devise Postal code, recognizes for ATT former Bell Labs and that you were using your network.",
            "Have important problem recognizing Postal codes and indeed just improvements of point 1.2% big in this area."
        ],
        [
            "Actually, it's a slightly dated example, but Dennis to cost and Benadryl cough looked at this problem and were able to achieve with SVM around about .15% test error reduction over the best alternative which urine liquid had.",
            "So it was state of the art must have been beaten since, but it was certainly."
        ],
        [
            "Yes, FAM was stated there on this important problem.",
            "So come the end this afternoon we will be doing actually trying out in practice and I give a small talk before we actually start.",
            "Because depending on your background you can go from relatively easy to to more demanding project.",
            "So cool methods are so powerful.",
            "Systematic do does lots of things.",
            "Many other kernelized many other types of learning machines.",
            "I've applied it the medicine by informatics machine vision.",
            "Postal codes.",
            "For example, many applications and finance not even got onto that text analysis etc.",
            "OK, so I'll finish there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Yes, good choice will come.",
                    "label": 0
                },
                {
                    "sent": "Well, the kernel parameter I come onto in this talk here.",
                    "label": 0
                },
                {
                    "sent": "OK, now there has been some work on choosing the kernels.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you go to thekernelmachines.org website, come across these papers.",
                    "label": 0
                },
                {
                    "sent": "My experience is it's best to have an idea of what type of kernel should be used.",
                    "label": 1
                },
                {
                    "sent": "If you know your data is to be separable, OK, then I would just simply use a linear kernel.",
                    "label": 0
                },
                {
                    "sent": "In other words, you don't map to high dimensional spaces simply XI dot XJ.",
                    "label": 0
                },
                {
                    "sent": "Now I often deal with gene expression datasets.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about that later on.",
                    "label": 0
                },
                {
                    "sent": "I know that these extremely high dimensional datasets you've got 30 patients, 30,000 gene measurements stream Lee, high dimensional input space.",
                    "label": 1
                },
                {
                    "sent": "Intuitively, I know because you got relatively small data, number of data points in a very high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The linear kernel work, so I would use a linear kernel if you're going to use Linux kernel, you may then think will perhaps it could be not separable.",
                    "label": 0
                },
                {
                    "sent": "I just don't know this well.",
                    "label": 0
                },
                {
                    "sent": "What would happen when you train the SVM is you.",
                    "label": 0
                },
                {
                    "sent": "Would wouldn't get to 0 training error if you choose a linear kernel and you get a zero training error.",
                    "label": 0
                },
                {
                    "sent": "And I always have code checked, I get zero training error then.",
                    "label": 0
                },
                {
                    "sent": "I know it's an appropriate choice if I suspect it's a linear kernel an I found that the IT can't actually get zero training error.",
                    "label": 0
                },
                {
                    "sent": "OK then that would indicate to me it's not separable and I should start thinking about other kernels.",
                    "label": 0
                },
                {
                    "sent": "If I start thinking about other kernels then my normal choices to go actually for the Gaussian kernel OK?",
                    "label": 0
                },
                {
                    "sent": "Is very well behaved.",
                    "label": 0
                },
                {
                    "sent": "Works in practice fine, and that's any other kernel I've ever really used in my life.",
                    "label": 0
                },
                {
                    "sent": "Having said that, I think I discussed all the kernels, but I would go for Gauss income.",
                    "label": 0
                },
                {
                    "sent": "However, of course, depending on the type of problem, it maybe I wanted to text strings.",
                    "label": 1
                },
                {
                    "sent": "OK, in that case I obviously would do an edit kernel or graphs.",
                    "label": 0
                },
                {
                    "sent": "In this case, obviously do diffusion kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so in practice I think from experience I would probably know what kernel to use anyway.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "The funny thing I should mention when when I first started with SVM's I made a mistake in which I ran a program with all the Alpha is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "OK, now by choosing my kernel parameter this was with the Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "I could actually get a reasonable type of test error.",
                    "label": 0
                },
                {
                    "sent": "OK, in other words it's giving me what would be called a parsing Windows type solution OK?",
                    "label": 0
                },
                {
                    "sent": "I then went on experiment with family era and change my alphas and got a little bit better solution.",
                    "label": 0
                },
                {
                    "sent": "But there is a partial Windows solution.",
                    "label": 0
                },
                {
                    "sent": "If you got the right value of Alpha will give you a test error which is you know.",
                    "label": 0
                },
                {
                    "sent": "Frame will beat it, but it's not far from the SVM solution.",
                    "label": 0
                },
                {
                    "sent": "OK, so the parser Windows solution with a Gaussian kernel would be equivalent to what you mentioned are sort of sort of just a hyperplane.",
                    "label": 0
                },
                {
                    "sent": "In this high dimensional space suffering two points.",
                    "label": 0
                },
                {
                    "sent": "I don't know how it does it, but it's sort of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe that answers your question.",
                    "label": 0
                },
                {
                    "sent": "Well, anyway, I'll I'll.",
                    "label": 0
                },
                {
                    "sent": "Well, I just want to come to the second part and what I want to do in the second lecture.",
                    "label": 0
                },
                {
                    "sent": "Is the following.",
                    "label": 0
                },
                {
                    "sent": "At first lecture everything I did was really quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "OK, what I want to do here is really show you can use other parts of optimization theory.",
                    "label": 0
                },
                {
                    "sent": "Indeed we can formulate classifiers using linear programming or nonlinear optimization and semidefinite programming.",
                    "label": 0
                },
                {
                    "sent": "Indeed, you can really use a nearly all of optimization theory.",
                    "label": 0
                },
                {
                    "sent": "Combine it with kernel methods to come up with all sorts of weird and wonderful things so.",
                    "label": 0
                },
                {
                    "sent": "So what we want to show you is a broader class of kernel methods.",
                    "label": 0
                },
                {
                    "sent": "And indeed.",
                    "label": 0
                },
                {
                    "sent": "The opposite of observation to make is that when I did the SVM, I heard my data in the form of a dot product OK, and then I use the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "Indeed, there's a whole range of non SVM things where I come across my data is in the form of a dot product and I concur nallies.",
                    "label": 0
                },
                {
                    "sent": "In other words, there's alot of things I can Colonel eyes.",
                    "label": 0
                },
                {
                    "sent": "I can take PCA and I can do kernel PCA or independent components analysis.",
                    "label": 0
                },
                {
                    "sent": "ICA kernel eyes that.",
                    "label": 0
                },
                {
                    "sent": "Guys, I see a perceptron.",
                    "label": 0
                },
                {
                    "sent": "You probably come across Perceptron.",
                    "label": 0
                },
                {
                    "sent": "I can do the kernelized perceptron, a very crude algorithm I did years ago was something called added for an algorithm which is just a fast variant on the perceptron.",
                    "label": 0
                },
                {
                    "sent": "The data appears in the form of a dot product XI.",
                    "label": 0
                },
                {
                    "sent": "XJ kernelized.",
                    "label": 0
                },
                {
                    "sent": "It came up with the kernel out of Tron.",
                    "label": 0
                },
                {
                    "sent": "You can actually try it this afternoon.",
                    "label": 0
                },
                {
                    "sent": "I give you the code for it.",
                    "label": 0
                },
                {
                    "sent": "So 40 line thing has a Colonel in there so you can handle non separable data.",
                    "label": 0
                },
                {
                    "sent": "Put a soft margin if you want.",
                    "label": 0
                },
                {
                    "sent": "But it is a kernel algorithm.",
                    "label": 0
                },
                {
                    "sent": "The kernel out of Tron is, which you can try.",
                    "label": 0
                },
                {
                    "sent": "This afternoon is a good example of something which is not an SVM, but you can use a kernel kernels with it.",
                    "label": 0
                },
                {
                    "sent": "So I'm really saying now that in fact the SVM which introduces this earlier is 1 example for much bigger class of kernel based methods, OK?",
                    "label": 0
                },
                {
                    "sent": "So I can use other other methods from optimiza.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Theory and.",
                    "label": 0
                },
                {
                    "sent": "I can do other things apart from classification of Gresham.",
                    "label": 0
                },
                {
                    "sent": "OK, to illustrate that I'll be doing novelty detection.",
                    "label": 0
                },
                {
                    "sent": "OK, some of some of the topics I mentioned in the second talk were actually research stuff that I did in the past.",
                    "label": 0
                },
                {
                    "sent": "The novel detector was a projected with Christine Bennett and.",
                    "label": 0
                },
                {
                    "sent": "There's also other things I sort of kernel machines which I miss out.",
                    "label": 1
                },
                {
                    "sent": "There is a thing called the Bayes Point Machine which I did with Ralf Herbrich Centora Grapel some years ago, which is actually better than a support vector machine.",
                    "label": 0
                },
                {
                    "sent": "I might just say something about it right at the end.",
                    "label": 0
                },
                {
                    "sent": "It's rather slow and awkward to train.",
                    "label": 0
                },
                {
                    "sent": "It's a problem, but it consistently outperform a support vector machine, so I say here all men are different ideas from optimization theory can be used, and there's a huge range other things.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apart from classification regression and I want to comment also on a lot more about actually how you do the learning model selection.",
                    "label": 0
                },
                {
                    "sent": "I've introduced the kernel in some cases or kernel parameter like in accounting kernel.",
                    "label": 0
                },
                {
                    "sent": "But what value should you choose for that kernel parameter?",
                    "label": 0
                },
                {
                    "sent": "So I want to do that.",
                    "label": 0
                },
                {
                    "sent": "Then again a very interesting but little bit more recent errors research is composite kernels OK in other words?",
                    "label": 0
                },
                {
                    "sent": "I may have different types of data, and each of those different types of data is encoded as a kernel OK, and what I want to do is classification using multiple types of data.",
                    "label": 0
                },
                {
                    "sent": "OK, give an example of this.",
                    "label": 0
                },
                {
                    "sent": "One of my collaborators, marjana me, was very interested in the problem of protein folds.",
                    "label": 0
                },
                {
                    "sent": "OK, it's a sort of classification type problem.",
                    "label": 0
                },
                {
                    "sent": "You had six different types of data.",
                    "label": 0
                },
                {
                    "sent": "OK, some could be encoded as linear kernels, Gaussian kernels.",
                    "label": 0
                },
                {
                    "sent": "I think one was a graph kernel.",
                    "label": 0
                },
                {
                    "sent": "Really quite different types of data now.",
                    "label": 0
                },
                {
                    "sent": "He was therefore able to derive a composite kernel data Fusion and he.",
                    "label": 0
                },
                {
                    "sent": "Did the learning machine using the composite kernel.",
                    "label": 0
                },
                {
                    "sent": "And it was 11 percentage points better than the best type of data he could use.",
                    "label": 0
                },
                {
                    "sent": "Of those six classes.",
                    "label": 0
                },
                {
                    "sent": "So there are six types of data.",
                    "label": 0
                },
                {
                    "sent": "If you chose that data set, which gave you the best performance found out what is test error is then using the composite Colonel was 11 percentage points better?",
                    "label": 0
                },
                {
                    "sent": "OK, so obviously if you've got multiple types of data OK then using composite kernel is better than just using one type of data.",
                    "label": 0
                },
                {
                    "sent": "Only recently I've been I said at the beginning I work in cancer informatics.",
                    "label": 0
                },
                {
                    "sent": "What we are interested there is actually using multiple types of data to improve prediction on patients.",
                    "label": 0
                },
                {
                    "sent": "OK, so we have many types of measurements from the patient and we should use all of these to try and do the best prediction possible.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll talk about linear programming, nonlinear programming, novel detection, and then again, I want to talk about very interesting variants.",
                    "label": 1
                },
                {
                    "sent": "Don't forget all of this very interesting variants on kernel methods.",
                    "label": 0
                },
                {
                    "sent": "In particular what I did up to now is what you called passive learning.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Passive learning would be.",
                    "label": 0
                },
                {
                    "sent": "I could present some data to you.",
                    "label": 0
                },
                {
                    "sent": "You're learning Machine, learns this data and then tries to predict on new data points.",
                    "label": 0
                },
                {
                    "sent": "Active learning is.",
                    "label": 1
                },
                {
                    "sent": "This is a project that did with Yellow and Alex Mueller out of learning.",
                    "label": 0
                },
                {
                    "sent": "Is the learning machine actually poses queries to to you to try and optimally learn a task OK?",
                    "label": 0
                },
                {
                    "sent": "Other words, perhaps I'm trying to teach arithmetic to you.",
                    "label": 0
                },
                {
                    "sent": "OK, I would say 2 + 2 equals four 3 + 3 = 6.",
                    "label": 0
                },
                {
                    "sent": "You learn these.",
                    "label": 0
                },
                {
                    "sent": "You set up your learning machine and then you hope to generalize to new instances.",
                    "label": 0
                },
                {
                    "sent": "I might give you arithmetic.",
                    "label": 0
                },
                {
                    "sent": "Active learning would be that I say 2 + 2 = 4 and then somebody says, well, OK, What's the answer to 3 + 7?",
                    "label": 0
                },
                {
                    "sent": "I said 10 and somebody else sticks up their hand, now opposing maximally informed queries.",
                    "label": 0
                },
                {
                    "sent": "You can learn the rule more efficiently now that you can easily do it in the context of kernel methods are illustrated later on is projected with Alex and Mellow.",
                    "label": 0
                },
                {
                    "sent": "You may think what real life applications I'll show you briefly at drug application, where if you like what the learning machine is, presents a molecule.",
                    "label": 0
                },
                {
                    "sent": "An the experimenter says yes, it's biologically active or no, it's not, an it can optionally maximally find the drug using active learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so we hope to cover all of these and other things as well, right?",
                    "label": 0
                },
                {
                    "sent": "Let's briefly do some of these.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "20 minutes, right?",
                    "label": 0
                },
                {
                    "sent": "So a linear programming approach, so rather than quadratic programming, perhaps you don't know quadratic programming, but you do know linear programming.",
                    "label": 0
                },
                {
                    "sent": "Can you do that?",
                    "label": 0
                },
                {
                    "sent": "This approach was really evolved by Ovi Mangasarian.",
                    "label": 0
                },
                {
                    "sent": "Anne Christine Bennett rents are Polytechnic Institute Ann.",
                    "label": 0
                },
                {
                    "sent": "Just say, here Colonel substitutions not restricted to that SVM framework.",
                    "label": 0
                },
                {
                    "sent": "I gave earlier.",
                    "label": 0
                },
                {
                    "sent": "Large range of methods can be kernelized.",
                    "label": 1
                },
                {
                    "sent": "OK, when suddenly someone day somebody realizing kernelized everything.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Huge explosion of papers where people kernelized everything right?",
                    "label": 0
                },
                {
                    "sent": "So you can kernelized simple neural network learning algorithms such as perceptron, kernel, perceptron, kernel, min over kernel and it ran.",
                    "label": 1
                },
                {
                    "sent": "I did that right back in 98 and they can handle non inseparable.",
                    "label": 0
                },
                {
                    "sent": "They sets.",
                    "label": 1
                },
                {
                    "sent": "You can try to call out from this afternoon principle kernel, PCA, kernel, ICA, Colonel, fish and chips.",
                    "label": 0
                },
                {
                    "sent": "You know sort of goes on them so you can kernelized other types of learning machine based.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Machine somebody can ask me about that later.",
                    "label": 0
                },
                {
                    "sent": "I can member it works so.",
                    "label": 0
                },
                {
                    "sent": "Sample of linear programming.",
                    "label": 0
                },
                {
                    "sent": "This is due to Christine Bennett and I did this this L or W for memory.",
                    "label": 0
                },
                {
                    "sent": "Somehow it slightly wrong, but you minimize someone.",
                    "label": 0
                },
                {
                    "sent": "I Alpha now linear and Alpha and this term, so this looks rather similar.",
                    "label": 0
                },
                {
                    "sent": "Well sort of sort of expressions you find before, but importantly, it's linear programming type problem.",
                    "label": 0
                },
                {
                    "sent": "You can do both classification and regression.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using linear programming and that linear programming task is subject to constraints that give the constraints here.",
                    "label": 0
                },
                {
                    "sent": "OK subject to this alphas, positive stack variables previous OK.",
                    "label": 0
                },
                {
                    "sent": "I've experimented this, I think with this some time ago and slightly slower than the QPS VM, but it's quite robust and gave similar sorts of performance OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, it's not so.",
                    "label": 0
                },
                {
                    "sent": "It's not so clear where this comes from.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Well, it probably was clear to Christina.",
                    "label": 0
                },
                {
                    "sent": "Probably need to look back through our papers.",
                    "label": 0
                },
                {
                    "sent": "But the one novel detection which I did with Christina, she comes up shortly again, there's no form.",
                    "label": 0
                },
                {
                    "sent": "There's no primal formulation, straighter dual formulation.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can't justify it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's why I say about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do linear programming, classification and linear programming regression OK. You can do other things apart from classification regression.",
                    "label": 0
                },
                {
                    "sent": "Now show you novelty detection or what you might call a one class classifier.",
                    "label": 0
                },
                {
                    "sent": "Indeed, the datasets are used.",
                    "label": 0
                },
                {
                    "sent": "One was condition monitoring of machinery and I use the data set by Keith Ward.",
                    "label": 0
                },
                {
                    "sent": "New might be around today or he was around at the coffee break.",
                    "label": 0
                },
                {
                    "sent": "So explain where this problem comes from and then I'll show you a scheme I did with Christine Bennett for novelty detection.",
                    "label": 0
                },
                {
                    "sent": "I'll give you.",
                    "label": 0
                },
                {
                    "sent": "A medical example of this.",
                    "label": 0
                },
                {
                    "sent": "In many real world problems, the task is not to classify but for detect, but to detect novel or abnormal instances.",
                    "label": 1
                },
                {
                    "sent": "I'll give you an actual problem which was done in Bristol University, not by myself but by PhD student in computer science he was handling a data set for something called an acoustic neuroma, which is a type of tumor pops up in the brain, largely benign.",
                    "label": 0
                },
                {
                    "sent": "It unfortunately puts pressure on the brain and can kill the person, and it's relatively rare.",
                    "label": 0
                },
                {
                    "sent": "In the UK Puranam, there's about 150 cases of this particular type of tumor.",
                    "label": 0
                },
                {
                    "sent": "Now they were interested in building.",
                    "label": 0
                },
                {
                    "sent": "Typically you detect it by doing NMR scan of the brain and then you try and spot it.",
                    "label": 0
                },
                {
                    "sent": "They wanted to build a classifier that would actually find obviously the presence of this acoustic neuroma map even had a case where the neuron was present in Bristol Royal Infirmary wasn't spotted while spotted too late and the patient died.",
                    "label": 0
                },
                {
                    "sent": "So you want to.",
                    "label": 0
                },
                {
                    "sent": "Detect that rumor, certainly, and it attaches to cranial nerve so it tends to introduce an asymmetry in your NMR scan.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a head cranial nerve sits on one side.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course the first point to do was to segment your image OK, you got your regions in the image, perhaps derived through descriptors or whatever.",
                    "label": 0
                },
                {
                    "sent": "Use these to actually build a classifier that would say, not a neuroma not in your OMA.",
                    "label": 0
                },
                {
                    "sent": "Yes, a new Roman.",
                    "label": 0
                },
                {
                    "sent": "Now that's fine.",
                    "label": 0
                },
                {
                    "sent": "It sort of worked, but you could exploit the fact that when it was present in interesting asymmetry, so symmetry took you along way.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately I said just now in the UK that's around about 150 cases per annum.",
                    "label": 0
                },
                {
                    "sent": "Is quite easy, therefore to have a shape for tumor, which won't be anything similar to anything.",
                    "label": 0
                },
                {
                    "sent": "Your training set.",
                    "label": 0
                },
                {
                    "sent": "OK, it's an unusual shape and how it's growing, it's really quite dissimilar to anything you've seen before, and your classifier is going to miss it.",
                    "label": 0
                },
                {
                    "sent": "OK, and you must be able to spot it well.",
                    "label": 0
                },
                {
                    "sent": "This introduces the novel detector.",
                    "label": 0
                },
                {
                    "sent": "In the novel, detector really tries to spot something which diverges from normality.",
                    "label": 0
                },
                {
                    "sent": "That's it OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, what they developed a system was your first passes to try and classify everything in the image.",
                    "label": 0
                },
                {
                    "sent": "However, your second passes to use anomaly detector, which will highlight the vergence from normality.",
                    "label": 0
                },
                {
                    "sent": "So your classifier may not be may be unable to classify an unusual shape, but your normal detector will still say it's novel.",
                    "label": 0
                },
                {
                    "sent": "It's abnormal and that second stage will pick up something which could be.",
                    "label": 0
                },
                {
                    "sent": "Your medic will look at and probably recognize this abnormality.",
                    "label": 0
                },
                {
                    "sent": "Now there are many applications that enable detection.",
                    "label": 0
                },
                {
                    "sent": "Of course, in medical diagnosis you're trying to really spot divergent from normality in a patient all the time.",
                    "label": 0
                },
                {
                    "sent": "But you'll be perhaps interesting novelty detection for condition monitoring you have your machine.",
                    "label": 0
                },
                {
                    "sent": "This is a project we did with Berkley Power Station.",
                    "label": 0
                },
                {
                    "sent": "You have a shaft of a nuclear power station.",
                    "label": 0
                },
                {
                    "sent": "You have a lot of data for the normal operation of the shaft.",
                    "label": 0
                },
                {
                    "sent": "You want to spot an error coming up, not abnormal behavior or novel behavior, because if it comes up and your shaft goes down in nuclear power station, you lose 1,000,000 pounds every 10 minutes or something.",
                    "label": 0
                },
                {
                    "sent": "So you want to be able to do your normal detection very well.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the most.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient detection, so there's actually two approaches you could have to double detection one.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "They can.",
                    "label": 0
                },
                {
                    "sent": "It's a big storage, normal detection like I only give you an impression here.",
                    "label": 0
                },
                {
                    "sent": "There's two ways of doing it, which I'm going to give you.",
                    "label": 0
                },
                {
                    "sent": "In fact, if I did really did Noble detection, I wouldn't do it.",
                    "label": 0
                },
                {
                    "sent": "Using the scheme about to give you, I'd actually use probably some ideas that Mark Geronimi head again so he actually has used it in practice.",
                    "label": 0
                },
                {
                    "sent": "There's actually two ways in which you might approach not detection.",
                    "label": 0
                },
                {
                    "sent": "One is to estimate the support of a distribution, which is I want to give you now.",
                    "label": 0
                },
                {
                    "sent": "The 2nd is a probabilist.",
                    "label": 0
                },
                {
                    "sent": "The way I would actually do it is a probabilistic sort of density estimator.",
                    "label": 0
                },
                {
                    "sent": "Now what do I mean by all of that in plain English?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The way I give you now is a Noble detector, which it did with Christine Bennett.",
                    "label": 0
                },
                {
                    "sent": "Anna really, I'm just trying to illustrate.",
                    "label": 0
                },
                {
                    "sent": "You can do things beyond classification, regression, but estimating the support is this is your data OK?",
                    "label": 0
                },
                {
                    "sent": "Estimating support is you find a boundary round your data and everything in here is regarded as normal and anything outside is regardless abnormal.",
                    "label": 0
                },
                {
                    "sent": "That's estimating the support.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, what you do is here's your data, perhaps looking at it sideways and you try and estimate a probability function which goes over that data.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is the probability is decreasing away and say it's more more abnormal that is harder to do than this OK?",
                    "label": 0
                },
                {
                    "sent": "So those schemes are more modular schemes of what I would actually go for in practice, but I'm just wanting to illustrate here quite quickly so you can do this sort of novelty detection, and there's actually two schemes using quadratic programming burner, short cough and tax, and urine, and I just give you the LP wave.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing it, which I did with Christine.",
                    "label": 0
                },
                {
                    "sent": "Now this one, rather like Christine's linear programming.",
                    "label": 0
                },
                {
                    "sent": "You actually formulate it directly in the feature space.",
                    "label": 1
                },
                {
                    "sent": "You don't have this picture in input space.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A map across I'll show you the essential ideas.",
                    "label": 0
                },
                {
                    "sent": "In fact, it uses a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "And I want to note one thing about the Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Before I say something about this picture, I give here on this slide.",
                    "label": 0
                },
                {
                    "sent": "Now Garrison kernel for getting the kernel parameter looks a bit like this.",
                    "label": 0
                },
                {
                    "sent": "OK, well, perhaps I put the kernel parameter in.",
                    "label": 0
                },
                {
                    "sent": "OK, that was it.",
                    "label": 0
                },
                {
                    "sent": "Just a straight Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Now this was my K OK and that's simply equal to 5 X .5 X prime OK. Now let X equal X prime OK. Then obviously this will be easy to zero with just the one.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I make this choice OK, 5X dot 5X will be one.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying to you is if I make the choice of a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "Then actually what happens is you map your data to infinite dimensional space in which every point actually sits on the surface of a hypersphere.",
                    "label": 0
                },
                {
                    "sent": "OK, that's it's how it's represented in this feature space.",
                    "label": 0
                },
                {
                    "sent": "Here is this hyper sphere OK?",
                    "label": 0
                },
                {
                    "sent": "And indeed, if I were to give you a bunch of data where I'm wanted to estimate the support, those data points will sit somewhere on that hyper sphere.",
                    "label": 0
                },
                {
                    "sent": "OK, of unit radius.",
                    "label": 0
                },
                {
                    "sent": "Here they are a little blobs.",
                    "label": 0
                },
                {
                    "sent": "The way to estimate the support of the distribution, but I did with Christine Bennett in the NIPS paper was we basically suck the hyperplane onto the data.",
                    "label": 0
                },
                {
                    "sent": "So it sort of impales itself.",
                    "label": 0
                },
                {
                    "sent": "And what we call the support objects OK. And we got nips paper out of it, but I reckon now the scheme was bit crew.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So anyway, how do you do this?",
                    "label": 0
                },
                {
                    "sent": "Well, it's a cheap.",
                    "label": 0
                },
                {
                    "sent": "By minimizing this, OK, you notice linear and Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Subject to constraints and the constraints are as given OK.",
                    "label": 0
                },
                {
                    "sent": "So it's just.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A linear programming constraint.",
                    "label": 0
                },
                {
                    "sent": "Linear programming problem OK.",
                    "label": 0
                },
                {
                    "sent": "I don't really, I won't say.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the bars, you can introduce off margins.",
                    "label": 0
                },
                {
                    "sent": "Here is a very simple example of what's going on.",
                    "label": 0
                },
                {
                    "sent": "I've sprinkled data as data points here and here.",
                    "label": 0
                },
                {
                    "sent": "Let's put a boundary around the data OK, and so anything here would be viewed as abnormal.",
                    "label": 0
                },
                {
                    "sent": "But if you're inside this region here, it views it as normal OK?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's a little bit of a crude illustration.",
                    "label": 0
                },
                {
                    "sent": "If you introduce a soft margin, you can allow for some points to be outside the boundary OK, after all, you have to be aware that if I'm trying to.",
                    "label": 1
                },
                {
                    "sent": "If I'm trying to learn abnormality OK, and presumably therefore I'm trying to train off normal data and then on new instances I try and list them as normal or abnormal.",
                    "label": 0
                },
                {
                    "sent": "I do unfortunately have the possibility that my normal data may contain some abnormalities I may not know that, in which case I may want to lessen the influence of such points and these might be abnormal points in my training data, so I'm wanting to build a model of the normal to try and spot the abnormal.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, training data for my normal.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have problems with it now.",
                    "label": 0
                },
                {
                    "sent": "This was a better type of kernel to use and which would put a much better sort of boundary around the data.",
                    "label": 0
                },
                {
                    "sent": "You can make the.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scheme a lot more elaborate by actually make it probabilistic and have a problem.",
                    "label": 0
                },
                {
                    "sent": "See going away from this boundary.",
                    "label": 0
                },
                {
                    "sent": "OK then it makes a much better sort of scheme which makes a lot more sense.",
                    "label": 0
                },
                {
                    "sent": "OK. Well, that's all I'm going to say about.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No detection, I really want to illustrate there are other things apart from classification regression.",
                    "label": 0
                },
                {
                    "sent": "Right, moving on not my next topic.",
                    "label": 0
                },
                {
                    "sent": "I want to come to is passive learning versus active learning.",
                    "label": 0
                },
                {
                    "sent": "A relatively little bill described in textbooks on kernel methods, but it's interesting little topic in itself that I did with Alex Smolen hello.",
                    "label": 0
                },
                {
                    "sent": "So I actually define is just now.",
                    "label": 0
                },
                {
                    "sent": "I said there's two ways in which you might learn a problem.",
                    "label": 0
                },
                {
                    "sent": "Up to now, everything I've given you has been passive.",
                    "label": 0
                },
                {
                    "sent": "Learning the learning machine takes us are most learns from these types of generalized new instances active learning the learning machine poses queries or questions.",
                    "label": 1
                },
                {
                    "sent": "The Oracle or source of information.",
                    "label": 0
                },
                {
                    "sent": "So if I have an efficient query learner, then it will learn the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem much more efficiently now.",
                    "label": 0
                },
                {
                    "sent": "Actually I want to.",
                    "label": 0
                },
                {
                    "sent": "First of all, point out why.",
                    "label": 0
                },
                {
                    "sent": "Why am I introducing this in the context of kernel methods?",
                    "label": 0
                },
                {
                    "sent": "Well, let me go back to this scheme here.",
                    "label": 0
                },
                {
                    "sent": "I said that these particular data points here were non support vectors.",
                    "label": 0
                },
                {
                    "sent": "When I build the final hypothesis, they don't appear to have Alpha equals zero in a hypothesis.",
                    "label": 0
                },
                {
                    "sent": "In other words, I never need to know about them.",
                    "label": 0
                },
                {
                    "sent": "I don't need to learn the non support vectors, only these guys are support vectors will actually be influential.",
                    "label": 0
                },
                {
                    "sent": "In other words, if I have unlabeled data, OK?",
                    "label": 0
                },
                {
                    "sent": "It costs me to label the data.",
                    "label": 0
                },
                {
                    "sent": "Then if I have an efficient learning scheme, an active learner, it will try and go for the support vectors and never bother learning the labels of these guys here.",
                    "label": 0
                },
                {
                    "sent": "OK, because they're not going to have any influence in a final final hypothesis, they don't need to know to know them, OK?",
                    "label": 0
                },
                {
                    "sent": "Now indeed, query learning active learning the various different approaches and subject.",
                    "label": 0
                },
                {
                    "sent": "There are several strategies possible, and what the first one I give here called membership queries.",
                    "label": 0
                },
                {
                    "sent": "And if you like in this case, the algorithm selects unlabeled examples for the human expert to label human example, this could be I wanted to spot Postal codes.",
                    "label": 1
                },
                {
                    "sent": "OK, now every the every digit written down by person on an envelope would have meant something OK and your human expert has to come along and label alot of these and then you try and learn the learning machine learning season.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you have.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "3 zip codes or Postal code switch is a nine.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a 9 very badly written.",
                    "label": 0
                },
                {
                    "sent": "OK, but it has a meaning, whereas if I create a query I may ask what does that mean and of course has a meaning.",
                    "label": 0
                },
                {
                    "sent": "So creating queries is a different task for membership queries.",
                    "label": 0
                },
                {
                    "sent": "I'll be doing mem.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Queries here.",
                    "label": 0
                },
                {
                    "sent": "OK, we did a theorem that you can prove in the worst case that.",
                    "label": 0
                },
                {
                    "sent": "The number of queries you are asked actually equals a sample size.",
                    "label": 1
                },
                {
                    "sent": "In other words, you don't make any gain from using active learning but such.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is don't appear in much in practice.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll show you what the general there for active learning is better than passive learning.",
                    "label": 1
                },
                {
                    "sent": "The theorem we proved is at that point the best membership queries.",
                    "label": 1
                },
                {
                    "sent": "The best point to request the query is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point line closest current.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm playing OK, I'll show you what exactly I mean.",
                    "label": 0
                },
                {
                    "sent": "This was the story I gave you for the support vector machine at the beginning.",
                    "label": 0
                },
                {
                    "sent": "OK, and I said actually I really don't need to ever ask the for the labels of water going to turn out to be.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Support vectors.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "The IT turns out we proved a theorem that the best points to query are those points which are closest to current hyperplane.",
                    "label": 1
                },
                {
                    "sent": "They're maximally ambiguous.",
                    "label": 0
                },
                {
                    "sent": "I think I could probably summarize this story with.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Following picture OK now.",
                    "label": 0
                },
                {
                    "sent": "This is some sort of intermediate stage of my algorithm, in which I've have some points which are labeled.",
                    "label": 0
                },
                {
                    "sent": "I've asked what Sis and etc plus and what's this and it said a minus OK and those circles.",
                    "label": 1
                },
                {
                    "sent": "Oh, circles are unlabeled data points, setting off power.",
                    "label": 0
                },
                {
                    "sent": "What would that be due to?",
                    "label": 0
                },
                {
                    "sent": "Short intermission technical interlude what happened?",
                    "label": 0
                },
                {
                    "sent": "Did it?",
                    "label": 0
                },
                {
                    "sent": "Just clear the ball burns down.",
                    "label": 0
                },
                {
                    "sent": "Take time.",
                    "label": 0
                },
                {
                    "sent": "I'm really doing this table one.",
                    "label": 0
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So this was it, yeah?",
                    "label": 0
                },
                {
                    "sent": "Well yeah, but what are sort of commenting was if I did this separable data these guys were turn up with our free equals zero.",
                    "label": 0
                },
                {
                    "sent": "OK there are non support vectors, they don't influence where the hyperlink should be.",
                    "label": 0
                },
                {
                    "sent": "If you like I might never need to either learn that point or even know its label.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, I'm going to sort of answer that point shortly.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll answer it.",
                    "label": 0
                },
                {
                    "sent": "Actually when I come on to some plots now, this is an intermediate situation.",
                    "label": 0
                },
                {
                    "sent": "These are positive.",
                    "label": 0
                },
                {
                    "sent": "Negative an unlabeled so I can ask for label these guys.",
                    "label": 1
                },
                {
                    "sent": "OK, what the theorem we did establish with the best point to query is that unlabeled point, which is closer to current hyperplane.",
                    "label": 0
                },
                {
                    "sent": "Here it is OK.",
                    "label": 0
                },
                {
                    "sent": "This will be the one I'll go for an ask for this label.",
                    "label": 1
                },
                {
                    "sent": "In fact again is saying something pretty obvious.",
                    "label": 0
                },
                {
                    "sent": "These points are maximally ambiguous.",
                    "label": 0
                },
                {
                    "sent": "OK, so go for them.",
                    "label": 0
                },
                {
                    "sent": "They're going to influence with a high place, should be so often when you prove this dreadfully long theorems, you arrive at something which is perfectly clear from the beginning, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                },
                {
                    "sent": "So that's what the theorem said.",
                    "label": 0
                },
                {
                    "sent": "Indeed, what the theorem says it was always worth querying these points within the margin band.",
                    "label": 0
                },
                {
                    "sent": "However it was actually inadvisable to query these points here.",
                    "label": 0
                },
                {
                    "sent": "You asked just now the reason why is 'cause.",
                    "label": 0
                },
                {
                    "sent": "If this is negative, it's OK.",
                    "label": 0
                },
                {
                    "sent": "It's going to have no effect.",
                    "label": 1
                },
                {
                    "sent": "It fits in this supposed to be negatives here fits in with the story, but in effect I did not need to know it was negative.",
                    "label": 0
                },
                {
                    "sent": "OK if it's negative.",
                    "label": 0
                },
                {
                    "sent": "And these are negatives.",
                    "label": 0
                },
                {
                    "sent": "It has no influence.",
                    "label": 0
                },
                {
                    "sent": "So you know, don't make any game from that.",
                    "label": 0
                },
                {
                    "sent": "Suppose that was a positive.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to have some sort of disastrous impact.",
                    "label": 0
                },
                {
                    "sent": "And this whole thing, because these are all positives.",
                    "label": 0
                },
                {
                    "sent": "This is a positive.",
                    "label": 0
                },
                {
                    "sent": "These are negatives within this 2 dimensional domain I've got looks a bit of a mess in a high dimensional domain, it's going to really push my hyperplane into some totally different orientation.",
                    "label": 0
                },
                {
                    "sent": "So actually I've got two outcomes for these guys.",
                    "label": 0
                },
                {
                    "sent": "I'd have no influence or be they are most likely to be outliers because they would be totally wrongly positioned according to all the other data.",
                    "label": 0
                },
                {
                    "sent": "OK, So what the theorem proved was yes query these guys.",
                    "label": 0
                },
                {
                    "sent": "Once you empty the margin band stop because it's most likely it's a high probability that the other points are going to query are going to be uninformative or wrong, in fact.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you that with some plots coming up OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just give you a silly toy example.",
                    "label": 0
                },
                {
                    "sent": "OK, now this was a thing called the majority rule.",
                    "label": 0
                },
                {
                    "sent": "Majority rule is a quite random zeros and ones, and the label is a one.",
                    "label": 0
                },
                {
                    "sent": "If there are more ones and zeros and it would be a -- 1 if there more zeros and one little trivial rule.",
                    "label": 0
                },
                {
                    "sent": "OK, majority rule.",
                    "label": 0
                },
                {
                    "sent": "Now your learning algorithm does not know what the rule is.",
                    "label": 0
                },
                {
                    "sent": "It simply has examples presented to it and I have the two instances.",
                    "label": 0
                },
                {
                    "sent": "First of all, I have a random xamples presented to it, OK. Alternatively, it can actually ask what's the label of given examples.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you do passive learning eventually after run about 200 examples.",
                    "label": 0
                },
                {
                    "sent": "It's relatively simple rule to learn OK. Apps had 10 inputs after Turner's amples it's found it with random passive learning.",
                    "label": 0
                },
                {
                    "sent": "However, after 60 well posed queries OK, then it's got the role perfectly.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a good example of passive active learning winning over passive learning.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The question was posed, how do we know when to stop asking queries?",
                    "label": 1
                },
                {
                    "sent": "There's no use doing query learning, I just go on and on several several criteria use.",
                    "label": 0
                },
                {
                    "sent": "If you have noise.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Status, so you'd have to sort of know that days is noiseless.",
                    "label": 0
                },
                {
                    "sent": "Then the simple criterion to use is that I predict ahead what the label should be, and if I'm finding my prediction always agrees with what that label was, then I've learned it and I notice stop in fact, on my noiseless majority rule example I just gave you is making mistakes some of the time it thinks is going to be a plus, but it's actually a minus, so it makes mistakes when it's got to 60.",
                    "label": 0
                },
                {
                    "sent": "Is learn the role perfectly.",
                    "label": 0
                },
                {
                    "sent": "It then will predict A plus one and it is a plus one or minus.",
                    "label": 0
                },
                {
                    "sent": "One is a -- 1.",
                    "label": 0
                },
                {
                    "sent": "It makes no further mistakes so it knows to stop at that point.",
                    "label": 0
                },
                {
                    "sent": "OK, so you do have a criterion in the noiseless case that you know when you should stop.",
                    "label": 0
                },
                {
                    "sent": "OK, I unfortunately don't give the noisy case here.",
                    "label": 0
                },
                {
                    "sent": "The noisy case that we established was the theorem I just explained just now.",
                    "label": 0
                },
                {
                    "sent": "It's in fact we did it.",
                    "label": 0
                },
                {
                    "sent": "I did it on a number of examples.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Basically, I keep asking queries until the marching band is empty and then I stop if you actually go on off asking queries.",
                    "label": 0
                },
                {
                    "sent": "It turns out you Start learning some outliers and typically you go through a minimum when the margin band is empty.",
                    "label": 0
                },
                {
                    "sent": "OK, we did this with the heart dates from UCI various other datasets, all noisy and actually the minimum.",
                    "label": 0
                },
                {
                    "sent": "The test error was actually at that point when the margin band empted OK.",
                    "label": 0
                },
                {
                    "sent": "The Y axis is is going to be number of errors.",
                    "label": 0
                },
                {
                    "sent": "OK, so it makes when it's learned those 60 OK it had made 25 or 24 it had predicted ahead what the label should be.",
                    "label": 0
                },
                {
                    "sent": "An it was wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, so it knew had to go on learning OK. Well, it's got to.",
                    "label": 0
                },
                {
                    "sent": "Here is no longer making errors.",
                    "label": 0
                },
                {
                    "sent": "OK, it's got.",
                    "label": 0
                },
                {
                    "sent": "It's got its rule OK, predicts ahead and the answer of the unlabeled point agrees with what it should be.",
                    "label": 0
                },
                {
                    "sent": "Yeah it is.",
                    "label": 0
                },
                {
                    "sent": "It's accumulative.",
                    "label": 0
                },
                {
                    "sent": "It's accumulative error OK?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well, I think it's the way it actually searches the space.",
                    "label": 0
                },
                {
                    "sent": "I don't really know why this is a noiseless case.",
                    "label": 0
                },
                {
                    "sent": "I think if we do a noisy case you might get something bit more exponential, OK?",
                    "label": 0
                },
                {
                    "sent": "This one was original.",
                    "label": 0
                },
                {
                    "sent": "At Test error OK.",
                    "label": 0
                },
                {
                    "sent": "Zero test error here and so on so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I don't know why for this noise it's probably to do with its noiseless problem.",
                    "label": 0
                },
                {
                    "sent": "So I don't really say much about.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you do when you have noise just give you a little example from drug discovery.",
                    "label": 0
                },
                {
                    "sent": "Each compound described by vector 139 features groups.",
                    "label": 1
                },
                {
                    "sent": "At two types of sperm.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Friends round one and Round Zero Round zero datasets over 1000 chemically diverse examples 39 are positive.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And or active.",
                    "label": 0
                },
                {
                    "sent": "And if you do the passive learning, you have the black line and the red line.",
                    "label": 0
                },
                {
                    "sent": "If you have active learning.",
                    "label": 0
                },
                {
                    "sent": "So in this case it is that the learning algorithm pick a molecule it knows something about its characteristics and says this is biologically active or not, and it rapidly closes in on the 39 active compounds.",
                    "label": 0
                },
                {
                    "sent": "You can see what's going on if I use passive learning then my hypothesis.",
                    "label": 0
                },
                {
                    "sent": "My current hypothesis is not informing me.",
                    "label": 0
                },
                {
                    "sent": "If I'm using active learning, what's happening is I learn something.",
                    "label": 0
                },
                {
                    "sent": "I update my hypothesis.",
                    "label": 0
                },
                {
                    "sent": "My hypothesis is a little bit better, so the query at last it's going to be a little bit more accurate, so it gains from the new information hypothesis improves a little bit more, and then it posts is even better query.",
                    "label": 0
                },
                {
                    "sent": "So it sort of is running away from the active learning hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The passive learning hypothesis, which is never altered Journal learning process just simply learn some stuff.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a drug discovery example.",
                    "label": 0
                },
                {
                    "sent": "This was done by Gunnar Rage, not me.",
                    "label": 0
                },
                {
                    "sent": "That's.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least one site bounded right?",
                    "label": 0
                },
                {
                    "sent": "So that's active learning versus passive learning.",
                    "label": 0
                },
                {
                    "sent": "Now I want to say a little bit about training SVM's.",
                    "label": 0
                },
                {
                    "sent": "So we've done linear.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Programming quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of quadratic programming routines out there.",
                    "label": 1
                },
                {
                    "sent": "You can get packages like Minasan local Aku.",
                    "label": 0
                },
                {
                    "sent": "You can also download some mathematicians have done very nice implementations of quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "Classic algorithm could use as conjugate gradient.",
                    "label": 1
                },
                {
                    "sent": "Now there is one problem.",
                    "label": 0
                },
                {
                    "sent": "If user kernel methods in practice let me suppose I have a big data set.",
                    "label": 0
                },
                {
                    "sent": "This is my kernel KIJ and I ranges one up to M as oppose.",
                    "label": 0
                },
                {
                    "sent": "I've got a million.",
                    "label": 0
                },
                {
                    "sent": "Samples, that's going to be a whacking great big kernel.",
                    "label": 0
                },
                {
                    "sent": "So what do I do about that?",
                    "label": 0
                },
                {
                    "sent": "I might have a problem loading the kernel into the.",
                    "label": 0
                },
                {
                    "sent": "Into the loading the data into a kernel OK.",
                    "label": 0
                },
                {
                    "sent": "There are actually several ways to handle such a problem.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is the kernel components are valuated, discarded during learning the kernel auditor.",
                    "label": 1
                },
                {
                    "sent": "I gave you give you this afternoon.",
                    "label": 0
                },
                {
                    "sent": "You can do that, but it's slower than SVM and it's not quite as accurate as a less relevant thing, but it can do that.",
                    "label": 0
                },
                {
                    "sent": "You can handle arbitrary sized datasets.",
                    "label": 0
                },
                {
                    "sent": "Another one is the working set method in which you haven't involving subset of data.",
                    "label": 0
                },
                {
                    "sent": "If you like it.",
                    "label": 0
                },
                {
                    "sent": "It's a subset.",
                    "label": 0
                },
                {
                    "sent": "Here it is.",
                    "label": 0
                },
                {
                    "sent": "OK, it learns it's found these are support vectors.",
                    "label": 0
                },
                {
                    "sent": "These guys are non support vectors.",
                    "label": 0
                },
                {
                    "sent": "It chucks out it then brings in some new data having kept the original support vectors only and then learns the new data chucks out the non support vectors takes in your next chunk.",
                    "label": 0
                },
                {
                    "sent": "OK that is a working set method in which you have a evolving subset of data.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately still won't help you in certain cases.",
                    "label": 0
                },
                {
                    "sent": "The next one to use this algorithms which explicitly exploit the structure of the problem OK. And I just want to.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually lead through to something called SMO.",
                    "label": 0
                },
                {
                    "sent": "OK, now chunky and decomposition are really to do with that evolving subset of data which I just gave you just now.",
                    "label": 1
                },
                {
                    "sent": "So I see here the support vectors are found and retained and all other data points are discarded.",
                    "label": 1
                },
                {
                    "sent": "OK, so that's this chunking method or folding subset?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of data OK. Anne.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's what I'm going to say about that.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Right, they're going to want to come onto is decomposition and small algorithm now.",
                    "label": 0
                },
                {
                    "sent": "This was done by John Platt at Microsoft in Redmond and it's a limited case of decomposition.",
                    "label": 0
                },
                {
                    "sent": "Indeed, when I said here you can do chunking an evolving subset of the data.",
                    "label": 0
                },
                {
                    "sent": "OK, so you never saturate, you never have a kernel which is so big you can't put it into memory.",
                    "label": 0
                },
                {
                    "sent": "So the limiting case of.",
                    "label": 1
                },
                {
                    "sent": "Chunking or decomposition is actually just have a subset of two.",
                    "label": 0
                },
                {
                    "sent": "OK, I can't go below 2 samples because in fact I can't do the learning task, but I can go down to two.",
                    "label": 0
                },
                {
                    "sent": "In other words, just to choose two samples at a time, and my kernel is only simply 2 by 2.",
                    "label": 0
                },
                {
                    "sent": "The reason why I inclined to go for two OK is that I can actually count have an analytic expression for telling me what the Alpha should be when I have a subsets of two, so the small set of parameters which can be optimized with each iterations is 2.",
                    "label": 1
                },
                {
                    "sent": "That's because of this constraint OK?",
                    "label": 0
                },
                {
                    "sent": "So I have a very minimalist kernel and this is the idea behind small algorithm which is.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very good algorithm to use in practice.",
                    "label": 0
                },
                {
                    "sent": "OK, whoops.",
                    "label": 0
                },
                {
                    "sent": "I say here possible derive an analytic expression with relatively few numerical operations and.",
                    "label": 1
                },
                {
                    "sent": "But I shouldn't seem to say much more about SMO algorithm itself than than that it can be applied to classification, regression, song and novel detection even.",
                    "label": 0
                },
                {
                    "sent": "John Platt is at Microsoft.",
                    "label": 0
                },
                {
                    "sent": "He can't release code as a result, so but I think somewhere on the web you can get somebody was implemented SMO in C, But.",
                    "label": 0
                },
                {
                    "sent": "It's essentially involves the following sort of idea.",
                    "label": 0
                },
                {
                    "sent": "At each stage, only consider tiny little two by two.",
                    "label": 0
                },
                {
                    "sent": "Kernel solved this problem and it's within a bigger loop.",
                    "label": 0
                },
                {
                    "sent": "OK, so the solution of the Alpha for this sub problem is very very quick and it uses a sort of fact will be able to handle data sets of any size OK.",
                    "label": 0
                },
                {
                    "sent": "Right, that's why I say about SMO.",
                    "label": 0
                },
                {
                    "sent": "I think I might skip further algorithms and.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do do model selection.",
                    "label": 0
                },
                {
                    "sent": "I might also skip kernel massage ING, right?",
                    "label": 0
                },
                {
                    "sent": "So there's one further top to further topics I'm covering my last moments.",
                    "label": 0
                },
                {
                    "sent": "One is model selection now this is important.",
                    "label": 1
                },
                {
                    "sent": "An important issue is a choice of the kernel parameter.",
                    "label": 1
                },
                {
                    "sent": "Indeed, I had a kernel kernel on the board just now an I had a kernel parameter which is Sigma in that kernel or D at the polynomial kernel.",
                    "label": 0
                },
                {
                    "sent": "What actually should be the values of those kernel parameters?",
                    "label": 1
                },
                {
                    "sent": "If it's poorly chosen, then I'll overfit the problem, leading to poor generalization OK?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I illustrate that in the next slide, yes, this is a mirror symmetry problem.",
                    "label": 0
                },
                {
                    "sent": "A noiseless problem, bit like the majority rule.",
                    "label": 0
                },
                {
                    "sent": "If I make one choice of my poor choice of my Sigma and getting a rather bad test error 11% and make the best choice of my Sigma and get 5% error which looks quite good.",
                    "label": 0
                },
                {
                    "sent": "If I go in the opposite direction it takes actually longer to overfit than sort of under fit.",
                    "label": 0
                },
                {
                    "sent": "If I go off in this direction then my Gaussians or my Gaussian kernel of a broad things by Sigma is very broad.",
                    "label": 0
                },
                {
                    "sent": "And it has a relatively hard task finding good solution with a broad Gaussians.",
                    "label": 0
                },
                {
                    "sent": "OK, so I have a sort of point like Gaussians of parsing window type solution.",
                    "label": 0
                },
                {
                    "sent": "Here really doesn't work very well here.",
                    "label": 0
                },
                {
                    "sent": "Very broad Gaussians has a problem, less so than here, but there's this optimum value of Sigma that I really want to know.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the best value of that parameter could be found using cross validation and practice.",
                    "label": 1
                },
                {
                    "sent": "I think I would use cross validation, however, cross validations wasteful of data given example this which I will show you in my last minute, which is Wilms tumor cancer which predict relapse versus non relapse.",
                    "label": 0
                },
                {
                    "sent": "So children's tumor we only had 29 examples so I can't be wasteful with data.",
                    "label": 0
                },
                {
                    "sent": "OK so I can't do a validation study so in certain cases.",
                    "label": 0
                },
                {
                    "sent": "If I can do cross validation, I would do it.",
                    "label": 0
                },
                {
                    "sent": "If I can't do it for some reason then I would fall back on these theoretical results which I'm about to present.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, but leave one out is really a test test statistic I'm saying.",
                    "label": 0
                },
                {
                    "sent": "Well, I could I could do the following for 29.",
                    "label": 0
                },
                {
                    "sent": "I got a really horrible thing that.",
                    "label": 0
                },
                {
                    "sent": "I have training data validation data defined my Sigma and then finally a set of data which is going to be my test performance.",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "Yeah I can do with 29 I could do cross leave one out so I'd have 28 plus one with the 28.",
                    "label": 0
                },
                {
                    "sent": "I've gotta do a cross validation within that as well.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit awkward.",
                    "label": 0
                },
                {
                    "sent": "You could do it, but it's sort of awkward.",
                    "label": 0
                },
                {
                    "sent": "Also leave one out, you have the bias variance sort of issue coming in, so you might be tempted.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But to do that I want to show you that a number of schemes have been presented to actually try and find the kernel parameter from the data without recourse to cross validation.",
                    "label": 0
                },
                {
                    "sent": "OK, I wanted to point out that these.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This now there are a number of these.",
                    "label": 1
                },
                {
                    "sent": "The span rule of fat Nick and there's a nice scheme by Peter Sallick, one of the simplest ones in practice is tossed in Jochims theorem which is in acnl surround.",
                    "label": 0
                },
                {
                    "sent": "About 2000, OK?",
                    "label": 0
                },
                {
                    "sent": "The theorem is the following.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to cut down the theorem A little bit by doing the following OK, the theorems formally stated here.",
                    "label": 0
                },
                {
                    "sent": "B squared is an upper bound on the kernel.",
                    "label": 1
                },
                {
                    "sent": "OK, now I'm going to choose a Gaussian kernel, a Gaussian kernel for getting the stuff at the front is E to the whatever, and so the maximum I could have is going to be one.",
                    "label": 0
                },
                {
                    "sent": "OK, it's like this Gaussian kernel I'm going to say is 1.",
                    "label": 0
                },
                {
                    "sent": "Forget it getting the normalization OK.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose it's a one.",
                    "label": 0
                },
                {
                    "sent": "OK, that's this.",
                    "label": 0
                },
                {
                    "sent": "So be I'm going to set to one.",
                    "label": 0
                },
                {
                    "sent": "I'll also do the hard margin case, which is going to be XI.",
                    "label": 0
                },
                {
                    "sent": "The slack variable is going to be zero.",
                    "label": 0
                },
                {
                    "sent": "OK, just to cut it down.",
                    "label": 0
                },
                {
                    "sent": "In that case the theorem would be the number of points such that 2A is greater than or equal to 1 over the sample size.",
                    "label": 0
                },
                {
                    "sent": "OK, so how to use this theorem in practice?",
                    "label": 0
                },
                {
                    "sent": "Would be for getting us off margin is I would learn I would choose a Sigma value.",
                    "label": 0
                },
                {
                    "sent": "I would run my SVM.",
                    "label": 0
                },
                {
                    "sent": "Can be done quite quickly.",
                    "label": 0
                },
                {
                    "sent": "I would find and in fact say 29 of these data points of the Alphas are.",
                    "label": 0
                },
                {
                    "sent": "Two hour is going to be greater than one out of a data set of hundreds, say and then change my Sigma slightly rerun my SVM, find my new alphas.",
                    "label": 0
                },
                {
                    "sent": "How many two hours was greater than wild?",
                    "label": 0
                },
                {
                    "sent": "Perhaps is 28 / 100 and then I just run it and I've done this in practice and roughly the Sigma value is approximate in the region it should be OK.",
                    "label": 0
                },
                {
                    "sent": "So it does work OK in practice.",
                    "label": 0
                },
                {
                    "sent": "I'm just saying to you if you can do cross validation, I would do it that way, But if you're a little bit desperate then you can use these bounds OK.",
                    "label": 0
                },
                {
                    "sent": "The rough in the right region they looked a bit like the following when I did these simulations.",
                    "label": 0
                },
                {
                    "sent": "Here was the when you did this sort of validation study, it went a bit like this, but like my plot previously and I would find that the result coming out of this theorem be a bit like that, it would be off the actual minimum, but.",
                    "label": 1
                },
                {
                    "sent": "Not too far off, OK?",
                    "label": 0
                },
                {
                    "sent": "Right, so that's the recourse of the desperate.",
                    "label": 0
                },
                {
                    "sent": "If you've got too little data.",
                    "label": 0
                },
                {
                    "sent": "Right, so there are other theorems I'm going to Miss Kernel massage ING and semidefinite programming and.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do data Fusion briefly?",
                    "label": 0
                },
                {
                    "sent": "I talked about these different types of kernels, but wrong.",
                    "label": 0
                },
                {
                    "sent": "I think string kernels, graph kernels, etc.",
                    "label": 0
                },
                {
                    "sent": "And here's an example.",
                    "label": 0
                },
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "Data Fusion actually introduced this subject.",
                    "label": 1
                },
                {
                    "sent": "In fact, this is one of the big nice things about kernel methods is that they are capable of handling multiple types of data using a composite kernel.",
                    "label": 1
                },
                {
                    "sent": "OK, in this case I need to find the beta as well as the Alpha OK.",
                    "label": 0
                },
                {
                    "sent": "The first schemes are know about was Steve Garland.",
                    "label": 0
                },
                {
                    "sent": "Yes, scandal in 2002.",
                    "label": 0
                },
                {
                    "sent": "There's around about 8 methods or six or eight methods and we got one ourselves, not publish where you find the coefficients in composite kernels.",
                    "label": 0
                },
                {
                    "sent": "Very nice ones.",
                    "label": 0
                },
                {
                    "sent": "Marshalla needed a Bayesian way of doing this.",
                    "label": 0
                },
                {
                    "sent": "OK, relative recent paper data integration is in the title in NIPS 2007 or 2006.",
                    "label": 0
                },
                {
                    "sent": "Another similar nice schemes as a semidefinite programming way of doing it, which I think I described.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "There's also a paper called multiple kernels by for large problems in general machine learning research around about 2006 burner short cough and Sonnenburg I think was the first author, so there are a number of schemes for doing composite kernels, and it's not even the only way of doing multiple datasets, right?",
                    "label": 0
                },
                {
                    "sent": "So I just really.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, do it rather briefly.",
                    "label": 0
                },
                {
                    "sent": "Kurt Lancret by Informatica no sorry Journal machine learning research.",
                    "label": 0
                },
                {
                    "sent": "Protein functional classification using yeast protein.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Five types of data, so this is it.",
                    "label": 0
                },
                {
                    "sent": "You got very different data.",
                    "label": 0
                },
                {
                    "sent": "Amino acid sequences in a product kernel graph kernel for protein protein graph kernel for jetting interactions as a graph.",
                    "label": 1
                },
                {
                    "sent": "We contract another graph.",
                    "label": 0
                },
                {
                    "sent": "Kernel expression data.",
                    "label": 0
                },
                {
                    "sent": "Real valued numbers.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We use probably linear kernel or Gaussian kernel I think told to be actually did these experiments.",
                    "label": 0
                },
                {
                    "sent": "You find that 5 using all this data is used better than using a single kernel.",
                    "label": 1
                },
                {
                    "sent": "OK, complex story, but I think what the improvement was from the error test.",
                    "label": 0
                },
                {
                    "sent": "Error was from 71.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm doing classification?",
                    "label": 0
                },
                {
                    "sent": "So it went from 71 to 85 and so must be a classification error.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Test error so definite improvement from.",
                    "label": 0
                },
                {
                    "sent": "From the comment from using composite kernels actually know both the people have been doing composite kernels and I can say one thing that the results presented in their papers look quite good.",
                    "label": 0
                },
                {
                    "sent": "I mentioned 11% just now, so I do know that there's a lot of data sets out there where they got frustrated and told me about their frustration.",
                    "label": 0
                },
                {
                    "sent": "It's quite common in datasets, particularly bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "If I'm using gene expression data.",
                    "label": 0
                },
                {
                    "sent": "In fact, within that data set, it really has the information about the graph, so that implicitly in it.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, one data set can be a whacking great size and other data can be quite small.",
                    "label": 0
                },
                {
                    "sent": "They did find quite a number of sort of cases where what tends to happen is the dominant data set, wins out and forces the other data set.",
                    "label": 0
                },
                {
                    "sent": "Just just gets rid of it, but if the data is implicit, one type of data is implicit in another.",
                    "label": 1
                },
                {
                    "sent": "OK, so there's a.",
                    "label": 0
                },
                {
                    "sent": "It's not a totally rosy stories when I'm sort of saying there so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are instances where it's definitely worked.",
                    "label": 0
                },
                {
                    "sent": "I sort of gives some schemes here, semidefinite programming, but this is slightly out.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Update by the way, right so?",
                    "label": 0
                },
                {
                    "sent": "So as I said, Mark Johnny's done some very nice.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this area by final things, just want to point out and I'm sure getting hungry.",
                    "label": 0
                },
                {
                    "sent": "Just pointing out a few applications.",
                    "label": 0
                },
                {
                    "sent": "Indeed, just one of.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Phone and some other customer applications can't possibly summarize them, I know.",
                    "label": 0
                },
                {
                    "sent": "Isabelle Guyon and she has a web page where she tried to click together all the applications of SVM's and then she ran out of time 'cause there's so much so many applications, but.",
                    "label": 0
                },
                {
                    "sent": "Just give you applications for my own research.",
                    "label": 0
                },
                {
                    "sent": "In fact, I sort of moved on to probabilistic graphical models have been doing a lot in the error cancer formatics which is applying machine learning to cancer type problems.",
                    "label": 0
                },
                {
                    "sent": "So the most interesting stuff I can't tell you about today is not connected to SPMS, but we've actually identified knockdown targets with these guys in super Cancer Research and actually found.",
                    "label": 0
                },
                {
                    "sent": "That means to kill off cancer cells in specific contexts, but here's an example I did with Richard Williams, Kathy Pritchard Jones and others win super Cancer Research, London, published in this Journal here what we wanted to do was predict relapse versus non relapse for something called Wilms tumor from microdata, is a tumor affect children and young adults and you wish therefore to predict ahead.",
                    "label": 1
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will a patient relapse or not OK, I won't tell you much about the technologies MIC array.",
                    "label": 0
                },
                {
                    "sent": "I think many of you might have heard of microarray technology by now, but essentially each of these probes on the substrate is for the activity of a given gene.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think my here.",
                    "label": 0
                },
                {
                    "sent": "Here's what you sort of get out.",
                    "label": 0
                },
                {
                    "sent": "This is an illustration only, but you get out this and that could be normal operation and green an red could be overexpressing under expressing genes, so you'd be looking for genes which.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Abnormal operating OK.",
                    "label": 0
                },
                {
                    "sent": "In our particular sample, this is rather out of date.",
                    "label": 0
                },
                {
                    "sent": "Example typically have many more pros, but this had 17,800 features.",
                    "label": 0
                },
                {
                    "sent": "You couldn't use a neural network on that with 17,000 inputs, but what you realize is that big number.",
                    "label": 0
                },
                {
                    "sent": "The 17,000 gets absorbed in this kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "The kernel matrix actually depends on the smaller number.",
                    "label": 0
                },
                {
                    "sent": "The 29 examples or samples that you have.",
                    "label": 1
                },
                {
                    "sent": "OK, user support vector machine with a linear kernel.",
                    "label": 1
                },
                {
                    "sent": "Big high dimensional input space.",
                    "label": 0
                },
                {
                    "sent": "Relatively small number of samples.",
                    "label": 0
                },
                {
                    "sent": "You therefore guess you can use a linear kernel.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what turns out is that relatively few.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do genes are relevant to the problem?",
                    "label": 0
                },
                {
                    "sent": "There's various things you have to do.",
                    "label": 0
                },
                {
                    "sent": "You have to do leave without testing.",
                    "label": 0
                },
                {
                    "sent": "Well turned out was if you used all the jeans or even the top 100 we had a means of selecting which were relevant features, then your test error 29 roughly balanced relapse versus non relapse was really quite poor.",
                    "label": 0
                },
                {
                    "sent": "14 errors out of 29 you really not got a predictor there at all.",
                    "label": 0
                },
                {
                    "sent": "However this particular cancer.",
                    "label": 0
                },
                {
                    "sent": "Whether you have relapse or non relapse is rather critically dependent on a small number of genes.",
                    "label": 1
                },
                {
                    "sent": "In fact, it's now being found that the primary cause of this particular cancer is due to duplication of the DNA on chromosome one.",
                    "label": 0
                },
                {
                    "sent": "Predominantly OK, so relatively small number of genes are duplicated.",
                    "label": 1
                },
                {
                    "sent": "They simply overexpressed 'cause they've been copied several times.",
                    "label": 0
                },
                {
                    "sent": "So if you get down to this relatively small set of genes, roughly 10 of them they're using leave one out prediction at the 29 you get down to just one or two errors.",
                    "label": 0
                },
                {
                    "sent": "OK, in other words, you have a predictor which is around about 90% accurate.",
                    "label": 0
                },
                {
                    "sent": "We've managed to repeat this and totally novel data.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recently we just writing the paper at the moment, but.",
                    "label": 0
                },
                {
                    "sent": "But you do still get away with about 90% accuracy.",
                    "label": 0
                },
                {
                    "sent": "Big long story, especially with this.",
                    "label": 0
                },
                {
                    "sent": "With this, just show a second example of a quickly from important one, just to show you the nice things PHMSA do.",
                    "label": 0
                },
                {
                    "sent": "This is due to burn, a short cough and.",
                    "label": 0
                },
                {
                    "sent": "Yep, and yellow Coonan other peoples in Red Bank, New York.",
                    "label": 0
                },
                {
                    "sent": "They devise Postal code, recognizes for ATT former Bell Labs and that you were using your network.",
                    "label": 0
                },
                {
                    "sent": "Have important problem recognizing Postal codes and indeed just improvements of point 1.2% big in this area.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, it's a slightly dated example, but Dennis to cost and Benadryl cough looked at this problem and were able to achieve with SVM around about .15% test error reduction over the best alternative which urine liquid had.",
                    "label": 0
                },
                {
                    "sent": "So it was state of the art must have been beaten since, but it was certainly.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, FAM was stated there on this important problem.",
                    "label": 0
                },
                {
                    "sent": "So come the end this afternoon we will be doing actually trying out in practice and I give a small talk before we actually start.",
                    "label": 0
                },
                {
                    "sent": "Because depending on your background you can go from relatively easy to to more demanding project.",
                    "label": 0
                },
                {
                    "sent": "So cool methods are so powerful.",
                    "label": 0
                },
                {
                    "sent": "Systematic do does lots of things.",
                    "label": 0
                },
                {
                    "sent": "Many other kernelized many other types of learning machines.",
                    "label": 1
                },
                {
                    "sent": "I've applied it the medicine by informatics machine vision.",
                    "label": 1
                },
                {
                    "sent": "Postal codes.",
                    "label": 0
                },
                {
                    "sent": "For example, many applications and finance not even got onto that text analysis etc.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll finish there.",
                    "label": 0
                }
            ]
        }
    }
}