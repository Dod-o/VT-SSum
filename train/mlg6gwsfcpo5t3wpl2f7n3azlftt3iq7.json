{
    "id": "mlg6gwsfcpo5t3wpl2f7n3azlftt3iq7",
    "title": "Multiplicative Updates for L1-Regularized Linear and Logistic Regression",
    "info": {
        "coauthor": [
            "Lawrence Saul, Department of Computer Science and Engineering, UC San Diego"
        ],
        "published": "Oct. 8, 2007",
        "recorded": "September 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Linear Models",
            "Top->Computer Science->Machine Learning->Regression"
        ]
    },
    "url": "http://videolectures.net/ida07_saul_mufl1/",
    "segmentation": [
        [
            "Alright, so thank you to all the organizers for arranging this an right now for me and my collaborators.",
            "It's four in the morning, so if I say anything incoherent, please feel free to ask a question."
        ],
        [
            "We cleared up.",
            "It's just due to jet lag and not the actual substance, so let me try to motivate the big picture here, which is by stating some really obvious trends in data analysis and that is that we're getting more and more data so datasets are getting much, much larger.",
            "You know the 90s we had dealing with thousands of examples.",
            "Now we're dealing with millions or billions.",
            "To themselves are getting to be larger and increased dimensionality.",
            "So whether you're working with high resolution or multi spectral images, we're doing text processing where all the sudden you're considering vocabularies with not just thousands of words, But 10s of thousands, hundreds of thousands, or you're working in some biological domain gene expression data.",
            "The each data item itself has has potentially hundreds of thousands or millions of elements to deal with."
        ],
        [
            "Until the question natural question is how are we going to scale with all this data and if your computer architect you say, well, we're just going to be faster computers in every year computers speed will double, but that's probably not enough because actually our datasets are probably they're doubling in size each year, at least if you work in distributed systems networking.",
            "You say we're just going to have tons of computers will bring it all together, but sometimes that's very expensive, and it's also not very easy to program a lot of algorithms in a distributed way, so you have to reorder.",
            "So why are we here?",
            "Well, we believe not just in sort of these purely muscle techniques, but using some more intelligent data analysis and come up with some better algorithms that sort of naturally scale with more data."
        ],
        [
            "An one way to do that I would claim is to look for what I'll call sparse models.",
            "So you're looking for models where essentially the number of parameters you have to estimate, or you have to store some how to use the model.",
            "Don't scale linearly in the size of your data set, or if the dimensionality somehow a lot of the parameters are going to vanish in your ultimate solution.",
            "There's been a number of different models in machine or statistical learning that sort of have this property.",
            "See maybe this not familiar with some of them, not the other button support vector machines or number of model coefficients in the final solution?",
            "Or exactly 0, maybe you're trying to find a decomposition of a matrix such that the individual factors in this decomposition have alot of zero elements, or you're trying to do a regression where you're trying to get a lot of the elements in your weight vector to go exactly to 0."
        ],
        [
            "And so this particular talk piece of research that I'm going to talk about came up because we found that in these very different problems these different models for large margin classification or sort of unsupervised learning in data analysis or linear logistic regression that we found there were similar learning algorithms we could use for all of them.",
            "And they had some very interesting properties, both theoretical and practical, and so that's what I'm going to talk about today.",
            "For the case of linear and logistic."
        ],
        [
            "And.",
            "So the algorithms that have these properties, we usually go into the multiplicative updates.",
            "So instead of simple algorithms where you're optimizing some function by a very integrating descent, these update rules have a more unusual form, so I'll review those for one particular case of interests and I'll talk about how that case relates to problems in sparse linear logistic regression, and at the very end I'll give just sort of overview of some experimental results, and I'm going to try to do this all at a very high level, so the details are in the paper.",
            "I'll just try to convey the most important."
        ],
        [
            "It's here.",
            "So part one on multi."
        ],
        [
            "Looking at updates.",
            "So here's a case study.",
            "It's a very simple optimization problem to state, although not necessarily a very simple one to solve an, just consider you're trying to minimize some quadratic form or some vector V, and this would be very simple, but I'm going to impose a constraint that all the elements of the vector V have to be non negative.",
            "So if you don't impose that constraint, it's a simple least squares problem.",
            "If you do impose that constraint then you can't find the solution analytically.",
            "You have to resort to some sort of iterative solution and what's interesting about the just imposing that constraint is that.",
            "Often you get very very sparse solutions, and that's pretty easy to see why that might be the case if there wasn't that constraint there, then to get a sparse solution for one element to be exactly 0, you know the partial derivative has to exactly vanish right at zero.",
            "But in the case where you can post these constraints, essentially the partial derivative doesn't have to vanish at 04 for the element to vanish, solution just has to point in the one particular direction, so it's much easier for this sort of problem to generate a sparse or highly spa."
        ],
        [
            "Solutions.",
            "So here's a particular way to solving this problem that first I think it almost looks a little.",
            "It looks very mysterious and almost magical, but you can actually show theoretically that it works and it converges rather well.",
            "So we have this quadratic form.",
            "What I want you to do is take this a matrix that appears in the quadratic form and separated into two different pieces, essentially one component of the matrix that contains all the positive elements, and another component that contains all the negative elements, right?",
            "So if I call these two components, a positive and a minus.",
            "That essentially my matrix is just the difference between AM one contains all the positive and I subtract off on negative, and that's sort of what's illustrated by this color coding there.",
            "So.",
            "It's such a simple decomposition sort of thing.",
            "You might see high school.",
            "Nothing, you don't need any linear algebra to do that sort of decomposition."
        ],
        [
            "So here's a particular learning rule in terms of that decomposition.",
            "And what I want you to do is that take your current estimate for the solution, which is a non negative vector 'cause it has to obey those constraints an multiplied by those two matrices.",
            "So let's call it the two matrix vector product that you get out of a matrix vector multiplication to be little a little.",
            "See now I have.",
            "I have three vectors.",
            "I have little a little C and I."
        ],
        [
            "So I have this vector B that is the coefficient of the linear term in my quadratic form."
        ],
        [
            "And here's a very simple.",
            "What kind of strange looking update rule?",
            "Which says take this particular combination of the elements of BA&C which looks just like the quadratic form quadratic rule for solving quadratic equation.",
            "That's not a coincidence and take that and use that as a prefactor.",
            "Use it to multiply elementwise your current element.",
            "Of your solution and get and use that to get a new element.",
            "OK, so it's pretty easy to show that this prefactor is always in fact non negative, so if you start with a nonnegative vector, this multiplicative update will always preserve the non negativity, but you could also show is that.",
            "This update has some very nice properties.",
            "Not only does it not have a learning rate that you have to tweak so that the thing will eventually converge."
        ],
        [
            "Um?",
            "But actually you can show that it automatically converges to the right solution.",
            "So before before I go over that, let me just give you a bit of intuition of how this works.",
            "There are two possible fixed points to this equation that it can converge to.",
            "One is just zero fixed point, 'cause obviously if it's cool to zero, that's a fixed point.",
            "And what typically happens is if zero is a fixed point.",
            "That multiplicative factor is less than one, and this element sort of quickly decays to zero at exponential rate.",
            "If the true solution is greater than zero, then the condition for that prefactor to equal 1, which is necessary at the fixed point, is in fact that the partial derivative with respect to the I component of that quadratic form vanish is, so it's a very natural rule that says basically out of the gradient vanish is, or the gradient doesn't vanish in your slam to zero, and the multiplicative update basically."
        ],
        [
            "Feels that.",
            "Here are the properties.",
            "This mysterious update rule you can actually show that at every iteration your objective function is decreasing, so you never have to worry about getting worse, and in fact, not only do they decrease, but the updates converge.",
            "The global minimum of the problem.",
            "An additional theoretical properties.",
            "There's some very nice practical advantages.",
            "Don't worry about setting a learning rate.",
            "There's absolutely no constraint checking.",
            "Never have to worry about something becoming non negative.",
            "It's also extremely easy to implement an vectorize if you're used to programming in some higher level language like Matlab.",
            "This is like a 3 three line implementation."
        ],
        [
            "OK.",
            "So how does this relate to what I claimed I was going to talk about with sparse problems?"
        ],
        [
            "And regression.",
            "So here's a simple setup for linear regression.",
            "We have some inputs of real valued vectors.",
            "We have.",
            "Some outputs were trying to predict these outputs Y from the real valued vectors in a linear way.",
            "We fit this model as written.",
            "This is a very attractive optimization, just these squares, but if we're working in the case where the dimensionality of our vector inputs greatly exceeds the number of outputs we have, then sort of ill post problem there's many solutions we can take and we need to regularize the problem to get a unique solution."
        ],
        [
            "And there's two very popular forms of regularization.",
            "Basically just add some penalties.",
            "One you had a penalty on the L2 norm of weight vectors to the sum of the squares of the elements, the weight vector, and the other where you just add the sum of the absolute values.",
            "So the difference between these two norms of obviously in the second term an you know there's easy to see them."
        ],
        [
            "Difference between the qualitative difference.",
            "So the L2 norm, nice differential, differentiable, think penalty to add to your function.",
            "It's analytically tractable and it has the property that.",
            "And the solutions favor small weights, but not necessarily nonzero weights, because you still have this condition that the partial derivatives have to vanish exactly for this solution to occur.",
            "That doesn't happen at exactly 0.",
            "Too often the L1 norm is different.",
            "It's not differentiable, but it's still convex, so it still gives rise to a nice optimization landscape.",
            "It's a little harder to solve such a problem because it requires error solution, but it has a very nice property that the estimated weights are sparse, and what you can see that is that even if.",
            "In the first term."
        ],
        [
            "Of this optimization, you have a gradient that sort of pushing, pushing the value of your vector element and W away from zero.",
            "That gradient has to have.",
            "Where did it go?",
            "Somehow I lost a page here."
        ],
        [
            "Oh, there we go that gradient has to have not just a small value, but a finite and large value to overcome the gradient.",
            "The finite gradient from the L1 norm.",
            "So you tend to get a lot of sparse solutions when you add an L1 norm penalty, and that's why they're becoming very useful as a form of feature selection.",
            "Essentially in large scale regret."
        ],
        [
            "Problems.",
            "OK, now it's interesting about this problem is that you can actually reformulate it in a way that reduces the problem I talked about earlier.",
            "I wanna negative quadratic programming and the way you do that is by another one of these sort of non negative decompositions.",
            "So you basically do a change of variables where you select out the positive and negative elements of W and rewrite it W as a difference between a vector U and the vector V, both of which only contain positive elements and you introduce non negativity constraints on those vectors U&V."
        ],
        [
            "And what you can show is when you do that and just make that substitution.",
            "That was the first problem.",
            "You're minimizing a real valued vector W over this on the second problem, you're now minimizing these two vectors U&V with non negative elements.",
            "You change the first problem, which is a convex, but nondifferentiable optimization into one, which is a nice, smooth, objective function just over a vector with non negativity constraints and twice as many variables.",
            "So we're back into the land of."
        ],
        [
            "Not negative quadratic programming, for which there are some advantages so often it's simpler to optimize a smooth function even with constraints.",
            "Then nonsmooth function with construction without constraints, and most importantly for this talk, we're back into this land where we can use this highly appealing multiplicative update, which is well suited for nonnative quadratic programming, and has these convergence properties and these nice properties of implementation."
        ],
        [
            "Everything I just said can also be generalized in a pretty straightforward way to the case logistic regression, where you're not only trying to predict the real value outputs, but you're trying to predict binary outputs.",
            "In that case, you have a slightly different form for the optimization, or you basically plug in the log loss for a logistic function, and in this case you can solve this optimization by essentially at every stage where you are constructing a quadratic fit to the problem and solving it via multiple Ellen regularised linear regression.",
            "So if you can handle linear case.",
            "Sort of automatically gives rise to a iterative optimization procedure for the logistic logistic case."
        ],
        [
            "OK, So what I do now is just show some experiment.",
            "Few experiment results that sort of give you the main flavor.",
            "Why these updates are actually quite useful in these settings and then talk about where you might."
        ],
        [
            "Go next with these.",
            "So I've talked a little bit said how these updates are sort of natural for problems involving sparse solutions, and So what I wanted to 1st show is sort of give you an idea of what happens when you start with a weight vector implies updates and how quickly the elements sort of decay to 0.",
            "If their solution at the global minimum they are actually are zero.",
            "So this was just a problem we made up to demonstrate that typical sort of convergence.",
            "It was for problem where the weight vector was 48 dimensions.",
            "And the ground truth solution of this problem was essentially that you know if you knew the date ahead of time and you knew the noise modeling everything the weight vector would have 16 negative elements, 16 positive elements and 16 that were exactly 0.",
            "OK, so that's the shown at the upper left hand corner.",
            "Now we added a little bit of noise to the data so that if you just.",
            "If you just found the sort of unregularized solution.",
            "In that case, there won't be any zero elements, but then when you throw back in less L1 norm penalty, it sort of decay is those things that are very close to jail, not exactly 0, and pins him down to zero.",
            "So if you start with just a random vector at iteration 0 here to make it easier to visualize, we've sorted the elements by their by their magnitude.",
            "If you start with a random vector iteration 0, after four iterations, the multiplicative update you can just see things are starting to.",
            "Damp down, but after about 16 you can clearly see that it's basically nailed.",
            "Pretty much which elements are going to be 0 and by by the time you've gone to a couple.",
            "Couple you know two or four factors of the dimensionality of your problem, number of updates.",
            "It's pretty much got the solution and nailed the right sparsity pattern."
        ],
        [
            "OK, another another property of these updates and this problem that I haven't really talked about mentioned.",
            "I'll just mention it briefly is that every problem in convex optimization has a dual problem that's also convex and non negative quadratic programming.",
            "The dual of the non negative quadratic program is just another non negative quadratic program which you can also use these multiplicative updates to solve an primal dual nature of convex problems has a very nice structure that it can.",
            "Basically you can use the dual problem to give you a bound on the.",
            "Value of the primal problem, which means that you can basically say give a bound on how close you are to the final solution even before you get there, and that's a property that most boring albums don't have.",
            "You just have to mostly just run.",
            "And when you think it's converted to say, I got there this album, you can actually, if you can construct this tool, say I think I've got there and I can bound how far I actually am away."
        ],
        [
            "So the last result I wanted to talk about high level is just to show yes this these updates.",
            "Julie work.",
            "You can scale them up fairly large, so this was the largest problem we considered in the paper and it was basically a problem in L1 regularize logistic regression applied to a task and basically document classification.",
            "We're trying to classify the topic of a particular document.",
            "There are about 19,000 documents in this data set.",
            "Each of the elements, each of the examples in this problem was essentially had a feature vector consisting of over 1 million elements, and that was those elements were extracted by essentially running an engram counter over the document.",
            "So it's essentially not only does a particular word appear in the document, there's a particular combination of words appear in the document, and every element in this feature vector says whether certain word or certain combinations of words appear.",
            "And obviously you can generate a huge.",
            "Vector of length by just considering do 2 words appear together.",
            "Do 3 words appear together, etc.",
            "OK and you would think that the more current status you put, the more information you have about document.",
            "But of course you have to worry about this problem of overtraining.",
            "So one way to deal with that problem is to add L1 norm which pushes as many elements in your feature in your weight vector to zero as possible.",
            "So it picks up the most relevant cooccurrence features.",
            "So what this graph essentially shows is that even though we're dealing with many, many fewer orders of magnitude.",
            "Your examples then dimensionality the problem by by choosing a suitable regularization and working with a validation set, we can actually find a regularization that gives us a superior predictive value on an independent test set."
        ],
        [
            "That's it, I'll just put my conclusions.",
            "There's a lot of related work that's discussed in the paper, based on other techniques for solving these problems with to summarize the strength of our approach.",
            "Very simple.",
            "It's fairly scalable modular.",
            "You could use it in more complicated models where linear logistic regression here is sort of subcomponents, and I think what's also nice is just the insights you get from having similar algorithms for all these related models.",
            "Thanks.",
            "Any questions?",
            "You said said early on that then the results tend to be sparse.",
            "Are there any results, guarantee, guarantee, or sparse solution?",
            "Or there's been a lot of work?",
            "I think starting with stuff done at Stanford by David Donoho and just L1 norm.",
            "Showing when adding such a norm guarantees or results in a sparse solution with more theoretical nature.",
            "So yeah, I think that reference is actually in the paper.",
            "Have two questions.",
            "First one is what about the government?",
            "It's a free parameter, yes.",
            "Play around a little so that was."
        ],
        [
            "Yeah, I didn't mention that so clearly.",
            "One way to do that is just to just have a independent validation set that you used to tweak that parameter.",
            "So in this case we tried several values of gamma and saw what gave rise to the lowest error on that validation set, and then we use that value to sort of define the model.",
            "That's what get used to generate the test error.",
            "That you can use the other approaches.",
            "You can use a Bayesian approach to try to get that value, but this is sort of the simplest one small D. There doesn't make sense to select 1.2 million features.",
            "No, they're correlated.",
            "There's no doubt thought about this, and there are lots of approaches with using this very high dimensional space to say 500 hundred or something.",
            "Yeah, so.",
            "There are lots of approaches, but you can imagine that in this setting, most of these features are probably irrelevant.",
            "Right, that's right.",
            "Alright, OK, so one way to deal with that is to add L1 norm, which basically automatically prunes you know the hope is automatically prunes out the irrelevant features as opposed to trying to find possible linear combinations of relevant interval.",
            "It's just if you're in a domain where you think many features may be irrelevant, then you sort of want to feature selection approach as opposed to a linear prediction approach.",
            "That's the answer.",
            "Was curious now how?",
            "How quickly this conversion me it still.",
            "We've got a lot of parameters to update, right?",
            "You're still having a parameter for all of those 1.2.",
            "Yeah, so this was about I think.",
            "It's in the paper.",
            "I think it was about 20 minutes or so per run.",
            "The whole world.",
            "Yeah, yeah, not per iteration, no.",
            "So the nice thing about these updates is that I didn't get into this, but.",
            "You can take advantage of sparsity, and nonnegativity too.",
            "So typically these feature vectors.",
            "Each one has 1.2 million, but most of those are elements are in fact 0.",
            "'cause for any particular document, most words don't appear in most word combinations don't appear.",
            "So in fact the sort of scaling time of the algorithm isn't the dimensionality, but sort of the number of non zero elements per per feature vector.",
            "0.",
            "Alpha yes, yeah, that's true.",
            "That's true, that's true.",
            "Ternative approach would be boosting.",
            "Which would typically generate a new feature of consideration at each iteration.",
            "So you would only have a sort of finite small number of non 0.",
            "Rostra yeah I don't know bout just stick regression boosting, but certainly you know the L2 norm branches done boosting version of that with column generation, which is essentially.",
            "You know, generating a new feature that yeah, yeah.",
            "I mean the there always you know the advantage of that is, you do want one at a time.",
            "The disadvantages that inherently sort of a greedy approach 'cause you have to pick just one at a time to add.",
            "So you might approach might fail in the sense that the one you never add was actually the most relevant.",
            "Well I guess eventually yeah yeah.",
            "Yes, yeah yeah.",
            "So we haven't compared to that.",
            "That would be a good comparison, yeah?",
            "So there are things like large explore the full regularization path around one models.",
            "Colleague just published from GLM's.",
            "Do you think you could embed your procedure in it's an algorithm.",
            "Little exploitable regulation.",
            "Yeah, I I don't know the answer to that.",
            "I mean, you take it, you take a, you know the way they do.",
            "That is by noticing that in this particular regimes the solution is sort of piecewise linear or constant, an exploding that we haven't looked at that carefully enough for me to give you a an answer.",
            "I think that would be very interesting if you if you could, but we haven't done it.",
            "Is the Constitution, whatever this is of the matrix A of any concern to you?",
            "Well, it should be positive definite.",
            "Yeah, but.",
            "Just run your algorithm now 'cause you know in classical regression there isn't a Mens modern work to to have a look at that and what happens because it needed influence or speed of convergence.",
            "The linear those things, I mean the properties of a different matrices, able give rise to different convergence, and we've done a little analysis of what makes for fast convergence, but.",
            "But not anything like the amount of literature you know that you're referring to when they talk about you know matrix being ill conditioned.",
            "I mean it's a little strange, 'cause here you you separate the matrix into these positive and negative components.",
            "So you can't do anything as straightforward as just looking at the eigenvalue spectrum of the original matrix.",
            "Conditional matrix somehow disappeared from the problem.",
            "My flip flop a little round if it's really on the pound, weigh something over measure.",
            "I. Yeah, I mean there are probably things to explore.",
            "Typically, you know the sparser solution, the solution, the better.",
            "We find our updates are suited to the problems, but you know, it's just an intuitive observation.",
            "OK, let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so thank you to all the organizers for arranging this an right now for me and my collaborators.",
                    "label": 0
                },
                {
                    "sent": "It's four in the morning, so if I say anything incoherent, please feel free to ask a question.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We cleared up.",
                    "label": 0
                },
                {
                    "sent": "It's just due to jet lag and not the actual substance, so let me try to motivate the big picture here, which is by stating some really obvious trends in data analysis and that is that we're getting more and more data so datasets are getting much, much larger.",
                    "label": 0
                },
                {
                    "sent": "You know the 90s we had dealing with thousands of examples.",
                    "label": 1
                },
                {
                    "sent": "Now we're dealing with millions or billions.",
                    "label": 1
                },
                {
                    "sent": "To themselves are getting to be larger and increased dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So whether you're working with high resolution or multi spectral images, we're doing text processing where all the sudden you're considering vocabularies with not just thousands of words, But 10s of thousands, hundreds of thousands, or you're working in some biological domain gene expression data.",
                    "label": 1
                },
                {
                    "sent": "The each data item itself has has potentially hundreds of thousands or millions of elements to deal with.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Until the question natural question is how are we going to scale with all this data and if your computer architect you say, well, we're just going to be faster computers in every year computers speed will double, but that's probably not enough because actually our datasets are probably they're doubling in size each year, at least if you work in distributed systems networking.",
                    "label": 0
                },
                {
                    "sent": "You say we're just going to have tons of computers will bring it all together, but sometimes that's very expensive, and it's also not very easy to program a lot of algorithms in a distributed way, so you have to reorder.",
                    "label": 1
                },
                {
                    "sent": "So why are we here?",
                    "label": 0
                },
                {
                    "sent": "Well, we believe not just in sort of these purely muscle techniques, but using some more intelligent data analysis and come up with some better algorithms that sort of naturally scale with more data.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An one way to do that I would claim is to look for what I'll call sparse models.",
                    "label": 1
                },
                {
                    "sent": "So you're looking for models where essentially the number of parameters you have to estimate, or you have to store some how to use the model.",
                    "label": 0
                },
                {
                    "sent": "Don't scale linearly in the size of your data set, or if the dimensionality somehow a lot of the parameters are going to vanish in your ultimate solution.",
                    "label": 0
                },
                {
                    "sent": "There's been a number of different models in machine or statistical learning that sort of have this property.",
                    "label": 0
                },
                {
                    "sent": "See maybe this not familiar with some of them, not the other button support vector machines or number of model coefficients in the final solution?",
                    "label": 1
                },
                {
                    "sent": "Or exactly 0, maybe you're trying to find a decomposition of a matrix such that the individual factors in this decomposition have alot of zero elements, or you're trying to do a regression where you're trying to get a lot of the elements in your weight vector to go exactly to 0.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so this particular talk piece of research that I'm going to talk about came up because we found that in these very different problems these different models for large margin classification or sort of unsupervised learning in data analysis or linear logistic regression that we found there were similar learning algorithms we could use for all of them.",
                    "label": 1
                },
                {
                    "sent": "And they had some very interesting properties, both theoretical and practical, and so that's what I'm going to talk about today.",
                    "label": 1
                },
                {
                    "sent": "For the case of linear and logistic.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So the algorithms that have these properties, we usually go into the multiplicative updates.",
                    "label": 0
                },
                {
                    "sent": "So instead of simple algorithms where you're optimizing some function by a very integrating descent, these update rules have a more unusual form, so I'll review those for one particular case of interests and I'll talk about how that case relates to problems in sparse linear logistic regression, and at the very end I'll give just sort of overview of some experimental results, and I'm going to try to do this all at a very high level, so the details are in the paper.",
                    "label": 0
                },
                {
                    "sent": "I'll just try to convey the most important.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's here.",
                    "label": 0
                },
                {
                    "sent": "So part one on multi.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at updates.",
                    "label": 0
                },
                {
                    "sent": "So here's a case study.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple optimization problem to state, although not necessarily a very simple one to solve an, just consider you're trying to minimize some quadratic form or some vector V, and this would be very simple, but I'm going to impose a constraint that all the elements of the vector V have to be non negative.",
                    "label": 0
                },
                {
                    "sent": "So if you don't impose that constraint, it's a simple least squares problem.",
                    "label": 0
                },
                {
                    "sent": "If you do impose that constraint then you can't find the solution analytically.",
                    "label": 0
                },
                {
                    "sent": "You have to resort to some sort of iterative solution and what's interesting about the just imposing that constraint is that.",
                    "label": 0
                },
                {
                    "sent": "Often you get very very sparse solutions, and that's pretty easy to see why that might be the case if there wasn't that constraint there, then to get a sparse solution for one element to be exactly 0, you know the partial derivative has to exactly vanish right at zero.",
                    "label": 0
                },
                {
                    "sent": "But in the case where you can post these constraints, essentially the partial derivative doesn't have to vanish at 04 for the element to vanish, solution just has to point in the one particular direction, so it's much easier for this sort of problem to generate a sparse or highly spa.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solutions.",
                    "label": 0
                },
                {
                    "sent": "So here's a particular way to solving this problem that first I think it almost looks a little.",
                    "label": 0
                },
                {
                    "sent": "It looks very mysterious and almost magical, but you can actually show theoretically that it works and it converges rather well.",
                    "label": 0
                },
                {
                    "sent": "So we have this quadratic form.",
                    "label": 1
                },
                {
                    "sent": "What I want you to do is take this a matrix that appears in the quadratic form and separated into two different pieces, essentially one component of the matrix that contains all the positive elements, and another component that contains all the negative elements, right?",
                    "label": 0
                },
                {
                    "sent": "So if I call these two components, a positive and a minus.",
                    "label": 0
                },
                {
                    "sent": "That essentially my matrix is just the difference between AM one contains all the positive and I subtract off on negative, and that's sort of what's illustrated by this color coding there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's such a simple decomposition sort of thing.",
                    "label": 0
                },
                {
                    "sent": "You might see high school.",
                    "label": 0
                },
                {
                    "sent": "Nothing, you don't need any linear algebra to do that sort of decomposition.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a particular learning rule in terms of that decomposition.",
                    "label": 0
                },
                {
                    "sent": "And what I want you to do is that take your current estimate for the solution, which is a non negative vector 'cause it has to obey those constraints an multiplied by those two matrices.",
                    "label": 0
                },
                {
                    "sent": "So let's call it the two matrix vector product that you get out of a matrix vector multiplication to be little a little.",
                    "label": 0
                },
                {
                    "sent": "See now I have.",
                    "label": 0
                },
                {
                    "sent": "I have three vectors.",
                    "label": 0
                },
                {
                    "sent": "I have little a little C and I.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have this vector B that is the coefficient of the linear term in my quadratic form.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's a very simple.",
                    "label": 0
                },
                {
                    "sent": "What kind of strange looking update rule?",
                    "label": 0
                },
                {
                    "sent": "Which says take this particular combination of the elements of BA&C which looks just like the quadratic form quadratic rule for solving quadratic equation.",
                    "label": 0
                },
                {
                    "sent": "That's not a coincidence and take that and use that as a prefactor.",
                    "label": 0
                },
                {
                    "sent": "Use it to multiply elementwise your current element.",
                    "label": 0
                },
                {
                    "sent": "Of your solution and get and use that to get a new element.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's pretty easy to show that this prefactor is always in fact non negative, so if you start with a nonnegative vector, this multiplicative update will always preserve the non negativity, but you could also show is that.",
                    "label": 0
                },
                {
                    "sent": "This update has some very nice properties.",
                    "label": 0
                },
                {
                    "sent": "Not only does it not have a learning rate that you have to tweak so that the thing will eventually converge.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But actually you can show that it automatically converges to the right solution.",
                    "label": 0
                },
                {
                    "sent": "So before before I go over that, let me just give you a bit of intuition of how this works.",
                    "label": 0
                },
                {
                    "sent": "There are two possible fixed points to this equation that it can converge to.",
                    "label": 0
                },
                {
                    "sent": "One is just zero fixed point, 'cause obviously if it's cool to zero, that's a fixed point.",
                    "label": 0
                },
                {
                    "sent": "And what typically happens is if zero is a fixed point.",
                    "label": 0
                },
                {
                    "sent": "That multiplicative factor is less than one, and this element sort of quickly decays to zero at exponential rate.",
                    "label": 1
                },
                {
                    "sent": "If the true solution is greater than zero, then the condition for that prefactor to equal 1, which is necessary at the fixed point, is in fact that the partial derivative with respect to the I component of that quadratic form vanish is, so it's a very natural rule that says basically out of the gradient vanish is, or the gradient doesn't vanish in your slam to zero, and the multiplicative update basically.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feels that.",
                    "label": 0
                },
                {
                    "sent": "Here are the properties.",
                    "label": 0
                },
                {
                    "sent": "This mysterious update rule you can actually show that at every iteration your objective function is decreasing, so you never have to worry about getting worse, and in fact, not only do they decrease, but the updates converge.",
                    "label": 0
                },
                {
                    "sent": "The global minimum of the problem.",
                    "label": 1
                },
                {
                    "sent": "An additional theoretical properties.",
                    "label": 1
                },
                {
                    "sent": "There's some very nice practical advantages.",
                    "label": 1
                },
                {
                    "sent": "Don't worry about setting a learning rate.",
                    "label": 0
                },
                {
                    "sent": "There's absolutely no constraint checking.",
                    "label": 1
                },
                {
                    "sent": "Never have to worry about something becoming non negative.",
                    "label": 0
                },
                {
                    "sent": "It's also extremely easy to implement an vectorize if you're used to programming in some higher level language like Matlab.",
                    "label": 0
                },
                {
                    "sent": "This is like a 3 three line implementation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how does this relate to what I claimed I was going to talk about with sparse problems?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And regression.",
                    "label": 0
                },
                {
                    "sent": "So here's a simple setup for linear regression.",
                    "label": 1
                },
                {
                    "sent": "We have some inputs of real valued vectors.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Some outputs were trying to predict these outputs Y from the real valued vectors in a linear way.",
                    "label": 0
                },
                {
                    "sent": "We fit this model as written.",
                    "label": 0
                },
                {
                    "sent": "This is a very attractive optimization, just these squares, but if we're working in the case where the dimensionality of our vector inputs greatly exceeds the number of outputs we have, then sort of ill post problem there's many solutions we can take and we need to regularize the problem to get a unique solution.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's two very popular forms of regularization.",
                    "label": 0
                },
                {
                    "sent": "Basically just add some penalties.",
                    "label": 0
                },
                {
                    "sent": "One you had a penalty on the L2 norm of weight vectors to the sum of the squares of the elements, the weight vector, and the other where you just add the sum of the absolute values.",
                    "label": 0
                },
                {
                    "sent": "So the difference between these two norms of obviously in the second term an you know there's easy to see them.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difference between the qualitative difference.",
                    "label": 0
                },
                {
                    "sent": "So the L2 norm, nice differential, differentiable, think penalty to add to your function.",
                    "label": 0
                },
                {
                    "sent": "It's analytically tractable and it has the property that.",
                    "label": 0
                },
                {
                    "sent": "And the solutions favor small weights, but not necessarily nonzero weights, because you still have this condition that the partial derivatives have to vanish exactly for this solution to occur.",
                    "label": 0
                },
                {
                    "sent": "That doesn't happen at exactly 0.",
                    "label": 0
                },
                {
                    "sent": "Too often the L1 norm is different.",
                    "label": 1
                },
                {
                    "sent": "It's not differentiable, but it's still convex, so it still gives rise to a nice optimization landscape.",
                    "label": 0
                },
                {
                    "sent": "It's a little harder to solve such a problem because it requires error solution, but it has a very nice property that the estimated weights are sparse, and what you can see that is that even if.",
                    "label": 1
                },
                {
                    "sent": "In the first term.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of this optimization, you have a gradient that sort of pushing, pushing the value of your vector element and W away from zero.",
                    "label": 0
                },
                {
                    "sent": "That gradient has to have.",
                    "label": 0
                },
                {
                    "sent": "Where did it go?",
                    "label": 0
                },
                {
                    "sent": "Somehow I lost a page here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh, there we go that gradient has to have not just a small value, but a finite and large value to overcome the gradient.",
                    "label": 0
                },
                {
                    "sent": "The finite gradient from the L1 norm.",
                    "label": 1
                },
                {
                    "sent": "So you tend to get a lot of sparse solutions when you add an L1 norm penalty, and that's why they're becoming very useful as a form of feature selection.",
                    "label": 0
                },
                {
                    "sent": "Essentially in large scale regret.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "OK, now it's interesting about this problem is that you can actually reformulate it in a way that reduces the problem I talked about earlier.",
                    "label": 0
                },
                {
                    "sent": "I wanna negative quadratic programming and the way you do that is by another one of these sort of non negative decompositions.",
                    "label": 0
                },
                {
                    "sent": "So you basically do a change of variables where you select out the positive and negative elements of W and rewrite it W as a difference between a vector U and the vector V, both of which only contain positive elements and you introduce non negativity constraints on those vectors U&V.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what you can show is when you do that and just make that substitution.",
                    "label": 0
                },
                {
                    "sent": "That was the first problem.",
                    "label": 0
                },
                {
                    "sent": "You're minimizing a real valued vector W over this on the second problem, you're now minimizing these two vectors U&V with non negative elements.",
                    "label": 0
                },
                {
                    "sent": "You change the first problem, which is a convex, but nondifferentiable optimization into one, which is a nice, smooth, objective function just over a vector with non negativity constraints and twice as many variables.",
                    "label": 0
                },
                {
                    "sent": "So we're back into the land of.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not negative quadratic programming, for which there are some advantages so often it's simpler to optimize a smooth function even with constraints.",
                    "label": 0
                },
                {
                    "sent": "Then nonsmooth function with construction without constraints, and most importantly for this talk, we're back into this land where we can use this highly appealing multiplicative update, which is well suited for nonnative quadratic programming, and has these convergence properties and these nice properties of implementation.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything I just said can also be generalized in a pretty straightforward way to the case logistic regression, where you're not only trying to predict the real value outputs, but you're trying to predict binary outputs.",
                    "label": 0
                },
                {
                    "sent": "In that case, you have a slightly different form for the optimization, or you basically plug in the log loss for a logistic function, and in this case you can solve this optimization by essentially at every stage where you are constructing a quadratic fit to the problem and solving it via multiple Ellen regularised linear regression.",
                    "label": 0
                },
                {
                    "sent": "So if you can handle linear case.",
                    "label": 0
                },
                {
                    "sent": "Sort of automatically gives rise to a iterative optimization procedure for the logistic logistic case.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I do now is just show some experiment.",
                    "label": 0
                },
                {
                    "sent": "Few experiment results that sort of give you the main flavor.",
                    "label": 0
                },
                {
                    "sent": "Why these updates are actually quite useful in these settings and then talk about where you might.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go next with these.",
                    "label": 0
                },
                {
                    "sent": "So I've talked a little bit said how these updates are sort of natural for problems involving sparse solutions, and So what I wanted to 1st show is sort of give you an idea of what happens when you start with a weight vector implies updates and how quickly the elements sort of decay to 0.",
                    "label": 0
                },
                {
                    "sent": "If their solution at the global minimum they are actually are zero.",
                    "label": 0
                },
                {
                    "sent": "So this was just a problem we made up to demonstrate that typical sort of convergence.",
                    "label": 0
                },
                {
                    "sent": "It was for problem where the weight vector was 48 dimensions.",
                    "label": 1
                },
                {
                    "sent": "And the ground truth solution of this problem was essentially that you know if you knew the date ahead of time and you knew the noise modeling everything the weight vector would have 16 negative elements, 16 positive elements and 16 that were exactly 0.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the shown at the upper left hand corner.",
                    "label": 0
                },
                {
                    "sent": "Now we added a little bit of noise to the data so that if you just.",
                    "label": 0
                },
                {
                    "sent": "If you just found the sort of unregularized solution.",
                    "label": 0
                },
                {
                    "sent": "In that case, there won't be any zero elements, but then when you throw back in less L1 norm penalty, it sort of decay is those things that are very close to jail, not exactly 0, and pins him down to zero.",
                    "label": 0
                },
                {
                    "sent": "So if you start with just a random vector at iteration 0 here to make it easier to visualize, we've sorted the elements by their by their magnitude.",
                    "label": 0
                },
                {
                    "sent": "If you start with a random vector iteration 0, after four iterations, the multiplicative update you can just see things are starting to.",
                    "label": 0
                },
                {
                    "sent": "Damp down, but after about 16 you can clearly see that it's basically nailed.",
                    "label": 0
                },
                {
                    "sent": "Pretty much which elements are going to be 0 and by by the time you've gone to a couple.",
                    "label": 0
                },
                {
                    "sent": "Couple you know two or four factors of the dimensionality of your problem, number of updates.",
                    "label": 0
                },
                {
                    "sent": "It's pretty much got the solution and nailed the right sparsity pattern.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, another another property of these updates and this problem that I haven't really talked about mentioned.",
                    "label": 0
                },
                {
                    "sent": "I'll just mention it briefly is that every problem in convex optimization has a dual problem that's also convex and non negative quadratic programming.",
                    "label": 0
                },
                {
                    "sent": "The dual of the non negative quadratic program is just another non negative quadratic program which you can also use these multiplicative updates to solve an primal dual nature of convex problems has a very nice structure that it can.",
                    "label": 1
                },
                {
                    "sent": "Basically you can use the dual problem to give you a bound on the.",
                    "label": 0
                },
                {
                    "sent": "Value of the primal problem, which means that you can basically say give a bound on how close you are to the final solution even before you get there, and that's a property that most boring albums don't have.",
                    "label": 0
                },
                {
                    "sent": "You just have to mostly just run.",
                    "label": 0
                },
                {
                    "sent": "And when you think it's converted to say, I got there this album, you can actually, if you can construct this tool, say I think I've got there and I can bound how far I actually am away.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the last result I wanted to talk about high level is just to show yes this these updates.",
                    "label": 0
                },
                {
                    "sent": "Julie work.",
                    "label": 0
                },
                {
                    "sent": "You can scale them up fairly large, so this was the largest problem we considered in the paper and it was basically a problem in L1 regularize logistic regression applied to a task and basically document classification.",
                    "label": 0
                },
                {
                    "sent": "We're trying to classify the topic of a particular document.",
                    "label": 0
                },
                {
                    "sent": "There are about 19,000 documents in this data set.",
                    "label": 0
                },
                {
                    "sent": "Each of the elements, each of the examples in this problem was essentially had a feature vector consisting of over 1 million elements, and that was those elements were extracted by essentially running an engram counter over the document.",
                    "label": 0
                },
                {
                    "sent": "So it's essentially not only does a particular word appear in the document, there's a particular combination of words appear in the document, and every element in this feature vector says whether certain word or certain combinations of words appear.",
                    "label": 0
                },
                {
                    "sent": "And obviously you can generate a huge.",
                    "label": 0
                },
                {
                    "sent": "Vector of length by just considering do 2 words appear together.",
                    "label": 0
                },
                {
                    "sent": "Do 3 words appear together, etc.",
                    "label": 0
                },
                {
                    "sent": "OK and you would think that the more current status you put, the more information you have about document.",
                    "label": 0
                },
                {
                    "sent": "But of course you have to worry about this problem of overtraining.",
                    "label": 0
                },
                {
                    "sent": "So one way to deal with that problem is to add L1 norm which pushes as many elements in your feature in your weight vector to zero as possible.",
                    "label": 0
                },
                {
                    "sent": "So it picks up the most relevant cooccurrence features.",
                    "label": 0
                },
                {
                    "sent": "So what this graph essentially shows is that even though we're dealing with many, many fewer orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "Your examples then dimensionality the problem by by choosing a suitable regularization and working with a validation set, we can actually find a regularization that gives us a superior predictive value on an independent test set.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's it, I'll just put my conclusions.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of related work that's discussed in the paper, based on other techniques for solving these problems with to summarize the strength of our approach.",
                    "label": 1
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "It's fairly scalable modular.",
                    "label": 0
                },
                {
                    "sent": "You could use it in more complicated models where linear logistic regression here is sort of subcomponents, and I think what's also nice is just the insights you get from having similar algorithms for all these related models.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "You said said early on that then the results tend to be sparse.",
                    "label": 0
                },
                {
                    "sent": "Are there any results, guarantee, guarantee, or sparse solution?",
                    "label": 0
                },
                {
                    "sent": "Or there's been a lot of work?",
                    "label": 0
                },
                {
                    "sent": "I think starting with stuff done at Stanford by David Donoho and just L1 norm.",
                    "label": 0
                },
                {
                    "sent": "Showing when adding such a norm guarantees or results in a sparse solution with more theoretical nature.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I think that reference is actually in the paper.",
                    "label": 0
                },
                {
                    "sent": "Have two questions.",
                    "label": 0
                },
                {
                    "sent": "First one is what about the government?",
                    "label": 0
                },
                {
                    "sent": "It's a free parameter, yes.",
                    "label": 0
                },
                {
                    "sent": "Play around a little so that was.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I didn't mention that so clearly.",
                    "label": 0
                },
                {
                    "sent": "One way to do that is just to just have a independent validation set that you used to tweak that parameter.",
                    "label": 0
                },
                {
                    "sent": "So in this case we tried several values of gamma and saw what gave rise to the lowest error on that validation set, and then we use that value to sort of define the model.",
                    "label": 0
                },
                {
                    "sent": "That's what get used to generate the test error.",
                    "label": 0
                },
                {
                    "sent": "That you can use the other approaches.",
                    "label": 0
                },
                {
                    "sent": "You can use a Bayesian approach to try to get that value, but this is sort of the simplest one small D. There doesn't make sense to select 1.2 million features.",
                    "label": 0
                },
                {
                    "sent": "No, they're correlated.",
                    "label": 0
                },
                {
                    "sent": "There's no doubt thought about this, and there are lots of approaches with using this very high dimensional space to say 500 hundred or something.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "There are lots of approaches, but you can imagine that in this setting, most of these features are probably irrelevant.",
                    "label": 0
                },
                {
                    "sent": "Right, that's right.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK, so one way to deal with that is to add L1 norm, which basically automatically prunes you know the hope is automatically prunes out the irrelevant features as opposed to trying to find possible linear combinations of relevant interval.",
                    "label": 0
                },
                {
                    "sent": "It's just if you're in a domain where you think many features may be irrelevant, then you sort of want to feature selection approach as opposed to a linear prediction approach.",
                    "label": 0
                },
                {
                    "sent": "That's the answer.",
                    "label": 0
                },
                {
                    "sent": "Was curious now how?",
                    "label": 0
                },
                {
                    "sent": "How quickly this conversion me it still.",
                    "label": 0
                },
                {
                    "sent": "We've got a lot of parameters to update, right?",
                    "label": 0
                },
                {
                    "sent": "You're still having a parameter for all of those 1.2.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this was about I think.",
                    "label": 0
                },
                {
                    "sent": "It's in the paper.",
                    "label": 0
                },
                {
                    "sent": "I think it was about 20 minutes or so per run.",
                    "label": 0
                },
                {
                    "sent": "The whole world.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, not per iteration, no.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about these updates is that I didn't get into this, but.",
                    "label": 0
                },
                {
                    "sent": "You can take advantage of sparsity, and nonnegativity too.",
                    "label": 0
                },
                {
                    "sent": "So typically these feature vectors.",
                    "label": 0
                },
                {
                    "sent": "Each one has 1.2 million, but most of those are elements are in fact 0.",
                    "label": 0
                },
                {
                    "sent": "'cause for any particular document, most words don't appear in most word combinations don't appear.",
                    "label": 0
                },
                {
                    "sent": "So in fact the sort of scaling time of the algorithm isn't the dimensionality, but sort of the number of non zero elements per per feature vector.",
                    "label": 0
                },
                {
                    "sent": "0.",
                    "label": 0
                },
                {
                    "sent": "Alpha yes, yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's true, that's true.",
                    "label": 0
                },
                {
                    "sent": "Ternative approach would be boosting.",
                    "label": 0
                },
                {
                    "sent": "Which would typically generate a new feature of consideration at each iteration.",
                    "label": 0
                },
                {
                    "sent": "So you would only have a sort of finite small number of non 0.",
                    "label": 0
                },
                {
                    "sent": "Rostra yeah I don't know bout just stick regression boosting, but certainly you know the L2 norm branches done boosting version of that with column generation, which is essentially.",
                    "label": 0
                },
                {
                    "sent": "You know, generating a new feature that yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean the there always you know the advantage of that is, you do want one at a time.",
                    "label": 0
                },
                {
                    "sent": "The disadvantages that inherently sort of a greedy approach 'cause you have to pick just one at a time to add.",
                    "label": 0
                },
                {
                    "sent": "So you might approach might fail in the sense that the one you never add was actually the most relevant.",
                    "label": 0
                },
                {
                    "sent": "Well I guess eventually yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So we haven't compared to that.",
                    "label": 0
                },
                {
                    "sent": "That would be a good comparison, yeah?",
                    "label": 0
                },
                {
                    "sent": "So there are things like large explore the full regularization path around one models.",
                    "label": 0
                },
                {
                    "sent": "Colleague just published from GLM's.",
                    "label": 0
                },
                {
                    "sent": "Do you think you could embed your procedure in it's an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Little exploitable regulation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I I don't know the answer to that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you take it, you take a, you know the way they do.",
                    "label": 0
                },
                {
                    "sent": "That is by noticing that in this particular regimes the solution is sort of piecewise linear or constant, an exploding that we haven't looked at that carefully enough for me to give you a an answer.",
                    "label": 0
                },
                {
                    "sent": "I think that would be very interesting if you if you could, but we haven't done it.",
                    "label": 0
                },
                {
                    "sent": "Is the Constitution, whatever this is of the matrix A of any concern to you?",
                    "label": 0
                },
                {
                    "sent": "Well, it should be positive definite.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but.",
                    "label": 0
                },
                {
                    "sent": "Just run your algorithm now 'cause you know in classical regression there isn't a Mens modern work to to have a look at that and what happens because it needed influence or speed of convergence.",
                    "label": 0
                },
                {
                    "sent": "The linear those things, I mean the properties of a different matrices, able give rise to different convergence, and we've done a little analysis of what makes for fast convergence, but.",
                    "label": 0
                },
                {
                    "sent": "But not anything like the amount of literature you know that you're referring to when they talk about you know matrix being ill conditioned.",
                    "label": 0
                },
                {
                    "sent": "I mean it's a little strange, 'cause here you you separate the matrix into these positive and negative components.",
                    "label": 0
                },
                {
                    "sent": "So you can't do anything as straightforward as just looking at the eigenvalue spectrum of the original matrix.",
                    "label": 0
                },
                {
                    "sent": "Conditional matrix somehow disappeared from the problem.",
                    "label": 0
                },
                {
                    "sent": "My flip flop a little round if it's really on the pound, weigh something over measure.",
                    "label": 0
                },
                {
                    "sent": "I. Yeah, I mean there are probably things to explore.",
                    "label": 0
                },
                {
                    "sent": "Typically, you know the sparser solution, the solution, the better.",
                    "label": 0
                },
                {
                    "sent": "We find our updates are suited to the problems, but you know, it's just an intuitive observation.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}