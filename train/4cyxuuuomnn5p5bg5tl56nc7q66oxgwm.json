{
    "id": "4cyxuuuomnn5p5bg5tl56nc7q66oxgwm",
    "title": "Using Minimum Description Length to make Grammatical Generalizations",
    "info": {
        "author": [
            "Mike Dowman, Department of General Systems Studies, Graduate School of Arts and Sciences, University of Tokyo"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Human Language Technology"
        ]
    },
    "url": "http://videolectures.net/mlcs07_dowman_umd/",
    "segmentation": [
        [
            "Right, so there's two main ideas.",
            "Something we talked about in this talk.",
            "Now.",
            "The first one is I'm going to tell you why.",
            "Minimal description that is such a good idea from the point of view of syntax, which quite a few other people have been doing.",
            "And certainly when I say minimal description that includes all kinds of related things, such as Bayesian inference like Amy talked about this money.",
            "An OK but the other thing is I want to just give a bit of an overview about the relationship between theoretical syntax and testicle approaches to machine learning.",
            "Yeah, OK."
        ],
        [
            "Anne.",
            "Right, OK, so I think well, various people have made this kind of point.",
            "Well, what is syntactic theory about?",
            "So I think the key things that theoretical syntacticians worry about is one of these two things.",
            "Either they're just trying to write a grammar that explains what sentences are grammatical and water, not grammatical in a language, or they're trying to get some explanation of how to transform observed sentences into a grammar.",
            "So either that's just some discovery procedure or it's a psychological model of learning, and but whichever of them I think MDL is certainly relevant to explain.",
            "Either of those, but better perspective you take on some."
        ],
        [
            "Tactic theory.",
            "OK well.",
            "There's kind of an issue about how we should study syntax because so the majority of people who study syntax are syntacticians and the way they do it is very, very different to the way that computational linguistics.",
            "Linguists study syntax, I think, so I don't know.",
            "I mean, if there are theoretical syntacticians here, I don't know if they're going to agree with what my characterization of what they're doing.",
            "So what I'm saying is essentially they decide on some subset of language that they think is coherent and interesting.",
            "They might look at WH question formation, phrase structure, different different subcommunities that syntacticians tend to focus on, often quite complementary aspects of language, and that's you know, some language linguists talk about core grammar.",
            "But if you talk to, say, descriptive community in Australia, their idea of what's important is very different than ever.",
            "Talk about some Jason C or things like that.",
            "Which is very, very important if you're working kind of jump skis frameworks.",
            "And then, well, what do we do with these sentences?",
            "Well, they get an informant, and the informant will tell them which sentence is grammatical and which aren't.",
            "So they take some sentences and some sensors I think could be grammatical, and ask them.",
            "And OK, so in the majority of cases.",
            "The informant is actually the linguists themselves, or in a large number of cases, but it doesn't have to be.",
            "They can go and ask someone else, so they can do.",
            "They can just get one for men to do some more psychological type study where your stores people doesn't matter, and then going to try and write themselves a grammar based on that data."
        ],
        [
            "OK.",
            "Right, well, computational linguists approach, and so I'm really talking supervised learning.",
            "That's the most relevant stuff is quite different, so they've taken a corpus.",
            "And of course, that Corpus has been generated by informants in a way we can call informance because it's their language that was really studying, just somewhat indirectly.",
            "OK, something went wrong with my font there, I think.",
            "The M yeah, this corpus is then fed into a computer and we lost the image and then the computer tries to make either it's going to extract some information about the language like verbs of categories or word classes as a lexicon, something like that, or it's going to try to learn some sort of grammar and the grammar doesn't.",
            "The grammar is not usually like a full tree structure.",
            "It could be a part of speech tagger that's some sort of grammar that explains some subset of language.",
            "But importantly, the real difference I think, is the kind of data using in this.",
            "This is run on, usually on naturally occurring texts, so it's much, much wider range of sentences."
        ],
        [
            "Syntacticians look at so well.",
            "Which of these approaches gives us the most insight into languages?",
            "Well, The thing is, the linguists.",
            "They really want to get high position.",
            "You want to explain exactly which sentence is.",
            "It could sentences and which aren't, and even even if there, even if the life of fuzziness in the grammar, they still want to explain which senses are fuzzy and which are not.",
            "We want to classify every sentence that is in the sentences taken thinking about.",
            "But of course the only achieve very limited.",
            "And I would argue, very arbitrary coverage.",
            "You know, given the huge number of possible sentences in a language, they only explain some of them.",
            "So conversation linguists, of course they get much, much better coverage in that you know their grammars relate to the whole corpus is very well defined.",
            "What data they're explaining, but they never account for the whole corpus completely.",
            "They only learned grammar.",
            "That's probabilistically correct, or capture, and usually captures some very limited aspects of the grammar as well.",
            "So essentially they're taking the train is not very different problems.",
            "And I think they're both explaining things about language that the other approach doesn't, but that basically complimentary."
        ],
        [
            "Complementary OK, so why?",
            "Why are we actually doing syntax?",
            "Why do we care?",
            "OK, well, I mean I'm coming in this talk more from the theoretical syntax perspective.",
            "So what I'm suggesting is, well syntacticians have different ideas on what you know, why they care, or why syntax is interesting.",
            "But one of the things is certainly within the Chomsky perspective.",
            "What they're really interested in is the human mind in learning in generalization.",
            "In this kind of process.",
            "OK, so one of the biggest problems is to explain how can how can a child cope with this massive boots brush staffing problem?",
            "I mean, one of the really, really distinctive features of natural human languages and the mix and distinct from, say animal communication systems or just other things we learn is absolutely massive and they really Contacts.",
            "They've got 10s of thousands of words and it's we have a massive sparse data problem, so the Lingus account certainly doesn't account for that kind of problem, which is for me.",
            "It's a very interesting psychological problem.",
            "But theoretical syntax has almost nothing to say about that.",
            "OK, so this suggests that, well, we should.",
            "We should actually be if we want to do syntax, and we really should be looking at the corporate naturally occurring data and trying to explain how to learn a gram, or at least to explain the sentences in those corporate.",
            "But that's not really going to shed any light on the kind of sentences that syntacticians focus on.",
            "You can't get very fast settings adjacency by looking at the corpus, 'cause most of the interesting sentences you're interested.",
            "I just so incredibly rare that you can only get them by thinking of them and.",
            "Asking informants and just selecting sentences that form some coherence subset."
        ],
        [
            "Anne.",
            "So.",
            "OK, so I mean I think even with that perspective it's not clear which way we should go.",
            "So just just to think about some of the things that Syntacticians are concerned with.",
            "That probably doesn't make it into consider.",
            "You wouldn't pick up by doing a corpus study or learning from a corpus.",
            "So of course one of the key issues in syntax is negative evidence.",
            "Think of these sentences, John Hurt himself, Mary hit John, and then look at another verb.",
            "John hated himself, Mary hated John, so.",
            "You've got a little language you've got to generalize from the sentences you've heard.",
            "And make some languages very productive.",
            "Most sentences anyone says novel sentences have never been said by anyone before.",
            "So.",
            "If children hear things like this that will constantly generalizing, so if they hear John behaved himself, presumably they're going to think Mary behave.",
            "John, now, of course, I think anyone any native speaker of English is going to agree that.",
            "I mean, it makes perfect sense.",
            "I can understand exactly what that would mean if it was English, but it's not English.",
            "You can't say that, right?",
            "So this is the kind of thing why do we have these gaps in language?",
            "What's somehow you've got to infer that some things you just can't."
        ],
        [
            "Say OK, so how do we do this?",
            "Well, of course if you don't hear something you can assume you can't see it.",
            "OK, but that's not any good because most sentences we've never heard before.",
            "Most possible English sentences have never been uttered and never will be at it, because this language is so productive the same for most phrases.",
            "Even verb argument constructions were constantly using verbs with new arguments or even making novel verbs.",
            "If you give you give people novel verbs, wanted verbs are used in all the different conjugations, there may be a little bit conservative, especially children, but they.",
            "They will, if necessary, using past tense, present tense, singular, plural, and so forth.",
            "So if we can use negative evidence, non occurrences, negative evidence we've got to answer these two questions.",
            "How often does something have to not occur before we decide it's not grammatical and at what level do we make structural generalization?",
            "This is a sentence of phrase, you know what are the relevant units.",
            "Essentially what I'm arguing is that this is what MDL gives us.",
            "Other methods don't, but I don't think unless using MDL type metric, I don't think you can get."
        ],
        [
            "Coming out in any general way.",
            "OK, right, so lots of people said Wendell does OK. We prefer simple grammars and grammar.",
            "Laugh is simple descriptions of the data.",
            "If we can capture this structure in the data that's in the grammar that's in the data craft and we don't have to put it in when we describe the data so."
        ],
        [
            "To have a more compact description.",
            "OK so I know people talked about this, but I'll just give another kind of analogy to help really understand the consequences of MDL so we can imagine that this square represents a space of possible sentences.",
            "And then we've observed these particular sentences.",
            "So what's our grammar learning task?",
            "A grammar learning task is to say which parts of the square correspond to grammatical sentences and which parts don't."
        ],
        [
            "OK, so the simplest kind of grammar we could have it.",
            "This is really simple.",
            "So MDL gives that grammar greater valuation except that it really doesn't explain the data very well.",
            "So there's lots of information need to say whereabouts in this big big space.",
            "The sentences are so overall it's not going."
        ],
        [
            "Very good evaluation.",
            "OK, the next option is, well, the other extreme.",
            "We can just make a grammar describes the data really well.",
            "'cause it says, right?",
            "The only data allowed is a data observed.",
            "No generalization beyond observed data there.",
            "So the data that we're going to have excellent evaluation in the data part of evaluation metric.",
            "But of course all we've done is we've taken all the complexity from the data input in the grammar, so we haven't achieved anything we've."
        ],
        [
            "You know, we've got a really complex grammar, so obviously I think quite the solution over the three solutions.",
            "This is the one that characterizes the data best.",
            "It's somewhere between those two extremes, so this is what NDL is telling us.",
            "It's just giving us a measure of formal measure that says how good a grammar is to fit data, and it tells us what's up."
        ],
        [
            "Generalization and what's not.",
            "OK, so it's fair enough to know that if you look at machine learning, there's lots of techniques out there and actually MDL isn't used very much in the computational linguistics community.",
            "Sophie velocity well where the fennville such a good technique.",
            "Why aren't there all these great results?",
            "There's not an amazing number of brilliant results in the, say, computation, linguistics, community using NDL for learning from big corpora so."
        ],
        [
            "Why do I think MD is a good idea?",
            "OK Well 2 two other techniques that people suggest I use instead.",
            "Its maximum likelihood and we can consider maximum likelihood to be MDL.",
            "Will we just haven't even prior on all dramas so it's not it's Bayesian inference and even prior or grammars.",
            "But of course then the best grammar is just the one that reproduces all the sentences that we observed in the 1st place.",
            "So if you want if you want maximum likelihood to make generalizations, there must be some limit on how well the parameters you're fitting can explain the data and it's not limit on how well.",
            "We can fit parameters that decides the degree of generalization.",
            "So really, what level or how much?",
            "It generalizes not decided by maximum likelihood.",
            "It's set externally.",
            "And of course, in any case you can set it up for any any particular problem to make appropriate generalizations, but it's not going to be general when you."
        ],
        [
            "Flight to different data set.",
            "An interesting maximum entropy is in some ways it's kind of the opposite, because instead of making as restrictive grammars possible, you make grammar as unrestricted as possible.",
            "So in this case, to stop it being overly general and just saying any any string of words is a valid sentence, you've got to put some constraints and you've got to fix certain parameters that this.",
            "This maximum entropy has to match certain properties of the data, so again.",
            "You also really fixing the degree of generality externally, so neither of those approaches is really going to."
        ],
        [
            "Solve the problem of generalization.",
            "OK, so getting onto the model so this.",
            "I've got basically the valuation measure that I've been using for some time, and I originally used it to learn very simple grammars describing an.",
            "Just very very very small compara simple data, so the grammar formalism is just binary branching or non branching context free phrase structure rules.",
            "And every writer grammar we can calculate the description of the grammar just by how many symbols are in it in the coding length of all those symbols.",
            "And then because the grammar realized certain data is going to reduce the cutting length of the data and we can cut the data just by saying which rules were used to produce it."
        ],
        [
            "OK.",
            "Right, so this just summarizes the actual encoding in the model has got full stages.",
            "First of all we need to say how often each symbol occurs in the grammar.",
            "But because the number of symbols is always fixed, even though not all the grammars always uses symbols.",
            "So I always just say there's more symbols than you need.",
            "It doesn't matter.",
            "You could give it a million symbols that we don't use a handful of them.",
            "So, but that's going to fix it grammar.",
            "We just ignore that I then the cutting length is grammar symbol by symbol, the frequency of each rule.",
            "So it just depends on the number of rules and then.",
            "The data which is coded as a list of which grammar rules we choose, so the grammar evaluation will be the grammar in the rule frequencies together and then the data part will be the data evaluation."
        ],
        [
            "OK, so this is kind of.",
            "Some of the original data is tested.",
            "This on that is the entire corpus that it learned from, and it can produce this kind of grammar.",
            "Of course, it doesn't use those labels because it doesn't know apriori Watson noun phrase or verb phrases, it just uses different average symbol for each one of them.",
            "So I think it's quite well established at MDL can learn.",
            "Very effectively, but with very small simple datasets.",
            "I mean, that's the kind of thing I think Stolcke originally showed that Bayesian model merging, which is essentially the same general idea."
        ],
        [
            "Jail.",
            "OK, so of course sceptical people said to me, yeah, that's great, but it doesn't look anything like real language.",
            "What happens if you apply MDL to natural language corpora well?",
            "Of course I could take my evaluation metric and apply it, but there's such a big search space of grammars it would just get nowhere.",
            "I mean, if you could run it in theory for an infant length of time it would do fine.",
            "I should learn your grammar or offended, It's a good concept program as you can get.",
            "But it's just, you know, that's never going to happen.",
            "If we could do that, we basically have finished syntax 'cause we know how to construct a grammar.",
            "So I've gotta find some way to simplify the task and So what I did is I decided that I'm only going to learn verbs."
        ],
        [
            "Categorization classes OK. And how did this?",
            "Well, I need to workout what the sub categorizations of verbs were.",
            "So I took the switchboard part as the Penn Treebank and I found all the past tense verbs.",
            "And a took.",
            "The verb and the top level constituents within the noun phrase and.",
            "I made a subcategorization frame for each occurrence of each verb, and I used to start to represent the position of the verb.",
            "So spend here occurs with two arguments.",
            "Anything within any top level, constituents within verb phrase for considered to be arguments.",
            "I mean, maybe someone reality agents, but that's I just put them all into parts of categorization so spent can take a noun phrase in the sentence."
        ],
        [
            "OK so I got.",
            "In the whole of the switchboard corpus or the switch will part the pantry banker got twenty 1759 training instances.",
            "Then there were 704 different verbs and 706 distinct subcategorization frames, and there were 25 different kinds of top level constituent alongside the verb.",
            "So things like Sbar, NB adverbial phrase, just, you know those labels will decide by whoever decided the coding scheme for the Penn Treebank."
        ],
        [
            "OK, Ann.",
            "Right, so how to?",
            "I wanted to use the points I wanted to show that this original metric that learns small grammars works on big grammars, so I wanted to use exactly the same evaluation metric as it used before.",
            "OK, So what I did is you can write rules like this which just say, well let's say two things they say which sub categorizations.",
            "An occur with verbs of each class and then which verbs appear in which class.",
            "So this little if this was, you know, if there any three verbs this grammar would say well grew and ended up can appear without the subnet one also cut two and did only a subset two, so it doesn't actually have any access to the individual structures subcategorization.",
            "So just each map to a number.",
            "I've lost information.",
            "It's got very restricted amount of information form of classes but.",
            "We would expect it to get an improved evaluation or the best grammar if some categorizations with simply similar versus similar categorizations were grouped."
        ],
        [
            "OK, now is the.",
            "That's great for evaluating these different grammars, where each grammar is just an assignment of verbs to classes.",
            "But if I want to actually learn, I've gotta have a search mechanism to generate candidate grammars that can then be evaluated by the MDM metric, so the NDL is going to be what decides which grammar keep on, which is the best grammar.",
            "But you need to search mechanism, just search the space of possible grammars and hopefully it will find the one with the lowest evaluation.",
            "It is I just started with the grammar in which all the verbs were in one class.",
            "And then it just.",
            "Every iteration took of urban moved at random to new class or to different class.",
            "So the first movie would have to be new class 'cause there's only one class and then it just if the source class was empty, was deleted and then always any redundant rules were removed.",
            "So there had to be a rule for each verb specifying each verb plus specifying all this other categorizations.",
            "Which verbs in that class were seen in the day?"
        ],
        [
            "To.",
            "OK, so I just didn't annealing search.",
            "It just means that you prefer you prefer moves to improve the evaluation loaders description length.",
            "But in order to overcome local Maxima, specially the beginning of the search, even if it makes a bit worse, you might accept it and then at the end of the search you only accept changes to make it worse, the improved evaluation OK and then after that got stuck and didn't find any changes for awhile.",
            "It went into emerging phase to see if there were just any redundant rules for classes and merge them altogether.",
            "And then after no changes were accepted for 2000 iterations, it switched back to the moving phase and it just kept alternating those to try and make sure that it found the best grammar an because there's a potential for this kind of search to get stuck in local Maxima.",
            "And it certainly didn't.",
            "Some cases I run that multiple times and just took the grammar with the lowest evaluation.",
            "And, well, we can't be sure that we actually got the lowest single grammar, but."
        ],
        [
            "It's the last one I found.",
            "OK, so here's the evaluation so.",
            "The original one verb plus grammar.",
            "The other extreme grammar.",
            "We've just put everything in the separate class that made in the generalizations and then the best learn grammar which has six classes.",
            "So unsurprisingly, the grammar evaluation is best of one class because it's the simplest possible grammar you can have, but it just doesn't characterize the data very well, so the data is a very big description length.",
            "If you put it in a separate class, well, we've got a really complicated describes the data the best.",
            "It's bound to do the best because it's not made any generalizations, but the grammar.",
            "Relation is very bad because the grammar is so complicated.",
            "So when you generalize and simplify the grammar, you're making some generalizations.",
            "You don't fit the observed data quite so well, the data valuations higher than each having separate class, but because the grammar evaluation is not so much, much better than even the separate class, we get a much better."
        ],
        [
            "Overall valuation OK, but we won't.",
            "We won't know.",
            "It's really learned anything linguistically interesting until you go and look at the data that we've learned, so I just looked at the web classes and said, well, are these linguistically coherent and these are the six verb classes that came out an.",
            "So OK, yeah, OK.",
            "So the first class seems to contain verbs that take an S or an sbar compliment.",
            "So you can say John thought that John value that John knew that, so that's certainly seems to be linguistically justified.",
            "There's something syntactically in common in these verbs and 2nd class.",
            "Essentially, the transitive verbs typically taken NP argument, and sometimes something else as well.",
            "3rd Class well did did has a distribution.",
            "It's really distinct from any other verb, so it seems quite reasonable to make a special class just to account for this one verb.",
            "And then, well, classful.",
            "I couldn't really see what that costs about adjusting to miscellaneous.",
            "It contained by far the vast majority of this hundreds of verbs.",
            "So in that case, well, no, it just said constant thing about these verbs.",
            "Just don't wanna miscellaneous class.",
            "We had this very small class that was verbs.",
            "It took an S but not an sbar and this is because in the Penn treebank, things like Jaune named the ship Mary or something.",
            "And that accounts for quite a lot of these.",
            "These verbs and the use of them.",
            "And then finally, this verbs that take a particle so it picked up on that fact.",
            "You only see when you see wound.",
            "You tend to see wound up.",
            "You tend to see grew up, ended up, closed up, backed up.",
            "OK, so that's why they've been put in a separate class, OK?"
        ],
        [
            "So, well, I think we can say that the learn verb classes linguistically coherent, but they certainly.",
            "Don't by any means account for exactly which verbs can appear with which sub cats.",
            "I mean, you can get descriptive grammars that list hundreds of their classes in English, so we come up with much more fine grained classes seem to be linguistically justified.",
            "And so it certainly would have overgeneralized quite a lot, because it hasn't formed.",
            "Will predict a lot of verbs can appear sub categorizations that they actually probably not grammatical in.",
            "Well, there's two.",
            "There's two major limitations.",
            "Of course, the sub cuts of new internal structure.",
            "It couldn't, for example, see the adverb S is more similar to S than it is to non phrase.",
            "Anne, and also the Penn Treebank labels, it's a fairly crude data to give it because we've lost so much information in just giving it noun phrase or sentence.",
            "Often the individual structure or the individual lexical items are probably very important for learning it, so it did have only very limited information.",
            "So obviously it be nice if you could just have the raw text, but then we've got the problem of how to learn all the structure and what the constituents are and we've got to learn the classes for the items.",
            "So we basically go back to the whole problem, just learning the whole syntax in one go.",
            "So I needed to simplify it in some way.",
            "OK, but it's worth noting, well, linguists can't actually explain which verbs appear, which stop cats either.",
            "I mean, if you get if you get really detailed descriptive grammar and it's easy to lose sight of the fact that linguists actually don't have a clue about the grammar of any language.",
            "So hopefully if any linguistic but all you know, I think.",
            "We can look at the biggest descriptive grammars easy and easy, like the biggest grammar, English comprehensive grammar for English language.",
            "I think it's the biggest French language in the world is quite easy to find facts about language.",
            "They are just not in there.",
            "So we still don't really know how to write grammar one language.",
            "So we shouldn't.",
            "We shouldn't be surprised that."
        ],
        [
            "This didn't do either.",
            "OK, so just to generally sum up what my main points are, I want to say that MDL and really only MDL.",
            "I don't see another way that you can learn in this general way.",
            "I mean, maybe this is maybe people have a universal grammar and you do it in some special way or they just have some.",
            "I mean people could have just some really weird quirky learning mechanism.",
            "Doesn't matter, it doesn't mean universal grammar assumes the data is not making a big contribution.",
            "Small contribution of the data, but even as a big contribution of the data, the learning mechanism just process in some weird way because everyone does it the same.",
            "It doesn't really matter.",
            "It just means that you come up speaking the same stuff.",
            "Normal is going to notice.",
            "The If it's some sort of general mechanism, I think it has to do something that incorporates SMDL principle, at least aren't claiming an so well.",
            "The new thing I've shown here really is that MDL, well, you can use the same metric on small small corporate on big corporate salons reasonably well, but obviously we don't get high position.",
            "We only get some general thing that captures some aspects of the structure, but only some tiny proportion of the structure in the corpus.",
            "But really, I think you know, because you know this is something that comes out of comments I've had in my work that because what I'm doing looks like statistical NLP.",
            "People say, well, you should be using real corporate and big corporate and they don't notice that.",
            "Well, theoretical syntacticians never do that.",
            "They never tested the corpus.",
            "They never do this rigorous quantitative valuation.",
            "So if we're 20, making contributions to theoretical syntax, we're never going to.",
            "We're never going to be able to both.",
            "I mean, it's completely.",
            "There's no way that in there.",
            "Image feature we're going to learn grammar from Corpus and get high position.",
            "We've got to do one or the other, and I think both of them are quite valid ways to approach the issue by their both addressing.",
            "Different problems are different different data.",
            "OK, that's all, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so there's two main ideas.",
                    "label": 0
                },
                {
                    "sent": "Something we talked about in this talk.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The first one is I'm going to tell you why.",
                    "label": 0
                },
                {
                    "sent": "Minimal description that is such a good idea from the point of view of syntax, which quite a few other people have been doing.",
                    "label": 0
                },
                {
                    "sent": "And certainly when I say minimal description that includes all kinds of related things, such as Bayesian inference like Amy talked about this money.",
                    "label": 0
                },
                {
                    "sent": "An OK but the other thing is I want to just give a bit of an overview about the relationship between theoretical syntax and testicle approaches to machine learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Right, OK, so I think well, various people have made this kind of point.",
                    "label": 0
                },
                {
                    "sent": "Well, what is syntactic theory about?",
                    "label": 1
                },
                {
                    "sent": "So I think the key things that theoretical syntacticians worry about is one of these two things.",
                    "label": 0
                },
                {
                    "sent": "Either they're just trying to write a grammar that explains what sentences are grammatical and water, not grammatical in a language, or they're trying to get some explanation of how to transform observed sentences into a grammar.",
                    "label": 1
                },
                {
                    "sent": "So either that's just some discovery procedure or it's a psychological model of learning, and but whichever of them I think MDL is certainly relevant to explain.",
                    "label": 0
                },
                {
                    "sent": "Either of those, but better perspective you take on some.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tactic theory.",
                    "label": 0
                },
                {
                    "sent": "OK well.",
                    "label": 0
                },
                {
                    "sent": "There's kind of an issue about how we should study syntax because so the majority of people who study syntax are syntacticians and the way they do it is very, very different to the way that computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "Linguists study syntax, I think, so I don't know.",
                    "label": 1
                },
                {
                    "sent": "I mean, if there are theoretical syntacticians here, I don't know if they're going to agree with what my characterization of what they're doing.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying is essentially they decide on some subset of language that they think is coherent and interesting.",
                    "label": 0
                },
                {
                    "sent": "They might look at WH question formation, phrase structure, different different subcommunities that syntacticians tend to focus on, often quite complementary aspects of language, and that's you know, some language linguists talk about core grammar.",
                    "label": 0
                },
                {
                    "sent": "But if you talk to, say, descriptive community in Australia, their idea of what's important is very different than ever.",
                    "label": 0
                },
                {
                    "sent": "Talk about some Jason C or things like that.",
                    "label": 0
                },
                {
                    "sent": "Which is very, very important if you're working kind of jump skis frameworks.",
                    "label": 1
                },
                {
                    "sent": "And then, well, what do we do with these sentences?",
                    "label": 0
                },
                {
                    "sent": "Well, they get an informant, and the informant will tell them which sentence is grammatical and which aren't.",
                    "label": 0
                },
                {
                    "sent": "So they take some sentences and some sensors I think could be grammatical, and ask them.",
                    "label": 1
                },
                {
                    "sent": "And OK, so in the majority of cases.",
                    "label": 0
                },
                {
                    "sent": "The informant is actually the linguists themselves, or in a large number of cases, but it doesn't have to be.",
                    "label": 0
                },
                {
                    "sent": "They can go and ask someone else, so they can do.",
                    "label": 0
                },
                {
                    "sent": "They can just get one for men to do some more psychological type study where your stores people doesn't matter, and then going to try and write themselves a grammar based on that data.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, well, computational linguists approach, and so I'm really talking supervised learning.",
                    "label": 1
                },
                {
                    "sent": "That's the most relevant stuff is quite different, so they've taken a corpus.",
                    "label": 0
                },
                {
                    "sent": "And of course, that Corpus has been generated by informants in a way we can call informance because it's their language that was really studying, just somewhat indirectly.",
                    "label": 0
                },
                {
                    "sent": "OK, something went wrong with my font there, I think.",
                    "label": 0
                },
                {
                    "sent": "The M yeah, this corpus is then fed into a computer and we lost the image and then the computer tries to make either it's going to extract some information about the language like verbs of categories or word classes as a lexicon, something like that, or it's going to try to learn some sort of grammar and the grammar doesn't.",
                    "label": 1
                },
                {
                    "sent": "The grammar is not usually like a full tree structure.",
                    "label": 0
                },
                {
                    "sent": "It could be a part of speech tagger that's some sort of grammar that explains some subset of language.",
                    "label": 0
                },
                {
                    "sent": "But importantly, the real difference I think, is the kind of data using in this.",
                    "label": 0
                },
                {
                    "sent": "This is run on, usually on naturally occurring texts, so it's much, much wider range of sentences.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Syntacticians look at so well.",
                    "label": 0
                },
                {
                    "sent": "Which of these approaches gives us the most insight into languages?",
                    "label": 0
                },
                {
                    "sent": "Well, The thing is, the linguists.",
                    "label": 0
                },
                {
                    "sent": "They really want to get high position.",
                    "label": 0
                },
                {
                    "sent": "You want to explain exactly which sentence is.",
                    "label": 0
                },
                {
                    "sent": "It could sentences and which aren't, and even even if there, even if the life of fuzziness in the grammar, they still want to explain which senses are fuzzy and which are not.",
                    "label": 0
                },
                {
                    "sent": "We want to classify every sentence that is in the sentences taken thinking about.",
                    "label": 0
                },
                {
                    "sent": "But of course the only achieve very limited.",
                    "label": 1
                },
                {
                    "sent": "And I would argue, very arbitrary coverage.",
                    "label": 1
                },
                {
                    "sent": "You know, given the huge number of possible sentences in a language, they only explain some of them.",
                    "label": 0
                },
                {
                    "sent": "So conversation linguists, of course they get much, much better coverage in that you know their grammars relate to the whole corpus is very well defined.",
                    "label": 1
                },
                {
                    "sent": "What data they're explaining, but they never account for the whole corpus completely.",
                    "label": 0
                },
                {
                    "sent": "They only learned grammar.",
                    "label": 0
                },
                {
                    "sent": "That's probabilistically correct, or capture, and usually captures some very limited aspects of the grammar as well.",
                    "label": 0
                },
                {
                    "sent": "So essentially they're taking the train is not very different problems.",
                    "label": 0
                },
                {
                    "sent": "And I think they're both explaining things about language that the other approach doesn't, but that basically complimentary.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Complementary OK, so why?",
                    "label": 0
                },
                {
                    "sent": "Why are we actually doing syntax?",
                    "label": 0
                },
                {
                    "sent": "Why do we care?",
                    "label": 0
                },
                {
                    "sent": "OK, well, I mean I'm coming in this talk more from the theoretical syntax perspective.",
                    "label": 0
                },
                {
                    "sent": "So what I'm suggesting is, well syntacticians have different ideas on what you know, why they care, or why syntax is interesting.",
                    "label": 0
                },
                {
                    "sent": "But one of the things is certainly within the Chomsky perspective.",
                    "label": 1
                },
                {
                    "sent": "What they're really interested in is the human mind in learning in generalization.",
                    "label": 1
                },
                {
                    "sent": "In this kind of process.",
                    "label": 0
                },
                {
                    "sent": "OK, so one of the biggest problems is to explain how can how can a child cope with this massive boots brush staffing problem?",
                    "label": 0
                },
                {
                    "sent": "I mean, one of the really, really distinctive features of natural human languages and the mix and distinct from, say animal communication systems or just other things we learn is absolutely massive and they really Contacts.",
                    "label": 0
                },
                {
                    "sent": "They've got 10s of thousands of words and it's we have a massive sparse data problem, so the Lingus account certainly doesn't account for that kind of problem, which is for me.",
                    "label": 0
                },
                {
                    "sent": "It's a very interesting psychological problem.",
                    "label": 0
                },
                {
                    "sent": "But theoretical syntax has almost nothing to say about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so this suggests that, well, we should.",
                    "label": 0
                },
                {
                    "sent": "We should actually be if we want to do syntax, and we really should be looking at the corporate naturally occurring data and trying to explain how to learn a gram, or at least to explain the sentences in those corporate.",
                    "label": 0
                },
                {
                    "sent": "But that's not really going to shed any light on the kind of sentences that syntacticians focus on.",
                    "label": 1
                },
                {
                    "sent": "You can't get very fast settings adjacency by looking at the corpus, 'cause most of the interesting sentences you're interested.",
                    "label": 0
                },
                {
                    "sent": "I just so incredibly rare that you can only get them by thinking of them and.",
                    "label": 0
                },
                {
                    "sent": "Asking informants and just selecting sentences that form some coherence subset.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "OK, so I mean I think even with that perspective it's not clear which way we should go.",
                    "label": 0
                },
                {
                    "sent": "So just just to think about some of the things that Syntacticians are concerned with.",
                    "label": 0
                },
                {
                    "sent": "That probably doesn't make it into consider.",
                    "label": 0
                },
                {
                    "sent": "You wouldn't pick up by doing a corpus study or learning from a corpus.",
                    "label": 0
                },
                {
                    "sent": "So of course one of the key issues in syntax is negative evidence.",
                    "label": 0
                },
                {
                    "sent": "Think of these sentences, John Hurt himself, Mary hit John, and then look at another verb.",
                    "label": 0
                },
                {
                    "sent": "John hated himself, Mary hated John, so.",
                    "label": 1
                },
                {
                    "sent": "You've got a little language you've got to generalize from the sentences you've heard.",
                    "label": 0
                },
                {
                    "sent": "And make some languages very productive.",
                    "label": 0
                },
                {
                    "sent": "Most sentences anyone says novel sentences have never been said by anyone before.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If children hear things like this that will constantly generalizing, so if they hear John behaved himself, presumably they're going to think Mary behave.",
                    "label": 0
                },
                {
                    "sent": "John, now, of course, I think anyone any native speaker of English is going to agree that.",
                    "label": 0
                },
                {
                    "sent": "I mean, it makes perfect sense.",
                    "label": 0
                },
                {
                    "sent": "I can understand exactly what that would mean if it was English, but it's not English.",
                    "label": 0
                },
                {
                    "sent": "You can't say that, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of thing why do we have these gaps in language?",
                    "label": 0
                },
                {
                    "sent": "What's somehow you've got to infer that some things you just can't.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say OK, so how do we do this?",
                    "label": 0
                },
                {
                    "sent": "Well, of course if you don't hear something you can assume you can't see it.",
                    "label": 0
                },
                {
                    "sent": "OK, but that's not any good because most sentences we've never heard before.",
                    "label": 0
                },
                {
                    "sent": "Most possible English sentences have never been uttered and never will be at it, because this language is so productive the same for most phrases.",
                    "label": 0
                },
                {
                    "sent": "Even verb argument constructions were constantly using verbs with new arguments or even making novel verbs.",
                    "label": 0
                },
                {
                    "sent": "If you give you give people novel verbs, wanted verbs are used in all the different conjugations, there may be a little bit conservative, especially children, but they.",
                    "label": 0
                },
                {
                    "sent": "They will, if necessary, using past tense, present tense, singular, plural, and so forth.",
                    "label": 0
                },
                {
                    "sent": "So if we can use negative evidence, non occurrences, negative evidence we've got to answer these two questions.",
                    "label": 0
                },
                {
                    "sent": "How often does something have to not occur before we decide it's not grammatical and at what level do we make structural generalization?",
                    "label": 1
                },
                {
                    "sent": "This is a sentence of phrase, you know what are the relevant units.",
                    "label": 0
                },
                {
                    "sent": "Essentially what I'm arguing is that this is what MDL gives us.",
                    "label": 0
                },
                {
                    "sent": "Other methods don't, but I don't think unless using MDL type metric, I don't think you can get.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coming out in any general way.",
                    "label": 0
                },
                {
                    "sent": "OK, right, so lots of people said Wendell does OK. We prefer simple grammars and grammar.",
                    "label": 1
                },
                {
                    "sent": "Laugh is simple descriptions of the data.",
                    "label": 1
                },
                {
                    "sent": "If we can capture this structure in the data that's in the grammar that's in the data craft and we don't have to put it in when we describe the data so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To have a more compact description.",
                    "label": 0
                },
                {
                    "sent": "OK so I know people talked about this, but I'll just give another kind of analogy to help really understand the consequences of MDL so we can imagine that this square represents a space of possible sentences.",
                    "label": 1
                },
                {
                    "sent": "And then we've observed these particular sentences.",
                    "label": 0
                },
                {
                    "sent": "So what's our grammar learning task?",
                    "label": 0
                },
                {
                    "sent": "A grammar learning task is to say which parts of the square correspond to grammatical sentences and which parts don't.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the simplest kind of grammar we could have it.",
                    "label": 0
                },
                {
                    "sent": "This is really simple.",
                    "label": 0
                },
                {
                    "sent": "So MDL gives that grammar greater valuation except that it really doesn't explain the data very well.",
                    "label": 0
                },
                {
                    "sent": "So there's lots of information need to say whereabouts in this big big space.",
                    "label": 0
                },
                {
                    "sent": "The sentences are so overall it's not going.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very good evaluation.",
                    "label": 0
                },
                {
                    "sent": "OK, the next option is, well, the other extreme.",
                    "label": 0
                },
                {
                    "sent": "We can just make a grammar describes the data really well.",
                    "label": 0
                },
                {
                    "sent": "'cause it says, right?",
                    "label": 0
                },
                {
                    "sent": "The only data allowed is a data observed.",
                    "label": 0
                },
                {
                    "sent": "No generalization beyond observed data there.",
                    "label": 0
                },
                {
                    "sent": "So the data that we're going to have excellent evaluation in the data part of evaluation metric.",
                    "label": 0
                },
                {
                    "sent": "But of course all we've done is we've taken all the complexity from the data input in the grammar, so we haven't achieved anything we've.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, we've got a really complex grammar, so obviously I think quite the solution over the three solutions.",
                    "label": 0
                },
                {
                    "sent": "This is the one that characterizes the data best.",
                    "label": 1
                },
                {
                    "sent": "It's somewhere between those two extremes, so this is what NDL is telling us.",
                    "label": 0
                },
                {
                    "sent": "It's just giving us a measure of formal measure that says how good a grammar is to fit data, and it tells us what's up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generalization and what's not.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's fair enough to know that if you look at machine learning, there's lots of techniques out there and actually MDL isn't used very much in the computational linguistics community.",
                    "label": 1
                },
                {
                    "sent": "Sophie velocity well where the fennville such a good technique.",
                    "label": 1
                },
                {
                    "sent": "Why aren't there all these great results?",
                    "label": 1
                },
                {
                    "sent": "There's not an amazing number of brilliant results in the, say, computation, linguistics, community using NDL for learning from big corpora so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why do I think MD is a good idea?",
                    "label": 0
                },
                {
                    "sent": "OK Well 2 two other techniques that people suggest I use instead.",
                    "label": 0
                },
                {
                    "sent": "Its maximum likelihood and we can consider maximum likelihood to be MDL.",
                    "label": 1
                },
                {
                    "sent": "Will we just haven't even prior on all dramas so it's not it's Bayesian inference and even prior or grammars.",
                    "label": 0
                },
                {
                    "sent": "But of course then the best grammar is just the one that reproduces all the sentences that we observed in the 1st place.",
                    "label": 1
                },
                {
                    "sent": "So if you want if you want maximum likelihood to make generalizations, there must be some limit on how well the parameters you're fitting can explain the data and it's not limit on how well.",
                    "label": 1
                },
                {
                    "sent": "We can fit parameters that decides the degree of generalization.",
                    "label": 0
                },
                {
                    "sent": "So really, what level or how much?",
                    "label": 0
                },
                {
                    "sent": "It generalizes not decided by maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "It's set externally.",
                    "label": 0
                },
                {
                    "sent": "And of course, in any case you can set it up for any any particular problem to make appropriate generalizations, but it's not going to be general when you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Flight to different data set.",
                    "label": 0
                },
                {
                    "sent": "An interesting maximum entropy is in some ways it's kind of the opposite, because instead of making as restrictive grammars possible, you make grammar as unrestricted as possible.",
                    "label": 1
                },
                {
                    "sent": "So in this case, to stop it being overly general and just saying any any string of words is a valid sentence, you've got to put some constraints and you've got to fix certain parameters that this.",
                    "label": 1
                },
                {
                    "sent": "This maximum entropy has to match certain properties of the data, so again.",
                    "label": 0
                },
                {
                    "sent": "You also really fixing the degree of generality externally, so neither of those approaches is really going to.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solve the problem of generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, so getting onto the model so this.",
                    "label": 0
                },
                {
                    "sent": "I've got basically the valuation measure that I've been using for some time, and I originally used it to learn very simple grammars describing an.",
                    "label": 0
                },
                {
                    "sent": "Just very very very small compara simple data, so the grammar formalism is just binary branching or non branching context free phrase structure rules.",
                    "label": 1
                },
                {
                    "sent": "And every writer grammar we can calculate the description of the grammar just by how many symbols are in it in the coding length of all those symbols.",
                    "label": 0
                },
                {
                    "sent": "And then because the grammar realized certain data is going to reduce the cutting length of the data and we can cut the data just by saying which rules were used to produce it.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Right, so this just summarizes the actual encoding in the model has got full stages.",
                    "label": 0
                },
                {
                    "sent": "First of all we need to say how often each symbol occurs in the grammar.",
                    "label": 0
                },
                {
                    "sent": "But because the number of symbols is always fixed, even though not all the grammars always uses symbols.",
                    "label": 0
                },
                {
                    "sent": "So I always just say there's more symbols than you need.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "You could give it a million symbols that we don't use a handful of them.",
                    "label": 0
                },
                {
                    "sent": "So, but that's going to fix it grammar.",
                    "label": 0
                },
                {
                    "sent": "We just ignore that I then the cutting length is grammar symbol by symbol, the frequency of each rule.",
                    "label": 0
                },
                {
                    "sent": "So it just depends on the number of rules and then.",
                    "label": 0
                },
                {
                    "sent": "The data which is coded as a list of which grammar rules we choose, so the grammar evaluation will be the grammar in the rule frequencies together and then the data part will be the data evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is kind of.",
                    "label": 0
                },
                {
                    "sent": "Some of the original data is tested.",
                    "label": 0
                },
                {
                    "sent": "This on that is the entire corpus that it learned from, and it can produce this kind of grammar.",
                    "label": 0
                },
                {
                    "sent": "Of course, it doesn't use those labels because it doesn't know apriori Watson noun phrase or verb phrases, it just uses different average symbol for each one of them.",
                    "label": 0
                },
                {
                    "sent": "So I think it's quite well established at MDL can learn.",
                    "label": 0
                },
                {
                    "sent": "Very effectively, but with very small simple datasets.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's the kind of thing I think Stolcke originally showed that Bayesian model merging, which is essentially the same general idea.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jail.",
                    "label": 0
                },
                {
                    "sent": "OK, so of course sceptical people said to me, yeah, that's great, but it doesn't look anything like real language.",
                    "label": 0
                },
                {
                    "sent": "What happens if you apply MDL to natural language corpora well?",
                    "label": 1
                },
                {
                    "sent": "Of course I could take my evaluation metric and apply it, but there's such a big search space of grammars it would just get nowhere.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you could run it in theory for an infant length of time it would do fine.",
                    "label": 0
                },
                {
                    "sent": "I should learn your grammar or offended, It's a good concept program as you can get.",
                    "label": 0
                },
                {
                    "sent": "But it's just, you know, that's never going to happen.",
                    "label": 0
                },
                {
                    "sent": "If we could do that, we basically have finished syntax 'cause we know how to construct a grammar.",
                    "label": 1
                },
                {
                    "sent": "So I've gotta find some way to simplify the task and So what I did is I decided that I'm only going to learn verbs.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Categorization classes OK. And how did this?",
                    "label": 0
                },
                {
                    "sent": "Well, I need to workout what the sub categorizations of verbs were.",
                    "label": 0
                },
                {
                    "sent": "So I took the switchboard part as the Penn Treebank and I found all the past tense verbs.",
                    "label": 0
                },
                {
                    "sent": "And a took.",
                    "label": 0
                },
                {
                    "sent": "The verb and the top level constituents within the noun phrase and.",
                    "label": 0
                },
                {
                    "sent": "I made a subcategorization frame for each occurrence of each verb, and I used to start to represent the position of the verb.",
                    "label": 0
                },
                {
                    "sent": "So spend here occurs with two arguments.",
                    "label": 0
                },
                {
                    "sent": "Anything within any top level, constituents within verb phrase for considered to be arguments.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe someone reality agents, but that's I just put them all into parts of categorization so spent can take a noun phrase in the sentence.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I got.",
                    "label": 0
                },
                {
                    "sent": "In the whole of the switchboard corpus or the switch will part the pantry banker got twenty 1759 training instances.",
                    "label": 0
                },
                {
                    "sent": "Then there were 704 different verbs and 706 distinct subcategorization frames, and there were 25 different kinds of top level constituent alongside the verb.",
                    "label": 1
                },
                {
                    "sent": "So things like Sbar, NB adverbial phrase, just, you know those labels will decide by whoever decided the coding scheme for the Penn Treebank.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, Ann.",
                    "label": 0
                },
                {
                    "sent": "Right, so how to?",
                    "label": 0
                },
                {
                    "sent": "I wanted to use the points I wanted to show that this original metric that learns small grammars works on big grammars, so I wanted to use exactly the same evaluation metric as it used before.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I did is you can write rules like this which just say, well let's say two things they say which sub categorizations.",
                    "label": 0
                },
                {
                    "sent": "An occur with verbs of each class and then which verbs appear in which class.",
                    "label": 0
                },
                {
                    "sent": "So this little if this was, you know, if there any three verbs this grammar would say well grew and ended up can appear without the subnet one also cut two and did only a subset two, so it doesn't actually have any access to the individual structures subcategorization.",
                    "label": 1
                },
                {
                    "sent": "So just each map to a number.",
                    "label": 0
                },
                {
                    "sent": "I've lost information.",
                    "label": 0
                },
                {
                    "sent": "It's got very restricted amount of information form of classes but.",
                    "label": 0
                },
                {
                    "sent": "We would expect it to get an improved evaluation or the best grammar if some categorizations with simply similar versus similar categorizations were grouped.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now is the.",
                    "label": 0
                },
                {
                    "sent": "That's great for evaluating these different grammars, where each grammar is just an assignment of verbs to classes.",
                    "label": 0
                },
                {
                    "sent": "But if I want to actually learn, I've gotta have a search mechanism to generate candidate grammars that can then be evaluated by the MDM metric, so the NDL is going to be what decides which grammar keep on, which is the best grammar.",
                    "label": 1
                },
                {
                    "sent": "But you need to search mechanism, just search the space of possible grammars and hopefully it will find the one with the lowest evaluation.",
                    "label": 1
                },
                {
                    "sent": "It is I just started with the grammar in which all the verbs were in one class.",
                    "label": 0
                },
                {
                    "sent": "And then it just.",
                    "label": 1
                },
                {
                    "sent": "Every iteration took of urban moved at random to new class or to different class.",
                    "label": 0
                },
                {
                    "sent": "So the first movie would have to be new class 'cause there's only one class and then it just if the source class was empty, was deleted and then always any redundant rules were removed.",
                    "label": 1
                },
                {
                    "sent": "So there had to be a rule for each verb specifying each verb plus specifying all this other categorizations.",
                    "label": 0
                },
                {
                    "sent": "Which verbs in that class were seen in the day?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To.",
                    "label": 0
                },
                {
                    "sent": "OK, so I just didn't annealing search.",
                    "label": 1
                },
                {
                    "sent": "It just means that you prefer you prefer moves to improve the evaluation loaders description length.",
                    "label": 0
                },
                {
                    "sent": "But in order to overcome local Maxima, specially the beginning of the search, even if it makes a bit worse, you might accept it and then at the end of the search you only accept changes to make it worse, the improved evaluation OK and then after that got stuck and didn't find any changes for awhile.",
                    "label": 0
                },
                {
                    "sent": "It went into emerging phase to see if there were just any redundant rules for classes and merge them altogether.",
                    "label": 0
                },
                {
                    "sent": "And then after no changes were accepted for 2000 iterations, it switched back to the moving phase and it just kept alternating those to try and make sure that it found the best grammar an because there's a potential for this kind of search to get stuck in local Maxima.",
                    "label": 1
                },
                {
                    "sent": "And it certainly didn't.",
                    "label": 1
                },
                {
                    "sent": "Some cases I run that multiple times and just took the grammar with the lowest evaluation.",
                    "label": 0
                },
                {
                    "sent": "And, well, we can't be sure that we actually got the lowest single grammar, but.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's the last one I found.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's the evaluation so.",
                    "label": 0
                },
                {
                    "sent": "The original one verb plus grammar.",
                    "label": 1
                },
                {
                    "sent": "The other extreme grammar.",
                    "label": 0
                },
                {
                    "sent": "We've just put everything in the separate class that made in the generalizations and then the best learn grammar which has six classes.",
                    "label": 0
                },
                {
                    "sent": "So unsurprisingly, the grammar evaluation is best of one class because it's the simplest possible grammar you can have, but it just doesn't characterize the data very well, so the data is a very big description length.",
                    "label": 0
                },
                {
                    "sent": "If you put it in a separate class, well, we've got a really complicated describes the data the best.",
                    "label": 1
                },
                {
                    "sent": "It's bound to do the best because it's not made any generalizations, but the grammar.",
                    "label": 0
                },
                {
                    "sent": "Relation is very bad because the grammar is so complicated.",
                    "label": 0
                },
                {
                    "sent": "So when you generalize and simplify the grammar, you're making some generalizations.",
                    "label": 0
                },
                {
                    "sent": "You don't fit the observed data quite so well, the data valuations higher than each having separate class, but because the grammar evaluation is not so much, much better than even the separate class, we get a much better.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overall valuation OK, but we won't.",
                    "label": 0
                },
                {
                    "sent": "We won't know.",
                    "label": 0
                },
                {
                    "sent": "It's really learned anything linguistically interesting until you go and look at the data that we've learned, so I just looked at the web classes and said, well, are these linguistically coherent and these are the six verb classes that came out an.",
                    "label": 0
                },
                {
                    "sent": "So OK, yeah, OK.",
                    "label": 0
                },
                {
                    "sent": "So the first class seems to contain verbs that take an S or an sbar compliment.",
                    "label": 1
                },
                {
                    "sent": "So you can say John thought that John value that John knew that, so that's certainly seems to be linguistically justified.",
                    "label": 0
                },
                {
                    "sent": "There's something syntactically in common in these verbs and 2nd class.",
                    "label": 0
                },
                {
                    "sent": "Essentially, the transitive verbs typically taken NP argument, and sometimes something else as well.",
                    "label": 0
                },
                {
                    "sent": "3rd Class well did did has a distribution.",
                    "label": 0
                },
                {
                    "sent": "It's really distinct from any other verb, so it seems quite reasonable to make a special class just to account for this one verb.",
                    "label": 0
                },
                {
                    "sent": "And then, well, classful.",
                    "label": 0
                },
                {
                    "sent": "I couldn't really see what that costs about adjusting to miscellaneous.",
                    "label": 0
                },
                {
                    "sent": "It contained by far the vast majority of this hundreds of verbs.",
                    "label": 0
                },
                {
                    "sent": "So in that case, well, no, it just said constant thing about these verbs.",
                    "label": 0
                },
                {
                    "sent": "Just don't wanna miscellaneous class.",
                    "label": 0
                },
                {
                    "sent": "We had this very small class that was verbs.",
                    "label": 0
                },
                {
                    "sent": "It took an S but not an sbar and this is because in the Penn treebank, things like Jaune named the ship Mary or something.",
                    "label": 0
                },
                {
                    "sent": "And that accounts for quite a lot of these.",
                    "label": 0
                },
                {
                    "sent": "These verbs and the use of them.",
                    "label": 0
                },
                {
                    "sent": "And then finally, this verbs that take a particle so it picked up on that fact.",
                    "label": 0
                },
                {
                    "sent": "You only see when you see wound.",
                    "label": 0
                },
                {
                    "sent": "You tend to see wound up.",
                    "label": 0
                },
                {
                    "sent": "You tend to see grew up, ended up, closed up, backed up.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why they've been put in a separate class, OK?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, well, I think we can say that the learn verb classes linguistically coherent, but they certainly.",
                    "label": 1
                },
                {
                    "sent": "Don't by any means account for exactly which verbs can appear with which sub cats.",
                    "label": 1
                },
                {
                    "sent": "I mean, you can get descriptive grammars that list hundreds of their classes in English, so we come up with much more fine grained classes seem to be linguistically justified.",
                    "label": 0
                },
                {
                    "sent": "And so it certainly would have overgeneralized quite a lot, because it hasn't formed.",
                    "label": 0
                },
                {
                    "sent": "Will predict a lot of verbs can appear sub categorizations that they actually probably not grammatical in.",
                    "label": 0
                },
                {
                    "sent": "Well, there's two.",
                    "label": 0
                },
                {
                    "sent": "There's two major limitations.",
                    "label": 0
                },
                {
                    "sent": "Of course, the sub cuts of new internal structure.",
                    "label": 0
                },
                {
                    "sent": "It couldn't, for example, see the adverb S is more similar to S than it is to non phrase.",
                    "label": 0
                },
                {
                    "sent": "Anne, and also the Penn Treebank labels, it's a fairly crude data to give it because we've lost so much information in just giving it noun phrase or sentence.",
                    "label": 0
                },
                {
                    "sent": "Often the individual structure or the individual lexical items are probably very important for learning it, so it did have only very limited information.",
                    "label": 0
                },
                {
                    "sent": "So obviously it be nice if you could just have the raw text, but then we've got the problem of how to learn all the structure and what the constituents are and we've got to learn the classes for the items.",
                    "label": 0
                },
                {
                    "sent": "So we basically go back to the whole problem, just learning the whole syntax in one go.",
                    "label": 1
                },
                {
                    "sent": "So I needed to simplify it in some way.",
                    "label": 0
                },
                {
                    "sent": "OK, but it's worth noting, well, linguists can't actually explain which verbs appear, which stop cats either.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you get if you get really detailed descriptive grammar and it's easy to lose sight of the fact that linguists actually don't have a clue about the grammar of any language.",
                    "label": 0
                },
                {
                    "sent": "So hopefully if any linguistic but all you know, I think.",
                    "label": 0
                },
                {
                    "sent": "We can look at the biggest descriptive grammars easy and easy, like the biggest grammar, English comprehensive grammar for English language.",
                    "label": 0
                },
                {
                    "sent": "I think it's the biggest French language in the world is quite easy to find facts about language.",
                    "label": 0
                },
                {
                    "sent": "They are just not in there.",
                    "label": 0
                },
                {
                    "sent": "So we still don't really know how to write grammar one language.",
                    "label": 0
                },
                {
                    "sent": "So we shouldn't.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't be surprised that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This didn't do either.",
                    "label": 0
                },
                {
                    "sent": "OK, so just to generally sum up what my main points are, I want to say that MDL and really only MDL.",
                    "label": 1
                },
                {
                    "sent": "I don't see another way that you can learn in this general way.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe this is maybe people have a universal grammar and you do it in some special way or they just have some.",
                    "label": 0
                },
                {
                    "sent": "I mean people could have just some really weird quirky learning mechanism.",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter, it doesn't mean universal grammar assumes the data is not making a big contribution.",
                    "label": 0
                },
                {
                    "sent": "Small contribution of the data, but even as a big contribution of the data, the learning mechanism just process in some weird way because everyone does it the same.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "It just means that you come up speaking the same stuff.",
                    "label": 0
                },
                {
                    "sent": "Normal is going to notice.",
                    "label": 0
                },
                {
                    "sent": "The If it's some sort of general mechanism, I think it has to do something that incorporates SMDL principle, at least aren't claiming an so well.",
                    "label": 0
                },
                {
                    "sent": "The new thing I've shown here really is that MDL, well, you can use the same metric on small small corporate on big corporate salons reasonably well, but obviously we don't get high position.",
                    "label": 0
                },
                {
                    "sent": "We only get some general thing that captures some aspects of the structure, but only some tiny proportion of the structure in the corpus.",
                    "label": 0
                },
                {
                    "sent": "But really, I think you know, because you know this is something that comes out of comments I've had in my work that because what I'm doing looks like statistical NLP.",
                    "label": 0
                },
                {
                    "sent": "People say, well, you should be using real corporate and big corporate and they don't notice that.",
                    "label": 0
                },
                {
                    "sent": "Well, theoretical syntacticians never do that.",
                    "label": 0
                },
                {
                    "sent": "They never tested the corpus.",
                    "label": 0
                },
                {
                    "sent": "They never do this rigorous quantitative valuation.",
                    "label": 0
                },
                {
                    "sent": "So if we're 20, making contributions to theoretical syntax, we're never going to.",
                    "label": 0
                },
                {
                    "sent": "We're never going to be able to both.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's completely.",
                    "label": 0
                },
                {
                    "sent": "There's no way that in there.",
                    "label": 0
                },
                {
                    "sent": "Image feature we're going to learn grammar from Corpus and get high position.",
                    "label": 0
                },
                {
                    "sent": "We've got to do one or the other, and I think both of them are quite valid ways to approach the issue by their both addressing.",
                    "label": 0
                },
                {
                    "sent": "Different problems are different different data.",
                    "label": 0
                },
                {
                    "sent": "OK, that's all, thanks.",
                    "label": 0
                }
            ]
        }
    }
}