{
    "id": "vuivd6yxhhowgshpawop5wntqpxvjq57",
    "title": "Online Similarity Prediction of Networked Data from Known and Unknown Graphs",
    "info": {
        "author": [
            "Mark Herbster, Department of Computer Science, University College London"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_herbster_graphs/",
    "segmentation": [
        [
            "OK, I would like to tell you a problem that we've been thinking about a little bit.",
            "This is a online similarity prediction we think about online similarity prediction over graphs in particular."
        ],
        [
            "And the model that we are going to do this in this we're going to do it in online learning model.",
            "Perhaps one of the simplest and oldest models, which is just the mistake bound model you'll all be familiar with this, but let me quickly review it.",
            "So learning proceeds in trials on any given round we received some instance.",
            "So here we receive this instance which is some Simpsons character, the learner's goal at this point is to make a prediction.",
            "I think maybe I want to predict Bart Learner receives.",
            "How come the outcomes part?",
            "I'm fairly happy I didn't make a mistake.",
            "My goal is to minimize the number of mistaken predictions so we continue the next round.",
            "Diets barred again, so I go ahead and I predict Bart Steven outcome.",
            "It's Lisa, I'm fairly unhappy.",
            "I've made a mistake and so forth.",
            "So the goal in the mistake, bounded learning and classification setting.",
            "Here we have K classes is to design good algorithms which minimize mistakes.",
            "OK, let's.",
            "But we're interested in in this similarity.",
            "What do we mean by similarity?"
        ],
        [
            "Prediction, but will compare it to classification now for similarity prediction.",
            "And what we receive is we receive bizarre instance and instance pair.",
            "So here we see a pair of instances in my aim here is to predict are these two instances similar or dissimilar?",
            "So I'll go ahead.",
            "I think they're similar, and it turns out they're similar.",
            "I made no mistakes, so it's pretty much the same setting.",
            "Except now I get another pair and once more my aim is to predict whether similar dissimilar the feedback I get is similar to similar, even though.",
            "We're assuming there's some underlying labeling.",
            "OK, so this is the problem we're interested in.",
            "So."
        ],
        [
            "We can say a few things immediately about this problem.",
            "Classification gives us a nice yard stick for similarity.",
            "If we have a good algorithm for classification, we're going to have a good algorithm for similarity.",
            "Let me just give you the notation behind this theorem.",
            "So Whoops, a concept.",
            "Y is a mapping between instances two classes.",
            "So the bound for any algorithm associated with some concept Y is just the maximal number of mistakes for any sequence consistent with that concept.",
            "OK, so sorry, notation in the theorem is straightforward.",
            "You hand me an algorithm for classification.",
            "I'll hand you an algorithm for similarity.",
            "This algorithm for similarity will make no more than five log K mistakes.",
            "Then that same algorithm would for classification and the other direction holds in fact.",
            "You hand me an algorithm for similarity.",
            "I'll hand you an algorithm for classification and I'll make no more than K mistakes.",
            "Nice property which will exploit to this other direction is if you had me a lower bound for classification.",
            "The other direction also gives me a lower bound for similarity.",
            "OK, so similarity in classification are tightly tide together, and what we know about classification will give us a yard stick to measure our similarity algorithms.",
            "OK."
        ],
        [
            "Would have been nice to been done on the last slide, but unfortunately the construction in the previous theorem is going to require exponential exponential time.",
            "So what we're interested in is efficient algorithms.",
            "So let's see what we can do from an efficiency perspective."
        ],
        [
            "So this is an outline of where we're going next at the cruelest level.",
            "What we want to do is we want to produce algorithms which are going to solve this problem using linear classification techniques and.",
            "Then we're going to also specialize this problem to the graph where on the graph will be able to get algorithms which are both optimal, and algorithms which are both very fast so.",
            "So that will get these linear classification algorithms from a metric learning kernel, and we'll use these classic online algorithms in the matrix.",
            "Setting the matrix perceptron and the matrix window algorithm.",
            "And then we're going to construct a particular approximate graph which will go into its properties in more detail later, and then we combine this approximate graph with the matrix window algorithm will get a algorithm which we can show is optimal in a strong sense, and then we combine it with the matrix.",
            "Perceptron will get an algorithm which is quite fast OK."
        ],
        [
            "OK, So what do we mean by predicting on a graph?",
            "How does this setup work?",
            "So here we have a particular graph.",
            "This graph has nine vertices, so here the each vertices may have a label.",
            "Here we've labeled the graph into three distinct classes.",
            "The similarity prediction case.",
            "A pair is a pair of vertices.",
            "So for instance VMW.",
            "Here is a pair and they happen to be dissimilar because there's different underlying classes.",
            "With any learning method, it's natural to think of well what's the inductive bias associated with it.",
            "So when we have these graph learning methods and natural inductive bias for a graph, problem is the cut.",
            "What do I mean by the cut?",
            "Well, if you look at this red edge here between the green and the orange, the cut is this edge between classes, so that contributes one to the cut.",
            "Then we have these other two edges connecting.",
            "So the cut here is 3 and so somehow grasp with large cut are going to be.",
            "Harder for us to learn, but we want to have a refined notion of cut and this refined notion of cut.",
            "We're going to use is the effective resistance weighted cuts to understand the effective resistance way to cut, let's quickly review effective resistance.",
            "So effective resistance gives us a nice metric on the graph.",
            "So let's look at what the effective resistances between V&W, so the distance.",
            "If we think of this graph now as a resistive network where each of these edges are unit resistors, we can see the resistance between the green and the orange is just unit resistance one.",
            "Between this green and purple vertex we have two edge disjoint paths of length two, and so this resistors in parallel in services of distance one.",
            "We combine these two distances, their distance to.",
            "So this is the notion of effective resistance on a graph.",
            "So now we refine our notion of cut to an effective resistance weighted cut.",
            "So once more each edge instead of this edge will still contribute a unit, because the effective resistance between the green and the orange edges just one.",
            "But if we look at this green and purple edge here we have one edge disjoint.",
            "Flying free one edge disjoint path of length one, combining things together gives us an effective resistance of 3/4 or this edge.",
            "By parallel we have effective resistance of 3/4 for that, so the effective resistance way to cut is 2 1/2 and so this is the refined notion of cut which will be aiming for as our notion of inductive bias in our bounds.",
            "did I just go backwards?",
            "Let's go forward."
        ],
        [
            "OK, so the basic technique we're going to do is do for similarity.",
            "Predictions will use these metric kernel constructions and basically the construction is.",
            "We just take this outer product of the differences between unit vectors.",
            "We bracketed by the square roots of the kernel of the square root of pseudo inverse of the graph Laplacian, which we can think of is just the square root of some kernel matrix.",
            "Then we're going to use this with the Matrix Perceptron in the matrix window algorithms for those of you aren't familiar with the matrix size version of these algorithms.",
            "These are the.",
            "Analog to the vector case where we can think about the matrix perceptron algorithm as kind of being some approximate algorithm.",
            "Two algorithm which is performing a Frobenius Frobenius norm regularization.",
            "An matrix window is an algorithm which is.",
            "Performing some analogue of quantum relative entropy organization.",
            "So once we use this thing, code are instances.",
            "We can basically just with a little bit of manipulation, take the bounce from these algorithms and pull off bounds.",
            "So the first thing we notice looking at the matrix window I want to draw your attention to this term between the brackets.",
            "This is essentially the term that we would have in the classification setting.",
            "The cut times, resistance, diameter where this is kind of a margin term and this is a diameter term matrix.",
            "When we get this term exactly.",
            "With the product of log in, which somehow isn't surprising given our upper bound is we have log K for the number of classes.",
            "So here we're getting log and so log in isn't so.",
            "They had.",
            "Matrix perceptron, it sounds not bad, but unfortunately now this term comes in squared.",
            "So matrix perceptron, a matrix will in general will have a much better bound for this problem.",
            "OK, but we want to do more, but we're not happy."
        ],
        [
            "What we want is we want algorithms which are both optimal and fast.",
            "We're not going to get both at the same time, we'll get one or the other.",
            "So we have this construction where we approximate the original graph with this randomized binary support tree, which with one algorithm will give us often maletis with the other one will give us fast.",
            "So what's the first step?",
            "We have our original graph, we get a new graph.",
            "How do we get this new graph when we consider the set of all spanning trees and we sample one at random uniformly at random?",
            "So when we do that?",
            "The property that has is that means that expectation with in respect to the sampling process of the cut size is now equal to the resistance way to cut size.",
            "This is a classic result.",
            "Then as our first intermediate step, we take this random tree G and we embedded into the path graph.",
            "I won't go into the details of how that works, but the rough idea is that we're going to lose some of the edges gained.",
            "A few new Ed edges, and in terms of what we pay for this embedding process, the cut will increase by no more than a factor of 2.",
            "Then the next step of our algorithm is we're going to then take this path graph, throw away all the edges, use this linear order generated by the path graph, build a binary tree on top of it, and we have all new edges purple.",
            "So we have this new graph, which is the binary support regenerated from this property, and it kind of gives us three properties that comment on 1st one having this very uniform binary tree shape in combination with the matrix perception is going to allow us to produce a very fast algorithm.",
            "A property which is, you know, slightly negative, but not so good is that the cut size is now going to blow up a little bit.",
            "It's going to blow up by a factor of log in, so it's a blow up.",
            "It's not such a bad blow property which we really want for optimality is now is our original spanning tree or tree.",
            "Here is likely a graph with large diameter.",
            "When we do this binary support re, it gives us a graph with this bounded diameter of log N, which is good for us.",
            "So combining the inequality of the cut together we have the following.",
            "The cuts just increased by a factor of.",
            "To log and then it's now in terms of resistance, waiting.",
            "OK, so how do we apply?"
        ],
        [
            "Buy this.",
            "OK, so let's look at the result we get with when we use the matrix window algorithm in combination with the random binary support tree.",
            "So the upper bound.",
            "Now we now get an upper bound in terms of the resistance way to cut size and this term log N cubed.",
            "We can match this on on the lower bound side with affectively just the resistance way to cut size.",
            "So essentially it's optimal up to this log cubed an factor.",
            "Downside is online learning.",
            "We'd like to be fast in practice.",
            "You know, N cubed is polynomial.",
            "We're happy, but one could be much happier if one really wants to run this algorithm on larger datasets.",
            "So for that reason."
        ],
        [
            "So we turn to the perceptron perceptron may not be optimal for us, but it's going to give us something fast.",
            "Let's first look at the expected mistakes.",
            "When we combine the matrix perceptron with the randomized binary support re, here are bound increases by a factor of the resistance weighted cotton.",
            "A factor of log N. So our bound is blowing up a bit.",
            "But on the other hand, the per time per round prediction time is now log squared N. So this is a exponential speedup of the Pro round prediction time, which is a nice thing in practice, OK?"
        ],
        [
            "Ideally.",
            "I would tell you all the details of how we achieve this log squared and per round prediction, but I think what I'm going to do instead of just quickly sketch it so the first step is we received this vertex pair VW, we find the path between these two vertices we maintain a set of variables fij overall pairs of vertices.",
            "If we sum up these variables.",
            "If it's over some threshold, we go ahead and predict dissimilar.",
            "We were then receive a similarity label after we receive this.",
            "Similarity label we go ahead and update first step of the update is to just generate this increasing sequence between V&W.",
            "We determine all the vertices adjacent to this increasing sequence.",
            "We extend the increasing sequence in a natural way to the bolded circles.",
            "Then we just need to update the bolded search or the variables corresponding to the bolded circles.",
            "We update over all pairs of these bolded variables.",
            "There's log in as those things, so our update essentially takes log N squared time.",
            "Fly, it works at.",
            "I'm not going to tell you at this moment, but there is a page."
        ],
        [
            "OK. We also consider another model, this assumption that we have a graph in advance is very strong.",
            "You know, in many natural scenarios we may not have the graph.",
            "We may only learn about this graph incrementally, so we're going to consider another model now where the graph is progressively disclosed to us."
        ],
        [
            "So here in the Progressive disclosure model, nature is going to present the vertex pair in addition to the vertex pair.",
            "We receive a path that connects the vertices, so all we had before was this upper bound, N on the number of vertices.",
            "That's all we know.",
            "But on any given trial.",
            "Will receive a pair in some given path, connecting those vertices.",
            "Then we just continue as before.",
            "We predict the similarity.",
            "Nature reveals the similarity.",
            "And so forth.",
            "I'm not going to go into the details of this algorithm beyond saying that essentially we do.",
            "We do implement a linear classifier.",
            "Again, we maintain a forest of trees.",
            "We add trees to the forest.",
            "We merge trees together in our forest.",
            "And we use that in combination with the P norm Perceptron algorithm to prove a bound which is now quartic in terms of the cotton inloggen otherwise.",
            "And it's really a different representation than the other representation, which is based on a Laplacian.",
            "And so we can't expect a resistance diameter type term here, OK?",
            "Spent through this talk."
        ],
        [
            "OK, let's conclude.",
            "So what if I showed you today?",
            "So first thing we've done is we've established some formal equivalence between classification and similarity in the mistake bounded setting.",
            "You hand me a classification algorithm.",
            "I give you a similarity algorithm, vice versa with similar bounds.",
            "We then went ahead and we looked for efficient and optimal algorithms in a graph labeling setting.",
            "The particular tool that we wanted to use there was this randomized binary support tree graph approximation.",
            "With that graph approximation, this gave us an algorithm which was.",
            "Optimal up to log factors with matrix window.",
            "With the.",
            "The Matrix perceptron algorithm.",
            "It gave us an algorithm which is optimal, not optimal but very fast polylog time predictions for round.",
            "And then we introduce this novel unknown graph setting lots of future directions were considering a few which we're considering are the three.",
            "So first thing is is underlying this assumption that everything the graph there exists some labeling in decay classes.",
            "This is equivalent to saying that similarity is essentially transitive.",
            "We'd like to weaken this assumption.",
            "This is a very strong assumption.",
            "Love the random binary.",
            "I mean personally I love the randomized binary support tree, but there is this sinking feeling that we have this whole graph or it's quite a destructive approximation.",
            "First we we find the spanning tree, then we find the linear graph and then we build a tree on it.",
            "Can we come up with a graph approximation which still allows fast prediction an for which we prove optimality on this is a open problem of particular concern.",
            "Finally this unknown graph setting.",
            "It's novel, we don't know a lot about it, even for inefficient algorithms, we haven't been able to sharpen the quartic bound.",
            "We don't know a lot about lower bounds.",
            "Probably can't prove abound in terms of the resistance diameter for various reasons.",
            "And I thank you.",
            "Oh"
        ],
        [
            "Useful references for work."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I would like to tell you a problem that we've been thinking about a little bit.",
                    "label": 0
                },
                {
                    "sent": "This is a online similarity prediction we think about online similarity prediction over graphs in particular.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the model that we are going to do this in this we're going to do it in online learning model.",
                    "label": 0
                },
                {
                    "sent": "Perhaps one of the simplest and oldest models, which is just the mistake bound model you'll all be familiar with this, but let me quickly review it.",
                    "label": 0
                },
                {
                    "sent": "So learning proceeds in trials on any given round we received some instance.",
                    "label": 0
                },
                {
                    "sent": "So here we receive this instance which is some Simpsons character, the learner's goal at this point is to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "I think maybe I want to predict Bart Learner receives.",
                    "label": 0
                },
                {
                    "sent": "How come the outcomes part?",
                    "label": 0
                },
                {
                    "sent": "I'm fairly happy I didn't make a mistake.",
                    "label": 0
                },
                {
                    "sent": "My goal is to minimize the number of mistaken predictions so we continue the next round.",
                    "label": 0
                },
                {
                    "sent": "Diets barred again, so I go ahead and I predict Bart Steven outcome.",
                    "label": 0
                },
                {
                    "sent": "It's Lisa, I'm fairly unhappy.",
                    "label": 0
                },
                {
                    "sent": "I've made a mistake and so forth.",
                    "label": 0
                },
                {
                    "sent": "So the goal in the mistake, bounded learning and classification setting.",
                    "label": 0
                },
                {
                    "sent": "Here we have K classes is to design good algorithms which minimize mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, let's.",
                    "label": 0
                },
                {
                    "sent": "But we're interested in in this similarity.",
                    "label": 0
                },
                {
                    "sent": "What do we mean by similarity?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Prediction, but will compare it to classification now for similarity prediction.",
                    "label": 0
                },
                {
                    "sent": "And what we receive is we receive bizarre instance and instance pair.",
                    "label": 0
                },
                {
                    "sent": "So here we see a pair of instances in my aim here is to predict are these two instances similar or dissimilar?",
                    "label": 0
                },
                {
                    "sent": "So I'll go ahead.",
                    "label": 0
                },
                {
                    "sent": "I think they're similar, and it turns out they're similar.",
                    "label": 0
                },
                {
                    "sent": "I made no mistakes, so it's pretty much the same setting.",
                    "label": 0
                },
                {
                    "sent": "Except now I get another pair and once more my aim is to predict whether similar dissimilar the feedback I get is similar to similar, even though.",
                    "label": 0
                },
                {
                    "sent": "We're assuming there's some underlying labeling.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the problem we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can say a few things immediately about this problem.",
                    "label": 0
                },
                {
                    "sent": "Classification gives us a nice yard stick for similarity.",
                    "label": 0
                },
                {
                    "sent": "If we have a good algorithm for classification, we're going to have a good algorithm for similarity.",
                    "label": 0
                },
                {
                    "sent": "Let me just give you the notation behind this theorem.",
                    "label": 0
                },
                {
                    "sent": "So Whoops, a concept.",
                    "label": 0
                },
                {
                    "sent": "Y is a mapping between instances two classes.",
                    "label": 1
                },
                {
                    "sent": "So the bound for any algorithm associated with some concept Y is just the maximal number of mistakes for any sequence consistent with that concept.",
                    "label": 1
                },
                {
                    "sent": "OK, so sorry, notation in the theorem is straightforward.",
                    "label": 0
                },
                {
                    "sent": "You hand me an algorithm for classification.",
                    "label": 0
                },
                {
                    "sent": "I'll hand you an algorithm for similarity.",
                    "label": 0
                },
                {
                    "sent": "This algorithm for similarity will make no more than five log K mistakes.",
                    "label": 0
                },
                {
                    "sent": "Then that same algorithm would for classification and the other direction holds in fact.",
                    "label": 0
                },
                {
                    "sent": "You hand me an algorithm for similarity.",
                    "label": 0
                },
                {
                    "sent": "I'll hand you an algorithm for classification and I'll make no more than K mistakes.",
                    "label": 0
                },
                {
                    "sent": "Nice property which will exploit to this other direction is if you had me a lower bound for classification.",
                    "label": 0
                },
                {
                    "sent": "The other direction also gives me a lower bound for similarity.",
                    "label": 0
                },
                {
                    "sent": "OK, so similarity in classification are tightly tide together, and what we know about classification will give us a yard stick to measure our similarity algorithms.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would have been nice to been done on the last slide, but unfortunately the construction in the previous theorem is going to require exponential exponential time.",
                    "label": 0
                },
                {
                    "sent": "So what we're interested in is efficient algorithms.",
                    "label": 0
                },
                {
                    "sent": "So let's see what we can do from an efficiency perspective.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an outline of where we're going next at the cruelest level.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to produce algorithms which are going to solve this problem using linear classification techniques and.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to also specialize this problem to the graph where on the graph will be able to get algorithms which are both optimal, and algorithms which are both very fast so.",
                    "label": 0
                },
                {
                    "sent": "So that will get these linear classification algorithms from a metric learning kernel, and we'll use these classic online algorithms in the matrix.",
                    "label": 0
                },
                {
                    "sent": "Setting the matrix perceptron and the matrix window algorithm.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to construct a particular approximate graph which will go into its properties in more detail later, and then we combine this approximate graph with the matrix window algorithm will get a algorithm which we can show is optimal in a strong sense, and then we combine it with the matrix.",
                    "label": 0
                },
                {
                    "sent": "Perceptron will get an algorithm which is quite fast OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what do we mean by predicting on a graph?",
                    "label": 1
                },
                {
                    "sent": "How does this setup work?",
                    "label": 0
                },
                {
                    "sent": "So here we have a particular graph.",
                    "label": 0
                },
                {
                    "sent": "This graph has nine vertices, so here the each vertices may have a label.",
                    "label": 1
                },
                {
                    "sent": "Here we've labeled the graph into three distinct classes.",
                    "label": 0
                },
                {
                    "sent": "The similarity prediction case.",
                    "label": 1
                },
                {
                    "sent": "A pair is a pair of vertices.",
                    "label": 0
                },
                {
                    "sent": "So for instance VMW.",
                    "label": 0
                },
                {
                    "sent": "Here is a pair and they happen to be dissimilar because there's different underlying classes.",
                    "label": 0
                },
                {
                    "sent": "With any learning method, it's natural to think of well what's the inductive bias associated with it.",
                    "label": 0
                },
                {
                    "sent": "So when we have these graph learning methods and natural inductive bias for a graph, problem is the cut.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by the cut?",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at this red edge here between the green and the orange, the cut is this edge between classes, so that contributes one to the cut.",
                    "label": 0
                },
                {
                    "sent": "Then we have these other two edges connecting.",
                    "label": 0
                },
                {
                    "sent": "So the cut here is 3 and so somehow grasp with large cut are going to be.",
                    "label": 0
                },
                {
                    "sent": "Harder for us to learn, but we want to have a refined notion of cut and this refined notion of cut.",
                    "label": 0
                },
                {
                    "sent": "We're going to use is the effective resistance weighted cuts to understand the effective resistance way to cut, let's quickly review effective resistance.",
                    "label": 0
                },
                {
                    "sent": "So effective resistance gives us a nice metric on the graph.",
                    "label": 0
                },
                {
                    "sent": "So let's look at what the effective resistances between V&W, so the distance.",
                    "label": 0
                },
                {
                    "sent": "If we think of this graph now as a resistive network where each of these edges are unit resistors, we can see the resistance between the green and the orange is just unit resistance one.",
                    "label": 0
                },
                {
                    "sent": "Between this green and purple vertex we have two edge disjoint paths of length two, and so this resistors in parallel in services of distance one.",
                    "label": 0
                },
                {
                    "sent": "We combine these two distances, their distance to.",
                    "label": 0
                },
                {
                    "sent": "So this is the notion of effective resistance on a graph.",
                    "label": 0
                },
                {
                    "sent": "So now we refine our notion of cut to an effective resistance weighted cut.",
                    "label": 0
                },
                {
                    "sent": "So once more each edge instead of this edge will still contribute a unit, because the effective resistance between the green and the orange edges just one.",
                    "label": 0
                },
                {
                    "sent": "But if we look at this green and purple edge here we have one edge disjoint.",
                    "label": 0
                },
                {
                    "sent": "Flying free one edge disjoint path of length one, combining things together gives us an effective resistance of 3/4 or this edge.",
                    "label": 0
                },
                {
                    "sent": "By parallel we have effective resistance of 3/4 for that, so the effective resistance way to cut is 2 1/2 and so this is the refined notion of cut which will be aiming for as our notion of inductive bias in our bounds.",
                    "label": 0
                },
                {
                    "sent": "did I just go backwards?",
                    "label": 0
                },
                {
                    "sent": "Let's go forward.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the basic technique we're going to do is do for similarity.",
                    "label": 0
                },
                {
                    "sent": "Predictions will use these metric kernel constructions and basically the construction is.",
                    "label": 0
                },
                {
                    "sent": "We just take this outer product of the differences between unit vectors.",
                    "label": 0
                },
                {
                    "sent": "We bracketed by the square roots of the kernel of the square root of pseudo inverse of the graph Laplacian, which we can think of is just the square root of some kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to use this with the Matrix Perceptron in the matrix window algorithms for those of you aren't familiar with the matrix size version of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "These are the.",
                    "label": 0
                },
                {
                    "sent": "Analog to the vector case where we can think about the matrix perceptron algorithm as kind of being some approximate algorithm.",
                    "label": 1
                },
                {
                    "sent": "Two algorithm which is performing a Frobenius Frobenius norm regularization.",
                    "label": 0
                },
                {
                    "sent": "An matrix window is an algorithm which is.",
                    "label": 0
                },
                {
                    "sent": "Performing some analogue of quantum relative entropy organization.",
                    "label": 0
                },
                {
                    "sent": "So once we use this thing, code are instances.",
                    "label": 0
                },
                {
                    "sent": "We can basically just with a little bit of manipulation, take the bounce from these algorithms and pull off bounds.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we notice looking at the matrix window I want to draw your attention to this term between the brackets.",
                    "label": 0
                },
                {
                    "sent": "This is essentially the term that we would have in the classification setting.",
                    "label": 1
                },
                {
                    "sent": "The cut times, resistance, diameter where this is kind of a margin term and this is a diameter term matrix.",
                    "label": 1
                },
                {
                    "sent": "When we get this term exactly.",
                    "label": 0
                },
                {
                    "sent": "With the product of log in, which somehow isn't surprising given our upper bound is we have log K for the number of classes.",
                    "label": 0
                },
                {
                    "sent": "So here we're getting log and so log in isn't so.",
                    "label": 0
                },
                {
                    "sent": "They had.",
                    "label": 0
                },
                {
                    "sent": "Matrix perceptron, it sounds not bad, but unfortunately now this term comes in squared.",
                    "label": 0
                },
                {
                    "sent": "So matrix perceptron, a matrix will in general will have a much better bound for this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, but we want to do more, but we're not happy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we want is we want algorithms which are both optimal and fast.",
                    "label": 0
                },
                {
                    "sent": "We're not going to get both at the same time, we'll get one or the other.",
                    "label": 0
                },
                {
                    "sent": "So we have this construction where we approximate the original graph with this randomized binary support tree, which with one algorithm will give us often maletis with the other one will give us fast.",
                    "label": 0
                },
                {
                    "sent": "So what's the first step?",
                    "label": 0
                },
                {
                    "sent": "We have our original graph, we get a new graph.",
                    "label": 0
                },
                {
                    "sent": "How do we get this new graph when we consider the set of all spanning trees and we sample one at random uniformly at random?",
                    "label": 0
                },
                {
                    "sent": "So when we do that?",
                    "label": 0
                },
                {
                    "sent": "The property that has is that means that expectation with in respect to the sampling process of the cut size is now equal to the resistance way to cut size.",
                    "label": 0
                },
                {
                    "sent": "This is a classic result.",
                    "label": 0
                },
                {
                    "sent": "Then as our first intermediate step, we take this random tree G and we embedded into the path graph.",
                    "label": 1
                },
                {
                    "sent": "I won't go into the details of how that works, but the rough idea is that we're going to lose some of the edges gained.",
                    "label": 0
                },
                {
                    "sent": "A few new Ed edges, and in terms of what we pay for this embedding process, the cut will increase by no more than a factor of 2.",
                    "label": 0
                },
                {
                    "sent": "Then the next step of our algorithm is we're going to then take this path graph, throw away all the edges, use this linear order generated by the path graph, build a binary tree on top of it, and we have all new edges purple.",
                    "label": 0
                },
                {
                    "sent": "So we have this new graph, which is the binary support regenerated from this property, and it kind of gives us three properties that comment on 1st one having this very uniform binary tree shape in combination with the matrix perception is going to allow us to produce a very fast algorithm.",
                    "label": 0
                },
                {
                    "sent": "A property which is, you know, slightly negative, but not so good is that the cut size is now going to blow up a little bit.",
                    "label": 0
                },
                {
                    "sent": "It's going to blow up by a factor of log in, so it's a blow up.",
                    "label": 0
                },
                {
                    "sent": "It's not such a bad blow property which we really want for optimality is now is our original spanning tree or tree.",
                    "label": 0
                },
                {
                    "sent": "Here is likely a graph with large diameter.",
                    "label": 0
                },
                {
                    "sent": "When we do this binary support re, it gives us a graph with this bounded diameter of log N, which is good for us.",
                    "label": 0
                },
                {
                    "sent": "So combining the inequality of the cut together we have the following.",
                    "label": 0
                },
                {
                    "sent": "The cuts just increased by a factor of.",
                    "label": 0
                },
                {
                    "sent": "To log and then it's now in terms of resistance, waiting.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we apply?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Buy this.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at the result we get with when we use the matrix window algorithm in combination with the random binary support tree.",
                    "label": 0
                },
                {
                    "sent": "So the upper bound.",
                    "label": 0
                },
                {
                    "sent": "Now we now get an upper bound in terms of the resistance way to cut size and this term log N cubed.",
                    "label": 0
                },
                {
                    "sent": "We can match this on on the lower bound side with affectively just the resistance way to cut size.",
                    "label": 0
                },
                {
                    "sent": "So essentially it's optimal up to this log cubed an factor.",
                    "label": 0
                },
                {
                    "sent": "Downside is online learning.",
                    "label": 0
                },
                {
                    "sent": "We'd like to be fast in practice.",
                    "label": 0
                },
                {
                    "sent": "You know, N cubed is polynomial.",
                    "label": 0
                },
                {
                    "sent": "We're happy, but one could be much happier if one really wants to run this algorithm on larger datasets.",
                    "label": 0
                },
                {
                    "sent": "So for that reason.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we turn to the perceptron perceptron may not be optimal for us, but it's going to give us something fast.",
                    "label": 0
                },
                {
                    "sent": "Let's first look at the expected mistakes.",
                    "label": 0
                },
                {
                    "sent": "When we combine the matrix perceptron with the randomized binary support re, here are bound increases by a factor of the resistance weighted cotton.",
                    "label": 1
                },
                {
                    "sent": "A factor of log N. So our bound is blowing up a bit.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, the per time per round prediction time is now log squared N. So this is a exponential speedup of the Pro round prediction time, which is a nice thing in practice, OK?",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ideally.",
                    "label": 0
                },
                {
                    "sent": "I would tell you all the details of how we achieve this log squared and per round prediction, but I think what I'm going to do instead of just quickly sketch it so the first step is we received this vertex pair VW, we find the path between these two vertices we maintain a set of variables fij overall pairs of vertices.",
                    "label": 0
                },
                {
                    "sent": "If we sum up these variables.",
                    "label": 0
                },
                {
                    "sent": "If it's over some threshold, we go ahead and predict dissimilar.",
                    "label": 0
                },
                {
                    "sent": "We were then receive a similarity label after we receive this.",
                    "label": 0
                },
                {
                    "sent": "Similarity label we go ahead and update first step of the update is to just generate this increasing sequence between V&W.",
                    "label": 0
                },
                {
                    "sent": "We determine all the vertices adjacent to this increasing sequence.",
                    "label": 0
                },
                {
                    "sent": "We extend the increasing sequence in a natural way to the bolded circles.",
                    "label": 0
                },
                {
                    "sent": "Then we just need to update the bolded search or the variables corresponding to the bolded circles.",
                    "label": 0
                },
                {
                    "sent": "We update over all pairs of these bolded variables.",
                    "label": 0
                },
                {
                    "sent": "There's log in as those things, so our update essentially takes log N squared time.",
                    "label": 0
                },
                {
                    "sent": "Fly, it works at.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to tell you at this moment, but there is a page.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. We also consider another model, this assumption that we have a graph in advance is very strong.",
                    "label": 0
                },
                {
                    "sent": "You know, in many natural scenarios we may not have the graph.",
                    "label": 0
                },
                {
                    "sent": "We may only learn about this graph incrementally, so we're going to consider another model now where the graph is progressively disclosed to us.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here in the Progressive disclosure model, nature is going to present the vertex pair in addition to the vertex pair.",
                    "label": 0
                },
                {
                    "sent": "We receive a path that connects the vertices, so all we had before was this upper bound, N on the number of vertices.",
                    "label": 0
                },
                {
                    "sent": "That's all we know.",
                    "label": 0
                },
                {
                    "sent": "But on any given trial.",
                    "label": 0
                },
                {
                    "sent": "Will receive a pair in some given path, connecting those vertices.",
                    "label": 0
                },
                {
                    "sent": "Then we just continue as before.",
                    "label": 0
                },
                {
                    "sent": "We predict the similarity.",
                    "label": 0
                },
                {
                    "sent": "Nature reveals the similarity.",
                    "label": 0
                },
                {
                    "sent": "And so forth.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to go into the details of this algorithm beyond saying that essentially we do.",
                    "label": 0
                },
                {
                    "sent": "We do implement a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "Again, we maintain a forest of trees.",
                    "label": 0
                },
                {
                    "sent": "We add trees to the forest.",
                    "label": 0
                },
                {
                    "sent": "We merge trees together in our forest.",
                    "label": 0
                },
                {
                    "sent": "And we use that in combination with the P norm Perceptron algorithm to prove a bound which is now quartic in terms of the cotton inloggen otherwise.",
                    "label": 0
                },
                {
                    "sent": "And it's really a different representation than the other representation, which is based on a Laplacian.",
                    "label": 0
                },
                {
                    "sent": "And so we can't expect a resistance diameter type term here, OK?",
                    "label": 0
                },
                {
                    "sent": "Spent through this talk.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's conclude.",
                    "label": 0
                },
                {
                    "sent": "So what if I showed you today?",
                    "label": 0
                },
                {
                    "sent": "So first thing we've done is we've established some formal equivalence between classification and similarity in the mistake bounded setting.",
                    "label": 1
                },
                {
                    "sent": "You hand me a classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "I give you a similarity algorithm, vice versa with similar bounds.",
                    "label": 1
                },
                {
                    "sent": "We then went ahead and we looked for efficient and optimal algorithms in a graph labeling setting.",
                    "label": 0
                },
                {
                    "sent": "The particular tool that we wanted to use there was this randomized binary support tree graph approximation.",
                    "label": 1
                },
                {
                    "sent": "With that graph approximation, this gave us an algorithm which was.",
                    "label": 0
                },
                {
                    "sent": "Optimal up to log factors with matrix window.",
                    "label": 0
                },
                {
                    "sent": "With the.",
                    "label": 0
                },
                {
                    "sent": "The Matrix perceptron algorithm.",
                    "label": 0
                },
                {
                    "sent": "It gave us an algorithm which is optimal, not optimal but very fast polylog time predictions for round.",
                    "label": 1
                },
                {
                    "sent": "And then we introduce this novel unknown graph setting lots of future directions were considering a few which we're considering are the three.",
                    "label": 0
                },
                {
                    "sent": "So first thing is is underlying this assumption that everything the graph there exists some labeling in decay classes.",
                    "label": 0
                },
                {
                    "sent": "This is equivalent to saying that similarity is essentially transitive.",
                    "label": 0
                },
                {
                    "sent": "We'd like to weaken this assumption.",
                    "label": 0
                },
                {
                    "sent": "This is a very strong assumption.",
                    "label": 0
                },
                {
                    "sent": "Love the random binary.",
                    "label": 0
                },
                {
                    "sent": "I mean personally I love the randomized binary support tree, but there is this sinking feeling that we have this whole graph or it's quite a destructive approximation.",
                    "label": 0
                },
                {
                    "sent": "First we we find the spanning tree, then we find the linear graph and then we build a tree on it.",
                    "label": 1
                },
                {
                    "sent": "Can we come up with a graph approximation which still allows fast prediction an for which we prove optimality on this is a open problem of particular concern.",
                    "label": 0
                },
                {
                    "sent": "Finally this unknown graph setting.",
                    "label": 0
                },
                {
                    "sent": "It's novel, we don't know a lot about it, even for inefficient algorithms, we haven't been able to sharpen the quartic bound.",
                    "label": 0
                },
                {
                    "sent": "We don't know a lot about lower bounds.",
                    "label": 0
                },
                {
                    "sent": "Probably can't prove abound in terms of the resistance diameter for various reasons.",
                    "label": 0
                },
                {
                    "sent": "And I thank you.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Useful references for work.",
                    "label": 0
                }
            ]
        }
    }
}