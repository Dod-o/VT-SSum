{
    "id": "pf5phqtoy52o3pubv3pffut3yjub6fav",
    "title": "Graph Clustering With Network Structure Indices",
    "info": {
        "author": [
            "Matthew J. Rattigan, University of Massachusetts Amherst"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Mathematics->Graph Theory",
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml07_rattigan_gcns/",
    "segmentation": [
        [
            "Hello, my name is Matt Radigan from the University of Massachusetts.",
            "I'm going to be talking about graph clustering today.",
            "This is some joint work done with Mark Mayer, a grad student in my lab, along with David my advisor."
        ],
        [
            "So start off.",
            "I'm gonna show you a picture of Chris Farley.",
            "Um?",
            "Going to use Chris as an example of what I'm after.",
            "When I say clustering, there's clustering gets used in a lot of different contexts.",
            "The type of clustering I'm looking for is.",
            "Nodes that are using instances of data that are associated in some way in some meaningful way with a given instance.",
            "So in our Chris Farley example, we have a bunch of actors that have Co starred in movies with Chris actors that have Co starred on Saturday Night Live with Chris.",
            "There's a group shot of them there.",
            "This is what I mean by clustering, so this is the kind of thing to keep in your head."
        ],
        [
            "Traditional data clustering.",
            "Operates on IID data.",
            "Here I have it represented in a small little table.",
            "Each row in the table is going to be a given instance.",
            "The attributes of those instances are going to be the columns.",
            "We take our IID data and we can project it into some nice geometric space.",
            "And then we form clusters out of that space such that the distances between nodes in the same cluster minimized while we maximize the distances between nodes in different clusters.",
            "In the relation."
        ],
        [
            "No data case.",
            "Things are a little bit different by relational data.",
            "I mean any data set that can be represented as a graph such that the nodes of the graph are are instances in the links between them represent some sort of relationship or statistical dependency.",
            "Between those instances, it's really going to vary with the domain when we do clustering in a graph context, the task is a little bit different.",
            "Here what we want to do is pull the graph apart such that the clusters of the graph are tightly connected with links.",
            "And the links between clusters or minimize."
        ],
        [
            "There are lot of graph clustering approaches in literature.",
            "I'm going to talk about a couple of them today.",
            "The one thing to remember is that most of the graph clustering algorithms share the same weakness and that is they have problems with complexity.",
            "By problems, I mean they're either quadratic or cubic, or worse in the size of the graph in terms of runtime and his relational datasets get bigger having thousands if not millions of nodes were going to hard time clustering them effectively.",
            "The good news about this is.",
            "Is it very simple?",
            "Graph clustering techniques work really, really well on some of the real datasets we've tested them on.",
            "That in mind, I'm going to talk about two simple techniques today.",
            "One is the Gervin Newman to visit clustering algorithm, the other is a K Metroids algorithm that's been adapted to work on relational data."
        ],
        [
            "So give you a quick outline of how we're going to spend our short time together.",
            "I'm going to introduce to you two clustered graph clustering algorithms, both based on graph theoretic or hop count or geodesic distance.",
            "I'm going to talk about some of the challenges that we run into when using those clustering algorithms and along the way I'm going to show you some synthetic results on some graphs we've generated as long as well as some more qualitative results on real datasets."
        ],
        [
            "Speak to me.",
            "So first step we have the Girvan Newman algorithm.",
            "Revenue algorithm is based on the concept of edge between the centrality.",
            "This is a strategy measure that came out of the social networking analysis community.",
            "You're between the centrality measures, the extent to which an edge links different regions or neighborhoods in the graph.",
            "So in this small toy example.",
            "The edge labeled X has a very low between the centrality as it only links to nodes and the edge labeled Y has higher.",
            "Between centrality.",
            "The formal definition is your between us is the proportion of shortest paths between pairs of nodes in the graph that had given edge or even node.",
            "Lisa pon.",
            "The algorithm itself is pretty straightforward.",
            "We take our graph, we calculate edge between the centrality on all of the edges, and then we find the one with the highest between the centrality and toss it out.",
            "We recalculate between us in the edges.",
            "Again, find the highest one, get rid of that too.",
            "Eventually the graph starts to break apart.",
            "We keep going until the connected components form a number of clusters, and that's our clustering."
        ],
        [
            "Second, how the algorithm is a graphical adaptation of the caimi toids clustering algorithm.",
            "This is based on the K medoids data clustering algorithm which is in itself a variant of the ubiquitous K means data clustering algorithm.",
            "Assuming everyone here."
        ],
        [
            "As well, K means works.",
            "The K Metalheadz variant is sort of a discretized version.",
            "Rather than calculate centroid of each cluster in each iteration of the algorithm, we pick a medoid.",
            "We pick a specific instance that's going to represent the center of that cluster."
        ],
        [
            "Adaptation to a graphical domain.",
            "Is pretty straightforward as well.",
            "We're going to take nodes of the graph as our seeds for clusters, and then assign nodes according to the closest cluster according to the hop count or graph theoretic distance in order to calculate a Metroid.",
            "At each iteration, we're going to use the concept of closest centrality.",
            "Closeness centrality is another centrality measure that was invented in the social networking community.",
            "Closeness measures your proximity to all the other nodes in the graph, so in this again small example node A has a closeness.",
            "Of 1.3 hops, every other node node B has a closeness of 2.1 hops."
        ],
        [
            "So.",
            "How do these algorithms perform in practice?",
            "Well, first plot I have for you shows that the accuracy of our clustering.",
            "Goes down as the difficulty of the clustering goes up.",
            "Shouldn't be too surprising to anyone.",
            "The three lines here show performance for three different sizes of clusters for synthetic datasets.",
            "Now I'm going to get into a little more detail so we know we're talking about what do I mean by accuracy in difficulty?"
        ],
        [
            "First clustering difficulty.",
            "How do we characterize the difficulty of clustering problem?",
            "Well, here I have an example of two graphs.",
            "They have the same number of nodes.",
            "They also have the same number of edges.",
            "In addition, they have the same inherent clustering assignment.",
            "However, the graph on the left.",
            "Has much more tightly connected clusters in the graph.",
            "On the right there.",
            "For clustering the graph on the left is going to be a lot more difficult.",
            "We define a measure called Inter cluster linkage, so simply the proportion of edges in your graph that go between clusters as opposed to within them.",
            "In this example we have graph in the left has an L value of .1 versus .3.",
            "On the right.",
            "I should note that this value gets higher over .5, say.",
            "The clustering problem goes from very difficult to possibly impossible.",
            "This is because that we may not have a graph structure that reflects the clustering.",
            "As we're assigning nodes clusters beforehand, these synthetic graph, so we may be trying to recover something that was never there in the first place."
        ],
        [
            "In terms of measuring clustering accuracy, we have two measures.",
            "One is pairwise intracluster accuracy, the other is pairwise intercluster accuracy.",
            "The first is the proportion of pairs of nodes whose true clusters are the same that are assigned to the same cluster by our algorithm.",
            "Likewise, intercluster accuracy is the proportion of pairs of nodes whose true clusters are different in the original graph that our algorithm puts in different clusters.",
            "The reason we use these measures instead of a more traditional measures such as the Rand index is it for our domains where separating our graph into hundreds if not thousands of clusters.",
            "And here we have the Rand index express in terms of these two accuracy measures.",
            "The right hand term of the numerator is really going to dominate the left hand term.",
            "For a graph with lots of lots of clusters, as the number of nodes in different clusters is going to dwarf the number of nodes in the same cluster, just the number of pairs of nodes in different clusters.",
            "So this just gives us a little more."
        ],
        [
            "Detail.",
            "First challenge we have when using these algorithms for clustering.",
            "Is clustering instability.",
            "This is a result of the fact that we're using geodesic distance in order to assign nodes clusters.",
            "Here's an example of how we can get into trouble.",
            "Node B is assigned to the red cluster mistakenly, even though it's more tightly connected to the blue one.",
            "The reason for this is it's only one hop away from the medoid of the red cluster node A and it's two hops away from the Metroid of the blue cluster node B.",
            "We solve these kinds of problems doing a post processing step on our clustering, which we're calling modal reassignment.",
            "We can perform modal reassignment on any clustering.",
            "It's very simple.",
            "After we do our clustering out.",
            "That's so that's going to correct for problems like we have here."
        ],
        [
            "Very simple."
        ],
        [
            "The second big challenge, and this is the one I mentioned at the beginning of the talk, is the complexity of these algorithms.",
            "The Girvan Newman algorithm is quadratic in the number of edges in the graph.",
            "The Kameeta Lloyd's algorithm is quadratic in the number of nodes, and if we're using even moderately large graphs, we're going to have a hard time running this on a computer that's a little."
        ],
        [
            "In this room.",
            "We can get around this using a network structure index or an NSI and incisors technique we introduced in a paper at KDD last year and NSI consists of a set of node annotations and then a distance function that operates on these annotations.",
            "The distance function takes in the annotations or labels of two nodes.",
            "And provides an estimated hop count between those nodes.",
            "Two ways we can use this for our clustering tasks.",
            "The first is to use the distance function directly.",
            "This is going to help us find closeness centrality, because now to find the distance between a node and the nearest centroid or scuse me toyed, we no longer have to do any sort of searching.",
            "We can simply get an estimate of the distance in constant time.",
            "The 2nd way to use an MSI.",
            "Is to use the distance function as a guide to a best first search.",
            "This is going to allow us to discover short paths in order.",
            "Calculate between a centrality in the Girvan Newman algorithm."
        ],
        [
            "Using NS eyes for these two algorithms takes us down from.",
            "Runtime complexity that squared in the size of the graph down to something linear.",
            "The DZ here.",
            "These are parameters of the construction of the NSA itself.",
            "If you want more detail on how that works, you can come see me at the poster tomorrow night.",
            "Now in case the Big O notation doesn't do it."
        ],
        [
            "For you, we have a plot here of the Wall Clock runtime of the Caimi toys algorithm as a function of the size of the graph.",
            "The top line shows a naive implementation of Kemi Toys in which we're doing traditional breadth first search to find the distance from an ode to a potential Metroid.",
            "The middle line shows what happens if we use best first search.",
            "However, this is an optimal best first search, so it's actually a ceiling, as if we never made a mistake when searching through the graph.",
            "The bottom blue line shows what happens when we use a constant time an aside distance estimation.",
            "Another question, this begs the question of how much is using estimated distance instead of exact graph distance affect our cluster."
        ],
        [
            "Quality.",
            "Well, as it turns out.",
            "Our accuracy actually goes up.",
            "The reason for this is that the size are introducing some estimation error in finding graph distance.",
            "This estimation error can actually smooth out some of the anomalous links in the graph.",
            "We want to make sure that it's not any error that's giving us this effect.",
            "Here we have a plot that shows what happens when we introduce Rant truly random Gaussian noise into our distance measures, and we see these dotted lines down below show that the more noise we introduce.",
            "The more performance goes down.",
            "So clearly there's something about the size that is giving us smoothing in the way we want it and giving more detail on the poster as well.",
            "Alot of it has to do with how the msis are constructed.",
            "Again, we can apply this model reassignment step after our clustering and get a big performance boost."
        ],
        [
            "Results for the German Newman algorithm are similar now here I don't have an exact.",
            "Exact version of the Governmen algorithms compare against.",
            "That's because for even a moderate sized graph, I think these were synthetic graphs of 5000 nodes.",
            "Universe a graph of that size, Girvan Newman algorithm wouldn't complete for us, at least in in a matter of days or weeks that we were willing to let it run.",
            "But German Newman also can be improved with model reassignment.",
            "You'll notice these these results are a little bit better than the same results for the caimi Toids algorithm.",
            "Thing to remember here is though is that the Government Newman algorithm scales with the size.",
            "The number of edges in the node in the graph, whereas Kimi toyed scales with the number of nodes in the graph.",
            "So for our synthetic graph here, I think this is 5000 nodes and the government algorithm was couple orders of magnitude longer in terms of runtime.",
            "So you sort of don't get anything for free.",
            "Now I'm going to show you some more."
        ],
        [
            "Updated results that we have from some real graphs that we looked at.",
            "The first is the core, a citation database.",
            "In this data set, the nodes of the graph for scientific papers.",
            "The links represent a citation from one paper to another.",
            "The good thing about using Quora is we have.",
            "Some sense of well in approximation of ground truth for our clustering.",
            "Each of the papers in Cora has been assigned a topic.",
            "The topics were based on the text in the papers and nothing to do with the link structure.",
            "But as you probably imagine, the links between the papers and the text within them are pretty correlated."
        ],
        [
            "Perform caimi toys clustering on Cora Ann.",
            "This bubble plot shows the relationship between the topics that were assigned to the papers and the clusters we found.",
            "So on one axis we have paper topics and each column is going to represent a paper cluster and the size of the circle inside is going to coincide with the number of papers of that topic that were in that cluster.",
            "So what you should take away from this plot is there's no row or column that has more than one real big circle in it.",
            "We sorted it so that the biggest circle or the most prevalent topic for each paper was stuck on the diagonal.",
            "And if we look at how many papers that represents, turns out that 31% of the mass of our data set lies on this diagonal.",
            "That's only one point 2% of the topic cluster pairs.",
            "So what we're seeing here is that most of the clusters we found have one dominant topic, possibly 2.",
            "Likewise, most of the topics are represented by one cluster we have found."
        ],
        [
            "The other real data set we looked at was the Internet movie database actor graph.",
            "In this graph, the nodes represent actors.",
            "There's a link between them when they have Co starred in a movie together performed, it came either it's clustering on this guy."
        ],
        [
            "Laugh.",
            "Here are some examples of the clusters we found, so the first one, if anyone recognizes the collection of Kevin Smith movies, clerks, chasing Amy, etc.",
            "These are all the actors who've been in those films.",
            "The second cluster, I'm sure is entirely unfamiliar to everyone in this room.",
            "But it's it's Star Trek.",
            "Star Trek People an my advisor, is very excited that this cluster actually represents both the old Star Trek Stars and the new Star Trek Stars together and the last cluster is the one I showed before, which is a collection of Saturday Night Live Stars, the.",
            "The names in blue are the medoids of each one of these clusters."
        ],
        [
            "So to sum up.",
            "Things I want you to remember when leaving relational data can be effectively clustered with some really simple graph.",
            "Clustering algorithms based on graph theoretic distance.",
            "The graphic Amy Toids in Girvan Newman algorithms.",
            "Are both pretty simple, but they both rely on some pretty expensive.",
            "Graph centrality measures and that can cause problems with tractability.",
            "However, we can get around these issues using a network structure index to efficiently index our graph and speed up the computation."
        ],
        [
            "OK. Top.",
            "Are you better compares to multi level graph partitioning techniques like medicine?",
            "No direct comparisons.",
            "We've done.",
            "We've looked at some division methods like that.",
            "Most of the time, most of those methods are still going to be quadratic in the size of the graph.",
            "So you know medicine.",
            "It's based on some sort of in cut calculations.",
            "I think isn't isn't it right?",
            "We have not compared directly, but it's certainly something worth doing.",
            "Looks like describe take care of the house.",
            "The nodes are connected but don't really think there like don't involve the labels, which seems to be very important, but it's a classical chemical structure.",
            "Looking at the labels on the nose.",
            "No, we may not at all.",
            "So these methods I talked about today meant to say this beginning are looking entirely graph structure.",
            "So the datasets we looked at have no attribute values on the nodes at all.",
            "Sort of certainly a next step is adapting these methods to work with both network structure and attributes on the labels, traditional data clustering.",
            "All you have is labels, graph clustering as a first step to graph clustering.",
            "All you have is.",
            "Structure.",
            "We talked about two families of algorithms.",
            "Based on.",
            "Um?",
            "What was the question in there?",
            "Just what about them?",
            "Yes.",
            "Based on.",
            "Certainly worth doing, and there's a whole family of Agglomerative methods.",
            "There's a whole family of of other division methods we just started with two of the simplest and said, how can we improve the performance here that we can scale up to big graphs?",
            "It's not.",
            "There's no decision we made.",
            "We're not going to study.",
            "This is just kind of.",
            "This is our first cut.",
            "Any obvious intuitions as to what would happen?",
            "Nothing obvious, both algorithms should be easily adaptable too.",
            "Two links of strength.",
            "Closest centrality is not rely on equal links, so it works fine with weighted links between the centrality.",
            "Not so much, but you can do it.",
            "So certainly again Futurestep.",
            "OK, so."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, my name is Matt Radigan from the University of Massachusetts.",
                    "label": 1
                },
                {
                    "sent": "I'm going to be talking about graph clustering today.",
                    "label": 0
                },
                {
                    "sent": "This is some joint work done with Mark Mayer, a grad student in my lab, along with David my advisor.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So start off.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna show you a picture of Chris Farley.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Going to use Chris as an example of what I'm after.",
                    "label": 0
                },
                {
                    "sent": "When I say clustering, there's clustering gets used in a lot of different contexts.",
                    "label": 0
                },
                {
                    "sent": "The type of clustering I'm looking for is.",
                    "label": 0
                },
                {
                    "sent": "Nodes that are using instances of data that are associated in some way in some meaningful way with a given instance.",
                    "label": 0
                },
                {
                    "sent": "So in our Chris Farley example, we have a bunch of actors that have Co starred in movies with Chris actors that have Co starred on Saturday Night Live with Chris.",
                    "label": 0
                },
                {
                    "sent": "There's a group shot of them there.",
                    "label": 0
                },
                {
                    "sent": "This is what I mean by clustering, so this is the kind of thing to keep in your head.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Traditional data clustering.",
                    "label": 0
                },
                {
                    "sent": "Operates on IID data.",
                    "label": 0
                },
                {
                    "sent": "Here I have it represented in a small little table.",
                    "label": 0
                },
                {
                    "sent": "Each row in the table is going to be a given instance.",
                    "label": 0
                },
                {
                    "sent": "The attributes of those instances are going to be the columns.",
                    "label": 0
                },
                {
                    "sent": "We take our IID data and we can project it into some nice geometric space.",
                    "label": 0
                },
                {
                    "sent": "And then we form clusters out of that space such that the distances between nodes in the same cluster minimized while we maximize the distances between nodes in different clusters.",
                    "label": 0
                },
                {
                    "sent": "In the relation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No data case.",
                    "label": 0
                },
                {
                    "sent": "Things are a little bit different by relational data.",
                    "label": 0
                },
                {
                    "sent": "I mean any data set that can be represented as a graph such that the nodes of the graph are are instances in the links between them represent some sort of relationship or statistical dependency.",
                    "label": 0
                },
                {
                    "sent": "Between those instances, it's really going to vary with the domain when we do clustering in a graph context, the task is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "Here what we want to do is pull the graph apart such that the clusters of the graph are tightly connected with links.",
                    "label": 0
                },
                {
                    "sent": "And the links between clusters or minimize.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are lot of graph clustering approaches in literature.",
                    "label": 1
                },
                {
                    "sent": "I'm going to talk about a couple of them today.",
                    "label": 0
                },
                {
                    "sent": "The one thing to remember is that most of the graph clustering algorithms share the same weakness and that is they have problems with complexity.",
                    "label": 0
                },
                {
                    "sent": "By problems, I mean they're either quadratic or cubic, or worse in the size of the graph in terms of runtime and his relational datasets get bigger having thousands if not millions of nodes were going to hard time clustering them effectively.",
                    "label": 0
                },
                {
                    "sent": "The good news about this is.",
                    "label": 0
                },
                {
                    "sent": "Is it very simple?",
                    "label": 0
                },
                {
                    "sent": "Graph clustering techniques work really, really well on some of the real datasets we've tested them on.",
                    "label": 0
                },
                {
                    "sent": "That in mind, I'm going to talk about two simple techniques today.",
                    "label": 0
                },
                {
                    "sent": "One is the Gervin Newman to visit clustering algorithm, the other is a K Metroids algorithm that's been adapted to work on relational data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So give you a quick outline of how we're going to spend our short time together.",
                    "label": 0
                },
                {
                    "sent": "I'm going to introduce to you two clustered graph clustering algorithms, both based on graph theoretic or hop count or geodesic distance.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about some of the challenges that we run into when using those clustering algorithms and along the way I'm going to show you some synthetic results on some graphs we've generated as long as well as some more qualitative results on real datasets.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Speak to me.",
                    "label": 0
                },
                {
                    "sent": "So first step we have the Girvan Newman algorithm.",
                    "label": 0
                },
                {
                    "sent": "Revenue algorithm is based on the concept of edge between the centrality.",
                    "label": 0
                },
                {
                    "sent": "This is a strategy measure that came out of the social networking analysis community.",
                    "label": 0
                },
                {
                    "sent": "You're between the centrality measures, the extent to which an edge links different regions or neighborhoods in the graph.",
                    "label": 0
                },
                {
                    "sent": "So in this small toy example.",
                    "label": 0
                },
                {
                    "sent": "The edge labeled X has a very low between the centrality as it only links to nodes and the edge labeled Y has higher.",
                    "label": 0
                },
                {
                    "sent": "Between centrality.",
                    "label": 0
                },
                {
                    "sent": "The formal definition is your between us is the proportion of shortest paths between pairs of nodes in the graph that had given edge or even node.",
                    "label": 0
                },
                {
                    "sent": "Lisa pon.",
                    "label": 0
                },
                {
                    "sent": "The algorithm itself is pretty straightforward.",
                    "label": 0
                },
                {
                    "sent": "We take our graph, we calculate edge between the centrality on all of the edges, and then we find the one with the highest between the centrality and toss it out.",
                    "label": 0
                },
                {
                    "sent": "We recalculate between us in the edges.",
                    "label": 0
                },
                {
                    "sent": "Again, find the highest one, get rid of that too.",
                    "label": 0
                },
                {
                    "sent": "Eventually the graph starts to break apart.",
                    "label": 0
                },
                {
                    "sent": "We keep going until the connected components form a number of clusters, and that's our clustering.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second, how the algorithm is a graphical adaptation of the caimi toids clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "This is based on the K medoids data clustering algorithm which is in itself a variant of the ubiquitous K means data clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "Assuming everyone here.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well, K means works.",
                    "label": 0
                },
                {
                    "sent": "The K Metalheadz variant is sort of a discretized version.",
                    "label": 0
                },
                {
                    "sent": "Rather than calculate centroid of each cluster in each iteration of the algorithm, we pick a medoid.",
                    "label": 0
                },
                {
                    "sent": "We pick a specific instance that's going to represent the center of that cluster.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Adaptation to a graphical domain.",
                    "label": 0
                },
                {
                    "sent": "Is pretty straightforward as well.",
                    "label": 0
                },
                {
                    "sent": "We're going to take nodes of the graph as our seeds for clusters, and then assign nodes according to the closest cluster according to the hop count or graph theoretic distance in order to calculate a Metroid.",
                    "label": 0
                },
                {
                    "sent": "At each iteration, we're going to use the concept of closest centrality.",
                    "label": 1
                },
                {
                    "sent": "Closeness centrality is another centrality measure that was invented in the social networking community.",
                    "label": 0
                },
                {
                    "sent": "Closeness measures your proximity to all the other nodes in the graph, so in this again small example node A has a closeness.",
                    "label": 0
                },
                {
                    "sent": "Of 1.3 hops, every other node node B has a closeness of 2.1 hops.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do these algorithms perform in practice?",
                    "label": 0
                },
                {
                    "sent": "Well, first plot I have for you shows that the accuracy of our clustering.",
                    "label": 0
                },
                {
                    "sent": "Goes down as the difficulty of the clustering goes up.",
                    "label": 0
                },
                {
                    "sent": "Shouldn't be too surprising to anyone.",
                    "label": 0
                },
                {
                    "sent": "The three lines here show performance for three different sizes of clusters for synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to get into a little more detail so we know we're talking about what do I mean by accuracy in difficulty?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First clustering difficulty.",
                    "label": 0
                },
                {
                    "sent": "How do we characterize the difficulty of clustering problem?",
                    "label": 0
                },
                {
                    "sent": "Well, here I have an example of two graphs.",
                    "label": 0
                },
                {
                    "sent": "They have the same number of nodes.",
                    "label": 0
                },
                {
                    "sent": "They also have the same number of edges.",
                    "label": 0
                },
                {
                    "sent": "In addition, they have the same inherent clustering assignment.",
                    "label": 0
                },
                {
                    "sent": "However, the graph on the left.",
                    "label": 0
                },
                {
                    "sent": "Has much more tightly connected clusters in the graph.",
                    "label": 0
                },
                {
                    "sent": "On the right there.",
                    "label": 0
                },
                {
                    "sent": "For clustering the graph on the left is going to be a lot more difficult.",
                    "label": 0
                },
                {
                    "sent": "We define a measure called Inter cluster linkage, so simply the proportion of edges in your graph that go between clusters as opposed to within them.",
                    "label": 0
                },
                {
                    "sent": "In this example we have graph in the left has an L value of .1 versus .3.",
                    "label": 0
                },
                {
                    "sent": "On the right.",
                    "label": 0
                },
                {
                    "sent": "I should note that this value gets higher over .5, say.",
                    "label": 0
                },
                {
                    "sent": "The clustering problem goes from very difficult to possibly impossible.",
                    "label": 0
                },
                {
                    "sent": "This is because that we may not have a graph structure that reflects the clustering.",
                    "label": 0
                },
                {
                    "sent": "As we're assigning nodes clusters beforehand, these synthetic graph, so we may be trying to recover something that was never there in the first place.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of measuring clustering accuracy, we have two measures.",
                    "label": 1
                },
                {
                    "sent": "One is pairwise intracluster accuracy, the other is pairwise intercluster accuracy.",
                    "label": 0
                },
                {
                    "sent": "The first is the proportion of pairs of nodes whose true clusters are the same that are assigned to the same cluster by our algorithm.",
                    "label": 0
                },
                {
                    "sent": "Likewise, intercluster accuracy is the proportion of pairs of nodes whose true clusters are different in the original graph that our algorithm puts in different clusters.",
                    "label": 0
                },
                {
                    "sent": "The reason we use these measures instead of a more traditional measures such as the Rand index is it for our domains where separating our graph into hundreds if not thousands of clusters.",
                    "label": 1
                },
                {
                    "sent": "And here we have the Rand index express in terms of these two accuracy measures.",
                    "label": 0
                },
                {
                    "sent": "The right hand term of the numerator is really going to dominate the left hand term.",
                    "label": 0
                },
                {
                    "sent": "For a graph with lots of lots of clusters, as the number of nodes in different clusters is going to dwarf the number of nodes in the same cluster, just the number of pairs of nodes in different clusters.",
                    "label": 0
                },
                {
                    "sent": "So this just gives us a little more.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Detail.",
                    "label": 0
                },
                {
                    "sent": "First challenge we have when using these algorithms for clustering.",
                    "label": 0
                },
                {
                    "sent": "Is clustering instability.",
                    "label": 0
                },
                {
                    "sent": "This is a result of the fact that we're using geodesic distance in order to assign nodes clusters.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of how we can get into trouble.",
                    "label": 0
                },
                {
                    "sent": "Node B is assigned to the red cluster mistakenly, even though it's more tightly connected to the blue one.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is it's only one hop away from the medoid of the red cluster node A and it's two hops away from the Metroid of the blue cluster node B.",
                    "label": 0
                },
                {
                    "sent": "We solve these kinds of problems doing a post processing step on our clustering, which we're calling modal reassignment.",
                    "label": 0
                },
                {
                    "sent": "We can perform modal reassignment on any clustering.",
                    "label": 1
                },
                {
                    "sent": "It's very simple.",
                    "label": 0
                },
                {
                    "sent": "After we do our clustering out.",
                    "label": 0
                },
                {
                    "sent": "That's so that's going to correct for problems like we have here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very simple.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second big challenge, and this is the one I mentioned at the beginning of the talk, is the complexity of these algorithms.",
                    "label": 0
                },
                {
                    "sent": "The Girvan Newman algorithm is quadratic in the number of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "The Kameeta Lloyd's algorithm is quadratic in the number of nodes, and if we're using even moderately large graphs, we're going to have a hard time running this on a computer that's a little.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this room.",
                    "label": 0
                },
                {
                    "sent": "We can get around this using a network structure index or an NSI and incisors technique we introduced in a paper at KDD last year and NSI consists of a set of node annotations and then a distance function that operates on these annotations.",
                    "label": 1
                },
                {
                    "sent": "The distance function takes in the annotations or labels of two nodes.",
                    "label": 0
                },
                {
                    "sent": "And provides an estimated hop count between those nodes.",
                    "label": 0
                },
                {
                    "sent": "Two ways we can use this for our clustering tasks.",
                    "label": 0
                },
                {
                    "sent": "The first is to use the distance function directly.",
                    "label": 0
                },
                {
                    "sent": "This is going to help us find closeness centrality, because now to find the distance between a node and the nearest centroid or scuse me toyed, we no longer have to do any sort of searching.",
                    "label": 0
                },
                {
                    "sent": "We can simply get an estimate of the distance in constant time.",
                    "label": 0
                },
                {
                    "sent": "The 2nd way to use an MSI.",
                    "label": 0
                },
                {
                    "sent": "Is to use the distance function as a guide to a best first search.",
                    "label": 0
                },
                {
                    "sent": "This is going to allow us to discover short paths in order.",
                    "label": 0
                },
                {
                    "sent": "Calculate between a centrality in the Girvan Newman algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using NS eyes for these two algorithms takes us down from.",
                    "label": 0
                },
                {
                    "sent": "Runtime complexity that squared in the size of the graph down to something linear.",
                    "label": 0
                },
                {
                    "sent": "The DZ here.",
                    "label": 0
                },
                {
                    "sent": "These are parameters of the construction of the NSA itself.",
                    "label": 0
                },
                {
                    "sent": "If you want more detail on how that works, you can come see me at the poster tomorrow night.",
                    "label": 0
                },
                {
                    "sent": "Now in case the Big O notation doesn't do it.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For you, we have a plot here of the Wall Clock runtime of the Caimi toys algorithm as a function of the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "The top line shows a naive implementation of Kemi Toys in which we're doing traditional breadth first search to find the distance from an ode to a potential Metroid.",
                    "label": 0
                },
                {
                    "sent": "The middle line shows what happens if we use best first search.",
                    "label": 0
                },
                {
                    "sent": "However, this is an optimal best first search, so it's actually a ceiling, as if we never made a mistake when searching through the graph.",
                    "label": 0
                },
                {
                    "sent": "The bottom blue line shows what happens when we use a constant time an aside distance estimation.",
                    "label": 0
                },
                {
                    "sent": "Another question, this begs the question of how much is using estimated distance instead of exact graph distance affect our cluster.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quality.",
                    "label": 0
                },
                {
                    "sent": "Well, as it turns out.",
                    "label": 0
                },
                {
                    "sent": "Our accuracy actually goes up.",
                    "label": 0
                },
                {
                    "sent": "The reason for this is that the size are introducing some estimation error in finding graph distance.",
                    "label": 0
                },
                {
                    "sent": "This estimation error can actually smooth out some of the anomalous links in the graph.",
                    "label": 0
                },
                {
                    "sent": "We want to make sure that it's not any error that's giving us this effect.",
                    "label": 0
                },
                {
                    "sent": "Here we have a plot that shows what happens when we introduce Rant truly random Gaussian noise into our distance measures, and we see these dotted lines down below show that the more noise we introduce.",
                    "label": 0
                },
                {
                    "sent": "The more performance goes down.",
                    "label": 0
                },
                {
                    "sent": "So clearly there's something about the size that is giving us smoothing in the way we want it and giving more detail on the poster as well.",
                    "label": 0
                },
                {
                    "sent": "Alot of it has to do with how the msis are constructed.",
                    "label": 0
                },
                {
                    "sent": "Again, we can apply this model reassignment step after our clustering and get a big performance boost.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results for the German Newman algorithm are similar now here I don't have an exact.",
                    "label": 0
                },
                {
                    "sent": "Exact version of the Governmen algorithms compare against.",
                    "label": 0
                },
                {
                    "sent": "That's because for even a moderate sized graph, I think these were synthetic graphs of 5000 nodes.",
                    "label": 0
                },
                {
                    "sent": "Universe a graph of that size, Girvan Newman algorithm wouldn't complete for us, at least in in a matter of days or weeks that we were willing to let it run.",
                    "label": 0
                },
                {
                    "sent": "But German Newman also can be improved with model reassignment.",
                    "label": 0
                },
                {
                    "sent": "You'll notice these these results are a little bit better than the same results for the caimi Toids algorithm.",
                    "label": 0
                },
                {
                    "sent": "Thing to remember here is though is that the Government Newman algorithm scales with the size.",
                    "label": 0
                },
                {
                    "sent": "The number of edges in the node in the graph, whereas Kimi toyed scales with the number of nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So for our synthetic graph here, I think this is 5000 nodes and the government algorithm was couple orders of magnitude longer in terms of runtime.",
                    "label": 0
                },
                {
                    "sent": "So you sort of don't get anything for free.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to show you some more.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Updated results that we have from some real graphs that we looked at.",
                    "label": 0
                },
                {
                    "sent": "The first is the core, a citation database.",
                    "label": 0
                },
                {
                    "sent": "In this data set, the nodes of the graph for scientific papers.",
                    "label": 0
                },
                {
                    "sent": "The links represent a citation from one paper to another.",
                    "label": 0
                },
                {
                    "sent": "The good thing about using Quora is we have.",
                    "label": 0
                },
                {
                    "sent": "Some sense of well in approximation of ground truth for our clustering.",
                    "label": 0
                },
                {
                    "sent": "Each of the papers in Cora has been assigned a topic.",
                    "label": 0
                },
                {
                    "sent": "The topics were based on the text in the papers and nothing to do with the link structure.",
                    "label": 0
                },
                {
                    "sent": "But as you probably imagine, the links between the papers and the text within them are pretty correlated.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perform caimi toys clustering on Cora Ann.",
                    "label": 0
                },
                {
                    "sent": "This bubble plot shows the relationship between the topics that were assigned to the papers and the clusters we found.",
                    "label": 0
                },
                {
                    "sent": "So on one axis we have paper topics and each column is going to represent a paper cluster and the size of the circle inside is going to coincide with the number of papers of that topic that were in that cluster.",
                    "label": 0
                },
                {
                    "sent": "So what you should take away from this plot is there's no row or column that has more than one real big circle in it.",
                    "label": 0
                },
                {
                    "sent": "We sorted it so that the biggest circle or the most prevalent topic for each paper was stuck on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And if we look at how many papers that represents, turns out that 31% of the mass of our data set lies on this diagonal.",
                    "label": 0
                },
                {
                    "sent": "That's only one point 2% of the topic cluster pairs.",
                    "label": 0
                },
                {
                    "sent": "So what we're seeing here is that most of the clusters we found have one dominant topic, possibly 2.",
                    "label": 0
                },
                {
                    "sent": "Likewise, most of the topics are represented by one cluster we have found.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other real data set we looked at was the Internet movie database actor graph.",
                    "label": 1
                },
                {
                    "sent": "In this graph, the nodes represent actors.",
                    "label": 0
                },
                {
                    "sent": "There's a link between them when they have Co starred in a movie together performed, it came either it's clustering on this guy.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Laugh.",
                    "label": 0
                },
                {
                    "sent": "Here are some examples of the clusters we found, so the first one, if anyone recognizes the collection of Kevin Smith movies, clerks, chasing Amy, etc.",
                    "label": 0
                },
                {
                    "sent": "These are all the actors who've been in those films.",
                    "label": 0
                },
                {
                    "sent": "The second cluster, I'm sure is entirely unfamiliar to everyone in this room.",
                    "label": 0
                },
                {
                    "sent": "But it's it's Star Trek.",
                    "label": 0
                },
                {
                    "sent": "Star Trek People an my advisor, is very excited that this cluster actually represents both the old Star Trek Stars and the new Star Trek Stars together and the last cluster is the one I showed before, which is a collection of Saturday Night Live Stars, the.",
                    "label": 0
                },
                {
                    "sent": "The names in blue are the medoids of each one of these clusters.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sum up.",
                    "label": 0
                },
                {
                    "sent": "Things I want you to remember when leaving relational data can be effectively clustered with some really simple graph.",
                    "label": 1
                },
                {
                    "sent": "Clustering algorithms based on graph theoretic distance.",
                    "label": 0
                },
                {
                    "sent": "The graphic Amy Toids in Girvan Newman algorithms.",
                    "label": 1
                },
                {
                    "sent": "Are both pretty simple, but they both rely on some pretty expensive.",
                    "label": 0
                },
                {
                    "sent": "Graph centrality measures and that can cause problems with tractability.",
                    "label": 0
                },
                {
                    "sent": "However, we can get around these issues using a network structure index to efficiently index our graph and speed up the computation.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Top.",
                    "label": 0
                },
                {
                    "sent": "Are you better compares to multi level graph partitioning techniques like medicine?",
                    "label": 0
                },
                {
                    "sent": "No direct comparisons.",
                    "label": 0
                },
                {
                    "sent": "We've done.",
                    "label": 0
                },
                {
                    "sent": "We've looked at some division methods like that.",
                    "label": 0
                },
                {
                    "sent": "Most of the time, most of those methods are still going to be quadratic in the size of the graph.",
                    "label": 0
                },
                {
                    "sent": "So you know medicine.",
                    "label": 0
                },
                {
                    "sent": "It's based on some sort of in cut calculations.",
                    "label": 0
                },
                {
                    "sent": "I think isn't isn't it right?",
                    "label": 0
                },
                {
                    "sent": "We have not compared directly, but it's certainly something worth doing.",
                    "label": 0
                },
                {
                    "sent": "Looks like describe take care of the house.",
                    "label": 0
                },
                {
                    "sent": "The nodes are connected but don't really think there like don't involve the labels, which seems to be very important, but it's a classical chemical structure.",
                    "label": 0
                },
                {
                    "sent": "Looking at the labels on the nose.",
                    "label": 0
                },
                {
                    "sent": "No, we may not at all.",
                    "label": 0
                },
                {
                    "sent": "So these methods I talked about today meant to say this beginning are looking entirely graph structure.",
                    "label": 0
                },
                {
                    "sent": "So the datasets we looked at have no attribute values on the nodes at all.",
                    "label": 0
                },
                {
                    "sent": "Sort of certainly a next step is adapting these methods to work with both network structure and attributes on the labels, traditional data clustering.",
                    "label": 0
                },
                {
                    "sent": "All you have is labels, graph clustering as a first step to graph clustering.",
                    "label": 0
                },
                {
                    "sent": "All you have is.",
                    "label": 0
                },
                {
                    "sent": "Structure.",
                    "label": 0
                },
                {
                    "sent": "We talked about two families of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Based on.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "What was the question in there?",
                    "label": 0
                },
                {
                    "sent": "Just what about them?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Based on.",
                    "label": 0
                },
                {
                    "sent": "Certainly worth doing, and there's a whole family of Agglomerative methods.",
                    "label": 0
                },
                {
                    "sent": "There's a whole family of of other division methods we just started with two of the simplest and said, how can we improve the performance here that we can scale up to big graphs?",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "There's no decision we made.",
                    "label": 0
                },
                {
                    "sent": "We're not going to study.",
                    "label": 0
                },
                {
                    "sent": "This is just kind of.",
                    "label": 0
                },
                {
                    "sent": "This is our first cut.",
                    "label": 0
                },
                {
                    "sent": "Any obvious intuitions as to what would happen?",
                    "label": 0
                },
                {
                    "sent": "Nothing obvious, both algorithms should be easily adaptable too.",
                    "label": 0
                },
                {
                    "sent": "Two links of strength.",
                    "label": 0
                },
                {
                    "sent": "Closest centrality is not rely on equal links, so it works fine with weighted links between the centrality.",
                    "label": 0
                },
                {
                    "sent": "Not so much, but you can do it.",
                    "label": 0
                },
                {
                    "sent": "So certainly again Futurestep.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        }
    }
}