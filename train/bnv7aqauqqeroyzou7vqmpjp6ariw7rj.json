{
    "id": "bnv7aqauqqeroyzou7vqmpjp6ariw7rj",
    "title": "Actively Learning Level-Sets of Composite Functions",
    "info": {
        "author": [
            "Brent Bryan, Carnegie Mellon University"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_bryan_alls/",
    "segmentation": [
        [
            "So this is my talk about active learning level sets of composite functions work that I did with."
        ],
        [
            "Jeff Schneider and so the motivation for this.",
            "This talk is that we've got three different astronomical datasets, and we've got a common set of parameter space, and so we have eight different parameters, and for the astronomers in the group, that's what they are for.",
            "The non astronomers.",
            "Essentially the idea is we're interested in parameters that will tell us about the age of the composition, dark matter, things like that about the universe.",
            "For each of these datasets.",
            "We then have a model associated with that, and So what we can do is take parameter vector out of the parameter space, run it through each of these models and then compare the output of the model.",
            "Work the data that we observed to get some kind of P value.",
            "And then at the end of the day, what we want to do is take these P values and combine them together.",
            "And so the goal of the project is to compute these minimally joint valid 1 minus Alpha confidence regions for all eight parameters simultaneously.",
            "So the minimal is important because if we didn't go for minimal then the best thing would do is just select the entire parameter space, because that would be a one myself."
        ],
        [
            "Confidence region.",
            "But the trick here is that the models can be expensive to compute.",
            "So first I'm going to talk about combining the P values and there's many ways to do this, so you could do like bonferroni's method or an inverse normal inverse logit.",
            "What I'm going to talk about here is using Fisher's method and so the idea is that you just take the log of the P values and we sum up and we check to see if that's greater than a critical value where the critical value comes from a chi squared distribution.",
            "And so the idea is if we define a function FI which is just this log of a P value, then what we really have is a function which is a sum of some other sets of functions and what we're interested in is then where that function F is greater than equal to see.",
            "I should point out that Fisher's method is not the only method that you can combine P values with and factor."
        ],
        [
            "This kind of matter.",
            "So.",
            "So now let's talk about the level set problem.",
            "So suppose we're given some function F, denoted by this black line here, and we're interested in where this F is equal to, say, 11.",
            "So that would be this pink dotted line, and so there's."
        ],
        [
            "In ways that we could go about sampling this and trying to figure out what's going on.",
            "As we've heard in these active the talks already, there's different approaches, so we could pick points randomly or uniformly or something like that, just as kind of a motivating example.",
            "Here's example that will come to later, and you can see that just picking them randomly.",
            "The predicted boundaries in red and the true boundaries in blue, and the result you get is pretty poor."
        ],
        [
            "So, so it seems like we should be able to do something better using active learning, and so the idea is that we're going to take out the samples that we have an we can approximate the function that we're trying to observe F, and then also the noise associated with the approximation.",
            "And so there's many different heuristics that we could try.",
            "Things like entropy variance, misclassification, probability, etc.",
            "So we showed a couple years ago that this trial heuristic works best for this kind of problem for the."
        ],
        [
            "Set detection problem and so the struggle heuristic is listed here and it's essentially we try to trade off the variance for the closeness to the boundary that we're interested in.",
            "I should point out here that when we sample a point F, we're assuming that there's no noise in that F. If there's noise in there, you have to tweak this here is took a little bit, but it will still work.",
            "So the idea is that now we're going to go in sample points according to this heuristic.",
            "And again we can toss it in some regression code and then come up with an approximate model and then the points that we would predict or than the points from the regression model.",
            "And like I said, this is a mix of variance in the first term and entropy in this."
        ],
        [
            "Second term essentially.",
            "So the trick here in this work is what if we had more information, right?",
            "So what I showed you earlier was that the function F that we're interested in is actually the sum of these other little FIS."
        ],
        [
            "And so so so RF there the black line is actually a composition of these three other functions, and so that means if we pick the sample on F, it's actually equal to the sum of these other three points, and so we can trivially take the strike you Ristic that we had before, and justice replace all the sigmas with the sum over the Sigma eyes and all the apps with the sum of the Sigma, although some of the F. But what we're hoping to do here.",
            "Oh, and like any active learning problem, right?",
            "We want to minimize the samples and what I mean by samples is not samples of now the sum F, But the sum of the individual FIS.",
            "So we want to minimize the number of samples we go to."
        ],
        [
            "FI.",
            "So, so the idea that we're trying to get away with is maybe we could get something more out of knowing that F is a sum of little FIS, and so for instance, what if we were interested in this point way out in the middle of nowhere, and we the question that we are interested in is F here greater than or equal to the threshold?",
            "Well, we know that that Black point is equal to the sum of these three points, but if we just looked at that first red point, we realized that we didn't even have to look at the other two points, and so we could get away with essentially only sampling one point instead of three.",
            "And so that's kind of the intuition of what we're trying to do here."
        ],
        [
            "So kind of.",
            "In summary, the difference between the single function that we did a couple years ago and this one here is that in the single function case we're using this trial heuristic to balance exploration and exploitation.",
            "And this was the straw heuristic.",
            "And the idea is that trying to mimic information gain in the multiple function what we function case what we have is, we only can sample one of the FIS at any given time.",
            "And what we instead of reducing the variance by the Sigma term over here because we're only sampling one of the functions, we only expect to reduce the variance by Sigma, I squared, and so a better estimate of the knowledge gained at any sample is not going to be the struggle heuristic, but rather this one where instead of you know it's instead of having this via Sigma squared term, it's a Sigma I squared term where the Sigma is associated with the function that we're sampling."
        ],
        [
            "So the the algorithm outline just pictorially is we're going to have some parameter space and some datasets that we start with possibly empty.",
            "And we're going to have one data set for each.",
            "Of the FIS that we are interested in, and so then we can just generate some candidates from our parameter space and then take those candidates and the information that we already have in our datasets and build up a regression model.",
            "We're using Gaussian processes, but you could use pretty much anything as long as you get the estimate of the variance as well, and then you can take those regression measurements and you're going to want to choose the best point to choose and also the best one of these sub functions that you want to look at.",
            "And once you've chosen F. XNFI then you actually compute that value and then return it to your data set and repeat.",
            "So like I said before, we've got many different heuristics that we could go with.",
            "We could just choose the points randomly.",
            "We could use some kind of variance heuristic.",
            "The combined star, which was essentially we just take straddle an we replace all the F by the sum of the FIS and all the signals and with the sum of the sickness and then the variance Max variant struggle heuristic, which is the one I just showed on the."
        ],
        [
            "This slide.",
            "So to give you an idea what this is going to look like, this is just a 2 dimensional example.",
            "So I have four different functions here that I've plotted, so that's two dimensions are the the Theta and then the third dimension is the.",
            "The output that we're trying to predict the level set on an.",
            "What I'm going to do is just take those four functions.",
            "We can sell them together to get the function F, and So what we're interested in here is predicting where F is equal to minus two, which is kind of a.",
            "Blue purple color on the screen so you can see it kind of in the.",
            "In the top, in the middle of the right one.",
            "And I'll show you on the next slide with the shape should look like.",
            "And so to show you the points that are actually getting sampled by the algorithm, I'm just going to label each of these different functions by a different point, shape or a different color here."
        ],
        [
            "So, so let's look at how these sampling heuristics do on that previous function.",
            "So here is random and variance and again the red lines are the predicted level.",
            "Set.",
            "The blue lines are the true level set.",
            "So the thing to notice about the true level set is actually disjoint.",
            "There's two little pieces and they both go off the boundary there on the right hand side, and as you can see, both random invariants don't do very well on this data set.",
            "We could use the combined straddle heuristic, which is going to do a little bit better.",
            "The thing to notice here is that all the points are black because just because the way can you plot of them, but all the points are picked right on top of each other, which means that there should be 4 different symbols, all stacked right on top of each other in that plot.",
            "Another way you could do is you could just use the variance heuristic with straddle.",
            "So the idea is that we're going to use the strong heuristic to pick which points to pick, and so by point I mean X and once we have X then we can just go through and ask which of the FIS has the maximum variance, and then we'll pick that one.",
            "And then the one that actually works the best is this variance Max variance, one which replaces the Sigma term in the straw heuristic with the Sigma I.",
            "So the thing to note that about these plots is that the Max variance one can pick points off in the left hand corner of the plot and it only has to pick one of the functions and it can realize very quickly that it doesn't need to pick points from the other functions in that area and as a result it can move.",
            "Almost all of it sampling over to the right hand area where there were actually interested in.",
            "The."
        ],
        [
            "The level set.",
            "So I don't want to go into it too much, but here's some experimental results on different different datasets.",
            "The variance, Max variance one is the red blinds here on the bottom, and as you can see it outperforms the other heuristics."
        ],
        [
            "These data sets as well.",
            "So let's go back to the Cosmo logical example.",
            "So our goal again is to minimize the jointly valid 1 minus Alpha confidence regions for all these parameters, and just to give you an idea of how we're going to take that active learning algorithm and get these confidence regions first, I want to just look at the supernova model.",
            "Just by itself, and so the reason I'm picking this model is a 3 dimensional model, so I can do some.",
            "I can kind of display it on this."
        ],
        [
            "Being a little easier.",
            "So, so here's the result of the supernova model on the left.",
            "I show what the 95% confidence regions are using a chi squared test.",
            "So like I said, it's a 3 dimensional model.",
            "And so, for instance, if we took this point X one, it's where the Hubble constant is equal to 65 and the Omega matters equals 0.2, three.",
            "And so the question is, well, what does it mean that it's red, right?",
            "That it's included and what I mean is that there exists some value of Omega Lambda.",
            "The third parameter, which was the question mark such that the P value is greater than Alpha.",
            "We can't test all of the points right in this Omega Lambda space, So what we're going to do is create these plots by gridding up the."
        ],
        [
            "Samples, so more specifically, what I mean by that is we're going to.",
            "So if I take this 2 dimensional plot and kind of rotate it into 3 dimensional space, right?",
            "So Mega Lambdas this kind of third dimension down this point corresponds to a column in the third dimension, and so then what I'm going to do is the square is actually going to be included if any cell in that.",
            "Acts as a P value greater than Alpha.",
            "Cool.",
            "We can't create this 3 dimensional grid.",
            "Well, we could probably create this 3 dimensional one, but we can create an 8 dimensional one and go through and check every single cell in there.",
            "So we're going to do is we're going to take all the points so for every point in our data set X that we've sampled what we can then do is go through and say hey which square would it be in?",
            "And if that if that X value corresponds to AP value that's greater than Alpha, then we'll just color the square greater than a, so will never actually going to create.",
            "All these grid cells we only have to iterate through the samples after we're done sampling.",
            "And I'd like to point out that this is a conservative estimate because the two dimensional plot at the top right we're not slicing this third dimension.",
            "We're actually taking kind of the Max over the entire thing, so this plot is actually kind of a paper thin thing through the third dimension."
        ],
        [
            "So here's what happens when you do it with the three other datasets.",
            "So the CMB data set is on the top left, the supernova.",
            "The top right?",
            "So these are different parameters.",
            "That's why it looks different and large scale structure is the third one is the bottom left and when you combine them together using our that straddle heuristic that we're showing, you get a much smaller, much tighter confidence region.",
            "So you might be wondering why this one, the large scale structure 1 looks so blocky and the reason is is because we didn't actually use this trial heuristic to pick the points there.",
            "We actually picked them off the grid.",
            "You can see that when you pick them off the grid, even though you're using billion samples, it's still not that great.",
            "And that's in seven dimensions, but in using the strike, your stick in eight dimensions and only picking you know 3 million points, you get a pretty tight estimate of what's going."
        ],
        [
            "So in conclusion, what we've done is we've extended the straw heuristic to work in with modeled multiple datasets.",
            "We showed that combining the P values can be written as a sum of these observable functions, and then we've used that technology to derive confidence regions for three different.",
            "Cosmological datasets.",
            "They might have questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is my talk about active learning level sets of composite functions work that I did with.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jeff Schneider and so the motivation for this.",
                    "label": 0
                },
                {
                    "sent": "This talk is that we've got three different astronomical datasets, and we've got a common set of parameter space, and so we have eight different parameters, and for the astronomers in the group, that's what they are for.",
                    "label": 0
                },
                {
                    "sent": "The non astronomers.",
                    "label": 0
                },
                {
                    "sent": "Essentially the idea is we're interested in parameters that will tell us about the age of the composition, dark matter, things like that about the universe.",
                    "label": 0
                },
                {
                    "sent": "For each of these datasets.",
                    "label": 0
                },
                {
                    "sent": "We then have a model associated with that, and So what we can do is take parameter vector out of the parameter space, run it through each of these models and then compare the output of the model.",
                    "label": 0
                },
                {
                    "sent": "Work the data that we observed to get some kind of P value.",
                    "label": 0
                },
                {
                    "sent": "And then at the end of the day, what we want to do is take these P values and combine them together.",
                    "label": 0
                },
                {
                    "sent": "And so the goal of the project is to compute these minimally joint valid 1 minus Alpha confidence regions for all eight parameters simultaneously.",
                    "label": 1
                },
                {
                    "sent": "So the minimal is important because if we didn't go for minimal then the best thing would do is just select the entire parameter space, because that would be a one myself.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Confidence region.",
                    "label": 0
                },
                {
                    "sent": "But the trick here is that the models can be expensive to compute.",
                    "label": 0
                },
                {
                    "sent": "So first I'm going to talk about combining the P values and there's many ways to do this, so you could do like bonferroni's method or an inverse normal inverse logit.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to talk about here is using Fisher's method and so the idea is that you just take the log of the P values and we sum up and we check to see if that's greater than a critical value where the critical value comes from a chi squared distribution.",
                    "label": 0
                },
                {
                    "sent": "And so the idea is if we define a function FI which is just this log of a P value, then what we really have is a function which is a sum of some other sets of functions and what we're interested in is then where that function F is greater than equal to see.",
                    "label": 0
                },
                {
                    "sent": "I should point out that Fisher's method is not the only method that you can combine P values with and factor.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of matter.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So now let's talk about the level set problem.",
                    "label": 0
                },
                {
                    "sent": "So suppose we're given some function F, denoted by this black line here, and we're interested in where this F is equal to, say, 11.",
                    "label": 0
                },
                {
                    "sent": "So that would be this pink dotted line, and so there's.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In ways that we could go about sampling this and trying to figure out what's going on.",
                    "label": 0
                },
                {
                    "sent": "As we've heard in these active the talks already, there's different approaches, so we could pick points randomly or uniformly or something like that, just as kind of a motivating example.",
                    "label": 1
                },
                {
                    "sent": "Here's example that will come to later, and you can see that just picking them randomly.",
                    "label": 0
                },
                {
                    "sent": "The predicted boundaries in red and the true boundaries in blue, and the result you get is pretty poor.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so it seems like we should be able to do something better using active learning, and so the idea is that we're going to take out the samples that we have an we can approximate the function that we're trying to observe F, and then also the noise associated with the approximation.",
                    "label": 0
                },
                {
                    "sent": "And so there's many different heuristics that we could try.",
                    "label": 1
                },
                {
                    "sent": "Things like entropy variance, misclassification, probability, etc.",
                    "label": 1
                },
                {
                    "sent": "So we showed a couple years ago that this trial heuristic works best for this kind of problem for the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Set detection problem and so the struggle heuristic is listed here and it's essentially we try to trade off the variance for the closeness to the boundary that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "I should point out here that when we sample a point F, we're assuming that there's no noise in that F. If there's noise in there, you have to tweak this here is took a little bit, but it will still work.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that now we're going to go in sample points according to this heuristic.",
                    "label": 0
                },
                {
                    "sent": "And again we can toss it in some regression code and then come up with an approximate model and then the points that we would predict or than the points from the regression model.",
                    "label": 0
                },
                {
                    "sent": "And like I said, this is a mix of variance in the first term and entropy in this.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Second term essentially.",
                    "label": 0
                },
                {
                    "sent": "So the trick here in this work is what if we had more information, right?",
                    "label": 1
                },
                {
                    "sent": "So what I showed you earlier was that the function F that we're interested in is actually the sum of these other little FIS.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so so so RF there the black line is actually a composition of these three other functions, and so that means if we pick the sample on F, it's actually equal to the sum of these other three points, and so we can trivially take the strike you Ristic that we had before, and justice replace all the sigmas with the sum over the Sigma eyes and all the apps with the sum of the Sigma, although some of the F. But what we're hoping to do here.",
                    "label": 0
                },
                {
                    "sent": "Oh, and like any active learning problem, right?",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the samples and what I mean by samples is not samples of now the sum F, But the sum of the individual FIS.",
                    "label": 1
                },
                {
                    "sent": "So we want to minimize the number of samples we go to.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "FI.",
                    "label": 0
                },
                {
                    "sent": "So, so the idea that we're trying to get away with is maybe we could get something more out of knowing that F is a sum of little FIS, and so for instance, what if we were interested in this point way out in the middle of nowhere, and we the question that we are interested in is F here greater than or equal to the threshold?",
                    "label": 0
                },
                {
                    "sent": "Well, we know that that Black point is equal to the sum of these three points, but if we just looked at that first red point, we realized that we didn't even have to look at the other two points, and so we could get away with essentially only sampling one point instead of three.",
                    "label": 0
                },
                {
                    "sent": "And so that's kind of the intuition of what we're trying to do here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So kind of.",
                    "label": 0
                },
                {
                    "sent": "In summary, the difference between the single function that we did a couple years ago and this one here is that in the single function case we're using this trial heuristic to balance exploration and exploitation.",
                    "label": 1
                },
                {
                    "sent": "And this was the straw heuristic.",
                    "label": 1
                },
                {
                    "sent": "And the idea is that trying to mimic information gain in the multiple function what we function case what we have is, we only can sample one of the FIS at any given time.",
                    "label": 1
                },
                {
                    "sent": "And what we instead of reducing the variance by the Sigma term over here because we're only sampling one of the functions, we only expect to reduce the variance by Sigma, I squared, and so a better estimate of the knowledge gained at any sample is not going to be the struggle heuristic, but rather this one where instead of you know it's instead of having this via Sigma squared term, it's a Sigma I squared term where the Sigma is associated with the function that we're sampling.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the algorithm outline just pictorially is we're going to have some parameter space and some datasets that we start with possibly empty.",
                    "label": 1
                },
                {
                    "sent": "And we're going to have one data set for each.",
                    "label": 0
                },
                {
                    "sent": "Of the FIS that we are interested in, and so then we can just generate some candidates from our parameter space and then take those candidates and the information that we already have in our datasets and build up a regression model.",
                    "label": 0
                },
                {
                    "sent": "We're using Gaussian processes, but you could use pretty much anything as long as you get the estimate of the variance as well, and then you can take those regression measurements and you're going to want to choose the best point to choose and also the best one of these sub functions that you want to look at.",
                    "label": 0
                },
                {
                    "sent": "And once you've chosen F. XNFI then you actually compute that value and then return it to your data set and repeat.",
                    "label": 0
                },
                {
                    "sent": "So like I said before, we've got many different heuristics that we could go with.",
                    "label": 0
                },
                {
                    "sent": "We could just choose the points randomly.",
                    "label": 0
                },
                {
                    "sent": "We could use some kind of variance heuristic.",
                    "label": 0
                },
                {
                    "sent": "The combined star, which was essentially we just take straddle an we replace all the F by the sum of the FIS and all the signals and with the sum of the sickness and then the variance Max variant struggle heuristic, which is the one I just showed on the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This slide.",
                    "label": 0
                },
                {
                    "sent": "So to give you an idea what this is going to look like, this is just a 2 dimensional example.",
                    "label": 0
                },
                {
                    "sent": "So I have four different functions here that I've plotted, so that's two dimensions are the the Theta and then the third dimension is the.",
                    "label": 0
                },
                {
                    "sent": "The output that we're trying to predict the level set on an.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do is just take those four functions.",
                    "label": 0
                },
                {
                    "sent": "We can sell them together to get the function F, and So what we're interested in here is predicting where F is equal to minus two, which is kind of a.",
                    "label": 0
                },
                {
                    "sent": "Blue purple color on the screen so you can see it kind of in the.",
                    "label": 0
                },
                {
                    "sent": "In the top, in the middle of the right one.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you on the next slide with the shape should look like.",
                    "label": 0
                },
                {
                    "sent": "And so to show you the points that are actually getting sampled by the algorithm, I'm just going to label each of these different functions by a different point, shape or a different color here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so let's look at how these sampling heuristics do on that previous function.",
                    "label": 1
                },
                {
                    "sent": "So here is random and variance and again the red lines are the predicted level.",
                    "label": 0
                },
                {
                    "sent": "Set.",
                    "label": 0
                },
                {
                    "sent": "The blue lines are the true level set.",
                    "label": 1
                },
                {
                    "sent": "So the thing to notice about the true level set is actually disjoint.",
                    "label": 0
                },
                {
                    "sent": "There's two little pieces and they both go off the boundary there on the right hand side, and as you can see, both random invariants don't do very well on this data set.",
                    "label": 0
                },
                {
                    "sent": "We could use the combined straddle heuristic, which is going to do a little bit better.",
                    "label": 0
                },
                {
                    "sent": "The thing to notice here is that all the points are black because just because the way can you plot of them, but all the points are picked right on top of each other, which means that there should be 4 different symbols, all stacked right on top of each other in that plot.",
                    "label": 0
                },
                {
                    "sent": "Another way you could do is you could just use the variance heuristic with straddle.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we're going to use the strong heuristic to pick which points to pick, and so by point I mean X and once we have X then we can just go through and ask which of the FIS has the maximum variance, and then we'll pick that one.",
                    "label": 0
                },
                {
                    "sent": "And then the one that actually works the best is this variance Max variance, one which replaces the Sigma term in the straw heuristic with the Sigma I.",
                    "label": 0
                },
                {
                    "sent": "So the thing to note that about these plots is that the Max variance one can pick points off in the left hand corner of the plot and it only has to pick one of the functions and it can realize very quickly that it doesn't need to pick points from the other functions in that area and as a result it can move.",
                    "label": 0
                },
                {
                    "sent": "Almost all of it sampling over to the right hand area where there were actually interested in.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The level set.",
                    "label": 0
                },
                {
                    "sent": "So I don't want to go into it too much, but here's some experimental results on different different datasets.",
                    "label": 1
                },
                {
                    "sent": "The variance, Max variance one is the red blinds here on the bottom, and as you can see it outperforms the other heuristics.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These data sets as well.",
                    "label": 0
                },
                {
                    "sent": "So let's go back to the Cosmo logical example.",
                    "label": 0
                },
                {
                    "sent": "So our goal again is to minimize the jointly valid 1 minus Alpha confidence regions for all these parameters, and just to give you an idea of how we're going to take that active learning algorithm and get these confidence regions first, I want to just look at the supernova model.",
                    "label": 1
                },
                {
                    "sent": "Just by itself, and so the reason I'm picking this model is a 3 dimensional model, so I can do some.",
                    "label": 0
                },
                {
                    "sent": "I can kind of display it on this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Being a little easier.",
                    "label": 0
                },
                {
                    "sent": "So, so here's the result of the supernova model on the left.",
                    "label": 0
                },
                {
                    "sent": "I show what the 95% confidence regions are using a chi squared test.",
                    "label": 1
                },
                {
                    "sent": "So like I said, it's a 3 dimensional model.",
                    "label": 0
                },
                {
                    "sent": "And so, for instance, if we took this point X one, it's where the Hubble constant is equal to 65 and the Omega matters equals 0.2, three.",
                    "label": 0
                },
                {
                    "sent": "And so the question is, well, what does it mean that it's red, right?",
                    "label": 0
                },
                {
                    "sent": "That it's included and what I mean is that there exists some value of Omega Lambda.",
                    "label": 0
                },
                {
                    "sent": "The third parameter, which was the question mark such that the P value is greater than Alpha.",
                    "label": 0
                },
                {
                    "sent": "We can't test all of the points right in this Omega Lambda space, So what we're going to do is create these plots by gridding up the.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Samples, so more specifically, what I mean by that is we're going to.",
                    "label": 0
                },
                {
                    "sent": "So if I take this 2 dimensional plot and kind of rotate it into 3 dimensional space, right?",
                    "label": 0
                },
                {
                    "sent": "So Mega Lambdas this kind of third dimension down this point corresponds to a column in the third dimension, and so then what I'm going to do is the square is actually going to be included if any cell in that.",
                    "label": 1
                },
                {
                    "sent": "Acts as a P value greater than Alpha.",
                    "label": 0
                },
                {
                    "sent": "Cool.",
                    "label": 0
                },
                {
                    "sent": "We can't create this 3 dimensional grid.",
                    "label": 0
                },
                {
                    "sent": "Well, we could probably create this 3 dimensional one, but we can create an 8 dimensional one and go through and check every single cell in there.",
                    "label": 0
                },
                {
                    "sent": "So we're going to do is we're going to take all the points so for every point in our data set X that we've sampled what we can then do is go through and say hey which square would it be in?",
                    "label": 0
                },
                {
                    "sent": "And if that if that X value corresponds to AP value that's greater than Alpha, then we'll just color the square greater than a, so will never actually going to create.",
                    "label": 0
                },
                {
                    "sent": "All these grid cells we only have to iterate through the samples after we're done sampling.",
                    "label": 0
                },
                {
                    "sent": "And I'd like to point out that this is a conservative estimate because the two dimensional plot at the top right we're not slicing this third dimension.",
                    "label": 0
                },
                {
                    "sent": "We're actually taking kind of the Max over the entire thing, so this plot is actually kind of a paper thin thing through the third dimension.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's what happens when you do it with the three other datasets.",
                    "label": 0
                },
                {
                    "sent": "So the CMB data set is on the top left, the supernova.",
                    "label": 0
                },
                {
                    "sent": "The top right?",
                    "label": 0
                },
                {
                    "sent": "So these are different parameters.",
                    "label": 0
                },
                {
                    "sent": "That's why it looks different and large scale structure is the third one is the bottom left and when you combine them together using our that straddle heuristic that we're showing, you get a much smaller, much tighter confidence region.",
                    "label": 0
                },
                {
                    "sent": "So you might be wondering why this one, the large scale structure 1 looks so blocky and the reason is is because we didn't actually use this trial heuristic to pick the points there.",
                    "label": 1
                },
                {
                    "sent": "We actually picked them off the grid.",
                    "label": 1
                },
                {
                    "sent": "You can see that when you pick them off the grid, even though you're using billion samples, it's still not that great.",
                    "label": 0
                },
                {
                    "sent": "And that's in seven dimensions, but in using the strike, your stick in eight dimensions and only picking you know 3 million points, you get a pretty tight estimate of what's going.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, what we've done is we've extended the straw heuristic to work in with modeled multiple datasets.",
                    "label": 0
                },
                {
                    "sent": "We showed that combining the P values can be written as a sum of these observable functions, and then we've used that technology to derive confidence regions for three different.",
                    "label": 1
                },
                {
                    "sent": "Cosmological datasets.",
                    "label": 0
                },
                {
                    "sent": "They might have questions.",
                    "label": 0
                }
            ]
        }
    }
}