{
    "id": "bk4qa5qsghrisyf2xxz5i7gzurmukzwi",
    "title": "DistLODStats: Distributed Computation of RDF Dataset Statistics",
    "info": {
        "author": [
            "Gezim Sejdiu, Institute of Computer Science, University of Bonn"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_sejdiu_distlodstats_distributed/",
    "segmentation": [
        [
            "Good afternoon everyone.",
            "My name is Gotham City.",
            "I'm PhD student at University of Bonn and today I will be talking about distributed load starts, distributed computation over RDF datasets statistiques.",
            "Basically, during my talk this."
        ],
        [
            "The outline of the talk is like following.",
            "I will give an introduction and motivation why this is important for the community and also I will go through the approach we have chosen a bit of empirical evaluation we did for the resource and also the use cases which we have.",
            "We are aware that they are using distributed Cloud starts already and I will conclude the talk and give some hints for the future work."
        ],
        [
            "Over the last two decades, there are semantic web community has grown from the main idea of the basically designing the modeling of the data to the more broad array or based on the protocols and standards which could be could be for the publication exchange on the web and this basically."
        ],
        [
            "Give us that for the record."
        ],
        [
            "Maybe count more than 10,000 datasets available out there using these semantic web standards."
        ],
        [
            "Plus many others which basically are kept private for so many reasons.",
            "Thanks to these standards, the data are being are becoming large.",
            "Datasets are becoming machine readable and we buy."
        ],
        [
            "Never, unless this data may not benefit fully benefit from the from the data without having an apriori statistical information from the internals of the data and the structure of the data.",
            "For that reason we may need statistics right and statistique."
        ],
        [
            "These are basically.",
            "Useful for such such cases, like vocabulary reuse when we want to find the vocabularies which are suitable for your datasets."
        ],
        [
            "Coverage analysis which did those datasets, contains the necessary information in your need."
        ],
        [
            "Privacy Analiza stores your data sets.",
            "Contains any sensitive data and also like."
        ],
        [
            "Bing prediction, which datasets are basically suitable for doing the interlinking or link prediction on your case as I mentioned."
        ],
        [
            "I need statistics and for that reason there have been many tools out there who those those statistics are based on the instances or even in the schema level.",
            "But they basically are.",
            "They were struggling on the data when the data becomes too huge or even it's larger.",
            "For that reason we need something which basically is capable to scale out to the larger number of the cluster, and for that we propose a distributed."
        ],
        [
            "Cloud starts, which is a software framework which compute 32 predefined statistiques of our large scale using Apache Spark and Apache Flink as well.",
            "But before I go to the internals of the distributed load styles, let me introduce our beautiful project."
        ],
        [
            "Which is called Sansa.",
            "It's it's core is basically distributed dataflow and join which does so.",
            "Reading inferencing, querying and machine learning over each of these large RDF datasets and it is basically a model or framework which contains the different layers like knowledge representation and distribution influencing, querying and on top of it we have some analytics and the distributive law starts is basically part of this larger framework which we have built on it.",
            "The."
        ],
        [
            "We adopted this criteria statistical criteria from the paper which was published from Dent or at all on 2012, and then those statistics we have just in contrast with them.",
            "We basically have used Overlord.",
            "Distributed computing framework like Apache Spark in our case and the statistical criteria basically is defined as the triple of FDP bar.",
            "F stands for the condition operator like sparkled and these derived datasets from that filter which we operate on top of it and the P is basically the post processing operator which could be optional.",
            "In case you don't want to use it and we basically use RDS as a.",
            "Basically, we do use our DDS of triples as an internal structure for applying those statistics on top of Apache Spark."
        ],
        [
            "First we have to load the data on the large scale storage which spark and read efficiency.",
            "In our case we use Hadoop distributed file System and afterwards we use Sons of functionality to basically read fast and apply a different strategy of querying the data and for that reason we have this RDF triple.",
            "This recall main data set and afterwards we apply those rules, filter and post processing defined from each of the statistical criteria.",
            "In context of Spark execution, basically it generates an execution plan which it's optimal for the executing those statistics over these large scale datasets and in the end we of course produce and void description of the statistics for better machine readable data."
        ],
        [
            "Just have to give you on a brief of the workflow of the disability law stocks works based on the data should be loaded on HTFS.",
            "I mention it and."
        ],
        [
            "If you sunshine join as distributed framework for computing those statistics which could be from a low 32, we have collected let's say just five here.",
            "But we will get an example for class distribution which covers the usage of the class to the whole datasets and give the frequency of the classes to the given data sets which is defined by this."
        ],
        [
            "Rule, let's say a filter which basically mentioned that that property should be typed on the object should be UI and then those are basically using on.",
            "So you can just read like easily which one line and perform those statistics and get those statistics on the fly using Hadoop ecosystem right?",
            "And in the end?"
        ],
        [
            "The set is basically the second is this filtering I mentioned and?"
        ],
        [
            "We apply this action which basically just aggregate or takes the counts for this class distribution over different nodes depends on your cluster configuration, right and after the class.",
            "This cluster configuration set reduce has been performed to the different nodes.",
            "Then we can."
        ],
        [
            "Like the data in the end to the master, which basically gave gets you the sum of each of the classes which has been used on the data set."
        ],
        [
            "And of course, this could be visualized on using web world or other web anthology visualization tools.",
            "And also we have a Samsung notebooks which basically can.",
            "Also you can also run your code snippets on the on the web at door."
        ],
        [
            "The aim of our evaluation was basically to see how well our approach performs against non distributed tools and also to see the to do a bit of analysis of our approach scales out to the number of bigger number of nodes on the cluster.",
            "We basically try to address those three question how does the runtime of the algorithm change when the nodes in the cluster are added and how does the algorithm scales to the larger datasets and the third one half does the algorithm scales to the number.",
            "So larger number of the datasets which we have played around on the Evolution section."
        ],
        [
            "We have basically stuck carry on two sets of experiments.",
            "The first one is sorry, mostly on.",
            "If checking out how well our approach basically performs on distributed manner and the setup of the cluster is as follow and we have run this on the cluster with six nodes, one nor does matter in the five nodes as a as a worker and BF carry out the data on the same HTFS cluster as well, and I guess the Hadoop was two point 8.0 and the spark two point 2.0.",
            "And this color 20 level and the datasets we have chosen to real world datasets and one synthetic datasets just to see out how the size up scalability works in our approach."
        ],
        [
            "In this case, we have basically set up two experiments, one checking out if the distributed load starts performs well.",
            "Also on the local mode and also on the cluster mode against the original load starts and we basically have seen that."
        ],
        [
            "In most of the cases distributed lots, that's basically finish the computation and load starts basically fail on most of the cases.",
            "In only one case, and also perform, a distributor starts and the reason is why Becausw load starts runs on the each file separately and further reason the time runtime basically was collected to each file, which are pretty small in all the normal case distributed lots basically compute as full graph.",
            "And then."
        ],
        [
            "We also have around this on the cluster when we kept the number of nodes 5 and we have seen that the geometric mean is like more than 7.4 times faster when we run on the cluster, even the cluster is smaller commodity cluster."
        ],
        [
            "Yeah, the ratio has been computed.",
            "Basically how the runtime divided by the runtime on the class of divided by the runtime on the local mode minus one."
        ],
        [
            "For an beach, this also implied that distributed large shows consistent improvement for each data set when the number of workers or the cluster has been bigger."
        ],
        [
            "We also wanted to measure the notes, couple scalability and size up scalability in order to measure the node scalability, we kept the we kept.",
            "Sorry."
        ],
        [
            "We kept the data set constants like 15 GBB SPM datasets and we basically vary with the number of.",
            "Not sorry, we kept the number of workers constant and we were at with the data set.",
            "So basically we increased the let's say the number of magnitude 3 the data set we were able to run until one terabyte of data.",
            "You know in this cluster and we have seen that the trend basically is quite a near linear or even sometimes it's super linear because the most of the tasks were able to perform using Apache Spark in the fast manner."
        ],
        [
            "In other cases, we also tested out the node scalability when we kept.",
            "Basically we kept the size of the of the size of the data set constant, divided the number of the of the nodes in the cluster, and we have seen that for sure the scalability of the speed up there.",
            "Pretty super linear and we're really happy that that happened on our evaluation.",
            "And we also computed the effect differences."
        ],
        [
            "Is of distributed Losos, which basically measure the efficiency of the of the resorts and also how effective is on which number of nodes it's reached.",
            "The point when you don't need to increase the number of nodes anymore and we can see here that will say the trend, basically super linear or even sometimes near linear and the efficiency goes until two number of four nodes that we can also.",
            "Remove one of the nodes.",
            "Depends.",
            "Let's say you want to use the cluster somewhere like Amazon Databricks, which basically you somehow assume that how many nodes you may need to run your application on the cluster."
        ],
        [
            "In order to do basically show you the usability of our resource, we have.",
            "Also, we are aware for those use cases one of the use cases which comes because we were inspired to do this work was from the load starts which basically crawled the state of state of the art or current state of the LOD on the cluster and the disabled list starts.",
            "Sorry on the L Odeon, distributed start has been used as an underlying engine for those datasets which basically are not able to run.",
            "From the yellow alot starts the 2nd."
        ],
        [
            "Use cases from the Alethia use case, which basically are Ethereum datasets artif eyes and contains more than 36 billion triples and the distributed Lascelles was rounder on databricks notebooks.",
            "Over 100 nodes cluster and the third one is from big."
        ],
        [
            "After your project, which is an easy to use framework which basically contains the different from big data frameworks and saw it contains a lot of logs and distributed load starts has been used to basically generate statistics out of those logs."
        ],
        [
            "And the 3rd or 4th one is for Upstart, which basically has been used for those datasets.",
            "Statistiques, which upset they, were not able to generate."
        ],
        [
            "Yeah, to recap, we basically obtaining the overview of the of the data.",
            "We it's considered to be data intensive and also computing intensive.",
            "For that reason we it's a challenge to find out really scalable and efficient framework which shows you the internals of the data and for that reason we propose distributed lots which basically it's a novel software framework, part of the larger sense of framework for distributing for computing those RDF datasets statistiques.",
            "Are on the cluster.",
            "And of course we are even we are glad that unhappy for the scalability.",
            "We still wants to perform further improvement on the scalability and the load balancing for doing more intelligent indexing and also interested in partition of the spark.",
            "Thank you for your attention and yeah I'm happy for your question.",
            "To answer your question.",
            "Thank you for your talk time for questions.",
            "What do we have question over here?",
            "Hello, thanks for the talk.",
            "I want to ask, it seems from the presentation that in order for you to be able to compute the statistics, you actually need to have the data load it in your storage.",
            "Is that right?",
            "Yeah, it's right.",
            "OK, so this is different right from the original stats, which you sparkline points that were already on the web.",
            "To compute the statistics over the data that they did not have, right?",
            "Yeah, it's right.",
            "For that reason, we basically have done this because relying on sparkle endpoint, it usually doesn't work for larger data sets, so we wanted.",
            "Like we, I also forgot to mention that we have this satisfied which basically is the rest interface for distributed lawsuits that everyone from elsewhere can basically run the to the bigger datasets which were not able to crossover from Elodie Cloud or Elodie starts, yeah.",
            "OK, so maybe a follow up question is can we then use your distributed load stats framework to compute the real overview of the web of data like based on the sparkline points out there, like lots dusted?",
            "Yes there is.",
            "I because right now we are working with the Pedia community and they have introduced Databus which basically you can also just generate your own datasets and plug into DB pedia.",
            "And the idea is that we will also rolled back the datasets to the DPI itself so everyone can basically generate their own datasets to DB pedia.",
            "We trigger goes by the sparkle endpoint and we generate the data sets on the fly and give them back to the DB pedia itself.",
            "So that's the plan.",
            "It's still on working.",
            "Ongoing work with us, but this wasn't as use case here because it came after the paper.",
            "OK, one more quick question.",
            "Could I check make a quick check?",
            "Usually we are not encouraged to switch talks in this session.",
            "However, Ricardo has a problem because he has in parallel the minute madness and they are quite late, so we would like to offer him to give the presentation 1st and then afterwards would be the presentation from Hun Day.",
            "I hope I pronounced your ride.",
            "Is anyone objecting to that solution?",
            "OK, so we do it like this.",
            "Please prepare there and we make the next question as well.",
            "Hi so I was wondering since you have edge edge bazer for like storing the graphs, have you thought about like?",
            "Computing statistics on the evolutions of graphs overtime, like exploiting one of the columns for like time and see like the updates of the graphs or something like that.",
            "Basically, we haven't used H pays for keeping the data there.",
            "We have used only HTFS to keep the data on the role of data, so we haven't thought about that right now.",
            "There is a work because sometimes a big project.",
            "Basically we are plugging in everything there and one work is on quality checks.",
            "We basically want to see the quality checks of the data or the quality assessment of the data on demand, which you can also see the indifferent.",
            "Timeslots what the data was there, but we can also consider that for statistics as well.",
            "Good point, thank you.",
            "Thank you very much to the speaker.",
            "Thank you, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is Gotham City.",
                    "label": 0
                },
                {
                    "sent": "I'm PhD student at University of Bonn and today I will be talking about distributed load starts, distributed computation over RDF datasets statistiques.",
                    "label": 1
                },
                {
                    "sent": "Basically, during my talk this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The outline of the talk is like following.",
                    "label": 0
                },
                {
                    "sent": "I will give an introduction and motivation why this is important for the community and also I will go through the approach we have chosen a bit of empirical evaluation we did for the resource and also the use cases which we have.",
                    "label": 0
                },
                {
                    "sent": "We are aware that they are using distributed Cloud starts already and I will conclude the talk and give some hints for the future work.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over the last two decades, there are semantic web community has grown from the main idea of the basically designing the modeling of the data to the more broad array or based on the protocols and standards which could be could be for the publication exchange on the web and this basically.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give us that for the record.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe count more than 10,000 datasets available out there using these semantic web standards.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus many others which basically are kept private for so many reasons.",
                    "label": 0
                },
                {
                    "sent": "Thanks to these standards, the data are being are becoming large.",
                    "label": 0
                },
                {
                    "sent": "Datasets are becoming machine readable and we buy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Never, unless this data may not benefit fully benefit from the from the data without having an apriori statistical information from the internals of the data and the structure of the data.",
                    "label": 0
                },
                {
                    "sent": "For that reason we may need statistics right and statistique.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are basically.",
                    "label": 0
                },
                {
                    "sent": "Useful for such such cases, like vocabulary reuse when we want to find the vocabularies which are suitable for your datasets.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Coverage analysis which did those datasets, contains the necessary information in your need.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Privacy Analiza stores your data sets.",
                    "label": 0
                },
                {
                    "sent": "Contains any sensitive data and also like.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing prediction, which datasets are basically suitable for doing the interlinking or link prediction on your case as I mentioned.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I need statistics and for that reason there have been many tools out there who those those statistics are based on the instances or even in the schema level.",
                    "label": 0
                },
                {
                    "sent": "But they basically are.",
                    "label": 0
                },
                {
                    "sent": "They were struggling on the data when the data becomes too huge or even it's larger.",
                    "label": 0
                },
                {
                    "sent": "For that reason we need something which basically is capable to scale out to the larger number of the cluster, and for that we propose a distributed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cloud starts, which is a software framework which compute 32 predefined statistiques of our large scale using Apache Spark and Apache Flink as well.",
                    "label": 0
                },
                {
                    "sent": "But before I go to the internals of the distributed load styles, let me introduce our beautiful project.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is called Sansa.",
                    "label": 0
                },
                {
                    "sent": "It's it's core is basically distributed dataflow and join which does so.",
                    "label": 0
                },
                {
                    "sent": "Reading inferencing, querying and machine learning over each of these large RDF datasets and it is basically a model or framework which contains the different layers like knowledge representation and distribution influencing, querying and on top of it we have some analytics and the distributive law starts is basically part of this larger framework which we have built on it.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We adopted this criteria statistical criteria from the paper which was published from Dent or at all on 2012, and then those statistics we have just in contrast with them.",
                    "label": 0
                },
                {
                    "sent": "We basically have used Overlord.",
                    "label": 0
                },
                {
                    "sent": "Distributed computing framework like Apache Spark in our case and the statistical criteria basically is defined as the triple of FDP bar.",
                    "label": 0
                },
                {
                    "sent": "F stands for the condition operator like sparkled and these derived datasets from that filter which we operate on top of it and the P is basically the post processing operator which could be optional.",
                    "label": 0
                },
                {
                    "sent": "In case you don't want to use it and we basically use RDS as a.",
                    "label": 0
                },
                {
                    "sent": "Basically, we do use our DDS of triples as an internal structure for applying those statistics on top of Apache Spark.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First we have to load the data on the large scale storage which spark and read efficiency.",
                    "label": 0
                },
                {
                    "sent": "In our case we use Hadoop distributed file System and afterwards we use Sons of functionality to basically read fast and apply a different strategy of querying the data and for that reason we have this RDF triple.",
                    "label": 0
                },
                {
                    "sent": "This recall main data set and afterwards we apply those rules, filter and post processing defined from each of the statistical criteria.",
                    "label": 0
                },
                {
                    "sent": "In context of Spark execution, basically it generates an execution plan which it's optimal for the executing those statistics over these large scale datasets and in the end we of course produce and void description of the statistics for better machine readable data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just have to give you on a brief of the workflow of the disability law stocks works based on the data should be loaded on HTFS.",
                    "label": 0
                },
                {
                    "sent": "I mention it and.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you sunshine join as distributed framework for computing those statistics which could be from a low 32, we have collected let's say just five here.",
                    "label": 0
                },
                {
                    "sent": "But we will get an example for class distribution which covers the usage of the class to the whole datasets and give the frequency of the classes to the given data sets which is defined by this.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rule, let's say a filter which basically mentioned that that property should be typed on the object should be UI and then those are basically using on.",
                    "label": 0
                },
                {
                    "sent": "So you can just read like easily which one line and perform those statistics and get those statistics on the fly using Hadoop ecosystem right?",
                    "label": 0
                },
                {
                    "sent": "And in the end?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The set is basically the second is this filtering I mentioned and?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We apply this action which basically just aggregate or takes the counts for this class distribution over different nodes depends on your cluster configuration, right and after the class.",
                    "label": 0
                },
                {
                    "sent": "This cluster configuration set reduce has been performed to the different nodes.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like the data in the end to the master, which basically gave gets you the sum of each of the classes which has been used on the data set.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course, this could be visualized on using web world or other web anthology visualization tools.",
                    "label": 0
                },
                {
                    "sent": "And also we have a Samsung notebooks which basically can.",
                    "label": 0
                },
                {
                    "sent": "Also you can also run your code snippets on the on the web at door.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The aim of our evaluation was basically to see how well our approach performs against non distributed tools and also to see the to do a bit of analysis of our approach scales out to the number of bigger number of nodes on the cluster.",
                    "label": 0
                },
                {
                    "sent": "We basically try to address those three question how does the runtime of the algorithm change when the nodes in the cluster are added and how does the algorithm scales to the larger datasets and the third one half does the algorithm scales to the number.",
                    "label": 1
                },
                {
                    "sent": "So larger number of the datasets which we have played around on the Evolution section.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have basically stuck carry on two sets of experiments.",
                    "label": 0
                },
                {
                    "sent": "The first one is sorry, mostly on.",
                    "label": 0
                },
                {
                    "sent": "If checking out how well our approach basically performs on distributed manner and the setup of the cluster is as follow and we have run this on the cluster with six nodes, one nor does matter in the five nodes as a as a worker and BF carry out the data on the same HTFS cluster as well, and I guess the Hadoop was two point 8.0 and the spark two point 2.0.",
                    "label": 0
                },
                {
                    "sent": "And this color 20 level and the datasets we have chosen to real world datasets and one synthetic datasets just to see out how the size up scalability works in our approach.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, we have basically set up two experiments, one checking out if the distributed load starts performs well.",
                    "label": 0
                },
                {
                    "sent": "Also on the local mode and also on the cluster mode against the original load starts and we basically have seen that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In most of the cases distributed lots, that's basically finish the computation and load starts basically fail on most of the cases.",
                    "label": 0
                },
                {
                    "sent": "In only one case, and also perform, a distributor starts and the reason is why Becausw load starts runs on the each file separately and further reason the time runtime basically was collected to each file, which are pretty small in all the normal case distributed lots basically compute as full graph.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also have around this on the cluster when we kept the number of nodes 5 and we have seen that the geometric mean is like more than 7.4 times faster when we run on the cluster, even the cluster is smaller commodity cluster.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, the ratio has been computed.",
                    "label": 0
                },
                {
                    "sent": "Basically how the runtime divided by the runtime on the class of divided by the runtime on the local mode minus one.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For an beach, this also implied that distributed large shows consistent improvement for each data set when the number of workers or the cluster has been bigger.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also wanted to measure the notes, couple scalability and size up scalability in order to measure the node scalability, we kept the we kept.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We kept the data set constants like 15 GBB SPM datasets and we basically vary with the number of.",
                    "label": 0
                },
                {
                    "sent": "Not sorry, we kept the number of workers constant and we were at with the data set.",
                    "label": 0
                },
                {
                    "sent": "So basically we increased the let's say the number of magnitude 3 the data set we were able to run until one terabyte of data.",
                    "label": 0
                },
                {
                    "sent": "You know in this cluster and we have seen that the trend basically is quite a near linear or even sometimes it's super linear because the most of the tasks were able to perform using Apache Spark in the fast manner.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In other cases, we also tested out the node scalability when we kept.",
                    "label": 0
                },
                {
                    "sent": "Basically we kept the size of the of the size of the data set constant, divided the number of the of the nodes in the cluster, and we have seen that for sure the scalability of the speed up there.",
                    "label": 1
                },
                {
                    "sent": "Pretty super linear and we're really happy that that happened on our evaluation.",
                    "label": 0
                },
                {
                    "sent": "And we also computed the effect differences.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is of distributed Losos, which basically measure the efficiency of the of the resorts and also how effective is on which number of nodes it's reached.",
                    "label": 0
                },
                {
                    "sent": "The point when you don't need to increase the number of nodes anymore and we can see here that will say the trend, basically super linear or even sometimes near linear and the efficiency goes until two number of four nodes that we can also.",
                    "label": 1
                },
                {
                    "sent": "Remove one of the nodes.",
                    "label": 0
                },
                {
                    "sent": "Depends.",
                    "label": 0
                },
                {
                    "sent": "Let's say you want to use the cluster somewhere like Amazon Databricks, which basically you somehow assume that how many nodes you may need to run your application on the cluster.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to do basically show you the usability of our resource, we have.",
                    "label": 0
                },
                {
                    "sent": "Also, we are aware for those use cases one of the use cases which comes because we were inspired to do this work was from the load starts which basically crawled the state of state of the art or current state of the LOD on the cluster and the disabled list starts.",
                    "label": 0
                },
                {
                    "sent": "Sorry on the L Odeon, distributed start has been used as an underlying engine for those datasets which basically are not able to run.",
                    "label": 1
                },
                {
                    "sent": "From the yellow alot starts the 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use cases from the Alethia use case, which basically are Ethereum datasets artif eyes and contains more than 36 billion triples and the distributed Lascelles was rounder on databricks notebooks.",
                    "label": 0
                },
                {
                    "sent": "Over 100 nodes cluster and the third one is from big.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After your project, which is an easy to use framework which basically contains the different from big data frameworks and saw it contains a lot of logs and distributed load starts has been used to basically generate statistics out of those logs.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the 3rd or 4th one is for Upstart, which basically has been used for those datasets.",
                    "label": 0
                },
                {
                    "sent": "Statistiques, which upset they, were not able to generate.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, to recap, we basically obtaining the overview of the of the data.",
                    "label": 0
                },
                {
                    "sent": "We it's considered to be data intensive and also computing intensive.",
                    "label": 0
                },
                {
                    "sent": "For that reason we it's a challenge to find out really scalable and efficient framework which shows you the internals of the data and for that reason we propose distributed lots which basically it's a novel software framework, part of the larger sense of framework for distributing for computing those RDF datasets statistiques.",
                    "label": 1
                },
                {
                    "sent": "Are on the cluster.",
                    "label": 0
                },
                {
                    "sent": "And of course we are even we are glad that unhappy for the scalability.",
                    "label": 0
                },
                {
                    "sent": "We still wants to perform further improvement on the scalability and the load balancing for doing more intelligent indexing and also interested in partition of the spark.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention and yeah I'm happy for your question.",
                    "label": 0
                },
                {
                    "sent": "To answer your question.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your talk time for questions.",
                    "label": 0
                },
                {
                    "sent": "What do we have question over here?",
                    "label": 0
                },
                {
                    "sent": "Hello, thanks for the talk.",
                    "label": 0
                },
                {
                    "sent": "I want to ask, it seems from the presentation that in order for you to be able to compute the statistics, you actually need to have the data load it in your storage.",
                    "label": 0
                },
                {
                    "sent": "Is that right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's right.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is different right from the original stats, which you sparkline points that were already on the web.",
                    "label": 0
                },
                {
                    "sent": "To compute the statistics over the data that they did not have, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's right.",
                    "label": 0
                },
                {
                    "sent": "For that reason, we basically have done this because relying on sparkle endpoint, it usually doesn't work for larger data sets, so we wanted.",
                    "label": 0
                },
                {
                    "sent": "Like we, I also forgot to mention that we have this satisfied which basically is the rest interface for distributed lawsuits that everyone from elsewhere can basically run the to the bigger datasets which were not able to crossover from Elodie Cloud or Elodie starts, yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so maybe a follow up question is can we then use your distributed load stats framework to compute the real overview of the web of data like based on the sparkline points out there, like lots dusted?",
                    "label": 0
                },
                {
                    "sent": "Yes there is.",
                    "label": 0
                },
                {
                    "sent": "I because right now we are working with the Pedia community and they have introduced Databus which basically you can also just generate your own datasets and plug into DB pedia.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that we will also rolled back the datasets to the DPI itself so everyone can basically generate their own datasets to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "We trigger goes by the sparkle endpoint and we generate the data sets on the fly and give them back to the DB pedia itself.",
                    "label": 0
                },
                {
                    "sent": "So that's the plan.",
                    "label": 0
                },
                {
                    "sent": "It's still on working.",
                    "label": 0
                },
                {
                    "sent": "Ongoing work with us, but this wasn't as use case here because it came after the paper.",
                    "label": 0
                },
                {
                    "sent": "OK, one more quick question.",
                    "label": 0
                },
                {
                    "sent": "Could I check make a quick check?",
                    "label": 0
                },
                {
                    "sent": "Usually we are not encouraged to switch talks in this session.",
                    "label": 0
                },
                {
                    "sent": "However, Ricardo has a problem because he has in parallel the minute madness and they are quite late, so we would like to offer him to give the presentation 1st and then afterwards would be the presentation from Hun Day.",
                    "label": 0
                },
                {
                    "sent": "I hope I pronounced your ride.",
                    "label": 0
                },
                {
                    "sent": "Is anyone objecting to that solution?",
                    "label": 0
                },
                {
                    "sent": "OK, so we do it like this.",
                    "label": 0
                },
                {
                    "sent": "Please prepare there and we make the next question as well.",
                    "label": 0
                },
                {
                    "sent": "Hi so I was wondering since you have edge edge bazer for like storing the graphs, have you thought about like?",
                    "label": 0
                },
                {
                    "sent": "Computing statistics on the evolutions of graphs overtime, like exploiting one of the columns for like time and see like the updates of the graphs or something like that.",
                    "label": 0
                },
                {
                    "sent": "Basically, we haven't used H pays for keeping the data there.",
                    "label": 0
                },
                {
                    "sent": "We have used only HTFS to keep the data on the role of data, so we haven't thought about that right now.",
                    "label": 0
                },
                {
                    "sent": "There is a work because sometimes a big project.",
                    "label": 0
                },
                {
                    "sent": "Basically we are plugging in everything there and one work is on quality checks.",
                    "label": 0
                },
                {
                    "sent": "We basically want to see the quality checks of the data or the quality assessment of the data on demand, which you can also see the indifferent.",
                    "label": 0
                },
                {
                    "sent": "Timeslots what the data was there, but we can also consider that for statistics as well.",
                    "label": 0
                },
                {
                    "sent": "Good point, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much to the speaker.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thanks.",
                    "label": 0
                }
            ]
        }
    }
}