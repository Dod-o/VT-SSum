{
    "id": "6o62xod7p3ylxmoqwz2bxpcsm3opkzcd",
    "title": "Learning with spectral representations and use of MDL principles",
    "info": {
        "author": [
            "Edwin Hancock, Department of Computer Science, University of York"
        ],
        "published": "July 19, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Pattern Recognition",
            "Top->Computer Science->Machine Learning->Preprocessing"
        ]
    },
    "url": "http://videolectures.net/gbr07_hancock_lwsr/",
    "segmentation": [
        [
            "OK, So what I wanted to do with this talk was really to give an overview of some work that Richard Wilson I have doing been doing now for a number of years.",
            "I think since 1999 and it sort of fits with the topic of this workshop because we've been concerned with trying to develop algorithms for learning with graph representations, particularly the vision domain."
        ],
        [
            "So this is the outline of the talk.",
            "I'll give some motivation and background, but I want to do is to really give you the way in which we were thinking about this.",
            "This problem along time ago, now probably back in 2000 and to show how that kind of influence what we did, how we identified a set of problems that looked as if they were worth looking at in the vision domain.",
            "As far as learning with graphs.",
            "Then I'll for those of you who don't come from computer vision.",
            "I'll talk a little bit about how graph data comes from images.",
            "And then I'll talk a little bit about some particular technical aspects of this program, which I think might might be interesting to this this audience, the first of the use of his use of spectral invariants to perform the clustering of graphs.",
            "And then I'll talk a little bit about some some work with trees, particularly using minimum description length and how description, links, ideas can be learned, used to learn generative models of tree structures, and then provide some conclusions which are really kind of.",
            "Up"
        ],
        [
            "Future directions for research.",
            "So."
        ],
        [
            "As the motivating part of the talk in computer vision, graph structures are used to abstract image structure, I'll show you in some concrete terms later on how this comes about.",
            "But the problem in vision is that the data we do with has not begun its life in a graph form.",
            "It's began its life in the form of an image, and this means that we have to have to use segmentation algorithms to extract graphs or the primitives from which the graphs are built.",
            "And these are not reliable and so it means that the graph structures that we're dealing with contain considerable NUM."
        ],
        [
            "Is virus.",
            "So as a result, there are both additional and missing nodes due to segmentation error and variations in edge structure analysis.",
            "Someone has to deal with both as a form of noise but also as a form of general genuine variation."
        ],
        [
            "So.",
            "If we're trying to deal with graph structures representing images, the image matching recognition can be reduced to a graph isomorphism or cannot be reduced.",
            "Iser to graph isomorphism, or a subgraph isomorphism problem.",
            "Instead, we have to develop inexact graph matching algorithms to deal with this kind of data."
        ],
        [
            "Now there's an enormous amount of literature on on measuring the similarity of graphs and trying to deal with these practical problems in computer vision, and so it's really a historical slide.",
            "You can retrace the work all the way back to Barrow and Popplestone the group working in Edinburgh, who introduced the idea of an Association graph and a very very early computer vision application invite involving a robot placement cell.",
            "And they showed how it was possible to match graphs by.",
            "Searching for the maximum common subgraph of the Association graph.",
            "Then in the 1980s, the field of syntactic and structural pattern recognition sort of came into being, and I think several people is audiences from that community, particularly from the GBR end of things, and this was really concerned with trying to formalize the problem of matching graph structures and one of the achievements here was to extend the concept of edit distance from strings to graphs.",
            "No more recently.",
            "Workers have really come round to trying to develop more principled algorithms for dealing with in exactness.",
            "In the graph matching problem and in comparing the similarity of graphs and so in the 1990s, and since then I've been a number of attempts to use principle techniques from both probability theory and also from optimization to develop principled graph matching techniques.",
            "And then most recently of all this kind of whole effort seems to have come full circle and a number of people, particularly bunker and palillo, have have shown how some of the early work on Association graph and the maximum common subgraph could be linked to these more recent ideas from optimization theory."
        ],
        [
            "So how can we view this work?",
            "Well, it's shown how to measure the similarity of graphs and can be used to locate inexact matches when there's significant levels of structural error present, it solves the graph matching problem in the sort of scenarios you get in computer vision.",
            "It also provides a means by which we can try to assess the ways in which structural variation occurs in sets of graphs, possibly from the same family of objects, in a principled way, and so it's really this.",
            "Topic that I want to focus on here.",
            "How we can we can use or how we can develop learning algorithms that make use of similarity measures to try to understand how the structure in sets of graphs representing maybe objects of the same class."
        ],
        [
            "Varies.",
            "So this is really how we looked at the problem.",
            "Quite a long long time ago, now around 1930 but around 2000.",
            "Looking at the literature and and considering the.",
            "Learning problem with graphs in the computer vision domain, there seem to be 4 distinct topics that we could look at in terms of increasing ambitiousness and complexity.",
            "So the first thing we want to do would be to try to learn class structure.",
            "So the idea here is that abstract shapes using graphs I'll show show you how you might do this in a moment, and then the objects that you have graphs for represent shapes coming from different classes.",
            "How can how can we use graph methods to learn the class structure of a set of a set of shapes well in order to do this what we need is.",
            "A distance measure or a vector of graph characteristics.",
            "If we if we have a vector of graph characteristics, and for instance, we can start to do central clustering and effective pattern recognition techniques are available for doing that.",
            "But when we when we have only a distance measure, things become more complicated and we have to do pairwise clustering, and in particular if you want to try to compute the similarity between a pair of graphs, what you need to do is to establish correspondence and this is in fact between the nodes of the graphs and this is a computationally.",
            "Demanding problem.",
            "So the first problem is to learn class structure.",
            "You can do this with a vector of graph characteristics.",
            "I'll give you an example of how you might do this later on the talk.",
            "Or you can use it with a pattern of distances.",
            "The next problem you might want to look at is is how to embed a graphs in a low dimensional space.",
            "Once you once we have graphs in a low dimensional space, then by traversing that space in some sense we could try to traverse the modes of variation in in the set of graphs that we're looking at, and this is sort of an important step in the direction of trying to construct a generative model for a family of graphs.",
            "So here we are going to do this.",
            "We're again going to need correspondence is because we will need the graphs in vector form and unfortunately graphs are not delivered in the form of vectors and that means we have to do some work and the use of intelligence in order to get them into a vectorial form.",
            "But here graph spectral methods offer offer away which may circumvent the need for correspondences between nodes.",
            "And again I'll talk a little bit about how we can.",
            "Extracts spectral characteristics from graphs which allow us to construct low dimensional spaces.",
            "And then with such representation, then we can use standard statistical and geometric learning methods on the embedded graphs.",
            "Coming up a level, what we might want to do is as well as trying to learn the class structure and the way in which graphs distribute themselves across a pattern space.",
            "We may actually want to learn in some detail.",
            "The modes of structural variation that are present in a family of graphs belonging to the same class.",
            "So I mean, if you want to think about what this involves, it really involves trying to understand how the connectivity structure of a graph may vary for instances belonging to the same class.",
            "And then really most ambitious of all.",
            "If we have a way of trying to characterize the way in which the structural variations take place within a set of graphs belonging to the same class, we may want to send build a generative model which allows us to sample these variations in a probabilistic way."
        ],
        [
            "So what are the obstacles to setting about these four different activities?",
            "Well, the first is no, don't do that.",
            "Restart later, please restart later and the first is that graphs are not vectors and most of what we have available in the pattern recognition literature really works with pattern vectors, and the reason that graphs are not vectors is that.",
            "There's no Canonical ordering for the nodes in a graph, and if we want to try to begin to try to vectorize graphs, then we need correspondence is to establish the order.",
            "There are guess ways of getting around this.",
            "So for instance, if one can effectively constructor vector of characteristics for a graph or somehow to summarize the properties of a graph using that using a histogram.",
            "Then we have a vector representation, but that's not.",
            "That's not a representation from which we can construct a generative model.",
            "It's simply a summary of the properties of the graphs.",
            "It doesn't actually say anything about the nodes, and the node connectivity structure.",
            "So then we come to the problem that we have structural variations in graphs and so generally speaking this is expressed in terms of differences of number of nodes and the pattern of connectivity between them.",
            "And as I pointed out earlier, if we're dealing with graphs from image data because of segmentation error, we sometimes we have not just genuine variation due to the structure of a class.",
            "But sometimes we have variation due to noise.",
            "So as a result of all of this, it means that graphs are not easily summarized.",
            "They don't reside in a vector space, and so this means that character extracting statistical characteristics such as mean covariance is quite difficult."
        ],
        [
            "So here's here's a sort of slide which summarizes what we're up against here.",
            "I have a sequence of images.",
            "As the camera pans around an object, what we've done is we've extracted corner features from these these images and then triangulated the corner features using Delaunay triangulation to construct a graph, and underneath you see the the pattern of variation in the graphs as we pan around the object number of things happen.",
            "First of all, the connectivity structure of the triangulation changes as points move.",
            "Closer and further apart from each other, but they're also points that come into view due to occlusion effects, and there are also points that come and go due to the fact that the corner detector which is pulling out the features in these images is sometimes producing false positives and sometimes producing false negatives."
        ],
        [
            "So, um, what Richard I did was to try to have a crack at solving some of those those problems I listed on the earlier slide, and so one of the things we've done is to look at how we can use how we can compute.",
            "Permutation invariant graph characteristics from the Laplacian spectrum.",
            "I'll talk a little bit more about that in detail.",
            "Later on.",
            "We focused very much on how edit distance between graphs can be computed in a principled way using both probabilistic methods and also relaxation methods.",
            "We've looked on at the issue of how we can embed graphs in pattern spaces using properties of the random walk and the looked at geometric characterizations.",
            "Of graphs and get embedded in fact pattern spaces.",
            "And then Lastly, we've looked at how how to learn generative models of the structure of sets of graphs using description, length, principles.",
            "And again I'll give you a little insight into some of the technical details that underpin that."
        ],
        [
            "So what I want to do is talk.",
            "Is some length about spectral methods and the idea of behind graph spectral methods that use the eigenvalues night vectors of the Laplacian matrix as a as a character characterization of graph structure and there are number of concrete examples of pursuing this particular computer vision literature.",
            "So for instance, AMA has a very early graph spectral technique for matching graphs that deals with graphs that have the same number of nodes.",
            "But where there is a variation in edge structure?",
            "Scott and Higgins and Shapiro and Brady have used graph spectral methods to solve the point correspondence problem in computer vision, and I guess all of you are aware of the work of shared Matic which use an eigenvector method for image segmentation and clustering, and there's also work due to Dickinson and check and fun day where graph spectral methods have been used to index index trees."
        ],
        [
            "So what I want to do is just to give you a quick idea of of how graphs come about in the computer vision domain because I said right at the beginning of this talk, we're actually struggling against a problem and that is to say our graphs don't begin life as graphs.",
            "They begin life as as images and so the process of actually getting a graph out of an image is a hard one."
        ],
        [
            "So here is one of the standard graph structures that is used to take image data and converted into a graph.",
            "It's a it's a Delaunay graph.",
            "The idea here is that you take feature points from an image.",
            "These might be things like corners.",
            "They could be the centroids of image regions.",
            "They could be points of interest.",
            "Once you have the points you you grow a seed circle from the point until the seeds collide.",
            "The collision fronts divide the image plane into polygons and then if you take the region adjacency graph of the polygons, you get the Delaunay triangulation."
        ],
        [
            "And this is exactly how I generated the sequence of graphs that I showed you earlier.",
            "So that's one way of."
        ],
        [
            "Doing it, another popular representation is there is a shot graph.",
            "The shock graph is a way of characterizing the structure of a skeleton of a 2D object.",
            "So you take an image you extract by binary, thresholding the silhouette of the object.",
            "Then you find the medial axis of that object.",
            "The medial axis is characterized by the locus of points where there is a bitangent circle to the boundary, and then if you look at the way in which that.",
            "The radius of that by tangent circle varies as you traverse the skeleton.",
            "Then you can characterize the skeleton as having four different shop classes.",
            "So these depend on whether the boundary associated with the branch with skeleton is tapering, constricted, constant, or bulging."
        ],
        [
            "So, um.",
            "What I wanted to do in this talk is to focus on two issues pretty quickly.",
            "I think to give you a flavor of some of the problems that we've tackled in this.",
            "This program of work, one of the first thing we tried to do was to turn to graph spectral methods and see whether we could generate a.",
            "A family of permutation invariants from the spectrum of the policy, and the idea here is being that the if you just take the the spectrum of the placement set of eigenvalues, then we'll throw throwing away an awful lot of information in terms of graph characterization because you've discarded the information conveyed in the eigenvectors, and So what we've done in this piece of work is to use symmetric polynomials to generate a rich family of permutation invariants from the spectrum.",
            "Matrix of a graph."
        ],
        [
            "Of.",
            "Once once one has this, this kind of characterization to hand, then it's possible to do."
        ],
        [
            "Graph clustering, I'll show you some results on that and."
        ],
        [
            "So graph embedding and then the second thing I want to touch on in some detail is the idea of building a generative model that captures the variations in structure of a family of graphs.",
            "This is work with Andrea Torsello and what we showed is that it's possible to build a generative model from a family of graphs.",
            "Particular particular trees, in this case using description length principles and have a nice result that comes out of this is that there's a link between the edit distance between the trees.",
            "And the description length advantage."
        ],
        [
            "Is that you you get?"
        ],
        [
            "OK, so let's let's stop touch briefly on the first of these technical topics.",
            "This is work on really trying to use algebraic graph theory.",
            "22 can use symmetric polynomials to construct permutation invariants from the spectral matrix of a graph, and then to use these for the purposes of graph characterization."
        ],
        [
            "And graph clustering.",
            "So this is a joint work with."
        ],
        [
            "As I said earlier with Richard Wilson.",
            "What we were trying to do in this work is to is to see how it's possible to construct a spectral representation, a set of characteristics for a graph that doesn't does not require correspondences between nodes from the spectrum of the plastic matrix.",
            "So just quickly revise what the the Laplacian matrix is.",
            "It's the degree matrix by the adjacency matrix where the degree matrix contains the degree.",
            "Other nodes down the leading diagonal and is zero everywhere else, and so if you have the Laplacian matrix, it's a straightforward task to compute its eigen decomposition.",
            "We can decompose it into a matrix of eigenvectors, fi, a diagonal matrix of eigenvectors, eigenvalues, Lambda and.",
            "So what I want to explore here is whether we can use both the.",
            "I can vectors and eigenvalues as a spectral characterization of a graph.",
            "Now I mean the number of attempts to use the eigenvalues of graphs as a spectral characterization.",
            "But as I said earlier, that discards information contained in the in the eigenvectors, and so in some sense that is wasteful.",
            "So if we want to take the Laplacian and decompose it into a spectral representation, straightforward thing to do is to construct the spectral matrix.",
            "The spectral matrix is the eigenvector matrix post multiplied by the square root of the eigenvalue matrix and with the the spectral matrix five we can just write the Laplacian as 5 * 5 transpose."
        ],
        [
            "So here is just a quick a quick review of the properties of Alaskan matrix is known to have positive eigenvalues.",
            "Smallest eigenvalue is zero.",
            "Multiplicity of zero eigenvalues and number of connected components.",
            "Zero eigenvalue is associated with new ones eigenvector and the eigenvector associated with the second smallest eigen.",
            "Sorry, eigenvalue associated the eigenvector associated with the second smallest eigenvalue is the Fiedler vector.",
            "This is the basis of many clustering and great graph by dissection algorithms.",
            "And it's well known literature as I said that the Fiedler vector can be used to perform clustering of nodes by recursive by section of graphs."
        ],
        [
            "So very, very simple characterization of a graph would be to compute the eigenvalue spectrum and so to construct the eigenvector eigenvalue spectrum.",
            "We can pack that into a vector simply by taking the eigenvalue matrix, diagonal eigenvalue matrix, Lambda post, multiplying that by either ones vector that will simply give us a vector of eigenvalues."
        ],
        [
            "And of course, the advantage of using eigenvalues is a characterization of a graph other than the once ordered.",
            "Their invariant to permutations for the policy and.",
            "But of course, if we just simply work with the eigen values as a representation, we've thrown away all the information contained in the eigenvectors of the of the Laplacian and the idea behind this piece of work was to construct a family of permutation invariants from this full spectral matrix."
        ],
        [
            "So the idea here.",
            "Is the following.",
            "We know the according to perturbation analysis, eigenvalues are relatively stable to noise.",
            "I convectors are not as stable to noise and principle can undergo large rotations for small additions of noise and so there is a danger here because by pulling in the eigenvectors we are essentially trying to make use of a source of information that could be very unstable and the addition."
        ],
        [
            "Ignore it, I'll show you this at the end of the day, things things workout quite well.",
            "The idea here is simply too.",
            "A construct the permutation invariants using symmetric polynomials.",
            "So I've written down at the family of symmetric polynomials here over a set of arguments, V1 through to VN, and so the first one is just the sum of the arguments, the second one is the sum of products of pairs of arguments, and then the final one is just the.",
            "The product of the arguments.",
            "So one way of thinking of this is it's like going from a trace through to determinant in terms of in terms of structure."
        ],
        [
            "And there are there alternatives to the symmetric polynomials, so here we have the power symmetric polynomials, where we raise the arguments to do a power rather than taking them in.",
            "In product wise for."
        ],
        [
            "And if you want to start this relationship between these two families of polynomials, then that that's given by the Newton 0 formula."
        ],
        [
            "So what we've done is we have.",
            "We've used these polynomials to construct a permutation invariant.",
            "Mutation invariants from the arguments of the spectral matrix.",
            "And then because these.",
            "Because there's a large dynamic range of the.",
            "The invariants that we get, basically because we're going from a trace.",
            "Some of arguments through to a determinant of product of arguments.",
            "At the end, we want to do some squashing on it, and we use a log rhythmic function to bring them into a reasonable dynamic range.",
            "And then once we once we have sets of permutation invariants, what we can do is we can construct a characteristic vector from them from the characteristic vector.",
            "We can build a data matrix and then go ahead and do things like compute."
        ],
        [
            "Current season means.",
            "No well showing, you just now just applies to.",
            "Uh, undirected and attributed graphs.",
            "We've looked at an extension of this where we try."
        ],
        [
            "To to bring attributes into the picture.",
            "The idea is to go from a real to complex representation of the Laplacian matrix, and here is the construction.",
            "So what we do is we.",
            "If we have an edge AB and we have an attribute on that edge, what we do is we characterize that attribute using a complex number each of DIY.",
            "And then pre multiply that by an edge weight WAB.",
            "So here we can have both weights and attributes on edges and so to put this into a structure which looks like Laplacian, we take minus the complex number WA B * E to the IY.",
            "And then to deal with unary attributes we, in the spirit of the policy and we we stack them along the diagonal.",
            "And so if we have a an attribute XA on the node A, then we add that to the weighted degree of the node to construct the diagonal elements of this extended Laplacian matrix.",
            "So this matrix is now complex, because we've represented the edge.",
            "Attributes using a complex number and then we have the the node attributes encoded on the lead."
        ],
        [
            "Diagonal.",
            "But it's straightforward procedure simply to go ahead and and repeat the construction of the spectral matrix.",
            "From this, this complex version of the policy and H and so rather than decomposing it into a matrix times its transpose, we decompose it into a matrix times its mission conjugate, and we can again apply the symmetric polynomials to the now complex arguments that we get.",
            "So now our polynomials are in terms of real.",
            "And imaginary part."
        ],
        [
            "So with the vectors either from the purely purely structural representation or the attributed representation where we have attributes on nodes and edges, we can then try to do apply some standard techniques from manifold learning theory to construct pattern spaces for sets of graphs.",
            "We've explored a number."
        ],
        [
            "Of possibilities here."
        ],
        [
            "And what I'll do is I'll."
        ],
        [
            "Show you some of the results.",
            "This is taking the pattern vectors, computing Euclidean distance between them and then performing multidimensional scaling on the pattern of distance is what we've done is we've taken a graph here, three different graphs.",
            "We've subjected them to variations in edge structure and what you see is that the under the embedding the three different graphs under variations need structure form 3 distinct clusters.",
            "And so this representation is able to cluster graphs which have the same basic structure, but variations within that into into distinct cluster."
        ],
        [
            "What we've?",
            "We've also done this is looked at the.",
            "The problem of assigning graphs representing shapes to classes, so we've taken 3 different image sequ."
        ],
        [
            "Is here of houses constructed along a triangle?"
        ],
        [
            "Actions and then we've we've extracted the.",
            "In this case, the unattributed spectral invariants."
        ],
        [
            "Trump from the from the resulting graphs, and this is the result of embedding those pattern vectors of spectral permutation invariants into a pattern space, this time using LP and what you see is that the position that each graph embeds at.",
            "We've put a thumbnail corresponding to the image from which the graph is extracted, and you see here that we were able to cluster the graphs.",
            "Into 3 into 3 distinct clusters."
        ],
        [
            "So that's the we've also applied this to trees.",
            "Just to quickly touch on an issue that arises here, one of the problems that arises with trees is that across spectrality.",
            "So in fact, if we try to compute the Laplacian spectrum of a tree, one of the difficulties that trees of different structure are going to give give rise to.",
            "Graphs was essentially the same, the same spectrum.",
            "So many of you were here earlier in the week when David M's talked about his his work on using quantum walks as an alternative to classical walks as a means of characterizing graphs.",
            "One of the interesting features of quantum walks is that they have the property of interference in cancellation and what we've done is we have investigated using the spectrum associated with the quantum walk as an alternative to the looking at the spectrum associated with the depletion.",
            "Well, the classical random walk to see whether we can deal with problems of Co spectrality when try."
        ],
        [
            "To analyze trees.",
            "And so it's been it's it's been known for a long time that almost every tree has a cospectral partner.",
            "Irrespective of whether you're working with your Jason C matrix or the Laplacian matrix.",
            "And So what we've done in this?"
        ],
        [
            "This piece of work is to explore the use of the quantum random walk as much as I said it out earlier allows for the possibility of interference between different paths between nodes and the matrix that characterizes the quantum random walk is a unitary matrix.",
            "I've written it down here, it's just twice the two divided by the degree minus a Delta function that.",
            "Indicates something about edge structure and if we look at the spectrum of this matrix you then it turns out to be related to the spectrum of the original Laplacian matrix.",
            "So in its raw form, it's not going to provide a solution to the problem of cospec."
        ],
        [
            "But what we've discovered is that if we look at the positive support of the power of that.",
            "Random walk matrix for the quantum walk.",
            "Then if we take the third power then due to the interference effects it allows us a way of.",
            "Lifting the problems of Co spectrality, particularly for trees but also for strong."
        ],
        [
            "Break into graphs.",
            "So here's an example.",
            "What I've done here is generated all trees on up to 24 nodes.",
            "The number of trees is given in the first column.",
            "Then what we've done is we've listed a number of cospectral partners that occur using L Anse aux, Laplacian Matrix and so here number of cospectral graphs and then what we've done is we've looked at a number of cospectral partners that occur if we use the positive support.",
            "Of youcubed and the the featured note here is that the number of cospectral partners is reduced by about a factor of 10, so it seems to suggest that quantum walks may provide a better spectral characterization that we could use in the work that I've just talked about about using spectral invariants."
        ],
        [
            "What is really impressive is for strongly regular graphs for strongly regular graphs, whole families can have this pain, same spectrum, so I've done here as I've I've looked at four families of strongly regular graphs.",
            "The I've looked at the difference between the Spectra and embedded them into a 2 dimensional space using multidimensional scaling and you see here the full families correspond to the green, red, black and blue curves, and so all the graphs in those families.",
            "Coincidence in this space then if I use the positive supportive you cubed to try to characterize the graph spectrum of the positive supportive youcubed, then what happens is that the Co spectrality of the three families is lifted and they're seen as as distinct distinct graphs."
        ],
        [
            "OK, so not much time.",
            "Probably OK 5 minutes.",
            "I just want to finish it off very quickly.",
            "What I've been speaking about so far is really been how to do how to cluster graphs in the spectral domain.",
            "Another another issue which is important is how to learn the generative model of."
        ],
        [
            "Of graph structure, this isn't the generative model.",
            "It's in fact Andrea Torsello who did the work with me."
        ],
        [
            "And the idea here is that what we'd like to do."
        ],
        [
            "Is to take a family of graphs and then to construct a generative model from which we can sample all the graphs which are effectively characterized by this by this class prototype.",
            "And the idea in his work is that what we do is we.",
            "Were we concerned with building a generative model over a set of trees?",
            "What we do is we take those trees and we merge them until we form a class prototype.",
            "What that means is that the class each of the sample trees from which we obtain the class prototype could be obtained by editing of the class prototype and then the what we can do is we can associate.",
            "With the nodes of that prototype probability that says how frequently that node was sampled from the original data.",
            "So what we'd like to do is to take a structure like this, and in fact a family of structures like this, and then to try to fit them to a sample of graphs in order to try to extract class structure.",
            "So the idea behind this piece of work has been to try."
        ],
        [
            "To do this using a."
        ],
        [
            "A description, length principle."
        ],
        [
            "I won't dwell on the details."
        ],
        [
            "The idea is really this is what we're going to do is we're going to try to fit to a sample of true data of mixture of these tree unions and then use that as a generative model for."
        ],
        [
            "Set of classes present in the data.",
            "The idea is to do this using a variant of the EM algo."
        ],
        [
            "Rhythm."
        ],
        [
            "What we've done is we have."
        ],
        [
            "Shown that.",
            "If we if we have such a structure that we're trying to fit the data, then it's possible to to to construct a description length.",
            "Cost associated with the fitting of the model to the data, which reflects a number of things.",
            "I'll just quickly take you.",
            "Take you through this.",
            "The idea behind the."
        ],
        [
            "Description Length principle is that.",
            "The cost associated with the fit of a Model 2 today to has two terms associated with it.",
            "The first is effectively the expectation of the longer the posterior probability that's effectively the error in curd in fitting the model to the data and then we have a term which depends on the complexity of the model that we fit to the data.",
            "So this is usually represented using a description length cost.",
            "And so we have a.",
            "Effectively, at a term here, which depends on the parametric number of parameters that we're using to try to describe that the data."
        ],
        [
            "So the idea in this work has been to extend that to.",
            "To the construction of a mixture of trade unions.",
            "And So what we've done is we have constructed a effectively a full step code cost associated with the fitting of the model today to the first one is the negative likelihood of the data given the model.",
            "And this is just determined by the entropy is associated with the node probabilities and then we have four different code costs.",
            "The first is the.",
            "Custom encoding the probabilities on the nodes.",
            "Second is the cost of encoding a mixture model over different trade unions for the data and then the final one is the cost of encoding the true structure that we used to represent each of the classes.",
            "So the."
        ],
        [
            "The this can all be brought together, but the into."
        ],
        [
            "In conclusion, that comes out of it is that if we take this approach, then it turns out there's a relationship between the gain in descripcion length that you make by merging the pair of trees together and the edit distance between trees.",
            "So it effectively gives an information theoretic way of learning the tree edit distance.",
            "The distance between trees.",
            "So I'm."
        ],
        [
            "I know I've shot over that very quickly.",
            "We've been able to use this this method to fit a mixture of tree models to a set of trees representing shop graphs.",
            "And So what we have on the left hand side or the right hand side is simply the set of clusters that you get if you apply pairwise clustering to the standard edit distance between the shop graphs.",
            "What you'll notice is that the different shape class here classes here is strongly confused.",
            "And then if we fit a mixture of tree unions using the description length principle and compute the edit distance is using the relationship between edit distance and description length advantage.",
            "We get to a much more cleanly separated set of shape classes, although their obvious errors.",
            "So for instance here the bunnies and the fish get merged.",
            "This Bunny becomes a tool, but generally speaking the quality of shape classification improves this whole process.",
            "The results shown here can be significantly improved if we add to this representation attributes, and in fact with that we get an ear to near to perfect."
        ],
        [
            "Shape classes so this is really a sort of an overview of things have been trying to do over a long period of time now, and what I've listed on this slide are really where we see the future going.",
            "One topic we'd like to look at in some detail is the link between spectral geometry and graph Spectra, and so in spectral geometry, if you have a manifold, it's possible to characterize the curvature invariants of that manifold using the spectrum of the associated continuous Laplacian matrix.",
            "Be interesting to see whether this this can be extended to graph Spectra to tell us something about the manifolds, the manifold in which the nodes of the graph reside, and then one imperative is to extend the description length principle into the spectral domain so that we can try to use it to control some of the spectral models that we've been developing, so thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what I wanted to do with this talk was really to give an overview of some work that Richard Wilson I have doing been doing now for a number of years.",
                    "label": 0
                },
                {
                    "sent": "I think since 1999 and it sort of fits with the topic of this workshop because we've been concerned with trying to develop algorithms for learning with graph representations, particularly the vision domain.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'll give some motivation and background, but I want to do is to really give you the way in which we were thinking about this.",
                    "label": 0
                },
                {
                    "sent": "This problem along time ago, now probably back in 2000 and to show how that kind of influence what we did, how we identified a set of problems that looked as if they were worth looking at in the vision domain.",
                    "label": 0
                },
                {
                    "sent": "As far as learning with graphs.",
                    "label": 0
                },
                {
                    "sent": "Then I'll for those of you who don't come from computer vision.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a little bit about how graph data comes from images.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk a little bit about some particular technical aspects of this program, which I think might might be interesting to this this audience, the first of the use of his use of spectral invariants to perform the clustering of graphs.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk a little bit about some some work with trees, particularly using minimum description length and how description, links, ideas can be learned, used to learn generative models of tree structures, and then provide some conclusions which are really kind of.",
                    "label": 0
                },
                {
                    "sent": "Up",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future directions for research.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As the motivating part of the talk in computer vision, graph structures are used to abstract image structure, I'll show you in some concrete terms later on how this comes about.",
                    "label": 1
                },
                {
                    "sent": "But the problem in vision is that the data we do with has not begun its life in a graph form.",
                    "label": 0
                },
                {
                    "sent": "It's began its life in the form of an image, and this means that we have to have to use segmentation algorithms to extract graphs or the primitives from which the graphs are built.",
                    "label": 1
                },
                {
                    "sent": "And these are not reliable and so it means that the graph structures that we're dealing with contain considerable NUM.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is virus.",
                    "label": 0
                },
                {
                    "sent": "So as a result, there are both additional and missing nodes due to segmentation error and variations in edge structure analysis.",
                    "label": 1
                },
                {
                    "sent": "Someone has to deal with both as a form of noise but also as a form of general genuine variation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we're trying to deal with graph structures representing images, the image matching recognition can be reduced to a graph isomorphism or cannot be reduced.",
                    "label": 1
                },
                {
                    "sent": "Iser to graph isomorphism, or a subgraph isomorphism problem.",
                    "label": 0
                },
                {
                    "sent": "Instead, we have to develop inexact graph matching algorithms to deal with this kind of data.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there's an enormous amount of literature on on measuring the similarity of graphs and trying to deal with these practical problems in computer vision, and so it's really a historical slide.",
                    "label": 0
                },
                {
                    "sent": "You can retrace the work all the way back to Barrow and Popplestone the group working in Edinburgh, who introduced the idea of an Association graph and a very very early computer vision application invite involving a robot placement cell.",
                    "label": 0
                },
                {
                    "sent": "And they showed how it was possible to match graphs by.",
                    "label": 1
                },
                {
                    "sent": "Searching for the maximum common subgraph of the Association graph.",
                    "label": 0
                },
                {
                    "sent": "Then in the 1980s, the field of syntactic and structural pattern recognition sort of came into being, and I think several people is audiences from that community, particularly from the GBR end of things, and this was really concerned with trying to formalize the problem of matching graph structures and one of the achievements here was to extend the concept of edit distance from strings to graphs.",
                    "label": 1
                },
                {
                    "sent": "No more recently.",
                    "label": 1
                },
                {
                    "sent": "Workers have really come round to trying to develop more principled algorithms for dealing with in exactness.",
                    "label": 0
                },
                {
                    "sent": "In the graph matching problem and in comparing the similarity of graphs and so in the 1990s, and since then I've been a number of attempts to use principle techniques from both probability theory and also from optimization to develop principled graph matching techniques.",
                    "label": 1
                },
                {
                    "sent": "And then most recently of all this kind of whole effort seems to have come full circle and a number of people, particularly bunker and palillo, have have shown how some of the early work on Association graph and the maximum common subgraph could be linked to these more recent ideas from optimization theory.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can we view this work?",
                    "label": 0
                },
                {
                    "sent": "Well, it's shown how to measure the similarity of graphs and can be used to locate inexact matches when there's significant levels of structural error present, it solves the graph matching problem in the sort of scenarios you get in computer vision.",
                    "label": 1
                },
                {
                    "sent": "It also provides a means by which we can try to assess the ways in which structural variation occurs in sets of graphs, possibly from the same family of objects, in a principled way, and so it's really this.",
                    "label": 0
                },
                {
                    "sent": "Topic that I want to focus on here.",
                    "label": 0
                },
                {
                    "sent": "How we can we can use or how we can develop learning algorithms that make use of similarity measures to try to understand how the structure in sets of graphs representing maybe objects of the same class.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Varies.",
                    "label": 0
                },
                {
                    "sent": "So this is really how we looked at the problem.",
                    "label": 0
                },
                {
                    "sent": "Quite a long long time ago, now around 1930 but around 2000.",
                    "label": 0
                },
                {
                    "sent": "Looking at the literature and and considering the.",
                    "label": 0
                },
                {
                    "sent": "Learning problem with graphs in the computer vision domain, there seem to be 4 distinct topics that we could look at in terms of increasing ambitiousness and complexity.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we want to do would be to try to learn class structure.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that abstract shapes using graphs I'll show show you how you might do this in a moment, and then the objects that you have graphs for represent shapes coming from different classes.",
                    "label": 0
                },
                {
                    "sent": "How can how can we use graph methods to learn the class structure of a set of a set of shapes well in order to do this what we need is.",
                    "label": 0
                },
                {
                    "sent": "A distance measure or a vector of graph characteristics.",
                    "label": 1
                },
                {
                    "sent": "If we if we have a vector of graph characteristics, and for instance, we can start to do central clustering and effective pattern recognition techniques are available for doing that.",
                    "label": 0
                },
                {
                    "sent": "But when we when we have only a distance measure, things become more complicated and we have to do pairwise clustering, and in particular if you want to try to compute the similarity between a pair of graphs, what you need to do is to establish correspondence and this is in fact between the nodes of the graphs and this is a computationally.",
                    "label": 0
                },
                {
                    "sent": "Demanding problem.",
                    "label": 1
                },
                {
                    "sent": "So the first problem is to learn class structure.",
                    "label": 0
                },
                {
                    "sent": "You can do this with a vector of graph characteristics.",
                    "label": 0
                },
                {
                    "sent": "I'll give you an example of how you might do this later on the talk.",
                    "label": 0
                },
                {
                    "sent": "Or you can use it with a pattern of distances.",
                    "label": 1
                },
                {
                    "sent": "The next problem you might want to look at is is how to embed a graphs in a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Once you once we have graphs in a low dimensional space, then by traversing that space in some sense we could try to traverse the modes of variation in in the set of graphs that we're looking at, and this is sort of an important step in the direction of trying to construct a generative model for a family of graphs.",
                    "label": 0
                },
                {
                    "sent": "So here we are going to do this.",
                    "label": 0
                },
                {
                    "sent": "We're again going to need correspondence is because we will need the graphs in vector form and unfortunately graphs are not delivered in the form of vectors and that means we have to do some work and the use of intelligence in order to get them into a vectorial form.",
                    "label": 0
                },
                {
                    "sent": "But here graph spectral methods offer offer away which may circumvent the need for correspondences between nodes.",
                    "label": 0
                },
                {
                    "sent": "And again I'll talk a little bit about how we can.",
                    "label": 0
                },
                {
                    "sent": "Extracts spectral characteristics from graphs which allow us to construct low dimensional spaces.",
                    "label": 1
                },
                {
                    "sent": "And then with such representation, then we can use standard statistical and geometric learning methods on the embedded graphs.",
                    "label": 0
                },
                {
                    "sent": "Coming up a level, what we might want to do is as well as trying to learn the class structure and the way in which graphs distribute themselves across a pattern space.",
                    "label": 0
                },
                {
                    "sent": "We may actually want to learn in some detail.",
                    "label": 0
                },
                {
                    "sent": "The modes of structural variation that are present in a family of graphs belonging to the same class.",
                    "label": 1
                },
                {
                    "sent": "So I mean, if you want to think about what this involves, it really involves trying to understand how the connectivity structure of a graph may vary for instances belonging to the same class.",
                    "label": 0
                },
                {
                    "sent": "And then really most ambitious of all.",
                    "label": 0
                },
                {
                    "sent": "If we have a way of trying to characterize the way in which the structural variations take place within a set of graphs belonging to the same class, we may want to send build a generative model which allows us to sample these variations in a probabilistic way.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are the obstacles to setting about these four different activities?",
                    "label": 0
                },
                {
                    "sent": "Well, the first is no, don't do that.",
                    "label": 1
                },
                {
                    "sent": "Restart later, please restart later and the first is that graphs are not vectors and most of what we have available in the pattern recognition literature really works with pattern vectors, and the reason that graphs are not vectors is that.",
                    "label": 0
                },
                {
                    "sent": "There's no Canonical ordering for the nodes in a graph, and if we want to try to begin to try to vectorize graphs, then we need correspondence is to establish the order.",
                    "label": 0
                },
                {
                    "sent": "There are guess ways of getting around this.",
                    "label": 0
                },
                {
                    "sent": "So for instance, if one can effectively constructor vector of characteristics for a graph or somehow to summarize the properties of a graph using that using a histogram.",
                    "label": 0
                },
                {
                    "sent": "Then we have a vector representation, but that's not.",
                    "label": 0
                },
                {
                    "sent": "That's not a representation from which we can construct a generative model.",
                    "label": 0
                },
                {
                    "sent": "It's simply a summary of the properties of the graphs.",
                    "label": 0
                },
                {
                    "sent": "It doesn't actually say anything about the nodes, and the node connectivity structure.",
                    "label": 1
                },
                {
                    "sent": "So then we come to the problem that we have structural variations in graphs and so generally speaking this is expressed in terms of differences of number of nodes and the pattern of connectivity between them.",
                    "label": 0
                },
                {
                    "sent": "And as I pointed out earlier, if we're dealing with graphs from image data because of segmentation error, we sometimes we have not just genuine variation due to the structure of a class.",
                    "label": 1
                },
                {
                    "sent": "But sometimes we have variation due to noise.",
                    "label": 0
                },
                {
                    "sent": "So as a result of all of this, it means that graphs are not easily summarized.",
                    "label": 1
                },
                {
                    "sent": "They don't reside in a vector space, and so this means that character extracting statistical characteristics such as mean covariance is quite difficult.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's here's a sort of slide which summarizes what we're up against here.",
                    "label": 0
                },
                {
                    "sent": "I have a sequence of images.",
                    "label": 0
                },
                {
                    "sent": "As the camera pans around an object, what we've done is we've extracted corner features from these these images and then triangulated the corner features using Delaunay triangulation to construct a graph, and underneath you see the the pattern of variation in the graphs as we pan around the object number of things happen.",
                    "label": 0
                },
                {
                    "sent": "First of all, the connectivity structure of the triangulation changes as points move.",
                    "label": 0
                },
                {
                    "sent": "Closer and further apart from each other, but they're also points that come into view due to occlusion effects, and there are also points that come and go due to the fact that the corner detector which is pulling out the features in these images is sometimes producing false positives and sometimes producing false negatives.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um, what Richard I did was to try to have a crack at solving some of those those problems I listed on the earlier slide, and so one of the things we've done is to look at how we can use how we can compute.",
                    "label": 0
                },
                {
                    "sent": "Permutation invariant graph characteristics from the Laplacian spectrum.",
                    "label": 1
                },
                {
                    "sent": "I'll talk a little bit more about that in detail.",
                    "label": 0
                },
                {
                    "sent": "Later on.",
                    "label": 1
                },
                {
                    "sent": "We focused very much on how edit distance between graphs can be computed in a principled way using both probabilistic methods and also relaxation methods.",
                    "label": 0
                },
                {
                    "sent": "We've looked on at the issue of how we can embed graphs in pattern spaces using properties of the random walk and the looked at geometric characterizations.",
                    "label": 0
                },
                {
                    "sent": "Of graphs and get embedded in fact pattern spaces.",
                    "label": 1
                },
                {
                    "sent": "And then Lastly, we've looked at how how to learn generative models of the structure of sets of graphs using description, length, principles.",
                    "label": 0
                },
                {
                    "sent": "And again I'll give you a little insight into some of the technical details that underpin that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I want to do is talk.",
                    "label": 0
                },
                {
                    "sent": "Is some length about spectral methods and the idea of behind graph spectral methods that use the eigenvalues night vectors of the Laplacian matrix as a as a character characterization of graph structure and there are number of concrete examples of pursuing this particular computer vision literature.",
                    "label": 0
                },
                {
                    "sent": "So for instance, AMA has a very early graph spectral technique for matching graphs that deals with graphs that have the same number of nodes.",
                    "label": 0
                },
                {
                    "sent": "But where there is a variation in edge structure?",
                    "label": 0
                },
                {
                    "sent": "Scott and Higgins and Shapiro and Brady have used graph spectral methods to solve the point correspondence problem in computer vision, and I guess all of you are aware of the work of shared Matic which use an eigenvector method for image segmentation and clustering, and there's also work due to Dickinson and check and fun day where graph spectral methods have been used to index index trees.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I want to do is just to give you a quick idea of of how graphs come about in the computer vision domain because I said right at the beginning of this talk, we're actually struggling against a problem and that is to say our graphs don't begin life as graphs.",
                    "label": 0
                },
                {
                    "sent": "They begin life as as images and so the process of actually getting a graph out of an image is a hard one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is one of the standard graph structures that is used to take image data and converted into a graph.",
                    "label": 0
                },
                {
                    "sent": "It's a it's a Delaunay graph.",
                    "label": 1
                },
                {
                    "sent": "The idea here is that you take feature points from an image.",
                    "label": 0
                },
                {
                    "sent": "These might be things like corners.",
                    "label": 0
                },
                {
                    "sent": "They could be the centroids of image regions.",
                    "label": 0
                },
                {
                    "sent": "They could be points of interest.",
                    "label": 0
                },
                {
                    "sent": "Once you have the points you you grow a seed circle from the point until the seeds collide.",
                    "label": 0
                },
                {
                    "sent": "The collision fronts divide the image plane into polygons and then if you take the region adjacency graph of the polygons, you get the Delaunay triangulation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is exactly how I generated the sequence of graphs that I showed you earlier.",
                    "label": 0
                },
                {
                    "sent": "So that's one way of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing it, another popular representation is there is a shot graph.",
                    "label": 0
                },
                {
                    "sent": "The shock graph is a way of characterizing the structure of a skeleton of a 2D object.",
                    "label": 0
                },
                {
                    "sent": "So you take an image you extract by binary, thresholding the silhouette of the object.",
                    "label": 0
                },
                {
                    "sent": "Then you find the medial axis of that object.",
                    "label": 0
                },
                {
                    "sent": "The medial axis is characterized by the locus of points where there is a bitangent circle to the boundary, and then if you look at the way in which that.",
                    "label": 0
                },
                {
                    "sent": "The radius of that by tangent circle varies as you traverse the skeleton.",
                    "label": 0
                },
                {
                    "sent": "Then you can characterize the skeleton as having four different shop classes.",
                    "label": 0
                },
                {
                    "sent": "So these depend on whether the boundary associated with the branch with skeleton is tapering, constricted, constant, or bulging.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "What I wanted to do in this talk is to focus on two issues pretty quickly.",
                    "label": 0
                },
                {
                    "sent": "I think to give you a flavor of some of the problems that we've tackled in this.",
                    "label": 0
                },
                {
                    "sent": "This program of work, one of the first thing we tried to do was to turn to graph spectral methods and see whether we could generate a.",
                    "label": 0
                },
                {
                    "sent": "A family of permutation invariants from the spectrum of the policy, and the idea here is being that the if you just take the the spectrum of the placement set of eigenvalues, then we'll throw throwing away an awful lot of information in terms of graph characterization because you've discarded the information conveyed in the eigenvectors, and So what we've done in this piece of work is to use symmetric polynomials to generate a rich family of permutation invariants from the spectrum.",
                    "label": 1
                },
                {
                    "sent": "Matrix of a graph.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "Once once one has this, this kind of characterization to hand, then it's possible to do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Graph clustering, I'll show you some results on that and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So graph embedding and then the second thing I want to touch on in some detail is the idea of building a generative model that captures the variations in structure of a family of graphs.",
                    "label": 0
                },
                {
                    "sent": "This is work with Andrea Torsello and what we showed is that it's possible to build a generative model from a family of graphs.",
                    "label": 0
                },
                {
                    "sent": "Particular particular trees, in this case using description length principles and have a nice result that comes out of this is that there's a link between the edit distance between the trees.",
                    "label": 1
                },
                {
                    "sent": "And the description length advantage.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that you you get?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's let's stop touch briefly on the first of these technical topics.",
                    "label": 0
                },
                {
                    "sent": "This is work on really trying to use algebraic graph theory.",
                    "label": 0
                },
                {
                    "sent": "22 can use symmetric polynomials to construct permutation invariants from the spectral matrix of a graph, and then to use these for the purposes of graph characterization.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And graph clustering.",
                    "label": 0
                },
                {
                    "sent": "So this is a joint work with.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I said earlier with Richard Wilson.",
                    "label": 0
                },
                {
                    "sent": "What we were trying to do in this work is to is to see how it's possible to construct a spectral representation, a set of characteristics for a graph that doesn't does not require correspondences between nodes from the spectrum of the plastic matrix.",
                    "label": 0
                },
                {
                    "sent": "So just quickly revise what the the Laplacian matrix is.",
                    "label": 1
                },
                {
                    "sent": "It's the degree matrix by the adjacency matrix where the degree matrix contains the degree.",
                    "label": 0
                },
                {
                    "sent": "Other nodes down the leading diagonal and is zero everywhere else, and so if you have the Laplacian matrix, it's a straightforward task to compute its eigen decomposition.",
                    "label": 0
                },
                {
                    "sent": "We can decompose it into a matrix of eigenvectors, fi, a diagonal matrix of eigenvectors, eigenvalues, Lambda and.",
                    "label": 0
                },
                {
                    "sent": "So what I want to explore here is whether we can use both the.",
                    "label": 0
                },
                {
                    "sent": "I can vectors and eigenvalues as a spectral characterization of a graph.",
                    "label": 0
                },
                {
                    "sent": "Now I mean the number of attempts to use the eigenvalues of graphs as a spectral characterization.",
                    "label": 0
                },
                {
                    "sent": "But as I said earlier, that discards information contained in the in the eigenvectors, and so in some sense that is wasteful.",
                    "label": 0
                },
                {
                    "sent": "So if we want to take the Laplacian and decompose it into a spectral representation, straightforward thing to do is to construct the spectral matrix.",
                    "label": 0
                },
                {
                    "sent": "The spectral matrix is the eigenvector matrix post multiplied by the square root of the eigenvalue matrix and with the the spectral matrix five we can just write the Laplacian as 5 * 5 transpose.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is just a quick a quick review of the properties of Alaskan matrix is known to have positive eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "Smallest eigenvalue is zero.",
                    "label": 0
                },
                {
                    "sent": "Multiplicity of zero eigenvalues and number of connected components.",
                    "label": 1
                },
                {
                    "sent": "Zero eigenvalue is associated with new ones eigenvector and the eigenvector associated with the second smallest eigen.",
                    "label": 1
                },
                {
                    "sent": "Sorry, eigenvalue associated the eigenvector associated with the second smallest eigenvalue is the Fiedler vector.",
                    "label": 0
                },
                {
                    "sent": "This is the basis of many clustering and great graph by dissection algorithms.",
                    "label": 1
                },
                {
                    "sent": "And it's well known literature as I said that the Fiedler vector can be used to perform clustering of nodes by recursive by section of graphs.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So very, very simple characterization of a graph would be to compute the eigenvalue spectrum and so to construct the eigenvector eigenvalue spectrum.",
                    "label": 0
                },
                {
                    "sent": "We can pack that into a vector simply by taking the eigenvalue matrix, diagonal eigenvalue matrix, Lambda post, multiplying that by either ones vector that will simply give us a vector of eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of course, the advantage of using eigenvalues is a characterization of a graph other than the once ordered.",
                    "label": 0
                },
                {
                    "sent": "Their invariant to permutations for the policy and.",
                    "label": 0
                },
                {
                    "sent": "But of course, if we just simply work with the eigen values as a representation, we've thrown away all the information contained in the eigenvectors of the of the Laplacian and the idea behind this piece of work was to construct a family of permutation invariants from this full spectral matrix.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea here.",
                    "label": 0
                },
                {
                    "sent": "Is the following.",
                    "label": 0
                },
                {
                    "sent": "We know the according to perturbation analysis, eigenvalues are relatively stable to noise.",
                    "label": 1
                },
                {
                    "sent": "I convectors are not as stable to noise and principle can undergo large rotations for small additions of noise and so there is a danger here because by pulling in the eigenvectors we are essentially trying to make use of a source of information that could be very unstable and the addition.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ignore it, I'll show you this at the end of the day, things things workout quite well.",
                    "label": 0
                },
                {
                    "sent": "The idea here is simply too.",
                    "label": 0
                },
                {
                    "sent": "A construct the permutation invariants using symmetric polynomials.",
                    "label": 0
                },
                {
                    "sent": "So I've written down at the family of symmetric polynomials here over a set of arguments, V1 through to VN, and so the first one is just the sum of the arguments, the second one is the sum of products of pairs of arguments, and then the final one is just the.",
                    "label": 0
                },
                {
                    "sent": "The product of the arguments.",
                    "label": 0
                },
                {
                    "sent": "So one way of thinking of this is it's like going from a trace through to determinant in terms of in terms of structure.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there are there alternatives to the symmetric polynomials, so here we have the power symmetric polynomials, where we raise the arguments to do a power rather than taking them in.",
                    "label": 0
                },
                {
                    "sent": "In product wise for.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you want to start this relationship between these two families of polynomials, then that that's given by the Newton 0 formula.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we've done is we have.",
                    "label": 0
                },
                {
                    "sent": "We've used these polynomials to construct a permutation invariant.",
                    "label": 1
                },
                {
                    "sent": "Mutation invariants from the arguments of the spectral matrix.",
                    "label": 1
                },
                {
                    "sent": "And then because these.",
                    "label": 0
                },
                {
                    "sent": "Because there's a large dynamic range of the.",
                    "label": 0
                },
                {
                    "sent": "The invariants that we get, basically because we're going from a trace.",
                    "label": 0
                },
                {
                    "sent": "Some of arguments through to a determinant of product of arguments.",
                    "label": 0
                },
                {
                    "sent": "At the end, we want to do some squashing on it, and we use a log rhythmic function to bring them into a reasonable dynamic range.",
                    "label": 0
                },
                {
                    "sent": "And then once we once we have sets of permutation invariants, what we can do is we can construct a characteristic vector from them from the characteristic vector.",
                    "label": 0
                },
                {
                    "sent": "We can build a data matrix and then go ahead and do things like compute.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Current season means.",
                    "label": 0
                },
                {
                    "sent": "No well showing, you just now just applies to.",
                    "label": 0
                },
                {
                    "sent": "Uh, undirected and attributed graphs.",
                    "label": 1
                },
                {
                    "sent": "We've looked at an extension of this where we try.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To to bring attributes into the picture.",
                    "label": 0
                },
                {
                    "sent": "The idea is to go from a real to complex representation of the Laplacian matrix, and here is the construction.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we.",
                    "label": 0
                },
                {
                    "sent": "If we have an edge AB and we have an attribute on that edge, what we do is we characterize that attribute using a complex number each of DIY.",
                    "label": 0
                },
                {
                    "sent": "And then pre multiply that by an edge weight WAB.",
                    "label": 0
                },
                {
                    "sent": "So here we can have both weights and attributes on edges and so to put this into a structure which looks like Laplacian, we take minus the complex number WA B * E to the IY.",
                    "label": 0
                },
                {
                    "sent": "And then to deal with unary attributes we, in the spirit of the policy and we we stack them along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And so if we have a an attribute XA on the node A, then we add that to the weighted degree of the node to construct the diagonal elements of this extended Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "So this matrix is now complex, because we've represented the edge.",
                    "label": 0
                },
                {
                    "sent": "Attributes using a complex number and then we have the the node attributes encoded on the lead.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Diagonal.",
                    "label": 0
                },
                {
                    "sent": "But it's straightforward procedure simply to go ahead and and repeat the construction of the spectral matrix.",
                    "label": 0
                },
                {
                    "sent": "From this, this complex version of the policy and H and so rather than decomposing it into a matrix times its transpose, we decompose it into a matrix times its mission conjugate, and we can again apply the symmetric polynomials to the now complex arguments that we get.",
                    "label": 0
                },
                {
                    "sent": "So now our polynomials are in terms of real.",
                    "label": 0
                },
                {
                    "sent": "And imaginary part.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with the vectors either from the purely purely structural representation or the attributed representation where we have attributes on nodes and edges, we can then try to do apply some standard techniques from manifold learning theory to construct pattern spaces for sets of graphs.",
                    "label": 0
                },
                {
                    "sent": "We've explored a number.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of possibilities here.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'll do is I'll.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you some of the results.",
                    "label": 0
                },
                {
                    "sent": "This is taking the pattern vectors, computing Euclidean distance between them and then performing multidimensional scaling on the pattern of distance is what we've done is we've taken a graph here, three different graphs.",
                    "label": 0
                },
                {
                    "sent": "We've subjected them to variations in edge structure and what you see is that the under the embedding the three different graphs under variations need structure form 3 distinct clusters.",
                    "label": 0
                },
                {
                    "sent": "And so this representation is able to cluster graphs which have the same basic structure, but variations within that into into distinct cluster.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we've?",
                    "label": 0
                },
                {
                    "sent": "We've also done this is looked at the.",
                    "label": 0
                },
                {
                    "sent": "The problem of assigning graphs representing shapes to classes, so we've taken 3 different image sequ.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is here of houses constructed along a triangle?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actions and then we've we've extracted the.",
                    "label": 0
                },
                {
                    "sent": "In this case, the unattributed spectral invariants.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trump from the from the resulting graphs, and this is the result of embedding those pattern vectors of spectral permutation invariants into a pattern space, this time using LP and what you see is that the position that each graph embeds at.",
                    "label": 0
                },
                {
                    "sent": "We've put a thumbnail corresponding to the image from which the graph is extracted, and you see here that we were able to cluster the graphs.",
                    "label": 0
                },
                {
                    "sent": "Into 3 into 3 distinct clusters.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the we've also applied this to trees.",
                    "label": 0
                },
                {
                    "sent": "Just to quickly touch on an issue that arises here, one of the problems that arises with trees is that across spectrality.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if we try to compute the Laplacian spectrum of a tree, one of the difficulties that trees of different structure are going to give give rise to.",
                    "label": 0
                },
                {
                    "sent": "Graphs was essentially the same, the same spectrum.",
                    "label": 0
                },
                {
                    "sent": "So many of you were here earlier in the week when David M's talked about his his work on using quantum walks as an alternative to classical walks as a means of characterizing graphs.",
                    "label": 0
                },
                {
                    "sent": "One of the interesting features of quantum walks is that they have the property of interference in cancellation and what we've done is we have investigated using the spectrum associated with the quantum walk as an alternative to the looking at the spectrum associated with the depletion.",
                    "label": 0
                },
                {
                    "sent": "Well, the classical random walk to see whether we can deal with problems of Co spectrality when try.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To analyze trees.",
                    "label": 0
                },
                {
                    "sent": "And so it's been it's it's been known for a long time that almost every tree has a cospectral partner.",
                    "label": 0
                },
                {
                    "sent": "Irrespective of whether you're working with your Jason C matrix or the Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "And So what we've done in this?",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This piece of work is to explore the use of the quantum random walk as much as I said it out earlier allows for the possibility of interference between different paths between nodes and the matrix that characterizes the quantum random walk is a unitary matrix.",
                    "label": 0
                },
                {
                    "sent": "I've written it down here, it's just twice the two divided by the degree minus a Delta function that.",
                    "label": 0
                },
                {
                    "sent": "Indicates something about edge structure and if we look at the spectrum of this matrix you then it turns out to be related to the spectrum of the original Laplacian matrix.",
                    "label": 1
                },
                {
                    "sent": "So in its raw form, it's not going to provide a solution to the problem of cospec.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what we've discovered is that if we look at the positive support of the power of that.",
                    "label": 0
                },
                {
                    "sent": "Random walk matrix for the quantum walk.",
                    "label": 0
                },
                {
                    "sent": "Then if we take the third power then due to the interference effects it allows us a way of.",
                    "label": 0
                },
                {
                    "sent": "Lifting the problems of Co spectrality, particularly for trees but also for strong.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Break into graphs.",
                    "label": 0
                },
                {
                    "sent": "So here's an example.",
                    "label": 0
                },
                {
                    "sent": "What I've done here is generated all trees on up to 24 nodes.",
                    "label": 0
                },
                {
                    "sent": "The number of trees is given in the first column.",
                    "label": 0
                },
                {
                    "sent": "Then what we've done is we've listed a number of cospectral partners that occur using L Anse aux, Laplacian Matrix and so here number of cospectral graphs and then what we've done is we've looked at a number of cospectral partners that occur if we use the positive support.",
                    "label": 0
                },
                {
                    "sent": "Of youcubed and the the featured note here is that the number of cospectral partners is reduced by about a factor of 10, so it seems to suggest that quantum walks may provide a better spectral characterization that we could use in the work that I've just talked about about using spectral invariants.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is really impressive is for strongly regular graphs for strongly regular graphs, whole families can have this pain, same spectrum, so I've done here as I've I've looked at four families of strongly regular graphs.",
                    "label": 0
                },
                {
                    "sent": "The I've looked at the difference between the Spectra and embedded them into a 2 dimensional space using multidimensional scaling and you see here the full families correspond to the green, red, black and blue curves, and so all the graphs in those families.",
                    "label": 0
                },
                {
                    "sent": "Coincidence in this space then if I use the positive supportive you cubed to try to characterize the graph spectrum of the positive supportive youcubed, then what happens is that the Co spectrality of the three families is lifted and they're seen as as distinct distinct graphs.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so not much time.",
                    "label": 0
                },
                {
                    "sent": "Probably OK 5 minutes.",
                    "label": 0
                },
                {
                    "sent": "I just want to finish it off very quickly.",
                    "label": 0
                },
                {
                    "sent": "What I've been speaking about so far is really been how to do how to cluster graphs in the spectral domain.",
                    "label": 0
                },
                {
                    "sent": "Another another issue which is important is how to learn the generative model of.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of graph structure, this isn't the generative model.",
                    "label": 0
                },
                {
                    "sent": "It's in fact Andrea Torsello who did the work with me.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the idea here is that what we'd like to do.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is to take a family of graphs and then to construct a generative model from which we can sample all the graphs which are effectively characterized by this by this class prototype.",
                    "label": 0
                },
                {
                    "sent": "And the idea in his work is that what we do is we.",
                    "label": 0
                },
                {
                    "sent": "Were we concerned with building a generative model over a set of trees?",
                    "label": 0
                },
                {
                    "sent": "What we do is we take those trees and we merge them until we form a class prototype.",
                    "label": 0
                },
                {
                    "sent": "What that means is that the class each of the sample trees from which we obtain the class prototype could be obtained by editing of the class prototype and then the what we can do is we can associate.",
                    "label": 0
                },
                {
                    "sent": "With the nodes of that prototype probability that says how frequently that node was sampled from the original data.",
                    "label": 0
                },
                {
                    "sent": "So what we'd like to do is to take a structure like this, and in fact a family of structures like this, and then to try to fit them to a sample of graphs in order to try to extract class structure.",
                    "label": 0
                },
                {
                    "sent": "So the idea behind this piece of work has been to try.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do this using a.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A description, length principle.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I won't dwell on the details.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea is really this is what we're going to do is we're going to try to fit to a sample of true data of mixture of these tree unions and then use that as a generative model for.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set of classes present in the data.",
                    "label": 0
                },
                {
                    "sent": "The idea is to do this using a variant of the EM algo.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rhythm.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we've done is we have.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shown that.",
                    "label": 0
                },
                {
                    "sent": "If we if we have such a structure that we're trying to fit the data, then it's possible to to to construct a description length.",
                    "label": 0
                },
                {
                    "sent": "Cost associated with the fitting of the model to the data, which reflects a number of things.",
                    "label": 0
                },
                {
                    "sent": "I'll just quickly take you.",
                    "label": 0
                },
                {
                    "sent": "Take you through this.",
                    "label": 0
                },
                {
                    "sent": "The idea behind the.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Description Length principle is that.",
                    "label": 0
                },
                {
                    "sent": "The cost associated with the fit of a Model 2 today to has two terms associated with it.",
                    "label": 0
                },
                {
                    "sent": "The first is effectively the expectation of the longer the posterior probability that's effectively the error in curd in fitting the model to the data and then we have a term which depends on the complexity of the model that we fit to the data.",
                    "label": 0
                },
                {
                    "sent": "So this is usually represented using a description length cost.",
                    "label": 0
                },
                {
                    "sent": "And so we have a.",
                    "label": 0
                },
                {
                    "sent": "Effectively, at a term here, which depends on the parametric number of parameters that we're using to try to describe that the data.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea in this work has been to extend that to.",
                    "label": 0
                },
                {
                    "sent": "To the construction of a mixture of trade unions.",
                    "label": 0
                },
                {
                    "sent": "And So what we've done is we have constructed a effectively a full step code cost associated with the fitting of the model today to the first one is the negative likelihood of the data given the model.",
                    "label": 0
                },
                {
                    "sent": "And this is just determined by the entropy is associated with the node probabilities and then we have four different code costs.",
                    "label": 0
                },
                {
                    "sent": "The first is the.",
                    "label": 0
                },
                {
                    "sent": "Custom encoding the probabilities on the nodes.",
                    "label": 0
                },
                {
                    "sent": "Second is the cost of encoding a mixture model over different trade unions for the data and then the final one is the cost of encoding the true structure that we used to represent each of the classes.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The this can all be brought together, but the into.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In conclusion, that comes out of it is that if we take this approach, then it turns out there's a relationship between the gain in descripcion length that you make by merging the pair of trees together and the edit distance between trees.",
                    "label": 0
                },
                {
                    "sent": "So it effectively gives an information theoretic way of learning the tree edit distance.",
                    "label": 0
                },
                {
                    "sent": "The distance between trees.",
                    "label": 0
                },
                {
                    "sent": "So I'm.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I know I've shot over that very quickly.",
                    "label": 0
                },
                {
                    "sent": "We've been able to use this this method to fit a mixture of tree models to a set of trees representing shop graphs.",
                    "label": 0
                },
                {
                    "sent": "And So what we have on the left hand side or the right hand side is simply the set of clusters that you get if you apply pairwise clustering to the standard edit distance between the shop graphs.",
                    "label": 0
                },
                {
                    "sent": "What you'll notice is that the different shape class here classes here is strongly confused.",
                    "label": 0
                },
                {
                    "sent": "And then if we fit a mixture of tree unions using the description length principle and compute the edit distance is using the relationship between edit distance and description length advantage.",
                    "label": 1
                },
                {
                    "sent": "We get to a much more cleanly separated set of shape classes, although their obvious errors.",
                    "label": 0
                },
                {
                    "sent": "So for instance here the bunnies and the fish get merged.",
                    "label": 0
                },
                {
                    "sent": "This Bunny becomes a tool, but generally speaking the quality of shape classification improves this whole process.",
                    "label": 0
                },
                {
                    "sent": "The results shown here can be significantly improved if we add to this representation attributes, and in fact with that we get an ear to near to perfect.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shape classes so this is really a sort of an overview of things have been trying to do over a long period of time now, and what I've listed on this slide are really where we see the future going.",
                    "label": 0
                },
                {
                    "sent": "One topic we'd like to look at in some detail is the link between spectral geometry and graph Spectra, and so in spectral geometry, if you have a manifold, it's possible to characterize the curvature invariants of that manifold using the spectrum of the associated continuous Laplacian matrix.",
                    "label": 0
                },
                {
                    "sent": "Be interesting to see whether this this can be extended to graph Spectra to tell us something about the manifolds, the manifold in which the nodes of the graph reside, and then one imperative is to extend the description length principle into the spectral domain so that we can try to use it to control some of the spectral models that we've been developing, so thanks.",
                    "label": 0
                }
            ]
        }
    }
}