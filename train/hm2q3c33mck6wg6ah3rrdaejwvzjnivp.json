{
    "id": "hm2q3c33mck6wg6ah3rrdaejwvzjnivp",
    "title": "Multiple Kernel Learning for Efficient Conformal Predictions",
    "info": {
        "author": [
            "Shayok Chakraborty, Arizona State University"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods",
            "Top->Computer Science->Machine Learning->Computational Learning Theory"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2010_chakraborty_mkl/",
    "segmentation": [
        [
            "So I'm sure you from the CU lab at Arizona State University and I'm here to present the paper multip."
        ],
        [
            "I'm gonna learning for efficient conformal predictions.",
            "I quoted this with my lab between Ethan, my advisor, Doctor Punch.",
            "The theory of conformal predictions was proposed by walk, Shuffle and Gamerman, and it's based on the theories of algorithmic randomness, transductive inference and hypothesis testing.",
            "The main beauty of the framework is that the results are calibrated in an online setting, which means that the number of errors made by the system is always upper bounded by a limit set by the user.",
            "So this is depicted in this diagram and you can see at every level of confidence the number of misclassifications made by the system is always less than the threshold set by the user."
        ],
        [
            "To ensure this calibration property, the system may sometimes produce multiple predictions as noted by the Red line in this diagrams.",
            "The number of multiple predictions may depend on the number of the classifier that we're using and also on the parameters that we set for a particular classifier.",
            "Now, this may limit the practical applicability of the system.",
            "For example, in case of a binary classification problem, if the system produces too many multiple predictions, it's actually throwing useless output.",
            "So given the fact that the output of the system is calibrated, can we reduce the number of multiple predictions and increase the efficiency of the framework?",
            "That is the main focus of this work?"
        ],
        [
            "We consider the K nearest neighbors classifiers in our work.",
            "So to reduce the number of multiple predictions, we need to ensure that points from different classes are far apart and points within the same class have minimum variance within them.",
            "We post this as an optimization problem where we have the usual double transpose double term to maximize the margin between different classes.",
            "And also we have the within class scatter matrix term to minimize the variance within each class.",
            "It turns out that using a simple."
        ],
        [
            "Substitution of variables introduces to the standard SVM problem on the transform data.",
            "So we solve for W hat from this formulation together with the weights of the kernels that we're combining."
        ],
        [
            "Nearly the extended level set method was used to solve the problem in this work."
        ],
        [
            "Here is a sample result.",
            "The X axis shows the confidence level set by the user and the Y axis denotes the number of multiple predictions we have compared our approach with three other techniques, simple KNN, kernel, KNN, an LDA plus Ken and as we see that the number of multiple predictions is the least in case of the proposed framework.",
            "So the proposed approach succeeds in reducing the number of multiple predictions while maintaining the calibration property.",
            "If you want more details, please stop by our poster.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm sure you from the CU lab at Arizona State University and I'm here to present the paper multip.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm gonna learning for efficient conformal predictions.",
                    "label": 0
                },
                {
                    "sent": "I quoted this with my lab between Ethan, my advisor, Doctor Punch.",
                    "label": 0
                },
                {
                    "sent": "The theory of conformal predictions was proposed by walk, Shuffle and Gamerman, and it's based on the theories of algorithmic randomness, transductive inference and hypothesis testing.",
                    "label": 1
                },
                {
                    "sent": "The main beauty of the framework is that the results are calibrated in an online setting, which means that the number of errors made by the system is always upper bounded by a limit set by the user.",
                    "label": 0
                },
                {
                    "sent": "So this is depicted in this diagram and you can see at every level of confidence the number of misclassifications made by the system is always less than the threshold set by the user.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To ensure this calibration property, the system may sometimes produce multiple predictions as noted by the Red line in this diagrams.",
                    "label": 0
                },
                {
                    "sent": "The number of multiple predictions may depend on the number of the classifier that we're using and also on the parameters that we set for a particular classifier.",
                    "label": 0
                },
                {
                    "sent": "Now, this may limit the practical applicability of the system.",
                    "label": 0
                },
                {
                    "sent": "For example, in case of a binary classification problem, if the system produces too many multiple predictions, it's actually throwing useless output.",
                    "label": 0
                },
                {
                    "sent": "So given the fact that the output of the system is calibrated, can we reduce the number of multiple predictions and increase the efficiency of the framework?",
                    "label": 0
                },
                {
                    "sent": "That is the main focus of this work?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We consider the K nearest neighbors classifiers in our work.",
                    "label": 0
                },
                {
                    "sent": "So to reduce the number of multiple predictions, we need to ensure that points from different classes are far apart and points within the same class have minimum variance within them.",
                    "label": 0
                },
                {
                    "sent": "We post this as an optimization problem where we have the usual double transpose double term to maximize the margin between different classes.",
                    "label": 1
                },
                {
                    "sent": "And also we have the within class scatter matrix term to minimize the variance within each class.",
                    "label": 1
                },
                {
                    "sent": "It turns out that using a simple.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Substitution of variables introduces to the standard SVM problem on the transform data.",
                    "label": 0
                },
                {
                    "sent": "So we solve for W hat from this formulation together with the weights of the kernels that we're combining.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nearly the extended level set method was used to solve the problem in this work.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is a sample result.",
                    "label": 0
                },
                {
                    "sent": "The X axis shows the confidence level set by the user and the Y axis denotes the number of multiple predictions we have compared our approach with three other techniques, simple KNN, kernel, KNN, an LDA plus Ken and as we see that the number of multiple predictions is the least in case of the proposed framework.",
                    "label": 1
                },
                {
                    "sent": "So the proposed approach succeeds in reducing the number of multiple predictions while maintaining the calibration property.",
                    "label": 0
                },
                {
                    "sent": "If you want more details, please stop by our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}