{
    "id": "s2y7bkhg2mqmoq3rotr3tatof2j5wipj",
    "title": "Restricted Eigen Condition for Heavy Tailed Designs",
    "info": {
        "author": [
            "Arindam Banerjee, Department of Computer Science and Engineering, University of Minnesota"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_banerjee_tailed_designs/",
    "segmentation": [
        [
            "So think of X as."
        ],
        [
            "Random matrix which is N cross B. Ann is small piece large so you can think of this as a random projection matrix.",
            "Frumpy dimensions to end dimensions.",
            "I'm calling it a random design matrix because we encounter this problem in high dimensional statistics problems like Lasso where your number of measurements Ann is much smaller than the ambient dimension TP and this is sort of a core thing that you have to encounter if you're working with high dimensional statistical problems like regression in less so.",
            "So you can assume every row is independent.",
            "They can be sub caution.",
            "Really thin tailed subexponential's a little fatter or heavy tailed.",
            "The open problem is about the heavy tail case.",
            "So what we want to bound is in order to get sort of these lasso type things work, we need a uniform lower bound on this random quadratic form 1 by NXQ square.",
            "So I've written it another form for you belonging to some set a, typically for sparse regression type problems and structured problems.",
            "This a typically becomes a spherical cap, so they use lie on the unit sphere, right?",
            "You're taking a subset of the unit sphere, a special case of this is when a is the entire.",
            "Unit sphere, you're talking about the smallest eigenvalue of that random matrix X transpose X.",
            "So there are classical results.",
            "Asymptotic results on this.",
            "We are focused on the non asymptotic case when N is finite and smaller than P. And the tools that people have been using over the last few years is this this concept called Gaussian width.",
            "It's a very powerful measure of of the complexity of a set.",
            "So if you take any set a belonging to our to the P, the caution with is defined as the expectation Supreme.",
            "Overall you in that set you project it to a Gaussian isotropic vector.",
            "So one way to think of this measure is that every inner product of U&G is like a Gaussian random variable.",
            "So it's a Supreme over a Gaussian process in the expectation of that.",
            "This comes up in many different analysis, right?",
            "So this is a very central structure in convex geometry within widely studied over 20 years.",
            "So what kinds of results are possible or already known, right?"
        ],
        [
            "So there is a class of results called restricted isometry property which gives 2 sided bounds.",
            "So these are bounds of the form that says one by NXQ squared will lie between one minus epsilon and one plus epsilon.",
            "As long as your number of measurements are samples is larger than some function of PNF silent, so one can show for example what happened in thin tail case and these are results people can show already.",
            "Is that if if that sample complexity is square of the caution with divided by epsilon.",
            "Then you get this kind of a 2 sided bound, which is a very general version of restrictors.",
            "Isometry property for submission designs.",
            "This is actually a general case of what is known as the Johnson Lindenstrauss lemma.",
            "The result originally came about in 1984 time frame.",
            "It was.",
            "The original result is I think for Gaussian design matrices.",
            "In this case this result goes through for general submission design matrices and it can be expressed in terms of the Gaussian width for the special case of Johnson Lindenstrauss lemma you are dealing with.",
            "You know, K elements in P dimensions and you're projecting into N dimensions, so the caution without that set is just squared of log.",
            "Can you get back the standard classical Johnson Lindenstrauss lemma?",
            "So these are all known?",
            "These are generalizations of Johnson Lindenstrauss lemma.",
            "What is of more interest recently for high dimensional statistics?"
        ],
        [
            "Are 1 sided bounds on lower uniform lower bounds of this type that in FEMA over 1 by end of that quadratic form is greater than one minus epsilon.",
            "This is needed so that in high dimensional statistiques your objective function becomes strongly convex on unrestricted set like a so you are not going to get strong convexity on the entire R to the P. You just need strong convexity unrestricted set.",
            "In this case it serves physical cap and that is sufficient for you to show all kinds of recovery results.",
            "So this is central to that kind of analysis that.",
            "We see in the literature 1 sided results.",
            "I mean this is one of the earlier results.",
            "Gardens inequality, 1985 timeframe.",
            "It's based on a caution comparison principle.",
            "It's a very powerful technique, doesn't generalize to beyond cautions.",
            "There other techniques like Telegram generic chaining which can be used for more general results.",
            "But these things exist for thin tail distributions.",
            "The open problem is about heavy tailed distribution, so you're dealing with a random matrix.",
            "Where you have."
        ],
        [
            "Independent rose"
        ],
        [
            "On every row is heavy tail and has a specific definition of heavy tail, meaning that you have lost moments right?",
            "All moments don't exist.",
            "In fact, the specific type of heavy tailed behavior that we're interested in is this thing called the small ball property.",
            "So if you don't know small ball property, do a Google search will find plenty of material.",
            "This characterization says that if you choose a direction V on you project, the distribution along that along that direction, any V, the infamous revolvy, they'll be some probability mass left, so the entirety mask is definitely not concentrated on.",
            "Orthogonal subspace of the direction V, you don't.",
            "You're not making any assumptions about existence of moments, so you know things like, oh she distributions will satisfy this.",
            "So this is really, I mean, the bare minimum you need in order to do this.",
            "So we are going to assume your design matrix.",
            "You random matrix is this heavy tailed right and the type of results we're looking for.",
            "Is a lower bound a uniform lower bound of the form infimum?",
            "Overall U of this quadratic form.",
            "Has to be greater than C 1 -- C two Times Square of the caution with by N. These equivalent results exist for thin tail distributions and I'm happy to discuss more details offline and this result exists for heavy tailed distributions.",
            "For special cases of sets, when a is the entire sphere, so you're looking at the minimum eigenvalue for a heavy tail matrix, that result actually exist.",
            "This is based on a pack Bayesian argument, so you need a some sort of uniform tool.",
            "You have last moment, so you can't really go with.",
            "Bornstein having type arguments.",
            "I think that's the problem and in the two pager I have more references and people who have been working on it and making progress in the last three four years.",
            "But we still don't have this result.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So think of X as.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Random matrix which is N cross B. Ann is small piece large so you can think of this as a random projection matrix.",
                    "label": 0
                },
                {
                    "sent": "Frumpy dimensions to end dimensions.",
                    "label": 0
                },
                {
                    "sent": "I'm calling it a random design matrix because we encounter this problem in high dimensional statistics problems like Lasso where your number of measurements Ann is much smaller than the ambient dimension TP and this is sort of a core thing that you have to encounter if you're working with high dimensional statistical problems like regression in less so.",
                    "label": 1
                },
                {
                    "sent": "So you can assume every row is independent.",
                    "label": 1
                },
                {
                    "sent": "They can be sub caution.",
                    "label": 1
                },
                {
                    "sent": "Really thin tailed subexponential's a little fatter or heavy tailed.",
                    "label": 1
                },
                {
                    "sent": "The open problem is about the heavy tail case.",
                    "label": 0
                },
                {
                    "sent": "So what we want to bound is in order to get sort of these lasso type things work, we need a uniform lower bound on this random quadratic form 1 by NXQ square.",
                    "label": 0
                },
                {
                    "sent": "So I've written it another form for you belonging to some set a, typically for sparse regression type problems and structured problems.",
                    "label": 0
                },
                {
                    "sent": "This a typically becomes a spherical cap, so they use lie on the unit sphere, right?",
                    "label": 1
                },
                {
                    "sent": "You're taking a subset of the unit sphere, a special case of this is when a is the entire.",
                    "label": 0
                },
                {
                    "sent": "Unit sphere, you're talking about the smallest eigenvalue of that random matrix X transpose X.",
                    "label": 0
                },
                {
                    "sent": "So there are classical results.",
                    "label": 0
                },
                {
                    "sent": "Asymptotic results on this.",
                    "label": 1
                },
                {
                    "sent": "We are focused on the non asymptotic case when N is finite and smaller than P. And the tools that people have been using over the last few years is this this concept called Gaussian width.",
                    "label": 0
                },
                {
                    "sent": "It's a very powerful measure of of the complexity of a set.",
                    "label": 0
                },
                {
                    "sent": "So if you take any set a belonging to our to the P, the caution with is defined as the expectation Supreme.",
                    "label": 0
                },
                {
                    "sent": "Overall you in that set you project it to a Gaussian isotropic vector.",
                    "label": 0
                },
                {
                    "sent": "So one way to think of this measure is that every inner product of U&G is like a Gaussian random variable.",
                    "label": 0
                },
                {
                    "sent": "So it's a Supreme over a Gaussian process in the expectation of that.",
                    "label": 0
                },
                {
                    "sent": "This comes up in many different analysis, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a very central structure in convex geometry within widely studied over 20 years.",
                    "label": 0
                },
                {
                    "sent": "So what kinds of results are possible or already known, right?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a class of results called restricted isometry property which gives 2 sided bounds.",
                    "label": 1
                },
                {
                    "sent": "So these are bounds of the form that says one by NXQ squared will lie between one minus epsilon and one plus epsilon.",
                    "label": 0
                },
                {
                    "sent": "As long as your number of measurements are samples is larger than some function of PNF silent, so one can show for example what happened in thin tail case and these are results people can show already.",
                    "label": 0
                },
                {
                    "sent": "Is that if if that sample complexity is square of the caution with divided by epsilon.",
                    "label": 0
                },
                {
                    "sent": "Then you get this kind of a 2 sided bound, which is a very general version of restrictors.",
                    "label": 0
                },
                {
                    "sent": "Isometry property for submission designs.",
                    "label": 0
                },
                {
                    "sent": "This is actually a general case of what is known as the Johnson Lindenstrauss lemma.",
                    "label": 0
                },
                {
                    "sent": "The result originally came about in 1984 time frame.",
                    "label": 0
                },
                {
                    "sent": "It was.",
                    "label": 0
                },
                {
                    "sent": "The original result is I think for Gaussian design matrices.",
                    "label": 0
                },
                {
                    "sent": "In this case this result goes through for general submission design matrices and it can be expressed in terms of the Gaussian width for the special case of Johnson Lindenstrauss lemma you are dealing with.",
                    "label": 1
                },
                {
                    "sent": "You know, K elements in P dimensions and you're projecting into N dimensions, so the caution without that set is just squared of log.",
                    "label": 0
                },
                {
                    "sent": "Can you get back the standard classical Johnson Lindenstrauss lemma?",
                    "label": 0
                },
                {
                    "sent": "So these are all known?",
                    "label": 0
                },
                {
                    "sent": "These are generalizations of Johnson Lindenstrauss lemma.",
                    "label": 0
                },
                {
                    "sent": "What is of more interest recently for high dimensional statistics?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are 1 sided bounds on lower uniform lower bounds of this type that in FEMA over 1 by end of that quadratic form is greater than one minus epsilon.",
                    "label": 1
                },
                {
                    "sent": "This is needed so that in high dimensional statistiques your objective function becomes strongly convex on unrestricted set like a so you are not going to get strong convexity on the entire R to the P. You just need strong convexity unrestricted set.",
                    "label": 0
                },
                {
                    "sent": "In this case it serves physical cap and that is sufficient for you to show all kinds of recovery results.",
                    "label": 0
                },
                {
                    "sent": "So this is central to that kind of analysis that.",
                    "label": 0
                },
                {
                    "sent": "We see in the literature 1 sided results.",
                    "label": 0
                },
                {
                    "sent": "I mean this is one of the earlier results.",
                    "label": 0
                },
                {
                    "sent": "Gardens inequality, 1985 timeframe.",
                    "label": 0
                },
                {
                    "sent": "It's based on a caution comparison principle.",
                    "label": 0
                },
                {
                    "sent": "It's a very powerful technique, doesn't generalize to beyond cautions.",
                    "label": 0
                },
                {
                    "sent": "There other techniques like Telegram generic chaining which can be used for more general results.",
                    "label": 0
                },
                {
                    "sent": "But these things exist for thin tail distributions.",
                    "label": 0
                },
                {
                    "sent": "The open problem is about heavy tailed distribution, so you're dealing with a random matrix.",
                    "label": 0
                },
                {
                    "sent": "Where you have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Independent rose",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On every row is heavy tail and has a specific definition of heavy tail, meaning that you have lost moments right?",
                    "label": 0
                },
                {
                    "sent": "All moments don't exist.",
                    "label": 0
                },
                {
                    "sent": "In fact, the specific type of heavy tailed behavior that we're interested in is this thing called the small ball property.",
                    "label": 0
                },
                {
                    "sent": "So if you don't know small ball property, do a Google search will find plenty of material.",
                    "label": 0
                },
                {
                    "sent": "This characterization says that if you choose a direction V on you project, the distribution along that along that direction, any V, the infamous revolvy, they'll be some probability mass left, so the entirety mask is definitely not concentrated on.",
                    "label": 0
                },
                {
                    "sent": "Orthogonal subspace of the direction V, you don't.",
                    "label": 0
                },
                {
                    "sent": "You're not making any assumptions about existence of moments, so you know things like, oh she distributions will satisfy this.",
                    "label": 0
                },
                {
                    "sent": "So this is really, I mean, the bare minimum you need in order to do this.",
                    "label": 0
                },
                {
                    "sent": "So we are going to assume your design matrix.",
                    "label": 1
                },
                {
                    "sent": "You random matrix is this heavy tailed right and the type of results we're looking for.",
                    "label": 0
                },
                {
                    "sent": "Is a lower bound a uniform lower bound of the form infimum?",
                    "label": 0
                },
                {
                    "sent": "Overall U of this quadratic form.",
                    "label": 1
                },
                {
                    "sent": "Has to be greater than C 1 -- C two Times Square of the caution with by N. These equivalent results exist for thin tail distributions and I'm happy to discuss more details offline and this result exists for heavy tailed distributions.",
                    "label": 1
                },
                {
                    "sent": "For special cases of sets, when a is the entire sphere, so you're looking at the minimum eigenvalue for a heavy tail matrix, that result actually exist.",
                    "label": 0
                },
                {
                    "sent": "This is based on a pack Bayesian argument, so you need a some sort of uniform tool.",
                    "label": 0
                },
                {
                    "sent": "You have last moment, so you can't really go with.",
                    "label": 0
                },
                {
                    "sent": "Bornstein having type arguments.",
                    "label": 0
                },
                {
                    "sent": "I think that's the problem and in the two pager I have more references and people who have been working on it and making progress in the last three four years.",
                    "label": 0
                },
                {
                    "sent": "But we still don't have this result.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}