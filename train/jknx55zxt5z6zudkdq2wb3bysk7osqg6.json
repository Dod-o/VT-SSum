{
    "id": "jknx55zxt5z6zudkdq2wb3bysk7osqg6",
    "title": "Surrogate Regret Bounds for Proper Losses",
    "info": {
        "author": [
            "Mark Reid, Research School of Information Sciences and Engineering, Australian National University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_reid_srbp/",
    "segmentation": [
        [
            "So this is some joint work I've done with Bob Williamson, his upper back there, and although the title surrogate regret bounds for proper losses, what I hope to achieve in this talk is to convince you that these proper losses, which I'll induce will introduce in this talk, are kind of natural.",
            "An really nice.",
            "Class of losses to study and they have some really elegant results around them."
        ],
        [
            "So the structure of the talk is.",
            "I'll talk briefly about the aims, will introduce what proper losses are, and then I'll introduce the two key concepts that underlie all of the results.",
            "In this talk.",
            "And that's the Fisher consistency that makes these losses proper.",
            "And essentially Taylors theorem from you all know from your undergraduate days with these two concepts, we're able to derive a lot of interesting existing and some new results.",
            "In particular, there are three representations of losses and regrets for this class of losses, which are interesting in their own right.",
            "These are savages theorem.",
            "The fact that we've read as a Bregman divergent, and the weighted integral representation of these proper losses, and we can use these results to get this surrogate regret bounds of the title, and also talk a little bit about convexity of these losses.",
            "So Bob and I have been looking at sort of unifying machine learning as part of a larger project, and this is sort of the result of."
        ],
        [
            "That work?"
        ],
        [
            "And we're trying part of this project is to get a better understanding of the class of loss functions, and so we've been doing this by looking at how risk is analyzed in other areas like economics.",
            "We're trying to find the sort of essential.",
            "The the essence of of these things and really trying to build strong foundations.",
            "And then we're trying to understand them and then generalize them.",
            "So in particular this approach."
        ],
        [
            "Has led to some very simple proofs of existing results plus these sorry get regret bounds and these knew surrogate regret bounds.",
            "Interesting because unlike the existing ones by Bartlett and others, our bounds work for the non symmetric surrogate losses and they also give bounds on not only 01 loss but also cost weighted losses.",
            "So the two concepts that underlie all."
        ],
        [
            "With these results.",
            "The notion of Fisher consistency and the integral form of Taylor's theorem, so Fisher consistency just says that.",
            "Your loss has to be minimized by the true probability.",
            "I'll.",
            "I'll formalize that in a little bit, but that's sort of key idea that if you have a loss and you estimate the actual probability as your estimate, then you will minimize that loss is kind of the right type of loss for probability estimation and Taylor's theorem.",
            "You may or may not have seen this form of it, it just expands a function out linearly and then captures all of the other terms in this integral part at the end.",
            "So I'll briefly talk about."
        ],
        [
            "What I mean by a loss here, a loss in general is just some way of assigning a penalty to a prediction relative to a label."
        ],
        [
            "And.",
            "In machine learning, a lot of the losses of studies margin losses, so you have some function Phi that assigns the penalties of the product of the label and the prediction.",
            "So if the label on the prediction of the same sign, then you generally don't incur a penalty, but if they have a different sign, you do so.",
            "These losses are always inherently symmetric in the sense that I have up there that the loss on one label has to be if you.",
            "Make a prediction on one label, then predicting the negative of that on the other label has to have the same penalty.",
            "So we've been studying a general class of losses which subsume margin losses."
        ],
        [
            "We call these the composite losses where you have some L superscript PSI, which essentially takes prediction, converts it into a probability estimate, and then assigns a loss according to that probability estimate."
        ],
        [
            "So for the rest of the talk, I'm sort of going to focus on the the loss on 01 rather than general off.",
            "Weather prediction is over the reals, so so when I talk about loss and the rest of this talk, I'll be talking about a function where the label is zero or one and the prediction is in the interval 01 and it's to be interpreted as the probability that you see y = 1.",
            "K. Right, so usually once you have these losses, what you want."
        ],
        [
            "Do is minimize the risk with respect to some distribution P, and that's just the expected loss over XY pairs drawn from this distribution.",
            "Now you can factor this risk as follows, where you can determine side we can refer to."
        ],
        [
            "This the pointwise riskware.",
            "For a particular X you draw wise according to the true conditional probability and then you just take the expectation over all the X is so this inner risk or pointwise risk is sort of the key objective this talk.",
            "So the pointwise risk is just as I said before, it's this in part of this decomposition where you're interested in.",
            "The expected loss for at a particular X value and.",
            "This."
        ],
        [
            "The pointwise Bayes risk is what you would expect.",
            "It's where you minimize the pointwise risk by considering all possible Easter hats in 01, that should be not are so.",
            "The point was Bayes risk is what's the best we can do at a particular value for the conditional probability eater.",
            "OK, so this point was based risk for proper losses.",
            "At least this point was based.",
            "Risk is kind of like the central object to study because it captures everything about the loss that you're interested."
        ],
        [
            "So Fisher consistency.",
            "Now that we have a definition for Bayes risk, Fisher consistency can be defined using this notation.",
            "Here will use L underbar for Eater to represent the minimal possible risk.",
            "When the true probability of y = 1 is ITA, so this is just.",
            "The.",
            "So the L bar is the Bayes risk and a loss is Fisher consistent if?",
            "OK, so there's a mistake inside.",
            "Sorry this here should be an eater, not Anita Hut.",
            "So basically, if you estimate the true probability, then you've minimized the risk.",
            "This is kind of natural.",
            "Form of a loss for probability estimation.",
            "It would be strange not to have this property right."
        ],
        [
            "So we just say loss is proper.",
            "If it's Fisher can."
        ],
        [
            "And these losses studied in the economics literature, and they usually called proper scoring rules but scoring rules, as we've seen with the last poker used to refer to other things.",
            "So I'll refer to these as proper losses.",
            "So one really nice thing about proper losses that you can compute the Bayes risk really easily, right?",
            "Because if you know that the risk is minimized when I estimate the actual value of Y to, I can just plug.",
            "It had equals to either into the whatever loss you have an you can compute the Bayes risk as a closed form.",
            "So for example with square loss this is the form of the risk and you just substitute teacher had equal Zita and you get the Bayes risk is 1 -- Y to either so.",
            "These are quite nice to study analytically."
        ],
        [
            "He's in a bunch of examples of things that are proper offers, so 01 loss is a proper loss.",
            "Log loss, square loss and the exponential or boosting loss are all examples of proper losses and also slightly more unusually, you can have asymmetric losses and cause."
        ],
        [
            "Weighted losses these families as well.",
            "But the proper losses don't capture every loss that is possible.",
            "So for example, this absolute loss and the hinge loss are not in this class of losses, so."
        ],
        [
            "If we imagine the space of all losses, this is a kind of schematic of the relationship between all these things so.",
            "The symmetrical margin losses contain 01 log loss square loss and hinge loss and the proper losses have some of them, but not all of them.",
            "But there are things that are proper that aren't symmetric as well."
        ],
        [
            "Right, so the other key concept that look at in this talk is Taylors Theorem Ann.",
            "Just quickly, this is the traditional statement of the integral form of Taylor's theorem.",
            "An this is a kind of alternate form that's a bit easy and bit easier to work with.",
            "And all that's happened here is that rather than having the limits of the integration based on these two points, we can expand it out to an interval Ann.",
            "Just have a function that does the right thing.",
            "And put the.",
            "It's zero outside these terms an at some correct for the sign."
        ],
        [
            "K. So using this notion of Fisher consistency and Taylor's theorem, I'll now quickly show you how we can derive a lot of."
        ],
        [
            "Really nice existing results, so there's this result from the economics and forecasting literature by Leo Savage.",
            "38 years old now.",
            "And it's a really nice characterization of of proper losses.",
            "And so he says that a loss is proper if and only if its Bayes risk is concave and satisfies this this property here.",
            "So it just says that you can write the loss as a function of the Bayes risk and its derivative.",
            "So this is kind of a structural result about proper losses."
        ],
        [
            "And I'll just sketch the proof really quickly.",
            "It's quite straightforward to go from properness to concave.",
            "You just note that.",
            "That the Bayes risk is Justin Infime of a whole lot of lines so it's concave.",
            "And there's then you can compute the derivative and submit back into that form up there and show that it holds going the other way.",
            "If you give me some concave function Lambda, I can do a Taylor expansion of it and."
        ],
        [
            "By this assumption here this part will be the loss and we can see that the loss is equal to this.",
            "Minus this, and it's clear that because Lambda is concave, its negative second derivative is always positive, and so this term will be minimized when either had data and so the loss will be proper.",
            "So essentially, Taylor's theorem and the fact that concave functions have a negative second derivative is what drives this result.",
            "So I'm."
        ],
        [
            "I like to think about this and a graphically and essentially what Savage Theorem says each.",
            "If you have a loss, which is the solid line for L = 1 and the dashed line for L = y = 0, you can imagine them on the side of this cube here, and for any particular eater you've just kind of taken a linear interpolation of these two losses at the end points and what Savage Theorem says is that.",
            "If you do this interpolation, the minimizer will give the minimizer.",
            "Every eater will give you this concave curves along the diagonal.",
            "So there is quite a structured class of losses I guess is the point of this."
        ],
        [
            "Right, so once we have this structural result, we get a lot of nice results for free, so Bregman."
        ],
        [
            "Vergence you may have you probably all know this definition is just if you have a convex function, you can sort of get this general notion of distance between two points called Bregman divergences, and it turns out that the regret of a particular loss is exactly a Bregman vergence when you're dealing with proper losses, so this is quite easy to see because the definition of the regret of a loss is just the loss at a particular estimate minus the Bayes risk.",
            "And since with this Savage result here we we can substitute this into the formula for regret and notice that it matches up exactly with the definition of Bregman divergent, which is far is equal to minus L bar.",
            "I'll come."
        ],
        [
            "Back to this result.",
            "In a bit later.",
            "Maybe have a really nice result.",
            "Is this integral representation of losses and.",
            "This basically is kind of like a Fourier decomposition for losses, in that you can, rather than thinking of losses defined pointwise.",
            "You can think of them as a sum of.",
            "How similar functions, and in particular, if I have a proper loss, I can write it as the integral of a whole lot of cost sensitive losses which are combined using this weight function here.",
            "So these cost sensitive losses.",
            "Just assign a positive or negative, wait for false or false positives, false negatives and the weight function is just the negative second derivative of the base risk.",
            "So graphic."
        ],
        [
            "Again, this integral representation you can think of it as a way of decomposing any proper loss, which we can represent.",
            "As these two functions as a weighted sum of all of these simple cost weighted losses here.",
            "So this is once again a nice structure results in the case of log loss.",
            "This weight function is 1 /, 1 -- C * C, and you can recompose.",
            "You can get this function if you kind of sum up all these cost sensitive losses against that weight function.",
            "This makes analysis really nice because you can sort of see where this loss stresses that you get certain things right, so around zero and one the log loss will give you massive penalties if you get your estimate wrong, whereas in the middle."
        ],
        [
            "It's not so important.",
            "So here is some weight functions for some sort of well known losses and not so well known losses.",
            "Square loss at a flat weight function an actually.",
            "Just noting this is what gives you essentially Johns probing reduction.",
            "This weight functions squared loss is flat and so you get his result and it's quite easy to compute the weight functions.",
            "Any proper loss that you have.",
            "I won't spend too much time on this."
        ],
        [
            "Essentially, the proof of this integral representation, a lot of people have proved that I found that if you just start with Taylor's theorem and the Savage result, it just sort of falls out.",
            "It's really quite straightforward.",
            "Yes."
        ],
        [
            "Come over that.",
            "Um?",
            "Because the expectation is linear, you get a couple of nice corollary's for free.",
            "The pointwise risk can also be written in this form, where the.",
            "Cox weighted pointwise risk is as a nice closed form.",
            "An pointwise regret also has a nice closed form here."
        ],
        [
            "So just quickly say how we can use these structural results to get."
        ],
        [
            "A couple of interesting results.",
            "So, um.",
            "The the surrogate regret bounds basically say they ask the answer the question if I know something about the regret of one of these simple losses at a particular point, what can I say about the regret of some more complicated loss and this surrogate regret bound essentially says gives you a closed form for a bound on the complex.",
            "Regret in terms of these similar regrets here."
        ],
        [
            "The proof is really quite simple, and this is actually a simpler proof from what's in the paper because I discovered this the other day when I was writing up these slides.",
            "Only if your cost weighted regret is some fixed value, you can just substitute it into the full regret you notice that.",
            "Rick, because regrets are Bregman divergences as you move closer to as these two points move closer together.",
            "The overall value will be minimized and so you just pushed the estimate as close as you can to this value.",
            "Given these constraints and you get the result after, break it down into two parts, but it's really quite straightforward.",
            "Um?",
            "So these structural properties of these losses give you really elegant ways of proving these existing results and extending them.",
            "OK."
        ],
        [
            "So a corollary that you get is for the case of 01 loss where C not equals 1/2.",
            "It simplifies dramatically, and here's a sort of worked example where you can show that the 01 loss is bounded by the square root of.",
            "Of these, the regret of the square loss."
        ],
        [
            "So just comparing the old results and our new results so the existing surrogate regret bounds by Bartlett and others worked for this subset of the symmetric loss is called the classification calibrated losses, whereas our result holds for this family of proper losses, so they're not directly compareable and they don't just assume each other, but they're similar in spirit.",
            "I'm going to have to stop so I won't talk about."
        ],
        [
            "Convexity, I'll just finish up by saying that class are proper losses.",
            "Really interesting there.",
            "The right type of loss and."
        ],
        [
            "Ability estimation and."
        ],
        [
            "I hope that I've can."
        ],
        [
            "It's doing this talk."
        ],
        [
            "I have a lot of very nice structural prop."
        ],
        [
            "These, and they're really nice to analyze using very."
        ],
        [
            "Very elementary results like Taylor's theorem, so we hope to use some of these results in answering questions such as are there principled ways of choosing good surrogate losses and trying to come up with a better analysis, better characterization of the convexity of composite losses?",
            "Thank you."
        ],
        [
            "Questions."
        ],
        [
            "Yep.",
            "So you mean say for non binary prediction?",
            "Yeah so.",
            "This.",
            "The results do extend quite nicely this notion of properness.",
            "Have been studied in depth by Nicholas Lambert in the HTML Conference on Electronic Commerce.",
            "Yeah, really nice paper last year, which talks about essentially proper losses for multiclass problems and not very beautiful characterization of their structure.",
            "So yeah, there's there's nice extensions from the binary case."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is some joint work I've done with Bob Williamson, his upper back there, and although the title surrogate regret bounds for proper losses, what I hope to achieve in this talk is to convince you that these proper losses, which I'll induce will introduce in this talk, are kind of natural.",
                    "label": 1
                },
                {
                    "sent": "An really nice.",
                    "label": 0
                },
                {
                    "sent": "Class of losses to study and they have some really elegant results around them.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the structure of the talk is.",
                    "label": 0
                },
                {
                    "sent": "I'll talk briefly about the aims, will introduce what proper losses are, and then I'll introduce the two key concepts that underlie all of the results.",
                    "label": 0
                },
                {
                    "sent": "In this talk.",
                    "label": 0
                },
                {
                    "sent": "And that's the Fisher consistency that makes these losses proper.",
                    "label": 1
                },
                {
                    "sent": "And essentially Taylors theorem from you all know from your undergraduate days with these two concepts, we're able to derive a lot of interesting existing and some new results.",
                    "label": 0
                },
                {
                    "sent": "In particular, there are three representations of losses and regrets for this class of losses, which are interesting in their own right.",
                    "label": 0
                },
                {
                    "sent": "These are savages theorem.",
                    "label": 0
                },
                {
                    "sent": "The fact that we've read as a Bregman divergent, and the weighted integral representation of these proper losses, and we can use these results to get this surrogate regret bounds of the title, and also talk a little bit about convexity of these losses.",
                    "label": 1
                },
                {
                    "sent": "So Bob and I have been looking at sort of unifying machine learning as part of a larger project, and this is sort of the result of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That work?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we're trying part of this project is to get a better understanding of the class of loss functions, and so we've been doing this by looking at how risk is analyzed in other areas like economics.",
                    "label": 0
                },
                {
                    "sent": "We're trying to find the sort of essential.",
                    "label": 0
                },
                {
                    "sent": "The the essence of of these things and really trying to build strong foundations.",
                    "label": 0
                },
                {
                    "sent": "And then we're trying to understand them and then generalize them.",
                    "label": 0
                },
                {
                    "sent": "So in particular this approach.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has led to some very simple proofs of existing results plus these sorry get regret bounds and these knew surrogate regret bounds.",
                    "label": 1
                },
                {
                    "sent": "Interesting because unlike the existing ones by Bartlett and others, our bounds work for the non symmetric surrogate losses and they also give bounds on not only 01 loss but also cost weighted losses.",
                    "label": 0
                },
                {
                    "sent": "So the two concepts that underlie all.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With these results.",
                    "label": 0
                },
                {
                    "sent": "The notion of Fisher consistency and the integral form of Taylor's theorem, so Fisher consistency just says that.",
                    "label": 1
                },
                {
                    "sent": "Your loss has to be minimized by the true probability.",
                    "label": 0
                },
                {
                    "sent": "I'll.",
                    "label": 0
                },
                {
                    "sent": "I'll formalize that in a little bit, but that's sort of key idea that if you have a loss and you estimate the actual probability as your estimate, then you will minimize that loss is kind of the right type of loss for probability estimation and Taylor's theorem.",
                    "label": 1
                },
                {
                    "sent": "You may or may not have seen this form of it, it just expands a function out linearly and then captures all of the other terms in this integral part at the end.",
                    "label": 0
                },
                {
                    "sent": "So I'll briefly talk about.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I mean by a loss here, a loss in general is just some way of assigning a penalty to a prediction relative to a label.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In machine learning, a lot of the losses of studies margin losses, so you have some function Phi that assigns the penalties of the product of the label and the prediction.",
                    "label": 1
                },
                {
                    "sent": "So if the label on the prediction of the same sign, then you generally don't incur a penalty, but if they have a different sign, you do so.",
                    "label": 0
                },
                {
                    "sent": "These losses are always inherently symmetric in the sense that I have up there that the loss on one label has to be if you.",
                    "label": 0
                },
                {
                    "sent": "Make a prediction on one label, then predicting the negative of that on the other label has to have the same penalty.",
                    "label": 0
                },
                {
                    "sent": "So we've been studying a general class of losses which subsume margin losses.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We call these the composite losses where you have some L superscript PSI, which essentially takes prediction, converts it into a probability estimate, and then assigns a loss according to that probability estimate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the rest of the talk, I'm sort of going to focus on the the loss on 01 rather than general off.",
                    "label": 1
                },
                {
                    "sent": "Weather prediction is over the reals, so so when I talk about loss and the rest of this talk, I'll be talking about a function where the label is zero or one and the prediction is in the interval 01 and it's to be interpreted as the probability that you see y = 1.",
                    "label": 1
                },
                {
                    "sent": "K. Right, so usually once you have these losses, what you want.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is minimize the risk with respect to some distribution P, and that's just the expected loss over XY pairs drawn from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Now you can factor this risk as follows, where you can determine side we can refer to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This the pointwise riskware.",
                    "label": 0
                },
                {
                    "sent": "For a particular X you draw wise according to the true conditional probability and then you just take the expectation over all the X is so this inner risk or pointwise risk is sort of the key objective this talk.",
                    "label": 0
                },
                {
                    "sent": "So the pointwise risk is just as I said before, it's this in part of this decomposition where you're interested in.",
                    "label": 1
                },
                {
                    "sent": "The expected loss for at a particular X value and.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The pointwise Bayes risk is what you would expect.",
                    "label": 1
                },
                {
                    "sent": "It's where you minimize the pointwise risk by considering all possible Easter hats in 01, that should be not are so.",
                    "label": 0
                },
                {
                    "sent": "The point was Bayes risk is what's the best we can do at a particular value for the conditional probability eater.",
                    "label": 0
                },
                {
                    "sent": "OK, so this point was based risk for proper losses.",
                    "label": 0
                },
                {
                    "sent": "At least this point was based.",
                    "label": 0
                },
                {
                    "sent": "Risk is kind of like the central object to study because it captures everything about the loss that you're interested.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Fisher consistency.",
                    "label": 0
                },
                {
                    "sent": "Now that we have a definition for Bayes risk, Fisher consistency can be defined using this notation.",
                    "label": 0
                },
                {
                    "sent": "Here will use L underbar for Eater to represent the minimal possible risk.",
                    "label": 0
                },
                {
                    "sent": "When the true probability of y = 1 is ITA, so this is just.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "So the L bar is the Bayes risk and a loss is Fisher consistent if?",
                    "label": 1
                },
                {
                    "sent": "OK, so there's a mistake inside.",
                    "label": 0
                },
                {
                    "sent": "Sorry this here should be an eater, not Anita Hut.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you estimate the true probability, then you've minimized the risk.",
                    "label": 0
                },
                {
                    "sent": "This is kind of natural.",
                    "label": 0
                },
                {
                    "sent": "Form of a loss for probability estimation.",
                    "label": 0
                },
                {
                    "sent": "It would be strange not to have this property right.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we just say loss is proper.",
                    "label": 0
                },
                {
                    "sent": "If it's Fisher can.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these losses studied in the economics literature, and they usually called proper scoring rules but scoring rules, as we've seen with the last poker used to refer to other things.",
                    "label": 0
                },
                {
                    "sent": "So I'll refer to these as proper losses.",
                    "label": 0
                },
                {
                    "sent": "So one really nice thing about proper losses that you can compute the Bayes risk really easily, right?",
                    "label": 1
                },
                {
                    "sent": "Because if you know that the risk is minimized when I estimate the actual value of Y to, I can just plug.",
                    "label": 0
                },
                {
                    "sent": "It had equals to either into the whatever loss you have an you can compute the Bayes risk as a closed form.",
                    "label": 0
                },
                {
                    "sent": "So for example with square loss this is the form of the risk and you just substitute teacher had equal Zita and you get the Bayes risk is 1 -- Y to either so.",
                    "label": 1
                },
                {
                    "sent": "These are quite nice to study analytically.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He's in a bunch of examples of things that are proper offers, so 01 loss is a proper loss.",
                    "label": 0
                },
                {
                    "sent": "Log loss, square loss and the exponential or boosting loss are all examples of proper losses and also slightly more unusually, you can have asymmetric losses and cause.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Weighted losses these families as well.",
                    "label": 0
                },
                {
                    "sent": "But the proper losses don't capture every loss that is possible.",
                    "label": 0
                },
                {
                    "sent": "So for example, this absolute loss and the hinge loss are not in this class of losses, so.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we imagine the space of all losses, this is a kind of schematic of the relationship between all these things so.",
                    "label": 0
                },
                {
                    "sent": "The symmetrical margin losses contain 01 log loss square loss and hinge loss and the proper losses have some of them, but not all of them.",
                    "label": 0
                },
                {
                    "sent": "But there are things that are proper that aren't symmetric as well.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so the other key concept that look at in this talk is Taylors Theorem Ann.",
                    "label": 0
                },
                {
                    "sent": "Just quickly, this is the traditional statement of the integral form of Taylor's theorem.",
                    "label": 0
                },
                {
                    "sent": "An this is a kind of alternate form that's a bit easy and bit easier to work with.",
                    "label": 0
                },
                {
                    "sent": "And all that's happened here is that rather than having the limits of the integration based on these two points, we can expand it out to an interval Ann.",
                    "label": 0
                },
                {
                    "sent": "Just have a function that does the right thing.",
                    "label": 0
                },
                {
                    "sent": "And put the.",
                    "label": 0
                },
                {
                    "sent": "It's zero outside these terms an at some correct for the sign.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. So using this notion of Fisher consistency and Taylor's theorem, I'll now quickly show you how we can derive a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really nice existing results, so there's this result from the economics and forecasting literature by Leo Savage.",
                    "label": 0
                },
                {
                    "sent": "38 years old now.",
                    "label": 0
                },
                {
                    "sent": "And it's a really nice characterization of of proper losses.",
                    "label": 0
                },
                {
                    "sent": "And so he says that a loss is proper if and only if its Bayes risk is concave and satisfies this this property here.",
                    "label": 1
                },
                {
                    "sent": "So it just says that you can write the loss as a function of the Bayes risk and its derivative.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of a structural result about proper losses.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll just sketch the proof really quickly.",
                    "label": 0
                },
                {
                    "sent": "It's quite straightforward to go from properness to concave.",
                    "label": 0
                },
                {
                    "sent": "You just note that.",
                    "label": 0
                },
                {
                    "sent": "That the Bayes risk is Justin Infime of a whole lot of lines so it's concave.",
                    "label": 1
                },
                {
                    "sent": "And there's then you can compute the derivative and submit back into that form up there and show that it holds going the other way.",
                    "label": 0
                },
                {
                    "sent": "If you give me some concave function Lambda, I can do a Taylor expansion of it and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By this assumption here this part will be the loss and we can see that the loss is equal to this.",
                    "label": 0
                },
                {
                    "sent": "Minus this, and it's clear that because Lambda is concave, its negative second derivative is always positive, and so this term will be minimized when either had data and so the loss will be proper.",
                    "label": 0
                },
                {
                    "sent": "So essentially, Taylor's theorem and the fact that concave functions have a negative second derivative is what drives this result.",
                    "label": 0
                },
                {
                    "sent": "So I'm.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I like to think about this and a graphically and essentially what Savage Theorem says each.",
                    "label": 0
                },
                {
                    "sent": "If you have a loss, which is the solid line for L = 1 and the dashed line for L = y = 0, you can imagine them on the side of this cube here, and for any particular eater you've just kind of taken a linear interpolation of these two losses at the end points and what Savage Theorem says is that.",
                    "label": 0
                },
                {
                    "sent": "If you do this interpolation, the minimizer will give the minimizer.",
                    "label": 0
                },
                {
                    "sent": "Every eater will give you this concave curves along the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So there is quite a structured class of losses I guess is the point of this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so once we have this structural result, we get a lot of nice results for free, so Bregman.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Vergence you may have you probably all know this definition is just if you have a convex function, you can sort of get this general notion of distance between two points called Bregman divergences, and it turns out that the regret of a particular loss is exactly a Bregman vergence when you're dealing with proper losses, so this is quite easy to see because the definition of the regret of a loss is just the loss at a particular estimate minus the Bayes risk.",
                    "label": 0
                },
                {
                    "sent": "And since with this Savage result here we we can substitute this into the formula for regret and notice that it matches up exactly with the definition of Bregman divergent, which is far is equal to minus L bar.",
                    "label": 0
                },
                {
                    "sent": "I'll come.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Back to this result.",
                    "label": 0
                },
                {
                    "sent": "In a bit later.",
                    "label": 0
                },
                {
                    "sent": "Maybe have a really nice result.",
                    "label": 0
                },
                {
                    "sent": "Is this integral representation of losses and.",
                    "label": 1
                },
                {
                    "sent": "This basically is kind of like a Fourier decomposition for losses, in that you can, rather than thinking of losses defined pointwise.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as a sum of.",
                    "label": 0
                },
                {
                    "sent": "How similar functions, and in particular, if I have a proper loss, I can write it as the integral of a whole lot of cost sensitive losses which are combined using this weight function here.",
                    "label": 1
                },
                {
                    "sent": "So these cost sensitive losses.",
                    "label": 0
                },
                {
                    "sent": "Just assign a positive or negative, wait for false or false positives, false negatives and the weight function is just the negative second derivative of the base risk.",
                    "label": 0
                },
                {
                    "sent": "So graphic.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, this integral representation you can think of it as a way of decomposing any proper loss, which we can represent.",
                    "label": 1
                },
                {
                    "sent": "As these two functions as a weighted sum of all of these simple cost weighted losses here.",
                    "label": 0
                },
                {
                    "sent": "So this is once again a nice structure results in the case of log loss.",
                    "label": 0
                },
                {
                    "sent": "This weight function is 1 /, 1 -- C * C, and you can recompose.",
                    "label": 1
                },
                {
                    "sent": "You can get this function if you kind of sum up all these cost sensitive losses against that weight function.",
                    "label": 0
                },
                {
                    "sent": "This makes analysis really nice because you can sort of see where this loss stresses that you get certain things right, so around zero and one the log loss will give you massive penalties if you get your estimate wrong, whereas in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not so important.",
                    "label": 0
                },
                {
                    "sent": "So here is some weight functions for some sort of well known losses and not so well known losses.",
                    "label": 0
                },
                {
                    "sent": "Square loss at a flat weight function an actually.",
                    "label": 0
                },
                {
                    "sent": "Just noting this is what gives you essentially Johns probing reduction.",
                    "label": 0
                },
                {
                    "sent": "This weight functions squared loss is flat and so you get his result and it's quite easy to compute the weight functions.",
                    "label": 0
                },
                {
                    "sent": "Any proper loss that you have.",
                    "label": 0
                },
                {
                    "sent": "I won't spend too much time on this.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Essentially, the proof of this integral representation, a lot of people have proved that I found that if you just start with Taylor's theorem and the Savage result, it just sort of falls out.",
                    "label": 0
                },
                {
                    "sent": "It's really quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Come over that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Because the expectation is linear, you get a couple of nice corollary's for free.",
                    "label": 0
                },
                {
                    "sent": "The pointwise risk can also be written in this form, where the.",
                    "label": 0
                },
                {
                    "sent": "Cox weighted pointwise risk is as a nice closed form.",
                    "label": 1
                },
                {
                    "sent": "An pointwise regret also has a nice closed form here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just quickly say how we can use these structural results to get.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A couple of interesting results.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "The the surrogate regret bounds basically say they ask the answer the question if I know something about the regret of one of these simple losses at a particular point, what can I say about the regret of some more complicated loss and this surrogate regret bound essentially says gives you a closed form for a bound on the complex.",
                    "label": 1
                },
                {
                    "sent": "Regret in terms of these similar regrets here.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The proof is really quite simple, and this is actually a simpler proof from what's in the paper because I discovered this the other day when I was writing up these slides.",
                    "label": 0
                },
                {
                    "sent": "Only if your cost weighted regret is some fixed value, you can just substitute it into the full regret you notice that.",
                    "label": 0
                },
                {
                    "sent": "Rick, because regrets are Bregman divergences as you move closer to as these two points move closer together.",
                    "label": 0
                },
                {
                    "sent": "The overall value will be minimized and so you just pushed the estimate as close as you can to this value.",
                    "label": 0
                },
                {
                    "sent": "Given these constraints and you get the result after, break it down into two parts, but it's really quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So these structural properties of these losses give you really elegant ways of proving these existing results and extending them.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a corollary that you get is for the case of 01 loss where C not equals 1/2.",
                    "label": 0
                },
                {
                    "sent": "It simplifies dramatically, and here's a sort of worked example where you can show that the 01 loss is bounded by the square root of.",
                    "label": 0
                },
                {
                    "sent": "Of these, the regret of the square loss.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just comparing the old results and our new results so the existing surrogate regret bounds by Bartlett and others worked for this subset of the symmetric loss is called the classification calibrated losses, whereas our result holds for this family of proper losses, so they're not directly compareable and they don't just assume each other, but they're similar in spirit.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have to stop so I won't talk about.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Convexity, I'll just finish up by saying that class are proper losses.",
                    "label": 1
                },
                {
                    "sent": "Really interesting there.",
                    "label": 0
                },
                {
                    "sent": "The right type of loss and.",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ability estimation and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I hope that I've can.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's doing this talk.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a lot of very nice structural prop.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These, and they're really nice to analyze using very.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very elementary results like Taylor's theorem, so we hope to use some of these results in answering questions such as are there principled ways of choosing good surrogate losses and trying to come up with a better analysis, better characterization of the convexity of composite losses?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So you mean say for non binary prediction?",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "The results do extend quite nicely this notion of properness.",
                    "label": 0
                },
                {
                    "sent": "Have been studied in depth by Nicholas Lambert in the HTML Conference on Electronic Commerce.",
                    "label": 0
                },
                {
                    "sent": "Yeah, really nice paper last year, which talks about essentially proper losses for multiclass problems and not very beautiful characterization of their structure.",
                    "label": 0
                },
                {
                    "sent": "So yeah, there's there's nice extensions from the binary case.",
                    "label": 0
                }
            ]
        }
    }
}