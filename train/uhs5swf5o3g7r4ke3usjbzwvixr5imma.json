{
    "id": "uhs5swf5o3g7r4ke3usjbzwvixr5imma",
    "title": "LSTD with Random Projections",
    "info": {
        "author": [
            "Mohammad Ghavamzadeh, INRIA Lille - Nord Europe"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2010_ghavamzadeh_lstd/",
    "segmentation": [
        [
            "So this setting that we consider in this work is reinforcement learning.",
            "In high dimensional spaces where the."
        ],
        [
            "Number of features for value function approximation in reinforcement learning is larger or at the order of the number of samples.",
            "Therefore, similar to supervised learning methods, we are going to have the problem of overfitting and poor prediction performance in this setting.",
            "So one possible solution to overcome this problem is taking advantage of the regularity's in the problem.",
            "The smoothness and sparsity are two important forms of regularity's that have been studied in value function approximation reinforcement learning by adding L2 and L1 regularizers to this process.",
            "An alternative approach is based on random projections and preserve and the preservations of properties such as norm or inner product of high dimensional objects when they are projected into possibly much lower dimensional random spaces.",
            "So here we focus on Lee squared temporal difference learning LSD of widely used reinforcement learning algorithm for policy evaluation which is approximating the value function of a given policy.",
            "We present an algorithm called Le Square temporal difference learning with random projections or LST DRP, which takes a high dimensional function in space and not enough samples as input and performs LSD in a low dimensional space generated by random projections from the high dimensional space.",
            "We"
        ],
        [
            "First, present the Illest ERP algorithm and discuss is computational complexity.",
            "We show that is computationally less expensive than performing elected in the high dimensional space.",
            "We then provide finite sample performance bound for LSD RP for two different settings, Markov design setting, which is a form of fixed design in which we evaluate the performance on the training samples and then random design setting in which we show how this performance is generalized over the entire state space.",
            "Studying these bounds and comparing them with the bound for LSD in the high dimension.",
            "And shows us that we achieve smaller estimation error at the cost of not much large larger approximation error.",
            "More specifically, the approximation error that we achieved by performing listed in the low dimensional random space is equal to the approximation or performing it in the high dimension plus an extra term that can be controlled for specific function spaces.",
            "In these cases we can gain by doing this projection and avoid overfitting.",
            "And get better prediction performance.",
            "Now the next step we look at the uniqueness of the LSTR Peasel."
        ],
        [
            "And it means that we assume that the model based elastic dissolution in high dimension exists.",
            "Then we study how many samples are needed in order for LSD RP, which is LSD at the low dimensional random space to have a unique solution with high probability.",
            "Studying these results indicates that illicit ERP is more stable and requires less number of samples to have unique solution compared to performing a listed in the high dimension.",
            "Finally, we move from policy evaluation to control.",
            "From El STD to LSP, which is a policy iteration algorithm that uses an STD at each iteration and show how the error of the RP is propagated through the iterations of LSP and provide the fine example performance bound for LSD RP, which is a control reinforcement learning algorithm with random projections.",
            "If you are interested to know more about this work, please visit to our visit our posters this evening at the poster spot #59.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this setting that we consider in this work is reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "In high dimensional spaces where the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Number of features for value function approximation in reinforcement learning is larger or at the order of the number of samples.",
                    "label": 1
                },
                {
                    "sent": "Therefore, similar to supervised learning methods, we are going to have the problem of overfitting and poor prediction performance in this setting.",
                    "label": 0
                },
                {
                    "sent": "So one possible solution to overcome this problem is taking advantage of the regularity's in the problem.",
                    "label": 0
                },
                {
                    "sent": "The smoothness and sparsity are two important forms of regularity's that have been studied in value function approximation reinforcement learning by adding L2 and L1 regularizers to this process.",
                    "label": 0
                },
                {
                    "sent": "An alternative approach is based on random projections and preserve and the preservations of properties such as norm or inner product of high dimensional objects when they are projected into possibly much lower dimensional random spaces.",
                    "label": 1
                },
                {
                    "sent": "So here we focus on Lee squared temporal difference learning LSD of widely used reinforcement learning algorithm for policy evaluation which is approximating the value function of a given policy.",
                    "label": 1
                },
                {
                    "sent": "We present an algorithm called Le Square temporal difference learning with random projections or LST DRP, which takes a high dimensional function in space and not enough samples as input and performs LSD in a low dimensional space generated by random projections from the high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, present the Illest ERP algorithm and discuss is computational complexity.",
                    "label": 1
                },
                {
                    "sent": "We show that is computationally less expensive than performing elected in the high dimensional space.",
                    "label": 1
                },
                {
                    "sent": "We then provide finite sample performance bound for LSD RP for two different settings, Markov design setting, which is a form of fixed design in which we evaluate the performance on the training samples and then random design setting in which we show how this performance is generalized over the entire state space.",
                    "label": 1
                },
                {
                    "sent": "Studying these bounds and comparing them with the bound for LSD in the high dimension.",
                    "label": 1
                },
                {
                    "sent": "And shows us that we achieve smaller estimation error at the cost of not much large larger approximation error.",
                    "label": 0
                },
                {
                    "sent": "More specifically, the approximation error that we achieved by performing listed in the low dimensional random space is equal to the approximation or performing it in the high dimension plus an extra term that can be controlled for specific function spaces.",
                    "label": 0
                },
                {
                    "sent": "In these cases we can gain by doing this projection and avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "And get better prediction performance.",
                    "label": 0
                },
                {
                    "sent": "Now the next step we look at the uniqueness of the LSTR Peasel.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it means that we assume that the model based elastic dissolution in high dimension exists.",
                    "label": 0
                },
                {
                    "sent": "Then we study how many samples are needed in order for LSD RP, which is LSD at the low dimensional random space to have a unique solution with high probability.",
                    "label": 1
                },
                {
                    "sent": "Studying these results indicates that illicit ERP is more stable and requires less number of samples to have unique solution compared to performing a listed in the high dimension.",
                    "label": 0
                },
                {
                    "sent": "Finally, we move from policy evaluation to control.",
                    "label": 1
                },
                {
                    "sent": "From El STD to LSP, which is a policy iteration algorithm that uses an STD at each iteration and show how the error of the RP is propagated through the iterations of LSP and provide the fine example performance bound for LSD RP, which is a control reinforcement learning algorithm with random projections.",
                    "label": 0
                },
                {
                    "sent": "If you are interested to know more about this work, please visit to our visit our posters this evening at the poster spot #59.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}