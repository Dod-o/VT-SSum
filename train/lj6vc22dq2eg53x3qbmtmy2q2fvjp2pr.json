{
    "id": "lj6vc22dq2eg53x3qbmtmy2q2fvjp2pr",
    "title": "What Would Shannon Do? Bayesian Compression for DL",
    "info": {
        "author": [
            "Karen Ullrich, Informatics Institute, University of Amsterdam"
        ],
        "published": "July 27, 2017",
        "recorded": "June 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_ullrich_bayesian_compression/",
    "segmentation": [
        [
            "Hi yeah, I don't actually know how much Max covered already because I went out when the room had this nervous breakdown.",
            "Ann, anyways, my talk is going to be in large parts to convince you that Beijing compression is the way to deal with compression of neural networks, and I think that not everyone might even be convinced that compression is something of interest for machine learning or relevant topic at all, so I might be talking first a bit about why that is important and also why is algorithm developers.",
            "We can do some."
        ],
        [
            "In about it.",
            "So yesterday we heard already a lot about."
        ],
        [
            "Killer robots and that they're going to take over.",
            "However, I think is quite interesting.",
            "Now we if we compare how much energy out brain is using.",
            "I think about 5 button.",
            "What a tight next is usually using, namely is 35.",
            "What is kind of a long way to go for these killer robots?",
            "At least they're not going to be very long running.",
            "So robots is obviously hardware limited devices and better devices motivation.",
            "Why you want to compress neural networks?"
        ],
        [
            "Also, if I'm a company like Facebook, I might worry about inference at scale.",
            "So if you just imagine one hour costs oh .0225 euro cents an running a tight next then costs me 5.625 cents.",
            "And as in the case of Facebook, I have 1.86 billion active users every month and I take now just standard neural network like VGG and I want to make.",
            "Predictions for each and every of my users, or just one prediction for all of my users that would already cost me like 20 Ki mean there's to say that there is a lot of error margin in these estimates, but the order of magnitude I think is about right.",
            "Alright, so."
        ],
        [
            "So I talked about how we're limited devices energy costs, but there obviously also other things.",
            "Why we care about compressing neural networks.",
            "So one would be to send neural networks.",
            "We are bent limited channels, such as we will find with smart phones.",
            "So if you want to send models to smartphones that thing real time processing is obviously things are speeding them up.",
            "And also there is an obvious relation to privacy, not just the kind of obvious thing that you want to run in your network on your smartphone.",
            "And so prevent data to get out of there.",
            "But also there is a relation between differential privacy, an regularization of neural networks.",
            "I think that was a bit explored by a paper by an Goodfellow, so there's obviously also very interesting connection before I can start actually convincing you of the entire Beijing thing, I think I have to address the."
        ],
        [
            "Practical view of compression first, so your networks are usually represented by their weight matrices.",
            "I display a rate matrix here, and all the colors just represent numbers.",
            "And you kind of can think of like rainbow.",
            "So the most obvious way to save computation costs or to compress neural networks is to just get rid of all the weights that don't actually help you make any predictions.",
            "So that would look like that.",
            "In a computer that doesn't really help you right?",
            "So even though you have a lot of zeros now in your matrix, you still have to store the entire thing.",
            "In computer science, there has been a format developed.",
            "This cold compressed sparse column format where you would then only save this non zero weights and like a position index where that used to be in the original matrix and that can give you a compression rate of the number of weights used to have pour over two times the number of weights that you end up with after your compression.",
            "Because of this indexing scheme, so then more elegant way to do this thing, you might guess because it's an unstructured.",
            "There must be some structured pruning.",
            "You can also get rid of rows and columns and then just arrive at a smaller matrix and then your compression rate would look much nicer.",
            "So an getting rid of columns or rows that would kind of correspond to getting rid of entire neurons or features.",
            "In your network."
        ],
        [
            "A second way to get rid of bits in your in your representation of your neural networks is just to reduce the bits per weight that you save, so there's two options here.",
            "Again, one would be to reduce the bit precision with which you're saving the neural networks.",
            "So from 32 you might go to 10 bit representation of weights and the other alternative is to cluster all the weights that you have in your network, and what you then do is kind of you only save.",
            "The Pointer 2 ecookbook here.",
            "And that can be very cost efficient, so compression rates in first case would be in the order of three because you saw with 32 bit vectors and then you can get down to like say around 10 Pro.",
            "Here is very fast inference, but your compression savings might not be as high as the alternative where let's say if you have 16 cluster bits means you could get down to four bits to represent all those four cluster means and then you would have a higher compression rate.",
            "Let's say you know eight in this example and you could go even further down.",
            "With Huffman encoding your cluster clusters, the problem with that seems kind of that.",
            "We don't really know how to do inference with that, or like it's not implemented yet in any way."
        ],
        [
            "So I want to take a second to kind of compare these schemes.",
            "They are very orthogonal, meaning that you know if you have.",
            "A compression factor of 10 in the unstructured, pruning Anna compression factor in the quantization of like 3 you end up with a total compression of around 30.",
            "So I think there's two scenarios that are really interesting, so the first one would be a mix between set quantization and unstructured pruning, where you would expect the highest compression because of what we just talked about the set quantization you can.",
            "Compressed weights with very little clusters, and when you do unstructured pruning you have also very little expectation or priors on your weights.",
            "In contrast to structure pruning.",
            "However, in that scheme, the flop and energy savings would be rather moderate and the other scenario that is really interesting would be to combine with quantization and structure pruning because.",
            "Of the.",
            "Consider it now savings of flops an energy."
        ],
        [
            "And you could think of applications for both of these compression schemes.",
            "One would be like a ZIP format for neural networks, so if you want to transmit neural networks, we have benefited channels.",
            "Or if you just have millions of users and millions of networks that represent each one user, then you might want to save these networks efficiently and the other one is also kind of interesting because you can do inference at scale because you will save energy, real time predictions and running neural networks at Harper.",
            "Limited devices would kind of fall in this category."
        ],
        [
            "Right, so when we looked at concurrent work, we kind of missed that there was a clear connection to information theory or even that people kind of put a clear compression idea in there.",
            "In their objective an you know when you have been enough time for some time we kind of live and breathe variational lower bound.",
            "So we're kind of naturally thinking about variation.",
            "Variational lower bound how to connect that with?",
            "The compression topic can like more concretely with information theory, and it turns out there is already work from.",
            "25 years ago from Hinton from Camp who were connecting this very variational lower bound with compression."
        ],
        [
            "In particular, there we were connecting the minimum description length principle.",
            "I'm going to read that out with this objective.",
            "So the best model is the one that compresses my data best, and in particular you can decompose that cost into the cost for transmitting my model first and also for reporting the data Misfit afterward."
        ],
        [
            "I think we talked at length already about the log likelihood and that this is probably my transmitting data misfit term.",
            "Anne and then hidden in front camber not just kind of relating the entire thing with compression, but they would also propose a compression scheme.",
            "Kind of famously known as pit spec argument."
        ],
        [
            "So we talked about the log likelihood term here already in length, so we would assume IID data points and our targets to be normally distributed around a function of our inputs XN&W.",
            "So that's kind of well known.",
            "However, what we talk very little about this career virgins.",
            "We usually say this regularization to we don't actually care so much.",
            "But it turns out, I think it's really important.",
            "It's not just a regularization term when we formulated from the view of compression, it is actually our.",
            "Like our main objective and the data Misfit is just the thing that kind of regularize is our compression objective.",
            "So our work that I'm going to present in the second, we kind of focused very much on this cross entropy theorem here between our posterior Q and the.",
            "Cross entropy term between Q an hour prior over models PWM.",
            "It's interesting to note.",
            "That the entropy TM for Q.",
            "Here you will immediately see the connection to what we just discussed in the practical view that if you have clustered waits, for example, an entropy term will be really low obviously, and also if we have lower bit precision, you can kind of correlate the.",
            "The distances between sample points in a wider spaced grid with cautions and then you can also express an entropy for that, but as I said in the two works that I'm going to present now, mainly focusing on the prior here and how to make that more interesting than just saying that some Goshen."
        ],
        [
            "In the 1st paper that we published, we were focusing on this first interesting set of problems that I was introduced."
        ],
        [
            "Earlier there was a joint work with Ted meets Anmc swelling and we published it.",
            "I see largest here here.",
            "We would now again drain from a very old idea from the early 19th, where you would as a prior take caution mixture model and say posterior, just a Delta distribution and you would train your neural network with all your weights but also within the prior your.",
            "Mixture components and your means and your variance is here in your prior and that's known as a empirical Bayesian prior.",
            "And we Furthermore so this is the first part is kind of to make sure you have this clustering effect that I was talking earlier about and the second kind of heck in this thing would be that we said one mixing proportion to zero mixing proportion very high and the the mean of the thing to 0.",
            "So we guarantee our unstructured pruning.",
            "I'm going to show like a little plot about it.",
            "I think I should explain before I started.",
            "So you see my mouse I have so.",
            "So this thing here is the distribution of weights before training this, here is the distribution of weights while training at the moment they are identical.",
            "It's just that this is a log plot here.",
            "And this is just a normal scale.",
            "And here you see the weights and how they change overtime and this stuff.",
            "Here is the cluster means and the red shadings are the variances of the cluster.",
            "Right, so we see in the course of training that these clusters of weights kind of naturally emerge.",
            "Here.",
            "And we can do then is just that all the weights to their corresponding cluster means an we get."
        ],
        [
            "Really high compression rates.",
            "It's kind of nice at the time, but what I was saying is what I was unhappy with in this scheme was we couldn't really address the."
        ],
        [
            "Problem of so we wouldn't get actually any energy safe, so we also focused on a second scenario where you would have a mix between Bitcoin nization and structural pruning."
        ],
        [
            "So this was or is currently under submission for NIPS.",
            "This year is trying to work with just as Lucio San Mexican.",
            "Here we use.",
            "The idea to use dropout to learn the architecture so there is variation of the dropout version of variational dropout where in normal drop out you would sample former binary distribution.",
            "But it has been shown in fact that if you sample noise from a Gaussian distribution that is just fine as well.",
            "And our solution would be then to use this variational dropout to penalize weight structures and kind of get rid of weight structures in which structures means in in fully connected layers we would prune rows or columns and for convolutional layers we would prune entire filters.",
            "And if the dropout that is being learned is high, then we can safely ignore.",
            "Another thing that kind of comes out of this Bayesian thing, which is really nice, is that you can get uncertainty of the remaining weights for free and then from there you can kind of derive what position you really need."
        ],
        [
            "So.",
            "We sample from.",
            "I resemble from a hidden variables at this kind of hour drop out and we incorporate that in our prior in our prior is in our in our posterior or posterior is not this time not a Delta distribution, but a question.",
            "And we kind of want to think so we want our prior to do you remember we kind of want to have priors that.",
            "Let us do the right thing.",
            "So.",
            "So we want to do two things.",
            "First, 4th high dropout rates to get rid of as many structures as possible, and 2nd we wish to kind of push the mean of the posterior to 0.",
            "For those high dropout rate."
        ],
        [
            "And it turns out that there is one prior for which that works perfectly fine, namely this called the lock uniform prior and we kind of use that.",
            "In our work."
        ],
        [
            "And I think there's kind of nice results that came out of this one.",
            "I want to emphasize is So what we did here is for each layer we would get out of weight, position, kind of for free and you would see that many of these layers require very little precision in their way.",
            "It's like down to 5 bits, so it is extremely noisy.",
            "Weights on the network and still make just fine predictions."
        ],
        [
            "And so, somewhat to our surprise, we get even higher compression rates than the one before.",
            "So you remember that one before we had kind of 162 for that network.",
            "And here we would get down to even 771, right?",
            "I'm out of time already.",
            "OK?",
            "So then I."
        ],
        [
            "Yeah.",
            "I just want to say the method also proves to be very fast an energy efficient, but I'm going to."
        ],
        [
            "I doubt also going to revert all the warnings and thank you for your time."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi yeah, I don't actually know how much Max covered already because I went out when the room had this nervous breakdown.",
                    "label": 0
                },
                {
                    "sent": "Ann, anyways, my talk is going to be in large parts to convince you that Beijing compression is the way to deal with compression of neural networks, and I think that not everyone might even be convinced that compression is something of interest for machine learning or relevant topic at all, so I might be talking first a bit about why that is important and also why is algorithm developers.",
                    "label": 0
                },
                {
                    "sent": "We can do some.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In about it.",
                    "label": 0
                },
                {
                    "sent": "So yesterday we heard already a lot about.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Killer robots and that they're going to take over.",
                    "label": 0
                },
                {
                    "sent": "However, I think is quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Now we if we compare how much energy out brain is using.",
                    "label": 0
                },
                {
                    "sent": "I think about 5 button.",
                    "label": 0
                },
                {
                    "sent": "What a tight next is usually using, namely is 35.",
                    "label": 0
                },
                {
                    "sent": "What is kind of a long way to go for these killer robots?",
                    "label": 0
                },
                {
                    "sent": "At least they're not going to be very long running.",
                    "label": 0
                },
                {
                    "sent": "So robots is obviously hardware limited devices and better devices motivation.",
                    "label": 0
                },
                {
                    "sent": "Why you want to compress neural networks?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, if I'm a company like Facebook, I might worry about inference at scale.",
                    "label": 0
                },
                {
                    "sent": "So if you just imagine one hour costs oh .0225 euro cents an running a tight next then costs me 5.625 cents.",
                    "label": 0
                },
                {
                    "sent": "And as in the case of Facebook, I have 1.86 billion active users every month and I take now just standard neural network like VGG and I want to make.",
                    "label": 1
                },
                {
                    "sent": "Predictions for each and every of my users, or just one prediction for all of my users that would already cost me like 20 Ki mean there's to say that there is a lot of error margin in these estimates, but the order of magnitude I think is about right.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I talked about how we're limited devices energy costs, but there obviously also other things.",
                    "label": 1
                },
                {
                    "sent": "Why we care about compressing neural networks.",
                    "label": 0
                },
                {
                    "sent": "So one would be to send neural networks.",
                    "label": 0
                },
                {
                    "sent": "We are bent limited channels, such as we will find with smart phones.",
                    "label": 0
                },
                {
                    "sent": "So if you want to send models to smartphones that thing real time processing is obviously things are speeding them up.",
                    "label": 1
                },
                {
                    "sent": "And also there is an obvious relation to privacy, not just the kind of obvious thing that you want to run in your network on your smartphone.",
                    "label": 0
                },
                {
                    "sent": "And so prevent data to get out of there.",
                    "label": 0
                },
                {
                    "sent": "But also there is a relation between differential privacy, an regularization of neural networks.",
                    "label": 0
                },
                {
                    "sent": "I think that was a bit explored by a paper by an Goodfellow, so there's obviously also very interesting connection before I can start actually convincing you of the entire Beijing thing, I think I have to address the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Practical view of compression first, so your networks are usually represented by their weight matrices.",
                    "label": 1
                },
                {
                    "sent": "I display a rate matrix here, and all the colors just represent numbers.",
                    "label": 0
                },
                {
                    "sent": "And you kind of can think of like rainbow.",
                    "label": 0
                },
                {
                    "sent": "So the most obvious way to save computation costs or to compress neural networks is to just get rid of all the weights that don't actually help you make any predictions.",
                    "label": 0
                },
                {
                    "sent": "So that would look like that.",
                    "label": 0
                },
                {
                    "sent": "In a computer that doesn't really help you right?",
                    "label": 0
                },
                {
                    "sent": "So even though you have a lot of zeros now in your matrix, you still have to store the entire thing.",
                    "label": 0
                },
                {
                    "sent": "In computer science, there has been a format developed.",
                    "label": 0
                },
                {
                    "sent": "This cold compressed sparse column format where you would then only save this non zero weights and like a position index where that used to be in the original matrix and that can give you a compression rate of the number of weights used to have pour over two times the number of weights that you end up with after your compression.",
                    "label": 0
                },
                {
                    "sent": "Because of this indexing scheme, so then more elegant way to do this thing, you might guess because it's an unstructured.",
                    "label": 0
                },
                {
                    "sent": "There must be some structured pruning.",
                    "label": 1
                },
                {
                    "sent": "You can also get rid of rows and columns and then just arrive at a smaller matrix and then your compression rate would look much nicer.",
                    "label": 0
                },
                {
                    "sent": "So an getting rid of columns or rows that would kind of correspond to getting rid of entire neurons or features.",
                    "label": 0
                },
                {
                    "sent": "In your network.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A second way to get rid of bits in your in your representation of your neural networks is just to reduce the bits per weight that you save, so there's two options here.",
                    "label": 0
                },
                {
                    "sent": "Again, one would be to reduce the bit precision with which you're saving the neural networks.",
                    "label": 0
                },
                {
                    "sent": "So from 32 you might go to 10 bit representation of weights and the other alternative is to cluster all the weights that you have in your network, and what you then do is kind of you only save.",
                    "label": 0
                },
                {
                    "sent": "The Pointer 2 ecookbook here.",
                    "label": 0
                },
                {
                    "sent": "And that can be very cost efficient, so compression rates in first case would be in the order of three because you saw with 32 bit vectors and then you can get down to like say around 10 Pro.",
                    "label": 0
                },
                {
                    "sent": "Here is very fast inference, but your compression savings might not be as high as the alternative where let's say if you have 16 cluster bits means you could get down to four bits to represent all those four cluster means and then you would have a higher compression rate.",
                    "label": 0
                },
                {
                    "sent": "Let's say you know eight in this example and you could go even further down.",
                    "label": 0
                },
                {
                    "sent": "With Huffman encoding your cluster clusters, the problem with that seems kind of that.",
                    "label": 0
                },
                {
                    "sent": "We don't really know how to do inference with that, or like it's not implemented yet in any way.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to take a second to kind of compare these schemes.",
                    "label": 0
                },
                {
                    "sent": "They are very orthogonal, meaning that you know if you have.",
                    "label": 0
                },
                {
                    "sent": "A compression factor of 10 in the unstructured, pruning Anna compression factor in the quantization of like 3 you end up with a total compression of around 30.",
                    "label": 0
                },
                {
                    "sent": "So I think there's two scenarios that are really interesting, so the first one would be a mix between set quantization and unstructured pruning, where you would expect the highest compression because of what we just talked about the set quantization you can.",
                    "label": 1
                },
                {
                    "sent": "Compressed weights with very little clusters, and when you do unstructured pruning you have also very little expectation or priors on your weights.",
                    "label": 0
                },
                {
                    "sent": "In contrast to structure pruning.",
                    "label": 0
                },
                {
                    "sent": "However, in that scheme, the flop and energy savings would be rather moderate and the other scenario that is really interesting would be to combine with quantization and structure pruning because.",
                    "label": 1
                },
                {
                    "sent": "Of the.",
                    "label": 1
                },
                {
                    "sent": "Consider it now savings of flops an energy.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you could think of applications for both of these compression schemes.",
                    "label": 0
                },
                {
                    "sent": "One would be like a ZIP format for neural networks, so if you want to transmit neural networks, we have benefited channels.",
                    "label": 0
                },
                {
                    "sent": "Or if you just have millions of users and millions of networks that represent each one user, then you might want to save these networks efficiently and the other one is also kind of interesting because you can do inference at scale because you will save energy, real time predictions and running neural networks at Harper.",
                    "label": 0
                },
                {
                    "sent": "Limited devices would kind of fall in this category.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so when we looked at concurrent work, we kind of missed that there was a clear connection to information theory or even that people kind of put a clear compression idea in there.",
                    "label": 0
                },
                {
                    "sent": "In their objective an you know when you have been enough time for some time we kind of live and breathe variational lower bound.",
                    "label": 0
                },
                {
                    "sent": "So we're kind of naturally thinking about variation.",
                    "label": 0
                },
                {
                    "sent": "Variational lower bound how to connect that with?",
                    "label": 1
                },
                {
                    "sent": "The compression topic can like more concretely with information theory, and it turns out there is already work from.",
                    "label": 0
                },
                {
                    "sent": "25 years ago from Hinton from Camp who were connecting this very variational lower bound with compression.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, there we were connecting the minimum description length principle.",
                    "label": 0
                },
                {
                    "sent": "I'm going to read that out with this objective.",
                    "label": 0
                },
                {
                    "sent": "So the best model is the one that compresses my data best, and in particular you can decompose that cost into the cost for transmitting my model first and also for reporting the data Misfit afterward.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think we talked at length already about the log likelihood and that this is probably my transmitting data misfit term.",
                    "label": 0
                },
                {
                    "sent": "Anne and then hidden in front camber not just kind of relating the entire thing with compression, but they would also propose a compression scheme.",
                    "label": 0
                },
                {
                    "sent": "Kind of famously known as pit spec argument.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we talked about the log likelihood term here already in length, so we would assume IID data points and our targets to be normally distributed around a function of our inputs XN&W.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of well known.",
                    "label": 0
                },
                {
                    "sent": "However, what we talk very little about this career virgins.",
                    "label": 0
                },
                {
                    "sent": "We usually say this regularization to we don't actually care so much.",
                    "label": 0
                },
                {
                    "sent": "But it turns out, I think it's really important.",
                    "label": 0
                },
                {
                    "sent": "It's not just a regularization term when we formulated from the view of compression, it is actually our.",
                    "label": 0
                },
                {
                    "sent": "Like our main objective and the data Misfit is just the thing that kind of regularize is our compression objective.",
                    "label": 0
                },
                {
                    "sent": "So our work that I'm going to present in the second, we kind of focused very much on this cross entropy theorem here between our posterior Q and the.",
                    "label": 0
                },
                {
                    "sent": "Cross entropy term between Q an hour prior over models PWM.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to note.",
                    "label": 0
                },
                {
                    "sent": "That the entropy TM for Q.",
                    "label": 0
                },
                {
                    "sent": "Here you will immediately see the connection to what we just discussed in the practical view that if you have clustered waits, for example, an entropy term will be really low obviously, and also if we have lower bit precision, you can kind of correlate the.",
                    "label": 0
                },
                {
                    "sent": "The distances between sample points in a wider spaced grid with cautions and then you can also express an entropy for that, but as I said in the two works that I'm going to present now, mainly focusing on the prior here and how to make that more interesting than just saying that some Goshen.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the 1st paper that we published, we were focusing on this first interesting set of problems that I was introduced.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Earlier there was a joint work with Ted meets Anmc swelling and we published it.",
                    "label": 0
                },
                {
                    "sent": "I see largest here here.",
                    "label": 0
                },
                {
                    "sent": "We would now again drain from a very old idea from the early 19th, where you would as a prior take caution mixture model and say posterior, just a Delta distribution and you would train your neural network with all your weights but also within the prior your.",
                    "label": 1
                },
                {
                    "sent": "Mixture components and your means and your variance is here in your prior and that's known as a empirical Bayesian prior.",
                    "label": 1
                },
                {
                    "sent": "And we Furthermore so this is the first part is kind of to make sure you have this clustering effect that I was talking earlier about and the second kind of heck in this thing would be that we said one mixing proportion to zero mixing proportion very high and the the mean of the thing to 0.",
                    "label": 0
                },
                {
                    "sent": "So we guarantee our unstructured pruning.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show like a little plot about it.",
                    "label": 0
                },
                {
                    "sent": "I think I should explain before I started.",
                    "label": 0
                },
                {
                    "sent": "So you see my mouse I have so.",
                    "label": 0
                },
                {
                    "sent": "So this thing here is the distribution of weights before training this, here is the distribution of weights while training at the moment they are identical.",
                    "label": 0
                },
                {
                    "sent": "It's just that this is a log plot here.",
                    "label": 0
                },
                {
                    "sent": "And this is just a normal scale.",
                    "label": 0
                },
                {
                    "sent": "And here you see the weights and how they change overtime and this stuff.",
                    "label": 0
                },
                {
                    "sent": "Here is the cluster means and the red shadings are the variances of the cluster.",
                    "label": 0
                },
                {
                    "sent": "Right, so we see in the course of training that these clusters of weights kind of naturally emerge.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "And we can do then is just that all the weights to their corresponding cluster means an we get.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really high compression rates.",
                    "label": 0
                },
                {
                    "sent": "It's kind of nice at the time, but what I was saying is what I was unhappy with in this scheme was we couldn't really address the.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem of so we wouldn't get actually any energy safe, so we also focused on a second scenario where you would have a mix between Bitcoin nization and structural pruning.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was or is currently under submission for NIPS.",
                    "label": 1
                },
                {
                    "sent": "This year is trying to work with just as Lucio San Mexican.",
                    "label": 0
                },
                {
                    "sent": "Here we use.",
                    "label": 0
                },
                {
                    "sent": "The idea to use dropout to learn the architecture so there is variation of the dropout version of variational dropout where in normal drop out you would sample former binary distribution.",
                    "label": 1
                },
                {
                    "sent": "But it has been shown in fact that if you sample noise from a Gaussian distribution that is just fine as well.",
                    "label": 0
                },
                {
                    "sent": "And our solution would be then to use this variational dropout to penalize weight structures and kind of get rid of weight structures in which structures means in in fully connected layers we would prune rows or columns and for convolutional layers we would prune entire filters.",
                    "label": 1
                },
                {
                    "sent": "And if the dropout that is being learned is high, then we can safely ignore.",
                    "label": 0
                },
                {
                    "sent": "Another thing that kind of comes out of this Bayesian thing, which is really nice, is that you can get uncertainty of the remaining weights for free and then from there you can kind of derive what position you really need.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We sample from.",
                    "label": 0
                },
                {
                    "sent": "I resemble from a hidden variables at this kind of hour drop out and we incorporate that in our prior in our prior is in our in our posterior or posterior is not this time not a Delta distribution, but a question.",
                    "label": 0
                },
                {
                    "sent": "And we kind of want to think so we want our prior to do you remember we kind of want to have priors that.",
                    "label": 0
                },
                {
                    "sent": "Let us do the right thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we want to do two things.",
                    "label": 0
                },
                {
                    "sent": "First, 4th high dropout rates to get rid of as many structures as possible, and 2nd we wish to kind of push the mean of the posterior to 0.",
                    "label": 1
                },
                {
                    "sent": "For those high dropout rate.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that there is one prior for which that works perfectly fine, namely this called the lock uniform prior and we kind of use that.",
                    "label": 0
                },
                {
                    "sent": "In our work.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think there's kind of nice results that came out of this one.",
                    "label": 0
                },
                {
                    "sent": "I want to emphasize is So what we did here is for each layer we would get out of weight, position, kind of for free and you would see that many of these layers require very little precision in their way.",
                    "label": 0
                },
                {
                    "sent": "It's like down to 5 bits, so it is extremely noisy.",
                    "label": 0
                },
                {
                    "sent": "Weights on the network and still make just fine predictions.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, somewhat to our surprise, we get even higher compression rates than the one before.",
                    "label": 0
                },
                {
                    "sent": "So you remember that one before we had kind of 162 for that network.",
                    "label": 0
                },
                {
                    "sent": "And here we would get down to even 771, right?",
                    "label": 0
                },
                {
                    "sent": "I'm out of time already.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "So then I.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I just want to say the method also proves to be very fast an energy efficient, but I'm going to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I doubt also going to revert all the warnings and thank you for your time.",
                    "label": 0
                }
            ]
        }
    }
}