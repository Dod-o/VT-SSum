{
    "id": "py3m4qji6faug77mpvekmqyxal3354m6",
    "title": "Boosted optimization for network classification",
    "info": {
        "author": [
            "Timothy Hancock, Institute for Chemical Research, Kyoto University"
        ],
        "published": "June 3, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/aistats2010_hancock_bofnc/",
    "segmentation": [
        [
            "Good morning everybody.",
            "My name is Tim Hancock.",
            "I'm a postdoctoral fellow in kilter University and today I'm going to be talking about optimizing the classification performance about a known network structure.",
            "When you have observations of that network structure, this work is particularly important in areas such as bioinformatics, where the biologists have been kind enough to create these really quite large network structures such as metabolic pathways, and to lesser extent gene regulatory networks and so on.",
            "Where the network structure is large and you have observations of that network structure contained within gene expression.",
            "Data micro analysis, and so on, and what you want to do is try to create an accurate classifier from that gene expression data explicitly using the known network structure."
        ],
        [
            "So the motivation of what we want to do here is we have a known network structure which is a gene network.",
            "As I'm going to be using here.",
            "And we have a response variable Y, which we're trying to classify with a set of predictor variables which contained within nine network structure.",
            "When the structure is known and has a lot of literature behind it like metabolic pathways, it's common that.",
            "In your set of observations you will know single components which are likely to be important to classifying a given response variable.",
            "However, you don't know how the entire network structure is going to behave in terms of classifying that response.",
            "So what we have is we have a particular variable center variable X here where we know is reasonably good at classifying why, then you can probably make a fairly reasonable argument to say that any variable connected to X within our own network structure is probably going to be OK.",
            "Classifying why 2 otherwise, why would they be connected in the network?",
            "However, that's about as far as that argument is going to take you logically, because you don't know how strong the network interactions are going to be within your given set of experimental data.",
            "So in this simple network design here, even though we know that the center variable is good at classifying why we don't necessarily know the effect of these unknown variables here and how they're going to help us in predicting why and what we want to do is, we want to see if we can use the network structure and see if we can use information from the well predicted variable center variable X to try and improve the classification using the entire network structure.",
            "This is slightly different to feature selection approaches and regularised approaches, which.",
            "Basically, we'll look at the network and pick individual features out and return a subset of important information.",
            "We don't want to do that because that can often lead to a broken network.",
            "As a result, you get one or two little small facets, but you don't know how they're going to be connected and what relationships link them through the network.",
            "And if you've got a large, complicated network that's interesting biologically.",
            "So what we want to do is see if we can get a good performance by using the entire network distribution."
        ],
        [
            "So the outline of my talk is going to be on 1st going to set the problem up a little bit and to do this I use a.",
            "Common link between network classifiers and logistic regression and logistic regression and the exponential loss to propose two algorithms for crowd constructing networks.",
            "Network classifiers in the entire network which are boosted expectation propagation and boosted message."
        ],
        [
            "Passing the link between network classifiers and logistic regression has been established quite some time now and it basically says that each variable in your logistic regression is a node within the network and each edge is an interaction effect with the interaction effect between two variables or two nights within the network.",
            "So we have a predictor variable as a node and edge effects and interactions.",
            "We have network looking something similar to this where we have our X variables joined in network.",
            "And they're all in some way going to try and classify Y as best as possible.",
            "And we have our coefficients, beta decay and beta came here.",
            "Which are the logistic regression coefficients an we have a binary response variable and standard logistic regression will then go through and optimize over the logic function where the probability of Y is equal to that where the network is represented as a linear combination of no defects and edge effects within that logic function."
        ],
        [
            "We also know that logistic regression is very similar to optimizing the performance of an exponential potential, and we can simply see this by reorganizing the logic function, and we show that.",
            "Well, it's not exactly clear, but we can show that the probability of Y if you want to increase the probability of correctly classifying why we just simply increase the probability of we increase this exponential potential function in the direction of why this is, assuming of course the wise in minus one to one.",
            "And when we do that we find out that we can rephrase our problem as an expert, as minimizing an exponential loss.",
            "And that was exactly seen by Friedman when they were proposing the additive logistic regression linking with boosting.",
            "And we're going to use that link to construct our network class."
        ],
        [
            "Suffice.",
            "OK, when now we see that we know we can use exponential loss minimizing exponential loss to optimize the performance of a logistic regression.",
            "If we substitute in our known network structure into that exponential loss, we can observe a factorization about that.",
            "Where our exponential loss can be factorized into K terms, where each term one term for each node within the network, you know within their work and its neighbors, and this can be represented more simply over here, where each FK represents this factor nodes and edges.",
            "This F clearly of minimizing the exponential loss is achieved when we minimize the performance that will minimize the exponential loss for each FK.",
            "I know which is basically we maximize each FK in the direction of the known response label Y.",
            "So FK can be treated as an individual classifier and when we have a look at our entire network, treating FK as an individual classifier, we see that we have additive combination of individual classifiers which is very similar to what boosting does.",
            "However, there's a key difference here because boosting is a forward stagewise addition of models to our linear combination where the structure of each model is not known in advance.",
            "Where is here?",
            "We know the structure of every model we want to add to the boosted ensemble in advance, and what we want to do is we want to know if we can use the network structure in some way to optimize the performance of each FK by using the link in.",
            "We've just seen with boost."
        ],
        [
            "So I'm going to quickly go through and describe boosting.",
            "Then I'm going to go through and describe the two algorithms are proposed in the paper, which is X boosted expectation passing an boosted boosted expectation propagation and boosting message path."
        ],
        [
            "So, boosting as I said before constructor, linear combination of models through stagewise addition of individual classifiers to build this linear combination of classifiers after them here and where each of them is a new classifier throughout found through awaited minimization of an exponential loss function.",
            "And what boosting usually does is it usually uses something like a decision tree to feature select from a large predictor variable set and.",
            "And further optimize the exponential loss function an from boosting and particularly out of this definition of boosting, we get these parameters CMK which waits for each model coming into our booster on sambol, which measure how important the addition of that new model is to the entire predicted performance of the unsub."
        ],
        [
            "Mobile.",
            "So network inference we're going to discuss two algorithms of network inference here.",
            "Message passing and expectation propagation, and these two algorithms work on the factor graph representation of the network.",
            "And the basic idea is to break the problem of estimating the entire distribution network up into smaller little local problems which are more easy and more easily solved.",
            "So if we start from a factorization of pairwise exponential loss, pairwise exponential loss functions, we see that the contribution of an individual node XK to the entire network is simply going to be the product of all of its exponential loss, and you can see the little diagram here where I have X, can the node which is connected to factors.",
            "Of the factor graph which define how X case connected with in the networks that they define.",
            "Dependency structure across the network.",
            "So simply multiplying all of those together is going to give me the contribution of XC to the entire network.",
            "Then once I know the contribution of an individual node to a network, I can just simply make the product of that individual of all individual nodes over the entire distribution to get the factorized form of network.",
            "And now I have this.",
            "I can simply substitute this factorized form with exponential loss potentials into expectation propagation and message passing algorithms to optimize the."
        ],
        [
            "Parameters of these models.",
            "Expectation propagation algorithm is a way to minimize the callback library diversion sauna.",
            "Factorized distribution by an iterative refinement of the individual factors.",
            "So if we provided with a factorized distribution of form, here expectation propagation works in a simple three step algorithm which first removes where you first pick a factor that you want to refine the estimates for, and then you remove that factor from the current guess of your posterior distribution.",
            "And then you hold the estimates of you hold that you hold that posterior the removed posterior distribution constant and you re estimate the factor you just removed.",
            "With knowledge of that current guess of your posterior distribution.",
            "And then you just simply re substitute in reinsert that that new current estimate of the effect of back into your posterior.",
            "So simple three step take it out, you re estimate it with the current knowledge around the network, and then you put it back in."
        ],
        [
            "So if we substitute in our factorized form of the network with exponential loss potentials, what we have is when we're going to move the current estimate of fiks, which is a factor within our network.",
            "And this can be done quite simply through subtracting through the exponent of our complete exponential loss around time network classify.",
            "So it's very simple and what we have here, what we're left with there is simply the exponential loss of the entire network classifier.",
            "Minus that particular factor.",
            "So when we re estimate that factor given the error of the current network, what we get is this step here, which can be considered a boosted update where we're adding the new estimate of FL factor fiks into an ensemble which is defined by their current exponential loss which is defined by the OR the other factors within the network.",
            "And so treating that as a boosted.",
            "Addition treating Step 2 is a boosted addition of a factor to a network, what we get."
        ],
        [
            "Is a boosted expectation propagation algorithm that we described in the paper, and this simply defines a boosted update as an optimization step in Step 2, and we have three steps here.",
            "But because we've defined a boosted update, we simply add see we have added this parameter, CIK through our two hour, to wait the importance of each factor within our network classifier."
        ],
        [
            "Message passing algorithms are similar as a set of algorithms which are related to expectation propagation.",
            "However, they make a slightly different assumption.",
            "They assume that all information that you need to estimate with the distribution of a particular node can be given can be provided by the local surrounding network.",
            "So, given a factor graph here message passing algorithm in particular, in this paper, we use a loopy Max product algorithm with a parallel message passing scheduled to find 2 message types.",
            "The first is simply from the outside network propagated to affect the notes are on this edge here, where the action of that message is just to summarize the local state of the network.",
            "So you just.",
            "Take the take the current state."
        ],
        [
            "Network and you propagate it along that and once you along that edge and once you have that current state of the network.",
            "Ann, you hit a factor node.",
            "You then optimize that factor node.",
            "You insert that factor node into the network.",
            "Given the current state of the network can propagate that along the edge."
        ],
        [
            "You do that on each edge individually.",
            "So I.",
            "The message passing boosts and message passing algorithm is simply substituting our factorized potential affect exponential loss.",
            "Into that and what we find is in step two, we have a boosted update where we're trying to add an individual factor into the network, waited by the exponent, weighted by the current error of the local surrounding network structure.",
            "And this gives us a boosted weight here."
        ],
        [
            "So the difference between the boosted expectation propagation algorithm boosted message passing algorithm is that boosted expectation propagation algorithm by removing effective before estimation is adding every node to the network.",
            "Every node in the network to a boosted ensemble of the entire network structure.",
            "So we remove it, we remove the center node here, and then we sum up the entire network every reinserted.",
            "So the network is simply.",
            "An ensemble effect as well as boost of message passing doesn't do that.",
            "No removal, but instead just propagates information through the network.",
            "So at the first iteration, you're looking at the local environment, just the directly connected nodes, an you in certain propagate that information through, and then at the next iteration you're looking at the local environment plus the next set of neighbors, plus an extra measures.",
            "And as you increase.",
            "Your iterations you increase the number of.",
            "That the length you're looking back into the network that comes with a slight bit of computational complexity 'cause you do have to maintain your message history there."
        ],
        [
            "We also did a brief convergence analysis here and we found out that performing the boosted update should also minimize the conditional callback library versions of the network, and that agrees with previous literature."
        ],
        [
            "On that particular area.",
            "To test our model, we construct simulation experiments where we embed a network of simulated network in a random form simulated exponentially distributed network in random form UU distribution.",
            "And we define a network strength where we scale the parameters of that network theater J by a constant value Alpha between .5 and three, to test whether to test the effect of noise.",
            "Whether we can more accurately classify using boost and message passing, boosted expectation propagation in the face of noise, and we compare it to standard logistic regression.",
            "We also compare it to penalized approaches with Ridge regression lasso.",
            "An Elastic Ness penalties and we also compare it to just simple aggregation over the factors."
        ],
        [
            "And our simulation results.",
            "We do this on a 2 dimensional grid of course and we use 5 by 5 fold cross validation and on the Y axis.",
            "Here you find the mean AUC or area under the Roc curve for classification performance.",
            "And what we find here is boosted message passing is.",
            "Clearly.",
            "Beating all other all other methods as the network strength increases, but that again does come as a little bit of computational cost, but it is saying that the local network structure propagating local network structure is more uses, really improving the classification performance over the entire network, and we also find that boosted expectation propagation algorithm is performing about as what you'd expect with the penalized approaches."
        ],
        [
            "And more recently, we've done some work on the on the real data.",
            "This is not published in the paper, but as it was only done in the last month or so where we use the keg used carbohydrate metabolism network, which is 203 jeans, and about 1700 odd interactions and the full network structure is displayed here.",
            "And what we're trying to do is classify heat shock experiments from other environmental stress experiments using the benchmark.",
            "Cash microarray data and what we find here is boosted matches.",
            "Passing is outperforming all other models and boosted expectation propagation algorithm performing about as good as the lasso and elastic.",
            "Net model.",
            "Anan Ridge regression model penalties as well and that agrees with what we did in our simulation experiments.",
            "But more importantly, what we find is the lassoo and the elastic net returning sparser networks than the boosted expectation propagation of boosted message passing operate.",
            "Our algorithm when we look at the top 80% of selected coefficients and we also find that boost of message passing in particular is highlighting this more dominant local local structure within the network that we know exist within metabolic network.",
            "So it's more biologically interpretable."
        ],
        [
            "So in summary, we proposed two methods, boosted expectation propagation algorithm, and boosted message passing, and what we've shown here is that you can get good accurate classifiers by using the entire network distribution.",
            "Buying called by using the entire network.",
            "You don't have to go in through and feature select.",
            "And we are currently looking at ways to further advance our boosted expectation and message passing algorithm by exploiting factorizations of the network to incorporate local topological features such as reactions, pathways, and Geo ontology that exists across many biologic."
        ],
        [
            "Call networks.",
            "I would like to thank all the people of my lab, in particular Professor Hiroshima Mitica, for allowing me to be in Japan and also funding from JSP S and Bird and the conference organizers for allowing me to speak today.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "My name is Tim Hancock.",
                    "label": 0
                },
                {
                    "sent": "I'm a postdoctoral fellow in kilter University and today I'm going to be talking about optimizing the classification performance about a known network structure.",
                    "label": 0
                },
                {
                    "sent": "When you have observations of that network structure, this work is particularly important in areas such as bioinformatics, where the biologists have been kind enough to create these really quite large network structures such as metabolic pathways, and to lesser extent gene regulatory networks and so on.",
                    "label": 0
                },
                {
                    "sent": "Where the network structure is large and you have observations of that network structure contained within gene expression.",
                    "label": 0
                },
                {
                    "sent": "Data micro analysis, and so on, and what you want to do is try to create an accurate classifier from that gene expression data explicitly using the known network structure.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the motivation of what we want to do here is we have a known network structure which is a gene network.",
                    "label": 1
                },
                {
                    "sent": "As I'm going to be using here.",
                    "label": 1
                },
                {
                    "sent": "And we have a response variable Y, which we're trying to classify with a set of predictor variables which contained within nine network structure.",
                    "label": 0
                },
                {
                    "sent": "When the structure is known and has a lot of literature behind it like metabolic pathways, it's common that.",
                    "label": 1
                },
                {
                    "sent": "In your set of observations you will know single components which are likely to be important to classifying a given response variable.",
                    "label": 0
                },
                {
                    "sent": "However, you don't know how the entire network structure is going to behave in terms of classifying that response.",
                    "label": 0
                },
                {
                    "sent": "So what we have is we have a particular variable center variable X here where we know is reasonably good at classifying why, then you can probably make a fairly reasonable argument to say that any variable connected to X within our own network structure is probably going to be OK.",
                    "label": 0
                },
                {
                    "sent": "Classifying why 2 otherwise, why would they be connected in the network?",
                    "label": 1
                },
                {
                    "sent": "However, that's about as far as that argument is going to take you logically, because you don't know how strong the network interactions are going to be within your given set of experimental data.",
                    "label": 0
                },
                {
                    "sent": "So in this simple network design here, even though we know that the center variable is good at classifying why we don't necessarily know the effect of these unknown variables here and how they're going to help us in predicting why and what we want to do is, we want to see if we can use the network structure and see if we can use information from the well predicted variable center variable X to try and improve the classification using the entire network structure.",
                    "label": 0
                },
                {
                    "sent": "This is slightly different to feature selection approaches and regularised approaches, which.",
                    "label": 0
                },
                {
                    "sent": "Basically, we'll look at the network and pick individual features out and return a subset of important information.",
                    "label": 0
                },
                {
                    "sent": "We don't want to do that because that can often lead to a broken network.",
                    "label": 0
                },
                {
                    "sent": "As a result, you get one or two little small facets, but you don't know how they're going to be connected and what relationships link them through the network.",
                    "label": 0
                },
                {
                    "sent": "And if you've got a large, complicated network that's interesting biologically.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is see if we can get a good performance by using the entire network distribution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of my talk is going to be on 1st going to set the problem up a little bit and to do this I use a.",
                    "label": 0
                },
                {
                    "sent": "Common link between network classifiers and logistic regression and logistic regression and the exponential loss to propose two algorithms for crowd constructing networks.",
                    "label": 1
                },
                {
                    "sent": "Network classifiers in the entire network which are boosted expectation propagation and boosted message.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Passing the link between network classifiers and logistic regression has been established quite some time now and it basically says that each variable in your logistic regression is a node within the network and each edge is an interaction effect with the interaction effect between two variables or two nights within the network.",
                    "label": 1
                },
                {
                    "sent": "So we have a predictor variable as a node and edge effects and interactions.",
                    "label": 0
                },
                {
                    "sent": "We have network looking something similar to this where we have our X variables joined in network.",
                    "label": 0
                },
                {
                    "sent": "And they're all in some way going to try and classify Y as best as possible.",
                    "label": 0
                },
                {
                    "sent": "And we have our coefficients, beta decay and beta came here.",
                    "label": 1
                },
                {
                    "sent": "Which are the logistic regression coefficients an we have a binary response variable and standard logistic regression will then go through and optimize over the logic function where the probability of Y is equal to that where the network is represented as a linear combination of no defects and edge effects within that logic function.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also know that logistic regression is very similar to optimizing the performance of an exponential potential, and we can simply see this by reorganizing the logic function, and we show that.",
                    "label": 1
                },
                {
                    "sent": "Well, it's not exactly clear, but we can show that the probability of Y if you want to increase the probability of correctly classifying why we just simply increase the probability of we increase this exponential potential function in the direction of why this is, assuming of course the wise in minus one to one.",
                    "label": 0
                },
                {
                    "sent": "And when we do that we find out that we can rephrase our problem as an expert, as minimizing an exponential loss.",
                    "label": 0
                },
                {
                    "sent": "And that was exactly seen by Friedman when they were proposing the additive logistic regression linking with boosting.",
                    "label": 0
                },
                {
                    "sent": "And we're going to use that link to construct our network class.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suffice.",
                    "label": 0
                },
                {
                    "sent": "OK, when now we see that we know we can use exponential loss minimizing exponential loss to optimize the performance of a logistic regression.",
                    "label": 0
                },
                {
                    "sent": "If we substitute in our known network structure into that exponential loss, we can observe a factorization about that.",
                    "label": 1
                },
                {
                    "sent": "Where our exponential loss can be factorized into K terms, where each term one term for each node within the network, you know within their work and its neighbors, and this can be represented more simply over here, where each FK represents this factor nodes and edges.",
                    "label": 1
                },
                {
                    "sent": "This F clearly of minimizing the exponential loss is achieved when we minimize the performance that will minimize the exponential loss for each FK.",
                    "label": 1
                },
                {
                    "sent": "I know which is basically we maximize each FK in the direction of the known response label Y.",
                    "label": 1
                },
                {
                    "sent": "So FK can be treated as an individual classifier and when we have a look at our entire network, treating FK as an individual classifier, we see that we have additive combination of individual classifiers which is very similar to what boosting does.",
                    "label": 0
                },
                {
                    "sent": "However, there's a key difference here because boosting is a forward stagewise addition of models to our linear combination where the structure of each model is not known in advance.",
                    "label": 0
                },
                {
                    "sent": "Where is here?",
                    "label": 0
                },
                {
                    "sent": "We know the structure of every model we want to add to the boosted ensemble in advance, and what we want to do is we want to know if we can use the network structure in some way to optimize the performance of each FK by using the link in.",
                    "label": 0
                },
                {
                    "sent": "We've just seen with boost.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to quickly go through and describe boosting.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to go through and describe the two algorithms are proposed in the paper, which is X boosted expectation passing an boosted boosted expectation propagation and boosting message path.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, boosting as I said before constructor, linear combination of models through stagewise addition of individual classifiers to build this linear combination of classifiers after them here and where each of them is a new classifier throughout found through awaited minimization of an exponential loss function.",
                    "label": 1
                },
                {
                    "sent": "And what boosting usually does is it usually uses something like a decision tree to feature select from a large predictor variable set and.",
                    "label": 0
                },
                {
                    "sent": "And further optimize the exponential loss function an from boosting and particularly out of this definition of boosting, we get these parameters CMK which waits for each model coming into our booster on sambol, which measure how important the addition of that new model is to the entire predicted performance of the unsub.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mobile.",
                    "label": 0
                },
                {
                    "sent": "So network inference we're going to discuss two algorithms of network inference here.",
                    "label": 0
                },
                {
                    "sent": "Message passing and expectation propagation, and these two algorithms work on the factor graph representation of the network.",
                    "label": 1
                },
                {
                    "sent": "And the basic idea is to break the problem of estimating the entire distribution network up into smaller little local problems which are more easy and more easily solved.",
                    "label": 0
                },
                {
                    "sent": "So if we start from a factorization of pairwise exponential loss, pairwise exponential loss functions, we see that the contribution of an individual node XK to the entire network is simply going to be the product of all of its exponential loss, and you can see the little diagram here where I have X, can the node which is connected to factors.",
                    "label": 1
                },
                {
                    "sent": "Of the factor graph which define how X case connected with in the networks that they define.",
                    "label": 0
                },
                {
                    "sent": "Dependency structure across the network.",
                    "label": 0
                },
                {
                    "sent": "So simply multiplying all of those together is going to give me the contribution of XC to the entire network.",
                    "label": 0
                },
                {
                    "sent": "Then once I know the contribution of an individual node to a network, I can just simply make the product of that individual of all individual nodes over the entire distribution to get the factorized form of network.",
                    "label": 0
                },
                {
                    "sent": "And now I have this.",
                    "label": 1
                },
                {
                    "sent": "I can simply substitute this factorized form with exponential loss potentials into expectation propagation and message passing algorithms to optimize the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameters of these models.",
                    "label": 0
                },
                {
                    "sent": "Expectation propagation algorithm is a way to minimize the callback library diversion sauna.",
                    "label": 0
                },
                {
                    "sent": "Factorized distribution by an iterative refinement of the individual factors.",
                    "label": 1
                },
                {
                    "sent": "So if we provided with a factorized distribution of form, here expectation propagation works in a simple three step algorithm which first removes where you first pick a factor that you want to refine the estimates for, and then you remove that factor from the current guess of your posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "And then you hold the estimates of you hold that you hold that posterior the removed posterior distribution constant and you re estimate the factor you just removed.",
                    "label": 0
                },
                {
                    "sent": "With knowledge of that current guess of your posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "And then you just simply re substitute in reinsert that that new current estimate of the effect of back into your posterior.",
                    "label": 0
                },
                {
                    "sent": "So simple three step take it out, you re estimate it with the current knowledge around the network, and then you put it back in.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we substitute in our factorized form of the network with exponential loss potentials, what we have is when we're going to move the current estimate of fiks, which is a factor within our network.",
                    "label": 1
                },
                {
                    "sent": "And this can be done quite simply through subtracting through the exponent of our complete exponential loss around time network classify.",
                    "label": 1
                },
                {
                    "sent": "So it's very simple and what we have here, what we're left with there is simply the exponential loss of the entire network classifier.",
                    "label": 0
                },
                {
                    "sent": "Minus that particular factor.",
                    "label": 0
                },
                {
                    "sent": "So when we re estimate that factor given the error of the current network, what we get is this step here, which can be considered a boosted update where we're adding the new estimate of FL factor fiks into an ensemble which is defined by their current exponential loss which is defined by the OR the other factors within the network.",
                    "label": 0
                },
                {
                    "sent": "And so treating that as a boosted.",
                    "label": 1
                },
                {
                    "sent": "Addition treating Step 2 is a boosted addition of a factor to a network, what we get.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a boosted expectation propagation algorithm that we described in the paper, and this simply defines a boosted update as an optimization step in Step 2, and we have three steps here.",
                    "label": 0
                },
                {
                    "sent": "But because we've defined a boosted update, we simply add see we have added this parameter, CIK through our two hour, to wait the importance of each factor within our network classifier.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Message passing algorithms are similar as a set of algorithms which are related to expectation propagation.",
                    "label": 0
                },
                {
                    "sent": "However, they make a slightly different assumption.",
                    "label": 0
                },
                {
                    "sent": "They assume that all information that you need to estimate with the distribution of a particular node can be given can be provided by the local surrounding network.",
                    "label": 1
                },
                {
                    "sent": "So, given a factor graph here message passing algorithm in particular, in this paper, we use a loopy Max product algorithm with a parallel message passing scheduled to find 2 message types.",
                    "label": 1
                },
                {
                    "sent": "The first is simply from the outside network propagated to affect the notes are on this edge here, where the action of that message is just to summarize the local state of the network.",
                    "label": 0
                },
                {
                    "sent": "So you just.",
                    "label": 0
                },
                {
                    "sent": "Take the take the current state.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Network and you propagate it along that and once you along that edge and once you have that current state of the network.",
                    "label": 0
                },
                {
                    "sent": "Ann, you hit a factor node.",
                    "label": 0
                },
                {
                    "sent": "You then optimize that factor node.",
                    "label": 0
                },
                {
                    "sent": "You insert that factor node into the network.",
                    "label": 0
                },
                {
                    "sent": "Given the current state of the network can propagate that along the edge.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You do that on each edge individually.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                },
                {
                    "sent": "The message passing boosts and message passing algorithm is simply substituting our factorized potential affect exponential loss.",
                    "label": 1
                },
                {
                    "sent": "Into that and what we find is in step two, we have a boosted update where we're trying to add an individual factor into the network, waited by the exponent, weighted by the current error of the local surrounding network structure.",
                    "label": 0
                },
                {
                    "sent": "And this gives us a boosted weight here.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the difference between the boosted expectation propagation algorithm boosted message passing algorithm is that boosted expectation propagation algorithm by removing effective before estimation is adding every node to the network.",
                    "label": 0
                },
                {
                    "sent": "Every node in the network to a boosted ensemble of the entire network structure.",
                    "label": 1
                },
                {
                    "sent": "So we remove it, we remove the center node here, and then we sum up the entire network every reinserted.",
                    "label": 1
                },
                {
                    "sent": "So the network is simply.",
                    "label": 0
                },
                {
                    "sent": "An ensemble effect as well as boost of message passing doesn't do that.",
                    "label": 0
                },
                {
                    "sent": "No removal, but instead just propagates information through the network.",
                    "label": 0
                },
                {
                    "sent": "So at the first iteration, you're looking at the local environment, just the directly connected nodes, an you in certain propagate that information through, and then at the next iteration you're looking at the local environment plus the next set of neighbors, plus an extra measures.",
                    "label": 0
                },
                {
                    "sent": "And as you increase.",
                    "label": 0
                },
                {
                    "sent": "Your iterations you increase the number of.",
                    "label": 0
                },
                {
                    "sent": "That the length you're looking back into the network that comes with a slight bit of computational complexity 'cause you do have to maintain your message history there.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also did a brief convergence analysis here and we found out that performing the boosted update should also minimize the conditional callback library versions of the network, and that agrees with previous literature.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On that particular area.",
                    "label": 0
                },
                {
                    "sent": "To test our model, we construct simulation experiments where we embed a network of simulated network in a random form simulated exponentially distributed network in random form UU distribution.",
                    "label": 1
                },
                {
                    "sent": "And we define a network strength where we scale the parameters of that network theater J by a constant value Alpha between .5 and three, to test whether to test the effect of noise.",
                    "label": 1
                },
                {
                    "sent": "Whether we can more accurately classify using boost and message passing, boosted expectation propagation in the face of noise, and we compare it to standard logistic regression.",
                    "label": 0
                },
                {
                    "sent": "We also compare it to penalized approaches with Ridge regression lasso.",
                    "label": 0
                },
                {
                    "sent": "An Elastic Ness penalties and we also compare it to just simple aggregation over the factors.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And our simulation results.",
                    "label": 0
                },
                {
                    "sent": "We do this on a 2 dimensional grid of course and we use 5 by 5 fold cross validation and on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "Here you find the mean AUC or area under the Roc curve for classification performance.",
                    "label": 0
                },
                {
                    "sent": "And what we find here is boosted message passing is.",
                    "label": 0
                },
                {
                    "sent": "Clearly.",
                    "label": 0
                },
                {
                    "sent": "Beating all other all other methods as the network strength increases, but that again does come as a little bit of computational cost, but it is saying that the local network structure propagating local network structure is more uses, really improving the classification performance over the entire network, and we also find that boosted expectation propagation algorithm is performing about as what you'd expect with the penalized approaches.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And more recently, we've done some work on the on the real data.",
                    "label": 0
                },
                {
                    "sent": "This is not published in the paper, but as it was only done in the last month or so where we use the keg used carbohydrate metabolism network, which is 203 jeans, and about 1700 odd interactions and the full network structure is displayed here.",
                    "label": 0
                },
                {
                    "sent": "And what we're trying to do is classify heat shock experiments from other environmental stress experiments using the benchmark.",
                    "label": 1
                },
                {
                    "sent": "Cash microarray data and what we find here is boosted matches.",
                    "label": 0
                },
                {
                    "sent": "Passing is outperforming all other models and boosted expectation propagation algorithm performing about as good as the lasso and elastic.",
                    "label": 0
                },
                {
                    "sent": "Net model.",
                    "label": 0
                },
                {
                    "sent": "Anan Ridge regression model penalties as well and that agrees with what we did in our simulation experiments.",
                    "label": 0
                },
                {
                    "sent": "But more importantly, what we find is the lassoo and the elastic net returning sparser networks than the boosted expectation propagation of boosted message passing operate.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm when we look at the top 80% of selected coefficients and we also find that boost of message passing in particular is highlighting this more dominant local local structure within the network that we know exist within metabolic network.",
                    "label": 0
                },
                {
                    "sent": "So it's more biologically interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, we proposed two methods, boosted expectation propagation algorithm, and boosted message passing, and what we've shown here is that you can get good accurate classifiers by using the entire network distribution.",
                    "label": 0
                },
                {
                    "sent": "Buying called by using the entire network.",
                    "label": 1
                },
                {
                    "sent": "You don't have to go in through and feature select.",
                    "label": 0
                },
                {
                    "sent": "And we are currently looking at ways to further advance our boosted expectation and message passing algorithm by exploiting factorizations of the network to incorporate local topological features such as reactions, pathways, and Geo ontology that exists across many biologic.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Call networks.",
                    "label": 0
                },
                {
                    "sent": "I would like to thank all the people of my lab, in particular Professor Hiroshima Mitica, for allowing me to be in Japan and also funding from JSP S and Bird and the conference organizers for allowing me to speak today.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}