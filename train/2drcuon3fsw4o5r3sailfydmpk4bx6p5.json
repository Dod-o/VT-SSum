{
    "id": "2drcuon3fsw4o5r3sailfydmpk4bx6p5",
    "title": "Reinforcement Learning",
    "info": {
        "author": [
            "Joelle Pineau, School of Computer Science, McGill University"
        ],
        "published": "July 27, 2017",
        "recorded": "July 2017",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2017_pineau_reinforcement_learning/",
    "segmentation": [
        [
            "Great thank you for joining us bright and early on Monday morning.",
            "What is for some of us Canadians?",
            "A national holiday?",
            "I hope I will make it worth your while to have gotten up early.",
            "Quick thing to kind of get us warmed up.",
            "How many of you were with the deep Learning Summer School last week?",
            "Awesome.",
            "How many of you went to the Jazz Festival over the weekend?",
            "On a cruise last night.",
            "Alright, who is joining us for the first time this morning?",
            "Excellent you guys.",
            "Missed out a little bit on last weekend the weekend, but look around, there's tons of people who were here last week.",
            "They can tell you probably where to eat.",
            "What are fun places to visit in Montreal?",
            "You're not necessarily going to need the knowledge that was shared last week in terms of technical knowledge.",
            "We're starting afresh this morning, so I am starting with really the basics of reinforcement learning, but we have a fantastic lineup of speakers.",
            "Coming in the next three days.",
            "For those of you who are the deep learning Summer school, last year we had one day of NRL over the six or seven day program an because their interest in our Ella has really been growing.",
            "This year we decided to extend this to three days, so I sent out invitations to my favorite PRL researchers in the world and I'm so happy to say that most of them accepted the invitation are going to be here this week.",
            "Few of them had to say no.",
            "We've got them lined up for next year, but we have really a spectacular I think.",
            "Lineup of speakers.",
            "The schedule was down a little bit less last night or this morning, so I have it up here in case you were looking for it.",
            "I'm going to give you a first.",
            "I want to have session this morning that will be really the basics of reinforcement learning.",
            "So if you're one of these people who feels like you've never learned anything yet about reinforcement learning, I am here for you.",
            "I've got your back covered.",
            "You can ask questions.",
            "We're going to go at the pace that you want and after that we're going to be launching in some of the more advanced material.",
            "On Peter Beale, Rich son will be here later today an at 4:30.",
            "There's a practical sessions that is still a little bit in evolution, but essentially a few of the graduate student from the reasoning and Learning Lab will come and show you some of the tools environments that they've been using in their research that they find the most helpful.",
            "This is really geared towards people who haven't done research in reinforcement learning and are thinking like where should I start right?",
            "What's the equivalent of MNIST for reinforcement learning?",
            "How should I be training my first agent?",
            "What does the training loop looks like?",
            "We don't usually have a training set in the test set in RL.",
            "How do I think?",
            "Of training and testing in these kinds of context, so that's really the purpose of the practical session this afternoon.",
            "Tomorrow morning we have two of the deep mind researchers coming in telling us about what they have been building, doing research on some of the tricks that went into building the very famous Alphago system.",
            "Cheeba Cheeba scary is one of the best reinforcement learning theorists, so he'll be there in the afternoon and then we have more special topics on Wednesday all day.",
            "So I encourage you to come all the way through the week.",
            "There is coffee breaks and things like that.",
            "I just cut it out in terms of space so.",
            "Today I'm really giving you the sense of whether the basics for reinforcement learning this summer school was I should say, conceived up and planned with my colleague Joyner Precup, who's also a well known researcher in reinforcement learning.",
            "Unfortunately, Dino was called away this week due to family emergency, so she's not going to be around, but much of the work that went into the planning was encountered."
        ],
        [
            "Aberration with her.",
            "So when we talk about reinforcement learning, right?",
            "Let's start from sort of the intuition of what we're talking about.",
            "The idea is inspired by some of the concepts that are coming from psychology, and the idea is really that you can learn, train a system by allowing it to perform different actions in its environment, and by observing the effects of this action, there's some feedback system that sort of tells the agent when it's performing good actions and bad actions.",
            "And so any of you who has a pet, maybe a child, maybe even a spouse is quite familiar with this kind of situation, right?",
            "You try out different prompts, different actions, see what kind of response you get.",
            "Sometimes you give positive reward and sometimes you give negative rewards and you try to shape the behavior of the other agent using these kinds of guiding principles.",
            "And so when we do this in computer science and with machine.",
            "I insight is to try to use these same kinds of notions to try our to train our agents and so the idea is that through multiple actions the agent can learn to shape its behavior based on a reward mechanism."
        ],
        [
            "Probably for many years, the most famous reinforcement learning system was a system called TD Gammon, developed by Jerry Tesoro in the 1990s, and in this case the system had been trained to play backgammon at a world champion level, essentially through self play.",
            "So the system was not trained by playing actual people, it played against itself through many millions of games and overtime learn to beat human players."
        ],
        [
            "Most recently, we've had some other spectacular successes of reinforcement learning, particularly Alphago system where it was shown that again an agent trained with reinforcement learning can learn to play the very challenging game of go and do so at a level exceeding the best human players.",
            "There was some very nice results in 2016, we would be one of the top players, but maybe not the world champion, then most recently in 2017, again in a set of matches in China we saw this system beats.",
            "Well acknowledged human world champion and so hopefully later this week, we'll hear about some of the more advanced reinforcement learning techniques that went into the system for Alpha go."
        ],
        [
            "Couple weeks ago I was at the RLDM conference of them is reinforcement learning and decision making, and I quickly glanced through the proceedings.",
            "I only made it through about half of the proceedings, checking whether some of the recent applications of reinforcement learning.",
            "It's not all about games and this is a brief list of the applications like that I could find by just scanning titles in abstract and I only got through half of the accepted presentations and so you have reinforcement applied of course to robotics video games.",
            "Conversational system, right chatbot type of agents, or one other popular applications trying to train medical decision systems.",
            "But you see some more interesting things like improvisational theater showing up there so there's really a little bit of something for everyone's taste.",
            "If you are interested in that."
        ],
        [
            "One of the things I would like you to get out of today's session is some insight about when to think about applying reinforcement learning.",
            "Many of you, from what I understand, don't necessarily come from a reinforcement learning research lab, and you have a lot of expertise, probably in doing supervised learning.",
            "Maybe some unsupervised learning.",
            "So what I would like to help you develop is some insight for what problems should or must be cast as reinforcement learning problems and what kind of problems can we handle?",
            "Using a supervised learning framework, so one of the clues that you might be dealing with a reinforcement learning problem is that your data comes in the form of trajectories, so there's a sequence of observation, and there's a dependency between these observations, so that usually means that you're very precious.",
            "IID assumption is not going to hold.",
            "You have to take into account the fact that there's dependencies between these observations.",
            "That's not quite enough.",
            "Is a necessary condition, but it's not quite enough because you have time series data.",
            "For example, if you're observing a set of weather patterns or stock market patterns.",
            "In those case you have data in the form of sequence, but it's not yet a reinforcement learning problem, so the other thing that you need in a reinforcement learning problem is the need to make some decisions or interventions.",
            "So there's a notion that you're going to take actions that are going to affect the sequence of observations that you get.",
            "So not only does the data come in the trajectory, but there's a means to affect the course of that trajectory through a set of actions, decisions, interventions.",
            "These are usually the two indices.",
            "The two pieces of information you need, the third one, which is also crucial.",
            "Is that you need to have a way to observe feedback about their choice of actions.",
            "So there has to be some mechanism through which you can assess whether an action should be positively rewarded or negatively rewarded, and typically that reward isn't a binary signal.",
            "It's not a 01, but it's a real number that quantifies how good or how bad was the action.",
            "So it can be any real number.",
            "We usually assume that it's a bounded.",
            "Signal, and so.",
            "As long as you have that signal that gives you information about the choice of action, then you have all of the pieces you need for reinforcement learning, and I would add one of the things that reinforcement learning does quite nicely is dealing with sparse rewards, so you don't need to have direct feedback about each action that is applied in this system.",
            "If you think of training an agent to play a game like Backgammon, Go chess, you don't need at every move to receive a reward signal that tells you that was a good move.",
            "That was a bad move.",
            "It's enough to have reward signals at some point along the trajectory, and in the case of games, usually that reward comes at the very end.",
            "So you either win the game or you lose the game, and that's the reward signal that you get.",
            "So sparse reward along the trajectories are fine, but there has to be some reward to drive the learning.",
            "If there's no reward that is observed at all, it's very hard to learn anything you're not going to be able to shape the behavior of your system.",
            "And the last bit that I note in this one isn't.",
            "So if you meet the 1st three criteria, you're probably on good grounds to look at your problem as a reinforcement learning problem.",
            "The last one, though that is useful to think about, is that this is typically used for cases.",
            "Where your problem requires both a notion of learning right, you have some information that you can use to assess the quality of your decision, but also that there's a notion of planning, and so you're interested in making a sequence of decision.",
            "And maybe there's interaction between the choices of decisions along the way.",
            "If all of the decisions can be done independently, typically it's not required to move to the reinforcement learning framework, so these are sort of some of the things that you can."
        ],
        [
            "Look at for those of you who are most familiar with supervised learning, like to make this parallel where in supervised learning you have a notion of inputs and outputs.",
            "And in reinforcement learning, your inputs are typically called States and your outputs with the agent chooses are typically called actions.",
            "In supervised learning, you do have something akin to a reward function, right?",
            "It's your target signal, so it may be the correct class, right?",
            "Your supervised learner predicts an output, but then you get to look whether that was the actual output that we wanted.",
            "That's your label in reinforcement learning.",
            "Instead of a label, we have a reward.",
            "So this is this number that says how good or how bad was your choice of action.",
            "And because you're dealing with a system over several instances, there's a feedback loop that says depending which action I took.",
            "Imagine an agent that is playing chess, depending what action the agent took, there's going to be an effect on the environment.",
            "It's going to change the state of the environment.",
            "That effect will be fed back in the next state, and so this feedback loop.",
            "This dependency between the state at every time step.",
            "Is one of the reasons why we can't make the idea assumption and it's one of the reasons why in some cases some of the analysis that we have to conduct with reinforcement learning gets a little bit more complicated or requires a different set of tools than what you would do for supervised learning."
        ],
        [
            "And so there's some several practical technical challenges that come along, in particular with the fact that we have this feedback loop.",
            "One of them is the fact that typically to train a reinforcement learning agent.",
            "You can't just look at a static data set.",
            "It's not possible to have just the training set because you need to have a sense of how the actions are going to affect the environment, and that effect can only be measured from an actual environment.",
            "There's still some possibility to train directly from a data set if you have data set that has essentially covered all the choices of actions in all states in sufficient quantity that you can estimate stochastic effect, and so that's a pretty strong assumption, right?",
            "It's saying assume that my agent has been in all States and tried.",
            "All actions infinitely many time.",
            "Then I can do reinforcement learning from a fixed data set in reality that those conditions are only going to be satisfied for very small simple domains, and so most of the interesting domains that were working in, including all the domains that I had listed from these are LDM conference.",
            "Most of them are cases where you can't learn directly from a static data set.",
            "You need this kind of interactive system, so it means you need an environment to try out your actions.",
            "In some sense, this first point is one of the reasons why the.",
            "Rate of progress in reinforcement learning has been considerably slower than in supervised learning in terms of sharing knowledge between different research groups, it's very easy to send over a data set.",
            "Nowadays, you just put it on the web.",
            "People downloaded your download, your data set.",
            "If I'm doing reinforcement learning research, I have a dynamic system.",
            "Maybe it's a robot, maybe it's a biological system.",
            "Maybe it's a set of, for example, stock market type of situation.",
            "It's hard to encapsulate that in, send it over, and so one of the reasons most of the reinforcement learning successes in research has been done with games is because in the case of games, we can actually encapsulate the dynamics of that environment, write out the rules of the domain in a way that is concise and share that between research groups and so that enables facilitates progress.",
            "So we need access to the environment we need to plan an learn simultaneously and finally, the last point is the fact that because we have these dependencies right this feedback loop.",
            "The data distribution also changes overtime as a function of what actions are taken.",
            "So if you change your action strategy, you're going to see different things right?",
            "If you build a robot that is always going right, then the robot is going to see a certain part of the world, and now you suddenly make the robot always go left.",
            "It's going to see another part of the world.",
            "So depending what is your choice of action, you see a different distribution of the data, and you need to take that into account when you're doing learning, so you can't necessarily assume that your data distribution is stationary as you would.",
            "For a standard supervised learning."
        ],
        [
            "So, to formalize things a little bit more, we typically cast reinforcement learning problems using Markov decision process.",
            "So Markov decision process are.",
            "Formally specified by the following, the set of states so that it defines the space of the problem.",
            "These states can be discreet.",
            "They can be infinite.",
            "They can be continuous, but you need a set of states to be defined.",
            "There's a set of actions, typically discrete, but they can also be continuous.",
            "We have a probabilistic distribution that describes the dynamics of the environment, and so we make specifically the assumption that the probability of seeing a state.",
            "Is fully defined condition on the previous state and the action that was chosen will talk a little bit more about that assumption in a few seconds, but that is our model that defines how the agent changes overtime.",
            "But there's the reward function.",
            "That's a real number defined for each state and action pair, and typically in some cases we also assume an initial state distribution, so that can be the starting state.",
            "If you're playing a game right.",
            "The initial starting distribution is just the initial state of the board.",
            "Trajectories always start from that state, but for some other systems, robotic systems and others you can actually have a distribution over possible initial states."
        ],
        [
            "And so let's visit again the assumption that we're making when we specify or transition model, right?",
            "I said that the probability of the next state is fully defined by the previous state in the previous action, and this is known in the literature as the Markov assumption.",
            "The Markov property in this case.",
            "More generally, if we didn't want to make such a restrictive assumption, we could assume that the probability of the next state depends on the full history.",
            "All the states that this system has visited before and all this state, all the actions that have been taken previously.",
            "And so this would be a system that has much different structure in terms of the transition probability and so some of the reason we make this Markov assumption, one of them is really just to limit the complexity of the model that we're looking at.",
            "We're looking at this dynamic model and we want to learn this from data.",
            "If we start having a probabilistic model that depends on full histories, then it requires a lot more information to estimate accurately.",
            "Now, I haven't told you necessarily how you need to define your state, and so essentially you have to think of the state as the sufficient amount of information that you need to predict the future.",
            "That's probably the best way to think of how to define your state space."
        ],
        [
            "So if we look at simple systems that we've seen before, right?",
            "All of you have seen traffic lights before.",
            "If you are faced with the traffic light and the light is yellow, you don't need to think really long about what was the previous states of this traffic light to know what is coming next right is going to go to the red light, and so in that case the color is sufficient to predict the next color.",
            "So we would say that this traffic light meets the Markov assumption.",
            "Chess is another one of these cases.",
            "If the board isn't a particular configuration and you take a particular action, then you know what's going to be the board configuration when stepped down the line.",
            "In both of these cases, these are deterministic system, right?",
            "So if there's a state and action, you know what the next state is in reinforcement learning.",
            "We don't necessarily assume that there's a deterministic transition.",
            "We can handle stochastic transition where there's a probability distribution, but the point here is that the information is fully contained.",
            "One classic game where often the Markov assumption is not respected as a game like poker, right?",
            "You may see some information about the game.",
            "There's some cards on the table, maybe some bets have been placed and now you're trying to determine what's the next state based on how you're going to play.",
            "That can be affected by a lot of prior information.",
            "An if you look at poker using a very narrow notion of state.",
            "Then you're probably not going to do very well, and so if you want to have a state that has sufficient information to predict the future for a game like poker, you probably need to fold in other pieces of observation from your previous play history and call all of that the state so the state can incorporate information about what other players have done in previous rounds of the game."
        ],
        [
            "And so the objective when you're dealing with a reinforcement learning problem is to look at some trajectories an over the trajectory, try to choose actions that are going to yield good reward, and so if you choose several actions that all yield good reward, then you're probably doing well.",
            "And so there's two classes of tasks, and depending which class of task we look at this notion of a return is defined slightly differently.",
            "So in the first class we are dealing with episodic tasks, so these are tasks where we know the task is going to end.",
            "There's a finite horizon.",
            "Things like games, right?",
            "You're going to play, and at some point the game will end, so we know there's an endpoint.",
            "In those cases, the return of the trajectory.",
            "So trajectory is the whole sequence of action from the initial state.",
            "To win the game ends the return of that trajectory is the sum of the rewards over the trajectory.",
            "In the case of continuing task, these are tasks where we don't necessarily know whether there's going to be an end point.",
            "Then we look often at an infinite sum, so we assume that the task can go on forever, and we still want the return to be the sum of rewards over that potentially infinite sequence.",
            "Maybe you're building a robot to juggle balls, and you don't want to say you know, and after 10 rounds of this, juggle, as long as you can.",
            "And so the danger in defining the return, as we've done for episodic task in the case of a task that goes on forever, is you could essentially have an infinite return, right?",
            "Infinite return might sound great if you're thinking about your financial investments, but in practice, for analysis of stability of these systems, infinite returns tend to be not so convenient.",
            "And so in the case where we're looking at.",
            "Continuing task with potentially infinite length horizon, we introduce the notion of a discount factor.",
            "So often it's always denoted by gamma by convention, and so gamma is this discount factor that is added in front of the reward as a multiplicative factor in front of the reward on each round, and it's increasing in the sense of gamma is typically below one, and so it reduces the influence of the reward."
        ],
        [
            "Overtime and so this discount factor always bounded below at zero bounded above at one.",
            "Typically we keep it close to one and what happens if we look again at this equation over here?",
            "If my gamma is equal to 1, I get back my return for the episodic task.",
            "If I set my gamma to zero, what happens?",
            "Is all of my future rewards essentially don't matter, and so I'm a very myopic agent that is only picking actions to maximize the immediate rewards, and I don't care about long term consequences and by balancing gamma between zero and one, I can essentially trade off.",
            "How much do I care about?",
            "Immediate reward versus.",
            "How much do I care about long-term reward?",
            "And so there's some natural intuition about this discount factor in some sense, right?",
            "If I were to offer you certain amount of money today, right?",
            "I meet you ioffer you $100 today, and then I ask you, would you rather get have $100 today or maybe $120.00 tomorrow, right?",
            "Some of you might prefer the $100 today 'cause you don't know if I'm going to show up tomorrow or not.",
            "And some of you have a discount factor close to one and are thinking well.",
            "I'd rather wait till tomorrow.",
            "There was a pretty trustworthy person.",
            "She's going to come.",
            "She's invited all these speakers.",
            "Let's just wait till tomorrow and get the $120.00 and so the discount factor allows you to mitigate essentially this these different these different considerations between short term and long term according to a particular criteria, yes.",
            "Kind of power.",
            "Yeah.",
            "Function.",
            "Yeah, so in in this case, right?",
            "There's sort of a geometric decay that's given by gamma.",
            "I.",
            "There is very convenient mathematical reasons why to put it that way, and in particular we can show convergence of that sum with this particular formulation of gamma, depending what decay rate you assume, you may not have convergence of that sum, and so it's convenient from that point of view.",
            "So most of the reinforcement learning theory has all been derived with this, but there's potentially other formulations you could look at."
        ],
        [
            "So one more piece of notation that is going to be very, very useful is the notion of a policy.",
            "So the policies typically denoted pie.",
            "I'm going to stick to \u03c0 and hopefully my fellow speakers throughout the week are also going to use pie to denote the policy.",
            "This is how we define the selection strategy.",
            "So I've alluded to the fact that you want an agent that picks good actions, and so we need to define away for a formulation for how is that strategy going to be defined.",
            "So this is the policy we can have a stochastic policy, so that means that we have a probability of picking.",
            "Different actions at a given state \u03c0 S of A is my stochastic strategy, or we can have a deterministic strategy.",
            "In the second case, so deterministic strategy.",
            "Then we have a mapping from state to action.",
            "So the policy is not part of the environment.",
            "The policy is the thing that you're learning when you're learning NRL agent, right?",
            "You're not given the policy.",
            "The policy is what you're trying to get out of learning and planning.",
            "And so there's different cases where you may want to have a stochastic policy or deterministic policy, and we're going to review this as we get through some of the more advanced concept, and so more specifically, what I want in terms of objective is really a way to pick the policy of the space of all policies.",
            "What is the policy that maximizes my expected total reward over the trajectory?",
            "So I have an expectation over the policy, so as I change my policy.",
            "I can expect to receive different rewards an I have a maximization over all the policies right?",
            "For those of you who might not have seen the argmax function similar to the Max, but it returns that argument which maximizes the function.",
            "So it says overall, the policy's return the policy that gives me the best expected reward."
        ],
        [
            "So just to make this a little bit more concrete, right, let's look at a familiar decision-making problem for some of you.",
            "I've cast it in very simple terms, right?",
            "You have sort of four different states you might be in through the course of your karere.",
            "Some of you at some point.",
            "Not very many given current conditions are going to be unemployed.",
            "Some of you are probably going to work in industry.",
            "Some of your may be working or will work in academia, and some of you may be.",
            "Currently in the future or have been in the past attending grad school.",
            "And so in these cases there's different choices of actions that you can take, right?",
            "You can do nothing, and doing nothing will usually keep you in the same state that you are including in academia, except if you're in an industry.",
            "If you do nothing, there may be some chance that you end up somewhere else.",
            "Over here.",
            "And so you can apply to industry and most of the time if you apply to industry from unemployed or grad school, right?",
            "You might end up in industry.",
            "But presumably the probabilities are not the same.",
            "And if you're in academia and you apply for industry, then that's pretty deterministic that you're going to end up over there these days.",
            "So we can add numbers to this right?",
            "We can add probability distributions to each of these actions and we can also add rewards, and I've posited some reward function over there, but the reward function may be different for different individuals and so now you can start seeing that the choice of what action to take really depends on that sequence of things, right?",
            "If you're someone for whom academia is highly rewarded, then probably you need to go through grad school.",
            "To make it to that point, and so in this case you need to take into account both the transition probability and the reward function to figure out what is the best policy for you.",
            "And so in this case, if I'm asking what is the best policy, there's several policies that I can look at right in each of my states.",
            "I have four choices of actions.",
            "An I have 4 States and so that means the space of policies is essentially four choices of action for the first 8 * 4 choices of action for the 2nd State Times Fortress.",
            "So we have essentially.",
            "A different actions to the power of S my number of states in terms of my space of policies, so that's a pretty large space of policy.",
            "And when I'm solving a reinforcement learning problem.",
            "Essentially, my objective is to pick the best out of all of these policies.",
            "In this case, I've given you the transition probability and I've given you the reward function.",
            "So from that point it's not so much a learning problem.",
            "It's more planning problem, right?",
            "And so in the next half hour.",
            "So I'm going to review some algorithms that are useful for planning under the assumption that we know the transition probability and we know the reward function.",
            "That's not really learning, but it turns out to be really useful to have this basis before we make things harder, and we start tackling the case where we don't know the transition probability.",
            "We don't know the reward function and we need to estimate those from data, but bear with me over the last half hour or so while we talk about the case where we know the transition of the reward function."
        ],
        [
            "So let's get back to our notion of optimality in this case, right?",
            "I mentioned that we want to look at the expected return of a policy.",
            "So I define the expected return of the policy that is defined at every state.",
            "So depending which state you're in, you have a different expected return over your trajectory, and so we've defined a notion called the value function.",
            "The value function, by definition, is this expected future return, and that's defined for.",
            "Each of my States and so a very simple strategy, which I've alluded to for finding the best policy would be the following.",
            "Step number one, enumerate the space of all policies.",
            "Right, so I have 4 to the four different policies.",
            "I'm just going to list them all out.",
            "In an attempt to figure out what to do with my life, then estimate the expected return of each one.",
            "So I can look at the expectation for a particular strategy with my expected return over all of this in this case.",
            "I would use a discount factor because there's no notion of like a terminal state right in the MDP that I've posited over here.",
            "There is no kind of state in retirement where you live there forever, so in this case, since there is no termination point, I would use a discount factor.",
            "So I estimate the expected return of each of my different policies, and then I keep the policy that has the maximum expected return right?",
            "And so this is a simple strategy.",
            "It has some limitations, in particular because the space of policies quickly gets very very large.",
            "As you have just a few more states in action, so it's not a practical one.",
            "But conceptually it's going to be sort of based on that that we build some more efficient algorithms."
        ],
        [
            "So I want to pause just for a second before we get into these algorithms and make sure that everyone is clear about these different terms that sometimes get a little bit thrown in together, right?",
            "I've talked about reward, right reward is the one step.",
            "Feedback that you get so for particular action, the particular state.",
            "I get an atomic reward.",
            "I've talked about return return is the sum of rewards over the trajectory.",
            "I've talked about value.",
            "Write the value is the expected return over the trajectory, so the return maybe 1 sample along a trajectory, but value is in expectation.",
            "So if from a particular state I take several of these trajectories an I get a number that is my expected return.",
            "If you have a deterministic system then the return the value are going to be the same, but if you have a stochastic system then one trajectory will give you a sample of your return, but then when you run many trajectories you'll be able to estimate your expected.",
            "Value.",
            "And finally, we have a notion of utility, right?",
            "I didn't talk about utility very much in reinforcement learning.",
            "We essentially assume that the utility is the same as the return.",
            "And so we assume that we have a linear concept of returns, so the return is the sum of the rewards.",
            "In many other cases of decision theory there's a distinction between value and utility, and utilities can be nonlinear.",
            "Utilities can have different types of functions, and so for reinforcement learning we always make this assumption that value function is essentially utility is the same as value.",
            "We're not going to get into more complex notion.",
            "Of utility for the purposes of this, there's some of the reinforcement learning literature that does get into that space.",
            "For example, methods that are sensitive to risk or will have at the end of the week notions of safe MVP's.",
            "In that case, we look at slightly different notions of value, but for the purposes of today, probably the first couple of days, you can make the assumption that utility is the same as your value, yes, so in some sense, when you look at the discounted reward is one way to translate return issue.",
            "Exactly, yeah, and that's a very common way that we've looked at.",
            "Now that I should say that's coming in the literature and the you know everything I'm presenting here is essentially known in the literature for the last 40 years or so, yeah.",
            "So let's look a little bit more closely."
        ],
        [
            "The value of a policy."
        ],
        [
            "In particular, line number one over here is the definition I've given you so far, right?",
            "The value of a particular policy at a given state S is the expected value over that policy for the sum of reward given that you start in a particular state S. So I can rewrite this equation by just splitting up the first term compared to the other terms.",
            "And now in my first term, all I want is to get the expected value of my immediate reward.",
            "The expected value of my immediate reward depends on what's the probability that I'm going to take a particular action in that state.",
            "And what's the reward I get for taking that action?",
            "And so this pie activate.",
            "That's part of my definition of the policy and over here I've left all the expected sum of rewards from the step plus one, and they're on, and so it's very common to look at the value function in, split it up this way.",
            "Have the notion that you have a first term.",
            "That's the immediate reward, and then you have another term.",
            "That's the future reward, and so I've bundled up all the future rewards together, and I partitioned out just my first reward an when I do that, I can start looking.",
            "At my expectation over here, and essentially I'm trying to get a recursive formulation for the value function to make it more concise, 'cause right now my value function determine is depends on the full future an I'm trying to get a recursive formulation for that, so if I look at this term over here."
        ],
        [
            "Expectation with respect to my policy Pi of my future rewards starting at T + 1, but I'm still conditioning anesti so if I compare the term over here in the term over the same except in this case I started nesti and I have RT.",
            "In this case I start in St and I look at RT plus one.",
            "So I need to do a little bit of work to express this expectation condition on starting at T + 1 and the way to do that is simply take into account what's the expectation?",
            "Of where I'm going to be, what's going to be my S T + 1 and so to do that we just need to fold in the transition probability I need.",
            "I'm in a state S at time TI want to know where am I going to be at time T plus one well I need to factor in my transition probabilities and the probability of taking each of my different actions.",
            "So there's a probability that I can take each of these actions that's defined by my policy and given which action I took, there's a probability that it's going to take me to a new state."
        ],
        [
            "And so after I've folded in this one step expectation, the term over here is in exactly the same form as the term I had when I started with.",
            "So I can call this V Pi of S prime whatever state I'm going to be in.",
            "And now I have a recursive formulation where my value function at this time step depends on my immediate reward, the transition probability of being a new state, and the value at that state.",
            "And so this is essentially a form of a dynamic programming algorithm where the value at each of my state of the variables for that dynamic program.",
            "Yes, I'm confused here, because your horizon just went shorter on the.",
            "For the 2nd and so why?",
            "Why would it be the same value function if it's a shorter horizon?",
            "So in this case the horizon is defined over the lifetime of the system, so it's the expectation over that lifetime and so.",
            "In this case, or maybe it may be a deterministic right, maybe set or maybe a random variable and you can redo the same derivation with the discount factor in there, and then probably you'll be bothered a little bit less.",
            "Yes.",
            "It's not the most completely answer.",
            "Typically when you do it with finite horizon you actually keep time index value function, and if you want to prove convergence, you're going to need to take into these account.",
            "These time time index value function.",
            "But if you have the discount factor, then you don't.",
            "Yes.",
            "A policy and a trajectory good.",
            "So the pharmacy is just a set of projectors policy is like a policy is a strategy for acting, so the policy will have to specify in all states what is the action that I should take or what is a distribution of actions that I should take.",
            "In contrast, the trajectory is 1 particular experience through that space.",
            "So if we go back to my example about what occupation should you follow, write a policy, maybe when you're an unemployed, go to grad school when you're in grad school, apply for a job when you're in industry, keep on applying for other jobs, right?",
            "That would be a policy now.",
            "A trajectory might be.",
            "I was unemployed.",
            "Then I went to grad school.",
            "Then I went back to being unemployed.",
            "Then I went back to grad school.",
            "Then I ended up in industry.",
            "Right, it's one sample path, and that path depends on your choice of actions.",
            "That's the part that's defined by the policy, but it also depends on the stochastic effects.",
            "And so that's the expectation, the transition probability."
        ],
        [
            "So in this case I've just rewritten over here the value function we have for fixed policy.",
            "This doesn't take into account searching over the space of all policies, it's just for a fixed policy, and this particular case we sometimes have an alternate formulation that's called the Q function and the Q function, in this case corresponds to the value of a particular action, and so, whereas in this case I'm folding in the policy Pi essay in the Q function the policy.",
            "Is used as an argument, and we define it for each possible action, but the first term is the same.",
            "We have the immediate reward for doing a particular action in a particular state S we have gamma discount factor.",
            "We have the transition probability and then we have the future rewards.",
            "The future value function.",
            "So both of these are instances of a form we call the Bellman equation.",
            "The bellman equation always has these kinds of dynamic programming recurrence form where you have your value function or your Q function on one side.",
            "And the value function of the Q function of the next state on the other side, and it comes in different forms and flavors.",
            "But it's always going to have these kinds of structure with the immediate reward pulled out the future returned appearing here and the transition probability appearing also over here."
        ],
        [
            "And so if we look at this equation over here and I've gone back in this case to the formulation with the discount factor gamma, if we assume that your set of states is finite, and we assume that our set of actions is finite, we can actually see this as just a set of linear equations, right?",
            "We have S different equations, one for each of our States and we have equations, essentially with different terms depending on the value of all the future states.",
            "I could end up in.",
            "And so seeing this as a system of linear equation, we can actually write this in matrix form if we want.",
            "So in this case, via Pi is just a vector, is the value for all of my states are \u03c0 is another vector, is the reward for each of my states.",
            "My transition matrix that's going to be an S by S square matrix an.",
            "I have the value for all of my states also, so I can solve this as a system of linear equation.",
            "Subject to some certain conditions, these conditions turned out to be a little bit problematic.",
            "In particular, this matrix over here is not always well conditioned, and it's not always possible to do it, but so we don't typically solve systems this way.",
            "I bring it to your attention just so that you understand a little bit better than the nature of these dynamic programming equations.",
            "An also to insist on the fact that this is for a particular this condition on a fixed policy.",
            "We haven't yet tackled the problem of.",
            "Looking at finding good policies, but if you're going to go with my simple strategy of let me enumerate all my possible policy and now let me evaluate all of my possible policy.",
            "This is 1 method to evaluate these possible policies, because solving this exactly, and I've lost the mic.",
            "OK, so because we may not want to solve things exactly, this matrix maybe ill condition there is some."
        ],
        [
            "Convenient iterative methods to do the same thing again, drawn from dynamic programming because it's a dynamic programming algorithm, we can actually start with an initial guess of the value function.",
            "The reassuring part is you can the choice of that initial value function doesn't matter at all.",
            "In the case where you have a discrete set of States and actions, and so you should have some initial guess of what your value function might be when popular guesses to set it all to zero and other popular guesses to set it to the reward function.",
            "And now over several iterations, you're going to update the value for all of my States and so in this case I've indexed them by, which round which iteration you're on, and so at round K plus one, the value is going to be the immediate reward.",
            "Plus the future expected reward future expected value based on your current last round estimate of the value function at all these other states.",
            "And over several rounds, this is going to converge.",
            "And you can assess convergence because the value functions are going to stop changing or they're going to change less than some threshold value that you've put in.",
            "So in this case it's guaranteed to converge for simple reasons, in particular because you have this discount factor over here, and the effect of this discount factor we already alluded to is to sort of shrink how much you care about the future, and so that means on every round of this die."
        ],
        [
            "NMK programming you can actually show that the difference between the value function from one round to the next has to shrink.",
            "So if you define the value function at round K + 1 compared to the value function of your estimate that you have previously, then you can actually separate these two terms out.",
            "Of course, the initial reward terms cancel out, and you're left with just the difference between these values at the next round, and these two values are multiplied by your discount factor gamma.",
            "And because this gamma is.",
            "By definition less than one or it's less than one so that we achieve contraction property.",
            "Then you can actually show that you're going to shrink your difference between value functions on each round, and so over.",
            "Sometime this will actually go to zero and so we have convergence.",
            "Now this is for the case where you have discrete States and actions.",
            "In the case where you have continuous state and actions, things get a little bit more complicated, but in the discrete case is things are quite nice.",
            "And this is for the case with discount factor.",
            "If you have finite horizon episodic task we can also show that we have convergence, but we don't have the discount factor.",
            "So in that case in relation to my comment earlier the proof of convergence goes through the fact that you're keeping different value function estimates for each of the different time horizons."
        ],
        [
            "So now let's step back from the case where we assume we fixed the policy.",
            "This was the case where I fixed my policy and look at the case where we actually are looking for a good policy.",
            "So here there's a notion of an optimal value function I'm going to denote this V star.",
            "This is the best value function amongst the value of any of my policies.",
            "So in my simple approach I'm enumerating all of my policy looking at the value for each of them.",
            "Restart is the one that returns the best one over all of my policies.",
            "So any policy that achieves this greatest value is called an optimal policy, and I'm going to use paisar to denote an optimal policy, and so we have some results dating back to the Seminole work of Bellman that says that for each MDP there's actually a unique optimal value function.",
            "But at the same time, the optimal policy is not necessarily unique, and that's not very hard to understand.",
            "You can have two policies by two choice of actions that give you the same value.",
            "So in that case they both have the same value.",
            "It's the optimal value restarts, but there are different notions of policy.",
            "We can have multiple optimal policies.",
            "One optimal value function."
        ],
        [
            "And there's a nice correspondence between the optimal value and the optimal policy, and I've put in the equations and really small font here, 'cause I don't want you to look at them, but they are there in case you want to go back to the slides and puzzle through them later.",
            "But in this case the equivalence I want you to understand is the fact that if we know the value function V star, and I haven't told you yet, an efficient way to get it.",
            "I told you a very naive way to get it, but if we have the optimal function V star and we know the reward transition discount factor.",
            "Then we can compute the optimal policy very easily, easily in terms of like number of computing operations.",
            "Similarly, if we know the optimal policy.",
            "Then we can recover the optimal value function equally easily, and so there's a simple correspondence between these two.",
            "So the reason I emphasize this as I'd like you to think of both the staran pie stars as solutions to the MDP, but there's sort of alternate solutions and we can talk later about whether you know you're better defined the star and then calculate \u03c0 star from that, or find \u03c0 star and calculate the star from that.",
            "But both of them are essentially solutions to the problem.",
            "Question was confused.",
            "Define V star as a portion of S. Yes.",
            "Perhaps it could be different policies which achieves the maximum.",
            "So what you mean by the optimal policies?",
            "The policy is defined as if you think of the deterministic case, then the policy is a mapping from state to action.",
            "So for each state you need to define what's the action that's taken that spy star and that action is going to have a value that's V star.",
            "Saying the same optimal policy would give the.",
            "The optimal value function for every.",
            "Yes, 'cause the policy is defined at each state write.",
            "The policy isn't like 1 action that you use in all states.",
            "The policy says for each state.",
            "Here's the action you need to take.",
            "And so the policy is that full mapping for all the states.",
            "But it's possible that at a particular state, by there's two actions that give you equally good reward.",
            "So that's why we say that the optimal policy is not necessarily unique.",
            "But then the optimal value is."
        ],
        [
            "OK, so let's finally get to the stage where we're going to find a good policy.",
            "I'll present a slightly less naive algorithm, not a whole lot less, and then a slightly less naive one, and then colleagues will follow me throughout the year that we can present some really smart algorithms to do this.",
            "So I'm presenting the fundamentals here today, which are really used for small problems as well as theoretical analysis.",
            "So the first algorithm is called policy iteration.",
            "We're not very imaginative with the names in this field.",
            "There's not going to be 150 flavors of Gans.",
            "There's going to be value iteration and policy iteration.",
            "So in the case of policy iteration, we assume that we start with a random policy.",
            "So let's assume in each state Ridge is going to randomly pick something to do so.",
            "If I'm unemployed, I will randomly pick something to do.",
            "Maybe I'll apply for a faculty job and see how that goes.",
            "And then you're going to interleave 2 steps in iteration.",
            "So step number one is going to be to compute the value of that policy.",
            "So my initial policy, if I specify it is going to say for each of the states.",
            "Here's an action to do.",
            "Now I'm going to compute V \u03c0, and I can do that in closed form using the linear system of equations.",
            "We rarely do that, but you can, and you can do that with iterative method using the iterative policy evaluation method I just presented.",
            "And once you have the Pi as the value of that particular policy, then I'm going to compute a new policy Pi prime.",
            "That's going to be greedy with respect to that particular policy, V. \u03a0, and so in this case I'm going to actually.",
            "Do this in iterations until I'm done an.",
            "My policy doesn't change anymore and if I do several rounds of that, I can actually show that this is going to converge for the simple reason that I have a finite number of policy, so I can in the worst case go back to my naive algorithm, which is to enumerate them all.",
            "And thank you.",
            "Just going to make sure I don't want out.",
            "We ran out of batteries on one thing we don't want to run out of batteries on two things in the same stock OK, and so in this case I'm going to stop in the case where my policy doesn't change anymore.",
            "And so there's a few things in terms of computational cost for this.",
            "One of them is you need to think about what's my computational cost for computing the pie.",
            "One of them is what's my cost for computing that new policy.",
            "That's better policy is going to be better is going to be an improvement in some States and I need to think about how many of these iterations do I need to do before I'm going to terminate.",
            "So in the worst case, right, my number of repetition of round, maybe the number of policies that I have to consider.",
            "So that's something to worry about.",
            "The alternate methods for doing this is to say why am I bothering with computing the policy on every round?",
            "Let's go back here, right?",
            "I'm computing a value function here now I'm extracting a policy.",
            "I'm computing a new value function, so on.",
            "Let's take inspiration from the algorithm we have for policy evaluation, right?",
            "The method with the iterative method we have an instead of.",
            "Computing that only for fixed policy.",
            "Let me fold in this."
        ],
        [
            "Improvements step within that and so this is what value iteration essentially does.",
            "It's taking our Bellman equation and turning it into an iterative improvement.",
            "Rules so we start with some initial value V0 for all of our States and then on each iteration.",
            "This looks a lot like the equation we have for policy evaluation.",
            "The only modification is that on each round, instead of updating my value based on just the reward and the next value for a particular fixed policy, I'm going to fold in my greedy improvement.",
            "So my step of maximizing over my actions right in that.",
            "Update rule so for each of my states I will look at for all possible actions I can take for that action.",
            "What reward would I get?",
            "Where does it take me to an what's the value of that new state in this algorithm stops also when the change of value between two iterations is below some threshold and the algorithm is going to converge in the case where you have your gamma discount factor that imposes contraction on it.",
            "So we."
        ],
        [
            "Essentially three related algorithms, right?",
            "The first one policy evaluation fixes.",
            "The policy can only estimate the value.",
            "Algorithm #2 it finds the best policy at each state.",
            "And then mixes rounds of policy evaluation, which is algorithm 1 N rounds of greedy improvement in value iteration instead essentially folds in the greedy improvement right into the policy evaluation.",
            "So does all of this in a combined update rule to find the optimal value function.",
            "And so we can look briefly at the time complexity of each of these steps, right?",
            "We can think about how we're going to do policy evaluation.",
            "There's different ways to look at the complexity of that, but if we look at the simple case, which is for solving the system of linear equations, then that's cubic number of operations in terms of the number of states.",
            "If we look at policy iteration that has the policy evaluation step in there, so the cube term is over here.",
            "And then greedy improvement essentially requires you to estimate the Bellman equation once for every state.",
            "It.",
            "If we look at value iteration, you're essentially finding that greedy update rule without doing full evaluation, so you save yourself some operations.",
            "You have a per iteration complexity of S ^2 * A, but that's kind of obscure skating.",
            "A really important factor, which is how many iterations do you need to do that right?",
            "And so people often wonder, am I better off using value iteration?",
            "I'm better off using policy iteration in terms of per.",
            "Iteration cost value iteration is definitely better than policy iteration.",
            "If you do it the naive way.",
            "What often happens in practice?",
            "This policy iteration requires many fewer iterations.",
            "Then value iteration.",
            "But we don't have really good bounds on that in terms of we have very loose bounds on the number of iterations, But we don't have very good results that show us for specific problem which will be faster or slower.",
            "Back from.",
            "The value iteration.",
            "Yes, exactly.",
            "So that was the point of my slide with the tiny equation font, which is if you have the optimal value, you can extract the policy quite easily and it's not very hard to see if I look over here at this equation when things have converged.",
            "If I run this equation with an argmax operator instead of a Max over here.",
            "So instead of finding which action maximizes with the maximal value, I ask which action maximize this particular equation.",
            "So this equation here the complexity of evaluating that.",
            "Is a * S squared right?",
            "For each state?",
            "I need to look at each possible action and then I need to see where are all the next states I might see, and so that's the complexity of this.",
            "Retrieving the last policy, but I do that just once at the very end.",
            "The two will converge to the same optimal value function, right?",
            "But we know there can be multiple optimal policy, so they will converge to the same value function under some conditions, but those conditions are met for discrete state in action spaces for continuous state and action spaces.",
            "I can't run these kinds of Belmond iteration because I might never see the same state again.",
            "Then we need to we need to handle that case separately, but for discrete and often people will talk about tabular reinforcement.",
            "Learning the tabular is the case where you can discreetly nominate this statement actions, and you can essentially.",
            "Keep your values in a big table.",
            "Yes.",
            "There is an optimal spelaeus policy that is not stochastic, yes.",
            "So in the case of tabular, so discrete state in action spaces there is an optimal policy.",
            "There may be more than one optimal policy, but there's an optimal deterministic policy, so we talked about deterministic versus stochastic, and I've been mostly string in the space of deterministic policy because for these particular cases deterministic policies are sufficient.",
            "Now there's reasons you may want to look at policy.",
            "That are stochastic.",
            "Peter.",
            "A deal will give you many good reasons this afternoon.",
            "Why that's something that in some cases very sensible to do particular deals with the fact that when you're dealing with larger state action, space is not feasible to enumerate all the actions.",
            "It's not feasible to look at all the policies and then having notions of gradients.",
            "How much policy improvement can I get in using gradients to do that?",
            "It's a lot easier to do with stochastic policies and deterministic policies."
        ],
        [
            "Let's look at a simple example just to get a taste for how these things go.",
            "This is a really simple agent that moves in the different Cardinal directions.",
            "This particular agent is limited to a grid of size 3 by 4.",
            "Usually start somewhere up here and if it ends up over here, it gets a plus one reward and it but falls in the dump over there gets a negative 10 rewards, so you can imagine that it wants to get here can start in.",
            "I can move in four different Cardinal directions up, down, left, or right.",
            "Those are it's for actions, whether stochastic actions, so there's usually a 70% chance going to end up where it wanted and a 10% chance is going to randomly go in one of the other directions, accidental E. And so in this case we're using a pretty high discount factor.",
            "Close to one I should say, and so point 99.",
            "So that means that there's a little bit of discounting, but not very much."
        ],
        [
            "We start value iteration at round Number one.",
            "I've initialized my values to be equal to the reward function on round Number one.",
            "Ann, I run this."
        ],
        [
            "For one round after one round of value iteration, this is what the values will look like, right?",
            "If I run my valuation algorithm?",
            "Essentially, it's saying in this state, let's look at all the actions I can take for each of these actions.",
            "Let's see where I can end up and what's the value of ending up in there.",
            "So for all of the states over here, they can end up in neighboring states, but all those states had an initial value of 0, and so the value stays 0 even after one round of value iteration.",
            "We see the neighboring states which had neighbors with rewarded with value information.",
            "Pick up some of that value information.",
            "In this case, it seems like potentially you know the best action to do might be to avoid the negative 10 right to move left, and in this case the best action might be to move downwards to be able to avoid these kinds of."
        ],
        [
            "That place is if you run this for several rounds."
        ],
        [
            "At some point the value changes and converges to some amount that is quite consistent.",
            "What I've plotted down here is what we call the bellman residual.",
            "That's the maximum difference in value over all of the States and so in this case we see that's a phone number that's essentially getting smaller and smaller because of that discount factor that's imposing the contraction, and so after some time we have a value function that is plotted over here and now we ask, how do we extract the policy for this particular value function?",
            "And to do that you need to apply your Bellman value function again.",
            "But it's quite intuitive to look at what the policy might be for doing that.",
            "I usually have like a plot of the policy, but I see I didn't put it in today.",
            "Essentially, most of these states, right?",
            "The policy is to go right towards the plus one.",
            "In some cases though, the policy over here in particular is to go left.",
            "Because if you go through the top over here, there's a pretty good chance you're going to end up accidentally falling in the pit, and so the policy over here is to move to the left.",
            "In this case, you can just look at it greedily by saying for each state you know which neighboring state has the higher value, and the policy is going to be imposed in that direction.",
            "That's essentially what's going to come out of running the Bellman equation for that.",
            "And you."
        ],
        [
            "Do that for larger domains, right?",
            "The four rooms domain is another one that's commonly used when you do your value function updates.",
            "If you're trying this on your own at some point, it's usually a good sign.",
            "If your value information is sort of radiating away from the reward points when you initialize your value function to be equal to the rewards.",
            "So there's a way to sort of easily diagnose whether your value iteration algorithm is doing what it should should be.",
            "Sort of radiating away from the points of reward over here.",
            "There's reward over here.",
            "The reward gets propagated away from the goal, and if you have a more complex value function with different reward points, you see these same kinds of effects.",
            "The reward information propagating through."
        ],
        [
            "I've presented these notions of value, function, value iteration and policy iteration in pretty strict.",
            "Terms, it turns out that you have quite a bit of flexibility on some of the fronts in terms of how you implement this.",
            "So for example, for value iteration it's possible to do what we call asynchronous updates, right?",
            "Maybe you already know that some states are a lot more important than other states, so let's not run these Bellman backups synchronously through all of the States and do the same number of passes through all of the states.",
            "There may be a way to establish a priority order.",
            "Maybe excuse me, maybe you can generate some trajectory through your MDP as you generate the trajectories.",
            "You get some statistics about which states are visited.",
            "Often you do more value backups around those states.",
            "If some states are never visited, it's probably not worth wasting computation on them.",
            "You can also choose to update the states whenever they appear on the trajectory and so on."
        ],
        [
            "And it's the same with generalized policy iteration.",
            "So if you want to do policy iteration right, there's two steps in policy iteration.",
            "One is when you do the policy evaluation and one of them is when you do the policy improvement, you don't need to systematically do evaluation.",
            "An improvement on all of the states at every round.",
            "You can sort of mix improvements and evaluation steps in a non uniform way that represents some other information you have about the domain.",
            "And so this gives you some flexibility and they can be the way to get drastic improvement in terms of time complexity for applying this on some larger domains."
        ],
        [
            "And so let's step out for a second about these very simple cases that are the fundamentals of reinforcement learning and talk a little bit about what are some of the harder challenges that we face right?",
            "One of the things that is.",
            "Quite difficult in many cases, and I'm not going to be able to give you too many solutions on that is how to design the problem domain, state, action and cost.",
            "Reward one of them which will start talking about in a few minutes is this.",
            "Notion of incorporating learning into this.",
            "So far I've really been in just planning and solving.",
            "How do we incorporate in our data and learning?",
            "And then we'll get to some of the later topics down here, but it is a good time to take some questions, yes.",
            "When what sorry?",
            "When facing up problems, how do you choose gamma?",
            "Yeah, so.",
            "For many years I thought that gamma should be part of the environment.",
            "Someone should tell you what gamma is, right?",
            "I'm not designing the environment, I'm solving the system, so someone who's specifying the environment should tell me how to discount my future versus my present.",
            "And whenever you're in the case of planning so the case I've described so far where your transition probabilities are rewarded specified, you should also expect someone some problem designer in along with the first bullet to specify your discount factor.",
            "When we're moving to the case where we are dealing with learning, so assume that your transition probability, your reward function are estimated from data samples.",
            "And then there's a recent results in the literature that that are suggesting that you need to think of gamma as a hyperparameter, and in particular, if we look at our Bellman equation that's balancing immediate versus future.",
            "If my transition probabilities are poorly estimated, my value function is poorly estimated.",
            "Maybe I should use a more aggressive gamma to reduce the variance that comes in from poorly knowing the future values, and so using gamma as?",
            "The more hyper parameter that I can tune based on how complex so I want my plan to be versus how much data do I have is one thing that's really starting to get a little bit more traction in the last few years, and there's a few interesting papers and that's two to three years on this.",
            "Yes, you learn what?",
            "Scammer.",
            "Anne.",
            "In some sense, yes.",
            "If you buy learn you mean like estimates, right?",
            "In some cases in some models, if you think of supervised learning as having Lambda, your parameter that regularizes that weighs how much regularization you want, right?",
            "If you have a loss function that has like a.",
            "Supervised criteria and then you have your model complexity that swayed by Lambda.",
            "Usually you'll use maybe cross validation to estimate Lambda, so in that sense you can learn it, you can adapt it in a way, and so I think we don't have all the answers of how to adapt gamma yet in reinforcement learning, but I think that intuition is starting to permeate and there's going to be interesting results coming through.",
            "Yes.",
            "We want function was defined when you are leaving a state of doesn't make a difference in the formulation.",
            "If your reward is defined as you're entering a state.",
            "Yeah, so there it doesn't make a difference, it's just you need to be a little bit careful handling how you take the expectation, right?",
            "So in some most of the literature.",
            "Right, they define RSA.",
            "In some papers they define Rs S prime.",
            "So it's a function of where you are, what you took in, where you're going, and eventually you could define it as a function of a S prime, which is where you're going.",
            "All of these can be made equivalent by just taking expectation appropriately with respect your transition probabilities.",
            "In some cases, maybe just Rs, and in that case you assume that the reward just doesn't depend on the action.",
            "OK, I will make some grounds and we'll have time for questions at the end so."
        ],
        [
            "Let's talk specifically first about learning.",
            "In particular, how do we do on line reinforcement learning?",
            "Any?",
            "Many cases where you don't have your transition probability or reward function in advance, you need to acquire this, right?",
            "You need to take some actions, see how well you do get that information and leverage that.",
            "In some cases the TD Gammon system was based on that there was an agent playing against itself through many millions of games to be able to evolve a good function, and so in that case you have what we call the reinforcement learning loop, and that's probably one of the things you'll see on the more practical, hands-on session.",
            "There has to be a notion that you can.",
            "Take an action, observe the effect of that action so that action causes a transition in the system.",
            "You get to observe what's the new state you're in?",
            "What reward did you get for that action, and then you adjust something.",
            "What you adjust changes depending on different paradigms, but you can adjust your policy.",
            "You can adjust your value function.",
            "You can adjust your Q function.",
            "You can adjust your model of transitions and rewards, but there's some adjustment is.",
            "There's some learning that's going to happen, and then based on that learning, you're going to be able to take a new action.",
            "So most human animal agents operate in this kind of a loop, and so there's been quite a bit of work and reinforcement learning that is on what we call more the on line learning."
        ],
        [
            "Case, and so there's a few different classes of approach for online learning.",
            "One of them is based on very simple Monte Carlo estimation principles, which is to say we are going to look at samples of our empirical returns.",
            "So I go back to this notion of you as my empirical return an I'm going to look at the difference between what's my empirical return, which I'm observing for this trajectory, so I'm going to fix my policy, run a trajectory right, like play one game of backgammon, see how it goes.",
            "Estimate my return.",
            "You have S and now compare how far this return that I've observed on.",
            "This trajectory differs from my current estimate of the value function for that state.",
            "Call this my error signal.",
            "I'm going to weigh that by learning rate.",
            "We're going to call this Alpha and we're going to update our value function in proportion to this area signal.",
            "So this is essentially just an error update rule right here.",
            "Alpha can be thought of as the learning parameter.",
            "You is your actual prediction.",
            "Your actual observed sample envy is what your model predicted, and so you're going to do some correction based on that.",
            "So this is sort of a gradient rule, but just look like it doesn't require you to know the reward function.",
            "It doesn't require you to know the transition probabilities.",
            "What it does require you to do typically is run many, many rounds of this, because typically this.",
            "Empirical return is over the full trajectory, so if you have a lot of stochastic city in your system, you're going to need many trajectories to properly estimate this particular return.",
            "So these methods, in particular when the planning horizon are along when the trajectories are along.",
            "This method tends to have quite a bit of variance, and so you need a lot of sample to estimate it properly, but we can show that it actually gives you an unbiased estimate of your value function V when you do this.",
            "Jeanette Lee there is this."
        ],
        [
            "Acting class of method called the temporal difference learning methods and written this afternoon will tell you much more about this particular class of learning.",
            "The idea of temporal difference learning is to say.",
            "Instead of waiting till the end of my trajectory to estimate my return.",
            "Let's try to estimate that return based on just based on essentially a Bellman equation.",
            "So instead of estimating you at the end of the trajectory, I'm going to replace this by the immediate rewards.",
            "I'm going to take the one step sampled reward, and then for the future reward.",
            "I'm not going to wait till the end of the trajectory, I'm just going to plug in my current estimate of the value function at the next state and discount that so the V over here is.",
            "This corresponds to the V over there, right?",
            "That's my current prediction.",
            "Of what's the empirical with the expected return at State St?",
            "And this part over here.",
            "Together, that is my empirical observation of what it is.",
            "Now, one thing to note is that my function V shows up sort of on the left side of my error signal and it shows up on the right side also.",
            "Right, if you do this for supervised learning, it looks more like this, right?",
            "Maybe you're trying to do regression with a gradient descent approach over here this you would be within my regression.",
            "What is my actual output?",
            "Why that I'm observing in this be would be?",
            "What did my model predict the why should be and I look at the difference between those two.",
            "So in the supervised learning case EU is a sample and V as predicted, but one of them is the ground truth, right U is the ground truth and.",
            "Most of the time we assume that this is correct.",
            "When we do TD Learning V over here is produced by our model.",
            "Our current estimate of Y, and now this part.",
            "That should be the ground truth.",
            "Well, there's one little bit of ground truth which is R. And then there's this piece V that's also our model.",
            "And so when our model is poorly estimated when we haven't seen a lot of data, this estimate of the error tends to be very noisy.",
            "And so in many cases this TD learning approach.",
            "Can lead to instability, not in the case where we have discrete set of states.",
            "If we're in the tabular case, discrete discreet actions, things go quite well.",
            "We can actually show that we have convergence and so on.",
            "But when we start dealing with very large continuous state space, we'll see that we have stability problems, and many of the things that have been, I would say put forth over the last two years in terms of the Deep Q network and so on.",
            "Really look at how to have better stability.",
            "On this kind of estimator specifically, for this reason that the V function shows up on both sides and it gives you for stability, yes?",
            "Yeah, it's because at STO the RT plus one.",
            "This is the immediate reward.",
            "I should probably use RT over here, but then it's the immediate reward.",
            "Plus gamma times the value at the next one.",
            "Those two composed together these two pieces together correspond to UST, right?",
            "That's my current estimate of what's the empirical return, so I'm going to replace just the immediate reward, bias sample observation and the future.",
            "I'm going to plug in my current best estimator for the value function.",
            "So this team."
        ],
        [
            "That is essentially what was used in TD Gammon, with the addition that the value function wasn't expressed in a tabular case, but the value function was actually expressed with a neural network and will get to that in a second.",
            "But in cases where your number of states gets too large and you can't list the value function for all of them, you can actually replace that by a function estimator."
        ],
        [
            "So let's talk about this question of function approximation particular."
        ],
        [
            "Function approximation is used in all these cases where you have too many states or you have continuous states where you can actually go and list the full set of states.",
            "So in robotics, in most games this is crucial to getting good results."
        ],
        [
            "And so if we look at how to do this?",
            "Essentially what I'm positing is that my value function or my Q function can be replaced by.",
            "Approximation, and here I've put in sort of the simplest form of approximation that we consider, which is a linear approximation, and so in this case I'm going to assume I have a set of features.",
            "These features maybe features of the board right where my pieces on the board, how many of such and such pieces do I have?",
            "These features can be designed manually.",
            "For some games, an now instead of learning my function Q or V directly, I'm going to learn these linear coefficients.",
            "Such that when I take the linear combination of my features in my weight stayed are my weights that I'm estimating then I get a value function.",
            "So tabular case when we can list the States, we estimate V directly.",
            "Cases where I have large state space, I assume that there is a functional form to define my value function and I estimate the parameters of that."
        ],
        [
            "Function and so the linear function is the simplest one that has been used for many years.",
            "Over here I'm picturing one of the classic problem of reinforcement learning.",
            "They mounted car domain where you have a little car that's trying to get up a Hill and it doesn't have quite enough acceleration to get to the top.",
            "So you need to kind of go back and forth a few times to build up momentum.",
            "In that case, the state space.",
            "Is usually the position of the vehicle along the curve as well as its velocity.",
            "So you have a 2 dimensional state space.",
            "It can be discretized or continuous.",
            "An OPT in.",
            "We've used linear function approximation to represent the function of different actions.",
            "It's usually cast in this case, not with a continuous acceleration in terms of actions, but with an off acceleration.",
            "So you have binary action choices in this case, so you can have a different linear function for each of your actions."
        ],
        [
            "As we get to larger and larger domains and will see many of them this week, you need fancier function approximation because you have larger state spaces and more complex representation are needed.",
            "So a lot of the work that has been published, for example by some of the deep mind researchers and other people active in deep reinforcement learning, looks at taking as your state representation.",
            "So I'm really raw version of your observations.",
            "In this case, the case from the Atari learning environment, where the goal is to learn to play several different.",
            "Atari game, so in that case the input space, the state space is really in the space of pixels.",
            "And rather than look at linear function approximation, we use a convolutional neural net to capture the value function so the convolutional neural net essentially evolves a representation of the features from the raw input space and the output of the convolutional neural net is not a label.",
            "It's not like is this PAC man or not, PAC man, it's really a value function, so it's the value for a particular state in the system.",
            "We can learn that an from that value function extract an optimal policy when we've done that."
        ],
        [
            "An as deep reinforcement learning has been broadening to other games that are you learning environment?",
            "For example, there's been some work on using reinforcement learning in Minecraft.",
            "You might get a demo of that later this afternoon.",
            "You can incorporate many of the newest technology from deep learning, for example, notions of memory and context in attention, and so on can all be incorporated into that representation of the value function, and what changes is that the output of the value function is no longer label.",
            "But it's an estimate of the value function for that particular state, and so as research progresses on deep learning architectures and models, reinforcement learning leverages that in most cases.",
            "Through the use of more complex models for approximating the value function."
        ],
        [
            "Let me close off with just a few bits and pieces.",
            "I call it the IRL lingo.",
            "Is these kinds of terminology that comes up which when you've been in the field for five or ten years, you've kind of developed the understanding of all this.",
            "But if you haven't been in the field, it's useful to get a sense of what these terminology might be because it will pop up in several of the talks, probably in the next few days.",
            "I've already talked about episodic versus continuous task.",
            "I've also already talked about batch learning versus.",
            "I'm learning mostly I talked about online learning and I didn't talk much about batch learning.",
            "You'll hear more about it later on, but let me go through a few of these also quickly.",
            "In the five minutes we have left, one of them is the difference between an on policy reinforcement learning system and an off policy reinforcement."
        ],
        [
            "Bing system that goes back to something I mentioned early today, which is that when you change your policy in your RL agent, when you change how you choose your action, you're essentially inducing and you distribution over the states, right?",
            "You change how you play the game.",
            "You're going to see different States and so in some cases.",
            "You can actually show that the data distribution changes every time you change the policy, and one of the challenges with learning from a batch of data is that this batch of data was collected under a particular policy, right?",
            "Your agent had a particular strategy choice of action for collecting the data.",
            "Now if you tried to do reinforcement learning, it's possible that you're estimating a different policy.",
            "Think of the case where you're doing policy evaluation.",
            "You're estimating the value of a different policy.",
            "But you're trying to do that with data that is distributed according to the data collection policy, right?",
            "What we call the exploration policy, and if these two policies start diverging too much, then your ability to properly evaluate the policy your candidate policy is going to weaken as your distributions become further and further apart, and so one of the consequences of that is typically you need very very large batches of data.",
            "I mentioned earlier that you need and sensualita have tried every action from every state.",
            "That's not possible in some cases, right?",
            "So if you think of a.",
            "System that is trained for example to play a very large game.",
            "Think of the system that was trained to play go.",
            "It wasn't possible to look at all the different actions in all the different States and Furthermore right they were stuck to a distribution of data that they had observed from previous games.",
            "But that tells you nothing about what might be the distribution of data.",
            "If you start playing in a very different way.",
            "If you have a simulator, things are OK because you can go and try out your new strategy and get new data every time you consider a new policy, you can run that policy and get the data.",
            "But in cases where the data has been collected apriori and you're stuck with that, you need to think about how to correct for the differences in distributions in the data, and so one of the ways that is used to correct this is using an important sampling measure.",
            "So we look at an important factor between the behavior policy.",
            "That's usually the policy we used to collect the data and Pi are target policy.",
            "That's a policy we're wishing to evaluate right now, and we look at the difference between those and we re way our data according to this important factor for.",
            "Each state in action we re weigh the sample according to this.",
            "Those of you who've worked with important sampling, maybe in other cases, know that in many cases as your policy, start diverging.",
            "Your important factors can get really, really small, and so there's two problems.",
            "In one case, if you've never tried something under the behavior policy, then essentially right, you can't say much about it.",
            "And if you have your policy are really different, then there's not much that you can say about it, so you typically still need really a lot of data to use important sampling correction.",
            "There's more sophisticated mechanism, but that's one thing to watch out for.",
            "If you're dealing with a batch of data and now trying to estimate."
        ],
        [
            "Any different policies?",
            "The other thing to take into consideration is related to this.",
            "Where is your data coming from?",
            "How did you collect your data?",
            "In some cases the space of possible data and things that you need to try to collect the data is very, very large."
        ],
        [
            "As you're collecting some data, it's tempting to start acting according to what you think might be the better policy and to stop what we call exploring.",
            "Right exploring is the mechanism through which you randomize your choice of action to gather more diverse information.",
            "Exploitation is the mechanism through which you start acting according to what you've collected so far, and what seems like the best strategy so far, right?",
            "All of you are coming to University of Montreal for the first time last week, and this week probably found a path that somehow get you to this building.",
            "Maybe you are using the same path every day and you don't dare explore and take other pass 'cause it's a big mountain.",
            "What's one mechanism that you sometimes use to find another path?",
            "You follow someone else right?",
            "One day you meet someone at the bottom of the mountain and they go a different way.",
            "And then you find another way.",
            "So one practical way in which people have.",
            "Overcome this exploration problem is through methods related to imitation learning, where you get an agent to demonstrate a good behavior and now you start saying that this is at least some example of a good trajectory.",
            "You don't need to use random trial and error and explore all the paths around the Mount and."
        ],
        [
            "Finally.",
            "In some cases I haven't made a big distinction between these.",
            "It will come through several of the talks this week, but there's some attention called the fact that some reinforcement learning methods or what we call model based methods, whereas others are called model free methods.",
            "I'll just give you a brief definition of it and you can keep an eye out as you go through some of the other talks in model based learning.",
            "We typically assume that we're going to get a lot of data and use this data to estimate or transition in our reward model, and then we will plug into some of the policy.",
            "Value iteration methods that I presented earlier in model free RL.",
            "We typically assume that we're only going to directly estimate the value function.",
            "We will never carry over an estimate of the transition or the reward function.",
            "Never an explicit estimate of them.",
            "We will directly estimate the value function.",
            "There's pros and cons to both of them.",
            "I would say as kind of a one liner of pros and cons to both of them.",
            "Whenever you're tackling large, difficult problems, it's useful to be able to put in domain knowledge.",
            "In some cases it's easier to put the domain knowledge on the model itself, right?",
            "You can constrain the dynamics of your system.",
            "You can constrain the space of your reward function.",
            "If that's the case, maybe model based is better.",
            "In other cases, that's not feasible and you have some notion of regularity of your solution space.",
            "In that case, model free might be more appropriate.",
            "Essentially, where can you put in your domain knowledge to constrain the solution space efficiently is one way to think about this."
        ],
        [
            "The last piece of lingo I will clarify is the separation between what we call value function methods.",
            "I've only talked even though I talked about policy iteration.",
            "It's essentially all value function methods, methods that go through estimation of a value function on the other end, there's all the methods where you directly optimize the policy, and in that case this is going to be the material that's tackled by Peter.",
            "Peter be later today, and so stay tuned for all of that."
        ],
        [
            "Just a quick summary.",
            "I'll take one more minute of your time.",
            "Essentially, I hope you feel like you're a little bit more equipped to detect reinforcement learning problems as you go through the world in Solvay I problems.",
            "Broadly, I would say one thing to keep in mind is this last point over here, which is your intuition about what's easy and what's hard that you've possibly acquired through other cases of machine learning.",
            "Supervised learning in particular in some cases turns out to be quite different in reinforcement learning things that are.",
            "Easy and one turned out to be harder than the other, so keep an eye out for that as you stick through the other talks."
        ],
        [
            "I put this out.",
            "This is probably not for today, but you will have it.",
            "I should say all the slides, not just from my talk, but all of the speakers, the slides and the videos are going to be available on line.",
            "In the case of my talk, if you're really in a hurry, you can probably look at the slides and videos from last year.",
            "It's not all that different.",
            "At the talks for the other slides and videos are going to be up the slides very shortly within a day or two.",
            "Typically videos, probably within two weeks or so, and if that's not enough, there's a set of resources that you'll be able to reach out to.",
            "I will close it at that.",
            "For now, I'll take maybe one question and then let you have coffee."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Great thank you for joining us bright and early on Monday morning.",
                    "label": 0
                },
                {
                    "sent": "What is for some of us Canadians?",
                    "label": 0
                },
                {
                    "sent": "A national holiday?",
                    "label": 0
                },
                {
                    "sent": "I hope I will make it worth your while to have gotten up early.",
                    "label": 0
                },
                {
                    "sent": "Quick thing to kind of get us warmed up.",
                    "label": 0
                },
                {
                    "sent": "How many of you were with the deep Learning Summer School last week?",
                    "label": 1
                },
                {
                    "sent": "Awesome.",
                    "label": 0
                },
                {
                    "sent": "How many of you went to the Jazz Festival over the weekend?",
                    "label": 0
                },
                {
                    "sent": "On a cruise last night.",
                    "label": 0
                },
                {
                    "sent": "Alright, who is joining us for the first time this morning?",
                    "label": 0
                },
                {
                    "sent": "Excellent you guys.",
                    "label": 0
                },
                {
                    "sent": "Missed out a little bit on last weekend the weekend, but look around, there's tons of people who were here last week.",
                    "label": 0
                },
                {
                    "sent": "They can tell you probably where to eat.",
                    "label": 0
                },
                {
                    "sent": "What are fun places to visit in Montreal?",
                    "label": 0
                },
                {
                    "sent": "You're not necessarily going to need the knowledge that was shared last week in terms of technical knowledge.",
                    "label": 0
                },
                {
                    "sent": "We're starting afresh this morning, so I am starting with really the basics of reinforcement learning, but we have a fantastic lineup of speakers.",
                    "label": 0
                },
                {
                    "sent": "Coming in the next three days.",
                    "label": 0
                },
                {
                    "sent": "For those of you who are the deep learning Summer school, last year we had one day of NRL over the six or seven day program an because their interest in our Ella has really been growing.",
                    "label": 0
                },
                {
                    "sent": "This year we decided to extend this to three days, so I sent out invitations to my favorite PRL researchers in the world and I'm so happy to say that most of them accepted the invitation are going to be here this week.",
                    "label": 0
                },
                {
                    "sent": "Few of them had to say no.",
                    "label": 0
                },
                {
                    "sent": "We've got them lined up for next year, but we have really a spectacular I think.",
                    "label": 0
                },
                {
                    "sent": "Lineup of speakers.",
                    "label": 0
                },
                {
                    "sent": "The schedule was down a little bit less last night or this morning, so I have it up here in case you were looking for it.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you a first.",
                    "label": 0
                },
                {
                    "sent": "I want to have session this morning that will be really the basics of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So if you're one of these people who feels like you've never learned anything yet about reinforcement learning, I am here for you.",
                    "label": 0
                },
                {
                    "sent": "I've got your back covered.",
                    "label": 0
                },
                {
                    "sent": "You can ask questions.",
                    "label": 0
                },
                {
                    "sent": "We're going to go at the pace that you want and after that we're going to be launching in some of the more advanced material.",
                    "label": 0
                },
                {
                    "sent": "On Peter Beale, Rich son will be here later today an at 4:30.",
                    "label": 0
                },
                {
                    "sent": "There's a practical sessions that is still a little bit in evolution, but essentially a few of the graduate student from the reasoning and Learning Lab will come and show you some of the tools environments that they've been using in their research that they find the most helpful.",
                    "label": 0
                },
                {
                    "sent": "This is really geared towards people who haven't done research in reinforcement learning and are thinking like where should I start right?",
                    "label": 0
                },
                {
                    "sent": "What's the equivalent of MNIST for reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "How should I be training my first agent?",
                    "label": 0
                },
                {
                    "sent": "What does the training loop looks like?",
                    "label": 0
                },
                {
                    "sent": "We don't usually have a training set in the test set in RL.",
                    "label": 0
                },
                {
                    "sent": "How do I think?",
                    "label": 0
                },
                {
                    "sent": "Of training and testing in these kinds of context, so that's really the purpose of the practical session this afternoon.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow morning we have two of the deep mind researchers coming in telling us about what they have been building, doing research on some of the tricks that went into building the very famous Alphago system.",
                    "label": 0
                },
                {
                    "sent": "Cheeba Cheeba scary is one of the best reinforcement learning theorists, so he'll be there in the afternoon and then we have more special topics on Wednesday all day.",
                    "label": 0
                },
                {
                    "sent": "So I encourage you to come all the way through the week.",
                    "label": 0
                },
                {
                    "sent": "There is coffee breaks and things like that.",
                    "label": 0
                },
                {
                    "sent": "I just cut it out in terms of space so.",
                    "label": 0
                },
                {
                    "sent": "Today I'm really giving you the sense of whether the basics for reinforcement learning this summer school was I should say, conceived up and planned with my colleague Joyner Precup, who's also a well known researcher in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, Dino was called away this week due to family emergency, so she's not going to be around, but much of the work that went into the planning was encountered.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Aberration with her.",
                    "label": 0
                },
                {
                    "sent": "So when we talk about reinforcement learning, right?",
                    "label": 1
                },
                {
                    "sent": "Let's start from sort of the intuition of what we're talking about.",
                    "label": 0
                },
                {
                    "sent": "The idea is inspired by some of the concepts that are coming from psychology, and the idea is really that you can learn, train a system by allowing it to perform different actions in its environment, and by observing the effects of this action, there's some feedback system that sort of tells the agent when it's performing good actions and bad actions.",
                    "label": 0
                },
                {
                    "sent": "And so any of you who has a pet, maybe a child, maybe even a spouse is quite familiar with this kind of situation, right?",
                    "label": 0
                },
                {
                    "sent": "You try out different prompts, different actions, see what kind of response you get.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you give positive reward and sometimes you give negative rewards and you try to shape the behavior of the other agent using these kinds of guiding principles.",
                    "label": 0
                },
                {
                    "sent": "And so when we do this in computer science and with machine.",
                    "label": 0
                },
                {
                    "sent": "I insight is to try to use these same kinds of notions to try our to train our agents and so the idea is that through multiple actions the agent can learn to shape its behavior based on a reward mechanism.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probably for many years, the most famous reinforcement learning system was a system called TD Gammon, developed by Jerry Tesoro in the 1990s, and in this case the system had been trained to play backgammon at a world champion level, essentially through self play.",
                    "label": 0
                },
                {
                    "sent": "So the system was not trained by playing actual people, it played against itself through many millions of games and overtime learn to beat human players.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most recently, we've had some other spectacular successes of reinforcement learning, particularly Alphago system where it was shown that again an agent trained with reinforcement learning can learn to play the very challenging game of go and do so at a level exceeding the best human players.",
                    "label": 0
                },
                {
                    "sent": "There was some very nice results in 2016, we would be one of the top players, but maybe not the world champion, then most recently in 2017, again in a set of matches in China we saw this system beats.",
                    "label": 0
                },
                {
                    "sent": "Well acknowledged human world champion and so hopefully later this week, we'll hear about some of the more advanced reinforcement learning techniques that went into the system for Alpha go.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Couple weeks ago I was at the RLDM conference of them is reinforcement learning and decision making, and I quickly glanced through the proceedings.",
                    "label": 0
                },
                {
                    "sent": "I only made it through about half of the proceedings, checking whether some of the recent applications of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It's not all about games and this is a brief list of the applications like that I could find by just scanning titles in abstract and I only got through half of the accepted presentations and so you have reinforcement applied of course to robotics video games.",
                    "label": 1
                },
                {
                    "sent": "Conversational system, right chatbot type of agents, or one other popular applications trying to train medical decision systems.",
                    "label": 0
                },
                {
                    "sent": "But you see some more interesting things like improvisational theater showing up there so there's really a little bit of something for everyone's taste.",
                    "label": 0
                },
                {
                    "sent": "If you are interested in that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One of the things I would like you to get out of today's session is some insight about when to think about applying reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Many of you, from what I understand, don't necessarily come from a reinforcement learning research lab, and you have a lot of expertise, probably in doing supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Maybe some unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "So what I would like to help you develop is some insight for what problems should or must be cast as reinforcement learning problems and what kind of problems can we handle?",
                    "label": 0
                },
                {
                    "sent": "Using a supervised learning framework, so one of the clues that you might be dealing with a reinforcement learning problem is that your data comes in the form of trajectories, so there's a sequence of observation, and there's a dependency between these observations, so that usually means that you're very precious.",
                    "label": 0
                },
                {
                    "sent": "IID assumption is not going to hold.",
                    "label": 0
                },
                {
                    "sent": "You have to take into account the fact that there's dependencies between these observations.",
                    "label": 0
                },
                {
                    "sent": "That's not quite enough.",
                    "label": 0
                },
                {
                    "sent": "Is a necessary condition, but it's not quite enough because you have time series data.",
                    "label": 0
                },
                {
                    "sent": "For example, if you're observing a set of weather patterns or stock market patterns.",
                    "label": 0
                },
                {
                    "sent": "In those case you have data in the form of sequence, but it's not yet a reinforcement learning problem, so the other thing that you need in a reinforcement learning problem is the need to make some decisions or interventions.",
                    "label": 1
                },
                {
                    "sent": "So there's a notion that you're going to take actions that are going to affect the sequence of observations that you get.",
                    "label": 0
                },
                {
                    "sent": "So not only does the data come in the trajectory, but there's a means to affect the course of that trajectory through a set of actions, decisions, interventions.",
                    "label": 0
                },
                {
                    "sent": "These are usually the two indices.",
                    "label": 0
                },
                {
                    "sent": "The two pieces of information you need, the third one, which is also crucial.",
                    "label": 1
                },
                {
                    "sent": "Is that you need to have a way to observe feedback about their choice of actions.",
                    "label": 0
                },
                {
                    "sent": "So there has to be some mechanism through which you can assess whether an action should be positively rewarded or negatively rewarded, and typically that reward isn't a binary signal.",
                    "label": 0
                },
                {
                    "sent": "It's not a 01, but it's a real number that quantifies how good or how bad was the action.",
                    "label": 0
                },
                {
                    "sent": "So it can be any real number.",
                    "label": 0
                },
                {
                    "sent": "We usually assume that it's a bounded.",
                    "label": 0
                },
                {
                    "sent": "Signal, and so.",
                    "label": 0
                },
                {
                    "sent": "As long as you have that signal that gives you information about the choice of action, then you have all of the pieces you need for reinforcement learning, and I would add one of the things that reinforcement learning does quite nicely is dealing with sparse rewards, so you don't need to have direct feedback about each action that is applied in this system.",
                    "label": 0
                },
                {
                    "sent": "If you think of training an agent to play a game like Backgammon, Go chess, you don't need at every move to receive a reward signal that tells you that was a good move.",
                    "label": 0
                },
                {
                    "sent": "That was a bad move.",
                    "label": 0
                },
                {
                    "sent": "It's enough to have reward signals at some point along the trajectory, and in the case of games, usually that reward comes at the very end.",
                    "label": 0
                },
                {
                    "sent": "So you either win the game or you lose the game, and that's the reward signal that you get.",
                    "label": 0
                },
                {
                    "sent": "So sparse reward along the trajectories are fine, but there has to be some reward to drive the learning.",
                    "label": 0
                },
                {
                    "sent": "If there's no reward that is observed at all, it's very hard to learn anything you're not going to be able to shape the behavior of your system.",
                    "label": 0
                },
                {
                    "sent": "And the last bit that I note in this one isn't.",
                    "label": 0
                },
                {
                    "sent": "So if you meet the 1st three criteria, you're probably on good grounds to look at your problem as a reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "The last one, though that is useful to think about, is that this is typically used for cases.",
                    "label": 0
                },
                {
                    "sent": "Where your problem requires both a notion of learning right, you have some information that you can use to assess the quality of your decision, but also that there's a notion of planning, and so you're interested in making a sequence of decision.",
                    "label": 0
                },
                {
                    "sent": "And maybe there's interaction between the choices of decisions along the way.",
                    "label": 0
                },
                {
                    "sent": "If all of the decisions can be done independently, typically it's not required to move to the reinforcement learning framework, so these are sort of some of the things that you can.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at for those of you who are most familiar with supervised learning, like to make this parallel where in supervised learning you have a notion of inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "And in reinforcement learning, your inputs are typically called States and your outputs with the agent chooses are typically called actions.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning, you do have something akin to a reward function, right?",
                    "label": 0
                },
                {
                    "sent": "It's your target signal, so it may be the correct class, right?",
                    "label": 0
                },
                {
                    "sent": "Your supervised learner predicts an output, but then you get to look whether that was the actual output that we wanted.",
                    "label": 1
                },
                {
                    "sent": "That's your label in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Instead of a label, we have a reward.",
                    "label": 0
                },
                {
                    "sent": "So this is this number that says how good or how bad was your choice of action.",
                    "label": 0
                },
                {
                    "sent": "And because you're dealing with a system over several instances, there's a feedback loop that says depending which action I took.",
                    "label": 0
                },
                {
                    "sent": "Imagine an agent that is playing chess, depending what action the agent took, there's going to be an effect on the environment.",
                    "label": 0
                },
                {
                    "sent": "It's going to change the state of the environment.",
                    "label": 0
                },
                {
                    "sent": "That effect will be fed back in the next state, and so this feedback loop.",
                    "label": 0
                },
                {
                    "sent": "This dependency between the state at every time step.",
                    "label": 0
                },
                {
                    "sent": "Is one of the reasons why we can't make the idea assumption and it's one of the reasons why in some cases some of the analysis that we have to conduct with reinforcement learning gets a little bit more complicated or requires a different set of tools than what you would do for supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so there's some several practical technical challenges that come along, in particular with the fact that we have this feedback loop.",
                    "label": 0
                },
                {
                    "sent": "One of them is the fact that typically to train a reinforcement learning agent.",
                    "label": 0
                },
                {
                    "sent": "You can't just look at a static data set.",
                    "label": 0
                },
                {
                    "sent": "It's not possible to have just the training set because you need to have a sense of how the actions are going to affect the environment, and that effect can only be measured from an actual environment.",
                    "label": 0
                },
                {
                    "sent": "There's still some possibility to train directly from a data set if you have data set that has essentially covered all the choices of actions in all states in sufficient quantity that you can estimate stochastic effect, and so that's a pretty strong assumption, right?",
                    "label": 0
                },
                {
                    "sent": "It's saying assume that my agent has been in all States and tried.",
                    "label": 0
                },
                {
                    "sent": "All actions infinitely many time.",
                    "label": 0
                },
                {
                    "sent": "Then I can do reinforcement learning from a fixed data set in reality that those conditions are only going to be satisfied for very small simple domains, and so most of the interesting domains that were working in, including all the domains that I had listed from these are LDM conference.",
                    "label": 0
                },
                {
                    "sent": "Most of them are cases where you can't learn directly from a static data set.",
                    "label": 0
                },
                {
                    "sent": "You need this kind of interactive system, so it means you need an environment to try out your actions.",
                    "label": 0
                },
                {
                    "sent": "In some sense, this first point is one of the reasons why the.",
                    "label": 0
                },
                {
                    "sent": "Rate of progress in reinforcement learning has been considerably slower than in supervised learning in terms of sharing knowledge between different research groups, it's very easy to send over a data set.",
                    "label": 1
                },
                {
                    "sent": "Nowadays, you just put it on the web.",
                    "label": 0
                },
                {
                    "sent": "People downloaded your download, your data set.",
                    "label": 0
                },
                {
                    "sent": "If I'm doing reinforcement learning research, I have a dynamic system.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a robot, maybe it's a biological system.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a set of, for example, stock market type of situation.",
                    "label": 0
                },
                {
                    "sent": "It's hard to encapsulate that in, send it over, and so one of the reasons most of the reinforcement learning successes in research has been done with games is because in the case of games, we can actually encapsulate the dynamics of that environment, write out the rules of the domain in a way that is concise and share that between research groups and so that enables facilitates progress.",
                    "label": 0
                },
                {
                    "sent": "So we need access to the environment we need to plan an learn simultaneously and finally, the last point is the fact that because we have these dependencies right this feedback loop.",
                    "label": 0
                },
                {
                    "sent": "The data distribution also changes overtime as a function of what actions are taken.",
                    "label": 0
                },
                {
                    "sent": "So if you change your action strategy, you're going to see different things right?",
                    "label": 0
                },
                {
                    "sent": "If you build a robot that is always going right, then the robot is going to see a certain part of the world, and now you suddenly make the robot always go left.",
                    "label": 0
                },
                {
                    "sent": "It's going to see another part of the world.",
                    "label": 0
                },
                {
                    "sent": "So depending what is your choice of action, you see a different distribution of the data, and you need to take that into account when you're doing learning, so you can't necessarily assume that your data distribution is stationary as you would.",
                    "label": 1
                },
                {
                    "sent": "For a standard supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, to formalize things a little bit more, we typically cast reinforcement learning problems using Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "So Markov decision process are.",
                    "label": 1
                },
                {
                    "sent": "Formally specified by the following, the set of states so that it defines the space of the problem.",
                    "label": 1
                },
                {
                    "sent": "These states can be discreet.",
                    "label": 1
                },
                {
                    "sent": "They can be infinite.",
                    "label": 0
                },
                {
                    "sent": "They can be continuous, but you need a set of states to be defined.",
                    "label": 1
                },
                {
                    "sent": "There's a set of actions, typically discrete, but they can also be continuous.",
                    "label": 0
                },
                {
                    "sent": "We have a probabilistic distribution that describes the dynamics of the environment, and so we make specifically the assumption that the probability of seeing a state.",
                    "label": 0
                },
                {
                    "sent": "Is fully defined condition on the previous state and the action that was chosen will talk a little bit more about that assumption in a few seconds, but that is our model that defines how the agent changes overtime.",
                    "label": 0
                },
                {
                    "sent": "But there's the reward function.",
                    "label": 0
                },
                {
                    "sent": "That's a real number defined for each state and action pair, and typically in some cases we also assume an initial state distribution, so that can be the starting state.",
                    "label": 0
                },
                {
                    "sent": "If you're playing a game right.",
                    "label": 0
                },
                {
                    "sent": "The initial starting distribution is just the initial state of the board.",
                    "label": 0
                },
                {
                    "sent": "Trajectories always start from that state, but for some other systems, robotic systems and others you can actually have a distribution over possible initial states.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so let's visit again the assumption that we're making when we specify or transition model, right?",
                    "label": 0
                },
                {
                    "sent": "I said that the probability of the next state is fully defined by the previous state in the previous action, and this is known in the literature as the Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "The Markov property in this case.",
                    "label": 1
                },
                {
                    "sent": "More generally, if we didn't want to make such a restrictive assumption, we could assume that the probability of the next state depends on the full history.",
                    "label": 0
                },
                {
                    "sent": "All the states that this system has visited before and all this state, all the actions that have been taken previously.",
                    "label": 0
                },
                {
                    "sent": "And so this would be a system that has much different structure in terms of the transition probability and so some of the reason we make this Markov assumption, one of them is really just to limit the complexity of the model that we're looking at.",
                    "label": 0
                },
                {
                    "sent": "We're looking at this dynamic model and we want to learn this from data.",
                    "label": 0
                },
                {
                    "sent": "If we start having a probabilistic model that depends on full histories, then it requires a lot more information to estimate accurately.",
                    "label": 0
                },
                {
                    "sent": "Now, I haven't told you necessarily how you need to define your state, and so essentially you have to think of the state as the sufficient amount of information that you need to predict the future.",
                    "label": 0
                },
                {
                    "sent": "That's probably the best way to think of how to define your state space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look at simple systems that we've seen before, right?",
                    "label": 0
                },
                {
                    "sent": "All of you have seen traffic lights before.",
                    "label": 1
                },
                {
                    "sent": "If you are faced with the traffic light and the light is yellow, you don't need to think really long about what was the previous states of this traffic light to know what is coming next right is going to go to the red light, and so in that case the color is sufficient to predict the next color.",
                    "label": 1
                },
                {
                    "sent": "So we would say that this traffic light meets the Markov assumption.",
                    "label": 0
                },
                {
                    "sent": "Chess is another one of these cases.",
                    "label": 0
                },
                {
                    "sent": "If the board isn't a particular configuration and you take a particular action, then you know what's going to be the board configuration when stepped down the line.",
                    "label": 0
                },
                {
                    "sent": "In both of these cases, these are deterministic system, right?",
                    "label": 0
                },
                {
                    "sent": "So if there's a state and action, you know what the next state is in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We don't necessarily assume that there's a deterministic transition.",
                    "label": 0
                },
                {
                    "sent": "We can handle stochastic transition where there's a probability distribution, but the point here is that the information is fully contained.",
                    "label": 0
                },
                {
                    "sent": "One classic game where often the Markov assumption is not respected as a game like poker, right?",
                    "label": 0
                },
                {
                    "sent": "You may see some information about the game.",
                    "label": 0
                },
                {
                    "sent": "There's some cards on the table, maybe some bets have been placed and now you're trying to determine what's the next state based on how you're going to play.",
                    "label": 0
                },
                {
                    "sent": "That can be affected by a lot of prior information.",
                    "label": 0
                },
                {
                    "sent": "An if you look at poker using a very narrow notion of state.",
                    "label": 0
                },
                {
                    "sent": "Then you're probably not going to do very well, and so if you want to have a state that has sufficient information to predict the future for a game like poker, you probably need to fold in other pieces of observation from your previous play history and call all of that the state so the state can incorporate information about what other players have done in previous rounds of the game.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the objective when you're dealing with a reinforcement learning problem is to look at some trajectories an over the trajectory, try to choose actions that are going to yield good reward, and so if you choose several actions that all yield good reward, then you're probably doing well.",
                    "label": 0
                },
                {
                    "sent": "And so there's two classes of tasks, and depending which class of task we look at this notion of a return is defined slightly differently.",
                    "label": 0
                },
                {
                    "sent": "So in the first class we are dealing with episodic tasks, so these are tasks where we know the task is going to end.",
                    "label": 0
                },
                {
                    "sent": "There's a finite horizon.",
                    "label": 0
                },
                {
                    "sent": "Things like games, right?",
                    "label": 0
                },
                {
                    "sent": "You're going to play, and at some point the game will end, so we know there's an endpoint.",
                    "label": 0
                },
                {
                    "sent": "In those cases, the return of the trajectory.",
                    "label": 0
                },
                {
                    "sent": "So trajectory is the whole sequence of action from the initial state.",
                    "label": 0
                },
                {
                    "sent": "To win the game ends the return of that trajectory is the sum of the rewards over the trajectory.",
                    "label": 1
                },
                {
                    "sent": "In the case of continuing task, these are tasks where we don't necessarily know whether there's going to be an end point.",
                    "label": 0
                },
                {
                    "sent": "Then we look often at an infinite sum, so we assume that the task can go on forever, and we still want the return to be the sum of rewards over that potentially infinite sequence.",
                    "label": 0
                },
                {
                    "sent": "Maybe you're building a robot to juggle balls, and you don't want to say you know, and after 10 rounds of this, juggle, as long as you can.",
                    "label": 0
                },
                {
                    "sent": "And so the danger in defining the return, as we've done for episodic task in the case of a task that goes on forever, is you could essentially have an infinite return, right?",
                    "label": 0
                },
                {
                    "sent": "Infinite return might sound great if you're thinking about your financial investments, but in practice, for analysis of stability of these systems, infinite returns tend to be not so convenient.",
                    "label": 0
                },
                {
                    "sent": "And so in the case where we're looking at.",
                    "label": 1
                },
                {
                    "sent": "Continuing task with potentially infinite length horizon, we introduce the notion of a discount factor.",
                    "label": 0
                },
                {
                    "sent": "So often it's always denoted by gamma by convention, and so gamma is this discount factor that is added in front of the reward as a multiplicative factor in front of the reward on each round, and it's increasing in the sense of gamma is typically below one, and so it reduces the influence of the reward.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Overtime and so this discount factor always bounded below at zero bounded above at one.",
                    "label": 0
                },
                {
                    "sent": "Typically we keep it close to one and what happens if we look again at this equation over here?",
                    "label": 0
                },
                {
                    "sent": "If my gamma is equal to 1, I get back my return for the episodic task.",
                    "label": 1
                },
                {
                    "sent": "If I set my gamma to zero, what happens?",
                    "label": 0
                },
                {
                    "sent": "Is all of my future rewards essentially don't matter, and so I'm a very myopic agent that is only picking actions to maximize the immediate rewards, and I don't care about long term consequences and by balancing gamma between zero and one, I can essentially trade off.",
                    "label": 0
                },
                {
                    "sent": "How much do I care about?",
                    "label": 0
                },
                {
                    "sent": "Immediate reward versus.",
                    "label": 0
                },
                {
                    "sent": "How much do I care about long-term reward?",
                    "label": 0
                },
                {
                    "sent": "And so there's some natural intuition about this discount factor in some sense, right?",
                    "label": 0
                },
                {
                    "sent": "If I were to offer you certain amount of money today, right?",
                    "label": 0
                },
                {
                    "sent": "I meet you ioffer you $100 today, and then I ask you, would you rather get have $100 today or maybe $120.00 tomorrow, right?",
                    "label": 0
                },
                {
                    "sent": "Some of you might prefer the $100 today 'cause you don't know if I'm going to show up tomorrow or not.",
                    "label": 0
                },
                {
                    "sent": "And some of you have a discount factor close to one and are thinking well.",
                    "label": 0
                },
                {
                    "sent": "I'd rather wait till tomorrow.",
                    "label": 0
                },
                {
                    "sent": "There was a pretty trustworthy person.",
                    "label": 0
                },
                {
                    "sent": "She's going to come.",
                    "label": 0
                },
                {
                    "sent": "She's invited all these speakers.",
                    "label": 0
                },
                {
                    "sent": "Let's just wait till tomorrow and get the $120.00 and so the discount factor allows you to mitigate essentially this these different these different considerations between short term and long term according to a particular criteria, yes.",
                    "label": 0
                },
                {
                    "sent": "Kind of power.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so in in this case, right?",
                    "label": 0
                },
                {
                    "sent": "There's sort of a geometric decay that's given by gamma.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "There is very convenient mathematical reasons why to put it that way, and in particular we can show convergence of that sum with this particular formulation of gamma, depending what decay rate you assume, you may not have convergence of that sum, and so it's convenient from that point of view.",
                    "label": 0
                },
                {
                    "sent": "So most of the reinforcement learning theory has all been derived with this, but there's potentially other formulations you could look at.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one more piece of notation that is going to be very, very useful is the notion of a policy.",
                    "label": 0
                },
                {
                    "sent": "So the policies typically denoted pie.",
                    "label": 0
                },
                {
                    "sent": "I'm going to stick to \u03c0 and hopefully my fellow speakers throughout the week are also going to use pie to denote the policy.",
                    "label": 0
                },
                {
                    "sent": "This is how we define the selection strategy.",
                    "label": 0
                },
                {
                    "sent": "So I've alluded to the fact that you want an agent that picks good actions, and so we need to define away for a formulation for how is that strategy going to be defined.",
                    "label": 0
                },
                {
                    "sent": "So this is the policy we can have a stochastic policy, so that means that we have a probability of picking.",
                    "label": 0
                },
                {
                    "sent": "Different actions at a given state \u03c0 S of A is my stochastic strategy, or we can have a deterministic strategy.",
                    "label": 0
                },
                {
                    "sent": "In the second case, so deterministic strategy.",
                    "label": 0
                },
                {
                    "sent": "Then we have a mapping from state to action.",
                    "label": 0
                },
                {
                    "sent": "So the policy is not part of the environment.",
                    "label": 0
                },
                {
                    "sent": "The policy is the thing that you're learning when you're learning NRL agent, right?",
                    "label": 0
                },
                {
                    "sent": "You're not given the policy.",
                    "label": 0
                },
                {
                    "sent": "The policy is what you're trying to get out of learning and planning.",
                    "label": 0
                },
                {
                    "sent": "And so there's different cases where you may want to have a stochastic policy or deterministic policy, and we're going to review this as we get through some of the more advanced concept, and so more specifically, what I want in terms of objective is really a way to pick the policy of the space of all policies.",
                    "label": 0
                },
                {
                    "sent": "What is the policy that maximizes my expected total reward over the trajectory?",
                    "label": 0
                },
                {
                    "sent": "So I have an expectation over the policy, so as I change my policy.",
                    "label": 0
                },
                {
                    "sent": "I can expect to receive different rewards an I have a maximization over all the policies right?",
                    "label": 0
                },
                {
                    "sent": "For those of you who might not have seen the argmax function similar to the Max, but it returns that argument which maximizes the function.",
                    "label": 0
                },
                {
                    "sent": "So it says overall, the policy's return the policy that gives me the best expected reward.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to make this a little bit more concrete, right, let's look at a familiar decision-making problem for some of you.",
                    "label": 0
                },
                {
                    "sent": "I've cast it in very simple terms, right?",
                    "label": 0
                },
                {
                    "sent": "You have sort of four different states you might be in through the course of your karere.",
                    "label": 0
                },
                {
                    "sent": "Some of you at some point.",
                    "label": 0
                },
                {
                    "sent": "Not very many given current conditions are going to be unemployed.",
                    "label": 0
                },
                {
                    "sent": "Some of you are probably going to work in industry.",
                    "label": 0
                },
                {
                    "sent": "Some of your may be working or will work in academia, and some of you may be.",
                    "label": 0
                },
                {
                    "sent": "Currently in the future or have been in the past attending grad school.",
                    "label": 0
                },
                {
                    "sent": "And so in these cases there's different choices of actions that you can take, right?",
                    "label": 0
                },
                {
                    "sent": "You can do nothing, and doing nothing will usually keep you in the same state that you are including in academia, except if you're in an industry.",
                    "label": 0
                },
                {
                    "sent": "If you do nothing, there may be some chance that you end up somewhere else.",
                    "label": 0
                },
                {
                    "sent": "Over here.",
                    "label": 0
                },
                {
                    "sent": "And so you can apply to industry and most of the time if you apply to industry from unemployed or grad school, right?",
                    "label": 0
                },
                {
                    "sent": "You might end up in industry.",
                    "label": 0
                },
                {
                    "sent": "But presumably the probabilities are not the same.",
                    "label": 0
                },
                {
                    "sent": "And if you're in academia and you apply for industry, then that's pretty deterministic that you're going to end up over there these days.",
                    "label": 0
                },
                {
                    "sent": "So we can add numbers to this right?",
                    "label": 0
                },
                {
                    "sent": "We can add probability distributions to each of these actions and we can also add rewards, and I've posited some reward function over there, but the reward function may be different for different individuals and so now you can start seeing that the choice of what action to take really depends on that sequence of things, right?",
                    "label": 0
                },
                {
                    "sent": "If you're someone for whom academia is highly rewarded, then probably you need to go through grad school.",
                    "label": 0
                },
                {
                    "sent": "To make it to that point, and so in this case you need to take into account both the transition probability and the reward function to figure out what is the best policy for you.",
                    "label": 0
                },
                {
                    "sent": "And so in this case, if I'm asking what is the best policy, there's several policies that I can look at right in each of my states.",
                    "label": 0
                },
                {
                    "sent": "I have four choices of actions.",
                    "label": 0
                },
                {
                    "sent": "An I have 4 States and so that means the space of policies is essentially four choices of action for the first 8 * 4 choices of action for the 2nd State Times Fortress.",
                    "label": 0
                },
                {
                    "sent": "So we have essentially.",
                    "label": 0
                },
                {
                    "sent": "A different actions to the power of S my number of states in terms of my space of policies, so that's a pretty large space of policy.",
                    "label": 0
                },
                {
                    "sent": "And when I'm solving a reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "Essentially, my objective is to pick the best out of all of these policies.",
                    "label": 0
                },
                {
                    "sent": "In this case, I've given you the transition probability and I've given you the reward function.",
                    "label": 0
                },
                {
                    "sent": "So from that point it's not so much a learning problem.",
                    "label": 0
                },
                {
                    "sent": "It's more planning problem, right?",
                    "label": 0
                },
                {
                    "sent": "And so in the next half hour.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to review some algorithms that are useful for planning under the assumption that we know the transition probability and we know the reward function.",
                    "label": 0
                },
                {
                    "sent": "That's not really learning, but it turns out to be really useful to have this basis before we make things harder, and we start tackling the case where we don't know the transition probability.",
                    "label": 0
                },
                {
                    "sent": "We don't know the reward function and we need to estimate those from data, but bear with me over the last half hour or so while we talk about the case where we know the transition of the reward function.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's get back to our notion of optimality in this case, right?",
                    "label": 0
                },
                {
                    "sent": "I mentioned that we want to look at the expected return of a policy.",
                    "label": 0
                },
                {
                    "sent": "So I define the expected return of the policy that is defined at every state.",
                    "label": 0
                },
                {
                    "sent": "So depending which state you're in, you have a different expected return over your trajectory, and so we've defined a notion called the value function.",
                    "label": 0
                },
                {
                    "sent": "The value function, by definition, is this expected future return, and that's defined for.",
                    "label": 0
                },
                {
                    "sent": "Each of my States and so a very simple strategy, which I've alluded to for finding the best policy would be the following.",
                    "label": 0
                },
                {
                    "sent": "Step number one, enumerate the space of all policies.",
                    "label": 0
                },
                {
                    "sent": "Right, so I have 4 to the four different policies.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to list them all out.",
                    "label": 0
                },
                {
                    "sent": "In an attempt to figure out what to do with my life, then estimate the expected return of each one.",
                    "label": 0
                },
                {
                    "sent": "So I can look at the expectation for a particular strategy with my expected return over all of this in this case.",
                    "label": 0
                },
                {
                    "sent": "I would use a discount factor because there's no notion of like a terminal state right in the MDP that I've posited over here.",
                    "label": 0
                },
                {
                    "sent": "There is no kind of state in retirement where you live there forever, so in this case, since there is no termination point, I would use a discount factor.",
                    "label": 0
                },
                {
                    "sent": "So I estimate the expected return of each of my different policies, and then I keep the policy that has the maximum expected return right?",
                    "label": 0
                },
                {
                    "sent": "And so this is a simple strategy.",
                    "label": 0
                },
                {
                    "sent": "It has some limitations, in particular because the space of policies quickly gets very very large.",
                    "label": 0
                },
                {
                    "sent": "As you have just a few more states in action, so it's not a practical one.",
                    "label": 0
                },
                {
                    "sent": "But conceptually it's going to be sort of based on that that we build some more efficient algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to pause just for a second before we get into these algorithms and make sure that everyone is clear about these different terms that sometimes get a little bit thrown in together, right?",
                    "label": 0
                },
                {
                    "sent": "I've talked about reward, right reward is the one step.",
                    "label": 0
                },
                {
                    "sent": "Feedback that you get so for particular action, the particular state.",
                    "label": 0
                },
                {
                    "sent": "I get an atomic reward.",
                    "label": 0
                },
                {
                    "sent": "I've talked about return return is the sum of rewards over the trajectory.",
                    "label": 0
                },
                {
                    "sent": "I've talked about value.",
                    "label": 0
                },
                {
                    "sent": "Write the value is the expected return over the trajectory, so the return maybe 1 sample along a trajectory, but value is in expectation.",
                    "label": 1
                },
                {
                    "sent": "So if from a particular state I take several of these trajectories an I get a number that is my expected return.",
                    "label": 0
                },
                {
                    "sent": "If you have a deterministic system then the return the value are going to be the same, but if you have a stochastic system then one trajectory will give you a sample of your return, but then when you run many trajectories you'll be able to estimate your expected.",
                    "label": 0
                },
                {
                    "sent": "Value.",
                    "label": 0
                },
                {
                    "sent": "And finally, we have a notion of utility, right?",
                    "label": 0
                },
                {
                    "sent": "I didn't talk about utility very much in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "We essentially assume that the utility is the same as the return.",
                    "label": 0
                },
                {
                    "sent": "And so we assume that we have a linear concept of returns, so the return is the sum of the rewards.",
                    "label": 0
                },
                {
                    "sent": "In many other cases of decision theory there's a distinction between value and utility, and utilities can be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Utilities can have different types of functions, and so for reinforcement learning we always make this assumption that value function is essentially utility is the same as value.",
                    "label": 1
                },
                {
                    "sent": "We're not going to get into more complex notion.",
                    "label": 0
                },
                {
                    "sent": "Of utility for the purposes of this, there's some of the reinforcement learning literature that does get into that space.",
                    "label": 0
                },
                {
                    "sent": "For example, methods that are sensitive to risk or will have at the end of the week notions of safe MVP's.",
                    "label": 0
                },
                {
                    "sent": "In that case, we look at slightly different notions of value, but for the purposes of today, probably the first couple of days, you can make the assumption that utility is the same as your value, yes, so in some sense, when you look at the discounted reward is one way to translate return issue.",
                    "label": 0
                },
                {
                    "sent": "Exactly, yeah, and that's a very common way that we've looked at.",
                    "label": 0
                },
                {
                    "sent": "Now that I should say that's coming in the literature and the you know everything I'm presenting here is essentially known in the literature for the last 40 years or so, yeah.",
                    "label": 0
                },
                {
                    "sent": "So let's look a little bit more closely.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The value of a policy.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, line number one over here is the definition I've given you so far, right?",
                    "label": 0
                },
                {
                    "sent": "The value of a particular policy at a given state S is the expected value over that policy for the sum of reward given that you start in a particular state S. So I can rewrite this equation by just splitting up the first term compared to the other terms.",
                    "label": 0
                },
                {
                    "sent": "And now in my first term, all I want is to get the expected value of my immediate reward.",
                    "label": 1
                },
                {
                    "sent": "The expected value of my immediate reward depends on what's the probability that I'm going to take a particular action in that state.",
                    "label": 0
                },
                {
                    "sent": "And what's the reward I get for taking that action?",
                    "label": 0
                },
                {
                    "sent": "And so this pie activate.",
                    "label": 0
                },
                {
                    "sent": "That's part of my definition of the policy and over here I've left all the expected sum of rewards from the step plus one, and they're on, and so it's very common to look at the value function in, split it up this way.",
                    "label": 1
                },
                {
                    "sent": "Have the notion that you have a first term.",
                    "label": 0
                },
                {
                    "sent": "That's the immediate reward, and then you have another term.",
                    "label": 0
                },
                {
                    "sent": "That's the future reward, and so I've bundled up all the future rewards together, and I partitioned out just my first reward an when I do that, I can start looking.",
                    "label": 0
                },
                {
                    "sent": "At my expectation over here, and essentially I'm trying to get a recursive formulation for the value function to make it more concise, 'cause right now my value function determine is depends on the full future an I'm trying to get a recursive formulation for that, so if I look at this term over here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expectation with respect to my policy Pi of my future rewards starting at T + 1, but I'm still conditioning anesti so if I compare the term over here in the term over the same except in this case I started nesti and I have RT.",
                    "label": 0
                },
                {
                    "sent": "In this case I start in St and I look at RT plus one.",
                    "label": 0
                },
                {
                    "sent": "So I need to do a little bit of work to express this expectation condition on starting at T + 1 and the way to do that is simply take into account what's the expectation?",
                    "label": 0
                },
                {
                    "sent": "Of where I'm going to be, what's going to be my S T + 1 and so to do that we just need to fold in the transition probability I need.",
                    "label": 0
                },
                {
                    "sent": "I'm in a state S at time TI want to know where am I going to be at time T plus one well I need to factor in my transition probabilities and the probability of taking each of my different actions.",
                    "label": 0
                },
                {
                    "sent": "So there's a probability that I can take each of these actions that's defined by my policy and given which action I took, there's a probability that it's going to take me to a new state.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so after I've folded in this one step expectation, the term over here is in exactly the same form as the term I had when I started with.",
                    "label": 0
                },
                {
                    "sent": "So I can call this V Pi of S prime whatever state I'm going to be in.",
                    "label": 0
                },
                {
                    "sent": "And now I have a recursive formulation where my value function at this time step depends on my immediate reward, the transition probability of being a new state, and the value at that state.",
                    "label": 0
                },
                {
                    "sent": "And so this is essentially a form of a dynamic programming algorithm where the value at each of my state of the variables for that dynamic program.",
                    "label": 1
                },
                {
                    "sent": "Yes, I'm confused here, because your horizon just went shorter on the.",
                    "label": 0
                },
                {
                    "sent": "For the 2nd and so why?",
                    "label": 0
                },
                {
                    "sent": "Why would it be the same value function if it's a shorter horizon?",
                    "label": 0
                },
                {
                    "sent": "So in this case the horizon is defined over the lifetime of the system, so it's the expectation over that lifetime and so.",
                    "label": 0
                },
                {
                    "sent": "In this case, or maybe it may be a deterministic right, maybe set or maybe a random variable and you can redo the same derivation with the discount factor in there, and then probably you'll be bothered a little bit less.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It's not the most completely answer.",
                    "label": 0
                },
                {
                    "sent": "Typically when you do it with finite horizon you actually keep time index value function, and if you want to prove convergence, you're going to need to take into these account.",
                    "label": 0
                },
                {
                    "sent": "These time time index value function.",
                    "label": 0
                },
                {
                    "sent": "But if you have the discount factor, then you don't.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 1
                },
                {
                    "sent": "A policy and a trajectory good.",
                    "label": 0
                },
                {
                    "sent": "So the pharmacy is just a set of projectors policy is like a policy is a strategy for acting, so the policy will have to specify in all states what is the action that I should take or what is a distribution of actions that I should take.",
                    "label": 0
                },
                {
                    "sent": "In contrast, the trajectory is 1 particular experience through that space.",
                    "label": 0
                },
                {
                    "sent": "So if we go back to my example about what occupation should you follow, write a policy, maybe when you're an unemployed, go to grad school when you're in grad school, apply for a job when you're in industry, keep on applying for other jobs, right?",
                    "label": 0
                },
                {
                    "sent": "That would be a policy now.",
                    "label": 0
                },
                {
                    "sent": "A trajectory might be.",
                    "label": 0
                },
                {
                    "sent": "I was unemployed.",
                    "label": 0
                },
                {
                    "sent": "Then I went to grad school.",
                    "label": 0
                },
                {
                    "sent": "Then I went back to being unemployed.",
                    "label": 0
                },
                {
                    "sent": "Then I went back to grad school.",
                    "label": 0
                },
                {
                    "sent": "Then I ended up in industry.",
                    "label": 0
                },
                {
                    "sent": "Right, it's one sample path, and that path depends on your choice of actions.",
                    "label": 0
                },
                {
                    "sent": "That's the part that's defined by the policy, but it also depends on the stochastic effects.",
                    "label": 0
                },
                {
                    "sent": "And so that's the expectation, the transition probability.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this case I've just rewritten over here the value function we have for fixed policy.",
                    "label": 0
                },
                {
                    "sent": "This doesn't take into account searching over the space of all policies, it's just for a fixed policy, and this particular case we sometimes have an alternate formulation that's called the Q function and the Q function, in this case corresponds to the value of a particular action, and so, whereas in this case I'm folding in the policy Pi essay in the Q function the policy.",
                    "label": 1
                },
                {
                    "sent": "Is used as an argument, and we define it for each possible action, but the first term is the same.",
                    "label": 0
                },
                {
                    "sent": "We have the immediate reward for doing a particular action in a particular state S we have gamma discount factor.",
                    "label": 0
                },
                {
                    "sent": "We have the transition probability and then we have the future rewards.",
                    "label": 0
                },
                {
                    "sent": "The future value function.",
                    "label": 0
                },
                {
                    "sent": "So both of these are instances of a form we call the Bellman equation.",
                    "label": 1
                },
                {
                    "sent": "The bellman equation always has these kinds of dynamic programming recurrence form where you have your value function or your Q function on one side.",
                    "label": 0
                },
                {
                    "sent": "And the value function of the Q function of the next state on the other side, and it comes in different forms and flavors.",
                    "label": 0
                },
                {
                    "sent": "But it's always going to have these kinds of structure with the immediate reward pulled out the future returned appearing here and the transition probability appearing also over here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so if we look at this equation over here and I've gone back in this case to the formulation with the discount factor gamma, if we assume that your set of states is finite, and we assume that our set of actions is finite, we can actually see this as just a set of linear equations, right?",
                    "label": 0
                },
                {
                    "sent": "We have S different equations, one for each of our States and we have equations, essentially with different terms depending on the value of all the future states.",
                    "label": 1
                },
                {
                    "sent": "I could end up in.",
                    "label": 0
                },
                {
                    "sent": "And so seeing this as a system of linear equation, we can actually write this in matrix form if we want.",
                    "label": 0
                },
                {
                    "sent": "So in this case, via Pi is just a vector, is the value for all of my states are \u03c0 is another vector, is the reward for each of my states.",
                    "label": 0
                },
                {
                    "sent": "My transition matrix that's going to be an S by S square matrix an.",
                    "label": 0
                },
                {
                    "sent": "I have the value for all of my states also, so I can solve this as a system of linear equation.",
                    "label": 0
                },
                {
                    "sent": "Subject to some certain conditions, these conditions turned out to be a little bit problematic.",
                    "label": 0
                },
                {
                    "sent": "In particular, this matrix over here is not always well conditioned, and it's not always possible to do it, but so we don't typically solve systems this way.",
                    "label": 0
                },
                {
                    "sent": "I bring it to your attention just so that you understand a little bit better than the nature of these dynamic programming equations.",
                    "label": 0
                },
                {
                    "sent": "An also to insist on the fact that this is for a particular this condition on a fixed policy.",
                    "label": 1
                },
                {
                    "sent": "We haven't yet tackled the problem of.",
                    "label": 0
                },
                {
                    "sent": "Looking at finding good policies, but if you're going to go with my simple strategy of let me enumerate all my possible policy and now let me evaluate all of my possible policy.",
                    "label": 0
                },
                {
                    "sent": "This is 1 method to evaluate these possible policies, because solving this exactly, and I've lost the mic.",
                    "label": 0
                },
                {
                    "sent": "OK, so because we may not want to solve things exactly, this matrix maybe ill condition there is some.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Convenient iterative methods to do the same thing again, drawn from dynamic programming because it's a dynamic programming algorithm, we can actually start with an initial guess of the value function.",
                    "label": 0
                },
                {
                    "sent": "The reassuring part is you can the choice of that initial value function doesn't matter at all.",
                    "label": 0
                },
                {
                    "sent": "In the case where you have a discrete set of States and actions, and so you should have some initial guess of what your value function might be when popular guesses to set it all to zero and other popular guesses to set it to the reward function.",
                    "label": 1
                },
                {
                    "sent": "And now over several iterations, you're going to update the value for all of my States and so in this case I've indexed them by, which round which iteration you're on, and so at round K plus one, the value is going to be the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "Plus the future expected reward future expected value based on your current last round estimate of the value function at all these other states.",
                    "label": 0
                },
                {
                    "sent": "And over several rounds, this is going to converge.",
                    "label": 0
                },
                {
                    "sent": "And you can assess convergence because the value functions are going to stop changing or they're going to change less than some threshold value that you've put in.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's guaranteed to converge for simple reasons, in particular because you have this discount factor over here, and the effect of this discount factor we already alluded to is to sort of shrink how much you care about the future, and so that means on every round of this die.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "NMK programming you can actually show that the difference between the value function from one round to the next has to shrink.",
                    "label": 1
                },
                {
                    "sent": "So if you define the value function at round K + 1 compared to the value function of your estimate that you have previously, then you can actually separate these two terms out.",
                    "label": 0
                },
                {
                    "sent": "Of course, the initial reward terms cancel out, and you're left with just the difference between these values at the next round, and these two values are multiplied by your discount factor gamma.",
                    "label": 0
                },
                {
                    "sent": "And because this gamma is.",
                    "label": 0
                },
                {
                    "sent": "By definition less than one or it's less than one so that we achieve contraction property.",
                    "label": 0
                },
                {
                    "sent": "Then you can actually show that you're going to shrink your difference between value functions on each round, and so over.",
                    "label": 0
                },
                {
                    "sent": "Sometime this will actually go to zero and so we have convergence.",
                    "label": 0
                },
                {
                    "sent": "Now this is for the case where you have discrete States and actions.",
                    "label": 0
                },
                {
                    "sent": "In the case where you have continuous state and actions, things get a little bit more complicated, but in the discrete case is things are quite nice.",
                    "label": 1
                },
                {
                    "sent": "And this is for the case with discount factor.",
                    "label": 0
                },
                {
                    "sent": "If you have finite horizon episodic task we can also show that we have convergence, but we don't have the discount factor.",
                    "label": 0
                },
                {
                    "sent": "So in that case in relation to my comment earlier the proof of convergence goes through the fact that you're keeping different value function estimates for each of the different time horizons.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's step back from the case where we assume we fixed the policy.",
                    "label": 0
                },
                {
                    "sent": "This was the case where I fixed my policy and look at the case where we actually are looking for a good policy.",
                    "label": 0
                },
                {
                    "sent": "So here there's a notion of an optimal value function I'm going to denote this V star.",
                    "label": 0
                },
                {
                    "sent": "This is the best value function amongst the value of any of my policies.",
                    "label": 0
                },
                {
                    "sent": "So in my simple approach I'm enumerating all of my policy looking at the value for each of them.",
                    "label": 0
                },
                {
                    "sent": "Restart is the one that returns the best one over all of my policies.",
                    "label": 0
                },
                {
                    "sent": "So any policy that achieves this greatest value is called an optimal policy, and I'm going to use paisar to denote an optimal policy, and so we have some results dating back to the Seminole work of Bellman that says that for each MDP there's actually a unique optimal value function.",
                    "label": 0
                },
                {
                    "sent": "But at the same time, the optimal policy is not necessarily unique, and that's not very hard to understand.",
                    "label": 0
                },
                {
                    "sent": "You can have two policies by two choice of actions that give you the same value.",
                    "label": 0
                },
                {
                    "sent": "So in that case they both have the same value.",
                    "label": 0
                },
                {
                    "sent": "It's the optimal value restarts, but there are different notions of policy.",
                    "label": 0
                },
                {
                    "sent": "We can have multiple optimal policies.",
                    "label": 0
                },
                {
                    "sent": "One optimal value function.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there's a nice correspondence between the optimal value and the optimal policy, and I've put in the equations and really small font here, 'cause I don't want you to look at them, but they are there in case you want to go back to the slides and puzzle through them later.",
                    "label": 0
                },
                {
                    "sent": "But in this case the equivalence I want you to understand is the fact that if we know the value function V star, and I haven't told you yet, an efficient way to get it.",
                    "label": 0
                },
                {
                    "sent": "I told you a very naive way to get it, but if we have the optimal function V star and we know the reward transition discount factor.",
                    "label": 0
                },
                {
                    "sent": "Then we can compute the optimal policy very easily, easily in terms of like number of computing operations.",
                    "label": 1
                },
                {
                    "sent": "Similarly, if we know the optimal policy.",
                    "label": 1
                },
                {
                    "sent": "Then we can recover the optimal value function equally easily, and so there's a simple correspondence between these two.",
                    "label": 0
                },
                {
                    "sent": "So the reason I emphasize this as I'd like you to think of both the staran pie stars as solutions to the MDP, but there's sort of alternate solutions and we can talk later about whether you know you're better defined the star and then calculate \u03c0 star from that, or find \u03c0 star and calculate the star from that.",
                    "label": 0
                },
                {
                    "sent": "But both of them are essentially solutions to the problem.",
                    "label": 1
                },
                {
                    "sent": "Question was confused.",
                    "label": 0
                },
                {
                    "sent": "Define V star as a portion of S. Yes.",
                    "label": 0
                },
                {
                    "sent": "Perhaps it could be different policies which achieves the maximum.",
                    "label": 0
                },
                {
                    "sent": "So what you mean by the optimal policies?",
                    "label": 0
                },
                {
                    "sent": "The policy is defined as if you think of the deterministic case, then the policy is a mapping from state to action.",
                    "label": 0
                },
                {
                    "sent": "So for each state you need to define what's the action that's taken that spy star and that action is going to have a value that's V star.",
                    "label": 0
                },
                {
                    "sent": "Saying the same optimal policy would give the.",
                    "label": 0
                },
                {
                    "sent": "The optimal value function for every.",
                    "label": 1
                },
                {
                    "sent": "Yes, 'cause the policy is defined at each state write.",
                    "label": 0
                },
                {
                    "sent": "The policy isn't like 1 action that you use in all states.",
                    "label": 0
                },
                {
                    "sent": "The policy says for each state.",
                    "label": 0
                },
                {
                    "sent": "Here's the action you need to take.",
                    "label": 0
                },
                {
                    "sent": "And so the policy is that full mapping for all the states.",
                    "label": 0
                },
                {
                    "sent": "But it's possible that at a particular state, by there's two actions that give you equally good reward.",
                    "label": 0
                },
                {
                    "sent": "So that's why we say that the optimal policy is not necessarily unique.",
                    "label": 0
                },
                {
                    "sent": "But then the optimal value is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's finally get to the stage where we're going to find a good policy.",
                    "label": 0
                },
                {
                    "sent": "I'll present a slightly less naive algorithm, not a whole lot less, and then a slightly less naive one, and then colleagues will follow me throughout the year that we can present some really smart algorithms to do this.",
                    "label": 0
                },
                {
                    "sent": "So I'm presenting the fundamentals here today, which are really used for small problems as well as theoretical analysis.",
                    "label": 0
                },
                {
                    "sent": "So the first algorithm is called policy iteration.",
                    "label": 0
                },
                {
                    "sent": "We're not very imaginative with the names in this field.",
                    "label": 0
                },
                {
                    "sent": "There's not going to be 150 flavors of Gans.",
                    "label": 0
                },
                {
                    "sent": "There's going to be value iteration and policy iteration.",
                    "label": 0
                },
                {
                    "sent": "So in the case of policy iteration, we assume that we start with a random policy.",
                    "label": 0
                },
                {
                    "sent": "So let's assume in each state Ridge is going to randomly pick something to do so.",
                    "label": 0
                },
                {
                    "sent": "If I'm unemployed, I will randomly pick something to do.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll apply for a faculty job and see how that goes.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to interleave 2 steps in iteration.",
                    "label": 0
                },
                {
                    "sent": "So step number one is going to be to compute the value of that policy.",
                    "label": 0
                },
                {
                    "sent": "So my initial policy, if I specify it is going to say for each of the states.",
                    "label": 0
                },
                {
                    "sent": "Here's an action to do.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to compute V \u03c0, and I can do that in closed form using the linear system of equations.",
                    "label": 0
                },
                {
                    "sent": "We rarely do that, but you can, and you can do that with iterative method using the iterative policy evaluation method I just presented.",
                    "label": 0
                },
                {
                    "sent": "And once you have the Pi as the value of that particular policy, then I'm going to compute a new policy Pi prime.",
                    "label": 0
                },
                {
                    "sent": "That's going to be greedy with respect to that particular policy, V. \u03a0, and so in this case I'm going to actually.",
                    "label": 0
                },
                {
                    "sent": "Do this in iterations until I'm done an.",
                    "label": 0
                },
                {
                    "sent": "My policy doesn't change anymore and if I do several rounds of that, I can actually show that this is going to converge for the simple reason that I have a finite number of policy, so I can in the worst case go back to my naive algorithm, which is to enumerate them all.",
                    "label": 0
                },
                {
                    "sent": "And thank you.",
                    "label": 0
                },
                {
                    "sent": "Just going to make sure I don't want out.",
                    "label": 0
                },
                {
                    "sent": "We ran out of batteries on one thing we don't want to run out of batteries on two things in the same stock OK, and so in this case I'm going to stop in the case where my policy doesn't change anymore.",
                    "label": 0
                },
                {
                    "sent": "And so there's a few things in terms of computational cost for this.",
                    "label": 0
                },
                {
                    "sent": "One of them is you need to think about what's my computational cost for computing the pie.",
                    "label": 0
                },
                {
                    "sent": "One of them is what's my cost for computing that new policy.",
                    "label": 0
                },
                {
                    "sent": "That's better policy is going to be better is going to be an improvement in some States and I need to think about how many of these iterations do I need to do before I'm going to terminate.",
                    "label": 0
                },
                {
                    "sent": "So in the worst case, right, my number of repetition of round, maybe the number of policies that I have to consider.",
                    "label": 0
                },
                {
                    "sent": "So that's something to worry about.",
                    "label": 0
                },
                {
                    "sent": "The alternate methods for doing this is to say why am I bothering with computing the policy on every round?",
                    "label": 0
                },
                {
                    "sent": "Let's go back here, right?",
                    "label": 0
                },
                {
                    "sent": "I'm computing a value function here now I'm extracting a policy.",
                    "label": 0
                },
                {
                    "sent": "I'm computing a new value function, so on.",
                    "label": 0
                },
                {
                    "sent": "Let's take inspiration from the algorithm we have for policy evaluation, right?",
                    "label": 0
                },
                {
                    "sent": "The method with the iterative method we have an instead of.",
                    "label": 0
                },
                {
                    "sent": "Computing that only for fixed policy.",
                    "label": 0
                },
                {
                    "sent": "Let me fold in this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Improvements step within that and so this is what value iteration essentially does.",
                    "label": 0
                },
                {
                    "sent": "It's taking our Bellman equation and turning it into an iterative improvement.",
                    "label": 0
                },
                {
                    "sent": "Rules so we start with some initial value V0 for all of our States and then on each iteration.",
                    "label": 1
                },
                {
                    "sent": "This looks a lot like the equation we have for policy evaluation.",
                    "label": 1
                },
                {
                    "sent": "The only modification is that on each round, instead of updating my value based on just the reward and the next value for a particular fixed policy, I'm going to fold in my greedy improvement.",
                    "label": 0
                },
                {
                    "sent": "So my step of maximizing over my actions right in that.",
                    "label": 0
                },
                {
                    "sent": "Update rule so for each of my states I will look at for all possible actions I can take for that action.",
                    "label": 0
                },
                {
                    "sent": "What reward would I get?",
                    "label": 0
                },
                {
                    "sent": "Where does it take me to an what's the value of that new state in this algorithm stops also when the change of value between two iterations is below some threshold and the algorithm is going to converge in the case where you have your gamma discount factor that imposes contraction on it.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Essentially three related algorithms, right?",
                    "label": 0
                },
                {
                    "sent": "The first one policy evaluation fixes.",
                    "label": 0
                },
                {
                    "sent": "The policy can only estimate the value.",
                    "label": 1
                },
                {
                    "sent": "Algorithm #2 it finds the best policy at each state.",
                    "label": 1
                },
                {
                    "sent": "And then mixes rounds of policy evaluation, which is algorithm 1 N rounds of greedy improvement in value iteration instead essentially folds in the greedy improvement right into the policy evaluation.",
                    "label": 1
                },
                {
                    "sent": "So does all of this in a combined update rule to find the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "And so we can look briefly at the time complexity of each of these steps, right?",
                    "label": 0
                },
                {
                    "sent": "We can think about how we're going to do policy evaluation.",
                    "label": 1
                },
                {
                    "sent": "There's different ways to look at the complexity of that, but if we look at the simple case, which is for solving the system of linear equations, then that's cubic number of operations in terms of the number of states.",
                    "label": 0
                },
                {
                    "sent": "If we look at policy iteration that has the policy evaluation step in there, so the cube term is over here.",
                    "label": 0
                },
                {
                    "sent": "And then greedy improvement essentially requires you to estimate the Bellman equation once for every state.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "If we look at value iteration, you're essentially finding that greedy update rule without doing full evaluation, so you save yourself some operations.",
                    "label": 0
                },
                {
                    "sent": "You have a per iteration complexity of S ^2 * A, but that's kind of obscure skating.",
                    "label": 0
                },
                {
                    "sent": "A really important factor, which is how many iterations do you need to do that right?",
                    "label": 0
                },
                {
                    "sent": "And so people often wonder, am I better off using value iteration?",
                    "label": 0
                },
                {
                    "sent": "I'm better off using policy iteration in terms of per.",
                    "label": 0
                },
                {
                    "sent": "Iteration cost value iteration is definitely better than policy iteration.",
                    "label": 0
                },
                {
                    "sent": "If you do it the naive way.",
                    "label": 0
                },
                {
                    "sent": "What often happens in practice?",
                    "label": 0
                },
                {
                    "sent": "This policy iteration requires many fewer iterations.",
                    "label": 0
                },
                {
                    "sent": "Then value iteration.",
                    "label": 0
                },
                {
                    "sent": "But we don't have really good bounds on that in terms of we have very loose bounds on the number of iterations, But we don't have very good results that show us for specific problem which will be faster or slower.",
                    "label": 0
                },
                {
                    "sent": "Back from.",
                    "label": 0
                },
                {
                    "sent": "The value iteration.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly.",
                    "label": 1
                },
                {
                    "sent": "So that was the point of my slide with the tiny equation font, which is if you have the optimal value, you can extract the policy quite easily and it's not very hard to see if I look over here at this equation when things have converged.",
                    "label": 0
                },
                {
                    "sent": "If I run this equation with an argmax operator instead of a Max over here.",
                    "label": 0
                },
                {
                    "sent": "So instead of finding which action maximizes with the maximal value, I ask which action maximize this particular equation.",
                    "label": 0
                },
                {
                    "sent": "So this equation here the complexity of evaluating that.",
                    "label": 0
                },
                {
                    "sent": "Is a * S squared right?",
                    "label": 0
                },
                {
                    "sent": "For each state?",
                    "label": 0
                },
                {
                    "sent": "I need to look at each possible action and then I need to see where are all the next states I might see, and so that's the complexity of this.",
                    "label": 0
                },
                {
                    "sent": "Retrieving the last policy, but I do that just once at the very end.",
                    "label": 1
                },
                {
                    "sent": "The two will converge to the same optimal value function, right?",
                    "label": 0
                },
                {
                    "sent": "But we know there can be multiple optimal policy, so they will converge to the same value function under some conditions, but those conditions are met for discrete state in action spaces for continuous state and action spaces.",
                    "label": 0
                },
                {
                    "sent": "I can't run these kinds of Belmond iteration because I might never see the same state again.",
                    "label": 0
                },
                {
                    "sent": "Then we need to we need to handle that case separately, but for discrete and often people will talk about tabular reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning the tabular is the case where you can discreetly nominate this statement actions, and you can essentially.",
                    "label": 0
                },
                {
                    "sent": "Keep your values in a big table.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "There is an optimal spelaeus policy that is not stochastic, yes.",
                    "label": 0
                },
                {
                    "sent": "So in the case of tabular, so discrete state in action spaces there is an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "There may be more than one optimal policy, but there's an optimal deterministic policy, so we talked about deterministic versus stochastic, and I've been mostly string in the space of deterministic policy because for these particular cases deterministic policies are sufficient.",
                    "label": 0
                },
                {
                    "sent": "Now there's reasons you may want to look at policy.",
                    "label": 0
                },
                {
                    "sent": "That are stochastic.",
                    "label": 0
                },
                {
                    "sent": "Peter.",
                    "label": 0
                },
                {
                    "sent": "A deal will give you many good reasons this afternoon.",
                    "label": 0
                },
                {
                    "sent": "Why that's something that in some cases very sensible to do particular deals with the fact that when you're dealing with larger state action, space is not feasible to enumerate all the actions.",
                    "label": 0
                },
                {
                    "sent": "It's not feasible to look at all the policies and then having notions of gradients.",
                    "label": 0
                },
                {
                    "sent": "How much policy improvement can I get in using gradients to do that?",
                    "label": 0
                },
                {
                    "sent": "It's a lot easier to do with stochastic policies and deterministic policies.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's look at a simple example just to get a taste for how these things go.",
                    "label": 0
                },
                {
                    "sent": "This is a really simple agent that moves in the different Cardinal directions.",
                    "label": 0
                },
                {
                    "sent": "This particular agent is limited to a grid of size 3 by 4.",
                    "label": 0
                },
                {
                    "sent": "Usually start somewhere up here and if it ends up over here, it gets a plus one reward and it but falls in the dump over there gets a negative 10 rewards, so you can imagine that it wants to get here can start in.",
                    "label": 0
                },
                {
                    "sent": "I can move in four different Cardinal directions up, down, left, or right.",
                    "label": 0
                },
                {
                    "sent": "Those are it's for actions, whether stochastic actions, so there's usually a 70% chance going to end up where it wanted and a 10% chance is going to randomly go in one of the other directions, accidental E. And so in this case we're using a pretty high discount factor.",
                    "label": 0
                },
                {
                    "sent": "Close to one I should say, and so point 99.",
                    "label": 0
                },
                {
                    "sent": "So that means that there's a little bit of discounting, but not very much.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We start value iteration at round Number one.",
                    "label": 0
                },
                {
                    "sent": "I've initialized my values to be equal to the reward function on round Number one.",
                    "label": 0
                },
                {
                    "sent": "Ann, I run this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For one round after one round of value iteration, this is what the values will look like, right?",
                    "label": 1
                },
                {
                    "sent": "If I run my valuation algorithm?",
                    "label": 0
                },
                {
                    "sent": "Essentially, it's saying in this state, let's look at all the actions I can take for each of these actions.",
                    "label": 0
                },
                {
                    "sent": "Let's see where I can end up and what's the value of ending up in there.",
                    "label": 0
                },
                {
                    "sent": "So for all of the states over here, they can end up in neighboring states, but all those states had an initial value of 0, and so the value stays 0 even after one round of value iteration.",
                    "label": 0
                },
                {
                    "sent": "We see the neighboring states which had neighbors with rewarded with value information.",
                    "label": 0
                },
                {
                    "sent": "Pick up some of that value information.",
                    "label": 0
                },
                {
                    "sent": "In this case, it seems like potentially you know the best action to do might be to avoid the negative 10 right to move left, and in this case the best action might be to move downwards to be able to avoid these kinds of.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That place is if you run this for several rounds.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At some point the value changes and converges to some amount that is quite consistent.",
                    "label": 0
                },
                {
                    "sent": "What I've plotted down here is what we call the bellman residual.",
                    "label": 1
                },
                {
                    "sent": "That's the maximum difference in value over all of the States and so in this case we see that's a phone number that's essentially getting smaller and smaller because of that discount factor that's imposing the contraction, and so after some time we have a value function that is plotted over here and now we ask, how do we extract the policy for this particular value function?",
                    "label": 0
                },
                {
                    "sent": "And to do that you need to apply your Bellman value function again.",
                    "label": 0
                },
                {
                    "sent": "But it's quite intuitive to look at what the policy might be for doing that.",
                    "label": 0
                },
                {
                    "sent": "I usually have like a plot of the policy, but I see I didn't put it in today.",
                    "label": 0
                },
                {
                    "sent": "Essentially, most of these states, right?",
                    "label": 0
                },
                {
                    "sent": "The policy is to go right towards the plus one.",
                    "label": 0
                },
                {
                    "sent": "In some cases though, the policy over here in particular is to go left.",
                    "label": 0
                },
                {
                    "sent": "Because if you go through the top over here, there's a pretty good chance you're going to end up accidentally falling in the pit, and so the policy over here is to move to the left.",
                    "label": 0
                },
                {
                    "sent": "In this case, you can just look at it greedily by saying for each state you know which neighboring state has the higher value, and the policy is going to be imposed in that direction.",
                    "label": 0
                },
                {
                    "sent": "That's essentially what's going to come out of running the Bellman equation for that.",
                    "label": 0
                },
                {
                    "sent": "And you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do that for larger domains, right?",
                    "label": 0
                },
                {
                    "sent": "The four rooms domain is another one that's commonly used when you do your value function updates.",
                    "label": 0
                },
                {
                    "sent": "If you're trying this on your own at some point, it's usually a good sign.",
                    "label": 0
                },
                {
                    "sent": "If your value information is sort of radiating away from the reward points when you initialize your value function to be equal to the rewards.",
                    "label": 0
                },
                {
                    "sent": "So there's a way to sort of easily diagnose whether your value iteration algorithm is doing what it should should be.",
                    "label": 1
                },
                {
                    "sent": "Sort of radiating away from the points of reward over here.",
                    "label": 0
                },
                {
                    "sent": "There's reward over here.",
                    "label": 0
                },
                {
                    "sent": "The reward gets propagated away from the goal, and if you have a more complex value function with different reward points, you see these same kinds of effects.",
                    "label": 0
                },
                {
                    "sent": "The reward information propagating through.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've presented these notions of value, function, value iteration and policy iteration in pretty strict.",
                    "label": 1
                },
                {
                    "sent": "Terms, it turns out that you have quite a bit of flexibility on some of the fronts in terms of how you implement this.",
                    "label": 0
                },
                {
                    "sent": "So for example, for value iteration it's possible to do what we call asynchronous updates, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe you already know that some states are a lot more important than other states, so let's not run these Bellman backups synchronously through all of the States and do the same number of passes through all of the states.",
                    "label": 0
                },
                {
                    "sent": "There may be a way to establish a priority order.",
                    "label": 0
                },
                {
                    "sent": "Maybe excuse me, maybe you can generate some trajectory through your MDP as you generate the trajectories.",
                    "label": 0
                },
                {
                    "sent": "You get some statistics about which states are visited.",
                    "label": 0
                },
                {
                    "sent": "Often you do more value backups around those states.",
                    "label": 0
                },
                {
                    "sent": "If some states are never visited, it's probably not worth wasting computation on them.",
                    "label": 0
                },
                {
                    "sent": "You can also choose to update the states whenever they appear on the trajectory and so on.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it's the same with generalized policy iteration.",
                    "label": 0
                },
                {
                    "sent": "So if you want to do policy iteration right, there's two steps in policy iteration.",
                    "label": 0
                },
                {
                    "sent": "One is when you do the policy evaluation and one of them is when you do the policy improvement, you don't need to systematically do evaluation.",
                    "label": 0
                },
                {
                    "sent": "An improvement on all of the states at every round.",
                    "label": 1
                },
                {
                    "sent": "You can sort of mix improvements and evaluation steps in a non uniform way that represents some other information you have about the domain.",
                    "label": 0
                },
                {
                    "sent": "And so this gives you some flexibility and they can be the way to get drastic improvement in terms of time complexity for applying this on some larger domains.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so let's step out for a second about these very simple cases that are the fundamentals of reinforcement learning and talk a little bit about what are some of the harder challenges that we face right?",
                    "label": 0
                },
                {
                    "sent": "One of the things that is.",
                    "label": 0
                },
                {
                    "sent": "Quite difficult in many cases, and I'm not going to be able to give you too many solutions on that is how to design the problem domain, state, action and cost.",
                    "label": 0
                },
                {
                    "sent": "Reward one of them which will start talking about in a few minutes is this.",
                    "label": 0
                },
                {
                    "sent": "Notion of incorporating learning into this.",
                    "label": 0
                },
                {
                    "sent": "So far I've really been in just planning and solving.",
                    "label": 0
                },
                {
                    "sent": "How do we incorporate in our data and learning?",
                    "label": 0
                },
                {
                    "sent": "And then we'll get to some of the later topics down here, but it is a good time to take some questions, yes.",
                    "label": 0
                },
                {
                    "sent": "When what sorry?",
                    "label": 0
                },
                {
                    "sent": "When facing up problems, how do you choose gamma?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so.",
                    "label": 0
                },
                {
                    "sent": "For many years I thought that gamma should be part of the environment.",
                    "label": 0
                },
                {
                    "sent": "Someone should tell you what gamma is, right?",
                    "label": 0
                },
                {
                    "sent": "I'm not designing the environment, I'm solving the system, so someone who's specifying the environment should tell me how to discount my future versus my present.",
                    "label": 0
                },
                {
                    "sent": "And whenever you're in the case of planning so the case I've described so far where your transition probabilities are rewarded specified, you should also expect someone some problem designer in along with the first bullet to specify your discount factor.",
                    "label": 0
                },
                {
                    "sent": "When we're moving to the case where we are dealing with learning, so assume that your transition probability, your reward function are estimated from data samples.",
                    "label": 0
                },
                {
                    "sent": "And then there's a recent results in the literature that that are suggesting that you need to think of gamma as a hyperparameter, and in particular, if we look at our Bellman equation that's balancing immediate versus future.",
                    "label": 0
                },
                {
                    "sent": "If my transition probabilities are poorly estimated, my value function is poorly estimated.",
                    "label": 0
                },
                {
                    "sent": "Maybe I should use a more aggressive gamma to reduce the variance that comes in from poorly knowing the future values, and so using gamma as?",
                    "label": 0
                },
                {
                    "sent": "The more hyper parameter that I can tune based on how complex so I want my plan to be versus how much data do I have is one thing that's really starting to get a little bit more traction in the last few years, and there's a few interesting papers and that's two to three years on this.",
                    "label": 0
                },
                {
                    "sent": "Yes, you learn what?",
                    "label": 0
                },
                {
                    "sent": "Scammer.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In some sense, yes.",
                    "label": 0
                },
                {
                    "sent": "If you buy learn you mean like estimates, right?",
                    "label": 0
                },
                {
                    "sent": "In some cases in some models, if you think of supervised learning as having Lambda, your parameter that regularizes that weighs how much regularization you want, right?",
                    "label": 0
                },
                {
                    "sent": "If you have a loss function that has like a.",
                    "label": 0
                },
                {
                    "sent": "Supervised criteria and then you have your model complexity that swayed by Lambda.",
                    "label": 0
                },
                {
                    "sent": "Usually you'll use maybe cross validation to estimate Lambda, so in that sense you can learn it, you can adapt it in a way, and so I think we don't have all the answers of how to adapt gamma yet in reinforcement learning, but I think that intuition is starting to permeate and there's going to be interesting results coming through.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "We want function was defined when you are leaving a state of doesn't make a difference in the formulation.",
                    "label": 0
                },
                {
                    "sent": "If your reward is defined as you're entering a state.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there it doesn't make a difference, it's just you need to be a little bit careful handling how you take the expectation, right?",
                    "label": 0
                },
                {
                    "sent": "So in some most of the literature.",
                    "label": 0
                },
                {
                    "sent": "Right, they define RSA.",
                    "label": 0
                },
                {
                    "sent": "In some papers they define Rs S prime.",
                    "label": 0
                },
                {
                    "sent": "So it's a function of where you are, what you took in, where you're going, and eventually you could define it as a function of a S prime, which is where you're going.",
                    "label": 0
                },
                {
                    "sent": "All of these can be made equivalent by just taking expectation appropriately with respect your transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "In some cases, maybe just Rs, and in that case you assume that the reward just doesn't depend on the action.",
                    "label": 0
                },
                {
                    "sent": "OK, I will make some grounds and we'll have time for questions at the end so.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's talk specifically first about learning.",
                    "label": 0
                },
                {
                    "sent": "In particular, how do we do on line reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                },
                {
                    "sent": "Many cases where you don't have your transition probability or reward function in advance, you need to acquire this, right?",
                    "label": 0
                },
                {
                    "sent": "You need to take some actions, see how well you do get that information and leverage that.",
                    "label": 0
                },
                {
                    "sent": "In some cases the TD Gammon system was based on that there was an agent playing against itself through many millions of games to be able to evolve a good function, and so in that case you have what we call the reinforcement learning loop, and that's probably one of the things you'll see on the more practical, hands-on session.",
                    "label": 0
                },
                {
                    "sent": "There has to be a notion that you can.",
                    "label": 0
                },
                {
                    "sent": "Take an action, observe the effect of that action so that action causes a transition in the system.",
                    "label": 0
                },
                {
                    "sent": "You get to observe what's the new state you're in?",
                    "label": 0
                },
                {
                    "sent": "What reward did you get for that action, and then you adjust something.",
                    "label": 0
                },
                {
                    "sent": "What you adjust changes depending on different paradigms, but you can adjust your policy.",
                    "label": 0
                },
                {
                    "sent": "You can adjust your value function.",
                    "label": 0
                },
                {
                    "sent": "You can adjust your Q function.",
                    "label": 0
                },
                {
                    "sent": "You can adjust your model of transitions and rewards, but there's some adjustment is.",
                    "label": 0
                },
                {
                    "sent": "There's some learning that's going to happen, and then based on that learning, you're going to be able to take a new action.",
                    "label": 0
                },
                {
                    "sent": "So most human animal agents operate in this kind of a loop, and so there's been quite a bit of work and reinforcement learning that is on what we call more the on line learning.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case, and so there's a few different classes of approach for online learning.",
                    "label": 0
                },
                {
                    "sent": "One of them is based on very simple Monte Carlo estimation principles, which is to say we are going to look at samples of our empirical returns.",
                    "label": 0
                },
                {
                    "sent": "So I go back to this notion of you as my empirical return an I'm going to look at the difference between what's my empirical return, which I'm observing for this trajectory, so I'm going to fix my policy, run a trajectory right, like play one game of backgammon, see how it goes.",
                    "label": 0
                },
                {
                    "sent": "Estimate my return.",
                    "label": 0
                },
                {
                    "sent": "You have S and now compare how far this return that I've observed on.",
                    "label": 0
                },
                {
                    "sent": "This trajectory differs from my current estimate of the value function for that state.",
                    "label": 0
                },
                {
                    "sent": "Call this my error signal.",
                    "label": 0
                },
                {
                    "sent": "I'm going to weigh that by learning rate.",
                    "label": 0
                },
                {
                    "sent": "We're going to call this Alpha and we're going to update our value function in proportion to this area signal.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially just an error update rule right here.",
                    "label": 0
                },
                {
                    "sent": "Alpha can be thought of as the learning parameter.",
                    "label": 0
                },
                {
                    "sent": "You is your actual prediction.",
                    "label": 0
                },
                {
                    "sent": "Your actual observed sample envy is what your model predicted, and so you're going to do some correction based on that.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a gradient rule, but just look like it doesn't require you to know the reward function.",
                    "label": 0
                },
                {
                    "sent": "It doesn't require you to know the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "What it does require you to do typically is run many, many rounds of this, because typically this.",
                    "label": 0
                },
                {
                    "sent": "Empirical return is over the full trajectory, so if you have a lot of stochastic city in your system, you're going to need many trajectories to properly estimate this particular return.",
                    "label": 0
                },
                {
                    "sent": "So these methods, in particular when the planning horizon are along when the trajectories are along.",
                    "label": 0
                },
                {
                    "sent": "This method tends to have quite a bit of variance, and so you need a lot of sample to estimate it properly, but we can show that it actually gives you an unbiased estimate of your value function V when you do this.",
                    "label": 0
                },
                {
                    "sent": "Jeanette Lee there is this.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Acting class of method called the temporal difference learning methods and written this afternoon will tell you much more about this particular class of learning.",
                    "label": 0
                },
                {
                    "sent": "The idea of temporal difference learning is to say.",
                    "label": 0
                },
                {
                    "sent": "Instead of waiting till the end of my trajectory to estimate my return.",
                    "label": 0
                },
                {
                    "sent": "Let's try to estimate that return based on just based on essentially a Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "So instead of estimating you at the end of the trajectory, I'm going to replace this by the immediate rewards.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take the one step sampled reward, and then for the future reward.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to wait till the end of the trajectory, I'm just going to plug in my current estimate of the value function at the next state and discount that so the V over here is.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to the V over there, right?",
                    "label": 0
                },
                {
                    "sent": "That's my current prediction.",
                    "label": 0
                },
                {
                    "sent": "Of what's the empirical with the expected return at State St?",
                    "label": 0
                },
                {
                    "sent": "And this part over here.",
                    "label": 0
                },
                {
                    "sent": "Together, that is my empirical observation of what it is.",
                    "label": 0
                },
                {
                    "sent": "Now, one thing to note is that my function V shows up sort of on the left side of my error signal and it shows up on the right side also.",
                    "label": 0
                },
                {
                    "sent": "Right, if you do this for supervised learning, it looks more like this, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe you're trying to do regression with a gradient descent approach over here this you would be within my regression.",
                    "label": 0
                },
                {
                    "sent": "What is my actual output?",
                    "label": 0
                },
                {
                    "sent": "Why that I'm observing in this be would be?",
                    "label": 0
                },
                {
                    "sent": "What did my model predict the why should be and I look at the difference between those two.",
                    "label": 0
                },
                {
                    "sent": "So in the supervised learning case EU is a sample and V as predicted, but one of them is the ground truth, right U is the ground truth and.",
                    "label": 0
                },
                {
                    "sent": "Most of the time we assume that this is correct.",
                    "label": 0
                },
                {
                    "sent": "When we do TD Learning V over here is produced by our model.",
                    "label": 0
                },
                {
                    "sent": "Our current estimate of Y, and now this part.",
                    "label": 0
                },
                {
                    "sent": "That should be the ground truth.",
                    "label": 0
                },
                {
                    "sent": "Well, there's one little bit of ground truth which is R. And then there's this piece V that's also our model.",
                    "label": 0
                },
                {
                    "sent": "And so when our model is poorly estimated when we haven't seen a lot of data, this estimate of the error tends to be very noisy.",
                    "label": 0
                },
                {
                    "sent": "And so in many cases this TD learning approach.",
                    "label": 0
                },
                {
                    "sent": "Can lead to instability, not in the case where we have discrete set of states.",
                    "label": 0
                },
                {
                    "sent": "If we're in the tabular case, discrete discreet actions, things go quite well.",
                    "label": 0
                },
                {
                    "sent": "We can actually show that we have convergence and so on.",
                    "label": 0
                },
                {
                    "sent": "But when we start dealing with very large continuous state space, we'll see that we have stability problems, and many of the things that have been, I would say put forth over the last two years in terms of the Deep Q network and so on.",
                    "label": 0
                },
                {
                    "sent": "Really look at how to have better stability.",
                    "label": 0
                },
                {
                    "sent": "On this kind of estimator specifically, for this reason that the V function shows up on both sides and it gives you for stability, yes?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's because at STO the RT plus one.",
                    "label": 0
                },
                {
                    "sent": "This is the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "I should probably use RT over here, but then it's the immediate reward.",
                    "label": 0
                },
                {
                    "sent": "Plus gamma times the value at the next one.",
                    "label": 0
                },
                {
                    "sent": "Those two composed together these two pieces together correspond to UST, right?",
                    "label": 0
                },
                {
                    "sent": "That's my current estimate of what's the empirical return, so I'm going to replace just the immediate reward, bias sample observation and the future.",
                    "label": 0
                },
                {
                    "sent": "I'm going to plug in my current best estimator for the value function.",
                    "label": 0
                },
                {
                    "sent": "So this team.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is essentially what was used in TD Gammon, with the addition that the value function wasn't expressed in a tabular case, but the value function was actually expressed with a neural network and will get to that in a second.",
                    "label": 0
                },
                {
                    "sent": "But in cases where your number of states gets too large and you can't list the value function for all of them, you can actually replace that by a function estimator.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's talk about this question of function approximation particular.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function approximation is used in all these cases where you have too many states or you have continuous states where you can actually go and list the full set of states.",
                    "label": 0
                },
                {
                    "sent": "So in robotics, in most games this is crucial to getting good results.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if we look at how to do this?",
                    "label": 0
                },
                {
                    "sent": "Essentially what I'm positing is that my value function or my Q function can be replaced by.",
                    "label": 0
                },
                {
                    "sent": "Approximation, and here I've put in sort of the simplest form of approximation that we consider, which is a linear approximation, and so in this case I'm going to assume I have a set of features.",
                    "label": 0
                },
                {
                    "sent": "These features maybe features of the board right where my pieces on the board, how many of such and such pieces do I have?",
                    "label": 0
                },
                {
                    "sent": "These features can be designed manually.",
                    "label": 0
                },
                {
                    "sent": "For some games, an now instead of learning my function Q or V directly, I'm going to learn these linear coefficients.",
                    "label": 0
                },
                {
                    "sent": "Such that when I take the linear combination of my features in my weight stayed are my weights that I'm estimating then I get a value function.",
                    "label": 0
                },
                {
                    "sent": "So tabular case when we can list the States, we estimate V directly.",
                    "label": 0
                },
                {
                    "sent": "Cases where I have large state space, I assume that there is a functional form to define my value function and I estimate the parameters of that.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Function and so the linear function is the simplest one that has been used for many years.",
                    "label": 0
                },
                {
                    "sent": "Over here I'm picturing one of the classic problem of reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "They mounted car domain where you have a little car that's trying to get up a Hill and it doesn't have quite enough acceleration to get to the top.",
                    "label": 0
                },
                {
                    "sent": "So you need to kind of go back and forth a few times to build up momentum.",
                    "label": 0
                },
                {
                    "sent": "In that case, the state space.",
                    "label": 0
                },
                {
                    "sent": "Is usually the position of the vehicle along the curve as well as its velocity.",
                    "label": 0
                },
                {
                    "sent": "So you have a 2 dimensional state space.",
                    "label": 0
                },
                {
                    "sent": "It can be discretized or continuous.",
                    "label": 0
                },
                {
                    "sent": "An OPT in.",
                    "label": 1
                },
                {
                    "sent": "We've used linear function approximation to represent the function of different actions.",
                    "label": 0
                },
                {
                    "sent": "It's usually cast in this case, not with a continuous acceleration in terms of actions, but with an off acceleration.",
                    "label": 1
                },
                {
                    "sent": "So you have binary action choices in this case, so you can have a different linear function for each of your actions.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As we get to larger and larger domains and will see many of them this week, you need fancier function approximation because you have larger state spaces and more complex representation are needed.",
                    "label": 1
                },
                {
                    "sent": "So a lot of the work that has been published, for example by some of the deep mind researchers and other people active in deep reinforcement learning, looks at taking as your state representation.",
                    "label": 0
                },
                {
                    "sent": "So I'm really raw version of your observations.",
                    "label": 0
                },
                {
                    "sent": "In this case, the case from the Atari learning environment, where the goal is to learn to play several different.",
                    "label": 0
                },
                {
                    "sent": "Atari game, so in that case the input space, the state space is really in the space of pixels.",
                    "label": 0
                },
                {
                    "sent": "And rather than look at linear function approximation, we use a convolutional neural net to capture the value function so the convolutional neural net essentially evolves a representation of the features from the raw input space and the output of the convolutional neural net is not a label.",
                    "label": 0
                },
                {
                    "sent": "It's not like is this PAC man or not, PAC man, it's really a value function, so it's the value for a particular state in the system.",
                    "label": 0
                },
                {
                    "sent": "We can learn that an from that value function extract an optimal policy when we've done that.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An as deep reinforcement learning has been broadening to other games that are you learning environment?",
                    "label": 0
                },
                {
                    "sent": "For example, there's been some work on using reinforcement learning in Minecraft.",
                    "label": 0
                },
                {
                    "sent": "You might get a demo of that later this afternoon.",
                    "label": 0
                },
                {
                    "sent": "You can incorporate many of the newest technology from deep learning, for example, notions of memory and context in attention, and so on can all be incorporated into that representation of the value function, and what changes is that the output of the value function is no longer label.",
                    "label": 0
                },
                {
                    "sent": "But it's an estimate of the value function for that particular state, and so as research progresses on deep learning architectures and models, reinforcement learning leverages that in most cases.",
                    "label": 0
                },
                {
                    "sent": "Through the use of more complex models for approximating the value function.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me close off with just a few bits and pieces.",
                    "label": 0
                },
                {
                    "sent": "I call it the IRL lingo.",
                    "label": 0
                },
                {
                    "sent": "Is these kinds of terminology that comes up which when you've been in the field for five or ten years, you've kind of developed the understanding of all this.",
                    "label": 0
                },
                {
                    "sent": "But if you haven't been in the field, it's useful to get a sense of what these terminology might be because it will pop up in several of the talks, probably in the next few days.",
                    "label": 0
                },
                {
                    "sent": "I've already talked about episodic versus continuous task.",
                    "label": 0
                },
                {
                    "sent": "I've also already talked about batch learning versus.",
                    "label": 0
                },
                {
                    "sent": "I'm learning mostly I talked about online learning and I didn't talk much about batch learning.",
                    "label": 0
                },
                {
                    "sent": "You'll hear more about it later on, but let me go through a few of these also quickly.",
                    "label": 0
                },
                {
                    "sent": "In the five minutes we have left, one of them is the difference between an on policy reinforcement learning system and an off policy reinforcement.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bing system that goes back to something I mentioned early today, which is that when you change your policy in your RL agent, when you change how you choose your action, you're essentially inducing and you distribution over the states, right?",
                    "label": 0
                },
                {
                    "sent": "You change how you play the game.",
                    "label": 0
                },
                {
                    "sent": "You're going to see different States and so in some cases.",
                    "label": 0
                },
                {
                    "sent": "You can actually show that the data distribution changes every time you change the policy, and one of the challenges with learning from a batch of data is that this batch of data was collected under a particular policy, right?",
                    "label": 0
                },
                {
                    "sent": "Your agent had a particular strategy choice of action for collecting the data.",
                    "label": 0
                },
                {
                    "sent": "Now if you tried to do reinforcement learning, it's possible that you're estimating a different policy.",
                    "label": 0
                },
                {
                    "sent": "Think of the case where you're doing policy evaluation.",
                    "label": 0
                },
                {
                    "sent": "You're estimating the value of a different policy.",
                    "label": 0
                },
                {
                    "sent": "But you're trying to do that with data that is distributed according to the data collection policy, right?",
                    "label": 0
                },
                {
                    "sent": "What we call the exploration policy, and if these two policies start diverging too much, then your ability to properly evaluate the policy your candidate policy is going to weaken as your distributions become further and further apart, and so one of the consequences of that is typically you need very very large batches of data.",
                    "label": 0
                },
                {
                    "sent": "I mentioned earlier that you need and sensualita have tried every action from every state.",
                    "label": 0
                },
                {
                    "sent": "That's not possible in some cases, right?",
                    "label": 0
                },
                {
                    "sent": "So if you think of a.",
                    "label": 0
                },
                {
                    "sent": "System that is trained for example to play a very large game.",
                    "label": 0
                },
                {
                    "sent": "Think of the system that was trained to play go.",
                    "label": 0
                },
                {
                    "sent": "It wasn't possible to look at all the different actions in all the different States and Furthermore right they were stuck to a distribution of data that they had observed from previous games.",
                    "label": 0
                },
                {
                    "sent": "But that tells you nothing about what might be the distribution of data.",
                    "label": 0
                },
                {
                    "sent": "If you start playing in a very different way.",
                    "label": 0
                },
                {
                    "sent": "If you have a simulator, things are OK because you can go and try out your new strategy and get new data every time you consider a new policy, you can run that policy and get the data.",
                    "label": 0
                },
                {
                    "sent": "But in cases where the data has been collected apriori and you're stuck with that, you need to think about how to correct for the differences in distributions in the data, and so one of the ways that is used to correct this is using an important sampling measure.",
                    "label": 0
                },
                {
                    "sent": "So we look at an important factor between the behavior policy.",
                    "label": 0
                },
                {
                    "sent": "That's usually the policy we used to collect the data and Pi are target policy.",
                    "label": 0
                },
                {
                    "sent": "That's a policy we're wishing to evaluate right now, and we look at the difference between those and we re way our data according to this important factor for.",
                    "label": 0
                },
                {
                    "sent": "Each state in action we re weigh the sample according to this.",
                    "label": 0
                },
                {
                    "sent": "Those of you who've worked with important sampling, maybe in other cases, know that in many cases as your policy, start diverging.",
                    "label": 0
                },
                {
                    "sent": "Your important factors can get really, really small, and so there's two problems.",
                    "label": 0
                },
                {
                    "sent": "In one case, if you've never tried something under the behavior policy, then essentially right, you can't say much about it.",
                    "label": 0
                },
                {
                    "sent": "And if you have your policy are really different, then there's not much that you can say about it, so you typically still need really a lot of data to use important sampling correction.",
                    "label": 0
                },
                {
                    "sent": "There's more sophisticated mechanism, but that's one thing to watch out for.",
                    "label": 0
                },
                {
                    "sent": "If you're dealing with a batch of data and now trying to estimate.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any different policies?",
                    "label": 0
                },
                {
                    "sent": "The other thing to take into consideration is related to this.",
                    "label": 0
                },
                {
                    "sent": "Where is your data coming from?",
                    "label": 0
                },
                {
                    "sent": "How did you collect your data?",
                    "label": 0
                },
                {
                    "sent": "In some cases the space of possible data and things that you need to try to collect the data is very, very large.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you're collecting some data, it's tempting to start acting according to what you think might be the better policy and to stop what we call exploring.",
                    "label": 0
                },
                {
                    "sent": "Right exploring is the mechanism through which you randomize your choice of action to gather more diverse information.",
                    "label": 0
                },
                {
                    "sent": "Exploitation is the mechanism through which you start acting according to what you've collected so far, and what seems like the best strategy so far, right?",
                    "label": 0
                },
                {
                    "sent": "All of you are coming to University of Montreal for the first time last week, and this week probably found a path that somehow get you to this building.",
                    "label": 0
                },
                {
                    "sent": "Maybe you are using the same path every day and you don't dare explore and take other pass 'cause it's a big mountain.",
                    "label": 0
                },
                {
                    "sent": "What's one mechanism that you sometimes use to find another path?",
                    "label": 0
                },
                {
                    "sent": "You follow someone else right?",
                    "label": 0
                },
                {
                    "sent": "One day you meet someone at the bottom of the mountain and they go a different way.",
                    "label": 0
                },
                {
                    "sent": "And then you find another way.",
                    "label": 0
                },
                {
                    "sent": "So one practical way in which people have.",
                    "label": 0
                },
                {
                    "sent": "Overcome this exploration problem is through methods related to imitation learning, where you get an agent to demonstrate a good behavior and now you start saying that this is at least some example of a good trajectory.",
                    "label": 0
                },
                {
                    "sent": "You don't need to use random trial and error and explore all the paths around the Mount and.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally.",
                    "label": 0
                },
                {
                    "sent": "In some cases I haven't made a big distinction between these.",
                    "label": 0
                },
                {
                    "sent": "It will come through several of the talks this week, but there's some attention called the fact that some reinforcement learning methods or what we call model based methods, whereas others are called model free methods.",
                    "label": 0
                },
                {
                    "sent": "I'll just give you a brief definition of it and you can keep an eye out as you go through some of the other talks in model based learning.",
                    "label": 0
                },
                {
                    "sent": "We typically assume that we're going to get a lot of data and use this data to estimate or transition in our reward model, and then we will plug into some of the policy.",
                    "label": 0
                },
                {
                    "sent": "Value iteration methods that I presented earlier in model free RL.",
                    "label": 0
                },
                {
                    "sent": "We typically assume that we're only going to directly estimate the value function.",
                    "label": 0
                },
                {
                    "sent": "We will never carry over an estimate of the transition or the reward function.",
                    "label": 0
                },
                {
                    "sent": "Never an explicit estimate of them.",
                    "label": 0
                },
                {
                    "sent": "We will directly estimate the value function.",
                    "label": 0
                },
                {
                    "sent": "There's pros and cons to both of them.",
                    "label": 0
                },
                {
                    "sent": "I would say as kind of a one liner of pros and cons to both of them.",
                    "label": 0
                },
                {
                    "sent": "Whenever you're tackling large, difficult problems, it's useful to be able to put in domain knowledge.",
                    "label": 0
                },
                {
                    "sent": "In some cases it's easier to put the domain knowledge on the model itself, right?",
                    "label": 0
                },
                {
                    "sent": "You can constrain the dynamics of your system.",
                    "label": 0
                },
                {
                    "sent": "You can constrain the space of your reward function.",
                    "label": 0
                },
                {
                    "sent": "If that's the case, maybe model based is better.",
                    "label": 0
                },
                {
                    "sent": "In other cases, that's not feasible and you have some notion of regularity of your solution space.",
                    "label": 0
                },
                {
                    "sent": "In that case, model free might be more appropriate.",
                    "label": 0
                },
                {
                    "sent": "Essentially, where can you put in your domain knowledge to constrain the solution space efficiently is one way to think about this.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last piece of lingo I will clarify is the separation between what we call value function methods.",
                    "label": 0
                },
                {
                    "sent": "I've only talked even though I talked about policy iteration.",
                    "label": 0
                },
                {
                    "sent": "It's essentially all value function methods, methods that go through estimation of a value function on the other end, there's all the methods where you directly optimize the policy, and in that case this is going to be the material that's tackled by Peter.",
                    "label": 0
                },
                {
                    "sent": "Peter be later today, and so stay tuned for all of that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just a quick summary.",
                    "label": 0
                },
                {
                    "sent": "I'll take one more minute of your time.",
                    "label": 0
                },
                {
                    "sent": "Essentially, I hope you feel like you're a little bit more equipped to detect reinforcement learning problems as you go through the world in Solvay I problems.",
                    "label": 0
                },
                {
                    "sent": "Broadly, I would say one thing to keep in mind is this last point over here, which is your intuition about what's easy and what's hard that you've possibly acquired through other cases of machine learning.",
                    "label": 0
                },
                {
                    "sent": "Supervised learning in particular in some cases turns out to be quite different in reinforcement learning things that are.",
                    "label": 0
                },
                {
                    "sent": "Easy and one turned out to be harder than the other, so keep an eye out for that as you stick through the other talks.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I put this out.",
                    "label": 0
                },
                {
                    "sent": "This is probably not for today, but you will have it.",
                    "label": 0
                },
                {
                    "sent": "I should say all the slides, not just from my talk, but all of the speakers, the slides and the videos are going to be available on line.",
                    "label": 0
                },
                {
                    "sent": "In the case of my talk, if you're really in a hurry, you can probably look at the slides and videos from last year.",
                    "label": 0
                },
                {
                    "sent": "It's not all that different.",
                    "label": 0
                },
                {
                    "sent": "At the talks for the other slides and videos are going to be up the slides very shortly within a day or two.",
                    "label": 0
                },
                {
                    "sent": "Typically videos, probably within two weeks or so, and if that's not enough, there's a set of resources that you'll be able to reach out to.",
                    "label": 0
                },
                {
                    "sent": "I will close it at that.",
                    "label": 0
                },
                {
                    "sent": "For now, I'll take maybe one question and then let you have coffee.",
                    "label": 0
                }
            ]
        }
    }
}