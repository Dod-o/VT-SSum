{
    "id": "d3357pkmh2wxxt37zou7zunrrixnrgnz",
    "title": "Can Learning Kernels Help Performance?",
    "info": {
        "author": [
            "Corinna Cortes, Research at Google, Google, Inc."
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/icml09_cortes_clkh/",
    "segmentation": [
        [
            "Maybe I should just not have it working and it'll you'll hear me just as well.",
            "But now the microphone should be on an.",
            "I just also have to find out which one of these actually does something.",
            "Maybe you're he knows it.",
            "Both.",
            "Yes, here it is, at least OK.",
            "Thank you so much for the introduction.",
            "I'll try to stay here.",
            "I have running shoes on because those are the only issues that I have.",
            "My daughter has started commenting on if Mom can't get other kinds of shoes and I might actually need to get other kind of shoes as I have to go to San Diego at the end of this month and get the official plaque.",
            "What do we get your for?",
            "You must know.",
            "Say Mail it to me, but I wonder if I can show up and running shoes, and I think the answer is no.",
            "Anyway, thank you so much for showing up this early morning hour.",
            "I'm amazed how many of you made it out of bed.",
            "The title of this talk is can learning kernel help performance?",
            "And it's amazing to think of how specialized this community has become.",
            "Because if I just added one little word to the title, if any of these would work.",
            "This one doesn't work, so I had to use the other one.",
            "This one doesn't work either.",
            "Yeah, I know that one also works OK.",
            "So I will have to go back to that.",
            "We can give these may be OK if I just added one little word to the title, can learning."
        ],
        [
            "With kernels help performance, we had a completely different talk.",
            "Now this talk.",
            "I think we had an affirmative answer to about 15 years ago.",
            "But just to those of you that are now sitting in the audience, completely confused about what is the difference between these two?"
        ],
        [
            "I'm going to start out with a short introduction to learning with kernels.",
            "In particular, I'm going to briefly over the SVM framework because we're going to need that a lot more later, and then I'm going to present to you the framework of learning kernels, in which I'll go into this infinite loop almost, at least until I run out of time discussing the Upson Downs in learning kernels over the last 10 years.",
            "I'll discuss theoretical and algorithmic advances and I'll discuss frameworks.",
            "Is a convex nonconvex optimization, linear versus non linear kernel combinations?",
            "Human versus many kernels.",
            "L1 and L2 regularization, alot of things have been tried and I'll pack it up.",
            "All of these formulations with experimental results because after all why we should be doing this is to get better performance and not all of this has been so successful.",
            "When I run out of time, I'm going to conclude the talk and discuss some future direction."
        ],
        [
            "So anyway, let's start out with learning with kernels.",
            "As I said, we can indeed that later.",
            "Kernel methods became very popular in the 90s, and they still are.",
            "It kind of all started out with the optimal hyperplane, which is an algorithm for constructing two group linear classifier.",
            "Window points are linearly separable.",
            "Here we have the blue points and we have the red points and we can place a linear separator between the two points when they are separable.",
            "There's not only one.",
            "Linear separator that takes him apart, and the optimal hyperplane chooses the unique linear separator that maximizes the margin between the convex Hull of the two sets.",
            "Just a little bit of nomenclature here in the optimal hyperplane we talk about the Canonical hyperplane.",
            "That is, we normalize the weight vector such that the points that are closest to the decision surface they get the value one or minus one.",
            "Those are active.",
            "The ones that we call the support vectors with this notation.",
            "Then it's easy to find out what the margin is.",
            "We can just take two points, one here and one here.",
            "The difference in projected down under weight vector and we get that two times the margin the margin is here.",
            "Is nothing but two over the norm of the rate vector.",
            "So when we choose the one unique separating surface that maximizes the margin between the two classes is equivalent to chooses to choose the separator with the smallest norm now.",
            "This all works very well when the points are linearly separable, but as you know, in real life points are not and that was what we worked on."
        ],
        [
            "With a soft margin hyperplane when the points are not separable, what we did was we equipped every point with a slack variable and non negative slack variable.",
            "This site here and now we get that the support vectors are not only the.",
            "Vectors here right under decision surface, but also everyone that has one of these non negative excited variables associated with it.",
            "Now the soft margin hyperplane algorithm preserves the."
        ],
        [
            "Of the nice properties of the optimal hyperplane.",
            "The cost function that we are optimizing as this we are minimizing the weight factor, which is we are maximizing the margin between the convex Hull of the two sets.",
            "Subject to that, we don't want to have too many of these non negative slack variables, so it's a constraint optimization problem.",
            "Every point should be classified correctly within the accuracy of the slack and everybody has a slack variable.",
            "The see here is up to the user to choose.",
            "That's a tradeoff between the errors and the margin, and it's a convex optimization problem.",
            "It's a quadratic optimization problem, which means that it has a unique solution.",
            "This context and everything, so we can just either do a gradient descent or whatever.",
            "They are.",
            "Fortunately, many other things that come around to do it efficiently.",
            "So it's a lot of nice property."
        ],
        [
            "Is under way that we solve it is that we write the Lagrangian for the problem.",
            "We introduce non negative electrons, variables Alpha and beta for all the constraints and at the saddle point of the Lagrangian we know the.",
            "Coach can toggle conditions they have to hold, so we get the gradient with respect to their version with respect to the weight vector is zero, which leads us to the this.",
            "The weight vector can be represented in terms of the the support vectors that went with the non 0A variables we gave.",
            "Similarly for the other two constraints and we have.",
            "Finally the complementarity conditions here, that either a point has a positive Alpha.",
            "What is correctly classified?",
            "Substituting those equations back into.",
            "The."
        ],
        [
            "SVM equation leaves us with this dual optimization problem.",
            "That you probably all have seen a billion times, so we have resolving it with respect to the alphas and the solution is nothing more than an expansion on the support vectors.",
            "The only difference actually between this optimization problem and one without the slack variables are the things that I have in blue boxes here.",
            "So here we have that Alpha should be between zero and C. And if Alpha is C then it has a non negative slack variable.",
            "It's and so called error.",
            "And we get that the B now the offset is in terms of only you should find it only for the ones that have an Alpha less than C. Now, um.",
            "The nice thing about this problem is, as you can see, the X is they never appear alone, and that was actually what made it all possible to Colonel eyes.",
            "We have an expert appears here, but it appears together with another X and the solution also here appears in together with the other axes.",
            "So thanks to work that had already preceded the soft margin hyperplane, we had the."
        ],
        [
            "The Kernelized formulation, where we can, instead of the inner product between 2X, is insert a kernel.",
            "And that's what we have here.",
            "So here we have the kernel, and we have a kernel down here.",
            "All that is required for for this optimization problem still to be convex is that the kernel is positive, symmetric and some definite.",
            "So this was this was the optimization problem that we.",
            "Worked on in 95 and it's interesting to go back and read the paper for for this soft margin hyperplane because we actually didn't have much of a theoretical justification for it.",
            "It's kind of nice to say that we select the hyperplane that has the largest margin, but despite we knew kind of emerging pounds, if you read the papers interesting to see that we sort of justice cast as the reason why this is working.",
            "In terms of and or comes races principle that will minimize the weight vectors.",
            "So we have the fewest number of parameters.",
            "We had a little bit of a margin bound, but it was only for for separable classes, so the real nice theoretical treatment of the support vector algorithm came with this paper."
        ],
        [
            "In 99, where we actually get a nice explanation for why it's working here, we have that the generalization error is upper bounded.",
            "We have the error, so this is like a typical structural risk minimization.",
            "We have the error on the training set is access a fraction of training points with margin less than row.",
            "That's all the ones with the with a non negative PSI they come in here.",
            "And then we have this slack term that represents the capacity of the separating hyperplane.",
            "You see here, there is a radius that's the smallest radius of the ball.",
            "In closing all the points and we have the margin here Rose Squared.",
            "So now we actually have this theoretical justification for their algorithm.",
            "Work is simply structural risk minimization where we have this as how Slack term later on it has been shown how to get rid of this log term.",
            "So.",
            "Now everything you could say it's perfect, right?",
            "We we had an algorithm with great performance.",
            "It's a convex optimization problem.",
            "We have a theoretical justification for always working and everybody said it using it it.",
            "And it was just everything got kernelized.",
            "We had kernel which we crash."
        ],
        [
            "And we have kernel PCA.",
            "Anything that could be kernelized with, kernelized and probably in the in the late 90s.",
            "One of the most frequent keywords were nips would have been kernel methods in one way or the other.",
            "So this is my short introduction to two kernel methods that.",
            "One unfortunate thing about the kernel methods here that we now are going to discuss is what kernel should we be using?",
            "Because it's kind of just left up to the user to come with a good kernel.",
            "And if you take the standard package is out on the market.",
            "We have lip SVM uses one kernel as a default.",
            "That is the Gaussian kernel, while.",
            "SPM Night uses at polynomial kernel, so you could kind of think this is make any difference with kernel you using and it does indeed for base performance.",
            "It doesn't really matter I think, but if you really want a competitive algorithm, you have to fetch around with Colonel.",
            "So the problem is can we learn the Colonel from the data instead?",
            "And that's what we're going to be discussing next now.",
            "The first"
        ],
        [
            "This is difficult.",
            "OK so the 1st.",
            "The first"
        ],
        [
            "But started thinking about it.",
            "Did think about it in terms of structural risk minimization.",
            "Here we have the dual formulation of the SVM problem and it's natural that we want to select the kernel that minimizes an upper bound on the generalization error.",
            "But at the early stages there is sort of a little bit.",
            "You know, what should we act and minimize?",
            "Which ground should we be minimizing and."
        ],
        [
            "At first you would say, well, we just had this beautiful bound.",
            "It included an R-squared.",
            "The radius of the smallest enclosing up all of the points and a bro.",
            "So since the SVM cost function itself doesn't include there are squared, it's kind of natural that you want to minimize that bound in order to get good generalization error.",
            "And then Fortunately the radius can be expressed in terms of the kernel.",
            "And so in this early work, people either minimize the margin bound in terms of R-squared or rose squared, or a span bound which is expressed in terms of the support vectors that come out of solving the SVM problem.",
            "Now did it work?"
        ],
        [
            "It did and in certain respects it.",
            "It thank you for the experiment with salt.",
            "I must say here that I am going to borrow heavily from all of your work.",
            "All of you here in the audience throughout this talk.",
            "And it's not to place judgment on any of the experimental results.",
            "I'm just trying to simply give a little bit of a story of what has been done, and it's not too too single.",
            "Anybody out, if I haven't included your work also?",
            "There's so much that has been done, so just bear with me here.",
            "Some of the first experimental results is in some of the UC Irvine datasets that we all know and what they're trying to do here in this alternate approach of of solving the SVM problem and doing a seabass descent on abound on the generalization here, they're trying to find the best setting for Gaussian kernel and the this parameter.",
            "See that you know in the cost function of SVM's.",
            "And what they're comparing to is the very best they could obtain.",
            "If they did a full cross validation, and indeed they can get the same results in same performance.",
            "So in that respect it works beautifully, right?",
            "Because they can get the same and it's 100 times faster than doing this full cross validation over the combined space of the Sigma of the Gaussian.",
            "And this C there were a number of other early successes of learning kernels.",
            "You can train some.",
            "Think of it as a feature."
        ],
        [
            "Just selecting mechanism.",
            "If we take every variable in where's my pointer?",
            "Here we take the variables and we multiply it by a non negative weight.",
            "So we sort of try to.",
            "Do we have a feature importance weighting off of the features and we can maybe restrict the weights and in some fashion where P could be one or two, depending on whether you want one or L2 regularization you can again do this alternate between in your kernel you solve the SVM and you do a gradient step in merging pound or these people they activated directly on the SVM cost function itself.",
            "Now does it work?",
            "Yes, it worked beautifully.",
            "Here we have a."
        ],
        [
            "Comparison if you can see it, it might be a little bit difficult, but we have here it's these artificial datasets, so the ones that left is a linear data set and they use the linear kernel, the one to the right is a nonlinear.",
            "They used a polynomial kernel, so here they are doing feature selection.",
            "They know because they know the toy data here how many features to select and it and they can select the appropriate number and they can compare it to other classic methods for.",
            "For feature selection filter methods where they can take the same number of variables and it was beautiful.",
            "Here we have the number of training examples and performance of the of the SVM after they have selected the features and the learned kernels here easily drops down while all the other methods.",
            "This is an SVM itself, it doesn't do any feature selection and the other methods are not competitive.",
            "One bit over here we have the same performance the.",
            "The learn kernels quickly drop down while the other ones they say so.",
            "Learning kernels help great promise and the papers started coming.",
            "The next big step up, it comes with the papers of Lancret at Al, where there."
        ],
        [
            "Problem is given as serious mathematical formulation with convergent guarantees, because so far we have been doing some gradient steps, alternating between solving the SVM and and doing a gradient and some bound.",
            "We needed a serious framework for it and it came in about 2000 two 2003.",
            "The problem is here completely formulated in terms of the SVM cost function.",
            "Because you could say after all it doesn't contain that R-squared.",
            "But hey, it is supposed to be an upper bound on the generalization error.",
            "So what language at all is formulating it here is is we want to find the kernel that minimizes this upper bound.",
            "On the generalization here and we're going to make it subject to that the kernel should be PDS, and we're going to make some constraint under colonels one way or the other.",
            "We're going to parameterize the family of kernels, and the way it was done here was through the trace of the kernel, where that restricts the family of kernels in order to make this a computational feasible path problem would likely get loose."
        ],
        [
            "He linearized other kernel, so he said, let's take a set of kernel matrices and let's just assume that the kernel is a linear combination of those matrices.",
            "Actually, Wade was formulated in this paper within a transductive setting, but there's really no need for it because he had the expression of the kernel matrices.",
            "But doing that, especially if he said so, here we have here, is linearized the kernel.",
            "We just have a linear expansion on it, especially if he restricts the expansion to be non negative coefficients.",
            "And so he has.",
            "And he has this L1 regularization constraints that the sum of the expansion coefficients, trams, the trace of the individual kernels should be less than Lambda.",
            "He can actually end up getting a Q seek problem, so that's very nice as a convex optimization problems with efficient efficient algorithm."
        ],
        [
            "To solve it, the reason he does the positive combination and that the general also positive and negative combination is that in general then it would be an SDP problem.",
            "We set some much higher computational complexity, but by restricting it to these non negative linear combinations of kernels we get acoustic problems later, also reformulated as some infinite LP problem which can be solved very very efficiently.",
            "They have solved it on on.",
            "Up to 10,000,000 examples.",
            "Actually, it can be all the way restricted down to a QP problem.",
            "In case that you have it's rank one matrices so beautiful, we have a convex formulation.",
            "Now does it work?",
            "Well, here."
        ],
        [
            "We have an example of it from from this paper.",
            "It's a lot of numbers, so let me try to work it through it.",
            "He has three kernels he is trying to form a combination of this is a polynomial of degree two.",
            "This is a Gaussian.",
            "Here we have the Sigma and this is a linear kernel and here we have the result if we are combining possibly none positive or a negative combination of the kernels.",
            "So this is the SDP problem.",
            "Here we have the QCQP problem, it's a positive.",
            "Span of the kernels and here we have the result.",
            "If we just did one kernel but did cross validation and see to see which one we could get the best we saw it with with an RBF kernel.",
            "What you can see here is a number of algorithms they were running.",
            "Here we have the hard margin classifier, so it's not linearly.",
            "Since this is a hard data from user ID so that data set before so it's not linearly separable.",
            "But once we get to a soft margin classifier, either with the normal one and this lag variables or the number 2.",
            "Here we have additional tuning of the C parameter, then it actually gets to be linearly separable and you can see here this is the very best performance he can get when he does all possible combinations.",
            "It doesn't really beat the single best kernel ever.",
            "And there isn't much to be gained from going from the Q seek problem to the SCP problem.",
            "The performance difference between these two is miniscule here.",
            "I guess they ran out of time because it's a computationally harder problem.",
            "So despite we now have a very nice framework for it.",
            "At least this didn't seem to be buying us a whole lot, and a lot of paper is a kind of came in the same direction.",
            "There isn't much to be gained from it.",
            "There's one thing that's missing that later became a baseline for for learning kernels.",
            "That is the even combination of all the kernels that is sadly missing from this picture, but I'm sure that it wouldn't be doing much different from the 84% performance, so.",
            "Fortunately, we all honest people.",
            "When we write our papers and I have been writing some of these papers myself.",
            "When you set out to solve a problem to get better performance and it actually doesn't do a whole lot better, you gotta find another reason for having written a paper, right so?",
            "So people have been very, very good at finding good reasons for having done learning."
        ],
        [
            "Journals write speed.",
            "It's much faster 100 times faster, so that's like that's a good reason for doing that.",
            "Instead of doing cross validation and good ranking properties also turned out to be an advantage of these learning kernels, here's just one."
        ],
        [
            "Example from a computational biology paper where we have we have a number of currents.",
            "We have 5 kernels that that are known to be good for the problem at hand.",
            "It's a very biased problem.",
            "We have like 98% positive in 2% negative in this data set and accuracy wise learning kernels didn't help the whole lot, but Fortunately there were other things here.",
            "They you see it says I received, but I actually think it should have been a.",
            "You see when they are learning the kernels we can get from 96% up to 99% and there's also this the true positive percentage is at 1% false positive.",
            "Seems to be going steadily up from 42% to 88%.",
            "This actually one very nice thing about this this this table, first of all, here we have the five meaningful kernels, but then they added nicely some random noise and what they're showing here is that if you put in here, we have it.",
            "We learning the kernels in the framework from before, but we throw in a little bit of noise we don't reduce.",
            "Performance by a whole lot.",
            "The learning kernel framework is surprisingly robust to adding in noise.",
            "What you also see here is that actually it seems to be doing better than the even combination.",
            "Here we have that we are not learning the kernels.",
            "All the weights for the Colonels is the same we are adding in noise with also the same ratio and these ranking measures at least drop down.",
            "So maybe there are still good things about running kernels, just not in terms of the accuracy."
        ],
        [
            "We have another example, also from computational biology in terms of feature selection, being able to understand the models, there seems to be a lot to be said, so here we have an example where they're trying to to find out the importance of.",
            "It's some splice sites and I wish there was somebody in the audience that could explain better to be what it is, but.",
            "Here they have the excellent and they're trying to find out what importance having neighboring sites in the DNA with respect to this excellent and they can actually explain that these sites back here corresponds to the the previous excellent and these are inhibitory or whatever so they can get these from the importance weighting that we so earlier they can get these nice explanations of of the models and they can look at it and see why it's working.",
            "Now."
        ],
        [
            "Accuracy was obviously not the what was coming out of it.",
            "Maybe the whole problem was that we were learning with two few kernels here indeed.",
            "So we had here like 5 after 50 kernels.",
            "Maybe indeed the early theoretical results were not too encouraging for learning with a lot of kernels.",
            "The only non vacuous bounds that we had said that if you learn with currency if you have P kernels here you can it be paying a price that is multiplicative in the number of kernels.",
            "So if you have 10 times as many kernels then you need 10 times as much training data and with these SDP problems.",
            "That's probably not the way you want to go.",
            "Fortunately, also that doesn't seem to be the case."
        ],
        [
            "In 2006, Shampoo and then David came with this other pound.",
            "It shows that I should learning kernel here.",
            "It's in the linear classification framework with an L1.",
            "Regularization is not going to cost us that much after all they came up with an additive bound instead.",
            "So here you have now that the number of kernels is to only additive and here we have this data stack term you saw before.",
            "Also arose squared over N out.",
            "Here we have.",
            "Oh, children attention that hires conveniently a number of factors and back here we have the same term as you saw before in the margin.",
            "Beyond this is the fraction of training points with margin less than than row.",
            "So maybe the problem all the time had been that we've been learning with two few kernels.",
            "We should just learn with a lot more kernels.",
            "Fortunately, people also tried that here."
        ],
        [
            "Do you have an example of people learning with infinitely many kernels?",
            "This so called hyper kernels framework where it's not just a kernel is the kernel between kernels.",
            "That might be a little hard to grasp, but so here we have data kernel between two points is parameterized by.",
            "It can be expected, can be represented by 4 points that need these four points, and we have M ^2 parameters here to estimate, and they can still formulate this as an SDP problem, so hard computational complexity.",
            "But you know, we can do it, and we know what we're doing doesn't work then.",
            "Now we do.",
            "Infinitely many kernels."
        ],
        [
            "Well, I don't know for sure here we have the kernel they're using in this experiment is a Gaussian kernel.",
            "So here you can get an idea of what these hyper kermes they are.",
            "We have 4.",
            "PT training points here, and you see how they they go in like in a Gaussian.",
            "The sigmas here are determined before hand the Lambda.",
            "Here's determined beforehand.",
            "We're not optimizing with respect to these parameters, we're only optimizing the betas that go in and control, which X is should be sitting here.",
            "Or which prime should be sitting here so they run this and they run it on here we have our standard UC Irvine datasets.",
            "We have the heart.",
            "For instance, we send out a number of times and they run this SDP problem.",
            "On three different cost functions, the standard SVM than USB embrace the number at the fraction of support vectors, and here we have a squared error SVM and when they compare what they can get here to the very best they could have gotten.",
            "If they only used one Gaussian, so not this infinite number of Gaussians but only one Gaussian, and that's what we get over here.",
            "The cross validation so you can see, for instance, if we take the hard data set.",
            "But they get significantly better performance than if they had only used one kernel.",
            "However, they have honest enough to report the very best result they can find in the literature, and they're not beating that, so you can ask yourself, does it actually buy us anything?",
            "Additionally, I know it's so hard to compare these results.",
            "Often when you sit and stay at the tables you wonder you wonder how?",
            "How do I compare because here we have like a 23% error rate for the best Gaussian kernel for the hard data set.",
            "Despite that in a previous slide I showed you 16% error rate for a Gaussian kernel, so but it can all be in the preprocessing of in.",
            "Here there is absolutely no preprocessing, never very honest about that so.",
            "Learning with infinitely many kernels doesn't seem to bias anything still.",
            "In the."
        ],
        [
            "Framework, so maybe it is because we're doing classification.",
            "We should have done regression instead.",
            "So I said this is something I worked on myself and this is a paper that you'll see later in the conference, not in this conference, but in UA I.",
            "So here we set it out looking at learning kernel spread for regression.",
            "Maybe now we're going to see that it all works right?",
            "So we also managed to come up with a bound that is nicely additive and it's actually with the square root of P. This is disability based bound, so it's a different duration than the other bound you saw before.",
            "It kind of makes it hard to do this.",
            "This stability analysis is that you have to change the point all the time, but it's not only that you change the hypothesis by changing one point, you have to change the kernel because now it's a kernel that's optimized on a different set of points.",
            "So we ended up having to have a little technical conditions that is orthogonal kernels, but does not a whole lot different from a lot of the other things that we see, which requires it to be concatenation kernels so.",
            "Here we we again end up with that.",
            "Maybe it's not so bad to use a lot of kernels in regression we chose the and L2 regularization.",
            "Which would mean that we have."
        ],
        [
            "This problem formulation is that this is the kernel Ridge regression that you also saw before and you have now that we have an L2 regularization on the expansion coefficients.",
            "In order to get an efficient solution for this problem we reformulated."
        ],
        [
            "We can we have here the formulation, but an we introduce these variables V. Fortunately, this can be rewritten in terms of using a fundaments generalized minimax theorem, so we can, instead of minimizing album you after we have maximized or Alpha, we can permute the order and then you can see you can move in them.",
            "In here you can actually solve explicitly for the muse.",
            "You get a closed form solution for them.",
            "You so now you got rid of those completely.",
            "You're left with a problem that's only a maximization or Alpha.",
            "It's actually a little bit like the standard kernel Ridge regression with musiro kernel, except from you have that.",
            "So the solution comes in in about the same form, except from you now is given us this Musial plus this Lambda V over normal V where Viva.",
            "These these parameters from up here so it can be solved.",
            "Fairly efficiently, it's a convex problem, so we could either have done just a gradient descent on it, but we don't do that.",
            "This whole formulation encourages sort of an iterative solution to the problem where you start out with."
        ],
        [
            "One kernel you saw, you saw it out, maybe with the uniform kernel you solve with respect to Alpha, and now you repeat until convergence in terms of some epsilon parameter that you solve for Alpha, you update the VIS you update them, use and then you re solve for Alpha and blended in and so and so it reaches convergence so.",
            "Regression with a large number of kernels does it work?",
            "Well."
        ],
        [
            "Maybe a little bit.",
            "Here we have.",
            "The baseline is are even weighted kernel, so this got just kernel Ridge regression.",
            "And we are here using rank one kernel, so it's a.",
            "It's the very simplest kernel that you can imagine not doing anything fancy on top of it.",
            "Here we have the L2 regularised kernel.",
            "So as we increase the number of kernels from 1000 to 6000 we actually have some performance improvement as opposed to if we done the L1 regularised for for regression.",
            "It actually only seems to be doing worse than the baseline as we continue on here we have it with data deviation.",
            "So this seems to be something at least to be gained from doing L2 regularization.",
            "And regression instead if you want to do learning kernels that also is supported by another paper.",
            "In the audience."
        ],
        [
            "Other hierarchical kernel learning where we have also learning with a very, very large number of of kernels in a very elegant and intricate set up where we only selecting the kernels in a hierarchical fashion.",
            "Also, this can be formalized in the convex optimization problems and he is using L1 regularization.",
            "Their whistles here seem to confirm that in regression with a lot of kernels there is.",
            "Something to be current gained."
        ],
        [
            "From learning kernels, I shorten down your tables.",
            "I'm sorry a little bit about the so here we are comparing.",
            "This is an L1, though with regression.",
            "So here we have their hierarchical kernel learning for a number of datasets and the number of kernels here is stunning.",
            "Really stunning for Air 4th degree polynomial since we're selecting polynomials up to a degree of four and we have like.",
            "10 parameters we get to to the 10 to the 7th number of kernels to speed selecting from.",
            "It's amazing they can do this efficiently, but they can really do it efficiently and you see here that some performance improvement can be gained over here.",
            "We just have the selecting it not in this hierarchical fashion.",
            "This is the multiple kernel learning distances tainted formulation and here we have the kernel which regression as it is.",
            "So there is.",
            "I mean there is in general more black in this column then in this column over here.",
            "So there is something to be gained a little bit from.",
            "Doing regression with learning kernels and multiple kernels.",
            "Large number of kernels.",
            "So here we see that L1 regularization does somewhat better than the kernel Ridge regression.",
            "We can only hope for if it could be formulated with L2 regularization that would be doing even better.",
            "So what we have learned so far is that it's really, really hard to get learning kernels to work surprising."
        ],
        [
            "We heard it's such a nice and intuitive notion that we should be able to learn to Colonel from the data, but we just can't be consistently get it to work.",
            "We sometimes get somewhat better performance than just having the evenly weighted kernel, but in general is hard to say that we get it.",
            "L2 regularization seems to be doing slightly better than L1 regularization.",
            "We had a workshop at NIPS and that also seemed to be the the story we heard all the time now.",
            "One regularization doesn't really do a whole lot, though it seems to do in the hierarchical kernel learning.",
            "There seems to be some something to be gained from it.",
            "Not that we need a lot of kernels in order for it to work.",
            "We have seen very little performance improvement with just a few kernels, so we handled it all.",
            "These other redeeming properties of learning curve is much faster.",
            "The optimization community has been doing so many wonderful things for learning kernels that today it's just blazingly fast despite their hard computational properties of the problem.",
            "It's great for.",
            "For feature selection that still holds up.",
            "But aside from that, there isn't much that can be said in to defend that.",
            "We continue writing, learning Colonel previous, I must admit.",
            "I think we have to try something completely different and some people have started during the non linear combinations have the current.",
            "So far you've seen as everything is linear and these these news."
        ],
        [
            "So the linear explain.",
            "So what can we do if we if we go in order of non linear combinations and there has been papers in that direction as well 2005 there was a very elegant paper, I would say where they turned it into a DC probe."
        ],
        [
            "Coming problem Ann and I can just maybe quickly say a few words about it here.",
            "They have that.",
            "It's again.",
            "It's continuously parameterized set of kernels, and here we are trying to learn the Sigma in these these Gaussians.",
            "So instead of having this as a fixed one, we're trying to learn these and.",
            "They they can manage to do it in.",
            "It's not convex.",
            "That's a problem when we go to these nonlinear parameterized kernel expansions.",
            "It's not a convex optimization problem anymore, but they still manage to solve this because they can formulate it as a DC.",
            "So it's a difference between convex, welcoming problems, so they can.",
            "They can still allow themselves to do these alternate steps of solving the optimization problem, the kernel problem, and then do a gradient descent step.",
            "In order to estimate new Gaussians that they're adding into the problem, so they still know what they're doing and."
        ],
        [
            "Well, does that work?",
            "So this is really hard to see.",
            "If I were you, should you should look at the at the table here.",
            "Down at the bottom and they can get some better performance and they should compare to here where where their brute force trying to find the best parameters here are trying to optimize with two of the Gaussian sigmas.",
            "Yes, they can get some performance improvement, but it here not compared to any baseline.",
            "So so the main thing is here that they they can at least formulated in a way where they can tackle their problem.",
            "Otherwise, we kind of."
        ],
        [
            "We have another paper.",
            "It's actually at this.",
            "I CML the generalized MCL learning algorithm.",
            "It's it's sort of the same formulation as you had in the previous slide, where they're trying to learn the sigmas, either for Gaussian or in a polynomial.",
            "You're trying to learn the new expansion coefficients here, where P can be any number.",
            "So.",
            "It's interesting to see because with this paper we actually go full circle, right?",
            "We're getting back to the formulations that we had in the very very early kernel learning papers.",
            "We're doing this solving the SVM problem.",
            "We're doing a gradient descent step.",
            "And we have no guarantees for where we're going and this this paper doesn't discuss anything about the convexity of the problem.",
            "Can we get stuck in a local minimum?",
            "It's worth trying and I guess that's where you end up where we are.",
            "Nothing else work, just try everything and see if it works.",
            "But still, even though."
        ],
        [
            "Then when they try everything and I think it's honorable to try.",
            "At the end of the day, here, they have their performance.",
            "If I still have this little pointer here, they have their performance and it doesn't.",
            "It works great for feature selection, so if they're only allowed to use 15 features then they can get a better performance and with any of the other algorithms.",
            "But if they let the other algorithms use all the features, they don't beat the performance, so again.",
            "We don't have a clear advantage of learning kernels.",
            "So on this happy note.",
            "I want to end the talk and say."
        ],
        [
            "We we need to get this to work.",
            "It was a great idea, but so far I haven't seen any proves out that it actually buys us anything.",
            "It has heard a lot of interest in the community, lots of food optimization algorithms have come out of it.",
            "Lots of interesting feature selection where it as an accuracy improvement.",
            "We haven't seen much of it so far.",
            "Maybe we have to try something completely different and I don't know where it is because then I wouldn't be standing here.",
            "Would be riding on it, but I think we really have to step back and say.",
            "What did we do that that was just too limiting in the way we did learning kernels because so far everything seems to just have been getting us very little over the baseline performance.",
            "I remember William Noble saying I should add the Nips workshop we had in December, but he had the kernel for learning kernels floating around, and if anybody was interested in using it then feel free to, but he was not going to spend more time on it, so there we go.",
            "Of course you know is there any way that theory can guide us to how we could be doing this differently, because anything that's experimental seems to have not been too successful.",
            "So far, so good luck guys."
        ],
        [
            "So we have some time for questions.",
            "She mentioned a lot of work from people in the room, so I imagine there's lots of things in common done I I'm sure.",
            "I'm neutral.",
            "You were not ask you.",
            "OK.",
            "Try to look at more complicated problems.",
            "A lot of the datasets we shoot.",
            "OK, I think Leon he is advocating that we did UCI datasets.",
            "Ceiling effect and all this stuff.",
            "So you're not going to see a benefit because it's everyone's doing as well as can be done.",
            "Yes and no.",
            "I mean, these these experiments have also been run on other datasets.",
            "Fancy far has been running and some of the much larger ones.",
            "These are actually synthetic datasets.",
            "They also UC Irvine.",
            "But you also think that they're not complex enough.",
            "If you are you looking for something large scale that is hard and.",
            "If only you work somewhere that had access to lots of, I know, I know.",
            "I know, I know this person is.",
            "No, no Google does not give Arteta.",
            "I am not not, maybe and you see would be coming out data.",
            "It's really hard to give our data because we saw there was some search logs that another company released that had a little bit of an unfortunate snafu so they weren't quite as anonymized as they should have been.",
            "Yes, we should probably do it UC Irvine datasets, but I think on the other hand they serve as a good baseline.",
            "And.",
            "But then I think this hasn't worked on any data set, no matter.",
            "I mean the brightest data set.",
            "Tested.",
            "I enter.",
            "I think it would be nice if we got some new and challenging datasets, but it's really, really hard to come up with them, and it's really, really hard to get the organizations to give them away.",
            "Trust me, I'm coming from a company and I have worked really hard to try to get some of the datasets out there and it's just up against roadblocks, time and again.",
            "If you can do it from your company, I only congratulate you on it.",
            "Netflix stated yes.",
            "No, no, no.",
            "It was the other one.",
            "It was it wasn't there.",
            "AOL data set wait where we took about 24 hours and they had identified people in Pennsylvania and stuff like that because people they tend to search for themselves and their problems with their pets or whatever it was so.",
            "Earth two related questions one.",
            "What is the game of having the kernel function?",
            "Yeah.",
            "As opposed to simply maybe training different as VM's for each of the kernels and then combining ESPN is out of the box with some narrating combination or something like that, and the 2nd is.",
            "Have there been any attempts of?",
            "Completely learning the kernel function from scratch instead of combining existing kernel function.",
            "So to the first one.",
            "Where does the advances you go back to boosting scenario right?",
            "If you want to train the SVM's and then combining them.",
            "And as we saw in some of the slides, this framework doesn't significantly significantly outperform existing methods, so no, so far there isn't anything to be gained and doing the kernel altogether has there been any attempt at learning there?",
            "The kernel matrix directly?",
            "Well, if I say that hasn't been, then it would be wrong because I'm sure that I mean the original problem.",
            "Formulation is language processing.",
            "It was actually he only had the kernel matrix.",
            "I think we have to go in that direction because we're restricting ourselves too much if we if we define first, we have the functions and these are the functions we're going to be using.",
            "We're not gaining too much or away if we actually try to learn the kernel matrix directly, it's going to be a much more complex optimization problem with much more free parameters.",
            "Additionally, you're going to be in the transductive setting, I'm afraid, because how are you going to?",
            "Classify and you point.",
            "Then you don't have then the kernel for how your test point is going to be interacting with your training point.",
            "So you're going to have to train a new problem for each time you have.",
            "Testpoint but if you can something like I don't know.",
            "Two points and keep your distance.",
            "If you're training well that that that you can do, then you have the Colonel Major, but then you get back to the boosting scenario pretty much right.",
            "You have your classifier, so your training and all network.",
            "I mean now you can have several of these and you can combining them together.",
            "It's not quite the same then as as learning the kernel right then you're learning in all network.",
            "How can I help and I'm not surprised.",
            "As well.",
            "Work better.",
            "Whatever.",
            "Together I think what he was trying to say is that you have those techniques where you train is still sign in alright, so you trying to normalize their identical and we trained them to produce identical outputs.",
            "If the two inputs are similar semantically similar different ethnicity.",
            "And so you can use it to train people to run, and then you can use this as a means of.",
            "Features much better ways to do this.",
            "If you want then she put come to the workshop.",
            "Feel free to do it.",
            "The whole idea that with to do something different from neural networks, right?",
            "I mean, I think it's nice that we have we have complementing methods right and and but you're right.",
            "I mean by now, if we're getting to the point where we're just trying everything and we end up with nonconvex optimization problems and we are willing to do that, we could as well do neural networks.",
            "What there is a nice thing about the kernel methods in the in the framework of language and everybody else?",
            "With it we had a convex optimization problem, so we kind of knew what rate during.",
            "Unfortunate thing is that so far it hasn't proven to be significantly better.",
            "I'm not looking for network Crusader, it's just that you can use whatever architecture in the 19 by created the sentence.",
            "Or not.",
            "And so, in particular, into the freedom to use architecture we can put a lot of prior knowledge about the task.",
            "So which is so?",
            "In the sense of visibility.",
            "Would you consider?",
            "So then it opens up to also a lot of local minimums, and you don't necessarily know where you're ending up.",
            "I mean, you can.",
            "You can do it.",
            "So here we have it.",
            "That isn't.",
            "If you do what you say and you see I won't get any better.",
            "Three of these guys are all in the same place at AT&T at one point, so it's, Oh yeah, Oh yeah, and I mean, it's all.",
            "It's all in friendly terms so it will be will be good friends also afterwards.",
            "Don't worry about that.",
            "To me that the wonderful thing about kernel methods is that you know what you're doing, and but I.",
            "Help us after all these papers written after like eight years after learning has been introducing you.",
            "Finally have console said that someone doesn't work.",
            "What do you think the community can learn from this?",
            "Um, well, I wish I could say that that it could learn to be more selective about it's it's it's choice of problems.",
            "I I you know what I actually think that I I congratulate this community for being so, stopping and believing in its idea is right, because it is an intuitive, nice thing to think that we can learn to kernels and we shouldn't just toss idea because it doesn't work at first attempt, right?",
            "So stop earnest sometimes pace off, right?",
            "I actually, I want to encourage people not just through through the ideas out when it doesn't seem to give any performance improvement.",
            "At the first attempt, and I think it's great also that how, how, how, how, people have managed to come up with other redeeming properties of this.",
            "So it has some good qualities, right?",
            "I hate to see good ideas being thrown out and on the other hand, I also hate to see tons of papers being submitted to conferences, right that that only advances the field a tiny bit.",
            "Then you know we all sit and review them and all that.",
            "I think still I want to encourage people to keep trying even if it fails.",
            "Linguist.",
            "Or risk being station and I was wondering with this girl anything who we really?",
            "I would really the setting up of SRM where we have like you know, embedded.",
            "Class of complexity and then try to find an optimum somewhere in terms of generalization error, are we really in this in this setting here with a different approach or were.",
            "I think we are, I mean because we have we are minimizing abound on the generalization error, right?",
            "And we have parameterized the kernels one way or the other.",
            "So the question is, if you with this this.",
            "My point is that you should somehow maybe search for different along different capacity.",
            "When you do you do right.",
            "You always optimize.",
            "You have a Lambda right?",
            "The capital lamp that is found in the trace of your kernels, right?",
            "And as you you try different lamp is you're not stuck in one right?",
            "So that way you March through the various with the capacity classes.",
            "I actually believe that we are.",
            "How would you ask one to do it?",
            "And then you know, like like you grew from one plus the other one, which is, which is just Boulder Red, but into the previous one where you do and the trace right?",
            "You have one trace.",
            "Then you have a bigger trace on.",
            "Your biggest reason, your big trace of the matrices.",
            "The bigger the tracing work, the more freedom you have in how finely you want to follow this the features, so I actually.",
            "Believe it.",
            "Any question in Gaussian process regression that comment on my state of time.",
            "It is and then they do something.",
            "So it's not just because it's.",
            "What's the?",
            "It's a different framework.",
            "I shouldn't ask her why they did manage to get the performance improvement since I haven't tried to carry out any of the results.",
            "May or the experiments myself, so I'm reluctant to to place judgment of why they might be working and may not be working.",
            "But is it?",
            "Is it the same things that would correspond to anything much more than doing cross validation?",
            "Yeah.",
            "Um?",
            "I don't know for sure why they get it to work.",
            "I shouldn't if there's anybody who wants to comment on it.",
            "I'm happy to let other people comment on why they think that may work.",
            "Question.",
            "Would you agree if I say that basically pattern recognition and regression Arvin Corner there is something between maybe?",
            "OK, I think I think I started a can of worms with this icy American just fold problem.",
            "Commission regulation so so, so classification and regression.",
            "There's not much more to be gained from it.",
            "I think you're worried that one of the big advantages are there areas where we really need to get better at is more in the semi supervised learning setting, right?",
            "Because we at least coming from Google.",
            "We have a lot of data right and we have very very little labeled data.",
            "Is labeling is expensive and I think that the pure classification that pure regression problems.",
            "They're not.",
            "Maybe that interesting to us anymore, but better ways of making use of all the unlabeled data would be wonderful.",
            "We also these things that they are all very beautiful, right?",
            "But now if I'm beginning to speak like a Google Representative, we're talking about taking advantage of very fine signal in the data, right?",
            "And that kind of requires also that we have fairly clean data in order to see any performance improvement from the 18 to the 16%, right?",
            "Or something like that?",
            "And that is in general not clean.",
            "Data is not what we see in real life.",
            "It's all one big mess and.",
            "I think we need much more in terms of robust classifiers, if anything, if we want to stay in classification and regression, we should do something that is robust towards having.",
            "Almost 50% noise because that's the scenario we typically live in.",
            "So if you can come, I would encourage you still maybe to do something in classification and regression.",
            "If you can do something at that noise level that is efficient and works for large scale, but otherwise try to grow into anything that can use all the unlabeled data in order to gain performance improvements.",
            "Yup.",
            "I would.",
            "To counter that, there is nothing much.",
            "And I think that we have a my view that they presented yesterday is that.",
            "We are using context minimization.",
            "Methodology is because that's what we believe can be done.",
            "But yesterday I showed that you know there are methods to do nonconvex optimization, and classification is a non convex problem inherent in.",
            "So I think that there is a big area there that even before going into active learning, semi supervised and so on that we don't really know.",
            "You don't really know.",
            "So there's a big gap between the bounds were always you say I give up on those examples, so there's a term that how many examples have a margin below and the actual algorithm that has a soft margin.",
            "There's just two different criteria we know how to minimize self margin.",
            "We don't really know how to minimize the bound, so there's I think there's actually a very big open problem.",
            "Search for long time.",
            "Well, I think I think that I think that you cannot.",
            "I think that the problem of almost separable datasets has been solved.",
            "The problem of datasets in which the base error is maybe 50% and you're trying to get from 60% to 55%.",
            "That is far from being so OK. Sure, go ahead.",
            "Maybe one of the other culprits is the kernels that we're using to begin with it.",
            "Look at the base space of all possible kernels and then we consider the kernels that we are using.",
            "This sort of pointing very limited directions in that subspace, and on top of that, so you take your RBS and then you said you set Sigma.",
            "All you're doing is perturbing a little bit the kernel in that area of the big space of all possible kernels.",
            "Maybe if we had rules we had to set several hyperparameters, then we be covering a bigger chunk of the.",
            "Overall space, I guess.",
            "Well they are.",
            "They were just combining whole bunch of different varieties of apples instead of providing apples with oranges with potatoes, there is something to be said for that, but I actually think that some of the algorithms they do care I mean, especially when you get to these later things, raise a non linear combinations where they're trying to find a new Sigma all the time, right?",
            "Then I think we're getting to the infinite kernel learning, right?",
            "And it's not just doing apples and apples, but there is a tendency to apples and apples.",
            "But still there.",
            "I don't see the performance improvement.",
            "Maybe there is an even better where you are doing, but then you also do end up in the non convex optimization problems where you don't really know what you.",
            "You may know what you're doing.",
            "I shouldn't say you don't know what you're doing because that's a seems to be, but we believe in convex optimization problems that please.",
            "Yeah.",
            "Would be something.",
            "I I think it's weather weather learning kernels would be useful for structure predictions.",
            "Trade.",
            "I mean with this material presented here, there are little hopes that it will be, but.",
            "Maybe you can prove the community wrong.",
            "Learning kernels can be good for something.",
            "Issues with.",
            "Features we're using our real numbers.",
            "Giving you any more information.",
            "So in any real application, you don't actually have real numbers are like.",
            "Proxy to the real features so that you define kernels on real objects.",
            "Then you have some people actually increasing the amount of information by defining moral codes, and then starts making sense to look at, you know.",
            "Increase the number of cars he actually increased the types of features.",
            "Um, at some point of time you have to turn it into a real number.",
            "Otherwise I don't know how to put it into the computer so.",
            "Objects.",
            "Objects which are not represented as vectors of real numbers.",
            "Right, so ends up being a, I mean a Colonel in away is nothing further features selector, right?",
            "You cast it into maybe infinite dimensional space where you still have a real number, so I'm not quite sure I understand the distinction you're making here.",
            "Example.",
            "They use that.",
            "She's basically not.",
            "Between 2 two sequences like the sequences are represented in some tree like structure.",
            "It's some really large numbers.",
            "So, but now the kernel on sequences we actually have, like a new feature space of dimension.",
            "New structures in your data.",
            "So what I'm saying?",
            "On size dimensions are 10 is not interesting.",
            "This is why we're not getting anything.",
            "But if you start looking at real real objects then there's some both of gaming.",
            "Probably just.",
            "Comparing what you use.",
            "But are you then arguing for having just a higher dimensional feature space or so?",
            "UC Irvine would be OK if we just had some higher dimensions.",
            "In some sense yes.",
            "But I mean you couldn't really have.",
            "You couldn't represent all the dimensionality of two sequences.",
            "Wait?",
            "Ray, right?",
            "So where do we end up with the question being if it would work if we just had not real numbers?",
            "So then the signal of Gaussian is somehow.",
            "We have to learn the structure of the kernel, so I guess that goes back to that.",
            "If we are doing it too simplistically, we should.",
            "We should really try to learn the structure of the Gaussian and or the structure of the kernel instead of.",
            "Of just saying we have these predefined families of kernels that we're combining together.",
            "Computationally, it gets you a little harder problem.",
            "I encourage you though to try it out.",
            "We should have time for maybe two more questions.",
            "This one over here.",
            "Meeting I think we should give this man a hand.",
            "Better datasets, so we're all.",
            "Where?",
            "And then that's fine.",
            "Super impressive.",
            "Without this is like Colonel best.",
            "Our actions are given a teacher.",
            "Image of his right to have many different feature classes at 16 years property currently.",
            "While Peter.",
            "You already.",
            "So if you combine.",
            "Very high school.",
            "So there are papers also with the image recognition, and I don't see.",
            "Also there a consistent performance improvement.",
            "I must admit.",
            "And if you take some of these computational biology papers, they have very.",
            "Is that from the bottom at the moment?",
            "They envision problems there.",
            "Fire.",
            "Fire.",
            "Yeah, they don't perform.",
            "They don't improve performance, not accuracy.",
            "Not one bit.",
            "As far as I know it, because I mean they they they don't and they also started out from a problem that was 98% positive and 2% negative, so they're already at 9899% no matter what they do and they stay at 99%.",
            "So that's where they say let's try other criteria and see if it gains anything there.",
            "'cause they don't gain anything when it comes to accuracy.",
            "So I've seen one vision data set where you get a 15% improvement in performance by learning the kernel.",
            "Over uniform weight.",
            "So that's not a consistent across the board performance on multiple data, right?",
            "And you of course, we can see examples, but consistently we don't see the performance improvement over the baseline of this uniform combining it.",
            "So I think.",
            "Coffee break in a moment.",
            "Before we do that, we need to have some room change announcements.",
            "But before we do that, let's please thank our speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe I should just not have it working and it'll you'll hear me just as well.",
                    "label": 0
                },
                {
                    "sent": "But now the microphone should be on an.",
                    "label": 0
                },
                {
                    "sent": "I just also have to find out which one of these actually does something.",
                    "label": 0
                },
                {
                    "sent": "Maybe you're he knows it.",
                    "label": 0
                },
                {
                    "sent": "Both.",
                    "label": 0
                },
                {
                    "sent": "Yes, here it is, at least OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you so much for the introduction.",
                    "label": 0
                },
                {
                    "sent": "I'll try to stay here.",
                    "label": 0
                },
                {
                    "sent": "I have running shoes on because those are the only issues that I have.",
                    "label": 0
                },
                {
                    "sent": "My daughter has started commenting on if Mom can't get other kinds of shoes and I might actually need to get other kind of shoes as I have to go to San Diego at the end of this month and get the official plaque.",
                    "label": 0
                },
                {
                    "sent": "What do we get your for?",
                    "label": 0
                },
                {
                    "sent": "You must know.",
                    "label": 0
                },
                {
                    "sent": "Say Mail it to me, but I wonder if I can show up and running shoes, and I think the answer is no.",
                    "label": 0
                },
                {
                    "sent": "Anyway, thank you so much for showing up this early morning hour.",
                    "label": 0
                },
                {
                    "sent": "I'm amazed how many of you made it out of bed.",
                    "label": 0
                },
                {
                    "sent": "The title of this talk is can learning kernel help performance?",
                    "label": 1
                },
                {
                    "sent": "And it's amazing to think of how specialized this community has become.",
                    "label": 0
                },
                {
                    "sent": "Because if I just added one little word to the title, if any of these would work.",
                    "label": 0
                },
                {
                    "sent": "This one doesn't work, so I had to use the other one.",
                    "label": 0
                },
                {
                    "sent": "This one doesn't work either.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know that one also works OK.",
                    "label": 0
                },
                {
                    "sent": "So I will have to go back to that.",
                    "label": 0
                },
                {
                    "sent": "We can give these may be OK if I just added one little word to the title, can learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With kernels help performance, we had a completely different talk.",
                    "label": 1
                },
                {
                    "sent": "Now this talk.",
                    "label": 0
                },
                {
                    "sent": "I think we had an affirmative answer to about 15 years ago.",
                    "label": 0
                },
                {
                    "sent": "But just to those of you that are now sitting in the audience, completely confused about what is the difference between these two?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to start out with a short introduction to learning with kernels.",
                    "label": 1
                },
                {
                    "sent": "In particular, I'm going to briefly over the SVM framework because we're going to need that a lot more later, and then I'm going to present to you the framework of learning kernels, in which I'll go into this infinite loop almost, at least until I run out of time discussing the Upson Downs in learning kernels over the last 10 years.",
                    "label": 0
                },
                {
                    "sent": "I'll discuss theoretical and algorithmic advances and I'll discuss frameworks.",
                    "label": 0
                },
                {
                    "sent": "Is a convex nonconvex optimization, linear versus non linear kernel combinations?",
                    "label": 1
                },
                {
                    "sent": "Human versus many kernels.",
                    "label": 0
                },
                {
                    "sent": "L1 and L2 regularization, alot of things have been tried and I'll pack it up.",
                    "label": 0
                },
                {
                    "sent": "All of these formulations with experimental results because after all why we should be doing this is to get better performance and not all of this has been so successful.",
                    "label": 0
                },
                {
                    "sent": "When I run out of time, I'm going to conclude the talk and discuss some future direction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So anyway, let's start out with learning with kernels.",
                    "label": 0
                },
                {
                    "sent": "As I said, we can indeed that later.",
                    "label": 0
                },
                {
                    "sent": "Kernel methods became very popular in the 90s, and they still are.",
                    "label": 0
                },
                {
                    "sent": "It kind of all started out with the optimal hyperplane, which is an algorithm for constructing two group linear classifier.",
                    "label": 0
                },
                {
                    "sent": "Window points are linearly separable.",
                    "label": 0
                },
                {
                    "sent": "Here we have the blue points and we have the red points and we can place a linear separator between the two points when they are separable.",
                    "label": 0
                },
                {
                    "sent": "There's not only one.",
                    "label": 0
                },
                {
                    "sent": "Linear separator that takes him apart, and the optimal hyperplane chooses the unique linear separator that maximizes the margin between the convex Hull of the two sets.",
                    "label": 0
                },
                {
                    "sent": "Just a little bit of nomenclature here in the optimal hyperplane we talk about the Canonical hyperplane.",
                    "label": 1
                },
                {
                    "sent": "That is, we normalize the weight vector such that the points that are closest to the decision surface they get the value one or minus one.",
                    "label": 0
                },
                {
                    "sent": "Those are active.",
                    "label": 0
                },
                {
                    "sent": "The ones that we call the support vectors with this notation.",
                    "label": 0
                },
                {
                    "sent": "Then it's easy to find out what the margin is.",
                    "label": 0
                },
                {
                    "sent": "We can just take two points, one here and one here.",
                    "label": 0
                },
                {
                    "sent": "The difference in projected down under weight vector and we get that two times the margin the margin is here.",
                    "label": 0
                },
                {
                    "sent": "Is nothing but two over the norm of the rate vector.",
                    "label": 0
                },
                {
                    "sent": "So when we choose the one unique separating surface that maximizes the margin between the two classes is equivalent to chooses to choose the separator with the smallest norm now.",
                    "label": 0
                },
                {
                    "sent": "This all works very well when the points are linearly separable, but as you know, in real life points are not and that was what we worked on.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With a soft margin hyperplane when the points are not separable, what we did was we equipped every point with a slack variable and non negative slack variable.",
                    "label": 0
                },
                {
                    "sent": "This site here and now we get that the support vectors are not only the.",
                    "label": 0
                },
                {
                    "sent": "Vectors here right under decision surface, but also everyone that has one of these non negative excited variables associated with it.",
                    "label": 0
                },
                {
                    "sent": "Now the soft margin hyperplane algorithm preserves the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the nice properties of the optimal hyperplane.",
                    "label": 0
                },
                {
                    "sent": "The cost function that we are optimizing as this we are minimizing the weight factor, which is we are maximizing the margin between the convex Hull of the two sets.",
                    "label": 0
                },
                {
                    "sent": "Subject to that, we don't want to have too many of these non negative slack variables, so it's a constraint optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Every point should be classified correctly within the accuracy of the slack and everybody has a slack variable.",
                    "label": 0
                },
                {
                    "sent": "The see here is up to the user to choose.",
                    "label": 0
                },
                {
                    "sent": "That's a tradeoff between the errors and the margin, and it's a convex optimization problem.",
                    "label": 1
                },
                {
                    "sent": "It's a quadratic optimization problem, which means that it has a unique solution.",
                    "label": 1
                },
                {
                    "sent": "This context and everything, so we can just either do a gradient descent or whatever.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, many other things that come around to do it efficiently.",
                    "label": 0
                },
                {
                    "sent": "So it's a lot of nice property.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is under way that we solve it is that we write the Lagrangian for the problem.",
                    "label": 0
                },
                {
                    "sent": "We introduce non negative electrons, variables Alpha and beta for all the constraints and at the saddle point of the Lagrangian we know the.",
                    "label": 0
                },
                {
                    "sent": "Coach can toggle conditions they have to hold, so we get the gradient with respect to their version with respect to the weight vector is zero, which leads us to the this.",
                    "label": 0
                },
                {
                    "sent": "The weight vector can be represented in terms of the the support vectors that went with the non 0A variables we gave.",
                    "label": 0
                },
                {
                    "sent": "Similarly for the other two constraints and we have.",
                    "label": 0
                },
                {
                    "sent": "Finally the complementarity conditions here, that either a point has a positive Alpha.",
                    "label": 0
                },
                {
                    "sent": "What is correctly classified?",
                    "label": 0
                },
                {
                    "sent": "Substituting those equations back into.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "SVM equation leaves us with this dual optimization problem.",
                    "label": 1
                },
                {
                    "sent": "That you probably all have seen a billion times, so we have resolving it with respect to the alphas and the solution is nothing more than an expansion on the support vectors.",
                    "label": 1
                },
                {
                    "sent": "The only difference actually between this optimization problem and one without the slack variables are the things that I have in blue boxes here.",
                    "label": 0
                },
                {
                    "sent": "So here we have that Alpha should be between zero and C. And if Alpha is C then it has a non negative slack variable.",
                    "label": 0
                },
                {
                    "sent": "It's and so called error.",
                    "label": 0
                },
                {
                    "sent": "And we get that the B now the offset is in terms of only you should find it only for the ones that have an Alpha less than C. Now, um.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this problem is, as you can see, the X is they never appear alone, and that was actually what made it all possible to Colonel eyes.",
                    "label": 0
                },
                {
                    "sent": "We have an expert appears here, but it appears together with another X and the solution also here appears in together with the other axes.",
                    "label": 0
                },
                {
                    "sent": "So thanks to work that had already preceded the soft margin hyperplane, we had the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The Kernelized formulation, where we can, instead of the inner product between 2X, is insert a kernel.",
                    "label": 0
                },
                {
                    "sent": "And that's what we have here.",
                    "label": 0
                },
                {
                    "sent": "So here we have the kernel, and we have a kernel down here.",
                    "label": 0
                },
                {
                    "sent": "All that is required for for this optimization problem still to be convex is that the kernel is positive, symmetric and some definite.",
                    "label": 0
                },
                {
                    "sent": "So this was this was the optimization problem that we.",
                    "label": 0
                },
                {
                    "sent": "Worked on in 95 and it's interesting to go back and read the paper for for this soft margin hyperplane because we actually didn't have much of a theoretical justification for it.",
                    "label": 0
                },
                {
                    "sent": "It's kind of nice to say that we select the hyperplane that has the largest margin, but despite we knew kind of emerging pounds, if you read the papers interesting to see that we sort of justice cast as the reason why this is working.",
                    "label": 0
                },
                {
                    "sent": "In terms of and or comes races principle that will minimize the weight vectors.",
                    "label": 0
                },
                {
                    "sent": "So we have the fewest number of parameters.",
                    "label": 0
                },
                {
                    "sent": "We had a little bit of a margin bound, but it was only for for separable classes, so the real nice theoretical treatment of the support vector algorithm came with this paper.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In 99, where we actually get a nice explanation for why it's working here, we have that the generalization error is upper bounded.",
                    "label": 0
                },
                {
                    "sent": "We have the error, so this is like a typical structural risk minimization.",
                    "label": 0
                },
                {
                    "sent": "We have the error on the training set is access a fraction of training points with margin less than row.",
                    "label": 1
                },
                {
                    "sent": "That's all the ones with the with a non negative PSI they come in here.",
                    "label": 0
                },
                {
                    "sent": "And then we have this slack term that represents the capacity of the separating hyperplane.",
                    "label": 0
                },
                {
                    "sent": "You see here, there is a radius that's the smallest radius of the ball.",
                    "label": 0
                },
                {
                    "sent": "In closing all the points and we have the margin here Rose Squared.",
                    "label": 0
                },
                {
                    "sent": "So now we actually have this theoretical justification for their algorithm.",
                    "label": 0
                },
                {
                    "sent": "Work is simply structural risk minimization where we have this as how Slack term later on it has been shown how to get rid of this log term.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now everything you could say it's perfect, right?",
                    "label": 0
                },
                {
                    "sent": "We we had an algorithm with great performance.",
                    "label": 0
                },
                {
                    "sent": "It's a convex optimization problem.",
                    "label": 0
                },
                {
                    "sent": "We have a theoretical justification for always working and everybody said it using it it.",
                    "label": 0
                },
                {
                    "sent": "And it was just everything got kernelized.",
                    "label": 0
                },
                {
                    "sent": "We had kernel which we crash.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we have kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "Anything that could be kernelized with, kernelized and probably in the in the late 90s.",
                    "label": 0
                },
                {
                    "sent": "One of the most frequent keywords were nips would have been kernel methods in one way or the other.",
                    "label": 0
                },
                {
                    "sent": "So this is my short introduction to two kernel methods that.",
                    "label": 0
                },
                {
                    "sent": "One unfortunate thing about the kernel methods here that we now are going to discuss is what kernel should we be using?",
                    "label": 0
                },
                {
                    "sent": "Because it's kind of just left up to the user to come with a good kernel.",
                    "label": 0
                },
                {
                    "sent": "And if you take the standard package is out on the market.",
                    "label": 0
                },
                {
                    "sent": "We have lip SVM uses one kernel as a default.",
                    "label": 0
                },
                {
                    "sent": "That is the Gaussian kernel, while.",
                    "label": 0
                },
                {
                    "sent": "SPM Night uses at polynomial kernel, so you could kind of think this is make any difference with kernel you using and it does indeed for base performance.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter I think, but if you really want a competitive algorithm, you have to fetch around with Colonel.",
                    "label": 0
                },
                {
                    "sent": "So the problem is can we learn the Colonel from the data instead?",
                    "label": 0
                },
                {
                    "sent": "And that's what we're going to be discussing next now.",
                    "label": 0
                },
                {
                    "sent": "The first",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is difficult.",
                    "label": 0
                },
                {
                    "sent": "OK so the 1st.",
                    "label": 0
                },
                {
                    "sent": "The first",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But started thinking about it.",
                    "label": 0
                },
                {
                    "sent": "Did think about it in terms of structural risk minimization.",
                    "label": 1
                },
                {
                    "sent": "Here we have the dual formulation of the SVM problem and it's natural that we want to select the kernel that minimizes an upper bound on the generalization error.",
                    "label": 1
                },
                {
                    "sent": "But at the early stages there is sort of a little bit.",
                    "label": 1
                },
                {
                    "sent": "You know, what should we act and minimize?",
                    "label": 0
                },
                {
                    "sent": "Which ground should we be minimizing and.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At first you would say, well, we just had this beautiful bound.",
                    "label": 0
                },
                {
                    "sent": "It included an R-squared.",
                    "label": 0
                },
                {
                    "sent": "The radius of the smallest enclosing up all of the points and a bro.",
                    "label": 0
                },
                {
                    "sent": "So since the SVM cost function itself doesn't include there are squared, it's kind of natural that you want to minimize that bound in order to get good generalization error.",
                    "label": 0
                },
                {
                    "sent": "And then Fortunately the radius can be expressed in terms of the kernel.",
                    "label": 0
                },
                {
                    "sent": "And so in this early work, people either minimize the margin bound in terms of R-squared or rose squared, or a span bound which is expressed in terms of the support vectors that come out of solving the SVM problem.",
                    "label": 1
                },
                {
                    "sent": "Now did it work?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It did and in certain respects it.",
                    "label": 0
                },
                {
                    "sent": "It thank you for the experiment with salt.",
                    "label": 0
                },
                {
                    "sent": "I must say here that I am going to borrow heavily from all of your work.",
                    "label": 0
                },
                {
                    "sent": "All of you here in the audience throughout this talk.",
                    "label": 0
                },
                {
                    "sent": "And it's not to place judgment on any of the experimental results.",
                    "label": 0
                },
                {
                    "sent": "I'm just trying to simply give a little bit of a story of what has been done, and it's not too too single.",
                    "label": 0
                },
                {
                    "sent": "Anybody out, if I haven't included your work also?",
                    "label": 0
                },
                {
                    "sent": "There's so much that has been done, so just bear with me here.",
                    "label": 0
                },
                {
                    "sent": "Some of the first experimental results is in some of the UC Irvine datasets that we all know and what they're trying to do here in this alternate approach of of solving the SVM problem and doing a seabass descent on abound on the generalization here, they're trying to find the best setting for Gaussian kernel and the this parameter.",
                    "label": 1
                },
                {
                    "sent": "See that you know in the cost function of SVM's.",
                    "label": 0
                },
                {
                    "sent": "And what they're comparing to is the very best they could obtain.",
                    "label": 0
                },
                {
                    "sent": "If they did a full cross validation, and indeed they can get the same results in same performance.",
                    "label": 0
                },
                {
                    "sent": "So in that respect it works beautifully, right?",
                    "label": 0
                },
                {
                    "sent": "Because they can get the same and it's 100 times faster than doing this full cross validation over the combined space of the Sigma of the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And this C there were a number of other early successes of learning kernels.",
                    "label": 0
                },
                {
                    "sent": "You can train some.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a feature.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just selecting mechanism.",
                    "label": 0
                },
                {
                    "sent": "If we take every variable in where's my pointer?",
                    "label": 0
                },
                {
                    "sent": "Here we take the variables and we multiply it by a non negative weight.",
                    "label": 0
                },
                {
                    "sent": "So we sort of try to.",
                    "label": 0
                },
                {
                    "sent": "Do we have a feature importance weighting off of the features and we can maybe restrict the weights and in some fashion where P could be one or two, depending on whether you want one or L2 regularization you can again do this alternate between in your kernel you solve the SVM and you do a gradient step in merging pound or these people they activated directly on the SVM cost function itself.",
                    "label": 1
                },
                {
                    "sent": "Now does it work?",
                    "label": 0
                },
                {
                    "sent": "Yes, it worked beautifully.",
                    "label": 0
                },
                {
                    "sent": "Here we have a.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comparison if you can see it, it might be a little bit difficult, but we have here it's these artificial datasets, so the ones that left is a linear data set and they use the linear kernel, the one to the right is a nonlinear.",
                    "label": 0
                },
                {
                    "sent": "They used a polynomial kernel, so here they are doing feature selection.",
                    "label": 1
                },
                {
                    "sent": "They know because they know the toy data here how many features to select and it and they can select the appropriate number and they can compare it to other classic methods for.",
                    "label": 0
                },
                {
                    "sent": "For feature selection filter methods where they can take the same number of variables and it was beautiful.",
                    "label": 0
                },
                {
                    "sent": "Here we have the number of training examples and performance of the of the SVM after they have selected the features and the learned kernels here easily drops down while all the other methods.",
                    "label": 0
                },
                {
                    "sent": "This is an SVM itself, it doesn't do any feature selection and the other methods are not competitive.",
                    "label": 0
                },
                {
                    "sent": "One bit over here we have the same performance the.",
                    "label": 0
                },
                {
                    "sent": "The learn kernels quickly drop down while the other ones they say so.",
                    "label": 0
                },
                {
                    "sent": "Learning kernels help great promise and the papers started coming.",
                    "label": 0
                },
                {
                    "sent": "The next big step up, it comes with the papers of Lancret at Al, where there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem is given as serious mathematical formulation with convergent guarantees, because so far we have been doing some gradient steps, alternating between solving the SVM and and doing a gradient and some bound.",
                    "label": 0
                },
                {
                    "sent": "We needed a serious framework for it and it came in about 2000 two 2003.",
                    "label": 0
                },
                {
                    "sent": "The problem is here completely formulated in terms of the SVM cost function.",
                    "label": 0
                },
                {
                    "sent": "Because you could say after all it doesn't contain that R-squared.",
                    "label": 0
                },
                {
                    "sent": "But hey, it is supposed to be an upper bound on the generalization error.",
                    "label": 0
                },
                {
                    "sent": "So what language at all is formulating it here is is we want to find the kernel that minimizes this upper bound.",
                    "label": 0
                },
                {
                    "sent": "On the generalization here and we're going to make it subject to that the kernel should be PDS, and we're going to make some constraint under colonels one way or the other.",
                    "label": 0
                },
                {
                    "sent": "We're going to parameterize the family of kernels, and the way it was done here was through the trace of the kernel, where that restricts the family of kernels in order to make this a computational feasible path problem would likely get loose.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "He linearized other kernel, so he said, let's take a set of kernel matrices and let's just assume that the kernel is a linear combination of those matrices.",
                    "label": 0
                },
                {
                    "sent": "Actually, Wade was formulated in this paper within a transductive setting, but there's really no need for it because he had the expression of the kernel matrices.",
                    "label": 0
                },
                {
                    "sent": "But doing that, especially if he said so, here we have here, is linearized the kernel.",
                    "label": 0
                },
                {
                    "sent": "We just have a linear expansion on it, especially if he restricts the expansion to be non negative coefficients.",
                    "label": 0
                },
                {
                    "sent": "And so he has.",
                    "label": 0
                },
                {
                    "sent": "And he has this L1 regularization constraints that the sum of the expansion coefficients, trams, the trace of the individual kernels should be less than Lambda.",
                    "label": 0
                },
                {
                    "sent": "He can actually end up getting a Q seek problem, so that's very nice as a convex optimization problems with efficient efficient algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To solve it, the reason he does the positive combination and that the general also positive and negative combination is that in general then it would be an SDP problem.",
                    "label": 0
                },
                {
                    "sent": "We set some much higher computational complexity, but by restricting it to these non negative linear combinations of kernels we get acoustic problems later, also reformulated as some infinite LP problem which can be solved very very efficiently.",
                    "label": 1
                },
                {
                    "sent": "They have solved it on on.",
                    "label": 0
                },
                {
                    "sent": "Up to 10,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "Actually, it can be all the way restricted down to a QP problem.",
                    "label": 0
                },
                {
                    "sent": "In case that you have it's rank one matrices so beautiful, we have a convex formulation.",
                    "label": 0
                },
                {
                    "sent": "Now does it work?",
                    "label": 0
                },
                {
                    "sent": "Well, here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have an example of it from from this paper.",
                    "label": 0
                },
                {
                    "sent": "It's a lot of numbers, so let me try to work it through it.",
                    "label": 0
                },
                {
                    "sent": "He has three kernels he is trying to form a combination of this is a polynomial of degree two.",
                    "label": 0
                },
                {
                    "sent": "This is a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "Here we have the Sigma and this is a linear kernel and here we have the result if we are combining possibly none positive or a negative combination of the kernels.",
                    "label": 0
                },
                {
                    "sent": "So this is the SDP problem.",
                    "label": 0
                },
                {
                    "sent": "Here we have the QCQP problem, it's a positive.",
                    "label": 0
                },
                {
                    "sent": "Span of the kernels and here we have the result.",
                    "label": 0
                },
                {
                    "sent": "If we just did one kernel but did cross validation and see to see which one we could get the best we saw it with with an RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "What you can see here is a number of algorithms they were running.",
                    "label": 0
                },
                {
                    "sent": "Here we have the hard margin classifier, so it's not linearly.",
                    "label": 0
                },
                {
                    "sent": "Since this is a hard data from user ID so that data set before so it's not linearly separable.",
                    "label": 0
                },
                {
                    "sent": "But once we get to a soft margin classifier, either with the normal one and this lag variables or the number 2.",
                    "label": 0
                },
                {
                    "sent": "Here we have additional tuning of the C parameter, then it actually gets to be linearly separable and you can see here this is the very best performance he can get when he does all possible combinations.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really beat the single best kernel ever.",
                    "label": 0
                },
                {
                    "sent": "And there isn't much to be gained from going from the Q seek problem to the SCP problem.",
                    "label": 0
                },
                {
                    "sent": "The performance difference between these two is miniscule here.",
                    "label": 0
                },
                {
                    "sent": "I guess they ran out of time because it's a computationally harder problem.",
                    "label": 0
                },
                {
                    "sent": "So despite we now have a very nice framework for it.",
                    "label": 0
                },
                {
                    "sent": "At least this didn't seem to be buying us a whole lot, and a lot of paper is a kind of came in the same direction.",
                    "label": 0
                },
                {
                    "sent": "There isn't much to be gained from it.",
                    "label": 0
                },
                {
                    "sent": "There's one thing that's missing that later became a baseline for for learning kernels.",
                    "label": 0
                },
                {
                    "sent": "That is the even combination of all the kernels that is sadly missing from this picture, but I'm sure that it wouldn't be doing much different from the 84% performance, so.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, we all honest people.",
                    "label": 0
                },
                {
                    "sent": "When we write our papers and I have been writing some of these papers myself.",
                    "label": 0
                },
                {
                    "sent": "When you set out to solve a problem to get better performance and it actually doesn't do a whole lot better, you gotta find another reason for having written a paper, right so?",
                    "label": 0
                },
                {
                    "sent": "So people have been very, very good at finding good reasons for having done learning.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Journals write speed.",
                    "label": 0
                },
                {
                    "sent": "It's much faster 100 times faster, so that's like that's a good reason for doing that.",
                    "label": 0
                },
                {
                    "sent": "Instead of doing cross validation and good ranking properties also turned out to be an advantage of these learning kernels, here's just one.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example from a computational biology paper where we have we have a number of currents.",
                    "label": 0
                },
                {
                    "sent": "We have 5 kernels that that are known to be good for the problem at hand.",
                    "label": 0
                },
                {
                    "sent": "It's a very biased problem.",
                    "label": 0
                },
                {
                    "sent": "We have like 98% positive in 2% negative in this data set and accuracy wise learning kernels didn't help the whole lot, but Fortunately there were other things here.",
                    "label": 0
                },
                {
                    "sent": "They you see it says I received, but I actually think it should have been a.",
                    "label": 0
                },
                {
                    "sent": "You see when they are learning the kernels we can get from 96% up to 99% and there's also this the true positive percentage is at 1% false positive.",
                    "label": 0
                },
                {
                    "sent": "Seems to be going steadily up from 42% to 88%.",
                    "label": 0
                },
                {
                    "sent": "This actually one very nice thing about this this this table, first of all, here we have the five meaningful kernels, but then they added nicely some random noise and what they're showing here is that if you put in here, we have it.",
                    "label": 0
                },
                {
                    "sent": "We learning the kernels in the framework from before, but we throw in a little bit of noise we don't reduce.",
                    "label": 0
                },
                {
                    "sent": "Performance by a whole lot.",
                    "label": 0
                },
                {
                    "sent": "The learning kernel framework is surprisingly robust to adding in noise.",
                    "label": 0
                },
                {
                    "sent": "What you also see here is that actually it seems to be doing better than the even combination.",
                    "label": 0
                },
                {
                    "sent": "Here we have that we are not learning the kernels.",
                    "label": 0
                },
                {
                    "sent": "All the weights for the Colonels is the same we are adding in noise with also the same ratio and these ranking measures at least drop down.",
                    "label": 0
                },
                {
                    "sent": "So maybe there are still good things about running kernels, just not in terms of the accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have another example, also from computational biology in terms of feature selection, being able to understand the models, there seems to be a lot to be said, so here we have an example where they're trying to to find out the importance of.",
                    "label": 0
                },
                {
                    "sent": "It's some splice sites and I wish there was somebody in the audience that could explain better to be what it is, but.",
                    "label": 0
                },
                {
                    "sent": "Here they have the excellent and they're trying to find out what importance having neighboring sites in the DNA with respect to this excellent and they can actually explain that these sites back here corresponds to the the previous excellent and these are inhibitory or whatever so they can get these from the importance weighting that we so earlier they can get these nice explanations of of the models and they can look at it and see why it's working.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accuracy was obviously not the what was coming out of it.",
                    "label": 0
                },
                {
                    "sent": "Maybe the whole problem was that we were learning with two few kernels here indeed.",
                    "label": 0
                },
                {
                    "sent": "So we had here like 5 after 50 kernels.",
                    "label": 0
                },
                {
                    "sent": "Maybe indeed the early theoretical results were not too encouraging for learning with a lot of kernels.",
                    "label": 0
                },
                {
                    "sent": "The only non vacuous bounds that we had said that if you learn with currency if you have P kernels here you can it be paying a price that is multiplicative in the number of kernels.",
                    "label": 0
                },
                {
                    "sent": "So if you have 10 times as many kernels then you need 10 times as much training data and with these SDP problems.",
                    "label": 0
                },
                {
                    "sent": "That's probably not the way you want to go.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, also that doesn't seem to be the case.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In 2006, Shampoo and then David came with this other pound.",
                    "label": 0
                },
                {
                    "sent": "It shows that I should learning kernel here.",
                    "label": 0
                },
                {
                    "sent": "It's in the linear classification framework with an L1.",
                    "label": 0
                },
                {
                    "sent": "Regularization is not going to cost us that much after all they came up with an additive bound instead.",
                    "label": 0
                },
                {
                    "sent": "So here you have now that the number of kernels is to only additive and here we have this data stack term you saw before.",
                    "label": 0
                },
                {
                    "sent": "Also arose squared over N out.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "Oh, children attention that hires conveniently a number of factors and back here we have the same term as you saw before in the margin.",
                    "label": 0
                },
                {
                    "sent": "Beyond this is the fraction of training points with margin less than than row.",
                    "label": 1
                },
                {
                    "sent": "So maybe the problem all the time had been that we've been learning with two few kernels.",
                    "label": 0
                },
                {
                    "sent": "We should just learn with a lot more kernels.",
                    "label": 0
                },
                {
                    "sent": "Fortunately, people also tried that here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do you have an example of people learning with infinitely many kernels?",
                    "label": 1
                },
                {
                    "sent": "This so called hyper kernels framework where it's not just a kernel is the kernel between kernels.",
                    "label": 0
                },
                {
                    "sent": "That might be a little hard to grasp, but so here we have data kernel between two points is parameterized by.",
                    "label": 0
                },
                {
                    "sent": "It can be expected, can be represented by 4 points that need these four points, and we have M ^2 parameters here to estimate, and they can still formulate this as an SDP problem, so hard computational complexity.",
                    "label": 0
                },
                {
                    "sent": "But you know, we can do it, and we know what we're doing doesn't work then.",
                    "label": 0
                },
                {
                    "sent": "Now we do.",
                    "label": 0
                },
                {
                    "sent": "Infinitely many kernels.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I don't know for sure here we have the kernel they're using in this experiment is a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "So here you can get an idea of what these hyper kermes they are.",
                    "label": 0
                },
                {
                    "sent": "We have 4.",
                    "label": 0
                },
                {
                    "sent": "PT training points here, and you see how they they go in like in a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "The sigmas here are determined before hand the Lambda.",
                    "label": 0
                },
                {
                    "sent": "Here's determined beforehand.",
                    "label": 0
                },
                {
                    "sent": "We're not optimizing with respect to these parameters, we're only optimizing the betas that go in and control, which X is should be sitting here.",
                    "label": 0
                },
                {
                    "sent": "Or which prime should be sitting here so they run this and they run it on here we have our standard UC Irvine datasets.",
                    "label": 0
                },
                {
                    "sent": "We have the heart.",
                    "label": 0
                },
                {
                    "sent": "For instance, we send out a number of times and they run this SDP problem.",
                    "label": 0
                },
                {
                    "sent": "On three different cost functions, the standard SVM than USB embrace the number at the fraction of support vectors, and here we have a squared error SVM and when they compare what they can get here to the very best they could have gotten.",
                    "label": 0
                },
                {
                    "sent": "If they only used one Gaussian, so not this infinite number of Gaussians but only one Gaussian, and that's what we get over here.",
                    "label": 0
                },
                {
                    "sent": "The cross validation so you can see, for instance, if we take the hard data set.",
                    "label": 0
                },
                {
                    "sent": "But they get significantly better performance than if they had only used one kernel.",
                    "label": 0
                },
                {
                    "sent": "However, they have honest enough to report the very best result they can find in the literature, and they're not beating that, so you can ask yourself, does it actually buy us anything?",
                    "label": 0
                },
                {
                    "sent": "Additionally, I know it's so hard to compare these results.",
                    "label": 0
                },
                {
                    "sent": "Often when you sit and stay at the tables you wonder you wonder how?",
                    "label": 0
                },
                {
                    "sent": "How do I compare because here we have like a 23% error rate for the best Gaussian kernel for the hard data set.",
                    "label": 0
                },
                {
                    "sent": "Despite that in a previous slide I showed you 16% error rate for a Gaussian kernel, so but it can all be in the preprocessing of in.",
                    "label": 0
                },
                {
                    "sent": "Here there is absolutely no preprocessing, never very honest about that so.",
                    "label": 0
                },
                {
                    "sent": "Learning with infinitely many kernels doesn't seem to bias anything still.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Framework, so maybe it is because we're doing classification.",
                    "label": 0
                },
                {
                    "sent": "We should have done regression instead.",
                    "label": 0
                },
                {
                    "sent": "So I said this is something I worked on myself and this is a paper that you'll see later in the conference, not in this conference, but in UA I.",
                    "label": 0
                },
                {
                    "sent": "So here we set it out looking at learning kernel spread for regression.",
                    "label": 0
                },
                {
                    "sent": "Maybe now we're going to see that it all works right?",
                    "label": 0
                },
                {
                    "sent": "So we also managed to come up with a bound that is nicely additive and it's actually with the square root of P. This is disability based bound, so it's a different duration than the other bound you saw before.",
                    "label": 0
                },
                {
                    "sent": "It kind of makes it hard to do this.",
                    "label": 0
                },
                {
                    "sent": "This stability analysis is that you have to change the point all the time, but it's not only that you change the hypothesis by changing one point, you have to change the kernel because now it's a kernel that's optimized on a different set of points.",
                    "label": 0
                },
                {
                    "sent": "So we ended up having to have a little technical conditions that is orthogonal kernels, but does not a whole lot different from a lot of the other things that we see, which requires it to be concatenation kernels so.",
                    "label": 0
                },
                {
                    "sent": "Here we we again end up with that.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not so bad to use a lot of kernels in regression we chose the and L2 regularization.",
                    "label": 0
                },
                {
                    "sent": "Which would mean that we have.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This problem formulation is that this is the kernel Ridge regression that you also saw before and you have now that we have an L2 regularization on the expansion coefficients.",
                    "label": 0
                },
                {
                    "sent": "In order to get an efficient solution for this problem we reformulated.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can we have here the formulation, but an we introduce these variables V. Fortunately, this can be rewritten in terms of using a fundaments generalized minimax theorem, so we can, instead of minimizing album you after we have maximized or Alpha, we can permute the order and then you can see you can move in them.",
                    "label": 0
                },
                {
                    "sent": "In here you can actually solve explicitly for the muse.",
                    "label": 0
                },
                {
                    "sent": "You get a closed form solution for them.",
                    "label": 0
                },
                {
                    "sent": "You so now you got rid of those completely.",
                    "label": 0
                },
                {
                    "sent": "You're left with a problem that's only a maximization or Alpha.",
                    "label": 0
                },
                {
                    "sent": "It's actually a little bit like the standard kernel Ridge regression with musiro kernel, except from you have that.",
                    "label": 0
                },
                {
                    "sent": "So the solution comes in in about the same form, except from you now is given us this Musial plus this Lambda V over normal V where Viva.",
                    "label": 0
                },
                {
                    "sent": "These these parameters from up here so it can be solved.",
                    "label": 0
                },
                {
                    "sent": "Fairly efficiently, it's a convex problem, so we could either have done just a gradient descent on it, but we don't do that.",
                    "label": 0
                },
                {
                    "sent": "This whole formulation encourages sort of an iterative solution to the problem where you start out with.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One kernel you saw, you saw it out, maybe with the uniform kernel you solve with respect to Alpha, and now you repeat until convergence in terms of some epsilon parameter that you solve for Alpha, you update the VIS you update them, use and then you re solve for Alpha and blended in and so and so it reaches convergence so.",
                    "label": 0
                },
                {
                    "sent": "Regression with a large number of kernels does it work?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe a little bit.",
                    "label": 0
                },
                {
                    "sent": "Here we have.",
                    "label": 0
                },
                {
                    "sent": "The baseline is are even weighted kernel, so this got just kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "And we are here using rank one kernel, so it's a.",
                    "label": 0
                },
                {
                    "sent": "It's the very simplest kernel that you can imagine not doing anything fancy on top of it.",
                    "label": 0
                },
                {
                    "sent": "Here we have the L2 regularised kernel.",
                    "label": 0
                },
                {
                    "sent": "So as we increase the number of kernels from 1000 to 6000 we actually have some performance improvement as opposed to if we done the L1 regularised for for regression.",
                    "label": 0
                },
                {
                    "sent": "It actually only seems to be doing worse than the baseline as we continue on here we have it with data deviation.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be something at least to be gained from doing L2 regularization.",
                    "label": 0
                },
                {
                    "sent": "And regression instead if you want to do learning kernels that also is supported by another paper.",
                    "label": 0
                },
                {
                    "sent": "In the audience.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other hierarchical kernel learning where we have also learning with a very, very large number of of kernels in a very elegant and intricate set up where we only selecting the kernels in a hierarchical fashion.",
                    "label": 1
                },
                {
                    "sent": "Also, this can be formalized in the convex optimization problems and he is using L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "Their whistles here seem to confirm that in regression with a lot of kernels there is.",
                    "label": 0
                },
                {
                    "sent": "Something to be current gained.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From learning kernels, I shorten down your tables.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry a little bit about the so here we are comparing.",
                    "label": 0
                },
                {
                    "sent": "This is an L1, though with regression.",
                    "label": 0
                },
                {
                    "sent": "So here we have their hierarchical kernel learning for a number of datasets and the number of kernels here is stunning.",
                    "label": 0
                },
                {
                    "sent": "Really stunning for Air 4th degree polynomial since we're selecting polynomials up to a degree of four and we have like.",
                    "label": 0
                },
                {
                    "sent": "10 parameters we get to to the 10 to the 7th number of kernels to speed selecting from.",
                    "label": 0
                },
                {
                    "sent": "It's amazing they can do this efficiently, but they can really do it efficiently and you see here that some performance improvement can be gained over here.",
                    "label": 0
                },
                {
                    "sent": "We just have the selecting it not in this hierarchical fashion.",
                    "label": 0
                },
                {
                    "sent": "This is the multiple kernel learning distances tainted formulation and here we have the kernel which regression as it is.",
                    "label": 0
                },
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "I mean there is in general more black in this column then in this column over here.",
                    "label": 0
                },
                {
                    "sent": "So there is something to be gained a little bit from.",
                    "label": 0
                },
                {
                    "sent": "Doing regression with learning kernels and multiple kernels.",
                    "label": 0
                },
                {
                    "sent": "Large number of kernels.",
                    "label": 0
                },
                {
                    "sent": "So here we see that L1 regularization does somewhat better than the kernel Ridge regression.",
                    "label": 0
                },
                {
                    "sent": "We can only hope for if it could be formulated with L2 regularization that would be doing even better.",
                    "label": 0
                },
                {
                    "sent": "So what we have learned so far is that it's really, really hard to get learning kernels to work surprising.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We heard it's such a nice and intuitive notion that we should be able to learn to Colonel from the data, but we just can't be consistently get it to work.",
                    "label": 0
                },
                {
                    "sent": "We sometimes get somewhat better performance than just having the evenly weighted kernel, but in general is hard to say that we get it.",
                    "label": 0
                },
                {
                    "sent": "L2 regularization seems to be doing slightly better than L1 regularization.",
                    "label": 0
                },
                {
                    "sent": "We had a workshop at NIPS and that also seemed to be the the story we heard all the time now.",
                    "label": 0
                },
                {
                    "sent": "One regularization doesn't really do a whole lot, though it seems to do in the hierarchical kernel learning.",
                    "label": 0
                },
                {
                    "sent": "There seems to be some something to be gained from it.",
                    "label": 0
                },
                {
                    "sent": "Not that we need a lot of kernels in order for it to work.",
                    "label": 0
                },
                {
                    "sent": "We have seen very little performance improvement with just a few kernels, so we handled it all.",
                    "label": 0
                },
                {
                    "sent": "These other redeeming properties of learning curve is much faster.",
                    "label": 0
                },
                {
                    "sent": "The optimization community has been doing so many wonderful things for learning kernels that today it's just blazingly fast despite their hard computational properties of the problem.",
                    "label": 0
                },
                {
                    "sent": "It's great for.",
                    "label": 0
                },
                {
                    "sent": "For feature selection that still holds up.",
                    "label": 1
                },
                {
                    "sent": "But aside from that, there isn't much that can be said in to defend that.",
                    "label": 0
                },
                {
                    "sent": "We continue writing, learning Colonel previous, I must admit.",
                    "label": 0
                },
                {
                    "sent": "I think we have to try something completely different and some people have started during the non linear combinations have the current.",
                    "label": 0
                },
                {
                    "sent": "So far you've seen as everything is linear and these these news.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the linear explain.",
                    "label": 0
                },
                {
                    "sent": "So what can we do if we if we go in order of non linear combinations and there has been papers in that direction as well 2005 there was a very elegant paper, I would say where they turned it into a DC probe.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Coming problem Ann and I can just maybe quickly say a few words about it here.",
                    "label": 0
                },
                {
                    "sent": "They have that.",
                    "label": 0
                },
                {
                    "sent": "It's again.",
                    "label": 0
                },
                {
                    "sent": "It's continuously parameterized set of kernels, and here we are trying to learn the Sigma in these these Gaussians.",
                    "label": 1
                },
                {
                    "sent": "So instead of having this as a fixed one, we're trying to learn these and.",
                    "label": 0
                },
                {
                    "sent": "They they can manage to do it in.",
                    "label": 0
                },
                {
                    "sent": "It's not convex.",
                    "label": 0
                },
                {
                    "sent": "That's a problem when we go to these nonlinear parameterized kernel expansions.",
                    "label": 0
                },
                {
                    "sent": "It's not a convex optimization problem anymore, but they still manage to solve this because they can formulate it as a DC.",
                    "label": 0
                },
                {
                    "sent": "So it's a difference between convex, welcoming problems, so they can.",
                    "label": 0
                },
                {
                    "sent": "They can still allow themselves to do these alternate steps of solving the optimization problem, the kernel problem, and then do a gradient descent step.",
                    "label": 0
                },
                {
                    "sent": "In order to estimate new Gaussians that they're adding into the problem, so they still know what they're doing and.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, does that work?",
                    "label": 0
                },
                {
                    "sent": "So this is really hard to see.",
                    "label": 0
                },
                {
                    "sent": "If I were you, should you should look at the at the table here.",
                    "label": 0
                },
                {
                    "sent": "Down at the bottom and they can get some better performance and they should compare to here where where their brute force trying to find the best parameters here are trying to optimize with two of the Gaussian sigmas.",
                    "label": 0
                },
                {
                    "sent": "Yes, they can get some performance improvement, but it here not compared to any baseline.",
                    "label": 0
                },
                {
                    "sent": "So so the main thing is here that they they can at least formulated in a way where they can tackle their problem.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, we kind of.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have another paper.",
                    "label": 0
                },
                {
                    "sent": "It's actually at this.",
                    "label": 0
                },
                {
                    "sent": "I CML the generalized MCL learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's it's sort of the same formulation as you had in the previous slide, where they're trying to learn the sigmas, either for Gaussian or in a polynomial.",
                    "label": 0
                },
                {
                    "sent": "You're trying to learn the new expansion coefficients here, where P can be any number.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's interesting to see because with this paper we actually go full circle, right?",
                    "label": 0
                },
                {
                    "sent": "We're getting back to the formulations that we had in the very very early kernel learning papers.",
                    "label": 0
                },
                {
                    "sent": "We're doing this solving the SVM problem.",
                    "label": 1
                },
                {
                    "sent": "We're doing a gradient descent step.",
                    "label": 0
                },
                {
                    "sent": "And we have no guarantees for where we're going and this this paper doesn't discuss anything about the convexity of the problem.",
                    "label": 0
                },
                {
                    "sent": "Can we get stuck in a local minimum?",
                    "label": 0
                },
                {
                    "sent": "It's worth trying and I guess that's where you end up where we are.",
                    "label": 0
                },
                {
                    "sent": "Nothing else work, just try everything and see if it works.",
                    "label": 0
                },
                {
                    "sent": "But still, even though.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then when they try everything and I think it's honorable to try.",
                    "label": 0
                },
                {
                    "sent": "At the end of the day, here, they have their performance.",
                    "label": 0
                },
                {
                    "sent": "If I still have this little pointer here, they have their performance and it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It works great for feature selection, so if they're only allowed to use 15 features then they can get a better performance and with any of the other algorithms.",
                    "label": 0
                },
                {
                    "sent": "But if they let the other algorithms use all the features, they don't beat the performance, so again.",
                    "label": 0
                },
                {
                    "sent": "We don't have a clear advantage of learning kernels.",
                    "label": 0
                },
                {
                    "sent": "So on this happy note.",
                    "label": 0
                },
                {
                    "sent": "I want to end the talk and say.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We we need to get this to work.",
                    "label": 1
                },
                {
                    "sent": "It was a great idea, but so far I haven't seen any proves out that it actually buys us anything.",
                    "label": 0
                },
                {
                    "sent": "It has heard a lot of interest in the community, lots of food optimization algorithms have come out of it.",
                    "label": 0
                },
                {
                    "sent": "Lots of interesting feature selection where it as an accuracy improvement.",
                    "label": 0
                },
                {
                    "sent": "We haven't seen much of it so far.",
                    "label": 0
                },
                {
                    "sent": "Maybe we have to try something completely different and I don't know where it is because then I wouldn't be standing here.",
                    "label": 0
                },
                {
                    "sent": "Would be riding on it, but I think we really have to step back and say.",
                    "label": 0
                },
                {
                    "sent": "What did we do that that was just too limiting in the way we did learning kernels because so far everything seems to just have been getting us very little over the baseline performance.",
                    "label": 0
                },
                {
                    "sent": "I remember William Noble saying I should add the Nips workshop we had in December, but he had the kernel for learning kernels floating around, and if anybody was interested in using it then feel free to, but he was not going to spend more time on it, so there we go.",
                    "label": 0
                },
                {
                    "sent": "Of course you know is there any way that theory can guide us to how we could be doing this differently, because anything that's experimental seems to have not been too successful.",
                    "label": 1
                },
                {
                    "sent": "So far, so good luck guys.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have some time for questions.",
                    "label": 0
                },
                {
                    "sent": "She mentioned a lot of work from people in the room, so I imagine there's lots of things in common done I I'm sure.",
                    "label": 0
                },
                {
                    "sent": "I'm neutral.",
                    "label": 0
                },
                {
                    "sent": "You were not ask you.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Try to look at more complicated problems.",
                    "label": 0
                },
                {
                    "sent": "A lot of the datasets we shoot.",
                    "label": 0
                },
                {
                    "sent": "OK, I think Leon he is advocating that we did UCI datasets.",
                    "label": 0
                },
                {
                    "sent": "Ceiling effect and all this stuff.",
                    "label": 0
                },
                {
                    "sent": "So you're not going to see a benefit because it's everyone's doing as well as can be done.",
                    "label": 0
                },
                {
                    "sent": "Yes and no.",
                    "label": 0
                },
                {
                    "sent": "I mean, these these experiments have also been run on other datasets.",
                    "label": 0
                },
                {
                    "sent": "Fancy far has been running and some of the much larger ones.",
                    "label": 0
                },
                {
                    "sent": "These are actually synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "They also UC Irvine.",
                    "label": 0
                },
                {
                    "sent": "But you also think that they're not complex enough.",
                    "label": 0
                },
                {
                    "sent": "If you are you looking for something large scale that is hard and.",
                    "label": 0
                },
                {
                    "sent": "If only you work somewhere that had access to lots of, I know, I know.",
                    "label": 0
                },
                {
                    "sent": "I know, I know this person is.",
                    "label": 0
                },
                {
                    "sent": "No, no Google does not give Arteta.",
                    "label": 0
                },
                {
                    "sent": "I am not not, maybe and you see would be coming out data.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to give our data because we saw there was some search logs that another company released that had a little bit of an unfortunate snafu so they weren't quite as anonymized as they should have been.",
                    "label": 0
                },
                {
                    "sent": "Yes, we should probably do it UC Irvine datasets, but I think on the other hand they serve as a good baseline.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But then I think this hasn't worked on any data set, no matter.",
                    "label": 0
                },
                {
                    "sent": "I mean the brightest data set.",
                    "label": 0
                },
                {
                    "sent": "Tested.",
                    "label": 0
                },
                {
                    "sent": "I enter.",
                    "label": 0
                },
                {
                    "sent": "I think it would be nice if we got some new and challenging datasets, but it's really, really hard to come up with them, and it's really, really hard to get the organizations to give them away.",
                    "label": 0
                },
                {
                    "sent": "Trust me, I'm coming from a company and I have worked really hard to try to get some of the datasets out there and it's just up against roadblocks, time and again.",
                    "label": 0
                },
                {
                    "sent": "If you can do it from your company, I only congratulate you on it.",
                    "label": 0
                },
                {
                    "sent": "Netflix stated yes.",
                    "label": 0
                },
                {
                    "sent": "No, no, no.",
                    "label": 0
                },
                {
                    "sent": "It was the other one.",
                    "label": 0
                },
                {
                    "sent": "It was it wasn't there.",
                    "label": 0
                },
                {
                    "sent": "AOL data set wait where we took about 24 hours and they had identified people in Pennsylvania and stuff like that because people they tend to search for themselves and their problems with their pets or whatever it was so.",
                    "label": 0
                },
                {
                    "sent": "Earth two related questions one.",
                    "label": 0
                },
                {
                    "sent": "What is the game of having the kernel function?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "As opposed to simply maybe training different as VM's for each of the kernels and then combining ESPN is out of the box with some narrating combination or something like that, and the 2nd is.",
                    "label": 0
                },
                {
                    "sent": "Have there been any attempts of?",
                    "label": 0
                },
                {
                    "sent": "Completely learning the kernel function from scratch instead of combining existing kernel function.",
                    "label": 0
                },
                {
                    "sent": "So to the first one.",
                    "label": 0
                },
                {
                    "sent": "Where does the advances you go back to boosting scenario right?",
                    "label": 0
                },
                {
                    "sent": "If you want to train the SVM's and then combining them.",
                    "label": 0
                },
                {
                    "sent": "And as we saw in some of the slides, this framework doesn't significantly significantly outperform existing methods, so no, so far there isn't anything to be gained and doing the kernel altogether has there been any attempt at learning there?",
                    "label": 0
                },
                {
                    "sent": "The kernel matrix directly?",
                    "label": 0
                },
                {
                    "sent": "Well, if I say that hasn't been, then it would be wrong because I'm sure that I mean the original problem.",
                    "label": 0
                },
                {
                    "sent": "Formulation is language processing.",
                    "label": 0
                },
                {
                    "sent": "It was actually he only had the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "I think we have to go in that direction because we're restricting ourselves too much if we if we define first, we have the functions and these are the functions we're going to be using.",
                    "label": 0
                },
                {
                    "sent": "We're not gaining too much or away if we actually try to learn the kernel matrix directly, it's going to be a much more complex optimization problem with much more free parameters.",
                    "label": 0
                },
                {
                    "sent": "Additionally, you're going to be in the transductive setting, I'm afraid, because how are you going to?",
                    "label": 0
                },
                {
                    "sent": "Classify and you point.",
                    "label": 0
                },
                {
                    "sent": "Then you don't have then the kernel for how your test point is going to be interacting with your training point.",
                    "label": 0
                },
                {
                    "sent": "So you're going to have to train a new problem for each time you have.",
                    "label": 0
                },
                {
                    "sent": "Testpoint but if you can something like I don't know.",
                    "label": 0
                },
                {
                    "sent": "Two points and keep your distance.",
                    "label": 0
                },
                {
                    "sent": "If you're training well that that that you can do, then you have the Colonel Major, but then you get back to the boosting scenario pretty much right.",
                    "label": 0
                },
                {
                    "sent": "You have your classifier, so your training and all network.",
                    "label": 0
                },
                {
                    "sent": "I mean now you can have several of these and you can combining them together.",
                    "label": 0
                },
                {
                    "sent": "It's not quite the same then as as learning the kernel right then you're learning in all network.",
                    "label": 0
                },
                {
                    "sent": "How can I help and I'm not surprised.",
                    "label": 0
                },
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "Work better.",
                    "label": 0
                },
                {
                    "sent": "Whatever.",
                    "label": 0
                },
                {
                    "sent": "Together I think what he was trying to say is that you have those techniques where you train is still sign in alright, so you trying to normalize their identical and we trained them to produce identical outputs.",
                    "label": 0
                },
                {
                    "sent": "If the two inputs are similar semantically similar different ethnicity.",
                    "label": 0
                },
                {
                    "sent": "And so you can use it to train people to run, and then you can use this as a means of.",
                    "label": 0
                },
                {
                    "sent": "Features much better ways to do this.",
                    "label": 0
                },
                {
                    "sent": "If you want then she put come to the workshop.",
                    "label": 0
                },
                {
                    "sent": "Feel free to do it.",
                    "label": 0
                },
                {
                    "sent": "The whole idea that with to do something different from neural networks, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think it's nice that we have we have complementing methods right and and but you're right.",
                    "label": 0
                },
                {
                    "sent": "I mean by now, if we're getting to the point where we're just trying everything and we end up with nonconvex optimization problems and we are willing to do that, we could as well do neural networks.",
                    "label": 0
                },
                {
                    "sent": "What there is a nice thing about the kernel methods in the in the framework of language and everybody else?",
                    "label": 0
                },
                {
                    "sent": "With it we had a convex optimization problem, so we kind of knew what rate during.",
                    "label": 0
                },
                {
                    "sent": "Unfortunate thing is that so far it hasn't proven to be significantly better.",
                    "label": 0
                },
                {
                    "sent": "I'm not looking for network Crusader, it's just that you can use whatever architecture in the 19 by created the sentence.",
                    "label": 0
                },
                {
                    "sent": "Or not.",
                    "label": 0
                },
                {
                    "sent": "And so, in particular, into the freedom to use architecture we can put a lot of prior knowledge about the task.",
                    "label": 0
                },
                {
                    "sent": "So which is so?",
                    "label": 0
                },
                {
                    "sent": "In the sense of visibility.",
                    "label": 0
                },
                {
                    "sent": "Would you consider?",
                    "label": 0
                },
                {
                    "sent": "So then it opens up to also a lot of local minimums, and you don't necessarily know where you're ending up.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "So here we have it.",
                    "label": 0
                },
                {
                    "sent": "That isn't.",
                    "label": 0
                },
                {
                    "sent": "If you do what you say and you see I won't get any better.",
                    "label": 0
                },
                {
                    "sent": "Three of these guys are all in the same place at AT&T at one point, so it's, Oh yeah, Oh yeah, and I mean, it's all.",
                    "label": 0
                },
                {
                    "sent": "It's all in friendly terms so it will be will be good friends also afterwards.",
                    "label": 0
                },
                {
                    "sent": "Don't worry about that.",
                    "label": 0
                },
                {
                    "sent": "To me that the wonderful thing about kernel methods is that you know what you're doing, and but I.",
                    "label": 0
                },
                {
                    "sent": "Help us after all these papers written after like eight years after learning has been introducing you.",
                    "label": 0
                },
                {
                    "sent": "Finally have console said that someone doesn't work.",
                    "label": 0
                },
                {
                    "sent": "What do you think the community can learn from this?",
                    "label": 0
                },
                {
                    "sent": "Um, well, I wish I could say that that it could learn to be more selective about it's it's it's choice of problems.",
                    "label": 0
                },
                {
                    "sent": "I I you know what I actually think that I I congratulate this community for being so, stopping and believing in its idea is right, because it is an intuitive, nice thing to think that we can learn to kernels and we shouldn't just toss idea because it doesn't work at first attempt, right?",
                    "label": 0
                },
                {
                    "sent": "So stop earnest sometimes pace off, right?",
                    "label": 0
                },
                {
                    "sent": "I actually, I want to encourage people not just through through the ideas out when it doesn't seem to give any performance improvement.",
                    "label": 0
                },
                {
                    "sent": "At the first attempt, and I think it's great also that how, how, how, how, people have managed to come up with other redeeming properties of this.",
                    "label": 0
                },
                {
                    "sent": "So it has some good qualities, right?",
                    "label": 0
                },
                {
                    "sent": "I hate to see good ideas being thrown out and on the other hand, I also hate to see tons of papers being submitted to conferences, right that that only advances the field a tiny bit.",
                    "label": 0
                },
                {
                    "sent": "Then you know we all sit and review them and all that.",
                    "label": 0
                },
                {
                    "sent": "I think still I want to encourage people to keep trying even if it fails.",
                    "label": 0
                },
                {
                    "sent": "Linguist.",
                    "label": 0
                },
                {
                    "sent": "Or risk being station and I was wondering with this girl anything who we really?",
                    "label": 0
                },
                {
                    "sent": "I would really the setting up of SRM where we have like you know, embedded.",
                    "label": 0
                },
                {
                    "sent": "Class of complexity and then try to find an optimum somewhere in terms of generalization error, are we really in this in this setting here with a different approach or were.",
                    "label": 0
                },
                {
                    "sent": "I think we are, I mean because we have we are minimizing abound on the generalization error, right?",
                    "label": 0
                },
                {
                    "sent": "And we have parameterized the kernels one way or the other.",
                    "label": 0
                },
                {
                    "sent": "So the question is, if you with this this.",
                    "label": 0
                },
                {
                    "sent": "My point is that you should somehow maybe search for different along different capacity.",
                    "label": 0
                },
                {
                    "sent": "When you do you do right.",
                    "label": 0
                },
                {
                    "sent": "You always optimize.",
                    "label": 0
                },
                {
                    "sent": "You have a Lambda right?",
                    "label": 0
                },
                {
                    "sent": "The capital lamp that is found in the trace of your kernels, right?",
                    "label": 0
                },
                {
                    "sent": "And as you you try different lamp is you're not stuck in one right?",
                    "label": 0
                },
                {
                    "sent": "So that way you March through the various with the capacity classes.",
                    "label": 0
                },
                {
                    "sent": "I actually believe that we are.",
                    "label": 0
                },
                {
                    "sent": "How would you ask one to do it?",
                    "label": 0
                },
                {
                    "sent": "And then you know, like like you grew from one plus the other one, which is, which is just Boulder Red, but into the previous one where you do and the trace right?",
                    "label": 0
                },
                {
                    "sent": "You have one trace.",
                    "label": 0
                },
                {
                    "sent": "Then you have a bigger trace on.",
                    "label": 0
                },
                {
                    "sent": "Your biggest reason, your big trace of the matrices.",
                    "label": 0
                },
                {
                    "sent": "The bigger the tracing work, the more freedom you have in how finely you want to follow this the features, so I actually.",
                    "label": 0
                },
                {
                    "sent": "Believe it.",
                    "label": 0
                },
                {
                    "sent": "Any question in Gaussian process regression that comment on my state of time.",
                    "label": 0
                },
                {
                    "sent": "It is and then they do something.",
                    "label": 0
                },
                {
                    "sent": "So it's not just because it's.",
                    "label": 0
                },
                {
                    "sent": "What's the?",
                    "label": 0
                },
                {
                    "sent": "It's a different framework.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't ask her why they did manage to get the performance improvement since I haven't tried to carry out any of the results.",
                    "label": 0
                },
                {
                    "sent": "May or the experiments myself, so I'm reluctant to to place judgment of why they might be working and may not be working.",
                    "label": 0
                },
                {
                    "sent": "But is it?",
                    "label": 0
                },
                {
                    "sent": "Is it the same things that would correspond to anything much more than doing cross validation?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I don't know for sure why they get it to work.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't if there's anybody who wants to comment on it.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to let other people comment on why they think that may work.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Would you agree if I say that basically pattern recognition and regression Arvin Corner there is something between maybe?",
                    "label": 0
                },
                {
                    "sent": "OK, I think I think I started a can of worms with this icy American just fold problem.",
                    "label": 0
                },
                {
                    "sent": "Commission regulation so so, so classification and regression.",
                    "label": 0
                },
                {
                    "sent": "There's not much more to be gained from it.",
                    "label": 0
                },
                {
                    "sent": "I think you're worried that one of the big advantages are there areas where we really need to get better at is more in the semi supervised learning setting, right?",
                    "label": 0
                },
                {
                    "sent": "Because we at least coming from Google.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of data right and we have very very little labeled data.",
                    "label": 0
                },
                {
                    "sent": "Is labeling is expensive and I think that the pure classification that pure regression problems.",
                    "label": 0
                },
                {
                    "sent": "They're not.",
                    "label": 0
                },
                {
                    "sent": "Maybe that interesting to us anymore, but better ways of making use of all the unlabeled data would be wonderful.",
                    "label": 0
                },
                {
                    "sent": "We also these things that they are all very beautiful, right?",
                    "label": 0
                },
                {
                    "sent": "But now if I'm beginning to speak like a Google Representative, we're talking about taking advantage of very fine signal in the data, right?",
                    "label": 0
                },
                {
                    "sent": "And that kind of requires also that we have fairly clean data in order to see any performance improvement from the 18 to the 16%, right?",
                    "label": 0
                },
                {
                    "sent": "Or something like that?",
                    "label": 0
                },
                {
                    "sent": "And that is in general not clean.",
                    "label": 0
                },
                {
                    "sent": "Data is not what we see in real life.",
                    "label": 0
                },
                {
                    "sent": "It's all one big mess and.",
                    "label": 0
                },
                {
                    "sent": "I think we need much more in terms of robust classifiers, if anything, if we want to stay in classification and regression, we should do something that is robust towards having.",
                    "label": 0
                },
                {
                    "sent": "Almost 50% noise because that's the scenario we typically live in.",
                    "label": 0
                },
                {
                    "sent": "So if you can come, I would encourage you still maybe to do something in classification and regression.",
                    "label": 0
                },
                {
                    "sent": "If you can do something at that noise level that is efficient and works for large scale, but otherwise try to grow into anything that can use all the unlabeled data in order to gain performance improvements.",
                    "label": 0
                },
                {
                    "sent": "Yup.",
                    "label": 0
                },
                {
                    "sent": "I would.",
                    "label": 0
                },
                {
                    "sent": "To counter that, there is nothing much.",
                    "label": 0
                },
                {
                    "sent": "And I think that we have a my view that they presented yesterday is that.",
                    "label": 0
                },
                {
                    "sent": "We are using context minimization.",
                    "label": 0
                },
                {
                    "sent": "Methodology is because that's what we believe can be done.",
                    "label": 0
                },
                {
                    "sent": "But yesterday I showed that you know there are methods to do nonconvex optimization, and classification is a non convex problem inherent in.",
                    "label": 0
                },
                {
                    "sent": "So I think that there is a big area there that even before going into active learning, semi supervised and so on that we don't really know.",
                    "label": 0
                },
                {
                    "sent": "You don't really know.",
                    "label": 0
                },
                {
                    "sent": "So there's a big gap between the bounds were always you say I give up on those examples, so there's a term that how many examples have a margin below and the actual algorithm that has a soft margin.",
                    "label": 0
                },
                {
                    "sent": "There's just two different criteria we know how to minimize self margin.",
                    "label": 0
                },
                {
                    "sent": "We don't really know how to minimize the bound, so there's I think there's actually a very big open problem.",
                    "label": 0
                },
                {
                    "sent": "Search for long time.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I think that I think that you cannot.",
                    "label": 0
                },
                {
                    "sent": "I think that the problem of almost separable datasets has been solved.",
                    "label": 0
                },
                {
                    "sent": "The problem of datasets in which the base error is maybe 50% and you're trying to get from 60% to 55%.",
                    "label": 0
                },
                {
                    "sent": "That is far from being so OK. Sure, go ahead.",
                    "label": 0
                },
                {
                    "sent": "Maybe one of the other culprits is the kernels that we're using to begin with it.",
                    "label": 0
                },
                {
                    "sent": "Look at the base space of all possible kernels and then we consider the kernels that we are using.",
                    "label": 0
                },
                {
                    "sent": "This sort of pointing very limited directions in that subspace, and on top of that, so you take your RBS and then you said you set Sigma.",
                    "label": 0
                },
                {
                    "sent": "All you're doing is perturbing a little bit the kernel in that area of the big space of all possible kernels.",
                    "label": 0
                },
                {
                    "sent": "Maybe if we had rules we had to set several hyperparameters, then we be covering a bigger chunk of the.",
                    "label": 0
                },
                {
                    "sent": "Overall space, I guess.",
                    "label": 0
                },
                {
                    "sent": "Well they are.",
                    "label": 0
                },
                {
                    "sent": "They were just combining whole bunch of different varieties of apples instead of providing apples with oranges with potatoes, there is something to be said for that, but I actually think that some of the algorithms they do care I mean, especially when you get to these later things, raise a non linear combinations where they're trying to find a new Sigma all the time, right?",
                    "label": 0
                },
                {
                    "sent": "Then I think we're getting to the infinite kernel learning, right?",
                    "label": 0
                },
                {
                    "sent": "And it's not just doing apples and apples, but there is a tendency to apples and apples.",
                    "label": 0
                },
                {
                    "sent": "But still there.",
                    "label": 0
                },
                {
                    "sent": "I don't see the performance improvement.",
                    "label": 0
                },
                {
                    "sent": "Maybe there is an even better where you are doing, but then you also do end up in the non convex optimization problems where you don't really know what you.",
                    "label": 0
                },
                {
                    "sent": "You may know what you're doing.",
                    "label": 0
                },
                {
                    "sent": "I shouldn't say you don't know what you're doing because that's a seems to be, but we believe in convex optimization problems that please.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Would be something.",
                    "label": 0
                },
                {
                    "sent": "I I think it's weather weather learning kernels would be useful for structure predictions.",
                    "label": 0
                },
                {
                    "sent": "Trade.",
                    "label": 0
                },
                {
                    "sent": "I mean with this material presented here, there are little hopes that it will be, but.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can prove the community wrong.",
                    "label": 0
                },
                {
                    "sent": "Learning kernels can be good for something.",
                    "label": 1
                },
                {
                    "sent": "Issues with.",
                    "label": 0
                },
                {
                    "sent": "Features we're using our real numbers.",
                    "label": 0
                },
                {
                    "sent": "Giving you any more information.",
                    "label": 0
                },
                {
                    "sent": "So in any real application, you don't actually have real numbers are like.",
                    "label": 0
                },
                {
                    "sent": "Proxy to the real features so that you define kernels on real objects.",
                    "label": 0
                },
                {
                    "sent": "Then you have some people actually increasing the amount of information by defining moral codes, and then starts making sense to look at, you know.",
                    "label": 0
                },
                {
                    "sent": "Increase the number of cars he actually increased the types of features.",
                    "label": 0
                },
                {
                    "sent": "Um, at some point of time you have to turn it into a real number.",
                    "label": 0
                },
                {
                    "sent": "Otherwise I don't know how to put it into the computer so.",
                    "label": 0
                },
                {
                    "sent": "Objects.",
                    "label": 0
                },
                {
                    "sent": "Objects which are not represented as vectors of real numbers.",
                    "label": 0
                },
                {
                    "sent": "Right, so ends up being a, I mean a Colonel in away is nothing further features selector, right?",
                    "label": 0
                },
                {
                    "sent": "You cast it into maybe infinite dimensional space where you still have a real number, so I'm not quite sure I understand the distinction you're making here.",
                    "label": 0
                },
                {
                    "sent": "Example.",
                    "label": 0
                },
                {
                    "sent": "They use that.",
                    "label": 0
                },
                {
                    "sent": "She's basically not.",
                    "label": 0
                },
                {
                    "sent": "Between 2 two sequences like the sequences are represented in some tree like structure.",
                    "label": 0
                },
                {
                    "sent": "It's some really large numbers.",
                    "label": 0
                },
                {
                    "sent": "So, but now the kernel on sequences we actually have, like a new feature space of dimension.",
                    "label": 0
                },
                {
                    "sent": "New structures in your data.",
                    "label": 0
                },
                {
                    "sent": "So what I'm saying?",
                    "label": 0
                },
                {
                    "sent": "On size dimensions are 10 is not interesting.",
                    "label": 0
                },
                {
                    "sent": "This is why we're not getting anything.",
                    "label": 0
                },
                {
                    "sent": "But if you start looking at real real objects then there's some both of gaming.",
                    "label": 0
                },
                {
                    "sent": "Probably just.",
                    "label": 0
                },
                {
                    "sent": "Comparing what you use.",
                    "label": 0
                },
                {
                    "sent": "But are you then arguing for having just a higher dimensional feature space or so?",
                    "label": 0
                },
                {
                    "sent": "UC Irvine would be OK if we just had some higher dimensions.",
                    "label": 0
                },
                {
                    "sent": "In some sense yes.",
                    "label": 0
                },
                {
                    "sent": "But I mean you couldn't really have.",
                    "label": 0
                },
                {
                    "sent": "You couldn't represent all the dimensionality of two sequences.",
                    "label": 0
                },
                {
                    "sent": "Wait?",
                    "label": 0
                },
                {
                    "sent": "Ray, right?",
                    "label": 0
                },
                {
                    "sent": "So where do we end up with the question being if it would work if we just had not real numbers?",
                    "label": 0
                },
                {
                    "sent": "So then the signal of Gaussian is somehow.",
                    "label": 0
                },
                {
                    "sent": "We have to learn the structure of the kernel, so I guess that goes back to that.",
                    "label": 0
                },
                {
                    "sent": "If we are doing it too simplistically, we should.",
                    "label": 0
                },
                {
                    "sent": "We should really try to learn the structure of the Gaussian and or the structure of the kernel instead of.",
                    "label": 0
                },
                {
                    "sent": "Of just saying we have these predefined families of kernels that we're combining together.",
                    "label": 0
                },
                {
                    "sent": "Computationally, it gets you a little harder problem.",
                    "label": 0
                },
                {
                    "sent": "I encourage you though to try it out.",
                    "label": 0
                },
                {
                    "sent": "We should have time for maybe two more questions.",
                    "label": 0
                },
                {
                    "sent": "This one over here.",
                    "label": 0
                },
                {
                    "sent": "Meeting I think we should give this man a hand.",
                    "label": 0
                },
                {
                    "sent": "Better datasets, so we're all.",
                    "label": 0
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "And then that's fine.",
                    "label": 0
                },
                {
                    "sent": "Super impressive.",
                    "label": 0
                },
                {
                    "sent": "Without this is like Colonel best.",
                    "label": 0
                },
                {
                    "sent": "Our actions are given a teacher.",
                    "label": 0
                },
                {
                    "sent": "Image of his right to have many different feature classes at 16 years property currently.",
                    "label": 0
                },
                {
                    "sent": "While Peter.",
                    "label": 0
                },
                {
                    "sent": "You already.",
                    "label": 0
                },
                {
                    "sent": "So if you combine.",
                    "label": 0
                },
                {
                    "sent": "Very high school.",
                    "label": 0
                },
                {
                    "sent": "So there are papers also with the image recognition, and I don't see.",
                    "label": 0
                },
                {
                    "sent": "Also there a consistent performance improvement.",
                    "label": 0
                },
                {
                    "sent": "I must admit.",
                    "label": 0
                },
                {
                    "sent": "And if you take some of these computational biology papers, they have very.",
                    "label": 0
                },
                {
                    "sent": "Is that from the bottom at the moment?",
                    "label": 0
                },
                {
                    "sent": "They envision problems there.",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "Fire.",
                    "label": 0
                },
                {
                    "sent": "Yeah, they don't perform.",
                    "label": 0
                },
                {
                    "sent": "They don't improve performance, not accuracy.",
                    "label": 0
                },
                {
                    "sent": "Not one bit.",
                    "label": 0
                },
                {
                    "sent": "As far as I know it, because I mean they they they don't and they also started out from a problem that was 98% positive and 2% negative, so they're already at 9899% no matter what they do and they stay at 99%.",
                    "label": 0
                },
                {
                    "sent": "So that's where they say let's try other criteria and see if it gains anything there.",
                    "label": 0
                },
                {
                    "sent": "'cause they don't gain anything when it comes to accuracy.",
                    "label": 0
                },
                {
                    "sent": "So I've seen one vision data set where you get a 15% improvement in performance by learning the kernel.",
                    "label": 0
                },
                {
                    "sent": "Over uniform weight.",
                    "label": 0
                },
                {
                    "sent": "So that's not a consistent across the board performance on multiple data, right?",
                    "label": 0
                },
                {
                    "sent": "And you of course, we can see examples, but consistently we don't see the performance improvement over the baseline of this uniform combining it.",
                    "label": 0
                },
                {
                    "sent": "So I think.",
                    "label": 0
                },
                {
                    "sent": "Coffee break in a moment.",
                    "label": 0
                },
                {
                    "sent": "Before we do that, we need to have some room change announcements.",
                    "label": 0
                },
                {
                    "sent": "But before we do that, let's please thank our speaker.",
                    "label": 0
                }
            ]
        }
    }
}