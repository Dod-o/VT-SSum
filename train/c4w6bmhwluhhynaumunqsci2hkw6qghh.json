{
    "id": "c4w6bmhwluhhynaumunqsci2hkw6qghh",
    "title": "Learning Representations: A Challenge for Learning Theory",
    "info": {
        "author": [
            "Yann LeCun, Computer Science Department, New York University (NYU)"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_lecun_theory/",
    "segmentation": [
        [
            "Supposed to be here.",
            "I would like to thank the organizers for inviting me, particularly since I'm not known for sort of being kind of very close to the competition learning theory community.",
            "Although I had personal connections to it and a lot of the stuff I'm going to talk about today is things where learning theory is kind of difficult to apply to, and I'd like to sort of present this as a challenge for this community to study it.",
            "So I'm going to talk mostly about learning representations or deep learning as it's come to be known.",
            "You can think of deep learning as one particular way of learning representations, so we know that in the history of pattern recognition and machine learning, we've been doing a pretty good job at coming up with algorithms to learn classifiers and predictors of various kinds, always assuming that either the feature set or the kernel was given to us before hand, and.",
            "The kind of questions I've been interested in for a very long time, before, you know, before you know from the first day, I started working on machine learning, which was in the early 80s.",
            "Was learning representations 'cause I think it was.",
            "I thought from the start that it was kind of the secret towards sort of more powerful learning machines and so that's why I sort of never had a huge interest with things like kernel methods and stuff like that, because they just were not solving the problems I was interested in.",
            "It doesn't mean that I don't think they're useful, but I think my interest is just elsewhere.",
            "So here is where my interest is, it's."
        ],
        [
            "Running running representations and learning representations, I think is probably the next step for for AI machine learning as well As for things like neuroscience and cognitive science, North scientists have no idea how the brain learns representations, and I think machine learning can help them with that.",
            "So how do we learn representations of the world, say the visual perceptual world, and by merely looking at the world?",
            "May be acting on it as well.",
            "Neuro scientists ask themselves in similar question how does the visual cortex organized itself to perceive, you know, to turn itself into a visual cortex?",
            "And the sort of connected question or machine learning would be how do we learn feature hierarchies.",
            "The reason for hierarchy is is going to become clear in an interesting couple of minutes.",
            "So the deep learning problem really is the one of is one of learning representations of the world.",
            "By just looking at data.",
            "So traditional."
        ],
        [
            "Me.",
            "As we all know, pattern recognition systems have been built by either building handcrafted feature extractors followed by more or less generic classifiers.",
            "Although the more modern approach to things like object recognition or speech recognition actually has another stage in it in the middle, here the other will feature the fixed in the case of speech.",
            "There things like MCCS in the case of vision that things like Sift and Hog and the second level actually uses learning, usually it.",
            "It's based on some sort of unsupervised learning.",
            "Very simple one that K means or sparse coding to sort of turn the low level representations into sort of mid level representations you could call it this way and then it's followed by some sort of pooling and then the classifier supervised classifier.",
            "So one more stage fixed unsupervised learning, supervised training.",
            "That's kind of the mainstream approach to speech and image recognition until very recently.",
            "Um?",
            "But really, the."
        ],
        [
            "Model hasn't changed much since the early days of pattern recognition.",
            "When you look at the perceptron, that's pretty much what it is.",
            "You know, a feature extraction followed by some sort of classifier and.",
            "You know, maybe it's time after 50 years, 55 years or so to move away from that model.",
            "If we look at."
        ],
        [
            "How the brain performs vision.",
            "And I'm not a huge advocate advocate for, you know, getting inspired by biology but but it's sometimes useful to get intuition from it is much more hierarchical.",
            "You have kind of a for the sort of fast recognition process that people perform, and you know animals as well.",
            "There's sort of a feedforward path that we can trace called eventual pathway in the cortex visual cortex.",
            "That goes into several stages, each of which has, you know, very highly nonlinear operations.",
            "Built in and it goes through various areas of the brain called V1V2V4I key, each of which has kind of several layers of processing if you want so you might have to.",
            "The signal might have to go through maybe 10 synapses or something like this, or 10 neurons if you want to go to the area of the brain where object categories are encoded.",
            "So it's not just it's not shadow in the in the usual sense, it doesn't look very much like the kind of architecture that we've used so far.",
            "OK, so it's sometimes useful to get a little bit of inspiration from biology, but it's also very dangerous."
        ],
        [
            "Because we don't know what's important when we look at the piece of brain, we don't know what matters in it.",
            "And so here is an example of what not to do this.",
            "This guy called Klimawandel from the 19th century who built airplanes model after bats.",
            "And you know he didn't know much about aerodynamics.",
            "He was very good at building steam engines, but powerful in light steam engines, but not very good at.",
            "Like, you know, stability and things like this and so we build this airplane full size and things actually took off flew 50 meters at 1 meter.",
            "Altitude was completely uncontrollable, which is why you probably never heard of him.",
            "Every Frenchman, of course knows about him, but.",
            "But not on this side of the pond.",
            "This other point is the Wright brothers.",
            "This were 13 years before the Wright brothers.",
            "So you know he got he was too close to the biology.",
            "He really didn't know what was important about it and.",
            "He hadn't figured out completely, you know, the law of aerodynamics, instability, and we might ask the question, you know, in our case, there are people in the world who got lots of money from various governments who claim that they can reproduce the functions of the human brain by basically simulating a giant brain on the computer.",
            "And I think it's completely nuts.",
            "I think it's just never, you know it can't possibly work if we don't understand where the underlying principles are.",
            "So what are the underlying principles for intelligence?",
            "What is the equivalent aerodynamics?",
            "Namics for intelligence, right?",
            "That's what we might want to go after in terms of sort of conceptual goals."
        ],
        [
            "OK, so the architecture we might want to go after is something of this type where you know we feel the raw input and we have to go through a whole bunch of transformations, each of which is kind of a trainable machine.",
            "If you will, and.",
            "As as with side effect of this whole system, training itself to perform the task, it will kind of drive internal representations of the problem.",
            "So the first if you are theorist.",
            "At least some of theories I know first question I ask, is why do we need that many layers, why?"
        ],
        [
            "I can't, we just do this with just two layers, like say this OK have a kernel function, compare your input with all your training samples or subset of them computer linear combination of this and you're done now.",
            "So of course you know someone has to give you this kernel function which is just as complicated as coming up with the feature vector but but you know if you limit the complexity of this function here is pretty clear that there is a whole bunch of functions here that will require very very large number of terms in this sum to be able to do anything useful.",
            "Or if the number of terms is small, that means your kernel function here does all the work.",
            "And what is it coming from?",
            "Where is it coming from?",
            "So there's kind of a bit of a conundrum here, and you know, you would think that some functions that perceptual systems have to compute are inherently sequential.",
            "They require to have a certain number of sequential operations for them to be.",
            "How computable with sort of a relatively small amount of resources?",
            "There's a lot of complexity theorists in the room, and people who have studied circuit complexity.",
            "I'm sure who know that if you allow yourself to have a few layers of computations, you can gain an exponential factor on the amount of hardware you need to compute a particular function."
        ],
        [
            "For example, let's say let's take the example of, you know Andy parity.",
            "We know that we can do this with, you know, log in stages or you know, like over two stages with ex or Gates.",
            "But if we force ourselves to do it in two layers using the contract in normal form, we might need an exponential number of midterms.",
            "So you know it's just a very simple question of exchange of complexity between space and time.",
            "Essentially that we deal with all the time when we write programs.",
            "If we log ourselves multiple stages, then we might gain a huge factor on on the overall amount of resources we need so.",
            "It's very intuitive, you know I don't have any theorems to tell you to show you you guys other guys, other people to actually prove those theorems, but the basic idea is intuition of why we need multiple layers.",
            "Is just that this circuit complexity base."
        ],
        [
            "OK, so there's a bit of a theoretician nightmare here, which is that when we start having multiple layers in a system like saying a deep neural net and we tried to train it using, say, supervised running, we have loss functions that end up being non convex.",
            "And if you have non convex loss functions and you rely on things that you know like minimal.",
            "Communication and stuff like that.",
            "You know all those things go out the window because you can't prove anything about the optimality of your model.",
            "OK, but you know if they work, it's actually worth maybe spending the effort trying to figure out why they work so.",
            "So all bets are off for non convex losses, but then again you know every speech system ever deployed as use non convex optimization and it's not because they're non convex that they don't work.",
            "Arguably, to some of us, the only interesting learning actually is non convex because.",
            "If you have a convex optimization to solve for learning, then the order in which.",
            "You run things, doesn't matter.",
            "We know that humans.",
            "The order in which you learn things in humans matters.",
            "It's called pedagogy, right?",
            "I can start my talk with the last slide and then go backwards, but I don't think you'll get in the same state at the end.",
            "If I do this so.",
            "Even though the aggregated statistics is the same, so you know there's the order in which you see things actually matters and in which you learn things actually matters, and that suggests the fact that whatever loss function you know the human brain is minimizing if it actually minimizes the loss function, which is another question, it has to be nonconvex.",
            "Otherwise the order in which we learn things wouldn't matter.",
            "Um?",
            "OK, that doesn't mean that we can't actually."
        ],
        [
            "You know do AI without convex optimization, but.",
            "It is sort of an interesting observation perhaps.",
            "OK, the other theoretician nightmare is that you know if if we have all those things with deep learning algorithms, we don't have any good generalization bounds.",
            "Well, actually it's not true.",
            "You know you take a bigger role net.",
            "It actually has a bound VC bound type thing because it's visit dimension is finite.",
            "But know tight bounds.",
            "Now, that could be a problem, except that I don't actually know anybody in practice who uses bounds to do model selection.",
            "Um, I hate to say this in front of this crowd, but.",
            "But that's true.",
            "Does anyone know anyone know anyone who actually uses bounds in practice to the model selection as opposed to, say, cross validation?",
            "Raise your hands.",
            "OK you go, no hands.",
            "OK, I don't know but I know I know it's not true.",
            "OK.",
            "So you see, some of my best friends use bounds.",
            "So you know it's hard to prove anything about deep learning systems.",
            "But then again, if we only study models for which we can prove things, we wouldn't have speech, handwriting and object recognition systems today."
        ],
        [
            "OK. On the other hand, it's kind of a smorgasbord of stuff for theoreticians because there's so many things we don't know about all the methods that are used in deep learning, and I'll talk about them, some of them as I go.",
            "You know, during the talk so.",
            "You know, deep running is about representing high dimensional data or representing data in additional spaces, and there has to be interesting theoretical questions about this West of geometry of natural signals.",
            "So is there any equivalent to SQL learning theory for unsupervised learning?",
            "Our computational learning theory?",
            "For that matter?",
            "I mean I know there are efforts along those directions in this community.",
            "What are good criteria on which to base on supervised learning?",
            "Things like that questions that have been answered by this community for supervised learning, which I think would be worth thinking about four.",
            "Feature learning, which is sometimes unsupervised.",
            "Um?"
        ],
        [
            "OK. OK, so let me talk about the state of the state of affairs for deep learning today.",
            "It's been one of the hottest topics in speech recognition for about the last two years, the latest generation of commercial systems that are based on that do speech recognition are all based on deep learning.",
            "The ones from IBM, Google, Microsoft, all based on deep learning.",
            "The ones in the iPhone, which is actually indirectly produced by IBM through nuans.",
            "They all use deep neural Nets.",
            "And it's been a very, very fast transition from gas mixture models to deep neural Nets for acoustic modeling in the last two years.",
            "It's becoming the hottest topic in computer vision, mostly because of the last image net competition.",
            "I'll come back to that and it's becoming a bit of a interesting topic for natural language processing, although it's not there yet.",
            "And there's a huge interest from the applied math community into things like representing data in high dimensional spaces.",
            "People who used to do, how many.",
            "Can I use this when I do sparse coding and stuff like that?"
        ],
        [
            "OK, so.",
            "Here's another piece of history which kind of indicates in what direction things are going.",
            "If you look at the history of handwriting recognition, speech recognition, now computer vision, object recognition.",
            "Every time there's been an increase in the number of training samples that were available, people started using learning more and more for more and more of the system.",
            "So instead of just running the classifier, they would learn you know the billable features and other low level features in an integrated fashion, and every time a step like this that was previously built by hand have been replaced by.",
            "Hang out with them, especially performance has improved so we just saw this in speech recognition is just happening also in computer vision and you know it's been happening.",
            "It was happening before that happened before that in handwriting recognition back in the 90s.",
            "So there's sort of a direct, very clear sense of history as datasets go increase in size as our machines become more powerful, we use learning more and more and things become better as we replace more and more handcrafted modules in our system.",
            "It's a very clear path.",
            "And.",
            "So that's kind of a, you know interesting thing to say for people interested in learning more.",
            "If you rely more on learning and less on human engineering, things work better if you have more data."
        ],
        [
            "Nothing revolutionary there, but you know you have to kind of realize that that's going on.",
            "OK, so I'm going to talk about convolutional net because you know, everybody assumed that I'm going to talk about that, and also because they become very popular in recent years for practical applications and a number of companies have deployed image recognition application, some of the speech recognition applications based on convolutional Nets in the last year or so less you month in fact.",
            "So."
        ],
        [
            "Accomplishments are inspired by biology.",
            "To some extent, a little bit by the architecture.",
            "What we know about the architecture of the visual cortex has its roots in the car neutron model by pushing the 70s.",
            "Which itself at its root in the work of human results, classic work in our science from the 60s, where basically the first couple of stages of the visual system, the visual cortex are seen as filter banks.",
            "OK, so you have an input here or a bunch of planes of input which might represent say, color planes for example, and then have a bunch of planes here that extract features from the input using a convolution kernel.",
            "Essentially so you take a convolution kernel.",
            "Swipe it over the image you get, so it's like a linear filtering.",
            "You get the result of the application of this convolution to the input.",
            "For different filters you'll get different different outputs.",
            "Each of those is called a feature map.",
            "You pass through some nonlinearity, the latest, most popular nonlinearity is called value rectified linear unit.",
            "So it's basically half rectification positive, part very simple nonlinearity, and then there is a second operation cooling, which kind of aggregates the answers of the filters over a small.",
            "Region, sometimes over multiple features, and the resolution is reduced.",
            "That is, to build a little bit of."
        ],
        [
            "Shift in variance and distortion variance into the system."
        ],
        [
            "So this is sort of, you know, pretty old idea.",
            "The modern versions use what we call multiple stages, each of which has four layers, and so when that performs some sort of contrast normalization followed by this filter bank.",
            "I was just telling you about the nonlinear operation.",
            "This rectified linear units that thing about, and then this feature pooling and then you take this module, you stack multiple instances of it, stick your favorite classifier on top.",
            "Usually the whole thing is is trained with backpropagation.",
            "Just great Internet basically.",
            "So stochastic gradient descent.",
            "And use backdrop to compute the gradients of whatever loss function you want to minimize with respect to all the coefficients in all the filters in every layer."
        ],
        [
            "The pruning operation is just any kind of symmetric operation of Max and average square root of sum of square or something of that type."
        ],
        [
            "Um?",
            "So."
        ],
        [
            "We give you a."
        ],
        [
            "Show you an old animation of.",
            "Accomplishing that that was trained to recognize handwritten characters.",
            "The reason I'm showing this to you is.",
            "Because I would like to point the properties of feature extractors that I think is important.",
            "So this was trying entirely supervised to classify essentially end this digits.",
            "I think of it this way with a little bit of invariants to position.",
            "Now look at this particular pixel here.",
            "As the number 3 here goes up and down, it goes from white to black to white.",
            "I have a hard time keeping it fixed but OK, So what that means is that the line formed by all the translated versions of this three is not flat.",
            "It's got a curvature because one of the coordinate goes from one value to another value back to the first value.",
            "So it's going to have some curvature to it.",
            "Now the bad news is that.",
            "So if you imagine the set of all deformed threes, it's a surface whose intrinsic dimension is the number of possible.",
            "Deformations you can do locally.",
            "Many fold 4 digit 8.",
            "Is sort of entangled with the three is very highly nonlinear as well, but it's not.",
            "You know you separable from the one of the threes that most of the pixels are common, but the curvature of the two manifolds is very very very high, so you can't.",
            "You can't just do linear separation between them, you have to extract features.",
            "We know that.",
            "OK, so look at if you look at any value that appears.",
            "Here are the top layer is hardly any that goes from white to black to white or vice versa.",
            "They go from, you know, let's create a more Gray or vice versa, whatever that means.",
            "What that means is that the manifold of stuff of deformed translated patterns here is kind of is flatter.",
            "Um?",
            "Because of all the process that went during training of this.",
            "Of course it happened to be this way because the layer that came after this was a linear classifier and linear classifiers like to have you know space to separate from.",
            "So that's precisely why it happened.",
            "But it's a good property to have for a feature for a set of features that the manifold of variations you don't care about is flat."
        ],
        [
            "Perhaps there is some idea on how to derive a proper criterion for unsupervised training based on this sort of flatening manifold kind of idea."
        ],
        [
            "Let me skip this.",
            "Your choice of time.",
            "OK, so there's a whole number of tasks for which we know that deep convolutional Nets are the best method so far, for which we have the record.",
            "Handwriting recognition goes back a long time, OCR in natural images, traffic sign recognition, pedestrian detection, voluntary brain, image segmentation, human action recognition, and object recognition scene parsing, some pricing from death images, speech recognition, breast cancer, cell mitosis detection.",
            "This ones actually kill competition.",
            "There was one very recently.",
            "And.",
            "Of those were wrong by purely supervised convolutional Nets."
        ],
        [
            "OK, so we think like you know traffic sign recognition."
        ],
        [
            "That number is another application that we built."
        ],
        [
            "On the EG analysis this is 1 from."
        ],
        [
            "Investment songs group at MIT.",
            "Within Jane for segmenting brain tissue, where he applies a commercial net to a small volume of voxels and transit to classify the central voxel as being the boundary between two cells or not.",
            "This is a piece of brain tissue, and after the network is produced, labels for each of the things they can sort of reconstruct the.",
            "The brain circuit here is only showing a few percent of the neurons.",
            "There are very, very densely packed.",
            "So it's a very interesting application to connect to mix."
        ],
        [
            "But he was really what caused a big commotion in the last few months.",
            "Back in October or little earlier than this leski.",
            "Orange in Fenton won the image net 2012 competition, so this is one of the main competition in computer vision for object recognition and they wanted by huge margin they got something like 15% error rate by some measure where all other methods that competed in the same competition got around 2526% error.",
            "So it's really a big jump.",
            "It's not just you know small improvement, speed jump.",
            "Ann is essentially a big commercial net using.",
            "She was saying all the tricks I came up with in the last 20 years plus this dropout technique.",
            "But really, what made this secret or what made this successful is a very efficient implementation of this on GPU's, which allows which allowed them to train on very large data set or 1.3 million training samples in about a week.",
            "On the single GPU actually onto GPS, the filters are under the first layer and sort of somewhat interesting.",
            "The color and the black and white ones are separated for."
        ],
        [
            "This is kind of an artifact of the way that the system was designed.",
            "It works very well."
        ],
        [
            "Here is another example.",
            "This was kind of a similar system that was developed in why you would not by me by Rob Fergus and his student Matt Zeiler an so they have a conversation that is somewhat similar to electric jet skis, but a little different.",
            "Also trained on the on image net.",
            "Also using values using contrast normalization and the layer.",
            "These are the filters at the first layer that are trained.",
            "Also uses this dropout regularization which is essentially sort of very brutal.",
            "Regularization that consist in killing half of the units in the top layers and hoping that the network will recover from it and there's a different half that you kill at every sample.",
            "We think it's, you know.",
            "Murderous, but it seems to actually help a little bit.",
            "It uses SGD.",
            "Not a particularly sophisticated form of SGD."
        ],
        [
            "There is an online demo of this that you can play with the ratio that season where you can upload images and it will kind of tag it with all the categories."
        ],
        [
            "Mines.",
            "It works, you know, just as well as exquisite skis.",
            "A little better maybe, but I'm sure Alex is better versions now, but it's not going to talk about it not using Google.",
            "And."
        ],
        [
            "But here is an interesting point about this.",
            "This network, the features that it learns, are fairly generic.",
            "So if you if you take the network that is trained on Imagenet, chop off the task layer and then just retrain the last layer on a different data sets a Caltech 256, you basically get state of the art performance with only six training samples per category.",
            "I think that's amazing.",
            "So Cal tech.",
            "56 is not a particularly interesting set.",
            "Now is kind of out of date, but you know the state of the art is is here.",
            "Previous state of the art is here.",
            "You get to the state of the art with six training samples per category just using the features at the output of this commercial net right before the output layer.",
            "Just retraining the classic classification layer.",
            "What that means is that the features it's Lauren are really generic.",
            "You can use them for just about any object recognition tasks that you could imagine."
        ],
        [
            "Here is another example.",
            "This is with the Pascal BOC challenge, so here it's not as dramatic, but you know the accuracy is pretty much set of the article below.",
            "Mostly because of the nature of the labeling of the Pascal VLC data set.",
            "But it was kind of impressive because only the last linear classifier basically is is trained here on on this data set.",
            "The feature is just going from image net.",
            "Here's another."
        ],
        [
            "Example, there was only my tab by Kimmel Farbe and chemically this is for labeling images.",
            "This is called the semantic labeling problem or or seen parsing or these various names for it.",
            "And the problem here is to label every pixel in an image with the category of the object that it belongs to.",
            "OK, so you know Sky, grass and trees and Bubba.",
            "So it's a little more complicated than than object recognition, because you can have to figure out where every pixel what every pixel belongs to."
        ],
        [
            "Is used a convolutional net, but a multiscale kind of convolutional net.",
            "Where you take the input, you'll be able to pyramid out of it where you have kind of lower resolution versions.",
            "Of those of this image, and then you apply the same convolutional net.",
            "So let's say you want to classify the central voxel.",
            "Here the central pixel here you apply convolutional net who's window sort of context window that is going to influence.",
            "In particular output is say 46 by 46 pixel.",
            "If you apply the same conversation led to this image it will also get influenced by 40 by 46 window, but now there.",
            "Since the image resolution is half is going to take into account a complex that's twice as big, same here.",
            "So this guy is basically going to see the entire image at quarter resolution and you combine the features that are produced by the three copies of this commercial net with the same.",
            "They all have the same filters, concatenate them, feed them to a classifier you trying this whole thing, supervised on a couple thousand labeled images that have been labeled at the pixel level.",
            "And every decision for a particular pixel is going to be taking into account this 47 six window, and this was this was about 46 window, which really is 9192 here and this will be for this window which is really 180, four 24 here."
        ],
        [
            "So we can use this huge context to make a decision.",
            "And this pretty much works at state of the art, so the best results on this data set on the pixel pixel accuracy basis is 82% or assistant gets 81.4 instead of 81.9.",
            "We also have a simple system that is slightly lower win percent, but it's about 100 times faster because it doesn't use any kind of complex graphical models.",
            "Post processing and uses a very, very simple postprocessing after the convolutional net.",
            "This is on the Stanford background data set that has eight categories.",
            "This is the sea flow data set, or I should have mentioned.",
            "There are two measures of performance.",
            "This is pixel accuracy and this is pixel accuracy.",
            "Where and their accounts for more if it concerns a category that's rare so that you pay more for missing a human, even if it's only a few pixels in a large image.",
            "So it's you know it's more accurate measure on this one.",
            "We actually beat beat everyone."
        ],
        [
            "This is on the floor data set with 30 three categories.",
            "Then again, pretty much have the record here.",
            "Depending on how we train it, we can beat the record either on pixel accuracy or on class normalized pixel accuracy and this is a data set with 170 categories where nobody does well.",
            "We do just less badly than you know.",
            "This is Bennett's ethics group at.",
            "You know, several annoying."
        ],
        [
            "Urbana Champaign"
        ],
        [
            "So we got some examples of results.",
            "This works really well."
        ],
        [
            "Well, this is the 30 three categories of trees and buildings on windows and roads."
        ],
        [
            "And everything Sky San."
        ],
        [
            "And."
        ],
        [
            "Annual see example of a.",
            "The video here.",
            "So each frame here is processed independently.",
            "There's no sort of temporal consistency here.",
            "And it makes some stupid mistakes like you know when?",
            "When the when the ground is bright it's classified as sand.",
            "This is actually Greenwich Village, so I can tell you there's no sand.",
            "There's no beach.",
            "So this is someone riding a bike with a panoramic camera have been.",
            "Pictures have been stitched together.",
            "But it gets most of the.",
            "Important objects like humans, I mean people and.",
            "You know trees and stuff like that.",
            "Module lighting problems.",
            "How much speed of the Debbie Aga give me over CPU?",
            "So a CPU.",
            "So on the sort of beefy Mac laptop this runs at about 2 frames per second.",
            "The GPA, if we run entirely on the FDA, would run at about 20 frames per second, except that the communication with the FDA board is actually slow.",
            "So the system time is actually slower.",
            "Here's another.",
            "Another example, this uses temporal consistency.",
            "It's kind of similar video has fewer categories, but it's the same video, and there's a lot less kind of jumping around of categories here because of this consistent."
        ],
        [
            "Overtime, it's just the post processing.",
            "There's no change to the commercial net."
        ],
        [
            "Can you provide this also to images that have depth information within so connected with the Connect indoor data?"
        ],
        [
            "Again, this has the best results that we know of.",
            "And this is without temporal consistency and with temporal consistency.",
            "OK, so enough for super."
        ],
        [
            "Stuff, let's talk about unsupervised learning.",
            "So how do we devise unsupervised learning algorithms that will allow us to pre train the layers of Silicon net or any kind of deep learning system?",
            "Hierarchical feature extractor?",
            "Enable data 'cause of course we have a lot more enable data then we have labeled data.",
            "Now it used to be that all of deep learning all over deep learning was about was this and there was kind of a minor side interest in things like convolutional Nets which seem to work just fine if you just train them supervised.",
            "As long as you have enough data and because of the.",
            "Important because of the fact that we now have large datasets, large labeled datasets, this sort of slightly less interest in this kind of unsupervised running things because the supervised stuff works so well.",
            "So even people like Jeff Hinton, who kind of were really, really interested in this unsupervised running stuff.",
            "Essentially for practical purposes, just doing supervised convolutional Nets.",
            "Now you actually called them dread Nets, which stands for deep rectified.",
            "Uh.",
            "Networks we drop out DERDDRED right?"
        ],
        [
            "OK, so how do we do unsupervised learning?",
            "So you know, there's this hypothesis that the manifold of natural data if you take patches from natural images, the set of possible patches you will observe is actually a low dimensional subspace of all possible combinations of pixels.",
            "You could come up with, right?",
            "So there is this, you know, picture of sort of the manifold assumption and some people don't agree with this.",
            "People like if I say that's kind of wrong picture to think about.",
            "I think I agree with him, but it's still useful.",
            "So."
        ],
        [
            "What we'd like to do is, you know, turn ideally what an ideal feature extractor should do is take a bunch of samples data points.",
            "And tell you if there is a manifold of data.",
            "Tell you where you are on the manifold with a number of components and then have a separate set of components that tell you the distance to the manifold and all the other dimensions in the ambient space.",
            "OK, So what you really want is you want to kind of factor two sets of variables.",
            "One is where you want to manifold and the other one is where you are away from the manifold.",
            "OK, so if you had for example, the set of manifold of possible faces, all human faces, it's Admiral manifold.",
            "For a particular person is bounded by the number of muscles in your face and the number of degrees of freedom that you can move around, which is 6, and then if you include everybody then there is some sort of you know number of dimensions basically bounded by the genome of you know how many different faces you can have.",
            "Module accidents.",
            "Um?",
            "And so you know, if we had this manifold, then if we had this kind of feature separation, you could use.",
            "The first part to tell who you're looking at and what expression they're making, and you can use the second half.",
            "The part that tells you you know how far away you are from that manifold to tell you if it's a face or not.",
            "Or if it's something else.",
            "So."
        ],
        [
            "So distant angling the explanatory factors of variation is really what unsurprised running should be about.",
            "There is a general idea which is sort of emerging a little bit.",
            "There is no kind of concrete or theoretical justification for it, but it's just what people end up doing in the end, which is a feature extractor should really be composed of two steps in nonlinear step that basically embed the input into a very high dimensional space or high dimensional space?",
            "OK, this is similar to the kind of kernel kind of thing, right to embed things into high dimensional space so that things are more easily.",
            "Horrible in that space.",
            "Same stuff except you know, know based on the kernel trick based on other things, it has to be nonlinear because of his linear.",
            "It doesn't do anything useful for you, so I can know up.",
            "OK, so embed your input into high dimensional space in some sort of nonlinear way.",
            "But then what you do here is that you're breaking the space apart.",
            "There are things that are semantically similar or even identical.",
            "They will end up in different bins.",
            "In this representation will be very far apart, so the second step, which is sort of a generalized pulling similar to the polling we doing convolutional net.",
            "Is to regroup the things here that are supposed to be similar.",
            "OK, so whenever.",
            "You get 2 vectors here that represent two faces, and they happen to be very different.",
            "You somehow encode them in such a way that here they end up being similar.",
            "I did this dinner.",
            "It may or may not be linear.",
            "I'm sorry I didn't pick up, OK.",
            "Probably not, no.",
            "What people do in practice is something like.",
            "Like L2, so square root of sum of squares of some components, subset of components.",
            "Is someone picking some landmarks and just representing every object is the vector distances from those networks?",
            "Yeah, right.",
            "OK, so here's an example for very concrete example for the sort of intro to computer vision until convolutional Nets, right?",
            "So the data does ethnic PRB mechanism for example OK?",
            "Well for mid level features what you do here is K means.",
            "So you take so see vector, you run it through again means algorithm and the K means is going to give you a binary vector.",
            "With all zeros but an 11 at the location of the prototype that's closest to the input.",
            "OK, so it's just a winner.",
            "Take all kind of encoding if you want a word right.",
            "The visual word.",
            "The pulling here takes all of those feature vectors for the entire image and just average them.",
            "Or you know, combine them in some way.",
            "So now what you get is a histogram of words and you can feed that to your favorite classifier and the goal of this is to basically give you shift invariant, so once you do this aggregation, the position of a feature doesn't matter anymore.",
            "It's just the presence of it matters.",
            "OK, so you regroup things that were dissimilar.",
            "Now it turns out when you do something like sparse coding here, it works much better than if you do K means, because pass coding preserves a bit of similarity between things, right?",
            "Which means just completely.",
            "Explode"
        ],
        [
            "OK, so we have this manifold picture.",
            "We have a bunch of data points and what we'd like is to kind of learn a function that gives us the dependencies between X1 and X2.",
            "Here in this case.",
            "We're going to do is we're going to the sort of general framework.",
            "I think that is the most appropriate for this is the sort of energy based framework which, consistent essentially learning an energy function, or think of it as a contrast function that tells you if.",
            "I give you a point and the contrast function gives you a scalar that tells you whether you are on the manifold or not on the manifold.",
            "An if the function is.",
            "Is able to do this internally.",
            "There has to be able to kind of do this.",
            "Disentangled features right it has to be able to tell in which direction the manifold is.",
            "You know what's the closest point on the manifold and where am I going on the manifold and things like this, right?",
            "So that's what we're going to do.",
            "We're going to train the system to produce a single scalar and the scale is going to be some sort of contrast function that tells you how far away we are from the manifold of data."
        ],
        [
            "So let's say we take samples coming from this parallel here and run PCA PCA with only one dimension.",
            "So it's a 2 two input problem to output problem and we do PCA with one principal component.",
            "PCA will find a main axis here over the point Cloud, and the reconstruction error would be 0 for anything that's on the principal axis, and we grow quadratically as we move away from it.",
            "If you spot coding.",
            "It will sort of wrap up the the set of points into a kind of union of planes.",
            "If you want a union of.",
            "Lines.",
            "So that the sum of the distance to all the lines and ends up being the function we were looking for.",
            "K means we'll just put a whole bunch of prototypes all around the surface.",
            "This seems perfect, but it doesn't work in high dimension of course.",
            "OK, so this."
        ],
        [
            "Straight on this idea of sparse coding, so sparse coding is uses this energy function of this type, which has two arguments.",
            "One is the input and the other one is a code vector, which is really a latent variable.",
            "They were going to infer, and the energy function is the square reconstruction error where we multiply the code vector by some matrix called the dictionary matrix, and then we add a regularization term which is the sum of the absolute values of the components of the code.",
            "If we know the dictionary matrix, given a why we find the Z here that minimizes this.",
            "The energy function and that gives us our feature vector is going to be high dimensional, sparse, OK.",
            "So it has it's a nonlinear mapping, the one that Maps Y to the optimal Z is nonlinear mapping.",
            "But it's slower to compute because you have to do this augment operation.",
            "We can run the dictionary matrix.",
            "Of course.",
            "We've also Ninfield told us how to do this very long time ago using just edged basically just stochastic gradient descent to minimize the overall average energy of a collection of samples.",
            "Training samples.",
            "The columns of W have to be normalized for this to work.",
            "And the manifold assumption that's hidden behind the sparse coding is basically that the data is fits sort of a union of flow dimensional planes where the dimension of each plane corresponds to the number of components in Z that are non zero.",
            "Once we do this minimization."
        ],
        [
            "So in general, though, we have several ways of designing unsupervised learning algorithms.",
            "One is number one is to construct the energy function so that the volume of stuff that takes low energy is fixed.",
            "'cause the problem we have is that we have to build this energy function so that it has low energy on the points on the data points on the manifold, but higher energy everywhere else and it's easy enough to treat the parameters of an energy function to take low values on the points.",
            "OK, just do a quick descent.",
            "But then how do you make sure the energy is high everywhere else?",
            "And that's basically it's basically the the partition function problem.",
            "You know the people have been hitting in graphical models and stuff like that is very similar in spirit, so you want to contract when method one strategy is constructed energy so that the volume of low energy stuff is fixed so that if you give low energy to some points the other points will automatically have energy.",
            "PCA is one of those, K means is one of those.",
            "The second strategy is to push down on the energy of the samples and push up on the energy of everything else.",
            "This is a strategy used by contrastive divergent.",
            "Which is used to try and restricted Boltzmann machines.",
            "This is also a strategy used by maximum likelihood when you have a log partition function as a contrastive term that pushes up on the energy of other stuff, but it's very expensive to do if your partition function is not tractable, doesn't have a tractable gradient.",
            "So you choose Monte Carlo methods and stuff, and the third one is to use a regularizer to limit the volume of stuff that can take low energy.",
            "So basically build your model in such a way that it has a regularizer and by minimizing this regularizer is sort of shrink wraps the stuff of low energy into sort of a small value if you want, and that's basically what sparse coding does.",
            "The sparse the sparsity term limits the volume of stuff that is allowed to take low energy.",
            "That's essentially what it does."
        ],
        [
            "OK, so this fast coding method is what we call the decoder.",
            "Only method that goes from has a simple function to compute that goes from the code to the input, which means you have to run an optimization algorithm to figure out the optimal code for a given input."
        ],
        [
            "So we're going to do is we're going to add to this sort of feedforward function that goes from the input to the code and we will attempt to predict where the optimal code is, and this function would necessarily have to be nonlinear.",
            "Um?"
        ],
        [
            "This is frustrating.",
            "We call this PSD, which means predicting sparsity composition.",
            "We could give, you know, various architectures for this.",
            "Let's say a simple neural net or something."
        ],
        [
            "And so here is."
        ],
        [
            "The algorithm running this PS yoga rhythm running on a set of natural image patches and what I'm representing here are each square.",
            "Here is a column of the WE matrix represented as as an image of the same size as the input.",
            "So you get the end of training.",
            "You get sort of oriented toward that detectors, which is what you are more or less expected."
        ],
        [
            "Now to design this encoder this G function.",
            "Phone call Gregor has this really brilliant idea of essentially emulating an algorithm that we know will produce the optimal Z called Fisto orista iterative shrinkage thresholding algorithm.",
            "And the East algorithm basically is this kind of iteration where you take the input your multiplied by an encoding matrix and then you pass it through a shrinkage shrinkage function.",
            "Each component multiplied by square matrix, which is you can think of as a lateral inversion matrix added to the previous result you had and iterate this group so it's kind of this recurrence here.",
            "What we're going to do is we're going to learn instead of using.",
            "You know this definition for S and that definition for WE, which is going to learn S&WE."
        ],
        [
            "And so you can think of this as a recurrent neural net or some kind with two matrices WENS, and we're going to train this recurrent neural net which has a bounded complexity to do the best approximation it can of the optimal passcode.",
            "And this works really well in interest of time.",
            "I'm going to skip the result."
        ],
        [
            "Just to say that we've applied this to kind of various things, including one where we have criteria that combined this reconstruction, the sparsity as well as the sort of prediction as a recent paper by Jason Wolfe, myself, actually actually conference and."
        ],
        [
            "This produces very interesting results that we've never seen before in in neural Nets.",
            "For for recognition, in which inputs are kind of decomposed into sort of sort of a prototype if you want plus a whole bunch of little modifiers.",
            "This prototype that turn the prototype into the observed input.",
            "I don't have time to go into the details."
        ],
        [
            "But I can tell you more about this later if you want, if you're interested.",
            "Another version of this uses convolution, so instead of using sparse coding as a linear, just a matrix, view it as a bunch of convolutions applied to feature Maps to reconstruct the input.",
            "And."
        ],
        [
            "Also use this PSD algorithm and the result of this for various reasons that I'm not going to explain too much is that we get much more diverse filters, largely because the system is trying out the image level and doesn't need to learn translated versions of all the filters and so it ends up having more resources to spend on learning very diverse filters like centers around crosses and corners and gratings of various kinds.",
            "So you get much more diverse filter is doing this than you.",
            "Get with standard Patch based prosecuting."
        ],
        [
            "I think that's another example here.",
            "As we increase the number of number of filters."
        ],
        [
            "OK, so how do we?",
            "How do we use this to train to pretraining convolutional net?",
            "We will take one of those PSD sparse autoencoder.",
            "This is really a sparsity encoder in the convolutional form to pretend the filters of convolutional net and so wrapped into this encoder.",
            "We have the input, the filter bank and the nonlinearity and the pulling really resides in the next stage."
        ],
        [
            "And once we are happy with it, we get rid of the decoder we we just run through the encoder runner training set for this and then."
        ],
        [
            "Stick a second stage of that.",
            "Train this.",
            "Unsupervised then."
        ],
        [
            "Get rid of the feedback again, just keep the feed forward path."
        ],
        [
            "Stick your classifier on top and now what we have is a full competition that we can train supervised, but it's been pre trained to essentially carry as much information as possible about the input all the way to the output and so we start from a pretty good place so it turns out.",
            "This really helps in situations where the task you're trying to learn is very very poor in terms of the diversity of labels that you have.",
            "But for a big data set like image net, it doesn't seem to make much of a difference.",
            "Perhaps in speed of learning we will still exploring this.",
            "We don't know, but."
        ],
        [
            "Things like pedestrian detection, so pedestrian detection is a case where you have lots of images are very diverse.",
            "The background category is a huge amount of variability in it, but you only have two labels.",
            "It's either a pedestrian or not, and if you're trying to connect to do this, the features you get at the top layer really bad because the label doesn't give you any information really about what's in the image, only gives you one bit of information, so using this free training actually helps to feature be more kind of generic.",
            "If you want an actually makes a difference in performance so.",
            "This is a false negative versus false positive rate, so this is the false positive for image.",
            "This is 1 false positive for image right here.",
            "And this is the miss rate.",
            "And all of those curves are from all kinds of systems publishing literature.",
            "This is a commercial net that's been trained, purely supervised, and this is the same one that's been pre trained with this unsupervised training.",
            "And then refine supervised so it makes quite a big difference.",
            "He goes from, you know, middle of the pack to basically record Holder for this data set.",
            "This is the."
        ],
        [
            "The in real datasets is going faster as you get after training."
        ],
        [
            "This video of the system in action there's an important point to make.",
            "Which is that is very easy to turn a localized recognizer or detector.",
            "That is a combination that into a data into a full fledged detector on a full image is very cheap to do this.",
            "It's a point that a lot of people miss.",
            "But it's really the case is not there."
        ],
        [
            "Apple this is in front of a lab here.",
            "We kind of lowered the threshold so that we get some of the false positives to see what they look like.",
            "There are a few."
        ],
        [
            "So there are various forms of this."
        ],
        [
            "Sort of invariant recognition where we build the pooling inside of this regularizer."
        ],
        [
            "For things like that, where we can organize the features into topographic mapping, we do a group sparsity on groups that overlap each other, and the system organizes itself so that features that fire together end up being in the same groups, and so you get this sort of nice looking topographic Maps which basically are not particularly interesting for people like us.",
            "The machine learning computer vision, but they really kind of."
        ],
        [
            "Resonate with your friends in neuroscience."
        ],
        [
            "You know they produce nice code, pictures which."
        ],
        [
            "Showing to you without explaining.",
            "OK, I'm going to go any further than that and because I'm out of time and I'd like to mention sort of a few issue, I think that are somewhat theoretical nature that we really don't understand about deep learning.",
            "The first one is what is the nature of the.",
            "Loss function that we minimizing with deep learning is kind of practical problem.",
            "Practical optimization problem if you will.",
            "Essentially when you train a deep neural net, the cost function has lots and lots of set of points.",
            "It's not just the local minima that are annoying.",
            "Is the set of points.",
            "Because there's a big issue of breaking symmetries.",
            "You know, deciding if one unit should do something and the other unit should do something complementary, or it should be the other way around, so the symmetry breaking issues, which are which are the issues that limit the speed of training.",
            "Coalition that traditionally haven't been victim of this, which is why they've been one of the few very deep neural Nets that have been used for a long time is because they don't have that symmetry breaking problem to the same extent they have.",
            "They have it, but not to the same extent, so it's kind of an issue.",
            "Other sort of issue is.",
            "Something we didn't understand 20 years ago is that the systems that seem to work best are the ones that are ridiculously over parameterized.",
            "We make the networks bigger and they just work better even if we have limited amounts of data.",
            "What we have to do is just regularize the hell out of out of them.",
            "Using things like dropout and things of that type.",
            "But even if we don't regularize them, they work surprisingly well and we don't understand why the affective dimension of those things is much lower than what we think.",
            "Um?",
            "Let's see.",
            "Then I think the biggest, the biggest issue, I think, is how do we formalize unsupervised learning really is good criterion for training and supervised system.",
            "It's always been a problem of mine to figure out, you know how you even test if an unsupervised learning system works.",
            "Right, what's the criterion an the criterion is, you know, trying to extract features and then trying to classify on the features and see if it works right?",
            "That at least that's an objective criterion.",
            "But it's not a good criterion to use for training unsupervised, 'cause you're not allowed to use the labels.",
            "So how do we design loss functions for unsupervised learning?",
            "How do we and what are the principles on which it would be based so this energy based stuff is a little more general than the usual sort of maximum likelihood.",
            "Asians probably formulations, but you know, maybe it's not the right one.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Supposed to be here.",
                    "label": 0
                },
                {
                    "sent": "I would like to thank the organizers for inviting me, particularly since I'm not known for sort of being kind of very close to the competition learning theory community.",
                    "label": 0
                },
                {
                    "sent": "Although I had personal connections to it and a lot of the stuff I'm going to talk about today is things where learning theory is kind of difficult to apply to, and I'd like to sort of present this as a challenge for this community to study it.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to talk mostly about learning representations or deep learning as it's come to be known.",
                    "label": 0
                },
                {
                    "sent": "You can think of deep learning as one particular way of learning representations, so we know that in the history of pattern recognition and machine learning, we've been doing a pretty good job at coming up with algorithms to learn classifiers and predictors of various kinds, always assuming that either the feature set or the kernel was given to us before hand, and.",
                    "label": 0
                },
                {
                    "sent": "The kind of questions I've been interested in for a very long time, before, you know, before you know from the first day, I started working on machine learning, which was in the early 80s.",
                    "label": 1
                },
                {
                    "sent": "Was learning representations 'cause I think it was.",
                    "label": 0
                },
                {
                    "sent": "I thought from the start that it was kind of the secret towards sort of more powerful learning machines and so that's why I sort of never had a huge interest with things like kernel methods and stuff like that, because they just were not solving the problems I was interested in.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that I don't think they're useful, but I think my interest is just elsewhere.",
                    "label": 0
                },
                {
                    "sent": "So here is where my interest is, it's.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Running running representations and learning representations, I think is probably the next step for for AI machine learning as well As for things like neuroscience and cognitive science, North scientists have no idea how the brain learns representations, and I think machine learning can help them with that.",
                    "label": 0
                },
                {
                    "sent": "So how do we learn representations of the world, say the visual perceptual world, and by merely looking at the world?",
                    "label": 1
                },
                {
                    "sent": "May be acting on it as well.",
                    "label": 0
                },
                {
                    "sent": "Neuro scientists ask themselves in similar question how does the visual cortex organized itself to perceive, you know, to turn itself into a visual cortex?",
                    "label": 1
                },
                {
                    "sent": "And the sort of connected question or machine learning would be how do we learn feature hierarchies.",
                    "label": 1
                },
                {
                    "sent": "The reason for hierarchy is is going to become clear in an interesting couple of minutes.",
                    "label": 0
                },
                {
                    "sent": "So the deep learning problem really is the one of is one of learning representations of the world.",
                    "label": 0
                },
                {
                    "sent": "By just looking at data.",
                    "label": 0
                },
                {
                    "sent": "So traditional.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Me.",
                    "label": 0
                },
                {
                    "sent": "As we all know, pattern recognition systems have been built by either building handcrafted feature extractors followed by more or less generic classifiers.",
                    "label": 1
                },
                {
                    "sent": "Although the more modern approach to things like object recognition or speech recognition actually has another stage in it in the middle, here the other will feature the fixed in the case of speech.",
                    "label": 0
                },
                {
                    "sent": "There things like MCCS in the case of vision that things like Sift and Hog and the second level actually uses learning, usually it.",
                    "label": 0
                },
                {
                    "sent": "It's based on some sort of unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Very simple one that K means or sparse coding to sort of turn the low level representations into sort of mid level representations you could call it this way and then it's followed by some sort of pooling and then the classifier supervised classifier.",
                    "label": 1
                },
                {
                    "sent": "So one more stage fixed unsupervised learning, supervised training.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the mainstream approach to speech and image recognition until very recently.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But really, the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model hasn't changed much since the early days of pattern recognition.",
                    "label": 1
                },
                {
                    "sent": "When you look at the perceptron, that's pretty much what it is.",
                    "label": 0
                },
                {
                    "sent": "You know, a feature extraction followed by some sort of classifier and.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe it's time after 50 years, 55 years or so to move away from that model.",
                    "label": 0
                },
                {
                    "sent": "If we look at.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How the brain performs vision.",
                    "label": 0
                },
                {
                    "sent": "And I'm not a huge advocate advocate for, you know, getting inspired by biology but but it's sometimes useful to get intuition from it is much more hierarchical.",
                    "label": 0
                },
                {
                    "sent": "You have kind of a for the sort of fast recognition process that people perform, and you know animals as well.",
                    "label": 0
                },
                {
                    "sent": "There's sort of a feedforward path that we can trace called eventual pathway in the cortex visual cortex.",
                    "label": 1
                },
                {
                    "sent": "That goes into several stages, each of which has, you know, very highly nonlinear operations.",
                    "label": 0
                },
                {
                    "sent": "Built in and it goes through various areas of the brain called V1V2V4I key, each of which has kind of several layers of processing if you want so you might have to.",
                    "label": 0
                },
                {
                    "sent": "The signal might have to go through maybe 10 synapses or something like this, or 10 neurons if you want to go to the area of the brain where object categories are encoded.",
                    "label": 0
                },
                {
                    "sent": "So it's not just it's not shadow in the in the usual sense, it doesn't look very much like the kind of architecture that we've used so far.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's sometimes useful to get a little bit of inspiration from biology, but it's also very dangerous.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because we don't know what's important when we look at the piece of brain, we don't know what matters in it.",
                    "label": 0
                },
                {
                    "sent": "And so here is an example of what not to do this.",
                    "label": 0
                },
                {
                    "sent": "This guy called Klimawandel from the 19th century who built airplanes model after bats.",
                    "label": 0
                },
                {
                    "sent": "And you know he didn't know much about aerodynamics.",
                    "label": 0
                },
                {
                    "sent": "He was very good at building steam engines, but powerful in light steam engines, but not very good at.",
                    "label": 0
                },
                {
                    "sent": "Like, you know, stability and things like this and so we build this airplane full size and things actually took off flew 50 meters at 1 meter.",
                    "label": 0
                },
                {
                    "sent": "Altitude was completely uncontrollable, which is why you probably never heard of him.",
                    "label": 1
                },
                {
                    "sent": "Every Frenchman, of course knows about him, but.",
                    "label": 0
                },
                {
                    "sent": "But not on this side of the pond.",
                    "label": 1
                },
                {
                    "sent": "This other point is the Wright brothers.",
                    "label": 0
                },
                {
                    "sent": "This were 13 years before the Wright brothers.",
                    "label": 1
                },
                {
                    "sent": "So you know he got he was too close to the biology.",
                    "label": 0
                },
                {
                    "sent": "He really didn't know what was important about it and.",
                    "label": 0
                },
                {
                    "sent": "He hadn't figured out completely, you know, the law of aerodynamics, instability, and we might ask the question, you know, in our case, there are people in the world who got lots of money from various governments who claim that they can reproduce the functions of the human brain by basically simulating a giant brain on the computer.",
                    "label": 0
                },
                {
                    "sent": "And I think it's completely nuts.",
                    "label": 0
                },
                {
                    "sent": "I think it's just never, you know it can't possibly work if we don't understand where the underlying principles are.",
                    "label": 1
                },
                {
                    "sent": "So what are the underlying principles for intelligence?",
                    "label": 0
                },
                {
                    "sent": "What is the equivalent aerodynamics?",
                    "label": 0
                },
                {
                    "sent": "Namics for intelligence, right?",
                    "label": 0
                },
                {
                    "sent": "That's what we might want to go after in terms of sort of conceptual goals.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the architecture we might want to go after is something of this type where you know we feel the raw input and we have to go through a whole bunch of transformations, each of which is kind of a trainable machine.",
                    "label": 0
                },
                {
                    "sent": "If you will, and.",
                    "label": 0
                },
                {
                    "sent": "As as with side effect of this whole system, training itself to perform the task, it will kind of drive internal representations of the problem.",
                    "label": 0
                },
                {
                    "sent": "So the first if you are theorist.",
                    "label": 0
                },
                {
                    "sent": "At least some of theories I know first question I ask, is why do we need that many layers, why?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can't, we just do this with just two layers, like say this OK have a kernel function, compare your input with all your training samples or subset of them computer linear combination of this and you're done now.",
                    "label": 0
                },
                {
                    "sent": "So of course you know someone has to give you this kernel function which is just as complicated as coming up with the feature vector but but you know if you limit the complexity of this function here is pretty clear that there is a whole bunch of functions here that will require very very large number of terms in this sum to be able to do anything useful.",
                    "label": 0
                },
                {
                    "sent": "Or if the number of terms is small, that means your kernel function here does all the work.",
                    "label": 0
                },
                {
                    "sent": "And what is it coming from?",
                    "label": 0
                },
                {
                    "sent": "Where is it coming from?",
                    "label": 0
                },
                {
                    "sent": "So there's kind of a bit of a conundrum here, and you know, you would think that some functions that perceptual systems have to compute are inherently sequential.",
                    "label": 0
                },
                {
                    "sent": "They require to have a certain number of sequential operations for them to be.",
                    "label": 0
                },
                {
                    "sent": "How computable with sort of a relatively small amount of resources?",
                    "label": 0
                },
                {
                    "sent": "There's a lot of complexity theorists in the room, and people who have studied circuit complexity.",
                    "label": 0
                },
                {
                    "sent": "I'm sure who know that if you allow yourself to have a few layers of computations, you can gain an exponential factor on the amount of hardware you need to compute a particular function.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For example, let's say let's take the example of, you know Andy parity.",
                    "label": 0
                },
                {
                    "sent": "We know that we can do this with, you know, log in stages or you know, like over two stages with ex or Gates.",
                    "label": 0
                },
                {
                    "sent": "But if we force ourselves to do it in two layers using the contract in normal form, we might need an exponential number of midterms.",
                    "label": 1
                },
                {
                    "sent": "So you know it's just a very simple question of exchange of complexity between space and time.",
                    "label": 0
                },
                {
                    "sent": "Essentially that we deal with all the time when we write programs.",
                    "label": 0
                },
                {
                    "sent": "If we log ourselves multiple stages, then we might gain a huge factor on on the overall amount of resources we need so.",
                    "label": 0
                },
                {
                    "sent": "It's very intuitive, you know I don't have any theorems to tell you to show you you guys other guys, other people to actually prove those theorems, but the basic idea is intuition of why we need multiple layers.",
                    "label": 0
                },
                {
                    "sent": "Is just that this circuit complexity base.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's a bit of a theoretician nightmare here, which is that when we start having multiple layers in a system like saying a deep neural net and we tried to train it using, say, supervised running, we have loss functions that end up being non convex.",
                    "label": 0
                },
                {
                    "sent": "And if you have non convex loss functions and you rely on things that you know like minimal.",
                    "label": 0
                },
                {
                    "sent": "Communication and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "You know all those things go out the window because you can't prove anything about the optimality of your model.",
                    "label": 0
                },
                {
                    "sent": "OK, but you know if they work, it's actually worth maybe spending the effort trying to figure out why they work so.",
                    "label": 0
                },
                {
                    "sent": "So all bets are off for non convex losses, but then again you know every speech system ever deployed as use non convex optimization and it's not because they're non convex that they don't work.",
                    "label": 1
                },
                {
                    "sent": "Arguably, to some of us, the only interesting learning actually is non convex because.",
                    "label": 0
                },
                {
                    "sent": "If you have a convex optimization to solve for learning, then the order in which.",
                    "label": 0
                },
                {
                    "sent": "You run things, doesn't matter.",
                    "label": 1
                },
                {
                    "sent": "We know that humans.",
                    "label": 0
                },
                {
                    "sent": "The order in which you learn things in humans matters.",
                    "label": 0
                },
                {
                    "sent": "It's called pedagogy, right?",
                    "label": 0
                },
                {
                    "sent": "I can start my talk with the last slide and then go backwards, but I don't think you'll get in the same state at the end.",
                    "label": 0
                },
                {
                    "sent": "If I do this so.",
                    "label": 0
                },
                {
                    "sent": "Even though the aggregated statistics is the same, so you know there's the order in which you see things actually matters and in which you learn things actually matters, and that suggests the fact that whatever loss function you know the human brain is minimizing if it actually minimizes the loss function, which is another question, it has to be nonconvex.",
                    "label": 1
                },
                {
                    "sent": "Otherwise the order in which we learn things wouldn't matter.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, that doesn't mean that we can't actually.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know do AI without convex optimization, but.",
                    "label": 0
                },
                {
                    "sent": "It is sort of an interesting observation perhaps.",
                    "label": 0
                },
                {
                    "sent": "OK, the other theoretician nightmare is that you know if if we have all those things with deep learning algorithms, we don't have any good generalization bounds.",
                    "label": 0
                },
                {
                    "sent": "Well, actually it's not true.",
                    "label": 0
                },
                {
                    "sent": "You know you take a bigger role net.",
                    "label": 0
                },
                {
                    "sent": "It actually has a bound VC bound type thing because it's visit dimension is finite.",
                    "label": 0
                },
                {
                    "sent": "But know tight bounds.",
                    "label": 0
                },
                {
                    "sent": "Now, that could be a problem, except that I don't actually know anybody in practice who uses bounds to do model selection.",
                    "label": 0
                },
                {
                    "sent": "Um, I hate to say this in front of this crowd, but.",
                    "label": 0
                },
                {
                    "sent": "But that's true.",
                    "label": 0
                },
                {
                    "sent": "Does anyone know anyone know anyone who actually uses bounds in practice to the model selection as opposed to, say, cross validation?",
                    "label": 0
                },
                {
                    "sent": "Raise your hands.",
                    "label": 0
                },
                {
                    "sent": "OK you go, no hands.",
                    "label": 0
                },
                {
                    "sent": "OK, I don't know but I know I know it's not true.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you see, some of my best friends use bounds.",
                    "label": 0
                },
                {
                    "sent": "So you know it's hard to prove anything about deep learning systems.",
                    "label": 1
                },
                {
                    "sent": "But then again, if we only study models for which we can prove things, we wouldn't have speech, handwriting and object recognition systems today.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. On the other hand, it's kind of a smorgasbord of stuff for theoreticians because there's so many things we don't know about all the methods that are used in deep learning, and I'll talk about them, some of them as I go.",
                    "label": 0
                },
                {
                    "sent": "You know, during the talk so.",
                    "label": 0
                },
                {
                    "sent": "You know, deep running is about representing high dimensional data or representing data in additional spaces, and there has to be interesting theoretical questions about this West of geometry of natural signals.",
                    "label": 1
                },
                {
                    "sent": "So is there any equivalent to SQL learning theory for unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "Our computational learning theory?",
                    "label": 0
                },
                {
                    "sent": "For that matter?",
                    "label": 0
                },
                {
                    "sent": "I mean I know there are efforts along those directions in this community.",
                    "label": 1
                },
                {
                    "sent": "What are good criteria on which to base on supervised learning?",
                    "label": 0
                },
                {
                    "sent": "Things like that questions that have been answered by this community for supervised learning, which I think would be worth thinking about four.",
                    "label": 0
                },
                {
                    "sent": "Feature learning, which is sometimes unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. OK, so let me talk about the state of the state of affairs for deep learning today.",
                    "label": 1
                },
                {
                    "sent": "It's been one of the hottest topics in speech recognition for about the last two years, the latest generation of commercial systems that are based on that do speech recognition are all based on deep learning.",
                    "label": 1
                },
                {
                    "sent": "The ones from IBM, Google, Microsoft, all based on deep learning.",
                    "label": 0
                },
                {
                    "sent": "The ones in the iPhone, which is actually indirectly produced by IBM through nuans.",
                    "label": 0
                },
                {
                    "sent": "They all use deep neural Nets.",
                    "label": 0
                },
                {
                    "sent": "And it's been a very, very fast transition from gas mixture models to deep neural Nets for acoustic modeling in the last two years.",
                    "label": 1
                },
                {
                    "sent": "It's becoming the hottest topic in computer vision, mostly because of the last image net competition.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to that and it's becoming a bit of a interesting topic for natural language processing, although it's not there yet.",
                    "label": 0
                },
                {
                    "sent": "And there's a huge interest from the applied math community into things like representing data in high dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "People who used to do, how many.",
                    "label": 0
                },
                {
                    "sent": "Can I use this when I do sparse coding and stuff like that?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here's another piece of history which kind of indicates in what direction things are going.",
                    "label": 0
                },
                {
                    "sent": "If you look at the history of handwriting recognition, speech recognition, now computer vision, object recognition.",
                    "label": 1
                },
                {
                    "sent": "Every time there's been an increase in the number of training samples that were available, people started using learning more and more for more and more of the system.",
                    "label": 0
                },
                {
                    "sent": "So instead of just running the classifier, they would learn you know the billable features and other low level features in an integrated fashion, and every time a step like this that was previously built by hand have been replaced by.",
                    "label": 1
                },
                {
                    "sent": "Hang out with them, especially performance has improved so we just saw this in speech recognition is just happening also in computer vision and you know it's been happening.",
                    "label": 0
                },
                {
                    "sent": "It was happening before that happened before that in handwriting recognition back in the 90s.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of a direct, very clear sense of history as datasets go increase in size as our machines become more powerful, we use learning more and more and things become better as we replace more and more handcrafted modules in our system.",
                    "label": 0
                },
                {
                    "sent": "It's a very clear path.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a, you know interesting thing to say for people interested in learning more.",
                    "label": 0
                },
                {
                    "sent": "If you rely more on learning and less on human engineering, things work better if you have more data.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing revolutionary there, but you know you have to kind of realize that that's going on.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about convolutional net because you know, everybody assumed that I'm going to talk about that, and also because they become very popular in recent years for practical applications and a number of companies have deployed image recognition application, some of the speech recognition applications based on convolutional Nets in the last year or so less you month in fact.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accomplishments are inspired by biology.",
                    "label": 0
                },
                {
                    "sent": "To some extent, a little bit by the architecture.",
                    "label": 0
                },
                {
                    "sent": "What we know about the architecture of the visual cortex has its roots in the car neutron model by pushing the 70s.",
                    "label": 0
                },
                {
                    "sent": "Which itself at its root in the work of human results, classic work in our science from the 60s, where basically the first couple of stages of the visual system, the visual cortex are seen as filter banks.",
                    "label": 0
                },
                {
                    "sent": "OK, so you have an input here or a bunch of planes of input which might represent say, color planes for example, and then have a bunch of planes here that extract features from the input using a convolution kernel.",
                    "label": 0
                },
                {
                    "sent": "Essentially so you take a convolution kernel.",
                    "label": 0
                },
                {
                    "sent": "Swipe it over the image you get, so it's like a linear filtering.",
                    "label": 0
                },
                {
                    "sent": "You get the result of the application of this convolution to the input.",
                    "label": 0
                },
                {
                    "sent": "For different filters you'll get different different outputs.",
                    "label": 0
                },
                {
                    "sent": "Each of those is called a feature map.",
                    "label": 0
                },
                {
                    "sent": "You pass through some nonlinearity, the latest, most popular nonlinearity is called value rectified linear unit.",
                    "label": 0
                },
                {
                    "sent": "So it's basically half rectification positive, part very simple nonlinearity, and then there is a second operation cooling, which kind of aggregates the answers of the filters over a small.",
                    "label": 0
                },
                {
                    "sent": "Region, sometimes over multiple features, and the resolution is reduced.",
                    "label": 0
                },
                {
                    "sent": "That is, to build a little bit of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shift in variance and distortion variance into the system.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is sort of, you know, pretty old idea.",
                    "label": 0
                },
                {
                    "sent": "The modern versions use what we call multiple stages, each of which has four layers, and so when that performs some sort of contrast normalization followed by this filter bank.",
                    "label": 1
                },
                {
                    "sent": "I was just telling you about the nonlinear operation.",
                    "label": 1
                },
                {
                    "sent": "This rectified linear units that thing about, and then this feature pooling and then you take this module, you stack multiple instances of it, stick your favorite classifier on top.",
                    "label": 0
                },
                {
                    "sent": "Usually the whole thing is is trained with backpropagation.",
                    "label": 0
                },
                {
                    "sent": "Just great Internet basically.",
                    "label": 0
                },
                {
                    "sent": "So stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And use backdrop to compute the gradients of whatever loss function you want to minimize with respect to all the coefficients in all the filters in every layer.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The pruning operation is just any kind of symmetric operation of Max and average square root of sum of square or something of that type.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We give you a.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you an old animation of.",
                    "label": 0
                },
                {
                    "sent": "Accomplishing that that was trained to recognize handwritten characters.",
                    "label": 0
                },
                {
                    "sent": "The reason I'm showing this to you is.",
                    "label": 0
                },
                {
                    "sent": "Because I would like to point the properties of feature extractors that I think is important.",
                    "label": 0
                },
                {
                    "sent": "So this was trying entirely supervised to classify essentially end this digits.",
                    "label": 0
                },
                {
                    "sent": "I think of it this way with a little bit of invariants to position.",
                    "label": 0
                },
                {
                    "sent": "Now look at this particular pixel here.",
                    "label": 0
                },
                {
                    "sent": "As the number 3 here goes up and down, it goes from white to black to white.",
                    "label": 0
                },
                {
                    "sent": "I have a hard time keeping it fixed but OK, So what that means is that the line formed by all the translated versions of this three is not flat.",
                    "label": 0
                },
                {
                    "sent": "It's got a curvature because one of the coordinate goes from one value to another value back to the first value.",
                    "label": 0
                },
                {
                    "sent": "So it's going to have some curvature to it.",
                    "label": 0
                },
                {
                    "sent": "Now the bad news is that.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine the set of all deformed threes, it's a surface whose intrinsic dimension is the number of possible.",
                    "label": 0
                },
                {
                    "sent": "Deformations you can do locally.",
                    "label": 0
                },
                {
                    "sent": "Many fold 4 digit 8.",
                    "label": 0
                },
                {
                    "sent": "Is sort of entangled with the three is very highly nonlinear as well, but it's not.",
                    "label": 0
                },
                {
                    "sent": "You know you separable from the one of the threes that most of the pixels are common, but the curvature of the two manifolds is very very very high, so you can't.",
                    "label": 0
                },
                {
                    "sent": "You can't just do linear separation between them, you have to extract features.",
                    "label": 0
                },
                {
                    "sent": "We know that.",
                    "label": 0
                },
                {
                    "sent": "OK, so look at if you look at any value that appears.",
                    "label": 0
                },
                {
                    "sent": "Here are the top layer is hardly any that goes from white to black to white or vice versa.",
                    "label": 0
                },
                {
                    "sent": "They go from, you know, let's create a more Gray or vice versa, whatever that means.",
                    "label": 0
                },
                {
                    "sent": "What that means is that the manifold of stuff of deformed translated patterns here is kind of is flatter.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Because of all the process that went during training of this.",
                    "label": 0
                },
                {
                    "sent": "Of course it happened to be this way because the layer that came after this was a linear classifier and linear classifiers like to have you know space to separate from.",
                    "label": 0
                },
                {
                    "sent": "So that's precisely why it happened.",
                    "label": 0
                },
                {
                    "sent": "But it's a good property to have for a feature for a set of features that the manifold of variations you don't care about is flat.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Perhaps there is some idea on how to derive a proper criterion for unsupervised training based on this sort of flatening manifold kind of idea.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me skip this.",
                    "label": 0
                },
                {
                    "sent": "Your choice of time.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a whole number of tasks for which we know that deep convolutional Nets are the best method so far, for which we have the record.",
                    "label": 1
                },
                {
                    "sent": "Handwriting recognition goes back a long time, OCR in natural images, traffic sign recognition, pedestrian detection, voluntary brain, image segmentation, human action recognition, and object recognition scene parsing, some pricing from death images, speech recognition, breast cancer, cell mitosis detection.",
                    "label": 1
                },
                {
                    "sent": "This ones actually kill competition.",
                    "label": 0
                },
                {
                    "sent": "There was one very recently.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Of those were wrong by purely supervised convolutional Nets.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we think like you know traffic sign recognition.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That number is another application that we built.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the EG analysis this is 1 from.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Investment songs group at MIT.",
                    "label": 0
                },
                {
                    "sent": "Within Jane for segmenting brain tissue, where he applies a commercial net to a small volume of voxels and transit to classify the central voxel as being the boundary between two cells or not.",
                    "label": 0
                },
                {
                    "sent": "This is a piece of brain tissue, and after the network is produced, labels for each of the things they can sort of reconstruct the.",
                    "label": 0
                },
                {
                    "sent": "The brain circuit here is only showing a few percent of the neurons.",
                    "label": 0
                },
                {
                    "sent": "There are very, very densely packed.",
                    "label": 0
                },
                {
                    "sent": "So it's a very interesting application to connect to mix.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But he was really what caused a big commotion in the last few months.",
                    "label": 0
                },
                {
                    "sent": "Back in October or little earlier than this leski.",
                    "label": 0
                },
                {
                    "sent": "Orange in Fenton won the image net 2012 competition, so this is one of the main competition in computer vision for object recognition and they wanted by huge margin they got something like 15% error rate by some measure where all other methods that competed in the same competition got around 2526% error.",
                    "label": 1
                },
                {
                    "sent": "So it's really a big jump.",
                    "label": 0
                },
                {
                    "sent": "It's not just you know small improvement, speed jump.",
                    "label": 0
                },
                {
                    "sent": "Ann is essentially a big commercial net using.",
                    "label": 0
                },
                {
                    "sent": "She was saying all the tricks I came up with in the last 20 years plus this dropout technique.",
                    "label": 1
                },
                {
                    "sent": "But really, what made this secret or what made this successful is a very efficient implementation of this on GPU's, which allows which allowed them to train on very large data set or 1.3 million training samples in about a week.",
                    "label": 0
                },
                {
                    "sent": "On the single GPU actually onto GPS, the filters are under the first layer and sort of somewhat interesting.",
                    "label": 0
                },
                {
                    "sent": "The color and the black and white ones are separated for.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is kind of an artifact of the way that the system was designed.",
                    "label": 0
                },
                {
                    "sent": "It works very well.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another example.",
                    "label": 0
                },
                {
                    "sent": "This was kind of a similar system that was developed in why you would not by me by Rob Fergus and his student Matt Zeiler an so they have a conversation that is somewhat similar to electric jet skis, but a little different.",
                    "label": 0
                },
                {
                    "sent": "Also trained on the on image net.",
                    "label": 0
                },
                {
                    "sent": "Also using values using contrast normalization and the layer.",
                    "label": 0
                },
                {
                    "sent": "These are the filters at the first layer that are trained.",
                    "label": 0
                },
                {
                    "sent": "Also uses this dropout regularization which is essentially sort of very brutal.",
                    "label": 0
                },
                {
                    "sent": "Regularization that consist in killing half of the units in the top layers and hoping that the network will recover from it and there's a different half that you kill at every sample.",
                    "label": 0
                },
                {
                    "sent": "We think it's, you know.",
                    "label": 0
                },
                {
                    "sent": "Murderous, but it seems to actually help a little bit.",
                    "label": 0
                },
                {
                    "sent": "It uses SGD.",
                    "label": 0
                },
                {
                    "sent": "Not a particularly sophisticated form of SGD.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is an online demo of this that you can play with the ratio that season where you can upload images and it will kind of tag it with all the categories.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mines.",
                    "label": 0
                },
                {
                    "sent": "It works, you know, just as well as exquisite skis.",
                    "label": 0
                },
                {
                    "sent": "A little better maybe, but I'm sure Alex is better versions now, but it's not going to talk about it not using Google.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But here is an interesting point about this.",
                    "label": 0
                },
                {
                    "sent": "This network, the features that it learns, are fairly generic.",
                    "label": 0
                },
                {
                    "sent": "So if you if you take the network that is trained on Imagenet, chop off the task layer and then just retrain the last layer on a different data sets a Caltech 256, you basically get state of the art performance with only six training samples per category.",
                    "label": 1
                },
                {
                    "sent": "I think that's amazing.",
                    "label": 0
                },
                {
                    "sent": "So Cal tech.",
                    "label": 0
                },
                {
                    "sent": "56 is not a particularly interesting set.",
                    "label": 0
                },
                {
                    "sent": "Now is kind of out of date, but you know the state of the art is is here.",
                    "label": 0
                },
                {
                    "sent": "Previous state of the art is here.",
                    "label": 0
                },
                {
                    "sent": "You get to the state of the art with six training samples per category just using the features at the output of this commercial net right before the output layer.",
                    "label": 0
                },
                {
                    "sent": "Just retraining the classic classification layer.",
                    "label": 0
                },
                {
                    "sent": "What that means is that the features it's Lauren are really generic.",
                    "label": 0
                },
                {
                    "sent": "You can use them for just about any object recognition tasks that you could imagine.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is another example.",
                    "label": 0
                },
                {
                    "sent": "This is with the Pascal BOC challenge, so here it's not as dramatic, but you know the accuracy is pretty much set of the article below.",
                    "label": 0
                },
                {
                    "sent": "Mostly because of the nature of the labeling of the Pascal VLC data set.",
                    "label": 0
                },
                {
                    "sent": "But it was kind of impressive because only the last linear classifier basically is is trained here on on this data set.",
                    "label": 0
                },
                {
                    "sent": "The feature is just going from image net.",
                    "label": 0
                },
                {
                    "sent": "Here's another.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example, there was only my tab by Kimmel Farbe and chemically this is for labeling images.",
                    "label": 0
                },
                {
                    "sent": "This is called the semantic labeling problem or or seen parsing or these various names for it.",
                    "label": 0
                },
                {
                    "sent": "And the problem here is to label every pixel in an image with the category of the object that it belongs to.",
                    "label": 1
                },
                {
                    "sent": "OK, so you know Sky, grass and trees and Bubba.",
                    "label": 0
                },
                {
                    "sent": "So it's a little more complicated than than object recognition, because you can have to figure out where every pixel what every pixel belongs to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is used a convolutional net, but a multiscale kind of convolutional net.",
                    "label": 0
                },
                {
                    "sent": "Where you take the input, you'll be able to pyramid out of it where you have kind of lower resolution versions.",
                    "label": 0
                },
                {
                    "sent": "Of those of this image, and then you apply the same convolutional net.",
                    "label": 0
                },
                {
                    "sent": "So let's say you want to classify the central voxel.",
                    "label": 0
                },
                {
                    "sent": "Here the central pixel here you apply convolutional net who's window sort of context window that is going to influence.",
                    "label": 0
                },
                {
                    "sent": "In particular output is say 46 by 46 pixel.",
                    "label": 0
                },
                {
                    "sent": "If you apply the same conversation led to this image it will also get influenced by 40 by 46 window, but now there.",
                    "label": 0
                },
                {
                    "sent": "Since the image resolution is half is going to take into account a complex that's twice as big, same here.",
                    "label": 0
                },
                {
                    "sent": "So this guy is basically going to see the entire image at quarter resolution and you combine the features that are produced by the three copies of this commercial net with the same.",
                    "label": 0
                },
                {
                    "sent": "They all have the same filters, concatenate them, feed them to a classifier you trying this whole thing, supervised on a couple thousand labeled images that have been labeled at the pixel level.",
                    "label": 0
                },
                {
                    "sent": "And every decision for a particular pixel is going to be taking into account this 47 six window, and this was this was about 46 window, which really is 9192 here and this will be for this window which is really 180, four 24 here.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can use this huge context to make a decision.",
                    "label": 0
                },
                {
                    "sent": "And this pretty much works at state of the art, so the best results on this data set on the pixel pixel accuracy basis is 82% or assistant gets 81.4 instead of 81.9.",
                    "label": 0
                },
                {
                    "sent": "We also have a simple system that is slightly lower win percent, but it's about 100 times faster because it doesn't use any kind of complex graphical models.",
                    "label": 0
                },
                {
                    "sent": "Post processing and uses a very, very simple postprocessing after the convolutional net.",
                    "label": 0
                },
                {
                    "sent": "This is on the Stanford background data set that has eight categories.",
                    "label": 1
                },
                {
                    "sent": "This is the sea flow data set, or I should have mentioned.",
                    "label": 0
                },
                {
                    "sent": "There are two measures of performance.",
                    "label": 0
                },
                {
                    "sent": "This is pixel accuracy and this is pixel accuracy.",
                    "label": 0
                },
                {
                    "sent": "Where and their accounts for more if it concerns a category that's rare so that you pay more for missing a human, even if it's only a few pixels in a large image.",
                    "label": 0
                },
                {
                    "sent": "So it's you know it's more accurate measure on this one.",
                    "label": 0
                },
                {
                    "sent": "We actually beat beat everyone.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is on the floor data set with 30 three categories.",
                    "label": 0
                },
                {
                    "sent": "Then again, pretty much have the record here.",
                    "label": 0
                },
                {
                    "sent": "Depending on how we train it, we can beat the record either on pixel accuracy or on class normalized pixel accuracy and this is a data set with 170 categories where nobody does well.",
                    "label": 0
                },
                {
                    "sent": "We do just less badly than you know.",
                    "label": 0
                },
                {
                    "sent": "This is Bennett's ethics group at.",
                    "label": 0
                },
                {
                    "sent": "You know, several annoying.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Urbana Champaign",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we got some examples of results.",
                    "label": 0
                },
                {
                    "sent": "This works really well.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, this is the 30 three categories of trees and buildings on windows and roads.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And everything Sky San.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annual see example of a.",
                    "label": 0
                },
                {
                    "sent": "The video here.",
                    "label": 0
                },
                {
                    "sent": "So each frame here is processed independently.",
                    "label": 0
                },
                {
                    "sent": "There's no sort of temporal consistency here.",
                    "label": 0
                },
                {
                    "sent": "And it makes some stupid mistakes like you know when?",
                    "label": 0
                },
                {
                    "sent": "When the when the ground is bright it's classified as sand.",
                    "label": 0
                },
                {
                    "sent": "This is actually Greenwich Village, so I can tell you there's no sand.",
                    "label": 0
                },
                {
                    "sent": "There's no beach.",
                    "label": 0
                },
                {
                    "sent": "So this is someone riding a bike with a panoramic camera have been.",
                    "label": 0
                },
                {
                    "sent": "Pictures have been stitched together.",
                    "label": 0
                },
                {
                    "sent": "But it gets most of the.",
                    "label": 0
                },
                {
                    "sent": "Important objects like humans, I mean people and.",
                    "label": 0
                },
                {
                    "sent": "You know trees and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "Module lighting problems.",
                    "label": 0
                },
                {
                    "sent": "How much speed of the Debbie Aga give me over CPU?",
                    "label": 0
                },
                {
                    "sent": "So a CPU.",
                    "label": 0
                },
                {
                    "sent": "So on the sort of beefy Mac laptop this runs at about 2 frames per second.",
                    "label": 0
                },
                {
                    "sent": "The GPA, if we run entirely on the FDA, would run at about 20 frames per second, except that the communication with the FDA board is actually slow.",
                    "label": 0
                },
                {
                    "sent": "So the system time is actually slower.",
                    "label": 0
                },
                {
                    "sent": "Here's another.",
                    "label": 0
                },
                {
                    "sent": "Another example, this uses temporal consistency.",
                    "label": 0
                },
                {
                    "sent": "It's kind of similar video has fewer categories, but it's the same video, and there's a lot less kind of jumping around of categories here because of this consistent.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Overtime, it's just the post processing.",
                    "label": 0
                },
                {
                    "sent": "There's no change to the commercial net.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can you provide this also to images that have depth information within so connected with the Connect indoor data?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, this has the best results that we know of.",
                    "label": 0
                },
                {
                    "sent": "And this is without temporal consistency and with temporal consistency.",
                    "label": 0
                },
                {
                    "sent": "OK, so enough for super.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stuff, let's talk about unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "So how do we devise unsupervised learning algorithms that will allow us to pre train the layers of Silicon net or any kind of deep learning system?",
                    "label": 0
                },
                {
                    "sent": "Hierarchical feature extractor?",
                    "label": 0
                },
                {
                    "sent": "Enable data 'cause of course we have a lot more enable data then we have labeled data.",
                    "label": 0
                },
                {
                    "sent": "Now it used to be that all of deep learning all over deep learning was about was this and there was kind of a minor side interest in things like convolutional Nets which seem to work just fine if you just train them supervised.",
                    "label": 0
                },
                {
                    "sent": "As long as you have enough data and because of the.",
                    "label": 0
                },
                {
                    "sent": "Important because of the fact that we now have large datasets, large labeled datasets, this sort of slightly less interest in this kind of unsupervised running things because the supervised stuff works so well.",
                    "label": 0
                },
                {
                    "sent": "So even people like Jeff Hinton, who kind of were really, really interested in this unsupervised running stuff.",
                    "label": 0
                },
                {
                    "sent": "Essentially for practical purposes, just doing supervised convolutional Nets.",
                    "label": 0
                },
                {
                    "sent": "Now you actually called them dread Nets, which stands for deep rectified.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Networks we drop out DERDDRED right?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do we do unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "So you know, there's this hypothesis that the manifold of natural data if you take patches from natural images, the set of possible patches you will observe is actually a low dimensional subspace of all possible combinations of pixels.",
                    "label": 1
                },
                {
                    "sent": "You could come up with, right?",
                    "label": 0
                },
                {
                    "sent": "So there is this, you know, picture of sort of the manifold assumption and some people don't agree with this.",
                    "label": 1
                },
                {
                    "sent": "People like if I say that's kind of wrong picture to think about.",
                    "label": 0
                },
                {
                    "sent": "I think I agree with him, but it's still useful.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we'd like to do is, you know, turn ideally what an ideal feature extractor should do is take a bunch of samples data points.",
                    "label": 0
                },
                {
                    "sent": "And tell you if there is a manifold of data.",
                    "label": 1
                },
                {
                    "sent": "Tell you where you are on the manifold with a number of components and then have a separate set of components that tell you the distance to the manifold and all the other dimensions in the ambient space.",
                    "label": 1
                },
                {
                    "sent": "OK, So what you really want is you want to kind of factor two sets of variables.",
                    "label": 1
                },
                {
                    "sent": "One is where you want to manifold and the other one is where you are away from the manifold.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you had for example, the set of manifold of possible faces, all human faces, it's Admiral manifold.",
                    "label": 0
                },
                {
                    "sent": "For a particular person is bounded by the number of muscles in your face and the number of degrees of freedom that you can move around, which is 6, and then if you include everybody then there is some sort of you know number of dimensions basically bounded by the genome of you know how many different faces you can have.",
                    "label": 0
                },
                {
                    "sent": "Module accidents.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so you know, if we had this manifold, then if we had this kind of feature separation, you could use.",
                    "label": 0
                },
                {
                    "sent": "The first part to tell who you're looking at and what expression they're making, and you can use the second half.",
                    "label": 0
                },
                {
                    "sent": "The part that tells you you know how far away you are from that manifold to tell you if it's a face or not.",
                    "label": 0
                },
                {
                    "sent": "Or if it's something else.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So distant angling the explanatory factors of variation is really what unsurprised running should be about.",
                    "label": 0
                },
                {
                    "sent": "There is a general idea which is sort of emerging a little bit.",
                    "label": 0
                },
                {
                    "sent": "There is no kind of concrete or theoretical justification for it, but it's just what people end up doing in the end, which is a feature extractor should really be composed of two steps in nonlinear step that basically embed the input into a very high dimensional space or high dimensional space?",
                    "label": 0
                },
                {
                    "sent": "OK, this is similar to the kind of kernel kind of thing, right to embed things into high dimensional space so that things are more easily.",
                    "label": 0
                },
                {
                    "sent": "Horrible in that space.",
                    "label": 0
                },
                {
                    "sent": "Same stuff except you know, know based on the kernel trick based on other things, it has to be nonlinear because of his linear.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do anything useful for you, so I can know up.",
                    "label": 0
                },
                {
                    "sent": "OK, so embed your input into high dimensional space in some sort of nonlinear way.",
                    "label": 1
                },
                {
                    "sent": "But then what you do here is that you're breaking the space apart.",
                    "label": 0
                },
                {
                    "sent": "There are things that are semantically similar or even identical.",
                    "label": 1
                },
                {
                    "sent": "They will end up in different bins.",
                    "label": 0
                },
                {
                    "sent": "In this representation will be very far apart, so the second step, which is sort of a generalized pulling similar to the polling we doing convolutional net.",
                    "label": 0
                },
                {
                    "sent": "Is to regroup the things here that are supposed to be similar.",
                    "label": 0
                },
                {
                    "sent": "OK, so whenever.",
                    "label": 0
                },
                {
                    "sent": "You get 2 vectors here that represent two faces, and they happen to be very different.",
                    "label": 0
                },
                {
                    "sent": "You somehow encode them in such a way that here they end up being similar.",
                    "label": 0
                },
                {
                    "sent": "I did this dinner.",
                    "label": 0
                },
                {
                    "sent": "It may or may not be linear.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry I didn't pick up, OK.",
                    "label": 0
                },
                {
                    "sent": "Probably not, no.",
                    "label": 0
                },
                {
                    "sent": "What people do in practice is something like.",
                    "label": 0
                },
                {
                    "sent": "Like L2, so square root of sum of squares of some components, subset of components.",
                    "label": 0
                },
                {
                    "sent": "Is someone picking some landmarks and just representing every object is the vector distances from those networks?",
                    "label": 0
                },
                {
                    "sent": "Yeah, right.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's an example for very concrete example for the sort of intro to computer vision until convolutional Nets, right?",
                    "label": 0
                },
                {
                    "sent": "So the data does ethnic PRB mechanism for example OK?",
                    "label": 0
                },
                {
                    "sent": "Well for mid level features what you do here is K means.",
                    "label": 1
                },
                {
                    "sent": "So you take so see vector, you run it through again means algorithm and the K means is going to give you a binary vector.",
                    "label": 0
                },
                {
                    "sent": "With all zeros but an 11 at the location of the prototype that's closest to the input.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's just a winner.",
                    "label": 0
                },
                {
                    "sent": "Take all kind of encoding if you want a word right.",
                    "label": 0
                },
                {
                    "sent": "The visual word.",
                    "label": 0
                },
                {
                    "sent": "The pulling here takes all of those feature vectors for the entire image and just average them.",
                    "label": 0
                },
                {
                    "sent": "Or you know, combine them in some way.",
                    "label": 0
                },
                {
                    "sent": "So now what you get is a histogram of words and you can feed that to your favorite classifier and the goal of this is to basically give you shift invariant, so once you do this aggregation, the position of a feature doesn't matter anymore.",
                    "label": 0
                },
                {
                    "sent": "It's just the presence of it matters.",
                    "label": 1
                },
                {
                    "sent": "OK, so you regroup things that were dissimilar.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out when you do something like sparse coding here, it works much better than if you do K means, because pass coding preserves a bit of similarity between things, right?",
                    "label": 0
                },
                {
                    "sent": "Which means just completely.",
                    "label": 0
                },
                {
                    "sent": "Explode",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have this manifold picture.",
                    "label": 0
                },
                {
                    "sent": "We have a bunch of data points and what we'd like is to kind of learn a function that gives us the dependencies between X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "Here in this case.",
                    "label": 0
                },
                {
                    "sent": "We're going to do is we're going to the sort of general framework.",
                    "label": 0
                },
                {
                    "sent": "I think that is the most appropriate for this is the sort of energy based framework which, consistent essentially learning an energy function, or think of it as a contrast function that tells you if.",
                    "label": 1
                },
                {
                    "sent": "I give you a point and the contrast function gives you a scalar that tells you whether you are on the manifold or not on the manifold.",
                    "label": 0
                },
                {
                    "sent": "An if the function is.",
                    "label": 0
                },
                {
                    "sent": "Is able to do this internally.",
                    "label": 0
                },
                {
                    "sent": "There has to be able to kind of do this.",
                    "label": 0
                },
                {
                    "sent": "Disentangled features right it has to be able to tell in which direction the manifold is.",
                    "label": 0
                },
                {
                    "sent": "You know what's the closest point on the manifold and where am I going on the manifold and things like this, right?",
                    "label": 0
                },
                {
                    "sent": "So that's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to train the system to produce a single scalar and the scale is going to be some sort of contrast function that tells you how far away we are from the manifold of data.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's say we take samples coming from this parallel here and run PCA PCA with only one dimension.",
                    "label": 0
                },
                {
                    "sent": "So it's a 2 two input problem to output problem and we do PCA with one principal component.",
                    "label": 0
                },
                {
                    "sent": "PCA will find a main axis here over the point Cloud, and the reconstruction error would be 0 for anything that's on the principal axis, and we grow quadratically as we move away from it.",
                    "label": 0
                },
                {
                    "sent": "If you spot coding.",
                    "label": 0
                },
                {
                    "sent": "It will sort of wrap up the the set of points into a kind of union of planes.",
                    "label": 0
                },
                {
                    "sent": "If you want a union of.",
                    "label": 0
                },
                {
                    "sent": "Lines.",
                    "label": 0
                },
                {
                    "sent": "So that the sum of the distance to all the lines and ends up being the function we were looking for.",
                    "label": 0
                },
                {
                    "sent": "K means we'll just put a whole bunch of prototypes all around the surface.",
                    "label": 0
                },
                {
                    "sent": "This seems perfect, but it doesn't work in high dimension of course.",
                    "label": 0
                },
                {
                    "sent": "OK, so this.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Straight on this idea of sparse coding, so sparse coding is uses this energy function of this type, which has two arguments.",
                    "label": 0
                },
                {
                    "sent": "One is the input and the other one is a code vector, which is really a latent variable.",
                    "label": 0
                },
                {
                    "sent": "They were going to infer, and the energy function is the square reconstruction error where we multiply the code vector by some matrix called the dictionary matrix, and then we add a regularization term which is the sum of the absolute values of the components of the code.",
                    "label": 0
                },
                {
                    "sent": "If we know the dictionary matrix, given a why we find the Z here that minimizes this.",
                    "label": 0
                },
                {
                    "sent": "The energy function and that gives us our feature vector is going to be high dimensional, sparse, OK.",
                    "label": 0
                },
                {
                    "sent": "So it has it's a nonlinear mapping, the one that Maps Y to the optimal Z is nonlinear mapping.",
                    "label": 0
                },
                {
                    "sent": "But it's slower to compute because you have to do this augment operation.",
                    "label": 0
                },
                {
                    "sent": "We can run the dictionary matrix.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "We've also Ninfield told us how to do this very long time ago using just edged basically just stochastic gradient descent to minimize the overall average energy of a collection of samples.",
                    "label": 0
                },
                {
                    "sent": "Training samples.",
                    "label": 0
                },
                {
                    "sent": "The columns of W have to be normalized for this to work.",
                    "label": 0
                },
                {
                    "sent": "And the manifold assumption that's hidden behind the sparse coding is basically that the data is fits sort of a union of flow dimensional planes where the dimension of each plane corresponds to the number of components in Z that are non zero.",
                    "label": 0
                },
                {
                    "sent": "Once we do this minimization.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, though, we have several ways of designing unsupervised learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "One is number one is to construct the energy function so that the volume of stuff that takes low energy is fixed.",
                    "label": 1
                },
                {
                    "sent": "'cause the problem we have is that we have to build this energy function so that it has low energy on the points on the data points on the manifold, but higher energy everywhere else and it's easy enough to treat the parameters of an energy function to take low values on the points.",
                    "label": 0
                },
                {
                    "sent": "OK, just do a quick descent.",
                    "label": 0
                },
                {
                    "sent": "But then how do you make sure the energy is high everywhere else?",
                    "label": 0
                },
                {
                    "sent": "And that's basically it's basically the the partition function problem.",
                    "label": 0
                },
                {
                    "sent": "You know the people have been hitting in graphical models and stuff like that is very similar in spirit, so you want to contract when method one strategy is constructed energy so that the volume of low energy stuff is fixed so that if you give low energy to some points the other points will automatically have energy.",
                    "label": 0
                },
                {
                    "sent": "PCA is one of those, K means is one of those.",
                    "label": 0
                },
                {
                    "sent": "The second strategy is to push down on the energy of the samples and push up on the energy of everything else.",
                    "label": 1
                },
                {
                    "sent": "This is a strategy used by contrastive divergent.",
                    "label": 0
                },
                {
                    "sent": "Which is used to try and restricted Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "This is also a strategy used by maximum likelihood when you have a log partition function as a contrastive term that pushes up on the energy of other stuff, but it's very expensive to do if your partition function is not tractable, doesn't have a tractable gradient.",
                    "label": 0
                },
                {
                    "sent": "So you choose Monte Carlo methods and stuff, and the third one is to use a regularizer to limit the volume of stuff that can take low energy.",
                    "label": 1
                },
                {
                    "sent": "So basically build your model in such a way that it has a regularizer and by minimizing this regularizer is sort of shrink wraps the stuff of low energy into sort of a small value if you want, and that's basically what sparse coding does.",
                    "label": 0
                },
                {
                    "sent": "The sparse the sparsity term limits the volume of stuff that is allowed to take low energy.",
                    "label": 0
                },
                {
                    "sent": "That's essentially what it does.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this fast coding method is what we call the decoder.",
                    "label": 0
                },
                {
                    "sent": "Only method that goes from has a simple function to compute that goes from the code to the input, which means you have to run an optimization algorithm to figure out the optimal code for a given input.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to do is we're going to add to this sort of feedforward function that goes from the input to the code and we will attempt to predict where the optimal code is, and this function would necessarily have to be nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is frustrating.",
                    "label": 0
                },
                {
                    "sent": "We call this PSD, which means predicting sparsity composition.",
                    "label": 0
                },
                {
                    "sent": "We could give, you know, various architectures for this.",
                    "label": 0
                },
                {
                    "sent": "Let's say a simple neural net or something.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so here is.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm running this PS yoga rhythm running on a set of natural image patches and what I'm representing here are each square.",
                    "label": 0
                },
                {
                    "sent": "Here is a column of the WE matrix represented as as an image of the same size as the input.",
                    "label": 0
                },
                {
                    "sent": "So you get the end of training.",
                    "label": 0
                },
                {
                    "sent": "You get sort of oriented toward that detectors, which is what you are more or less expected.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to design this encoder this G function.",
                    "label": 0
                },
                {
                    "sent": "Phone call Gregor has this really brilliant idea of essentially emulating an algorithm that we know will produce the optimal Z called Fisto orista iterative shrinkage thresholding algorithm.",
                    "label": 0
                },
                {
                    "sent": "And the East algorithm basically is this kind of iteration where you take the input your multiplied by an encoding matrix and then you pass it through a shrinkage shrinkage function.",
                    "label": 0
                },
                {
                    "sent": "Each component multiplied by square matrix, which is you can think of as a lateral inversion matrix added to the previous result you had and iterate this group so it's kind of this recurrence here.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to learn instead of using.",
                    "label": 0
                },
                {
                    "sent": "You know this definition for S and that definition for WE, which is going to learn S&WE.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so you can think of this as a recurrent neural net or some kind with two matrices WENS, and we're going to train this recurrent neural net which has a bounded complexity to do the best approximation it can of the optimal passcode.",
                    "label": 1
                },
                {
                    "sent": "And this works really well in interest of time.",
                    "label": 0
                },
                {
                    "sent": "I'm going to skip the result.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just to say that we've applied this to kind of various things, including one where we have criteria that combined this reconstruction, the sparsity as well as the sort of prediction as a recent paper by Jason Wolfe, myself, actually actually conference and.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This produces very interesting results that we've never seen before in in neural Nets.",
                    "label": 0
                },
                {
                    "sent": "For for recognition, in which inputs are kind of decomposed into sort of sort of a prototype if you want plus a whole bunch of little modifiers.",
                    "label": 0
                },
                {
                    "sent": "This prototype that turn the prototype into the observed input.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to go into the details.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I can tell you more about this later if you want, if you're interested.",
                    "label": 0
                },
                {
                    "sent": "Another version of this uses convolution, so instead of using sparse coding as a linear, just a matrix, view it as a bunch of convolutions applied to feature Maps to reconstruct the input.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also use this PSD algorithm and the result of this for various reasons that I'm not going to explain too much is that we get much more diverse filters, largely because the system is trying out the image level and doesn't need to learn translated versions of all the filters and so it ends up having more resources to spend on learning very diverse filters like centers around crosses and corners and gratings of various kinds.",
                    "label": 0
                },
                {
                    "sent": "So you get much more diverse filter is doing this than you.",
                    "label": 0
                },
                {
                    "sent": "Get with standard Patch based prosecuting.",
                    "label": 1
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that's another example here.",
                    "label": 0
                },
                {
                    "sent": "As we increase the number of number of filters.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how do we?",
                    "label": 0
                },
                {
                    "sent": "How do we use this to train to pretraining convolutional net?",
                    "label": 0
                },
                {
                    "sent": "We will take one of those PSD sparse autoencoder.",
                    "label": 0
                },
                {
                    "sent": "This is really a sparsity encoder in the convolutional form to pretend the filters of convolutional net and so wrapped into this encoder.",
                    "label": 0
                },
                {
                    "sent": "We have the input, the filter bank and the nonlinearity and the pulling really resides in the next stage.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And once we are happy with it, we get rid of the decoder we we just run through the encoder runner training set for this and then.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick a second stage of that.",
                    "label": 0
                },
                {
                    "sent": "Train this.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised then.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get rid of the feedback again, just keep the feed forward path.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stick your classifier on top and now what we have is a full competition that we can train supervised, but it's been pre trained to essentially carry as much information as possible about the input all the way to the output and so we start from a pretty good place so it turns out.",
                    "label": 0
                },
                {
                    "sent": "This really helps in situations where the task you're trying to learn is very very poor in terms of the diversity of labels that you have.",
                    "label": 0
                },
                {
                    "sent": "But for a big data set like image net, it doesn't seem to make much of a difference.",
                    "label": 0
                },
                {
                    "sent": "Perhaps in speed of learning we will still exploring this.",
                    "label": 0
                },
                {
                    "sent": "We don't know, but.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things like pedestrian detection, so pedestrian detection is a case where you have lots of images are very diverse.",
                    "label": 1
                },
                {
                    "sent": "The background category is a huge amount of variability in it, but you only have two labels.",
                    "label": 0
                },
                {
                    "sent": "It's either a pedestrian or not, and if you're trying to connect to do this, the features you get at the top layer really bad because the label doesn't give you any information really about what's in the image, only gives you one bit of information, so using this free training actually helps to feature be more kind of generic.",
                    "label": 0
                },
                {
                    "sent": "If you want an actually makes a difference in performance so.",
                    "label": 0
                },
                {
                    "sent": "This is a false negative versus false positive rate, so this is the false positive for image.",
                    "label": 0
                },
                {
                    "sent": "This is 1 false positive for image right here.",
                    "label": 0
                },
                {
                    "sent": "And this is the miss rate.",
                    "label": 0
                },
                {
                    "sent": "And all of those curves are from all kinds of systems publishing literature.",
                    "label": 0
                },
                {
                    "sent": "This is a commercial net that's been trained, purely supervised, and this is the same one that's been pre trained with this unsupervised training.",
                    "label": 0
                },
                {
                    "sent": "And then refine supervised so it makes quite a big difference.",
                    "label": 0
                },
                {
                    "sent": "He goes from, you know, middle of the pack to basically record Holder for this data set.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The in real datasets is going faster as you get after training.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This video of the system in action there's an important point to make.",
                    "label": 0
                },
                {
                    "sent": "Which is that is very easy to turn a localized recognizer or detector.",
                    "label": 0
                },
                {
                    "sent": "That is a combination that into a data into a full fledged detector on a full image is very cheap to do this.",
                    "label": 0
                },
                {
                    "sent": "It's a point that a lot of people miss.",
                    "label": 0
                },
                {
                    "sent": "But it's really the case is not there.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Apple this is in front of a lab here.",
                    "label": 0
                },
                {
                    "sent": "We kind of lowered the threshold so that we get some of the false positives to see what they look like.",
                    "label": 0
                },
                {
                    "sent": "There are a few.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are various forms of this.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of invariant recognition where we build the pooling inside of this regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For things like that, where we can organize the features into topographic mapping, we do a group sparsity on groups that overlap each other, and the system organizes itself so that features that fire together end up being in the same groups, and so you get this sort of nice looking topographic Maps which basically are not particularly interesting for people like us.",
                    "label": 0
                },
                {
                    "sent": "The machine learning computer vision, but they really kind of.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Resonate with your friends in neuroscience.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know they produce nice code, pictures which.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Showing to you without explaining.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to go any further than that and because I'm out of time and I'd like to mention sort of a few issue, I think that are somewhat theoretical nature that we really don't understand about deep learning.",
                    "label": 0
                },
                {
                    "sent": "The first one is what is the nature of the.",
                    "label": 0
                },
                {
                    "sent": "Loss function that we minimizing with deep learning is kind of practical problem.",
                    "label": 0
                },
                {
                    "sent": "Practical optimization problem if you will.",
                    "label": 0
                },
                {
                    "sent": "Essentially when you train a deep neural net, the cost function has lots and lots of set of points.",
                    "label": 0
                },
                {
                    "sent": "It's not just the local minima that are annoying.",
                    "label": 0
                },
                {
                    "sent": "Is the set of points.",
                    "label": 0
                },
                {
                    "sent": "Because there's a big issue of breaking symmetries.",
                    "label": 0
                },
                {
                    "sent": "You know, deciding if one unit should do something and the other unit should do something complementary, or it should be the other way around, so the symmetry breaking issues, which are which are the issues that limit the speed of training.",
                    "label": 0
                },
                {
                    "sent": "Coalition that traditionally haven't been victim of this, which is why they've been one of the few very deep neural Nets that have been used for a long time is because they don't have that symmetry breaking problem to the same extent they have.",
                    "label": 0
                },
                {
                    "sent": "They have it, but not to the same extent, so it's kind of an issue.",
                    "label": 0
                },
                {
                    "sent": "Other sort of issue is.",
                    "label": 0
                },
                {
                    "sent": "Something we didn't understand 20 years ago is that the systems that seem to work best are the ones that are ridiculously over parameterized.",
                    "label": 0
                },
                {
                    "sent": "We make the networks bigger and they just work better even if we have limited amounts of data.",
                    "label": 0
                },
                {
                    "sent": "What we have to do is just regularize the hell out of out of them.",
                    "label": 0
                },
                {
                    "sent": "Using things like dropout and things of that type.",
                    "label": 0
                },
                {
                    "sent": "But even if we don't regularize them, they work surprisingly well and we don't understand why the affective dimension of those things is much lower than what we think.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "Then I think the biggest, the biggest issue, I think, is how do we formalize unsupervised learning really is good criterion for training and supervised system.",
                    "label": 0
                },
                {
                    "sent": "It's always been a problem of mine to figure out, you know how you even test if an unsupervised learning system works.",
                    "label": 0
                },
                {
                    "sent": "Right, what's the criterion an the criterion is, you know, trying to extract features and then trying to classify on the features and see if it works right?",
                    "label": 0
                },
                {
                    "sent": "That at least that's an objective criterion.",
                    "label": 0
                },
                {
                    "sent": "But it's not a good criterion to use for training unsupervised, 'cause you're not allowed to use the labels.",
                    "label": 0
                },
                {
                    "sent": "So how do we design loss functions for unsupervised learning?",
                    "label": 0
                },
                {
                    "sent": "How do we and what are the principles on which it would be based so this energy based stuff is a little more general than the usual sort of maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Asians probably formulations, but you know, maybe it's not the right one.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}