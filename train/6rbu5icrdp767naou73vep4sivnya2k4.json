{
    "id": "6rbu5icrdp767naou73vep4sivnya2k4",
    "title": "Fast DTT - A Near Linear Algorithm for Decomposing a Tensor into Factor Tensors",
    "info": {
        "author": [
            "Xiaomin Fang, Sun Yat-Sen University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_fang_factor_tensors/",
    "segmentation": [
        [
            "I'm shopping for a sign saying refers to.",
            "I'm glad to be here to show you our work fast beauty and nearly any algorithm for decomposing a tensor into factor tensors."
        ],
        [
            "Now let's start with the introduction to matrix and tensor.",
            "Most job you must be familiar with matrix, which can model the 2nd order relations.",
            "Work sample fight recommendation matrix can model the relation between users and items and for free recommendation matrix can model the relation between friends."
        ],
        [
            "But if you want to model the higher order relations, we need cancer.",
            "Pencil is an extension to matrix work sample with personalized tag recommendation problem that users can annotate items with text so that those information can organize into a tensor.",
            "Similarly for personalized search problem, we can use 10 search model.",
            "The relation between users, queries and URLs."
        ],
        [
            "Now we checked writer recommendation for example to explain matrix vectorization, matrix vectorization techniques, decomposed and user item matrix into two smaller vector matrices, one for the users and the other one for the items.",
            "Now we want to explain the meaning of each element in the vector matrices.",
            "What do you suspect of matrices?",
            "Nisero drop the use of vector matrix represents the hidden features of your side.",
            "We can simply assume that all the items can be divided into several item groups so that each item group corresponds to a column in the matrix.",
            "Now we can explain the meaning of the element now.",
            "The blue cell means the relation between that user ID and item one."
        ],
        [
            "For the following slides we will check personalized recommendation problem as example to explain different tensor factorization techniques.",
            "By making use of their history annotation data, we can Mail their user item tag tensor so that the problem can be divided as given.",
            "Use I an item J. Warner predicted rather use I will annotate item J.",
            "We have our work tax."
        ],
        [
            "So the traditional tensor conversation techniques CP antide decompose a tensor into several vector matrices.",
            "For example, CPT composes insert order tensor A into three factor matrices.",
            "We can assume that one for the users, one for the items, and raining, one for the tax, while TD decomposes a third order tensor into a three factor matrices, and I can't answer."
        ],
        [
            "As traditional times of factorization techniques, they compose a tensor interaction matrices and users hidden features are represented by a vector.",
            "Now we want to explain the meaning of drawn elements just like matrix factorization.",
            "We can assume that all the items can be divided into several groups and all the text can be divided into several tab groups.",
            "But this charm we cannot explain the manage optimal element.",
            "The blue cell doesn't match the relation between the users and the item.",
            "Grow one and it also means the relation between that lutherin Packer one neither."
        ],
        [
            "So we have the question how to design A tensor factorization model that is explainable alright here is DT.",
            "To decompose a tensor into factor tensors.",
            "Rather than fighting Microsoft so in the graph you can see that we could compose a third order tensor a into three factual tensors, XY and Z."
        ],
        [
            "So for dictating that, compose a tensor into factor tensors.",
            "Now we use those hidden features are represented by a matrix instead.",
            "And this time we can explain a million jobs each element.",
            "The Blue Room is the relation between that you are certain item Room 1.",
            "Under a column means the relation between that user and tackle one.",
            "So the green cell Mr relation between that dresser I want at work one.",
            "Suspected composing attention into factor tensors, we can now explain the meaning of each element into factor tensors.",
            "The model become explainable."
        ],
        [
            "Now let's introduce the team more details.",
            "Now we want to predict value jump AIJK, which means that wander predict weather use.",
            "I will annotate item J with testing in the future they input contains 3 matrices.",
            "The matrix of user I the matrix represent item J and matrix or represent Attack key.",
            "Now in this example.",
            "I'll three also groups for item group San 5 tag groups."
        ],
        [
            "In the first step, we computed the matrix product of the vector matrix up inside and a factor matrix of item J.",
            "In this step we can remove all the item groups and we can and then we get a new matrix in the bottom left that represents those sociation between user ID and item J."
        ],
        [
            "Then in the second step we transpose Daniel Matrix to make its dimension to be the same as the matrix of techie."
        ],
        [
            "Finally, in step three, we compared to the drop product that new matrix of that new matrix and the matrix of techie to predict the value job AI JK."
        ],
        [
            "Now let's do some mathematic.",
            "The prediction shop AI JK can be divided as this expression as you can save an expression that the subscribers are symmetric.",
            "In other words, the order of the vector matrix.",
            "Obvious I item Genentech case makes no difference.",
            "We can compute the relation between user I an item J first or you can compute the relation between inducer Antec verse.",
            "Or if you like, you can compute on the relation between tech an user first or the order doesn't matter."
        ],
        [
            "Also dating has some good properties.",
            "It contains some limitation.",
            "The time complexity is showed here that."
        ],
        [
            "Which is impractical for large scale real problems.",
            "Besides, as Stern users, features are not represented by matrix rather than vector, so that there are too many parameters in DT which may later over between.",
            "So which overcomes all this crap.",
            "All these problems we proposed Boston teaching that further decompose each slide or better tensors.",
            "Engine low rank matrices.",
            "So for example, the user spectral tensor, a slide job user spectral tensor matrix representing hidden features of some user to reduce the number of features, we decompose that matrix into 2 low rank matrix this."
        ],
        [
            "Now, this charm AIK campaign returned as the following expression after organized update formula.",
            "Oh, it takes that time complexity to compute AI JK.",
            "So by fast DT we can reduce the time and number job parameters.",
            "DTT"
        ],
        [
            "Now let's compare the time and space complexity.",
            "Of our proposed model FAST DT and traditional tensor vectorization techniques.",
            "TD and CP for the convenience of an analysis, we make some assumption that general job users, items and tax all the same and number of user groups, item groups and tag groups are the same as well.",
            "Notice that.",
            "The Smolny is much smaller than the capital D so that this group can be considered as an constant.",
            "The complexity of fast DT is between that of the TD and CP.",
            "And this very complexity of DT is between that job.",
            "Tedious CPS will."
        ],
        [
            "Now we come to the experiment part.",
            "We conducted some experiments on both synthetic data set on Realtek annotation datasets.",
            "We will show the performance on this datasets in the following slides for the objective function, will use BPR Bayesian personalized ranking function and commonly used pairwise ranking function.",
            "Besides respired matter last ready to optimize the objective function and use information retrieval matrix MAP&N DCG between evaluate the performance."
        ],
        [
            "We compared frustrating, with several baselines.",
            "The baselines can be divided into three categories.",
            "For the TD based models we have a lot of an actual I sweetie for the CP based models we have PTF and CP for the popularity algorithm rank the text based on the frequency that item and tekoa curd in the training set."
        ],
        [
            "We conduct some experiments on synthetic data set to evaluate different types of vectorization models.",
            "Capacity of expression.",
            "So since we have assumed that the users, items and text can be divided into several groups, so in the synthetic datasets we randomly assigned each user item tag Troy Group.",
            "We assume that new source in eyes, user group always annotate the items in the dress item group with the text energy job address tag groups actually actually is generated randomly."
        ],
        [
            "Now the performance on the synthetic Network says it showed it on the graph.",
            "As you can see that our proposed model Fast GT is much better than other tensor vectorization models so that we believe that fast DT has stronger capacity of expression than others other models."
        ],
        [
            "Then we conduct some experiments on three real Tek annotation datasets.",
            "Delicious last album and movie lines which are public.",
            "We used a car based approach to remove infrequent users, items and tags until every user item, Zantac or curd in at least five triplets."
        ],
        [
            "Now the performance on the real data sets are shown at in the table.",
            "As you can see that our proposed model was DT performance better than other models.",
            "Most of the time."
        ],
        [
            "We also conduct some experiments to investigate the influence influence of order.",
            "Number two of the features to DT.",
            "Run the graph.",
            "You can see that the prediction quality of the model with small D equals one is compatible with the models with small the koscho on all three datasets.",
            "We can see from the graph that.",
            "There are always true allies to stick together.",
            "So that our model is nearly new."
        ],
        [
            "Now let's summarize.",
            "Refers proposed a model DTT that, unlike traditional types of factorization techniques that compose a tensor into factor matrices with compose a tensor into factor cancers, so date it is more natural for the representation.",
            "Jump higher order relations becausw.",
            "We can explain the manage of each element in the factory tensors no.",
            "Besides dictating stronger capacity voice of expression.",
            "Enter, there is still time complexity to predict an element and the number of parameters we proposed fast DT that further decompose each slide job accurate answers into 2 low rank matrices which make the problem is near linear and practical for large scale real problems."
        ],
        [
            "OK, that's all for my child.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm shopping for a sign saying refers to.",
                    "label": 0
                },
                {
                    "sent": "I'm glad to be here to show you our work fast beauty and nearly any algorithm for decomposing a tensor into factor tensors.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's start with the introduction to matrix and tensor.",
                    "label": 0
                },
                {
                    "sent": "Most job you must be familiar with matrix, which can model the 2nd order relations.",
                    "label": 0
                },
                {
                    "sent": "Work sample fight recommendation matrix can model the relation between users and items and for free recommendation matrix can model the relation between friends.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But if you want to model the higher order relations, we need cancer.",
                    "label": 0
                },
                {
                    "sent": "Pencil is an extension to matrix work sample with personalized tag recommendation problem that users can annotate items with text so that those information can organize into a tensor.",
                    "label": 1
                },
                {
                    "sent": "Similarly for personalized search problem, we can use 10 search model.",
                    "label": 0
                },
                {
                    "sent": "The relation between users, queries and URLs.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we checked writer recommendation for example to explain matrix vectorization, matrix vectorization techniques, decomposed and user item matrix into two smaller vector matrices, one for the users and the other one for the items.",
                    "label": 1
                },
                {
                    "sent": "Now we want to explain the meaning of each element in the vector matrices.",
                    "label": 0
                },
                {
                    "sent": "What do you suspect of matrices?",
                    "label": 0
                },
                {
                    "sent": "Nisero drop the use of vector matrix represents the hidden features of your side.",
                    "label": 0
                },
                {
                    "sent": "We can simply assume that all the items can be divided into several item groups so that each item group corresponds to a column in the matrix.",
                    "label": 0
                },
                {
                    "sent": "Now we can explain the meaning of the element now.",
                    "label": 0
                },
                {
                    "sent": "The blue cell means the relation between that user ID and item one.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the following slides we will check personalized recommendation problem as example to explain different tensor factorization techniques.",
                    "label": 0
                },
                {
                    "sent": "By making use of their history annotation data, we can Mail their user item tag tensor so that the problem can be divided as given.",
                    "label": 1
                },
                {
                    "sent": "Use I an item J. Warner predicted rather use I will annotate item J.",
                    "label": 0
                },
                {
                    "sent": "We have our work tax.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the traditional tensor conversation techniques CP antide decompose a tensor into several vector matrices.",
                    "label": 1
                },
                {
                    "sent": "For example, CPT composes insert order tensor A into three factor matrices.",
                    "label": 0
                },
                {
                    "sent": "We can assume that one for the users, one for the items, and raining, one for the tax, while TD decomposes a third order tensor into a three factor matrices, and I can't answer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As traditional times of factorization techniques, they compose a tensor interaction matrices and users hidden features are represented by a vector.",
                    "label": 0
                },
                {
                    "sent": "Now we want to explain the meaning of drawn elements just like matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "We can assume that all the items can be divided into several groups and all the text can be divided into several tab groups.",
                    "label": 0
                },
                {
                    "sent": "But this charm we cannot explain the manage optimal element.",
                    "label": 0
                },
                {
                    "sent": "The blue cell doesn't match the relation between the users and the item.",
                    "label": 1
                },
                {
                    "sent": "Grow one and it also means the relation between that lutherin Packer one neither.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have the question how to design A tensor factorization model that is explainable alright here is DT.",
                    "label": 0
                },
                {
                    "sent": "To decompose a tensor into factor tensors.",
                    "label": 1
                },
                {
                    "sent": "Rather than fighting Microsoft so in the graph you can see that we could compose a third order tensor a into three factual tensors, XY and Z.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for dictating that, compose a tensor into factor tensors.",
                    "label": 1
                },
                {
                    "sent": "Now we use those hidden features are represented by a matrix instead.",
                    "label": 0
                },
                {
                    "sent": "And this time we can explain a million jobs each element.",
                    "label": 1
                },
                {
                    "sent": "The Blue Room is the relation between that you are certain item Room 1.",
                    "label": 0
                },
                {
                    "sent": "Under a column means the relation between that user and tackle one.",
                    "label": 1
                },
                {
                    "sent": "So the green cell Mr relation between that dresser I want at work one.",
                    "label": 0
                },
                {
                    "sent": "Suspected composing attention into factor tensors, we can now explain the meaning of each element into factor tensors.",
                    "label": 0
                },
                {
                    "sent": "The model become explainable.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's introduce the team more details.",
                    "label": 0
                },
                {
                    "sent": "Now we want to predict value jump AIJK, which means that wander predict weather use.",
                    "label": 0
                },
                {
                    "sent": "I will annotate item J with testing in the future they input contains 3 matrices.",
                    "label": 0
                },
                {
                    "sent": "The matrix of user I the matrix represent item J and matrix or represent Attack key.",
                    "label": 0
                },
                {
                    "sent": "Now in this example.",
                    "label": 0
                },
                {
                    "sent": "I'll three also groups for item group San 5 tag groups.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the first step, we computed the matrix product of the vector matrix up inside and a factor matrix of item J.",
                    "label": 0
                },
                {
                    "sent": "In this step we can remove all the item groups and we can and then we get a new matrix in the bottom left that represents those sociation between user ID and item J.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then in the second step we transpose Daniel Matrix to make its dimension to be the same as the matrix of techie.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, in step three, we compared to the drop product that new matrix of that new matrix and the matrix of techie to predict the value job AI JK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's do some mathematic.",
                    "label": 0
                },
                {
                    "sent": "The prediction shop AI JK can be divided as this expression as you can save an expression that the subscribers are symmetric.",
                    "label": 1
                },
                {
                    "sent": "In other words, the order of the vector matrix.",
                    "label": 1
                },
                {
                    "sent": "Obvious I item Genentech case makes no difference.",
                    "label": 1
                },
                {
                    "sent": "We can compute the relation between user I an item J first or you can compute the relation between inducer Antec verse.",
                    "label": 0
                },
                {
                    "sent": "Or if you like, you can compute on the relation between tech an user first or the order doesn't matter.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also dating has some good properties.",
                    "label": 0
                },
                {
                    "sent": "It contains some limitation.",
                    "label": 0
                },
                {
                    "sent": "The time complexity is showed here that.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is impractical for large scale real problems.",
                    "label": 0
                },
                {
                    "sent": "Besides, as Stern users, features are not represented by matrix rather than vector, so that there are too many parameters in DT which may later over between.",
                    "label": 0
                },
                {
                    "sent": "So which overcomes all this crap.",
                    "label": 0
                },
                {
                    "sent": "All these problems we proposed Boston teaching that further decompose each slide or better tensors.",
                    "label": 1
                },
                {
                    "sent": "Engine low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "So for example, the user spectral tensor, a slide job user spectral tensor matrix representing hidden features of some user to reduce the number of features, we decompose that matrix into 2 low rank matrix this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, this charm AIK campaign returned as the following expression after organized update formula.",
                    "label": 0
                },
                {
                    "sent": "Oh, it takes that time complexity to compute AI JK.",
                    "label": 0
                },
                {
                    "sent": "So by fast DT we can reduce the time and number job parameters.",
                    "label": 0
                },
                {
                    "sent": "DTT",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's compare the time and space complexity.",
                    "label": 0
                },
                {
                    "sent": "Of our proposed model FAST DT and traditional tensor vectorization techniques.",
                    "label": 0
                },
                {
                    "sent": "TD and CP for the convenience of an analysis, we make some assumption that general job users, items and tax all the same and number of user groups, item groups and tag groups are the same as well.",
                    "label": 0
                },
                {
                    "sent": "Notice that.",
                    "label": 0
                },
                {
                    "sent": "The Smolny is much smaller than the capital D so that this group can be considered as an constant.",
                    "label": 0
                },
                {
                    "sent": "The complexity of fast DT is between that of the TD and CP.",
                    "label": 1
                },
                {
                    "sent": "And this very complexity of DT is between that job.",
                    "label": 0
                },
                {
                    "sent": "Tedious CPS will.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we come to the experiment part.",
                    "label": 0
                },
                {
                    "sent": "We conducted some experiments on both synthetic data set on Realtek annotation datasets.",
                    "label": 0
                },
                {
                    "sent": "We will show the performance on this datasets in the following slides for the objective function, will use BPR Bayesian personalized ranking function and commonly used pairwise ranking function.",
                    "label": 1
                },
                {
                    "sent": "Besides respired matter last ready to optimize the objective function and use information retrieval matrix MAP&N DCG between evaluate the performance.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compared frustrating, with several baselines.",
                    "label": 0
                },
                {
                    "sent": "The baselines can be divided into three categories.",
                    "label": 0
                },
                {
                    "sent": "For the TD based models we have a lot of an actual I sweetie for the CP based models we have PTF and CP for the popularity algorithm rank the text based on the frequency that item and tekoa curd in the training set.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We conduct some experiments on synthetic data set to evaluate different types of vectorization models.",
                    "label": 0
                },
                {
                    "sent": "Capacity of expression.",
                    "label": 0
                },
                {
                    "sent": "So since we have assumed that the users, items and text can be divided into several groups, so in the synthetic datasets we randomly assigned each user item tag Troy Group.",
                    "label": 1
                },
                {
                    "sent": "We assume that new source in eyes, user group always annotate the items in the dress item group with the text energy job address tag groups actually actually is generated randomly.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the performance on the synthetic Network says it showed it on the graph.",
                    "label": 0
                },
                {
                    "sent": "As you can see that our proposed model Fast GT is much better than other tensor vectorization models so that we believe that fast DT has stronger capacity of expression than others other models.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we conduct some experiments on three real Tek annotation datasets.",
                    "label": 0
                },
                {
                    "sent": "Delicious last album and movie lines which are public.",
                    "label": 0
                },
                {
                    "sent": "We used a car based approach to remove infrequent users, items and tags until every user item, Zantac or curd in at least five triplets.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the performance on the real data sets are shown at in the table.",
                    "label": 0
                },
                {
                    "sent": "As you can see that our proposed model was DT performance better than other models.",
                    "label": 0
                },
                {
                    "sent": "Most of the time.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also conduct some experiments to investigate the influence influence of order.",
                    "label": 0
                },
                {
                    "sent": "Number two of the features to DT.",
                    "label": 1
                },
                {
                    "sent": "Run the graph.",
                    "label": 0
                },
                {
                    "sent": "You can see that the prediction quality of the model with small D equals one is compatible with the models with small the koscho on all three datasets.",
                    "label": 1
                },
                {
                    "sent": "We can see from the graph that.",
                    "label": 0
                },
                {
                    "sent": "There are always true allies to stick together.",
                    "label": 1
                },
                {
                    "sent": "So that our model is nearly new.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's summarize.",
                    "label": 0
                },
                {
                    "sent": "Refers proposed a model DTT that, unlike traditional types of factorization techniques that compose a tensor into factor matrices with compose a tensor into factor cancers, so date it is more natural for the representation.",
                    "label": 1
                },
                {
                    "sent": "Jump higher order relations becausw.",
                    "label": 0
                },
                {
                    "sent": "We can explain the manage of each element in the factory tensors no.",
                    "label": 1
                },
                {
                    "sent": "Besides dictating stronger capacity voice of expression.",
                    "label": 1
                },
                {
                    "sent": "Enter, there is still time complexity to predict an element and the number of parameters we proposed fast DT that further decompose each slide job accurate answers into 2 low rank matrices which make the problem is near linear and practical for large scale real problems.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's all for my child.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}