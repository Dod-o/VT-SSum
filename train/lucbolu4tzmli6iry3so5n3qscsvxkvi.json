{
    "id": "lucbolu4tzmli6iry3so5n3qscsvxkvi",
    "title": "Hierarchical Maximum Entropy Density Estimation",
    "info": {
        "author": [
            "Miroslav Dud\u00edk, Princeton University"
        ],
        "published": "June 23, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Mathematics->Statistics",
            "Top->Computer Science->Machine Learning->Density Estimation"
        ]
    },
    "url": "http://videolectures.net/icml07_dudik_hmed/",
    "segmentation": [
        [
            "So I'll be talking about how to estimate several several densities at the same time, and this densities will be organized in hierarchy, joint work with Dave and Dave client."
        ],
        [
            "Check for instance.",
            "So let me begin by by motivating it by by a single density estimation problem.",
            "The problem of estimating distribution of a single species, say all throated vireo over over a map.",
            "Our input consists of a set of occurrence localities or this bird was observed and a set of environmental variables.",
            "For example, annual precipitation altitude or the average daily temperature.",
            "Based on this, we would like to predict geographic distribution of this species and.",
            "There is a range of techniques to achieve this, and one of the most successful ones has been the regularised maximum entropy.",
            "The however, all of these techniques, including maximum entropy, have one big problem.",
            "It is a small number of samples, despite the regularization, we can't really estimate some of these species distributions and we are interested in estimating distributions of rare species because that's the speech that we want to save.",
            "So we want to know where to put National Park National parks.",
            "So."
        ],
        [
            "What do we do about this?",
            "So first observation is that typically the datasets consist of several species, and even though the number of observations for each individual piece species is small altogether, we."
        ],
        [
            "I have a decent datasets, for example, one of these datasets is that I'll be talking bout later in more detail contains 27 species and even though the number of samples per species ranges from 5 to 50 altogether, we have over 1000 samples, so we should be able to take advantage of this information in some way.",
            "So how can we do this?"
        ],
        [
            "In this talk, I'll be considering the problem of multiple density estimation where we are estimating density over sample space, which is a map divided into a discrete into discrete cells.",
            "I assume that the locations will be coming from the from the unknown density distribution and I will be placing no parametric assumption on these the.",
            "Furthermore, I'll assume that the dentist densities are organized into groups, such as the hierarchy that showing you before, and you probably notice that there were overlaps.",
            "So, for example, there might be nocturnal birds and birds of prey which are overlapping in some owls."
        ],
        [
            "The main results will be the generalization of the regularization of the regularizer, maximum entropy that has been successful in this area will.",
            "I'll introduce the hierarchical maximum entropy density estimation and the performance guarantees for this method, which will show how the information is actually shared among the groups and which will lead to systematic choice of hyperparameters.",
            "Then I'll demonstrate on the real world data that this.",
            "This works, and then I'll give also Bayesian interpretation of our framework."
        ],
        [
            "So what is the problem of Mount multiple density estimation?",
            "We're considering so as I said, the sample space is the map divided into a finite number of cells, and then we are given a set of classes organized into groups.",
            "For simplicity, I'll be just considering this very simple simple hierarchy which consists of two classes of class of red bird class of a Bluebird and a single group that contains just these two species.",
            "Then we are given a set of samples an I'll be writing these samples as spares where where X is location and Y is an identifier of the class so.",
            "I should really point out that we are not interested in classification, but it kind of looks like this.",
            "Why is simply an identifier and we're we're estimating the distributions over X is.",
            "Finally, the sample space is described by set of real value functions, which are features and.",
            "And in the examples that I'll be talking about, this set is finite, but it can be also infinite."
        ],
        [
            "So what this is our problem?",
            "We are given the set of observations a set of features and we would like to predict the individual class distributions.",
            "The maximum entropy principle is based on the following observations.",
            "We, for each class.",
            "We expect that the average of the feature that we observe is not too far from the true expectation.",
            "For example, the average altitude that we have seen among all the red birds should not be too far from the expected value that the altitude that the red bird likes.",
            "Similarly for the Bluebird.",
            "Therefore, it should intuitively make sense to be looking for distributions with respect this this.",
            "This constraint, however, since our sample is small, we do not.",
            "We do not want this constraint to be satisfied exactly, but only within some error bounds.",
            "Now I'll be formulating this problem actually join density estimation, but these constraints are conditional, so I have to I have to specify the individual class probabilities and perhaps a better way how to think about this is view them as the importance that we attached the individual classes.",
            "And well, I have to kind of explain the letters M1 will be the number of samples of the of the red bird and two number of samples of Bluebird.",
            "But in fact like just the map math will workout simpler, but but those can be any values that we consider as important of the image jewel.",
            "That's the estimation problems.",
            "So the problem is still under specified and the maximum entropy principle tells us that among all the distribution that satisfy this constraint, we should choose the one that maximizes the entropy.",
            "Now I've been telling that we should take advantage of the group information, but there is really no group information there yet, and in fact if we just formulate this problem, it will be equivalent to the individual density estimation problems.",
            "So people probably anticipate what follows next and we so we add a new set of constraints.",
            "For every groups in our.",
            "In our case, this is simply the constraint for for the group.",
            "For the group containing both the red and blue samples, since we do expect that now that the average altitude across all of the birds should not be too far from the true expected altitude that the that all of the words like.",
            "So let's take a look at this problem.",
            "Entropy is the is a concave function.",
            "We have fixed the class probabilities, so this all these constraints turn out to be convex.",
            "This is a convex convex problem, so we can just apply machinery of convex optimization to solve it and.",
            "If you applied and typically in the maximum entropy setting, we start by bulking at the jewel.",
            "So let's take a look."
        ],
        [
            "Put you on this problem is so.",
            "First of all, we know we find out that the individual class densities take a form of an exponential family distributions and.",
            "And this is not unexpected.",
            "This what we usually get in maximum entropy and the second part is.",
            "So what is it that we are maximizing?",
            "It turns out that the entropy from the previous slide is maximized by the joint density, which maximizes the likelihood plus minus minus a penalty.",
            "So the maximization is over three para meters.",
            "The 1st two are the class parameters and the third one is a group parameter.",
            "Now this metrics maximization encourages sharing.",
            "Of trade in a certain way, in particular the we are penalized for 4D for the class.",
            "For the class parameters, deviations from the group para meters and group parameters themselves are penalized by one norm.",
            "So we are we're maximizing the sum of likely, then something else.",
            "So so there's something else can be viewed as log of the prior so."
        ],
        [
            "Well, so so.",
            "So what is this prior?",
            "Like this prior amounts to 1st selecting a group parameter from from the Formula plus distribution, then selecting the individual class parameters.",
            "For from the.",
            "From Laplace distributions conditioned on the group and finally we select the individual samples from the from the class densities.",
            "So OK, so these models have been around for awhile, so So what?",
            "So what does maximum entropy bias?",
            "So what's new there the?",
            "The first insight is that the hyperparameters have a natural interpretation.",
            "Is uncertainties in the averages.",
            "The second benefit is that we get strong performance guarantees that do not pose."
        ],
        [
            "Assumptions on the unknown distributions that we are estimating.",
            "So let's let's let's get a flavor of what these performance guarantees are like.",
            "If we choose the appropriate parameters in the right way, then we can say that the maximum performance as measured by the test test log likelihood with respect to the true distribution, differs from the optimal performance by optimal.",
            "Here I mean the performance of the optimal exponential family distribution by by the term that we see.",
            "Do I have the?",
            "I think I have a pointer OK. Maybe I can OK so so by the term that we see that we see below this term takes form of is actually the regularization of the of the best exponential family distribution, which is the one denoted by the by those Lambda stars.",
            "And the nice thing about this gap that it does capture how how we achieve improvements due to the similarity within the group and due to the use of larger groups specifically and if we if we included no group information.",
            "Then the last term would be.",
            "Then the last thing would be 0 and the first 2 terms would be simply equal to the L1 norms.",
            "Now what happens if a certain set of para meters is similar?",
            "If the third set of parameters is similar, then then then we can simply set the Lambda 12 equal to that that similar set of parameters and we achieved a reduction in the first 2 terms that's proportional to the square root of the number of samples of 1st species.",
            "But the square root of the number of samples of the second species at the same time we are increasing this.",
            "Only by the amount that's proportional to the square root of the total number of samples.",
            "Now since the sum of the squares is larger than the square root, we are achieving an improvement.",
            "In this case, the improvement on two speeches is by by factor of sqrt 2.",
            "Now, if we have a larger group that shares and information that we are achieving an improvement, that's by the factor of square root of the number of species.",
            "In other words, both similarity and similarity within large groups will help the generalization performance, which is intuitively we expect.",
            "So this is all nice and good, so that's this.",
            "How does this perform?"
        ],
        [
            "In practice, so first let's take a look at the toy experiment in which we construct the synthetic distribution of a blue species and the first measure the performance of a single density distribution method.",
            "Without that doesn't use the group information, and then we see what happens if you add the group information will evaluate the performance as a function of the number of samples, and we are interested into into quantities.",
            "First is the relative entropy.",
            "The truth?",
            "In this case, the truth is from the exponential family.",
            "And then will be also taking a look at what happens to the parameter estimates."
        ],
        [
            "So we see that the single class method converges to the truth and also both of the parameters converge, which is, which is nice.",
            "So now what happens when we add the second species?",
            "The second species is a fixed number of samples to 100 and.",
            "And we see that the performance does of the blue species in this hierarchical setup does improve and improve is most dramatic on the small number of examples, which is indeed what we expect.",
            "So now where does this improvement come from?",
            "The improvement comes from the from the changes in the second parameter.",
            "This the precipitation parameter, and it turns out that both of these species like lower values of precipitation, and therefore the group parameter are shrunk towards the the.",
            "The perimeter of the of the blue speech is shrunk towards the perimeter of the rat species in in terms of temperature.",
            "They are.",
            "They like different kinds of temperature and the optimization therefore doesn't really affect the blue estimates."
        ],
        [
            "Now let's take a look at some real world data sets.",
            "The first of them is the data set of species from Australia with tropics.",
            "It contains 20 species and we have a set of three groups.",
            "The second data set contains species from North East NSW.",
            "27 species and 15 groups and this is the data set that I showed you in the first slide and really just wanting to notice that here.",
            "Heard the groups sound the groups are overlapping."
        ],
        [
            "So so here we again measured here.",
            "We just measured the improvement on the using the hierarchies, and each circle corresponds to one species and we report the difference from the from the case where we don't use the density information.",
            "So anything about the dashed line is an improvement.",
            "We see that in the first data set, the improvement is extremely consistent we are.",
            "Pretty much do almost always with the single exception.",
            "We do be do better, and the improvement is more dramatic on the small sample sizes, which is which will be expected, which is also what we saw in the synthetic experiment.",
            "The improvement on the second data set is.",
            "So very consistent even though less so than on the 1st and there.",
            "However, there we see that in one case we do significantly worse, so this is the case with the smallest number of examples.",
            "This we have only four 4 four sample points, and perhaps it suggests like where the limits of our of our techniques are getting at."
        ],
        [
            "Point so to sum up, I've introduced hierarchical maximum entropy, which is the principle approach to multiple density estimation.",
            "It allows sharing information across tasks, but Moreover it also.",
            "Explains how this information is shared by means of performance guarantees.",
            "This performance guarantees can be used as guides to setting hyperparameters, and we've seen that it is possible to obtain significant benefits on the real world data."
        ],
        [
            "Thank you for attention.",
            "Keep up with the question.",
            "So can we think of what you're doing as actually innocence, conditional density estimation, 'cause it's actually conditioned on knowing things like the temperature and rainfall at different locations.",
            "So if you had a cell where you didn't know the temperature or the rainfall, for example.",
            "Then you wouldn't be able to estimate the density of birds there, and can you extend it to be able to do that?",
            "Is that true?",
            "Or right this I am so?",
            "You might be able to correct for some missing information for some missing features, but I don't think I can think of would be the basis for predicting anything I'm missing.",
            "So in this in the distribution models you typically ignore all of the spatial information because you want to get an idea of what the potential distribution of species is just based on the environmental conditions.",
            "So when the scenario climate change under climate change, you would like to be able to make a prediction.",
            "So special special location is actually.",
            "No.",
            "No, yeah, it's really like X is an abstract space.",
            "It completely abstract space and all we are using is the value of the feature.",
            "So so really so we will be able to make a prediction.",
            "So what if you use different priors over Lambda driver so so so so?",
            "So the reason why I I've been using La Paz is that it gives very nice performance guarantees and the number of features can grow exponentially with the number of samples when you are using for example, when you're using other other priors, then its performance guarantees we were able to get were suggesting that that will not able to simply as many.",
            "Teacher, so it's really the reason why we are using Laplace prior is actually not the sparsity which is which was people is most often it's really just just a very nice ability to use a very large number of features.",
            "How many fish is?",
            "Do you just simply or maybe the most clearly?",
            "So actually this is some of this.",
            "This world is given to us a priority, so this is perhaps like this would be like a very natural extension of this work.",
            "Trying to figure what is trying to figure the group hierarchy itself.",
            "So group hierarchies right now, part of the specification.",
            "But it turns out that we can choose very large group hierarchies like the number of.",
            "The size of the group heart.",
            "He can be roughly polynomial almost in the number of classes or something like that, so so so we can use many, many overlapping groups, and then if some of them are just just not useful then they'll be ignored and it shouldn't hurt performance too much.",
            "But right now we just use.",
            "The hark is given to us by biologists.",
            "I'm wondering about your experimental results, where for water for a few of the species you actually get worse performance, and it seems to be conceptually that might be because the testator outliers?",
            "Oh yeah, it's certainly ma'am.",
            "Expected just like if you have a hypothesis testing procedure with a P value of 5% and you run it 20 times to expect it right.",
            "Right, well, I mean that's fine, but also The thing is that it's extremely consistent.",
            "I mean the I guess the well, you're right, it's expect to be wrong one time, but here we saw its worst three times out of 30, and that's that's significantly you right?",
            "There is certainly the variance part, but but it's, but it's.",
            "But improvement is still very consistent.",
            "But but you absolutely right.",
            "So of course this reminds me of powerful shrinkage, but it's quite.",
            "Very nice way coming out of the flexibility of where you can set up.",
            "So there's nothing in particular that requires this.",
            "Sharing to flow through a higher, no no, actually.",
            "Both clickability of not right right so?",
            "Right, right?",
            "So so very natural way.",
            "How to apply this is when you have some kind of a table design and you make a group for each column and the group for each row.",
            "So really you can think of it as a bag that has two parts like you have groups on top and classes at the bottom and you just connect them.",
            "It's just that it's easiest to represent it as a taxonomy.",
            "Taxonomy is easier represent by hierarchy.",
            "So let's thank yourself again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll be talking about how to estimate several several densities at the same time, and this densities will be organized in hierarchy, joint work with Dave and Dave client.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Check for instance.",
                    "label": 0
                },
                {
                    "sent": "So let me begin by by motivating it by by a single density estimation problem.",
                    "label": 0
                },
                {
                    "sent": "The problem of estimating distribution of a single species, say all throated vireo over over a map.",
                    "label": 1
                },
                {
                    "sent": "Our input consists of a set of occurrence localities or this bird was observed and a set of environmental variables.",
                    "label": 1
                },
                {
                    "sent": "For example, annual precipitation altitude or the average daily temperature.",
                    "label": 1
                },
                {
                    "sent": "Based on this, we would like to predict geographic distribution of this species and.",
                    "label": 0
                },
                {
                    "sent": "There is a range of techniques to achieve this, and one of the most successful ones has been the regularised maximum entropy.",
                    "label": 1
                },
                {
                    "sent": "The however, all of these techniques, including maximum entropy, have one big problem.",
                    "label": 0
                },
                {
                    "sent": "It is a small number of samples, despite the regularization, we can't really estimate some of these species distributions and we are interested in estimating distributions of rare species because that's the speech that we want to save.",
                    "label": 0
                },
                {
                    "sent": "So we want to know where to put National Park National parks.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do we do about this?",
                    "label": 0
                },
                {
                    "sent": "So first observation is that typically the datasets consist of several species, and even though the number of observations for each individual piece species is small altogether, we.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have a decent datasets, for example, one of these datasets is that I'll be talking bout later in more detail contains 27 species and even though the number of samples per species ranges from 5 to 50 altogether, we have over 1000 samples, so we should be able to take advantage of this information in some way.",
                    "label": 0
                },
                {
                    "sent": "So how can we do this?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this talk, I'll be considering the problem of multiple density estimation where we are estimating density over sample space, which is a map divided into a discrete into discrete cells.",
                    "label": 1
                },
                {
                    "sent": "I assume that the locations will be coming from the from the unknown density distribution and I will be placing no parametric assumption on these the.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, I'll assume that the dentist densities are organized into groups, such as the hierarchy that showing you before, and you probably notice that there were overlaps.",
                    "label": 0
                },
                {
                    "sent": "So, for example, there might be nocturnal birds and birds of prey which are overlapping in some owls.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The main results will be the generalization of the regularization of the regularizer, maximum entropy that has been successful in this area will.",
                    "label": 0
                },
                {
                    "sent": "I'll introduce the hierarchical maximum entropy density estimation and the performance guarantees for this method, which will show how the information is actually shared among the groups and which will lead to systematic choice of hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "Then I'll demonstrate on the real world data that this.",
                    "label": 0
                },
                {
                    "sent": "This works, and then I'll give also Bayesian interpretation of our framework.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the problem of Mount multiple density estimation?",
                    "label": 1
                },
                {
                    "sent": "We're considering so as I said, the sample space is the map divided into a finite number of cells, and then we are given a set of classes organized into groups.",
                    "label": 0
                },
                {
                    "sent": "For simplicity, I'll be just considering this very simple simple hierarchy which consists of two classes of class of red bird class of a Bluebird and a single group that contains just these two species.",
                    "label": 0
                },
                {
                    "sent": "Then we are given a set of samples an I'll be writing these samples as spares where where X is location and Y is an identifier of the class so.",
                    "label": 0
                },
                {
                    "sent": "I should really point out that we are not interested in classification, but it kind of looks like this.",
                    "label": 0
                },
                {
                    "sent": "Why is simply an identifier and we're we're estimating the distributions over X is.",
                    "label": 0
                },
                {
                    "sent": "Finally, the sample space is described by set of real value functions, which are features and.",
                    "label": 0
                },
                {
                    "sent": "And in the examples that I'll be talking about, this set is finite, but it can be also infinite.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what this is our problem?",
                    "label": 0
                },
                {
                    "sent": "We are given the set of observations a set of features and we would like to predict the individual class distributions.",
                    "label": 0
                },
                {
                    "sent": "The maximum entropy principle is based on the following observations.",
                    "label": 1
                },
                {
                    "sent": "We, for each class.",
                    "label": 0
                },
                {
                    "sent": "We expect that the average of the feature that we observe is not too far from the true expectation.",
                    "label": 0
                },
                {
                    "sent": "For example, the average altitude that we have seen among all the red birds should not be too far from the expected value that the altitude that the red bird likes.",
                    "label": 0
                },
                {
                    "sent": "Similarly for the Bluebird.",
                    "label": 0
                },
                {
                    "sent": "Therefore, it should intuitively make sense to be looking for distributions with respect this this.",
                    "label": 0
                },
                {
                    "sent": "This constraint, however, since our sample is small, we do not.",
                    "label": 0
                },
                {
                    "sent": "We do not want this constraint to be satisfied exactly, but only within some error bounds.",
                    "label": 0
                },
                {
                    "sent": "Now I'll be formulating this problem actually join density estimation, but these constraints are conditional, so I have to I have to specify the individual class probabilities and perhaps a better way how to think about this is view them as the importance that we attached the individual classes.",
                    "label": 0
                },
                {
                    "sent": "And well, I have to kind of explain the letters M1 will be the number of samples of the of the red bird and two number of samples of Bluebird.",
                    "label": 1
                },
                {
                    "sent": "But in fact like just the map math will workout simpler, but but those can be any values that we consider as important of the image jewel.",
                    "label": 0
                },
                {
                    "sent": "That's the estimation problems.",
                    "label": 0
                },
                {
                    "sent": "So the problem is still under specified and the maximum entropy principle tells us that among all the distribution that satisfy this constraint, we should choose the one that maximizes the entropy.",
                    "label": 0
                },
                {
                    "sent": "Now I've been telling that we should take advantage of the group information, but there is really no group information there yet, and in fact if we just formulate this problem, it will be equivalent to the individual density estimation problems.",
                    "label": 0
                },
                {
                    "sent": "So people probably anticipate what follows next and we so we add a new set of constraints.",
                    "label": 0
                },
                {
                    "sent": "For every groups in our.",
                    "label": 0
                },
                {
                    "sent": "In our case, this is simply the constraint for for the group.",
                    "label": 0
                },
                {
                    "sent": "For the group containing both the red and blue samples, since we do expect that now that the average altitude across all of the birds should not be too far from the true expected altitude that the that all of the words like.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look at this problem.",
                    "label": 0
                },
                {
                    "sent": "Entropy is the is a concave function.",
                    "label": 0
                },
                {
                    "sent": "We have fixed the class probabilities, so this all these constraints turn out to be convex.",
                    "label": 0
                },
                {
                    "sent": "This is a convex convex problem, so we can just apply machinery of convex optimization to solve it and.",
                    "label": 0
                },
                {
                    "sent": "If you applied and typically in the maximum entropy setting, we start by bulking at the jewel.",
                    "label": 0
                },
                {
                    "sent": "So let's take a look.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put you on this problem is so.",
                    "label": 0
                },
                {
                    "sent": "First of all, we know we find out that the individual class densities take a form of an exponential family distributions and.",
                    "label": 0
                },
                {
                    "sent": "And this is not unexpected.",
                    "label": 0
                },
                {
                    "sent": "This what we usually get in maximum entropy and the second part is.",
                    "label": 1
                },
                {
                    "sent": "So what is it that we are maximizing?",
                    "label": 0
                },
                {
                    "sent": "It turns out that the entropy from the previous slide is maximized by the joint density, which maximizes the likelihood plus minus minus a penalty.",
                    "label": 0
                },
                {
                    "sent": "So the maximization is over three para meters.",
                    "label": 0
                },
                {
                    "sent": "The 1st two are the class parameters and the third one is a group parameter.",
                    "label": 0
                },
                {
                    "sent": "Now this metrics maximization encourages sharing.",
                    "label": 0
                },
                {
                    "sent": "Of trade in a certain way, in particular the we are penalized for 4D for the class.",
                    "label": 0
                },
                {
                    "sent": "For the class parameters, deviations from the group para meters and group parameters themselves are penalized by one norm.",
                    "label": 0
                },
                {
                    "sent": "So we are we're maximizing the sum of likely, then something else.",
                    "label": 1
                },
                {
                    "sent": "So so there's something else can be viewed as log of the prior so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, so so.",
                    "label": 0
                },
                {
                    "sent": "So what is this prior?",
                    "label": 0
                },
                {
                    "sent": "Like this prior amounts to 1st selecting a group parameter from from the Formula plus distribution, then selecting the individual class parameters.",
                    "label": 0
                },
                {
                    "sent": "For from the.",
                    "label": 0
                },
                {
                    "sent": "From Laplace distributions conditioned on the group and finally we select the individual samples from the from the class densities.",
                    "label": 0
                },
                {
                    "sent": "So OK, so these models have been around for awhile, so So what?",
                    "label": 0
                },
                {
                    "sent": "So what does maximum entropy bias?",
                    "label": 0
                },
                {
                    "sent": "So what's new there the?",
                    "label": 0
                },
                {
                    "sent": "The first insight is that the hyperparameters have a natural interpretation.",
                    "label": 0
                },
                {
                    "sent": "Is uncertainties in the averages.",
                    "label": 0
                },
                {
                    "sent": "The second benefit is that we get strong performance guarantees that do not pose.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Assumptions on the unknown distributions that we are estimating.",
                    "label": 0
                },
                {
                    "sent": "So let's let's let's get a flavor of what these performance guarantees are like.",
                    "label": 1
                },
                {
                    "sent": "If we choose the appropriate parameters in the right way, then we can say that the maximum performance as measured by the test test log likelihood with respect to the true distribution, differs from the optimal performance by optimal.",
                    "label": 0
                },
                {
                    "sent": "Here I mean the performance of the optimal exponential family distribution by by the term that we see.",
                    "label": 0
                },
                {
                    "sent": "Do I have the?",
                    "label": 0
                },
                {
                    "sent": "I think I have a pointer OK. Maybe I can OK so so by the term that we see that we see below this term takes form of is actually the regularization of the of the best exponential family distribution, which is the one denoted by the by those Lambda stars.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing about this gap that it does capture how how we achieve improvements due to the similarity within the group and due to the use of larger groups specifically and if we if we included no group information.",
                    "label": 1
                },
                {
                    "sent": "Then the last term would be.",
                    "label": 0
                },
                {
                    "sent": "Then the last thing would be 0 and the first 2 terms would be simply equal to the L1 norms.",
                    "label": 0
                },
                {
                    "sent": "Now what happens if a certain set of para meters is similar?",
                    "label": 0
                },
                {
                    "sent": "If the third set of parameters is similar, then then then we can simply set the Lambda 12 equal to that that similar set of parameters and we achieved a reduction in the first 2 terms that's proportional to the square root of the number of samples of 1st species.",
                    "label": 0
                },
                {
                    "sent": "But the square root of the number of samples of the second species at the same time we are increasing this.",
                    "label": 0
                },
                {
                    "sent": "Only by the amount that's proportional to the square root of the total number of samples.",
                    "label": 0
                },
                {
                    "sent": "Now since the sum of the squares is larger than the square root, we are achieving an improvement.",
                    "label": 0
                },
                {
                    "sent": "In this case, the improvement on two speeches is by by factor of sqrt 2.",
                    "label": 0
                },
                {
                    "sent": "Now, if we have a larger group that shares and information that we are achieving an improvement, that's by the factor of square root of the number of species.",
                    "label": 0
                },
                {
                    "sent": "In other words, both similarity and similarity within large groups will help the generalization performance, which is intuitively we expect.",
                    "label": 0
                },
                {
                    "sent": "So this is all nice and good, so that's this.",
                    "label": 0
                },
                {
                    "sent": "How does this perform?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In practice, so first let's take a look at the toy experiment in which we construct the synthetic distribution of a blue species and the first measure the performance of a single density distribution method.",
                    "label": 0
                },
                {
                    "sent": "Without that doesn't use the group information, and then we see what happens if you add the group information will evaluate the performance as a function of the number of samples, and we are interested into into quantities.",
                    "label": 1
                },
                {
                    "sent": "First is the relative entropy.",
                    "label": 0
                },
                {
                    "sent": "The truth?",
                    "label": 0
                },
                {
                    "sent": "In this case, the truth is from the exponential family.",
                    "label": 0
                },
                {
                    "sent": "And then will be also taking a look at what happens to the parameter estimates.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we see that the single class method converges to the truth and also both of the parameters converge, which is, which is nice.",
                    "label": 0
                },
                {
                    "sent": "So now what happens when we add the second species?",
                    "label": 0
                },
                {
                    "sent": "The second species is a fixed number of samples to 100 and.",
                    "label": 0
                },
                {
                    "sent": "And we see that the performance does of the blue species in this hierarchical setup does improve and improve is most dramatic on the small number of examples, which is indeed what we expect.",
                    "label": 0
                },
                {
                    "sent": "So now where does this improvement come from?",
                    "label": 0
                },
                {
                    "sent": "The improvement comes from the from the changes in the second parameter.",
                    "label": 0
                },
                {
                    "sent": "This the precipitation parameter, and it turns out that both of these species like lower values of precipitation, and therefore the group parameter are shrunk towards the the.",
                    "label": 0
                },
                {
                    "sent": "The perimeter of the of the blue speech is shrunk towards the perimeter of the rat species in in terms of temperature.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "They like different kinds of temperature and the optimization therefore doesn't really affect the blue estimates.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's take a look at some real world data sets.",
                    "label": 1
                },
                {
                    "sent": "The first of them is the data set of species from Australia with tropics.",
                    "label": 1
                },
                {
                    "sent": "It contains 20 species and we have a set of three groups.",
                    "label": 0
                },
                {
                    "sent": "The second data set contains species from North East NSW.",
                    "label": 1
                },
                {
                    "sent": "27 species and 15 groups and this is the data set that I showed you in the first slide and really just wanting to notice that here.",
                    "label": 0
                },
                {
                    "sent": "Heard the groups sound the groups are overlapping.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so here we again measured here.",
                    "label": 0
                },
                {
                    "sent": "We just measured the improvement on the using the hierarchies, and each circle corresponds to one species and we report the difference from the from the case where we don't use the density information.",
                    "label": 0
                },
                {
                    "sent": "So anything about the dashed line is an improvement.",
                    "label": 0
                },
                {
                    "sent": "We see that in the first data set, the improvement is extremely consistent we are.",
                    "label": 0
                },
                {
                    "sent": "Pretty much do almost always with the single exception.",
                    "label": 0
                },
                {
                    "sent": "We do be do better, and the improvement is more dramatic on the small sample sizes, which is which will be expected, which is also what we saw in the synthetic experiment.",
                    "label": 0
                },
                {
                    "sent": "The improvement on the second data set is.",
                    "label": 0
                },
                {
                    "sent": "So very consistent even though less so than on the 1st and there.",
                    "label": 0
                },
                {
                    "sent": "However, there we see that in one case we do significantly worse, so this is the case with the smallest number of examples.",
                    "label": 0
                },
                {
                    "sent": "This we have only four 4 four sample points, and perhaps it suggests like where the limits of our of our techniques are getting at.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point so to sum up, I've introduced hierarchical maximum entropy, which is the principle approach to multiple density estimation.",
                    "label": 1
                },
                {
                    "sent": "It allows sharing information across tasks, but Moreover it also.",
                    "label": 0
                },
                {
                    "sent": "Explains how this information is shared by means of performance guarantees.",
                    "label": 1
                },
                {
                    "sent": "This performance guarantees can be used as guides to setting hyperparameters, and we've seen that it is possible to obtain significant benefits on the real world data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for attention.",
                    "label": 0
                },
                {
                    "sent": "Keep up with the question.",
                    "label": 0
                },
                {
                    "sent": "So can we think of what you're doing as actually innocence, conditional density estimation, 'cause it's actually conditioned on knowing things like the temperature and rainfall at different locations.",
                    "label": 0
                },
                {
                    "sent": "So if you had a cell where you didn't know the temperature or the rainfall, for example.",
                    "label": 0
                },
                {
                    "sent": "Then you wouldn't be able to estimate the density of birds there, and can you extend it to be able to do that?",
                    "label": 0
                },
                {
                    "sent": "Is that true?",
                    "label": 0
                },
                {
                    "sent": "Or right this I am so?",
                    "label": 0
                },
                {
                    "sent": "You might be able to correct for some missing information for some missing features, but I don't think I can think of would be the basis for predicting anything I'm missing.",
                    "label": 0
                },
                {
                    "sent": "So in this in the distribution models you typically ignore all of the spatial information because you want to get an idea of what the potential distribution of species is just based on the environmental conditions.",
                    "label": 0
                },
                {
                    "sent": "So when the scenario climate change under climate change, you would like to be able to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "So special special location is actually.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "No, yeah, it's really like X is an abstract space.",
                    "label": 0
                },
                {
                    "sent": "It completely abstract space and all we are using is the value of the feature.",
                    "label": 0
                },
                {
                    "sent": "So so really so we will be able to make a prediction.",
                    "label": 0
                },
                {
                    "sent": "So what if you use different priors over Lambda driver so so so so?",
                    "label": 0
                },
                {
                    "sent": "So the reason why I I've been using La Paz is that it gives very nice performance guarantees and the number of features can grow exponentially with the number of samples when you are using for example, when you're using other other priors, then its performance guarantees we were able to get were suggesting that that will not able to simply as many.",
                    "label": 0
                },
                {
                    "sent": "Teacher, so it's really the reason why we are using Laplace prior is actually not the sparsity which is which was people is most often it's really just just a very nice ability to use a very large number of features.",
                    "label": 0
                },
                {
                    "sent": "How many fish is?",
                    "label": 0
                },
                {
                    "sent": "Do you just simply or maybe the most clearly?",
                    "label": 0
                },
                {
                    "sent": "So actually this is some of this.",
                    "label": 0
                },
                {
                    "sent": "This world is given to us a priority, so this is perhaps like this would be like a very natural extension of this work.",
                    "label": 0
                },
                {
                    "sent": "Trying to figure what is trying to figure the group hierarchy itself.",
                    "label": 0
                },
                {
                    "sent": "So group hierarchies right now, part of the specification.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that we can choose very large group hierarchies like the number of.",
                    "label": 0
                },
                {
                    "sent": "The size of the group heart.",
                    "label": 0
                },
                {
                    "sent": "He can be roughly polynomial almost in the number of classes or something like that, so so so we can use many, many overlapping groups, and then if some of them are just just not useful then they'll be ignored and it shouldn't hurt performance too much.",
                    "label": 0
                },
                {
                    "sent": "But right now we just use.",
                    "label": 0
                },
                {
                    "sent": "The hark is given to us by biologists.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering about your experimental results, where for water for a few of the species you actually get worse performance, and it seems to be conceptually that might be because the testator outliers?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, it's certainly ma'am.",
                    "label": 0
                },
                {
                    "sent": "Expected just like if you have a hypothesis testing procedure with a P value of 5% and you run it 20 times to expect it right.",
                    "label": 0
                },
                {
                    "sent": "Right, well, I mean that's fine, but also The thing is that it's extremely consistent.",
                    "label": 0
                },
                {
                    "sent": "I mean the I guess the well, you're right, it's expect to be wrong one time, but here we saw its worst three times out of 30, and that's that's significantly you right?",
                    "label": 0
                },
                {
                    "sent": "There is certainly the variance part, but but it's, but it's.",
                    "label": 0
                },
                {
                    "sent": "But improvement is still very consistent.",
                    "label": 0
                },
                {
                    "sent": "But but you absolutely right.",
                    "label": 0
                },
                {
                    "sent": "So of course this reminds me of powerful shrinkage, but it's quite.",
                    "label": 0
                },
                {
                    "sent": "Very nice way coming out of the flexibility of where you can set up.",
                    "label": 0
                },
                {
                    "sent": "So there's nothing in particular that requires this.",
                    "label": 0
                },
                {
                    "sent": "Sharing to flow through a higher, no no, actually.",
                    "label": 0
                },
                {
                    "sent": "Both clickability of not right right so?",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "So so very natural way.",
                    "label": 0
                },
                {
                    "sent": "How to apply this is when you have some kind of a table design and you make a group for each column and the group for each row.",
                    "label": 0
                },
                {
                    "sent": "So really you can think of it as a bag that has two parts like you have groups on top and classes at the bottom and you just connect them.",
                    "label": 0
                },
                {
                    "sent": "It's just that it's easiest to represent it as a taxonomy.",
                    "label": 0
                },
                {
                    "sent": "Taxonomy is easier represent by hierarchy.",
                    "label": 0
                },
                {
                    "sent": "So let's thank yourself again.",
                    "label": 0
                }
            ]
        }
    }
}