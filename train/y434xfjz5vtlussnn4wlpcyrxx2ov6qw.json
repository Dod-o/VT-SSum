{
    "id": "y434xfjz5vtlussnn4wlpcyrxx2ov6qw",
    "title": "A Rate-Distortion One-Class Model and its Applications to Clustering",
    "info": {
        "author": [
            "Partha Pratim Talukdar, Indian Institute of Science Bangalore"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml08_talukdar_rdo/",
    "segmentation": [
        [
            "Thank everyone, so I'm part available, so in this talk I shall be presenting a new rate distortion based model for one class classification and we will see how we can extend that to standard multiclass clustering.",
            "Some experimental results, so this is joint work with Koby, Kraemer and Fernando Pereira.",
            "So OK, let's get started."
        ],
        [
            "So what exactly is one class classification problem?",
            "So basically we have we can think of the setting as we have lots and lots of unlabeled data and we are interested in only particular a single category.",
            "For example, you have say lots and lots of unlabeled images and you are interested in dog pictures of particular breed and unlabeled data has images of all types of objects like books, dogs, cats, everything and you want to like give a few dcid instances few positive instances?",
            "And starting from there you want to get more instances of the same type from the unlabeled data.",
            "So this is different from standard clustering or standard actually standard classification in the sense that we don't have explicit negative instances here.",
            "So you just start with a small number of positive instances and you get more instances of the same type.",
            "So this kind of problem comes up again and again in different application domains.",
            "For example document retrieval or information extraction from where actually we got the inspiration to start.",
            "Working on this problem and also in bioinformatics gene expression.",
            "If you know a particular gene you want to find other genes which are Co regulated with that or has similar expression patterns.",
            "So in this one class classification problem we are interested in high precision rather than high recall, so the number of extractions in the system gives us.",
            "It should be accurate rather than getting everything that's out there.",
            "OK."
        ],
        [
            "So there has been a body of work related to this problem starting back with density based clustering with the scam.",
            "Then our work is mostly built on this previous work by Kramer and check which is again derived from this work on one class SVM and related problems by text and doing all the way back in 99.",
            "So earlier people try to view this problem as outlier detection so you have.",
            "All these points, then you just want to filter out a few points that are of no interest.",
            "Then Kramer and check actually took the opposite view where they just wanted to get a few of substructure in all those unlabeled data.",
            "And like most of the points are irrelevant for the task, so you only select a subset of the instances and remove all everything else, and then guptan coach also built on the Creamer in Chiswick Framework."
        ],
        [
            "1006 So our approach is different from most of these approaches in the sense that we pose the problem as a lossy coding of each instance into an instance dependent code.",
            "Words where each code word will represent the cluster.",
            "So the important thing here is like this code words are instance dependent, so this is different from most previous approaches where you only have a small number of clusters.",
            "So in this case actually we have the number of clusters are more than the number of data points.",
            "In that data.",
            "And we enforce regularization by sparse coding by making sure each instance has to be assigned to only two code words."
        ],
        [
            "So we'll see the coding scheme next.",
            "So on the left we have these five points.",
            "For example, in pink, and these arrows demonstrate how we can code those points for each point.",
            "Basically, you can see this horizontal error, which is, which says like .3, for example, can be coded using this code word tree, which is specific to .3.",
            "Or you can assign it to this joint code word zero.",
            "OK, so for each point basically we had just 22 choices.",
            "Either you self code that is take the horizontal arrow or you assign it to this joint code word, which is the one class that we're finally interested in.",
            "And this joint code word zero, which represents which corresponds to the one class that we are interested in will be represented by this weight vector by this vector W."
        ],
        [
            "OK, so let's get a few notations out of the way.",
            "So each point X is either self coded by instance dependent code code, word X or you assign it to the joint code word.",
            "So the probabilities associated RQX given X and Q0 given X and both trips up to one.",
            "Then for each point X we have a priority.",
            "Find PX.",
            "These two conditional probability we have seen there an each point X will be represented by this vector VX and the centroid.",
            "The one class centroid will be represented by W and we have a divergent measure between the point and that code.",
            "One class code work."
        ],
        [
            "OK, So what exactly is the tradeoff here?",
            "Now we have these five points.",
            "At one extreme, we can assign all these points to this single code word W. So in this case you get very high compression, right?",
            "So you don't have to remember about all these different points and just assign to one code word and just remember about its identity.",
            "So basically this is all in one and we get very high compression but also high distortion because each one of these points you are just representing using a single vector.",
            "At the other extreme, you yourself code each of the points by taking this horizontal arrow and you don't do any compression.",
            "And also there is no distortion cause the points are represented by themselves.",
            "So between these two extremes we want some tradeoff, right?",
            "So we want something in between where some of the points are assigned to the self to the joint code word while some most of the other points are self coded.",
            "So the self coded points will be the points that we are not interested in."
        ],
        [
            "OK, so this leads us to this rate distortion optimization, where X the random variable X basically represents the instances to be coded, and this random variable T can take two values for each point.",
            "Either it could be 0 or it could be any of the X, but like for over all the values it will be for all instances one through M. Assuming there are M instances in the data.",
            "Ann Reid basically measures the amount of compression.",
            "When you map this X2 T, which is measured by this mutual information, an distortion actually measures like how well on average this code word W for this zero class represents as a proxy two instances in that class.",
            "So we have to trade off parameter beta, which is positive and basically we minimize this objectives.",
            "So you want to trade off between rate.",
            "That means how much compression you are going to do and the distortion as in how much loss you are going to suffer when you do that compression.",
            "So for different values of beta will get different assignments of the points to clusters and the parameters are this W and is coding policy or coding probabilities, queues and there is 1 four.",
            "Each instance X."
        ],
        [
            "OK, so when you solve that objective using standard way of.",
            "Lagrangian multipliers and also then you get these three self consistent equations.",
            "OK, so the first one is what is the total probability marginal probability of assigning points to the one class and these are the coding policies for each instance X.",
            "What's the probability of assigning it to the one class?",
            "And as you can see, these two things are coupled cause this Q0 and Q0 given X in Q0.",
            "Given X, there is Q0 and Q0.",
            "There is Q07X.",
            "OK, so you can't directly solve in an analytic way and the third equation is basically the centroid representation, which you can think of as a weighted average of all the points in the data, which are weighted by this.",
            "Number quantity there."
        ],
        [
            "OK. Now following the information bottleneck framework we define, we follow the same process and define this iterative optimization where at first step we basically fix two of the parameters or the Q zero given X&Q 0 and then like get this centroid W. So we fix these.",
            "So this is actually using Bayes rule you can derive from these parameters which are already fixed at the first step.",
            "Then you basically compute the centroid.",
            "Now, once you have found a centroid, you can basically compute the divergance to all the points in the data, because the centroid is fixed, you have a distance that our distance or divergent measure.",
            "So you can calculate all those divergences.",
            "Now the question is how we can find this coding policy?",
            "How we do find this Q zero given X and Q0?",
            "And basically you iterate till convergence.",
            "So the first part is easy as we have seen like once we have fixed that, we can easily compute.",
            "Now we'll see an efficient algorithm to solve the second part."
        ],
        [
            "So let's define the set C to be all those points whose coding policy sets it to be in the one class with all the probability.",
            "So the Q zero given X is 1, and for each point let's define the score for South of X, which is better times the distortion from the centroid, which we have fixed currently and is log prior.",
            "Then.",
            "This lemma says that there is a teacher or some threshold.",
            "At which will.",
            "Beyond if.",
            "If the score of the points is less than that Tita, then the points will be assigned to this one class that we're interested in an.",
            "If it's not, then it's going to be self coded or would be put outside the class.",
            "And as we can see this is just involves sorting an.",
            "You can do that in M log M time, where M is the number of points in the data.",
            "So this using this we can efficiently compute the assignment of the points though on class."
        ],
        [
            "OK, so let's look at this figure of a phase transition and which will give some intuition into working of the algorithm.",
            "So on this axis we have inverse of this parameter beta.",
            "So you can think of that as a temperature.",
            "So the temperature and beta are inversely related.",
            "And on this axis we have the identity of the points and also the distortion measures.",
            "OK, so .2 point #2 if it's put in the class one class cluster then it's always going to suffer a distortion of two.",
            "And this is our coding policy.",
            "I mean like with what probability we're going to assign it to the one class so when the temperature is very high initially.",
            "So at this point then you can think of it.",
            "Everything has melted, everything is in the one class.",
            "Now you keep on decreasing the temperature are increasing the beta, then we see like the point with the highest distortion is pulled out of the one class.",
            "So whenever it's less than one, that that point is not going to be put in the one class and will be going will be self coded an as you continuously decrease the temperature more and more points are pulled out of the cluster.",
            "And you can think of it like the points are getting crystallized at some critical temperature here.",
            "All the points are pulled out of the one class and everything is self coded.",
            "OK, so this shows in a clear way the behavior of the algorithm for different values of this parameter beta.",
            "OK."
        ],
        [
            "Now that was one class model for.",
            "Area distortion based model for one class.",
            "Now we will see how we can extend that to multiclass clustering.",
            "That's the standard setting."
        ],
        [
            "So basically, one could think that now we have M data points and K centroids, so we can instead of just having one class, we can just replace that with CK classes right now.",
            "But the problem here is that if you are not going to self code, Excel coding probabilities given by this QX given X so 1 -- Q X given X should be the probability that you are going to assign that point using one of those K centroids.",
            "The problem is that that.",
            "Quantity doesn't tell you which which of those.",
            "K centroids, you should assign that point."
        ],
        [
            "So our suggestion is to actually go through again this intermediate variable 0.",
            "So at first state you decide whether you want to self code or whether you want to cluster using one of these.",
            "Code words, in which case we have two and three points.",
            "Then once you have done that then you can use a hard clustering algorithm for all the points that you didn't want to self code.",
            "Assign it to all these different clusters and then you can iterate this process."
        ],
        [
            "So again, the same thing as I have said at the first step, use the original one class algorithm to decide whether you want to self code that point or not.",
            "So this in a way it is MCRD.",
            "This multiclass algorithm is using the first step as a subroutine to compute the first step here."
        ],
        [
            "And then in the second step we use a hard clustering algorithm to classify all those points that we decided not to self code.",
            "In the first instance and then again we iterate till convergence."
        ],
        [
            "So I have left the details regarding the multiclass algorithm for the interest of time, but there is a poster today evening.",
            "If you're interested, we'll be happy to describe more, so let's look at some experimental results.",
            "So we evaluated the algorithm in three different settings in first setting.",
            "Basically we compared it to previous one class document classification problem that have been published before, and we also do multiclass synthetic multiclass clustering on synthetic data.",
            "And multi class clustering on."
        ],
        [
            "Real world, so in this document classification task we compare it to this Creamer and Jessica's work which is represented by OCIB and previous convex loss based method due to Kramer and Singer represented by OC Convex.",
            "So we see that we do we.",
            "Achieve comparable performance to those methods.",
            "In one case we dominate both.",
            "Another case we actually get a trade off at the two extreme recall regions we perform as good as the best an in between we do, compareable.",
            "To the other methods and we have some reasons regarding this which I won't get into now, so so this experiment shows that we are doing compatible and in some cases better than that."
        ],
        [
            "Various methods.",
            "So in this.",
            "Clustering task let me first tell you that there are different colors and also some of the points.",
            "These black dots are.",
            "Actually, the self coded points.",
            "So basically we ran this multiclass clustering algorithm and we sample 400 points from 4 Gaussians and 500 points from this uniform background.",
            "Now we asked multiclass clustering algorithm to classify these points using five centroids.",
            "We specified 5 to that like it could model the noise using one of the clusters that we just wanted to see how it performs.",
            "So as you can see, like this centroids are represented by this bold circles, so some of the noise points are put in here, but most of the points are self coded, so these black points these points in between these clusters are not put into any of this.",
            "Five clusters and the interesting other interesting point is, as we increase the value of beta, so beta here is 100 and beta.",
            "Here is 140.",
            "So as we increase the beta, the distortion is much more much costly.",
            "So the algorithm rightly decides on the number of clusters to be 4 in this case.",
            "So even though we specified the number required number of clusters to be 5, it correctly learned a number correct number of.",
            "The clusters, which is 4 an all the noisy points in the in between are basically self coded, so the self coded points at the points we don't care or like they give up."
        ],
        [
            "So we also compared it to.",
            "Other method called sequential information bottleneck due to Norm Sloaney mentality, HP and so we wanted to see whether if we introduce this capability of giving up a few points whether we could do a better clustering task an from this plot, it shows that at all values of recall we achieve better precision simply by allowing this, allowing the algorithm to give up some of the points and do.",
            "Get clustering or estimation of the centroid with the remaining ones, and so that basically shows that if you have that capability which directly are naturally extends from this one class model that we propose is very helpful in this task."
        ],
        [
            "So that brings us to the conclusion.",
            "So to wrap up.",
            "We have cast the problem of identifying a small coherent subset using this one class model which trades off between this class size.",
            "How much you want to compress and how much distortion you are willing to suffer."
        ],
        [
            "And we have shown a way to move from one class to the standard multiclass clustering with the background noise left up, left out.",
            "So which is basically the ability to give up some of the points.",
            "Anne."
        ],
        [
            "And as part of future work we are now looking at whether we could extend these kind of methods to other instances and distortions.",
            "For example, data on graphs or manifolds."
        ],
        [
            "So that that's N thanks for time.",
            "Why do you have to go through a two stage approach for the Multicentre case rather than defining probability, for example and finding the self coding probabilities the marginal?",
            "So basically we wanted like first, so I think I hinted a little bit there.",
            "Basically, if you are not deciding to self code the points then like we don't know which cluster we want to assign that point right?",
            "So we thought like we'll split it up into 2 steps and in fact in the paper we show that like this, optimization is equivalent to the information bottleneck.",
            "So there is a theorem there.",
            "Just to follow up on that.",
            "Yeah, so instead of this, simply doing one class compression.",
            "So you could do exactly the same thing, right?",
            "So you could say hi to, Roger needs to be coded as one of five classes, or else can be covered as itself with that wind up producing equivalent thought of plots.",
            "Clustering as usual, two stage product life, prospering algorithm.",
            "Um?",
            "Yes, right now.",
            "The the sub consistent equations on that don't separate.",
            "Because of their marginalization.",
            "OK, another question.",
            "I was wondering whether in the in the one class this could review this EM with garbage components, right?",
            "So yeah, that's an interesting question, so we don't know.",
            "So we have acknowledged the connection to M in the paper and so.",
            "But we don't know exactly what form it is, but I agree that it's like very similar to this expectation and maximization steps as in standard.",
            "But we don't know exactly what form it could be.",
            "Yes.",
            "So you better is basically a regularizer of the solution you have array of estimating that counts.",
            "No, not as of now, but there are some works in that direction by other people which could be leveraged on.",
            "Text."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank everyone, so I'm part available, so in this talk I shall be presenting a new rate distortion based model for one class classification and we will see how we can extend that to standard multiclass clustering.",
                    "label": 0
                },
                {
                    "sent": "Some experimental results, so this is joint work with Koby, Kraemer and Fernando Pereira.",
                    "label": 0
                },
                {
                    "sent": "So OK, let's get started.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what exactly is one class classification problem?",
                    "label": 0
                },
                {
                    "sent": "So basically we have we can think of the setting as we have lots and lots of unlabeled data and we are interested in only particular a single category.",
                    "label": 0
                },
                {
                    "sent": "For example, you have say lots and lots of unlabeled images and you are interested in dog pictures of particular breed and unlabeled data has images of all types of objects like books, dogs, cats, everything and you want to like give a few dcid instances few positive instances?",
                    "label": 0
                },
                {
                    "sent": "And starting from there you want to get more instances of the same type from the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "So this is different from standard clustering or standard actually standard classification in the sense that we don't have explicit negative instances here.",
                    "label": 0
                },
                {
                    "sent": "So you just start with a small number of positive instances and you get more instances of the same type.",
                    "label": 1
                },
                {
                    "sent": "So this kind of problem comes up again and again in different application domains.",
                    "label": 1
                },
                {
                    "sent": "For example document retrieval or information extraction from where actually we got the inspiration to start.",
                    "label": 0
                },
                {
                    "sent": "Working on this problem and also in bioinformatics gene expression.",
                    "label": 0
                },
                {
                    "sent": "If you know a particular gene you want to find other genes which are Co regulated with that or has similar expression patterns.",
                    "label": 1
                },
                {
                    "sent": "So in this one class classification problem we are interested in high precision rather than high recall, so the number of extractions in the system gives us.",
                    "label": 0
                },
                {
                    "sent": "It should be accurate rather than getting everything that's out there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there has been a body of work related to this problem starting back with density based clustering with the scam.",
                    "label": 0
                },
                {
                    "sent": "Then our work is mostly built on this previous work by Kramer and check which is again derived from this work on one class SVM and related problems by text and doing all the way back in 99.",
                    "label": 0
                },
                {
                    "sent": "So earlier people try to view this problem as outlier detection so you have.",
                    "label": 0
                },
                {
                    "sent": "All these points, then you just want to filter out a few points that are of no interest.",
                    "label": 1
                },
                {
                    "sent": "Then Kramer and check actually took the opposite view where they just wanted to get a few of substructure in all those unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "And like most of the points are irrelevant for the task, so you only select a subset of the instances and remove all everything else, and then guptan coach also built on the Creamer in Chiswick Framework.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1006 So our approach is different from most of these approaches in the sense that we pose the problem as a lossy coding of each instance into an instance dependent code.",
                    "label": 1
                },
                {
                    "sent": "Words where each code word will represent the cluster.",
                    "label": 0
                },
                {
                    "sent": "So the important thing here is like this code words are instance dependent, so this is different from most previous approaches where you only have a small number of clusters.",
                    "label": 0
                },
                {
                    "sent": "So in this case actually we have the number of clusters are more than the number of data points.",
                    "label": 0
                },
                {
                    "sent": "In that data.",
                    "label": 0
                },
                {
                    "sent": "And we enforce regularization by sparse coding by making sure each instance has to be assigned to only two code words.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we'll see the coding scheme next.",
                    "label": 0
                },
                {
                    "sent": "So on the left we have these five points.",
                    "label": 0
                },
                {
                    "sent": "For example, in pink, and these arrows demonstrate how we can code those points for each point.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can see this horizontal error, which is, which says like .3, for example, can be coded using this code word tree, which is specific to .3.",
                    "label": 1
                },
                {
                    "sent": "Or you can assign it to this joint code word zero.",
                    "label": 0
                },
                {
                    "sent": "OK, so for each point basically we had just 22 choices.",
                    "label": 0
                },
                {
                    "sent": "Either you self code that is take the horizontal arrow or you assign it to this joint code word, which is the one class that we're finally interested in.",
                    "label": 1
                },
                {
                    "sent": "And this joint code word zero, which represents which corresponds to the one class that we are interested in will be represented by this weight vector by this vector W.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's get a few notations out of the way.",
                    "label": 0
                },
                {
                    "sent": "So each point X is either self coded by instance dependent code code, word X or you assign it to the joint code word.",
                    "label": 1
                },
                {
                    "sent": "So the probabilities associated RQX given X and Q0 given X and both trips up to one.",
                    "label": 0
                },
                {
                    "sent": "Then for each point X we have a priority.",
                    "label": 0
                },
                {
                    "sent": "Find PX.",
                    "label": 0
                },
                {
                    "sent": "These two conditional probability we have seen there an each point X will be represented by this vector VX and the centroid.",
                    "label": 0
                },
                {
                    "sent": "The one class centroid will be represented by W and we have a divergent measure between the point and that code.",
                    "label": 0
                },
                {
                    "sent": "One class code work.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what exactly is the tradeoff here?",
                    "label": 0
                },
                {
                    "sent": "Now we have these five points.",
                    "label": 0
                },
                {
                    "sent": "At one extreme, we can assign all these points to this single code word W. So in this case you get very high compression, right?",
                    "label": 0
                },
                {
                    "sent": "So you don't have to remember about all these different points and just assign to one code word and just remember about its identity.",
                    "label": 0
                },
                {
                    "sent": "So basically this is all in one and we get very high compression but also high distortion because each one of these points you are just representing using a single vector.",
                    "label": 1
                },
                {
                    "sent": "At the other extreme, you yourself code each of the points by taking this horizontal arrow and you don't do any compression.",
                    "label": 0
                },
                {
                    "sent": "And also there is no distortion cause the points are represented by themselves.",
                    "label": 0
                },
                {
                    "sent": "So between these two extremes we want some tradeoff, right?",
                    "label": 0
                },
                {
                    "sent": "So we want something in between where some of the points are assigned to the self to the joint code word while some most of the other points are self coded.",
                    "label": 0
                },
                {
                    "sent": "So the self coded points will be the points that we are not interested in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this leads us to this rate distortion optimization, where X the random variable X basically represents the instances to be coded, and this random variable T can take two values for each point.",
                    "label": 1
                },
                {
                    "sent": "Either it could be 0 or it could be any of the X, but like for over all the values it will be for all instances one through M. Assuming there are M instances in the data.",
                    "label": 0
                },
                {
                    "sent": "Ann Reid basically measures the amount of compression.",
                    "label": 1
                },
                {
                    "sent": "When you map this X2 T, which is measured by this mutual information, an distortion actually measures like how well on average this code word W for this zero class represents as a proxy two instances in that class.",
                    "label": 1
                },
                {
                    "sent": "So we have to trade off parameter beta, which is positive and basically we minimize this objectives.",
                    "label": 0
                },
                {
                    "sent": "So you want to trade off between rate.",
                    "label": 0
                },
                {
                    "sent": "That means how much compression you are going to do and the distortion as in how much loss you are going to suffer when you do that compression.",
                    "label": 0
                },
                {
                    "sent": "So for different values of beta will get different assignments of the points to clusters and the parameters are this W and is coding policy or coding probabilities, queues and there is 1 four.",
                    "label": 0
                },
                {
                    "sent": "Each instance X.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so when you solve that objective using standard way of.",
                    "label": 0
                },
                {
                    "sent": "Lagrangian multipliers and also then you get these three self consistent equations.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first one is what is the total probability marginal probability of assigning points to the one class and these are the coding policies for each instance X.",
                    "label": 0
                },
                {
                    "sent": "What's the probability of assigning it to the one class?",
                    "label": 0
                },
                {
                    "sent": "And as you can see, these two things are coupled cause this Q0 and Q0 given X in Q0.",
                    "label": 0
                },
                {
                    "sent": "Given X, there is Q0 and Q0.",
                    "label": 0
                },
                {
                    "sent": "There is Q07X.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can't directly solve in an analytic way and the third equation is basically the centroid representation, which you can think of as a weighted average of all the points in the data, which are weighted by this.",
                    "label": 0
                },
                {
                    "sent": "Number quantity there.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Now following the information bottleneck framework we define, we follow the same process and define this iterative optimization where at first step we basically fix two of the parameters or the Q zero given X&Q 0 and then like get this centroid W. So we fix these.",
                    "label": 1
                },
                {
                    "sent": "So this is actually using Bayes rule you can derive from these parameters which are already fixed at the first step.",
                    "label": 0
                },
                {
                    "sent": "Then you basically compute the centroid.",
                    "label": 1
                },
                {
                    "sent": "Now, once you have found a centroid, you can basically compute the divergance to all the points in the data, because the centroid is fixed, you have a distance that our distance or divergent measure.",
                    "label": 0
                },
                {
                    "sent": "So you can calculate all those divergences.",
                    "label": 1
                },
                {
                    "sent": "Now the question is how we can find this coding policy?",
                    "label": 0
                },
                {
                    "sent": "How we do find this Q zero given X and Q0?",
                    "label": 0
                },
                {
                    "sent": "And basically you iterate till convergence.",
                    "label": 0
                },
                {
                    "sent": "So the first part is easy as we have seen like once we have fixed that, we can easily compute.",
                    "label": 0
                },
                {
                    "sent": "Now we'll see an efficient algorithm to solve the second part.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's define the set C to be all those points whose coding policy sets it to be in the one class with all the probability.",
                    "label": 1
                },
                {
                    "sent": "So the Q zero given X is 1, and for each point let's define the score for South of X, which is better times the distortion from the centroid, which we have fixed currently and is log prior.",
                    "label": 0
                },
                {
                    "sent": "Then.",
                    "label": 0
                },
                {
                    "sent": "This lemma says that there is a teacher or some threshold.",
                    "label": 0
                },
                {
                    "sent": "At which will.",
                    "label": 0
                },
                {
                    "sent": "Beyond if.",
                    "label": 0
                },
                {
                    "sent": "If the score of the points is less than that Tita, then the points will be assigned to this one class that we're interested in an.",
                    "label": 0
                },
                {
                    "sent": "If it's not, then it's going to be self coded or would be put outside the class.",
                    "label": 0
                },
                {
                    "sent": "And as we can see this is just involves sorting an.",
                    "label": 1
                },
                {
                    "sent": "You can do that in M log M time, where M is the number of points in the data.",
                    "label": 0
                },
                {
                    "sent": "So this using this we can efficiently compute the assignment of the points though on class.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's look at this figure of a phase transition and which will give some intuition into working of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So on this axis we have inverse of this parameter beta.",
                    "label": 0
                },
                {
                    "sent": "So you can think of that as a temperature.",
                    "label": 0
                },
                {
                    "sent": "So the temperature and beta are inversely related.",
                    "label": 0
                },
                {
                    "sent": "And on this axis we have the identity of the points and also the distortion measures.",
                    "label": 0
                },
                {
                    "sent": "OK, so .2 point #2 if it's put in the class one class cluster then it's always going to suffer a distortion of two.",
                    "label": 0
                },
                {
                    "sent": "And this is our coding policy.",
                    "label": 0
                },
                {
                    "sent": "I mean like with what probability we're going to assign it to the one class so when the temperature is very high initially.",
                    "label": 0
                },
                {
                    "sent": "So at this point then you can think of it.",
                    "label": 0
                },
                {
                    "sent": "Everything has melted, everything is in the one class.",
                    "label": 0
                },
                {
                    "sent": "Now you keep on decreasing the temperature are increasing the beta, then we see like the point with the highest distortion is pulled out of the one class.",
                    "label": 0
                },
                {
                    "sent": "So whenever it's less than one, that that point is not going to be put in the one class and will be going will be self coded an as you continuously decrease the temperature more and more points are pulled out of the cluster.",
                    "label": 0
                },
                {
                    "sent": "And you can think of it like the points are getting crystallized at some critical temperature here.",
                    "label": 0
                },
                {
                    "sent": "All the points are pulled out of the one class and everything is self coded.",
                    "label": 0
                },
                {
                    "sent": "OK, so this shows in a clear way the behavior of the algorithm for different values of this parameter beta.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now that was one class model for.",
                    "label": 0
                },
                {
                    "sent": "Area distortion based model for one class.",
                    "label": 0
                },
                {
                    "sent": "Now we will see how we can extend that to multiclass clustering.",
                    "label": 0
                },
                {
                    "sent": "That's the standard setting.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically, one could think that now we have M data points and K centroids, so we can instead of just having one class, we can just replace that with CK classes right now.",
                    "label": 1
                },
                {
                    "sent": "But the problem here is that if you are not going to self code, Excel coding probabilities given by this QX given X so 1 -- Q X given X should be the probability that you are going to assign that point using one of those K centroids.",
                    "label": 0
                },
                {
                    "sent": "The problem is that that.",
                    "label": 0
                },
                {
                    "sent": "Quantity doesn't tell you which which of those.",
                    "label": 0
                },
                {
                    "sent": "K centroids, you should assign that point.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our suggestion is to actually go through again this intermediate variable 0.",
                    "label": 0
                },
                {
                    "sent": "So at first state you decide whether you want to self code or whether you want to cluster using one of these.",
                    "label": 0
                },
                {
                    "sent": "Code words, in which case we have two and three points.",
                    "label": 0
                },
                {
                    "sent": "Then once you have done that then you can use a hard clustering algorithm for all the points that you didn't want to self code.",
                    "label": 0
                },
                {
                    "sent": "Assign it to all these different clusters and then you can iterate this process.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the same thing as I have said at the first step, use the original one class algorithm to decide whether you want to self code that point or not.",
                    "label": 1
                },
                {
                    "sent": "So this in a way it is MCRD.",
                    "label": 0
                },
                {
                    "sent": "This multiclass algorithm is using the first step as a subroutine to compute the first step here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then in the second step we use a hard clustering algorithm to classify all those points that we decided not to self code.",
                    "label": 0
                },
                {
                    "sent": "In the first instance and then again we iterate till convergence.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I have left the details regarding the multiclass algorithm for the interest of time, but there is a poster today evening.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, we'll be happy to describe more, so let's look at some experimental results.",
                    "label": 0
                },
                {
                    "sent": "So we evaluated the algorithm in three different settings in first setting.",
                    "label": 0
                },
                {
                    "sent": "Basically we compared it to previous one class document classification problem that have been published before, and we also do multiclass synthetic multiclass clustering on synthetic data.",
                    "label": 1
                },
                {
                    "sent": "And multi class clustering on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real world, so in this document classification task we compare it to this Creamer and Jessica's work which is represented by OCIB and previous convex loss based method due to Kramer and Singer represented by OC Convex.",
                    "label": 0
                },
                {
                    "sent": "So we see that we do we.",
                    "label": 0
                },
                {
                    "sent": "Achieve comparable performance to those methods.",
                    "label": 0
                },
                {
                    "sent": "In one case we dominate both.",
                    "label": 0
                },
                {
                    "sent": "Another case we actually get a trade off at the two extreme recall regions we perform as good as the best an in between we do, compareable.",
                    "label": 0
                },
                {
                    "sent": "To the other methods and we have some reasons regarding this which I won't get into now, so so this experiment shows that we are doing compatible and in some cases better than that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Various methods.",
                    "label": 0
                },
                {
                    "sent": "So in this.",
                    "label": 0
                },
                {
                    "sent": "Clustering task let me first tell you that there are different colors and also some of the points.",
                    "label": 0
                },
                {
                    "sent": "These black dots are.",
                    "label": 0
                },
                {
                    "sent": "Actually, the self coded points.",
                    "label": 0
                },
                {
                    "sent": "So basically we ran this multiclass clustering algorithm and we sample 400 points from 4 Gaussians and 500 points from this uniform background.",
                    "label": 0
                },
                {
                    "sent": "Now we asked multiclass clustering algorithm to classify these points using five centroids.",
                    "label": 0
                },
                {
                    "sent": "We specified 5 to that like it could model the noise using one of the clusters that we just wanted to see how it performs.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, like this centroids are represented by this bold circles, so some of the noise points are put in here, but most of the points are self coded, so these black points these points in between these clusters are not put into any of this.",
                    "label": 0
                },
                {
                    "sent": "Five clusters and the interesting other interesting point is, as we increase the value of beta, so beta here is 100 and beta.",
                    "label": 0
                },
                {
                    "sent": "Here is 140.",
                    "label": 0
                },
                {
                    "sent": "So as we increase the beta, the distortion is much more much costly.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm rightly decides on the number of clusters to be 4 in this case.",
                    "label": 0
                },
                {
                    "sent": "So even though we specified the number required number of clusters to be 5, it correctly learned a number correct number of.",
                    "label": 0
                },
                {
                    "sent": "The clusters, which is 4 an all the noisy points in the in between are basically self coded, so the self coded points at the points we don't care or like they give up.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also compared it to.",
                    "label": 0
                },
                {
                    "sent": "Other method called sequential information bottleneck due to Norm Sloaney mentality, HP and so we wanted to see whether if we introduce this capability of giving up a few points whether we could do a better clustering task an from this plot, it shows that at all values of recall we achieve better precision simply by allowing this, allowing the algorithm to give up some of the points and do.",
                    "label": 0
                },
                {
                    "sent": "Get clustering or estimation of the centroid with the remaining ones, and so that basically shows that if you have that capability which directly are naturally extends from this one class model that we propose is very helpful in this task.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that brings us to the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So to wrap up.",
                    "label": 0
                },
                {
                    "sent": "We have cast the problem of identifying a small coherent subset using this one class model which trades off between this class size.",
                    "label": 1
                },
                {
                    "sent": "How much you want to compress and how much distortion you are willing to suffer.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we have shown a way to move from one class to the standard multiclass clustering with the background noise left up, left out.",
                    "label": 1
                },
                {
                    "sent": "So which is basically the ability to give up some of the points.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as part of future work we are now looking at whether we could extend these kind of methods to other instances and distortions.",
                    "label": 0
                },
                {
                    "sent": "For example, data on graphs or manifolds.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that that's N thanks for time.",
                    "label": 0
                },
                {
                    "sent": "Why do you have to go through a two stage approach for the Multicentre case rather than defining probability, for example and finding the self coding probabilities the marginal?",
                    "label": 0
                },
                {
                    "sent": "So basically we wanted like first, so I think I hinted a little bit there.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you are not deciding to self code the points then like we don't know which cluster we want to assign that point right?",
                    "label": 0
                },
                {
                    "sent": "So we thought like we'll split it up into 2 steps and in fact in the paper we show that like this, optimization is equivalent to the information bottleneck.",
                    "label": 0
                },
                {
                    "sent": "So there is a theorem there.",
                    "label": 0
                },
                {
                    "sent": "Just to follow up on that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so instead of this, simply doing one class compression.",
                    "label": 0
                },
                {
                    "sent": "So you could do exactly the same thing, right?",
                    "label": 0
                },
                {
                    "sent": "So you could say hi to, Roger needs to be coded as one of five classes, or else can be covered as itself with that wind up producing equivalent thought of plots.",
                    "label": 0
                },
                {
                    "sent": "Clustering as usual, two stage product life, prospering algorithm.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yes, right now.",
                    "label": 0
                },
                {
                    "sent": "The the sub consistent equations on that don't separate.",
                    "label": 0
                },
                {
                    "sent": "Because of their marginalization.",
                    "label": 0
                },
                {
                    "sent": "OK, another question.",
                    "label": 0
                },
                {
                    "sent": "I was wondering whether in the in the one class this could review this EM with garbage components, right?",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's an interesting question, so we don't know.",
                    "label": 0
                },
                {
                    "sent": "So we have acknowledged the connection to M in the paper and so.",
                    "label": 0
                },
                {
                    "sent": "But we don't know exactly what form it is, but I agree that it's like very similar to this expectation and maximization steps as in standard.",
                    "label": 0
                },
                {
                    "sent": "But we don't know exactly what form it could be.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So you better is basically a regularizer of the solution you have array of estimating that counts.",
                    "label": 0
                },
                {
                    "sent": "No, not as of now, but there are some works in that direction by other people which could be leveraged on.",
                    "label": 0
                },
                {
                    "sent": "Text.",
                    "label": 0
                }
            ]
        }
    }
}