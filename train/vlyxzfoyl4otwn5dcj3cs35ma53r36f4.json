{
    "id": "vlyxzfoyl4otwn5dcj3cs35ma53r36f4",
    "title": "Maximum Likelihood vs. Sequential Normalized Maximum Likelihood in On-line Density Estimation",
    "info": {
        "author": [
            "Wojciech Kotlowski, Institute of Computing Science, Poznan University of Technology"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2011_kotlowski_maximum/",
    "segmentation": [
        [
            "So it's a joint work with Peter Gunn fault and it's about maximum likelihood.",
            "An improvement of demand maximum likelihood, which is called sequential normalized maximum likelihood for online density estimation or online learning.",
            "In this case with exponential."
        ],
        [
            "Comedies, so here's the setting.",
            "So we will be.",
            "I'll be talking about sequential prediction with log loss and the set of experts.",
            "Is going to be an exponential family, so in this case the set of strategies against which we are going to play is going to be like uncountable set, but parametric set which forms exponential families.",
            "So each expert is going to be a single distribution from exponential family, so each expert is going to be like a parameter vector, and now the probably the easiest and mostly use strategy would be the maximum likelihood strategy, which can be summarized by.",
            "Saying predict with the best distribution on outcast outcomes seen so far, so it's like it corresponds to the follow the leader strategy in online learning and although follow leader is known to perform pretty bad in some situations, in this case it actually works quite well because you can get regret bound.",
            "All of log N, but to do.",
            "To do that you actually need to have boundedness, boundedness of the data, so the data sequence needs to be bounded.",
            "For example needs to be within some ball of a fix right radius around the origin, and the constant in front of this.",
            "Oh, unfortunately depends on the bound on the data, so we're trying to fix that by introducing a small modification to maximum likelihood, which we call sequential normalized maximum likelihood strategy.",
            "And this is based on the idea to predict with the best distribution, but not only on past outcomes, but also including the current output.",
            "This sounds quite weird because we haven't seen this outcome yet, but later I'll tell it say that it's actually possible and it leads to a valid prediction strategy.",
            "And the result which will get is that.",
            "In this case we don't need any additional assumptions like we don't need the assumption that the data are bounded and also we get the logarithmic regret and the constant in front of the regret.",
            "In this case what we get is the optimal constants on the best constants which we can get in the problem.",
            "So there are several applications of this problem such as prediction coding."
        ],
        [
            "Model selection.",
            "So as I said, the we're going to compete against experts with former family of distributions, so this family will be parameterized by some vector mu from some parameter space sci, and we will only deal with exponential families.",
            "Such as Gaussian, Bernoulli, multinomial and so on.",
            "So there is a sequence of outcomes from some outcome space revealed 1 by 1.",
            "And in each iteration, after observing first N -- 1 outcomes, the learner is supposed to predict the next outcome by playing with a distribution.",
            "So the learner assign a distribution over the outcome SpaceX.",
            "And so the distribution will always be written in this conditional form by because we want to stress that the learner can base his prediction on past outcomes and after the outcome is revealed, the learner suffers log loss, which is the minus log of the probability that the learner assigned to the outcome that actually was revealed and we measure the.",
            "We measured the accuracy of dollar random or the performance of.",
            "Learner with respect to the best experts from the family P using the notion of regret, which is just the difference between the cumulative loss of the expert and the cumulative loss of the best strategy from the model from the exponential family.",
            "In this case, the strategies here are not doesn't not have conditional form.",
            "Here, because the we assume that the data sequences in the exponential family are our model using IID assumptions.",
            "And.",
            "We don't make we try to make no assumptions about the data sequence, so we assume that the data might even be adversarial.",
            "There's no like IID assumptions that the data are generated are generated by some distr."
        ],
        [
            "Using that system.",
            "So here is a simple example.",
            "So P might be a set of all Bernoulli distribution while Mu will be in this case the.",
            "Mean value or the probability of success?",
            "And let's say we observe such an outcome.",
            "Such a sequence of outcomes.",
            "And in this case the best expert in this family will just be the expert which plays with the maximum likelihood distribution.",
            "This X and bar is just the maximum likelihood for Bernoulli is just the ratio of observed ones to the observed outcomes, and in this case we can come up with strategies like for example the known strategy, Laplace rule of succession, which are actually we're not going to talk about later.",
            "So this strategy plays with also with one of the distributions, one of the Bernoulli distributions with parameter mu I, which is just smooth maximum likelihood from past outcomes, and here are some predictions of the strategy and in this case actually one can show that as long as the.",
            "Sequence of data is such that the maximum likelihood parameter for that sequence does not come too close to the boundary, so to zero or one, then one can show that the regret of this strategy is equal to half of the log N plus some constant, and this is actually the opt."
        ],
        [
            "I'll think that one can do in this problem, but without additional assumption the problem.",
            "And there's still a serious problem that.",
            "If the data space.",
            "So if the space of the outcomes is unbounded then we can suffer.",
            "We will suffer infinite regret already in the first iteration, so.",
            "It doesn't happen for Bernoulli because for better needed outcome space is bounded between zero and one, but consider Gaussian distribution and suppose that we start with assigning some distribution as a learner.",
            "So whatever distribution we assign, this distribution is normalized, which means that it must.",
            "It must converge to zero at Infinity at the same time, the adversary can choose the outcome as far away from origin.",
            "As as he wants, and in this case.",
            "The adversary will always suffer finite loss, while by taking the outcome far away from origin, our loss become larger and larger at.",
            "At the end we have no guarantee for finite loss in this case, so we will suffer infinite regret in the first iteration.",
            "So how to avoid that?",
            "So we will try to introduce solution without making any assumptions on the data sequence.",
            "We will just try to bound the parameter space, so we assume that the parameter space will be a compact subset.",
            "Of the typical parameter space for exponential families will call this subset size 0, and we assume that we compete only with the distributions from this compact subset.",
            "So this is like in the spirit of prediction of experts because we rather want to put constraints on the set of distributions that we are trying to compete with, or do it strategies rather than on the data sequence.",
            "And in this case we will define the maximum likelihood relative to this.",
            "Subset size 0 by the minimizer of the cumulative loss of the of the distributions from this size 0.",
            "So this mu head of N is going to be the maximum likelihood within this size 0.",
            "And then the regret can be written as the difference between the performance of the learner and the performance of this maximum likelihood strategy.",
            "But this fact, sorry, not strategies maximum likelihood distribution which knows all the data in hindsight."
        ],
        [
            "One can also show that this maximum likelihood mu hat event is actually a Bregman projection of the unconstrained maximum likelihood, which in case of exponential families has a very simple form, so unconstrained maximum likelihood is just the arithmetic average of the all past data, and this mu hat event is just the Bregman projection of this unconstrained maximum likelihood.",
            "Onto the this compact subspace XIO.",
            "So we actually I didn't say that here, but we actually assume that this size 0 is also convex.",
            "So first let's define the maximum likelihood or follow the leader.",
            "Strategy is just the strategy which predicts in the current iteration with the maximum likelihood distribution based on all past outcomes.",
            "So for example, for Bernoulli distribution, the unconstrained maximum likelihood parameter xanny bar would be the ratio of ones, and all outcomes that the number of outcomes.",
            "And in case we take the size 0 to be the interval.",
            "Between epsilon and 1 minus epsilon, then what we need to do is we need to take this unconstrained maximum likelihood and project it into this interval simply by.",
            "Truncating the value if it's outside the this interval.",
            "Similar things happen, for example, for Gaussian distribution with fixed unit covariance matrix.",
            "And if we assume that size 0 is just the set of all mean value parameters which are bounded, which we've bounded norm by.",
            "Let's say some large air R, then as previously the maximum likelihood unconstrained maximum likelihood parameter is just the arithmetic average, while the constraint one is again the projected version of the arithmetic average.",
            "By doing by changing by rescaling, actually the maximum likelihood."
        ],
        [
            "Rosen.",
            "So the theorem which we are able to prove for the maximum likelihood strategy is that regret of this strategy can be bounded by constant times log N plus some additional constant factor and this constant in front of the log and depends on future on some quantities.",
            "So first one is B and this is actually this bound on the data sequence.",
            "Then the other one C is the let's say the radius of this size 0 is just the.",
            "It's just that their mind, by choice of this size 0.",
            "And then there's also a bound on the Fisher Information matrix appearing in this in this in this constant and.",
            "This bound is essentially fight, so for some special cases one can improve this bound.",
            "But basically this dependence on the bound on the data be an let sufficient information.",
            "This is unavoidable so.",
            "As we go with B2 Infinity, so there's no bound on the data set.",
            "An adversary can play as in any way as he wants.",
            "In this case, we cannot get.",
            "We cannot get any logarithmic bound at all.",
            "Actually will be suffering infinite regret.",
            "Even even if we bounded the set of strategies to this subset size."
        ],
        [
            "So we introduce the sequential normalized maximum likelihood, and this is the strategy which is based by on taking apart from the past outcomes.",
            "We also take the current outcome into account, so it is defined.",
            "So for given XN we assign to X and probability which is given by the maximum likelihood distribution based on not only the past outcomes but also the current outcome.",
            "And since this doesn't normalize to one.",
            "If we take the integral over the outcome space, we need to divide it by Z, which is additional parameters which is additional normalization.",
            "So.",
            "This strategy will also will also usually be outside the original family, so we will not play with one of the distributions from the exponential family.",
            "This strategy will not have the same form as the original distributions from exponential family.",
            "There were similar ideas proposed in the past, for example, the last step, minimax algorithm, or forward algorithm.",
            "There are small differences and unfortunately I will not have time to."
        ],
        [
            "Talk about that more.",
            "This this sequential normalized maximum likelihood.",
            "Such was introduced previously by recent and Ross, but they didn't give a general regret bound.",
            "They only managed to give a regret bound in a very specific situation.",
            "For Gaussian linear models so.",
            "Let's see some examples.",
            "So for example for Bernoulli distribution.",
            "This is what this what is in the numerator is just the maximum likelihood distribution for observing the sequence where in the past we observe K once out of N and Additionally at this moment we observe XN at.",
            "It needs to be normalized, So what is in the denominator is just a normalization.",
            "For Gaussian distribution with fixed covariance matrix, which is the identity we get, again Gaussian distribution.",
            "But this is not one of the distributions from the exponential family that we are playing with Becausw right now the variance is slightly larger.",
            "And also for exponential distribution, after playing this strategy, we actually get a distribution which look more like a power law distribution rather."
        ],
        [
            "Exponential.",
            "And we are able to show the following theorem.",
            "So we say we show that.",
            "For this sequential normalized maximum likelihood strategy, we are able to bound the regret by the term K / 2 log N plus some constant where the constant may depend on this size 0 on this size 0 set, but will does not depend on the data sequence and this constant in front of the Logan is actually optimal, so this is the best one can get in this kind of problems.",
            "And as I said this, this bound does not require any assumptions about the data at all."
        ],
        [
            "So here is the example how those two strategies would work in a very simple problem of estimating the mean of the Gaussian distribution.",
            "So we assume that the mean is unknown and the variances unit.",
            "And we're just trying to play with Gaussian distributions, and for simplicity will assume that we won't project.",
            "So there's going to be no projections to the size 0 or 0 is just equal to side, and in this case we can compare the predictions of both strategies.",
            "There are a set similar, only the variance of the.",
            "Mel strategies slightly larger, so this larger variance causes the.",
            "Distribution.",
            "Of SNL to have to have.",
            "Put more mass on the outcomes which are far away from origin to say to have heavier tails and in this case if we observe sequence of outcomes like 2 -- 2, two minus two which which actually does not look like being generated from one of the Gaussian distributions with unit variance."
        ],
        [
            "Then we will perform better."
        ],
        [
            "Playing the."
        ],
        [
            "SMS."
        ],
        [
            "Strategy, so here's the.",
            "The function of the regret."
        ],
        [
            "Of the iteration number so we can see that both regrets the regret of XML diverges."
        ],
        [
            "And one last thing I would like to talk about is the connection between SNL and base.",
            "So we define the base strategy by defining a prior distribution over the family over the distributions from the exponential family.",
            "And then we predict according to the posterior.",
            "So that's a normal based procedure.",
            "And then we update the posterior.",
            "And one can show that only under slightly stronger assumptions.",
            "Debates achieve the optimal regret and optimal rate for regret as well, so it's also K / 2 log N plus of one, and also this 01 term can be minimized by the circle."
        ],
        [
            "Jeffreys prior and now there is surprising connection between those two algorithms, namely for several distributions such as Gaussian Gamma.",
            "Also then, including exponential distribution, Gaussian with unknown mean with both under mean and variance and when projections onto the size 0 are skipped.",
            "In this case we observe that the SNL is actually equal to debates with Jeffreys prior, so those two strategies predict in the same way.",
            "But that doesn't that doesn't happen always because, for example, for Bernoulli.",
            "SNL seems to work better than base, so the question is what is general relation between SNL and base?",
            "And, well, we don't know this, but we will able to prove that in cases when SNL is equal to base, then one can also show that actually both algorithms are minimax optimal, not only asymptotically, but they play exactly in the same way as normalized maximum likelihood or starkov.",
            "Prediction strategy, which is not to be minmax optimal.",
            "There is a small problem that actually for many models charcoal might be not defined because the regret is infinite.",
            "But when we do some conditioning on the 1st outcome then things became defined again and then this is what I showed here actually hold."
        ],
        [
            "So concluding, what we managed to show is the general regret bound for maximum log likelihood strategy for exponential families, and we mention improve maximum likelihood by taking the current outcome into account leading, which will let us to this sequential normalized maximum likelihood strategy, which we have the optimal regret bounds without any additional assumptions on the data sequence.",
            "This might have several application and we still don't know what is exact relation between SNL and base, for example, for which models those two strategies are the same.",
            "Thank you very much.",
            "For finite alphabets, starkov is minimax optimal.",
            "Yes, yes, so that includes Bernoulli yes.",
            "So in case I'm very newly.",
            "SMS is different.",
            "Starkov is equal to SML, at least on Bernoulli multi.",
            "Now actually for Burnley.",
            "Not because for baloney SNL is different than base which means that this thing what I"
        ],
        [
            "Well I showed below doesn't hold.",
            "Not right, but stark of Amber.",
            "Newly stark often is not SNL.",
            "No, it's not.",
            "So somehow for Gaussian distributions and gamma base with Jeffreys priors equal to SNL is equal to start off from some reason.",
            "Actually, each time SNL is equal to base then they are both equal to start off.",
            "For Bernie, SNL is better than base and actually starkovich better than SNL.",
            "So what's can you?",
            "Can you explain it towards the difference how SNL is different from Stark off of Bernoulli?",
            "So for Bernoulli, SNL only takes like the current outcome into account by calculating the maximum likelihood.",
            "But what's darker would do is to go up to the time horizon to the end of the game and include all possible outcomes.",
            "So stucco like this Emma is like.",
            "Next steps tarkov, so it assumes that the game ends in the current iteration.",
            "Starkov would assume that the game ends actually because Tarkov knows the time horizon of the game.",
            "So that's the main difference, and it changes with the time horizon.",
            "Any other questions?",
            "Sorry I didn't get for Bernoulli in.",
            "In what respect is SNL better than base?",
            "So so this is the old we actually not here.",
            "This is the all result from the paper by Takemoto and War mode.",
            "About the was called last Step Minimax algorithm and they actually considered this kind of algorithm for Bernoulli."
        ],
        [
            "Yes, this kind of algorithm for Bernoulli and they managed to get better bound and better performance for this last step, minimize which corresponds to SNL here.",
            "Then, for base with Jefferies for Bernoulli, but the difference was only in, it was just a constant.",
            "So yes, small constant the rate did constant in front of the log in is already optimal so.",
            "It's only there, yes.",
            "Fire in the.",
            "Then I'm likely ahead.",
            "Can you characterize more settings where these things start agreeing with each other?",
            "Excuse me, can you allow yourself a prior an SNL like?",
            "Make it a map kind of thing?",
            "Then can you characterize more settings where these things might agree or?",
            "How it behaves?",
            "SNL is kind of prior free, but you need to put in a prior in and do like normalized map or something.",
            "So you mean you would map for example performing this setting.",
            "This is what you're explaining.",
            "How would map you could look at some sort of normalized map estimate.",
            "Oh OK, so you mean that I would put some kind of a prior on.",
            "Excellence prior would correspond to the prior would be included in the regrets over penalizing the grid?",
            "And then you get a normalized map and then now can you start making connections to basil little more closely?",
            "Because based technique is a priority two and then you get closer connections.",
            "So I think the base with 'cause regression at the same yes, yes.",
            "Basically that's kind of an easy case, so there are other cases when there.",
            "When they might.",
            "OK. Yeah, that's a that's interesting issue.",
            "I think that the base with Jeffreys prior taking relative to this to this prior would be then the equal to the normalized maximum likelihood.",
            "So.",
            "In general, I mean no no.",
            "For for Gaussians, for Gaussian, yes, yes, yes, right.",
            "But for anything else or just anything I don't know.",
            "Not sure.",
            "Thanks for the question.",
            "Thanks.",
            "So there's a paper of GAIL Shamiran on the MD's principle with large alphabets, so they he still had similarly results.",
            "So can you comment on if your method is similar to that or?",
            "I think I don't know this paper.",
            "OK, I can show you that.",
            "OK thanks.",
            "Any other questions?",
            "Alright, let's thank the speaker beginning."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's a joint work with Peter Gunn fault and it's about maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "An improvement of demand maximum likelihood, which is called sequential normalized maximum likelihood for online density estimation or online learning.",
                    "label": 1
                },
                {
                    "sent": "In this case with exponential.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comedies, so here's the setting.",
                    "label": 0
                },
                {
                    "sent": "So we will be.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about sequential prediction with log loss and the set of experts.",
                    "label": 1
                },
                {
                    "sent": "Is going to be an exponential family, so in this case the set of strategies against which we are going to play is going to be like uncountable set, but parametric set which forms exponential families.",
                    "label": 0
                },
                {
                    "sent": "So each expert is going to be a single distribution from exponential family, so each expert is going to be like a parameter vector, and now the probably the easiest and mostly use strategy would be the maximum likelihood strategy, which can be summarized by.",
                    "label": 1
                },
                {
                    "sent": "Saying predict with the best distribution on outcast outcomes seen so far, so it's like it corresponds to the follow the leader strategy in online learning and although follow leader is known to perform pretty bad in some situations, in this case it actually works quite well because you can get regret bound.",
                    "label": 0
                },
                {
                    "sent": "All of log N, but to do.",
                    "label": 0
                },
                {
                    "sent": "To do that you actually need to have boundedness, boundedness of the data, so the data sequence needs to be bounded.",
                    "label": 0
                },
                {
                    "sent": "For example needs to be within some ball of a fix right radius around the origin, and the constant in front of this.",
                    "label": 0
                },
                {
                    "sent": "Oh, unfortunately depends on the bound on the data, so we're trying to fix that by introducing a small modification to maximum likelihood, which we call sequential normalized maximum likelihood strategy.",
                    "label": 1
                },
                {
                    "sent": "And this is based on the idea to predict with the best distribution, but not only on past outcomes, but also including the current output.",
                    "label": 0
                },
                {
                    "sent": "This sounds quite weird because we haven't seen this outcome yet, but later I'll tell it say that it's actually possible and it leads to a valid prediction strategy.",
                    "label": 0
                },
                {
                    "sent": "And the result which will get is that.",
                    "label": 0
                },
                {
                    "sent": "In this case we don't need any additional assumptions like we don't need the assumption that the data are bounded and also we get the logarithmic regret and the constant in front of the regret.",
                    "label": 0
                },
                {
                    "sent": "In this case what we get is the optimal constants on the best constants which we can get in the problem.",
                    "label": 0
                },
                {
                    "sent": "So there are several applications of this problem such as prediction coding.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model selection.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the we're going to compete against experts with former family of distributions, so this family will be parameterized by some vector mu from some parameter space sci, and we will only deal with exponential families.",
                    "label": 0
                },
                {
                    "sent": "Such as Gaussian, Bernoulli, multinomial and so on.",
                    "label": 1
                },
                {
                    "sent": "So there is a sequence of outcomes from some outcome space revealed 1 by 1.",
                    "label": 0
                },
                {
                    "sent": "And in each iteration, after observing first N -- 1 outcomes, the learner is supposed to predict the next outcome by playing with a distribution.",
                    "label": 1
                },
                {
                    "sent": "So the learner assign a distribution over the outcome SpaceX.",
                    "label": 0
                },
                {
                    "sent": "And so the distribution will always be written in this conditional form by because we want to stress that the learner can base his prediction on past outcomes and after the outcome is revealed, the learner suffers log loss, which is the minus log of the probability that the learner assigned to the outcome that actually was revealed and we measure the.",
                    "label": 0
                },
                {
                    "sent": "We measured the accuracy of dollar random or the performance of.",
                    "label": 0
                },
                {
                    "sent": "Learner with respect to the best experts from the family P using the notion of regret, which is just the difference between the cumulative loss of the expert and the cumulative loss of the best strategy from the model from the exponential family.",
                    "label": 0
                },
                {
                    "sent": "In this case, the strategies here are not doesn't not have conditional form.",
                    "label": 0
                },
                {
                    "sent": "Here, because the we assume that the data sequences in the exponential family are our model using IID assumptions.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We don't make we try to make no assumptions about the data sequence, so we assume that the data might even be adversarial.",
                    "label": 0
                },
                {
                    "sent": "There's no like IID assumptions that the data are generated are generated by some distr.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Using that system.",
                    "label": 0
                },
                {
                    "sent": "So here is a simple example.",
                    "label": 0
                },
                {
                    "sent": "So P might be a set of all Bernoulli distribution while Mu will be in this case the.",
                    "label": 0
                },
                {
                    "sent": "Mean value or the probability of success?",
                    "label": 0
                },
                {
                    "sent": "And let's say we observe such an outcome.",
                    "label": 0
                },
                {
                    "sent": "Such a sequence of outcomes.",
                    "label": 0
                },
                {
                    "sent": "And in this case the best expert in this family will just be the expert which plays with the maximum likelihood distribution.",
                    "label": 0
                },
                {
                    "sent": "This X and bar is just the maximum likelihood for Bernoulli is just the ratio of observed ones to the observed outcomes, and in this case we can come up with strategies like for example the known strategy, Laplace rule of succession, which are actually we're not going to talk about later.",
                    "label": 1
                },
                {
                    "sent": "So this strategy plays with also with one of the distributions, one of the Bernoulli distributions with parameter mu I, which is just smooth maximum likelihood from past outcomes, and here are some predictions of the strategy and in this case actually one can show that as long as the.",
                    "label": 0
                },
                {
                    "sent": "Sequence of data is such that the maximum likelihood parameter for that sequence does not come too close to the boundary, so to zero or one, then one can show that the regret of this strategy is equal to half of the log N plus some constant, and this is actually the opt.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll think that one can do in this problem, but without additional assumption the problem.",
                    "label": 0
                },
                {
                    "sent": "And there's still a serious problem that.",
                    "label": 0
                },
                {
                    "sent": "If the data space.",
                    "label": 0
                },
                {
                    "sent": "So if the space of the outcomes is unbounded then we can suffer.",
                    "label": 0
                },
                {
                    "sent": "We will suffer infinite regret already in the first iteration, so.",
                    "label": 1
                },
                {
                    "sent": "It doesn't happen for Bernoulli because for better needed outcome space is bounded between zero and one, but consider Gaussian distribution and suppose that we start with assigning some distribution as a learner.",
                    "label": 0
                },
                {
                    "sent": "So whatever distribution we assign, this distribution is normalized, which means that it must.",
                    "label": 0
                },
                {
                    "sent": "It must converge to zero at Infinity at the same time, the adversary can choose the outcome as far away from origin.",
                    "label": 0
                },
                {
                    "sent": "As as he wants, and in this case.",
                    "label": 0
                },
                {
                    "sent": "The adversary will always suffer finite loss, while by taking the outcome far away from origin, our loss become larger and larger at.",
                    "label": 0
                },
                {
                    "sent": "At the end we have no guarantee for finite loss in this case, so we will suffer infinite regret in the first iteration.",
                    "label": 0
                },
                {
                    "sent": "So how to avoid that?",
                    "label": 0
                },
                {
                    "sent": "So we will try to introduce solution without making any assumptions on the data sequence.",
                    "label": 0
                },
                {
                    "sent": "We will just try to bound the parameter space, so we assume that the parameter space will be a compact subset.",
                    "label": 0
                },
                {
                    "sent": "Of the typical parameter space for exponential families will call this subset size 0, and we assume that we compete only with the distributions from this compact subset.",
                    "label": 1
                },
                {
                    "sent": "So this is like in the spirit of prediction of experts because we rather want to put constraints on the set of distributions that we are trying to compete with, or do it strategies rather than on the data sequence.",
                    "label": 1
                },
                {
                    "sent": "And in this case we will define the maximum likelihood relative to this.",
                    "label": 0
                },
                {
                    "sent": "Subset size 0 by the minimizer of the cumulative loss of the of the distributions from this size 0.",
                    "label": 0
                },
                {
                    "sent": "So this mu head of N is going to be the maximum likelihood within this size 0.",
                    "label": 0
                },
                {
                    "sent": "And then the regret can be written as the difference between the performance of the learner and the performance of this maximum likelihood strategy.",
                    "label": 0
                },
                {
                    "sent": "But this fact, sorry, not strategies maximum likelihood distribution which knows all the data in hindsight.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One can also show that this maximum likelihood mu hat event is actually a Bregman projection of the unconstrained maximum likelihood, which in case of exponential families has a very simple form, so unconstrained maximum likelihood is just the arithmetic average of the all past data, and this mu hat event is just the Bregman projection of this unconstrained maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Onto the this compact subspace XIO.",
                    "label": 0
                },
                {
                    "sent": "So we actually I didn't say that here, but we actually assume that this size 0 is also convex.",
                    "label": 0
                },
                {
                    "sent": "So first let's define the maximum likelihood or follow the leader.",
                    "label": 0
                },
                {
                    "sent": "Strategy is just the strategy which predicts in the current iteration with the maximum likelihood distribution based on all past outcomes.",
                    "label": 0
                },
                {
                    "sent": "So for example, for Bernoulli distribution, the unconstrained maximum likelihood parameter xanny bar would be the ratio of ones, and all outcomes that the number of outcomes.",
                    "label": 0
                },
                {
                    "sent": "And in case we take the size 0 to be the interval.",
                    "label": 0
                },
                {
                    "sent": "Between epsilon and 1 minus epsilon, then what we need to do is we need to take this unconstrained maximum likelihood and project it into this interval simply by.",
                    "label": 0
                },
                {
                    "sent": "Truncating the value if it's outside the this interval.",
                    "label": 0
                },
                {
                    "sent": "Similar things happen, for example, for Gaussian distribution with fixed unit covariance matrix.",
                    "label": 1
                },
                {
                    "sent": "And if we assume that size 0 is just the set of all mean value parameters which are bounded, which we've bounded norm by.",
                    "label": 0
                },
                {
                    "sent": "Let's say some large air R, then as previously the maximum likelihood unconstrained maximum likelihood parameter is just the arithmetic average, while the constraint one is again the projected version of the arithmetic average.",
                    "label": 0
                },
                {
                    "sent": "By doing by changing by rescaling, actually the maximum likelihood.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rosen.",
                    "label": 0
                },
                {
                    "sent": "So the theorem which we are able to prove for the maximum likelihood strategy is that regret of this strategy can be bounded by constant times log N plus some additional constant factor and this constant in front of the log and depends on future on some quantities.",
                    "label": 0
                },
                {
                    "sent": "So first one is B and this is actually this bound on the data sequence.",
                    "label": 1
                },
                {
                    "sent": "Then the other one C is the let's say the radius of this size 0 is just the.",
                    "label": 0
                },
                {
                    "sent": "It's just that their mind, by choice of this size 0.",
                    "label": 0
                },
                {
                    "sent": "And then there's also a bound on the Fisher Information matrix appearing in this in this in this constant and.",
                    "label": 0
                },
                {
                    "sent": "This bound is essentially fight, so for some special cases one can improve this bound.",
                    "label": 0
                },
                {
                    "sent": "But basically this dependence on the bound on the data be an let sufficient information.",
                    "label": 0
                },
                {
                    "sent": "This is unavoidable so.",
                    "label": 1
                },
                {
                    "sent": "As we go with B2 Infinity, so there's no bound on the data set.",
                    "label": 0
                },
                {
                    "sent": "An adversary can play as in any way as he wants.",
                    "label": 0
                },
                {
                    "sent": "In this case, we cannot get.",
                    "label": 0
                },
                {
                    "sent": "We cannot get any logarithmic bound at all.",
                    "label": 0
                },
                {
                    "sent": "Actually will be suffering infinite regret.",
                    "label": 0
                },
                {
                    "sent": "Even even if we bounded the set of strategies to this subset size.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we introduce the sequential normalized maximum likelihood, and this is the strategy which is based by on taking apart from the past outcomes.",
                    "label": 1
                },
                {
                    "sent": "We also take the current outcome into account, so it is defined.",
                    "label": 0
                },
                {
                    "sent": "So for given XN we assign to X and probability which is given by the maximum likelihood distribution based on not only the past outcomes but also the current outcome.",
                    "label": 0
                },
                {
                    "sent": "And since this doesn't normalize to one.",
                    "label": 0
                },
                {
                    "sent": "If we take the integral over the outcome space, we need to divide it by Z, which is additional parameters which is additional normalization.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This strategy will also will also usually be outside the original family, so we will not play with one of the distributions from the exponential family.",
                    "label": 0
                },
                {
                    "sent": "This strategy will not have the same form as the original distributions from exponential family.",
                    "label": 0
                },
                {
                    "sent": "There were similar ideas proposed in the past, for example, the last step, minimax algorithm, or forward algorithm.",
                    "label": 1
                },
                {
                    "sent": "There are small differences and unfortunately I will not have time to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about that more.",
                    "label": 0
                },
                {
                    "sent": "This this sequential normalized maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Such was introduced previously by recent and Ross, but they didn't give a general regret bound.",
                    "label": 0
                },
                {
                    "sent": "They only managed to give a regret bound in a very specific situation.",
                    "label": 0
                },
                {
                    "sent": "For Gaussian linear models so.",
                    "label": 0
                },
                {
                    "sent": "Let's see some examples.",
                    "label": 0
                },
                {
                    "sent": "So for example for Bernoulli distribution.",
                    "label": 0
                },
                {
                    "sent": "This is what this what is in the numerator is just the maximum likelihood distribution for observing the sequence where in the past we observe K once out of N and Additionally at this moment we observe XN at.",
                    "label": 0
                },
                {
                    "sent": "It needs to be normalized, So what is in the denominator is just a normalization.",
                    "label": 0
                },
                {
                    "sent": "For Gaussian distribution with fixed covariance matrix, which is the identity we get, again Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "But this is not one of the distributions from the exponential family that we are playing with Becausw right now the variance is slightly larger.",
                    "label": 0
                },
                {
                    "sent": "And also for exponential distribution, after playing this strategy, we actually get a distribution which look more like a power law distribution rather.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Exponential.",
                    "label": 0
                },
                {
                    "sent": "And we are able to show the following theorem.",
                    "label": 0
                },
                {
                    "sent": "So we say we show that.",
                    "label": 0
                },
                {
                    "sent": "For this sequential normalized maximum likelihood strategy, we are able to bound the regret by the term K / 2 log N plus some constant where the constant may depend on this size 0 on this size 0 set, but will does not depend on the data sequence and this constant in front of the Logan is actually optimal, so this is the best one can get in this kind of problems.",
                    "label": 1
                },
                {
                    "sent": "And as I said this, this bound does not require any assumptions about the data at all.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is the example how those two strategies would work in a very simple problem of estimating the mean of the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the mean is unknown and the variances unit.",
                    "label": 0
                },
                {
                    "sent": "And we're just trying to play with Gaussian distributions, and for simplicity will assume that we won't project.",
                    "label": 0
                },
                {
                    "sent": "So there's going to be no projections to the size 0 or 0 is just equal to side, and in this case we can compare the predictions of both strategies.",
                    "label": 0
                },
                {
                    "sent": "There are a set similar, only the variance of the.",
                    "label": 0
                },
                {
                    "sent": "Mel strategies slightly larger, so this larger variance causes the.",
                    "label": 0
                },
                {
                    "sent": "Distribution.",
                    "label": 0
                },
                {
                    "sent": "Of SNL to have to have.",
                    "label": 0
                },
                {
                    "sent": "Put more mass on the outcomes which are far away from origin to say to have heavier tails and in this case if we observe sequence of outcomes like 2 -- 2, two minus two which which actually does not look like being generated from one of the Gaussian distributions with unit variance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we will perform better.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Playing the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SMS.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Strategy, so here's the.",
                    "label": 0
                },
                {
                    "sent": "The function of the regret.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the iteration number so we can see that both regrets the regret of XML diverges.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one last thing I would like to talk about is the connection between SNL and base.",
                    "label": 0
                },
                {
                    "sent": "So we define the base strategy by defining a prior distribution over the family over the distributions from the exponential family.",
                    "label": 0
                },
                {
                    "sent": "And then we predict according to the posterior.",
                    "label": 0
                },
                {
                    "sent": "So that's a normal based procedure.",
                    "label": 0
                },
                {
                    "sent": "And then we update the posterior.",
                    "label": 0
                },
                {
                    "sent": "And one can show that only under slightly stronger assumptions.",
                    "label": 0
                },
                {
                    "sent": "Debates achieve the optimal regret and optimal rate for regret as well, so it's also K / 2 log N plus of one, and also this 01 term can be minimized by the circle.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Jeffreys prior and now there is surprising connection between those two algorithms, namely for several distributions such as Gaussian Gamma.",
                    "label": 1
                },
                {
                    "sent": "Also then, including exponential distribution, Gaussian with unknown mean with both under mean and variance and when projections onto the size 0 are skipped.",
                    "label": 0
                },
                {
                    "sent": "In this case we observe that the SNL is actually equal to debates with Jeffreys prior, so those two strategies predict in the same way.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't that doesn't happen always because, for example, for Bernoulli.",
                    "label": 0
                },
                {
                    "sent": "SNL seems to work better than base, so the question is what is general relation between SNL and base?",
                    "label": 1
                },
                {
                    "sent": "And, well, we don't know this, but we will able to prove that in cases when SNL is equal to base, then one can also show that actually both algorithms are minimax optimal, not only asymptotically, but they play exactly in the same way as normalized maximum likelihood or starkov.",
                    "label": 0
                },
                {
                    "sent": "Prediction strategy, which is not to be minmax optimal.",
                    "label": 0
                },
                {
                    "sent": "There is a small problem that actually for many models charcoal might be not defined because the regret is infinite.",
                    "label": 0
                },
                {
                    "sent": "But when we do some conditioning on the 1st outcome then things became defined again and then this is what I showed here actually hold.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So concluding, what we managed to show is the general regret bound for maximum log likelihood strategy for exponential families, and we mention improve maximum likelihood by taking the current outcome into account leading, which will let us to this sequential normalized maximum likelihood strategy, which we have the optimal regret bounds without any additional assumptions on the data sequence.",
                    "label": 1
                },
                {
                    "sent": "This might have several application and we still don't know what is exact relation between SNL and base, for example, for which models those two strategies are the same.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "For finite alphabets, starkov is minimax optimal.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, so that includes Bernoulli yes.",
                    "label": 0
                },
                {
                    "sent": "So in case I'm very newly.",
                    "label": 0
                },
                {
                    "sent": "SMS is different.",
                    "label": 0
                },
                {
                    "sent": "Starkov is equal to SML, at least on Bernoulli multi.",
                    "label": 0
                },
                {
                    "sent": "Now actually for Burnley.",
                    "label": 0
                },
                {
                    "sent": "Not because for baloney SNL is different than base which means that this thing what I",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well I showed below doesn't hold.",
                    "label": 0
                },
                {
                    "sent": "Not right, but stark of Amber.",
                    "label": 0
                },
                {
                    "sent": "Newly stark often is not SNL.",
                    "label": 0
                },
                {
                    "sent": "No, it's not.",
                    "label": 0
                },
                {
                    "sent": "So somehow for Gaussian distributions and gamma base with Jeffreys priors equal to SNL is equal to start off from some reason.",
                    "label": 0
                },
                {
                    "sent": "Actually, each time SNL is equal to base then they are both equal to start off.",
                    "label": 0
                },
                {
                    "sent": "For Bernie, SNL is better than base and actually starkovich better than SNL.",
                    "label": 1
                },
                {
                    "sent": "So what's can you?",
                    "label": 0
                },
                {
                    "sent": "Can you explain it towards the difference how SNL is different from Stark off of Bernoulli?",
                    "label": 0
                },
                {
                    "sent": "So for Bernoulli, SNL only takes like the current outcome into account by calculating the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "But what's darker would do is to go up to the time horizon to the end of the game and include all possible outcomes.",
                    "label": 0
                },
                {
                    "sent": "So stucco like this Emma is like.",
                    "label": 0
                },
                {
                    "sent": "Next steps tarkov, so it assumes that the game ends in the current iteration.",
                    "label": 0
                },
                {
                    "sent": "Starkov would assume that the game ends actually because Tarkov knows the time horizon of the game.",
                    "label": 0
                },
                {
                    "sent": "So that's the main difference, and it changes with the time horizon.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 1
                },
                {
                    "sent": "Sorry I didn't get for Bernoulli in.",
                    "label": 0
                },
                {
                    "sent": "In what respect is SNL better than base?",
                    "label": 0
                },
                {
                    "sent": "So so this is the old we actually not here.",
                    "label": 0
                },
                {
                    "sent": "This is the all result from the paper by Takemoto and War mode.",
                    "label": 0
                },
                {
                    "sent": "About the was called last Step Minimax algorithm and they actually considered this kind of algorithm for Bernoulli.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, this kind of algorithm for Bernoulli and they managed to get better bound and better performance for this last step, minimize which corresponds to SNL here.",
                    "label": 1
                },
                {
                    "sent": "Then, for base with Jefferies for Bernoulli, but the difference was only in, it was just a constant.",
                    "label": 0
                },
                {
                    "sent": "So yes, small constant the rate did constant in front of the log in is already optimal so.",
                    "label": 0
                },
                {
                    "sent": "It's only there, yes.",
                    "label": 0
                },
                {
                    "sent": "Fire in the.",
                    "label": 0
                },
                {
                    "sent": "Then I'm likely ahead.",
                    "label": 0
                },
                {
                    "sent": "Can you characterize more settings where these things start agreeing with each other?",
                    "label": 0
                },
                {
                    "sent": "Excuse me, can you allow yourself a prior an SNL like?",
                    "label": 0
                },
                {
                    "sent": "Make it a map kind of thing?",
                    "label": 0
                },
                {
                    "sent": "Then can you characterize more settings where these things might agree or?",
                    "label": 0
                },
                {
                    "sent": "How it behaves?",
                    "label": 0
                },
                {
                    "sent": "SNL is kind of prior free, but you need to put in a prior in and do like normalized map or something.",
                    "label": 0
                },
                {
                    "sent": "So you mean you would map for example performing this setting.",
                    "label": 0
                },
                {
                    "sent": "This is what you're explaining.",
                    "label": 0
                },
                {
                    "sent": "How would map you could look at some sort of normalized map estimate.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, so you mean that I would put some kind of a prior on.",
                    "label": 0
                },
                {
                    "sent": "Excellence prior would correspond to the prior would be included in the regrets over penalizing the grid?",
                    "label": 0
                },
                {
                    "sent": "And then you get a normalized map and then now can you start making connections to basil little more closely?",
                    "label": 0
                },
                {
                    "sent": "Because based technique is a priority two and then you get closer connections.",
                    "label": 0
                },
                {
                    "sent": "So I think the base with 'cause regression at the same yes, yes.",
                    "label": 0
                },
                {
                    "sent": "Basically that's kind of an easy case, so there are other cases when there.",
                    "label": 0
                },
                {
                    "sent": "When they might.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah, that's a that's interesting issue.",
                    "label": 0
                },
                {
                    "sent": "I think that the base with Jeffreys prior taking relative to this to this prior would be then the equal to the normalized maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In general, I mean no no.",
                    "label": 0
                },
                {
                    "sent": "For for Gaussians, for Gaussian, yes, yes, yes, right.",
                    "label": 0
                },
                {
                    "sent": "But for anything else or just anything I don't know.",
                    "label": 0
                },
                {
                    "sent": "Not sure.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the question.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "So there's a paper of GAIL Shamiran on the MD's principle with large alphabets, so they he still had similarly results.",
                    "label": 0
                },
                {
                    "sent": "So can you comment on if your method is similar to that or?",
                    "label": 0
                },
                {
                    "sent": "I think I don't know this paper.",
                    "label": 0
                },
                {
                    "sent": "OK, I can show you that.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "Alright, let's thank the speaker beginning.",
                    "label": 0
                }
            ]
        }
    }
}