{
    "id": "n4rdarhyc5t4tztchdw5xoramw5lzi63",
    "title": "Combining Logic and Probability: Languages, Algorithms and Applications",
    "info": {
        "author": [
            "Pedro Domingos, Dept. of Computer Science & Engineering, University of Washington",
            "Kristian Kersting, Fraunhofer IAIS"
        ],
        "published": "Aug. 24, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Artificial Intelligence"
        ]
    },
    "url": "http://videolectures.net/uai2011_domingos_kersting_combining/",
    "segmentation": [
        [
            "Yeah, first thank you for inviting, and in particular for.",
            "Giving me a chance of coming back to Barcelona.",
            "So last year when I was here I had not much time to explore the city.",
            "I hope I have some more time now.",
            "Yeah, so this tutorial jointly with petrol is about combining logic and probability.",
            "I'm very happy to give this talk because I'm quite excited about this research topic, but."
        ],
        [
            "Not only me, it's actually quite a lot of people who are excited about that so well.",
            "Petro and I would like to thank all of them because without them what we're talking about, wouldn't be there."
        ],
        [
            "So in case that you want to leave earlier, here's what you should take home with you, right?",
            "So the idea is that.",
            "Graphs are not enough.",
            "We need logic.",
            "I mean, given that we are here at you, I guess you know what we mean.",
            "So, graphical models are cool.",
            "We don't say we need logical models.",
            "We need most likely new forms of models.",
            "But we need models which combined probability and logic.",
            "So graphs are not just."
        ],
        [
            "Enough.",
            "The road map to this goal too.",
            "Well, so that you have to take away message that you go home and say, yeah, cool we can contribute.",
            "There is a little bit of motivation then a very biased short overview on what statistical Asia learning or statistical AI is.",
            "And then many of the concept mentioned here will be illustrated using Markov logic networks in the second or third part.",
            "There will be a break somewhere in the third part, but let's see how far we can get."
        ],
        [
            "So motivation."
        ],
        [
            "And this is currently my most favorite motivation for the whole topic for two reasons.",
            "If you look at it, I can ask you what do you see.",
            "And I can ask myself what I do?",
            "What do I learn about you by looking at it?",
            "But I also like it so my wife has a background just for you.",
            "My wife is a psychiatrist so I can really tell you this doesn't work right.",
            "We know that now for ages already, it really doesn't work.",
            "Doesn't tell me anything about you.",
            "It doesn't tell you anything about me by showing you that next to that my wife is maybe."
        ],
        [
            "But then there's or in a tuning or and he managed to come up with a roshak test telling a lot about computer scientists.",
            "'cause when I show you this slide and ask you what you see here, I guess you can come up with many answers, right?"
        ],
        [
            "So for example, you may say this is more slower, essentially right?"
        ],
        [
            "Or you may say this is storage."
        ],
        [
            "Capacity, you may say this is number of scientific reputations."
        ],
        [
            "You may say this is the number of Facebook users."
        ],
        [
            "You may say it's a number of web pages.",
            "Alright, any other kind of so?",
            "For example, right now I was just coming back for a meeting on computational sustainability and can fairly say that it's similar curve you get for the data you gather currently with rather low price sensors, right?",
            "And they're facing the problem.",
            "How to make any sense of the of the data they're gathering there?",
            "Anne.",
            "A similar project."
        ],
        [
            "Actually, many people not only in the AI community or UI community or machine learning community, but also in the database community are looking at this idea of a worldwide mind, right?",
            "So we have these tons of web pages.",
            "We have users who are willing to spend their time to put all the knowledge into the web and imagine that we can extract all the knowledge they put there in at every second.",
            "Of their life of our life.",
            "If we can extract that and hopefully we can make the machine turned smarter, right?",
            "So that's the idea here.",
            "And there's another project at the UW here I'm using here is slightly different citation here by Vikram.",
            "They've done that in a slightly different way, but they all try to extract the knowledge people are encoding in the web and so here for example, if the text runner system organics Daniel Weldon or in attorney at UW where you can ask now queries like Paper Wild Card topic asking you in which relations?",
            "Do paper and topic stand each other right?",
            "And so you get back.",
            "Things like paper discuss is, covers, addresses, discuss and so on and so on.",
            "So what do we learn from these kinds of systems?",
            "An what do we have to achieve or to manage to do if we want to make use of this data, right?",
            "So right now, this is just data.",
            "If you look a little bit closer than what you see, what we talk about here are kind of objects like paper and topic.",
            "We have certain relations between paper and topic and maybe some form of uncertainties.",
            "So the numbers here or something like that.",
            "Number of times you have extracted this kind of effect, so let's not have a debate about whether it's really uncertain T or not.",
            "You can view that in different ways, But for this tutorial, here is the best to view that as an uncertainty.",
            "Namely, if you see it quite often this relation with a high number of counts, then you're sure about it or more sure about it.",
            "And if you just have one or two counts."
        ],
        [
            "So.",
            "It tells us that talks we are facing or we will face in the future.",
            "Some are characterized by objects and these objects in contrast to traditional views in machine learning and AI are not just feature vectors, right?",
            "They are more than that.",
            "They have, maybe even parts.",
            "They certainly have relations to other objects.",
            "They might be trees, graphs, whatever, right.",
            "So an object is not just one line in a table, but much more than that.",
            "It's more flexible.",
            "Given that there are relations, we should definitely say objects are not really independent, identically distributed, but they're like neighborhoods, right?",
            "They are interconnected with other objects and information about the neighbors or related objects tells us a lot about the object we are currently facing.",
            "There might be class hierarchies in the semantic web community.",
            "I mean, this is all about or the triple stores.",
            "They talk about nowadays are still coming from the ontology.",
            "And definitely properties of 1 object depends on the properties of the other ones.",
            "Like if I look into the audience here, I don't know anything about most of you guys, but I definitely know that you're attending UI.",
            "Because of that.",
            "I also know that you have somehow interesting computer science.",
            "And also somehow interest in mathematics right by just using my world knowledge and using the relations between you and other entities.",
            "Like you, I I can draw much more conclusions than just looking at you like a single person.",
            "Each of you in dependently.",
            "That's all about.",
            "So.",
            "If we agree on that, definitely the question is how can we deal with this kind of knowledge?",
            "How can we make sense of the knowledge base of the data?",
            "How can we do inference in there?"
        ],
        [
            "So let's have a look at how people have done that in an AI.",
            "Over the last ten 2030 years, right?",
            "So one classical way of dealing with the complexity in there.",
            "Things like we have objects and relations among them is first order logic, right?",
            "So traditionally we may have an explicit enumeration.",
            "We have atomic representation or first order or relational logical representation.",
            "The benefit of using logic is essentially that you get a compressed representation of what you have encoded, otherwise in a long long list or numeration of all the facts.",
            "So here, for example, where you list.",
            "All the door to off relations for all pairs of individual separately.",
            "Here you just define it right?",
            "So we're now when we have any information, but rather often female, we can conclude that someone is the daughter or someone else.",
            "So you get this compression in there you can get general rules.",
            "Right and within then this kind of more traditional knowledge representation.",
            "There was alot interesting going on like you can ask how to reprint all the different knowledge we were talking about like trees, graphs, hierarchies and so on.",
            "But they are also very interesting inference algorithms.",
            "You typically in the early days at least few I have not seen much at you.",
            "I like satisfiability, SAT solvers, resolution theorem proving and so on.",
            "Right, so to even further illustrate that that's an example here.",
            "Russell lights to give else in his book If you want to encode chess in an atomic representation where you just list everything, this would maybe take, let's say millions of pages.",
            "Maybe I'm exaggerating here a little bit.",
            "If you use propositional logic, which is something like a factorized representation, you can get already much more compact, but only if you use if you abstract of certain places on the board, you can get one or two page.",
            "Kind of compression representation of the theory of the rules of chess, right?"
        ],
        [
            "So, but of course tasks are not just structured, they're not just complex.",
            "But there's also a lot of uncertainty involved.",
            "I mean, that's why we're all here.",
            "You AI and we want to deal with.",
            "Probabilities because information might be ambiguous, might be incomplete.",
            "We may just be uncertain about certain things.",
            "There might be typos in there.",
            "If you think of database and if you think of the task of cleaning a database, right?",
            "If you're a big company and you have people typing in all the information about customers, it's very likely that there is here and then few typos in there.",
            "So for example, I had reason troubles with our financial Department 'cause they were typing my last name wrongly, but only with two or little.",
            "2 little typos, but it took them quite long and what you face is people say, oh, it's in the system, so it's true and I always try to tell them well, it's in the system so it's more likely to be true, but they still don't get it.",
            "So let's try to work on that.",
            "Right so then again, we can ask, how do computers systems nowadays deal with that certainty?",
            "And well, that is all what you know of."
        ],
        [
            "About innocence mixture models, hidden Markov models, Bayesian networks, Markov random fields, maximum entropy models, whatever.",
            "Take your favorite one in this sense, right?",
            "So we have these two kind of axis in here.",
            "If you look at what classically in AI people would have done to tackle these."
        ],
        [
            "Problems right?",
            "And so given these two axis, you can ask, will traditional AI or UI scale?",
            "So given this simplified view on it, I hope you agree that this will not work.",
            "Huawei."
        ],
        [
            "Let's see how you traditionally do it, like with the Bayesian network.",
            "For example, what does it mean in the traditional view we have something like a table, right?",
            "So your features you may have class labels in there.",
            "OK, and then you do whatever you like.",
            "You can do prediction, you may do.",
            "Latent variable model or whatever.",
            "So what you do is essentially if you try to discover a lot of in dependencies within your random variables you have there and then try to get a compact representation of these in dependencies.",
            "Let's say in terms of the Bayesian network.",
            "In order to do efficient inference.",
            "That's what we all know.",
            "However, in many cases, like I was showing or you already with the world Wide Mind project or the text runner system, we know that we are not having just a single table, or indeed if you go for a universal table then this would be.",
            "Very large table with a lot of null values in there, right?",
            "So it's not a good idea.",
            "That's already what database theory tells you, so if you go for any kind of normal form then you would have not a single table but several tables with relations between them.",
            "So how do we do right?"
        ],
        [
            "So yet another example I was taking from Definion near is something at least I had to learn.",
            "When I was PhD student, so as everybody we're going to submit a paper to a conference and then you know your supervisor.",
            "If he has no time, just tells you yeah yeah, good papers get a so they get accepted, right?",
            "He's not talking a lot about you.",
            "The about the real rule there.",
            "Or he may say good papers sometimes get ID and so on.",
            "So he's giving you just these rules."
        ],
        [
            "So from that you build up your little structure in a Bayesian network like you have quality of the paper.",
            "How difficult is certain conferences and you try to predict the grade of your paper to see whether it's worthwhile submitting?",
            "Is it clear that it gets accepted or not, right?",
            "And then we have the condition probability distribution attached here and the overall joint distribution factorizes according to the graphs."
        ],
        [
            "Right, but this is not the full story.",
            "Again because I mean, as you know, here we don't have just UI here, but next week or actually with things, there's even an overlap here.",
            "There's each key, right?",
            "So let's see what that means.",
            "So we are talking here about one paper submitted to UI or we think about the grade at UI.",
            "We can think about the difficulty of AI and about the quality of paper one.",
            "So let's say you get back in a here.",
            "That means without any additional information evidence while getting into UI is not so difficult.",
            "Some assumption here OK?",
            "Of course, apriori the quality of the paper is pretty high, but that else would then sorry.",
            "That also means then if we would submit this paper or another paper to UI, let's say the paper 2, then apriori without knowing anything about the paper, we should also expect that it gets accepted at UI.",
            "That's definitely wrong.",
            "So when I was talking to, my supervisor was actually the opposite away around, so he never got something accepted at UI.",
            "And so I was scared by you.",
            "I I tried to submit I got in right?",
            "So it's the opposite.",
            "But anyhow, you see that you should take all these different conferences into account.",
            "So then we get additional evidence.",
            "Let's say we got rejected that actually tells us to explain the whole setting that the paper to the quality was rather low without really knowing it.",
            "And so we also know.",
            "Now if I want to submit this paper now to each card, because I want to get it published, I should expect to get it rejected as well, right?",
            "So what I want to tell you again, we should not try to think of random variables just as a single table or just as rows or columns in a single table.",
            "But we should think of different families or groups of random variables where we may have relations between them or not.",
            "So just think of a global state with more compact.",
            "More interesting scale.",
            "So in any case.",
            "Independency is nice, we should try to get it independency.",
            "But we should ask about it independency at the right level.",
            "So independency at the object level might not be the right thing to do for silver applications."
        ],
        [
            "So again, now I gave you time.",
            "Will it scale?",
            "I mean, I also told you the answer already, but if you now think again about it, but just treating them explicitly in a sense I mean exclusiveness saying either probability or we deal with logic, then certainly we can say no right?",
            "So scaling up the environment?",
            "Will inevitably overtake the resource of traditional AI or UI architectures.",
            "If you agree on what I say."
        ],
        [
            "So that's the motivation for many people, so actually started.",
            "Well, if you think about it, it started already quite early in the AI community, so Niels Nielsen maybe was the first coining the term probabilistic logic, but it took off early 90s to some extent, and then really around 2000 that people started to work on statistical relational learning.",
            "Now about 11 years later, actually turns out that we realize it's not just statistical relational learning, but many tasks we typically face in AI where very well.",
            "Fit under the umbrella of dealing with.",
            "Probability, complexity and learning together and actually many tasks solutions become easier than in the traditional case, but more about that later.",
            "So what we are facing now, what we're trying to do is to set up so to say the study and design of intelligence agent that act in noisy world composed of objects and relations among the objects that also very well fits.",
            "For example the tutorial this morning about causality.",
            "So note that ultimately we really want to have acting things right ultimately.",
            "So it's not just probabilities but a little bit more than.",
            "Probabilities you may even talk about utilities.",
            "You also have to maybe realize your robot acting in the world and so on and so on, so it's really covering the whole spectrum here."
        ],
        [
            "So why?",
            "Well, if you look at the conferences at AI conferences, what you typically see if different fields like common sense, natural languages, robotics, vision.",
            "If you at all, C is still, for example, division people at the AI conferences, so they actually specialize and have their own very good conferences.",
            "And why?",
            "Because if you talk to them, they always tell you, oh, I want to know how to solve robotics or I want to know how to solve computer vision.",
            "But is that actually true?",
            "I mean, sure, they want to solve their particular problems, but actually what I feel is true is that we all face the similar problem.",
            "Maybe we somehow have domain knowledge, language knowledge, or robot knowledge, or object knowledge about optics and objects.",
            "But in a sense, if you look in the end, if you look at what they publish, the inference algorithms are often quite similar.",
            "So the vision we have here is that we can abstract to some extent.",
            "I mean you still need to be an expert in the field.",
            "But to abstract to some extent, into something like each one is working on what really makes their trust different.",
            "Where they are the real experts, but we share a common toolbox and inference and learning task here so that we benefit cross fertilize each other right.",
            "So whatever the NLP guy is developing, the computer vision guy can use directly and actually that's happening.",
            "One of the in my opinion, very interesting.",
            "Cross benefits are topic models right?",
            "So they started as something like just to understand what's going on in the collection of text documents.",
            "But then if you look at CVP RCB the recent years you find a lot of topic models over visual words where people were inspired by the standard words in a text trying to identify what that means in an image coming up with ideas.",
            "So this is something like patches and then they try to learn topic models about this visual words which is then for impact on computer on machine learning where there's a lot about dictionary learning which means how can we extract or learn these words automatically.",
            "So if we can somehow their combined process, I guess we can make much more progress than just working independently of each other.",
            "That's the vision."
        ],
        [
            "So.",
            "Setting the stage a little bit motivating.",
            "Why we're interested in combining logic and probabilities.",
            "Why this tutorial?",
            "Well, it's very active.",
            "There's really a lot going on, and its multidisciplinary because as argued there, a lot of subdiscipline of AI involved, unfortunately, having said that, they still speak different languages.",
            "Right, so we manage now maybe within 10 years to almost have agreed on.",
            "Few languages, let's say 2 languages in a sense, within the statistical relational learning community, and actually this year, I guess we managed to also abstract from these languages and really identify some of the core problems.",
            "But then if you talk to other people, they still have their own language, right?",
            "And we have now two chances we can try to learn their languages, go there, solve their problems, show that they should be interested.",
            "That's what we will do.",
            "But this tutorials also to show you maybe it's a benefit for you as well.",
            "So just try to jump on the train.",
            "So if you say.",
            "So why should you be interested?",
            "Well, of course it advertisement here, but I guess it's really a success story and not because I'm defining that as a success story.",
            "Actually, one should say petrol showed it a success story several times, but it's also if you look at many of these papers.",
            "These papers got best paper awards at the conference as the values of the specific subdiscipline, right?",
            "So they have been best paper about, for example, in NLP, right?",
            "And so why should they give you in best paper what it?",
            "There's nothing.",
            "At all interesting for them.",
            "So there is really something going on in bioinformatics.",
            "There's a lot going on because you have these experimental design issues and you can ask about relation between different experiments.",
            "An if you make use of that, the group I definitely color has shown a lot improvements there as well within the planning community there's a lot going on.",
            "Currently there was a follow up champion at the probabilistic competition which was using planning competition which was using a relational probabilistic approach, right?",
            "So they are.",
            "Lot of subfields where people start to see.",
            "Oh yeah, there is really a benefit.",
            "OK."
        ],
        [
            "So after this motivation, I hope you got a little bit excited.",
            "At least let's."
        ],
        [
            "Give a short overview of what statistical relational learning is.",
            "So next to these two few applications I've shown you the text runner stuff already and this kind of Toy Story about submitting papers there.",
            "Of course, really a lot of applications up to date, so they have been in LP and information extraction alot.",
            "I think one of the main home bases so to say for applications is link prediction.",
            "So within machine learning all the collective classification tasks.",
            "Can quite often be nicely represented within a statistical relational learning framework within social network analysis.",
            "There's a lot of work going on.",
            "Think of mixed members and so on.",
            "You can view them as relational models in robot mapping.",
            "There has been some work by, for example, Petro again, but also by detail folks.",
            "We're trying to work on that currently in activity recognition, and Sebastian Truman had once one paper on using Markov logic to explain activities.",
            "Computation biology.",
            "There's a lot going on."
        ],
        [
            "And so on and so on.",
            "So let's have a very brief look at them, and particularly this first example petrol will talk much more about that later where you also don't understand how you can model this kind of problem using."
        ],
        [
            "Of logic networks.",
            "So in information extraction, for example, assume you have certain references.",
            "Let's say you crawled the net or whatever, right?",
            "So first task is to understand that these are different papers or they all denote Pape."
        ],
        [
            "So then next to that you may ask, oh, this is a paper.",
            "So then I know it talks about authors title and venue like hear about prog zingler petrol, memory, efficient inference in relational domains is the title and to play 06 is a conference right?",
            "And you do that for all these.",
            "Diff."
        ],
        [
            "Papers.",
            "Well then you start realizing that zingler, P and probes Engler are actually the same persons right?",
            "Although from the evidence you got, they're looking different.",
            "But if you do some reasoning, for example by making use of that title here, there are two papers with the same title.",
            "You expect that the two authors are identical, so you see how to take if you take relations into account, you get a better prediction on other thoughts, right?",
            "And then so on and so on.",
            "So you see that information extraction.",
            "NLP tasks definitely.",
            "We can benefit by making use of relations by solving tasks jointly by making use of all the knowledge available or all the information available, right?",
            "So relations are really at the harder."
        ],
        [
            "Entity resolution.",
            "Well, in the early days I did something on, for example, gene localization.",
            "If you do just a collective inference where you say the location of 1 gene also depends on genes with therapist similar function, then with a rather very simple kind of graphical relational model you can be better than the KDD Cup winner at that time, right?",
            "So I'm not saying this is the best model nowadays for this kind of task, but those days when you were just making use of the.",
            "Relations between the different genes you can improve quite a lot.",
            "What is currently going on in lot in robotics?",
            "Is this collective influence?",
            "If you want to do semantic labeling so you get these pointclouds and you would like to know is that tree is that wall is the ground floor, can my robot move there or not?",
            "And then definitely you know that at least locally you should have something like a smoothness assumption that one pixel or voxel in terms of his class label is not so much different from the neighboring ones, right?",
            "If you walk out of the building, you're not expecting that the road is all of a sudden stopping and then for the next millimeter.",
            "It's a.",
            "It's a tree.",
            "And then there's again a road.",
            "And then there's a hole.",
            "And then there's a tree.",
            "And then there's now you expect somehow, because there is a kind of manifold that locally everything is rather smooth and it's not changing radically.",
            "So again, you make use.",
            "You say if my neighboring pixel is a Rd, I'm more likely to be a Rd segment as well.",
            "Right, so it's also interesting because you may view here relations as constraints, so it's not like you just use the relations to propagate with.",
            "You may also see it as a constraint or another one where we did some work with tossed in your concern.",
            "For example on web search.",
            "So definitely if I do a query I get certain documents back and you can think about preference relations in there.",
            "But definitely you also have similar to reference relations between documents here and you know that one is relevant.",
            "Then similar ones should be relevant as well.",
            "You may also use the similarity relations to ask completely different interesting questions, like if you want to have diversification in here, then you know if I present that document I should not present that one anymore.",
            "So people are using relations all the time already, even if they don't give it innate statistical relational learning.",
            "But somehow they do it already and we just want to go one step ahead, make it more explicit, right?",
            "And of course you can show that relation approaches really outperform standard approaches.",
            "Of course, you have to pay the price.",
            "That may be learning and also your inference is taking a little bit more complex sometimes, but it really pays off doing so.",
            "Let me skip that one.",
            "I think we can also skip that one.",
            "So topic model there a lot of."
        ],
        [
            "Relational extension for that, but you may also use these kinds of techniques to develop within statistical relational learning with another task.",
            "So for example, if you look at how at least my student spent their spare time, they really watch YouTube a lot, right?",
            "So well, actually not only their spare time also while working.",
            "And so they but also YouTube is facing this problem of information broadcasting, right?",
            "So you want to have the latest movie question is where can I download it?",
            "What is the best policy to download it if one of my friends is already having some parts of a longer movie then I may be downloaded it there and then download the other pass from somebody."
        ],
        [
            "And so on, so.",
            "Many of these big companies nowadays are really facing this problem of having complex networks and trying to broadcast information in there, or I think in the last tutorial we saw this one problem as well with the Microsoft.",
            "How's it called service update assistant?",
            "It's the same problem, right?",
            "So Microsoft doesn't want to send everybody an email because this is really putting a lot of traffic.",
            "So instead you would like to have it more in a peer to peer network like style.",
            "So you send it to 1 computer, they send it to the next one.",
            "So to keep the load for the traffic quite low.",
            "So here petrol will talk a little bit more about lifted inference for C, But we were able to show that lifted inference techniques are faster than belief propagation perceived right?",
            "So you can really show that even in task where belief propagation algorithms already quite fast and really used, you can make use of redundancies in there to speed up computation."
        ],
        [
            "Yet another one recent application we were able to show with relational techniques you can better predict heart attack diseases than any of the propositional systems, and here we are now talking really about money, because cardiovascular diseases really costs already the EU several billions in a year, right?",
            "And so if we can somehow get that down, that's good, but you see something else which is important about relational systems.",
            "If they are sometimes more declarative, better to understand.",
            "So what you see here is.",
            "Kind of local conditional probability table, which is now making use of abstractions so variables the lower parts are here variables, so you test whether a person is female or male and what you, well, you can't see here, but you have to trust me, they may the female subtree is much smaller than their male subtree, and that's actually what many medical doctors know, but they never had the chance to validate that using their propositional techniques.",
            "So it's known that female.",
            "Females are women have had much lower risk for heart attacks than males, right?",
            "And so on and so on, and so it's very easy to sit down with the expert with a medical expert.",
            "That's what we did here and here, under or she understood.",
            "Right, so we were even able under submission to get new insights about hot topics."
        ],
        [
            "So I'm trying to illustrate interesting applications.",
            "I think it already popped up that we should ask the questions.",
            "What are relations, right?",
            "Because there might be different ideas about what relations are.",
            "So if you look into the literature, for example, zooming Ghahramani work on just on relational Gaussian process, is there.",
            "The idea is that relations just provide additional correlations.",
            "Right, so and closely related to that, you only still have only one table, but some additional knowledge, and you put that into your covariance function.",
            "Or you try to come up with a distance function between graphs or you still say each graph is a single object in a sense, and you define some relations over."
        ],
        [
            "Another view which I'm using a lot to come up with lifted inference algorithms again.",
            "Be confused now, petrol will tell you what lifted inference is later to some example is that you can view relations or so to say the science about symmetries redundancies because you try to make use that certain entities behave almost the same or identical, even if so, they're interchangeable.",
            "I can do my inference only for one of them, and then I can use the same inference for the other ones.",
            "Some other people have viewed relations as hypergraphs or the full fledged one where you really take your language.",
            "Your favorite logical language and try to attach weights or uncertainty."
        ],
        [
            "So because you have so many different views on that.",
            "It's not so surprising that there's also this.",
            "What is called SRL alphabets, which maybe also explains why it's not easy catching up sometimes to other people.",
            "So let's say everything more or less started in the early 90s with people by the harsh aniak and all these people around the Yurei community.",
            "But then actually it developed into a beast, in essence, right?",
            "And I apologize for all the ones I have not listed on the slides at some point I was really not interested anymore.",
            "And writing down all the different languages people came up with, or approaches, right?",
            "So there is really an Alpha."
        ],
        [
            "Soup, but there are few key dimensions we can distinguish.",
            "Many of these approaches, so like in UI, there's kind of directed models or undirected models.",
            "So for example directly at once, like where are they relation ull.",
            "Probabilistic relational models by Harvey Leaser.",
            "Then we have stochastic logic programs block relational Bayesian network problem, relational dependency networks, and so on.",
            "And I'm directed Markov logic networks, relational Markov networks, infinite hidden relational models and."
        ],
        [
            "So just to illustrate a little bit, so here's the abstract alot right?",
            "So there are many details.",
            "I'm happy to not present you.",
            "It's a little bit more shining every state stay at the abstract level, so here's how it looks like.",
            "So instead of having a Bayesian network as you know it, you have rather something like a rule graph.",
            "Rule graphs is similar to Bayesian network.",
            "It tells you something about the predicates which are, so to say containers or placeholders for a lot of instances in there.",
            "Now, instead of just having a condition probability distribution.",
            "We have something like a Bayesian network fragment in there, right?",
            "So for example here picking up the submission or the paper grading problem.",
            "You say whether a paper has a high quality depends on whether the author is smart or not.",
            "So you use a placeholder paper to link the author of a certain paper with weather.",
            "She was he was smart, right?",
            "So it's the main difference.",
            "Instead of just having one single.",
            "Fragmente like in a conditional probability table.",
            "You now have such a say a macro for several of these fragments.",
            "You can now instantiate where plugging in here any person and any author.",
            "Any paper here in any any author there."
        ],
        [
            "And that's exactly what you do.",
            "Imagine you're a few constants or really objects.",
            "We want to talk about, so you can now put in all these placeholders the different values, and then you come up with.",
            "So to say, Bayesian network fragments.",
            "Right, so for example here if you have paper one and paper two, you get these two little different.",
            "Bayesian networks.",
            "Right, So what this means is if you.",
            "I mean this way.",
            "Going back this would mean you have only one paper.",
            "Now assume we have two papers.",
            "You get a larger network if you have three papers.",
            "You will get more copies of that it is growing depending on how many objects you have, right?",
            "So we can deal with a variable number of object this way, so we were making use of ideas from logic.",
            "Combining that with idea from UI coming up with something like a Bayesian network.",
            "Depending on the context, depending on the domain."
        ],
        [
            "So, but now you can of course also ask what happens if we Additionally or instead have something like Bob was also the author or coauthor of paper one?",
            "Right?",
            "Then we have a problem because we have only specified partial knowledge about one of the authors, so to say.",
            "So we get multiple links in there.",
            "We have somehow to combine and then many people have."
        ],
        [
            "Post to use.",
            "Something like aggregation functions, right?",
            "So you take all your knowledge in there.",
            "Let's say you take the mean, the average Max.",
            "Sorry, the mean, the median Max.",
            "Or you may use noisy or noisy and whatever fits well.",
            "Your model at that point.",
            "This way from a set of conditional probability distribution which are not only specifying part of this sub problem, you get a complete conditional probability distribution, right?",
            "So that's one idea.",
            "One problem, of course is that it's acyclic.",
            "So if we want to learn this model, they still assume that you do not have a cycle.",
            "We have to check that it's a little bit more."
        ],
        [
            "Applicated so to overcome that, for example, Jennifer Neville and David Jensen came up with the idea of lifting relational dependency networks.",
            "It's very easy.",
            "You stay with the same idea now only you accept that there are cycles.",
            "Because of that you cannot have exact inference anymore, but instead you skip sample or mean field approximations or whatever to do inference or to do lower right?",
            "So works as well directly liftable by just saying OK, we have macros."
        ],
        [
            "We are insincere.",
            "Alright, that works.",
            "Really very same way you except now that you have cycles run everything on the ground.",
            "In network you are happy."
        ],
        [
            "On the other hand, you can also go for the undirected world.",
            "You will see much more about that in the second part.",
            "But as an illustration here already, in a sense you have rules like here if an author of the paper is smart, then the quality is high.",
            "If the quality is higher off the paper, then it gets accepted.",
            "Of course, that's not all true.",
            "That's why you place weights on them, right?",
            "So it's not always true, but in some possible worlds it's true.",
            "In some it's not true.",
            "So you get a distribution over it.",
            "How do you do that?",
            "Again, these placeholders here you put in all of these different constants, and by that you get the set of nodes or random variables.",
            "You can talk about and then depending on in which ground I mean, after you replace these placeholders, this constant.",
            "If two nodes are in one of these ground clauses, then you draw a line between them, so you can just view each of these rules as a macro again, right?",
            "So like before we just instantiate them, you get a big big network.",
            "You do whatever you like with it, inference, learning or whatever."
        ],
        [
            "So another dimension.",
            "Sense, so here I was using directed undirected mainly for focusing on this macro view, right?",
            "So each of these rules is something like a macro that you're instantiating.",
            "There are also other approaches which more look into what is called proof."
        ],
        [
            "In logic.",
            "I don't want to go into all the details here, but let me give you an idea here and actually the idea is to show you that this is essentially against something else so you can express as a distribution over possible worlds.",
            "So here's the logic so called prob.",
            "Log by Luke the Rats group in Lewiston, so again you have somehow logics rules in there telling you hear that they support if there's an edge between two nodes or there's an edge between two nodes and apart from the one node.",
            "The other node, right?",
            "And then you have probabilities attached to the edges so it's something like a uncertain network here, right?",
            "And so one way of defining some entry or to define a probabilistic program over it is now to view these.",
            "Probabilities you have here as a sampling distribution, so you sample.",
            "Whether you want to have this effect into your program or not, so put it differently, these ones are giving a distribution over programs, and so you sample these programs.",
            "You asked whether your query is true, and this gives you then because probabilities were attached or you only sample the programs to a certain probability, gives you a distribution over the program, right?",
            "In a certain way, again, you can turn that Elsa into possible worlds.",
            "That's a UI paper.",
            "You can.",
            "I don't know when is the talk.",
            "Is it on Friday Saturday?",
            "I don't know or poster Sunday, so you see that you can also turn that into something like an undirected model as well, and that there are benefits in doing so.",
            "Maybe also disadvantages, but the point here is that many of these programs are getting too large.",
            "If you really sample all these different programs, so it may be better to not think of the programs what you want to store.",
            "But you break these programs up into little pieces, and essentially that's the proof, right?",
            "Essentially, I mean I'm happy to clarify that offline, but for what we want to talk here, essentially there would be something like."
        ],
        [
            "So there are other dimensions, like there's some work on parametric and nonparametric.",
            "So for example here the infinite hidden relational model that was also.",
            "So I mean there was invented in parable by Josh Tenenbaum's, child's camp and folk art race in Java due so they try to combine nonparametric models and interconnect them using relations.",
            "So you can do that if you like.",
            "Or another dimension is closed world assumption.",
            "So we know what we're talking about.",
            "We know the number of objects and the objects in there.",
            "Or something like an open world assumption where you don't know how many objects you have about which objects we can talk at all.",
            "And there for example, you have non parametric block and block as well and so on and so on.",
            "So these are all these different dimension."
        ],
        [
            "And then you can go on further.",
            "Just to show you there is an alphabet soup, but there are dimensions.",
            "We know why we did it.",
            "And it's really covering all the work.",
            "All the work in AI.",
            "So for example, if you like ocean processes, there are nowadays relation abortion process is if you like reinforcement learning, there are quite a lot of approaches to relational reinforcement learning.",
            "There are even extensions to partially observable MDP switch our relational and where you can show there is a benefit in doing that for some cases we can even show that you can use lifted or relational technique to solve.",
            "Innocence more efficiently in a certain other sense, none more efficient for now, but at least you can solve linear equations more efficiently than ground versions using belief propagation, organ belief, propagation.",
            "There you can speed up a lot.",
            "That's UI a, each kind next week.",
            "There other papers at each card this year about common filters where you can show you can do common filtering else in a relational world, and you can also show again that you speed up, at least if you use caution.",
            "Belief propagation for common filtering.",
            "There's interesting work by Lisa.",
            "Good tour going into declarative information networks to really say revolutionalize in this sense.",
            "What is Google all about?",
            "Right so.",
            "I hope you started to worry at some point here.",
            "'cause all these names got.",
            "I mean, how can I ever keep up with that?",
            "Should we worry about?",
            "No, I did this exercise.",
            "Actually it's very much like the first proceedings of UI.",
            "So I checked the very first 2 proceedings of UI."
        ],
        [
            "And if you do that, you get all these different names in there.",
            "There were even papers talking about what based theory missed about, and whether that's interesting to handle uncertainty or not, right?",
            "So I like to say we wanted.",
            "So to say at this stage slightly behind that already, so we already a little bit further give us a little bit more time and I guess we have one big idea about what we."
        ],
        [
            "Talking about right and why well for you I we know that also somehow boiled down nowadays to something like factor graphs, right?",
            "If we abstract from certain details, right?",
            "So we have random variables we affect this between the random variables and so together they somehow give you a factorization of a joint distribution.",
            "Normalize or not directed.",
            "Whatever it doesn't care you abstract."
        ],
        [
            "Right?",
            "So we abstract also whether it's directed or not.",
            "You can all fit that into this vector graph representation.",
            "Similar.",
            "This boiling down process also happened in SRL."
        ],
        [
            "So if you look nowadays at the literature, what you see is a lot about something like lifted factor graphs or parameterized factor graphs.",
            "There's another field on Bedarra metrics circuits and waited, and essentially if you think about it, they all interconnect, right.",
            "So awaited CNF is nothing else than affecter graph and essentially Petra.",
            "I'm abstracting here Lil bit affecter graph is also something like an arithmetic circuit, let's see.",
            "But the best paper work this year UI is slightly different there indeed, but on a high level we also boiled down to a certain agreement on what is underlying."
        ],
        [
            "And I just would like to give you a little bit of a flavor there.",
            "So this is a factor graph and this is something like promised a factor graph, right?",
            "What is the difference?",
            "Well here it's all about attending a workshop and you're wondering whether it will start a series or whether it will be popular.",
            "And so you ask whether certain people will attend yes or no.",
            "And because you assume they're all identical here, we're not writing down the factors or potentials each time.",
            "But instead we say for all people.",
            "There is a factor between popular and attends X for that person right now.",
            "Again, if we instantiate it here, you get this part here similar there, right?",
            "So instead of having this more complex representation here, which is even depending on the number of people, you have a constant complexity representation here where you abstract it and now you give me the number of people or the people, and I can instantiate dependently on that.",
            "Right?"
        ],
        [
            "Let me speak that it's not so important.",
            "Point is and again petrol will give you more details about that.",
            "You can now make use of this abstraction here so they're lifted inference techniques that never actually built up this ground model anymore, so you just do inference so to say on the lifted on the abstract level so that you get even answers which just depend on the domain size so that you get a constant answer no matter how many.",
            "How many people there are at all?",
            "On the other hand, the other one, the weighted CNF, is coming from the fact that already each it's well known from the satisfiability community people have to starting work on not just using satisfiability problems, but where you wait the different.",
            "CLOSE is in there.",
            "So in a sense, you're giving costs for it, and you want to find this assignment with a minimal cost, right?",
            "And of course they can also be viewed as a factor graph, so this is a little bit more from the logic with a little bit of probabilities in there.",
            "If you have only one weight per clause, this is very natural.",
            "If you want to have a factor or power factor associated with your claws, then maybe the power factor is a little bit more interesting, but in the end you can convert between them, right?",
            "You can always transform between them and again, there's a lot of redundancy in there that you can make use at inference time or learning time.",
            "To speed up."
        ],
        [
            "So given that, So what is the current state of the state is the filling, so it is good to combine probabilities and logic because it gives you more compact models because they are more complex.",
            "Sometimes you can do infringe much faster because you can stay at the abstract level.",
            "Similar for learning to some extent, right?",
            "Because you are an abstract level, you do not have to learn each rule for each person again and then in the long run.",
            "You converge and you say, yeah, I know they're essentially identical, so you can place the additional constraint and test.",
            "There's this rule for all people because of that.",
            "Well, not in any complexity theory, it's faster, but it may be first in learning because instead of bottom up trying to figure out all these little potentials are identical, you go top down and if there's a lot of regularity then you're actually faster by testing only few of these abstract rules and how have people down there."
        ],
        [
            "Well, they use what is known in logic in learning these kinds of logical models that called inductive logic programming where you want to find these kinds of logic programs.",
            "Given few examples like certain molecules are Muta, genic yes or no and these molecules are complex beasts, right?",
            "So you can have as many atoms in there to describe that.",
            "So here you may have 10 here.",
            "You may have 100 atoms, so you have applied flexibly representation and you want given that you want to learn these kinds of programs so various.",
            "Very similar, you can now learn these probabilistic relational models, at least if you're happy to stay with this vanilla.",
            "Very basic learning algorithm, how would you do it?",
            "Let's have an example at a classical algorithm.",
            "IOP inductive logic programming, not integer linear programming algorithm foiled by Quinlan.",
            "How is he doing that?",
            "Well, very easily you start with no rule and then you start really in a dumb way in a systematic way.",
            "You just refine what you have there.",
            "You place all possible predicates you can think of in there and test them.",
            "Right, so for example, you may test Atom xac, which means there's a certain type of Atom certain other tepin Atom in the molecule, so another one, and then using one whatever heuristic you have a scoring function.",
            "So in our case we may use likelihood, turns out will not work.",
            "In most cases.",
            "You may use pseudo likelihood or conditional likelihood, but you score them and then in the greedy fashion Hill climbing you select the best one, try to refine.",
            "You do that as long as you can get better at some point you stop.",
            "You found your first rule and then you Start learning the next rule and then you Start learning the next food.",
            "So essentially, this idea of IOP of a systematic search in your space of hypothesis.",
            "This is also done in this era.",
            "Of course, they're much more improved versions nowadays, but that's the vanilla.",
            "Right, the only difference is that in Contra."
        ],
        [
            "To IOP where we were using.",
            "Hope you saw it that he was zero in once and you were essentially looking at this junction.",
            "You're now dealing with probabilities here, right?",
            "So because you assume independency, let's say among your molecules among your examples.",
            "So you just have a product there and then you get something like that.",
            "So that's the main difference because you have let's say your marker logic network used the Markov logic network to do scoring, that's the main difference on this very high level of abstraction.",
            "So this concludes with almost no delay.",
            "The first part.",
            "So the idea now is that many of these things that may be confused now, hopefully refined by doing the same exercise, but now with this concrete specific example, namely Markov logic networks.",
            "And that's what petrol will do."
        ],
        [
            "OK, so Christian just gave you a very broadview of the field of statistical rationally."
        ],
        [
            "The goal now is to get more concrete.",
            "Pick one specific language and look at the representation.",
            "Look at some inference and learning algorithms and look at some concrete applications and then I will conclude with a little bit of."
        ],
        [
            "Discussion.",
            "So the way I'm going to do this is I'm going to start from logic and build towards being able to do logic probabilistic Lee and I will do it with.",
            "You know first with the representation, then the inference, then the learning.",
            "So let's start with logic and the simplest form of logic is propositional logic, and I assume that everybody here is already more or less familiar with this, so this will be fairly brief.",
            "But in propositional logic we start with atoms.",
            "Atoms are symbols that represent propositions about the world, and there are the true or false and then out of those.",
            "Small sentences will build larger sentences in logic using connectives like negation, conjunction, disjunction, and so forth.",
            "And then all of our knowledge, we call it a knowledge base.",
            "It's a set of formulas that state what we know or what we've learned, and then we call a world or possible world or interpretation a truth assignment to all the atoms that we can form by by doing this truth assignment I have just stated everything that I could possibly know about the world.",
            "What we're going to be interested in here is probability distributions over truth assignments to possible worlds.",
            "Another thing that we're going to be making use of is the fact that every knowledge base can be converted into a standard form called CNF.",
            "For conjunctive normal form, a CNF is a conjunction of clause is a clause is a disjunction of literals, and the literal is an Atom or its negation.",
            "So I can take my knowledge base, turn the crank and turn into CNF, and this makes a lot of things easier.",
            "And usually the way we do things in logic is by asking entailment queries.",
            "Typically in logic, what we want to do is we want to answer questions of the form given.",
            "This knowledge base is the following query entailed.",
            "Given what I know about this patient, does she have breast cancer or does she not have breast cancer?",
            "Or maybe I just don't know.",
            "So usually these are the kinds of questions that were true."
        ],
        [
            "Answer.",
            "Now, propositional logic is very nice and simple.",
            "It's also not a very compact way to say most things or a very transparent way.",
            "The same most things.",
            "What people use most of the time is first order logic and in first order logic we have all this structure that we just saw from propositional logic.",
            "But in addition, the atoms now have internal structure.",
            "An Atom is no longer just a symbol, but it's a predicate symbol followed by a list of arguments, which can be either variables or constants.",
            "So for example, friends and above is a predicate, Anna friends is the predicate symbol.",
            "This is an Atom, friends and X is an Atom, Anna is a constant and X is a variable.",
            "And we're going to call a ground Atom an Atom where all the arguments are constant.",
            "So for example, friends and X is not around that and.",
            "But if I replace X by Bob friends and above, now is a ground Atom.",
            "And again, we're going to be concerned a lot with ground items here.",
            "And now in addition to atoms not having internal structure, we have two new connectors which are the quantifiers, the universal quantifier and the existential quantifier.",
            "So for example, if I write for all for all X friends an X, this means that N is friends with everybody, and if I right there exists an X such that trends and X.",
            "This means that Anna has at least one friend.",
            "And in this talk I'm going to focus on finite domains and her interpretations.",
            "If you don't know what this means, don't worry about it 'cause it won't matter if you know what this means, then you know what I'm saying."
        ],
        [
            "So the way we're going to extend 1st order logic to handle uncertainties by combining it with Markov networks.",
            "So here's just a couple, and I know everybody here knows about Markov networks, but here's a couple of slides just to get you know the basics and the terminology down, so Markov network is an undirected graphical model.",
            "There's a node for every variable, and there's an arc between any two variables that have a direct dependence on each other, and the arcs are undirected.",
            "2 two sets of nodes are conditionally independent given a third set.",
            "If when I remove the third set from the graph, the remaining graphs are separated.",
            "So independency is just graph separation, it's very simple.",
            "So the graph tells me what conditional independence is, hold the parameters of the distribution are specified using what are called potential functions.",
            "There's a potential function for every click in the graph, a click being a completely connected set of notes, so there's one click here which is asthma, cancer cough and my one click that smoking cancer and potentials can take on any non negative real values so they don't.",
            "They can be more than one.",
            "So assuming that all the variable variables are Boolean, here's the potential table for smoking cancer or possible potential table and you'll see that what it's really doing is just assigning a lower value to the state, smoking true and cancer false.",
            "And the way to think intuitively about potentials is that the value of a potential at a state represents how compatible the values in that state are.",
            "For example, because smoking causes cancer, having smoking and not having cancer, these two things are less compatible than the others.",
            "And that's why this has a lower value.",
            "Another probability of a complete state is just the product of the click potentials for the for the states of the clicks in that state.",
            "Of course, there's a little problem here, which is that with all this reason that we have the sum of the probabilities of all states is not necessarily one.",
            "And so we need, you know the partition function, which is just the sum over all of the states of the product for that state.",
            "And by dividing by the partition function, we cannot guarantee that this sum is not the partition function.",
            "Of course looks very innocent, but it's really at the core of everything.",
            "We're going to try to do.",
            "If the partition function was easy to handle, life would be good since it's not easy to handle, we're going to need to be smart about dealing with it, so we'll be hearing a lot more about."
        ],
        [
            "Another thing to notice here is that this representation is simple, but it doesn't scale because you need the size of the potential is exponential in the number of variables in it.",
            "So if I want to talk about dependencies among a lot of variables, this doesn't work very well.",
            "But then I can do something else.",
            "Which is, you know, which is an equivalent form that of a log linear model instead of a product of factors.",
            "I have an exponentiated sum of weighted features.",
            "And it's easy to see that I can convert 1 from to the other just by having a feature for defining each possible state of of the clique and having a weight.",
            "That's the log of the value of the potential right, so I can convert from.",
            "I can convert from that form to this form.",
            "For example, if I define a feature, that's one.",
            "If you don't smoke or have cancer in 0 otherwise, and give that feature weight of .51, then this represents the same as the previous model.",
            "But it does so more compact with.",
            "In this case, I only see you know, went from 4 states to 1 feature, but imagine having a click with 10 variables and two to the 10 states.",
            "As long as I know that there's a small number of features that are the relevant ones, I can just define the model compactly using those features and we're going to be making good use of that."
        ],
        [
            "OK. Alright, so we would like to have probabilistic knowledge bases.",
            "Knowledge base is where the formulas that we stated I know longer just true or false, but have probability this is a longstanding question.",
            "Goes back to the 19th century.",
            "In AI it started, you know.",
            "At least in the 80s with Nelson's work.",
            "And so let's think about how we would do this right?",
            "The simplest way to have a probabilistic knowledge base on the face of it is just to assign probabilities to formulas.",
            "Instead of saying that you know smoking causes cancer, I would say that with probability .8, smoking causes cancer.",
            "This is a very natural thing to do, but you immediately run into a very big problem.",
            "Their problem is that if I just ate a bunch of formulas and their probabilities, they may not be consistent.",
            "I cannot, for example, say that the probability of Ace .8 and the probability of A&B is .9.",
            "The probability of envy has to be small in the probability of a.",
            "So the first thing you have to do is check that the formulas and the probabilities are consistent and their algorithms for doing this.",
            "But unfortunately there you know this is an intractable problem.",
            "Let's suppose, though, that you know you had somehow guarantee that all the formulas were consistent.",
            "In particular, the formula probabilities are learned from data.",
            "Those probabilities are guaranteed to be consistent, which is a very good thing, but now you still have another problem.",
            "The next problem that you have is that just stating the probabilities of a set of formulas does not completely define a distribution.",
            "Right just because I see that you know this formula holds with this probability in that former husband probability that does not give me enough information to now go and compute the probability of an arbitrary third formula.",
            "And this is something that caused a lot of confusion in the early days.",
            "There's a fairly generally accepted way to get around it, which is to make the maximum entropy assumption.",
            "This is just to assume that my model my distribution contains no information other than modest stated in the formulas and you know why.",
            "This is an assumption that is hard to refuse, because if you know something else about the distribution, then give me the formula and the probability that goes with it.",
            "And once you're done then we will back will be back to the maximum entropy assumption.",
            "OK, so now we in addition to the farmers and their probabilities, we make the maximum entropy assumption.",
            "Now we do have a completely defined joint distribution over all my items.",
            "And now we can answer all questions that we want, compute the probabilities of any formula that comes along OK. Now when we do that, when we solve the maximum entropy problem using constrained optimization with LaGrange multipliers, what you get is instead of having probabilities associated with the formulas you have weights.",
            "So now what we get is a set of formulas and their weights.",
            "And the weights are in fact the LaGrange multipliers.",
            "So if I want to if I want to set up a probabilistic knowledge base, there's two ways I can do it.",
            "I can start with a set of formulas in their probabilities and then verify that the forms are consistent and make the maximum entropy assumption.",
            "Or I can directly define the probabilistic knowledge base via set of formulas in their weights.",
            "These two things are equivalent.",
            "This one is a lot easier because you can specify any weights that you want and not worry about consistency or about incompleteness.",
            "The potential disadvantage of this is that you know to semis at least the the weights of formulas don't have as intuitive and meaning as the probabilities of the formulas.",
            "But if I have the formulas and weights, I can also go back and compute the probabilities of the formula.",
            "So these two things really are equivalent OK?",
            "And in the same way, now that I have, let's say that I have a set of formulas in their weights as we just saw a certain farmers and their weights is equivalent to a set of formulas in their potentials.",
            "I can define for each formula A corresponding potential.",
            "And the potential has one value in the formulas true and one following the formula is false.",
            "And in fact, because of the of the partition function, because of the normalization, we actually have too many degrees of freedom here, so we can just make a convention that we're going to let the potential corresponding to formula be one.",
            "If the formula is true and befire I for formula FI otherwise.",
            "This is a nice feature that we recover the deterministic case when this fire goes to 0.",
            "So if I is 0, then basically what we have is potentials that are one when the formula is true and zero on the formula is false and now we recover logic as a special case.",
            "And of course, the interesting case is going to be when you know the formulas have non zero FIS, which means that when the formula is false, you're probably doesn't go to zero, it just becomes lower.",
            "So at the end of the day, what is the probability of a world going to be?",
            "Well, it's a log in your model, so it's a normalized product over all the formulas but not overstated.",
            "1st order formula can have many groundings in the world, and each of those groundings might be true or false.",
            "Like you know, if I have the formula smokes S, it might be true for N and it might be false for Bob.",
            "And now what I need to do for each formula is the product of all of those.",
            "But notice that by our convention for the groundings that are true, the value of the potential is just one.",
            "So I just have a big product of ones and I can ignore it.",
            "So it ended the day.",
            "What I need is for each formula the fire of the formula raised to the number of worlds in which it's false.",
            "If I had made the opposite convention, it would be raised to the number of words in which it's true, but you know it's going to be convenient for us to define things in this way.",
            "OK, so this is our basic."
        ],
        [
            "Representation for probabilistic knowledge basis to make things a little bit more formal.",
            "What is Markov logic?",
            "Markov logic is just, you know.",
            "A Markov logic network, as we call a probabilistic knowledge base, is just a set of pairs FW where F is a formula in first order logic and W is its weight.",
            "And again, there we can be a real number.",
            "It can be positive or negative, so this is the syntax.",
            "What is the semantics?",
            "The semantics is that a Markov logic network defines a Markov network as follows.",
            "This Markov network is going to have one node for each grounding of each predicate in the MLM.",
            "So we take all the predicates that appearing in the knowledge base and all the constants.",
            "Put the contents is all possible ways.",
            "There's arguments to the predicates and that gives me a large number of ground atoms, and this is going to be the world that my knowledge base is over, and then the Markov networks link to have one feature for each grounding of each formula.",
            "So the formula smokes, X is going to have a grounding smokes.",
            "Anna smokes balls.",
            "Most child smokes Diane etc etc.",
            "And that would be another very interesting feature, but we'll see more interesting ones pretty soon, so there's going to be a feature for each grounding of each formula.",
            "All the groundings of a particular formula inherited sweet.",
            "So in some sense, what I do and I state a formula with the wait is that I'm compact with specifying a very large number, potentially even an infinite number of features in a ground Markov network, OK?"
        ],
        [
            "Here's an example that will hopefully make things clearer and more concrete.",
            "Let's suppose that I have two objects in my world, Ann and Bob and two predicates, friends and happy so happy scenery.",
            "Bob could be happy or unhappy.",
            "Anne, Anne, and Bob could be friends or not friends, and there's other groundings of these predicates.",
            "But let us just consider these two.",
            "OK, so with these two predicates in their groundings, there are only four possible worlds, right?",
            "There's the world where N is not friends with Bob and Bob is not happy.",
            "There is the world where there not friends, but he still happy and so forth.",
            "And now if I give you a standard formula in logic, what this does?"
        ],
        [
            "Is make this?",
            "For example, here's the formula, not friends and above or happy Bob.",
            "Right, this is just the causal form for friends and above implies happy Bob I. I'm saying that if Anna is friends with Bob then Bob is happy.",
            "And the effect of this is simply to make this world here impossible.",
            "This is, this is the only world of all the four that violates this formula.",
            "Because you know, friend above are friends with Bob is not happy.",
            "So this world goes, you know, goes away is excluded.",
            "And only these three possible worlds are left and logic.",
            "If you think about it, is really just the calculus of what happens when you make some words impossible.",
            "And then you want you want to know about the others.",
            "Now what we want to do here of course, is to have statements like the probability that not friends and above are happy.",
            "Bob is .8.",
            "What does this do?",
            "What this does is that this world now doesn't become impossible, is just becomes less probable than the other three worlds.",
            "How much less probable?"
        ],
        [
            "All is going to depend on this probability.",
            "In particular, let's say that we do start with this knowledge base and we make them the maximum entropy assumption and we turn the crank.",
            "We do the optimization what?"
        ],
        [
            "What are the potentials that we get out right again following our convention that when a formula is true, its potential value is one we get that the potential for the formula is 1 and the potential for the negation is .75.",
            "So each of these and this is only one formula.",
            "This knowledge base.",
            "So my unnormalized probability is just one in each of these three words in this .75 in this world, why's it .75 here?",
            "Because remember I want the probability of this to be .8.",
            "And 1 + 1 + 1 IE 3 / 3 + .7.",
            "3 + .75 is 3 /, 3.75 is .8.",
            "OK.",
            "So basically this.",
            "With the maximum entropy assumption is equivalent to this.",
            "OK. And now, of course we can, instead of having things in potential form, we can put it in."
        ],
        [
            "In weight form right all I have to do is from this.",
            "From this potential there's only one.",
            "In this case, compute the corresponding weight.",
            "The weight is just going to be the log odds between the world where the where the formula is true in the world where the formula is false, so the weight of this formula is going to be 29.",
            "So equivalent to the two things that we saw before, we could just say that we're going to have an MLN with this formula in it with the weight of .29 and all these things are saying exactly the same, OK?",
            "By the way, if you have questions at any point, just let me know.",
            "OK. And how about we take a 10 minute break now and restart it?"
        ],
        [
            "20 and then we'll do the inference.",
            "Learning applications in discussion."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, first thank you for inviting, and in particular for.",
                    "label": 0
                },
                {
                    "sent": "Giving me a chance of coming back to Barcelona.",
                    "label": 0
                },
                {
                    "sent": "So last year when I was here I had not much time to explore the city.",
                    "label": 0
                },
                {
                    "sent": "I hope I have some more time now.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this tutorial jointly with petrol is about combining logic and probability.",
                    "label": 1
                },
                {
                    "sent": "I'm very happy to give this talk because I'm quite excited about this research topic, but.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not only me, it's actually quite a lot of people who are excited about that so well.",
                    "label": 0
                },
                {
                    "sent": "Petro and I would like to thank all of them because without them what we're talking about, wouldn't be there.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in case that you want to leave earlier, here's what you should take home with you, right?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that.",
                    "label": 0
                },
                {
                    "sent": "Graphs are not enough.",
                    "label": 0
                },
                {
                    "sent": "We need logic.",
                    "label": 0
                },
                {
                    "sent": "I mean, given that we are here at you, I guess you know what we mean.",
                    "label": 0
                },
                {
                    "sent": "So, graphical models are cool.",
                    "label": 0
                },
                {
                    "sent": "We don't say we need logical models.",
                    "label": 1
                },
                {
                    "sent": "We need most likely new forms of models.",
                    "label": 0
                },
                {
                    "sent": "But we need models which combined probability and logic.",
                    "label": 0
                },
                {
                    "sent": "So graphs are not just.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enough.",
                    "label": 0
                },
                {
                    "sent": "The road map to this goal too.",
                    "label": 0
                },
                {
                    "sent": "Well, so that you have to take away message that you go home and say, yeah, cool we can contribute.",
                    "label": 0
                },
                {
                    "sent": "There is a little bit of motivation then a very biased short overview on what statistical Asia learning or statistical AI is.",
                    "label": 0
                },
                {
                    "sent": "And then many of the concept mentioned here will be illustrated using Markov logic networks in the second or third part.",
                    "label": 1
                },
                {
                    "sent": "There will be a break somewhere in the third part, but let's see how far we can get.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So motivation.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is currently my most favorite motivation for the whole topic for two reasons.",
                    "label": 0
                },
                {
                    "sent": "If you look at it, I can ask you what do you see.",
                    "label": 0
                },
                {
                    "sent": "And I can ask myself what I do?",
                    "label": 0
                },
                {
                    "sent": "What do I learn about you by looking at it?",
                    "label": 0
                },
                {
                    "sent": "But I also like it so my wife has a background just for you.",
                    "label": 0
                },
                {
                    "sent": "My wife is a psychiatrist so I can really tell you this doesn't work right.",
                    "label": 0
                },
                {
                    "sent": "We know that now for ages already, it really doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Doesn't tell me anything about you.",
                    "label": 0
                },
                {
                    "sent": "It doesn't tell you anything about me by showing you that next to that my wife is maybe.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But then there's or in a tuning or and he managed to come up with a roshak test telling a lot about computer scientists.",
                    "label": 0
                },
                {
                    "sent": "'cause when I show you this slide and ask you what you see here, I guess you can come up with many answers, right?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, you may say this is more slower, essentially right?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you may say this is storage.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Capacity, you may say this is number of scientific reputations.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You may say this is the number of Facebook users.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You may say it's a number of web pages.",
                    "label": 1
                },
                {
                    "sent": "Alright, any other kind of so?",
                    "label": 0
                },
                {
                    "sent": "For example, right now I was just coming back for a meeting on computational sustainability and can fairly say that it's similar curve you get for the data you gather currently with rather low price sensors, right?",
                    "label": 0
                },
                {
                    "sent": "And they're facing the problem.",
                    "label": 0
                },
                {
                    "sent": "How to make any sense of the of the data they're gathering there?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "A similar project.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, many people not only in the AI community or UI community or machine learning community, but also in the database community are looking at this idea of a worldwide mind, right?",
                    "label": 0
                },
                {
                    "sent": "So we have these tons of web pages.",
                    "label": 0
                },
                {
                    "sent": "We have users who are willing to spend their time to put all the knowledge into the web and imagine that we can extract all the knowledge they put there in at every second.",
                    "label": 0
                },
                {
                    "sent": "Of their life of our life.",
                    "label": 0
                },
                {
                    "sent": "If we can extract that and hopefully we can make the machine turned smarter, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the idea here.",
                    "label": 0
                },
                {
                    "sent": "And there's another project at the UW here I'm using here is slightly different citation here by Vikram.",
                    "label": 0
                },
                {
                    "sent": "They've done that in a slightly different way, but they all try to extract the knowledge people are encoding in the web and so here for example, if the text runner system organics Daniel Weldon or in attorney at UW where you can ask now queries like Paper Wild Card topic asking you in which relations?",
                    "label": 0
                },
                {
                    "sent": "Do paper and topic stand each other right?",
                    "label": 0
                },
                {
                    "sent": "And so you get back.",
                    "label": 0
                },
                {
                    "sent": "Things like paper discuss is, covers, addresses, discuss and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So what do we learn from these kinds of systems?",
                    "label": 0
                },
                {
                    "sent": "An what do we have to achieve or to manage to do if we want to make use of this data, right?",
                    "label": 0
                },
                {
                    "sent": "So right now, this is just data.",
                    "label": 0
                },
                {
                    "sent": "If you look a little bit closer than what you see, what we talk about here are kind of objects like paper and topic.",
                    "label": 0
                },
                {
                    "sent": "We have certain relations between paper and topic and maybe some form of uncertainties.",
                    "label": 0
                },
                {
                    "sent": "So the numbers here or something like that.",
                    "label": 0
                },
                {
                    "sent": "Number of times you have extracted this kind of effect, so let's not have a debate about whether it's really uncertain T or not.",
                    "label": 0
                },
                {
                    "sent": "You can view that in different ways, But for this tutorial, here is the best to view that as an uncertainty.",
                    "label": 0
                },
                {
                    "sent": "Namely, if you see it quite often this relation with a high number of counts, then you're sure about it or more sure about it.",
                    "label": 0
                },
                {
                    "sent": "And if you just have one or two counts.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It tells us that talks we are facing or we will face in the future.",
                    "label": 0
                },
                {
                    "sent": "Some are characterized by objects and these objects in contrast to traditional views in machine learning and AI are not just feature vectors, right?",
                    "label": 1
                },
                {
                    "sent": "They are more than that.",
                    "label": 1
                },
                {
                    "sent": "They have, maybe even parts.",
                    "label": 0
                },
                {
                    "sent": "They certainly have relations to other objects.",
                    "label": 1
                },
                {
                    "sent": "They might be trees, graphs, whatever, right.",
                    "label": 0
                },
                {
                    "sent": "So an object is not just one line in a table, but much more than that.",
                    "label": 1
                },
                {
                    "sent": "It's more flexible.",
                    "label": 0
                },
                {
                    "sent": "Given that there are relations, we should definitely say objects are not really independent, identically distributed, but they're like neighborhoods, right?",
                    "label": 0
                },
                {
                    "sent": "They are interconnected with other objects and information about the neighbors or related objects tells us a lot about the object we are currently facing.",
                    "label": 0
                },
                {
                    "sent": "There might be class hierarchies in the semantic web community.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is all about or the triple stores.",
                    "label": 0
                },
                {
                    "sent": "They talk about nowadays are still coming from the ontology.",
                    "label": 0
                },
                {
                    "sent": "And definitely properties of 1 object depends on the properties of the other ones.",
                    "label": 0
                },
                {
                    "sent": "Like if I look into the audience here, I don't know anything about most of you guys, but I definitely know that you're attending UI.",
                    "label": 0
                },
                {
                    "sent": "Because of that.",
                    "label": 0
                },
                {
                    "sent": "I also know that you have somehow interesting computer science.",
                    "label": 0
                },
                {
                    "sent": "And also somehow interest in mathematics right by just using my world knowledge and using the relations between you and other entities.",
                    "label": 0
                },
                {
                    "sent": "Like you, I I can draw much more conclusions than just looking at you like a single person.",
                    "label": 0
                },
                {
                    "sent": "Each of you in dependently.",
                    "label": 0
                },
                {
                    "sent": "That's all about.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we agree on that, definitely the question is how can we deal with this kind of knowledge?",
                    "label": 0
                },
                {
                    "sent": "How can we make sense of the knowledge base of the data?",
                    "label": 0
                },
                {
                    "sent": "How can we do inference in there?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's have a look at how people have done that in an AI.",
                    "label": 0
                },
                {
                    "sent": "Over the last ten 2030 years, right?",
                    "label": 0
                },
                {
                    "sent": "So one classical way of dealing with the complexity in there.",
                    "label": 0
                },
                {
                    "sent": "Things like we have objects and relations among them is first order logic, right?",
                    "label": 0
                },
                {
                    "sent": "So traditionally we may have an explicit enumeration.",
                    "label": 1
                },
                {
                    "sent": "We have atomic representation or first order or relational logical representation.",
                    "label": 0
                },
                {
                    "sent": "The benefit of using logic is essentially that you get a compressed representation of what you have encoded, otherwise in a long long list or numeration of all the facts.",
                    "label": 0
                },
                {
                    "sent": "So here, for example, where you list.",
                    "label": 0
                },
                {
                    "sent": "All the door to off relations for all pairs of individual separately.",
                    "label": 0
                },
                {
                    "sent": "Here you just define it right?",
                    "label": 0
                },
                {
                    "sent": "So we're now when we have any information, but rather often female, we can conclude that someone is the daughter or someone else.",
                    "label": 0
                },
                {
                    "sent": "So you get this compression in there you can get general rules.",
                    "label": 1
                },
                {
                    "sent": "Right and within then this kind of more traditional knowledge representation.",
                    "label": 1
                },
                {
                    "sent": "There was alot interesting going on like you can ask how to reprint all the different knowledge we were talking about like trees, graphs, hierarchies and so on.",
                    "label": 1
                },
                {
                    "sent": "But they are also very interesting inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "You typically in the early days at least few I have not seen much at you.",
                    "label": 0
                },
                {
                    "sent": "I like satisfiability, SAT solvers, resolution theorem proving and so on.",
                    "label": 1
                },
                {
                    "sent": "Right, so to even further illustrate that that's an example here.",
                    "label": 0
                },
                {
                    "sent": "Russell lights to give else in his book If you want to encode chess in an atomic representation where you just list everything, this would maybe take, let's say millions of pages.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm exaggerating here a little bit.",
                    "label": 0
                },
                {
                    "sent": "If you use propositional logic, which is something like a factorized representation, you can get already much more compact, but only if you use if you abstract of certain places on the board, you can get one or two page.",
                    "label": 0
                },
                {
                    "sent": "Kind of compression representation of the theory of the rules of chess, right?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but of course tasks are not just structured, they're not just complex.",
                    "label": 1
                },
                {
                    "sent": "But there's also a lot of uncertainty involved.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's why we're all here.",
                    "label": 0
                },
                {
                    "sent": "You AI and we want to deal with.",
                    "label": 0
                },
                {
                    "sent": "Probabilities because information might be ambiguous, might be incomplete.",
                    "label": 0
                },
                {
                    "sent": "We may just be uncertain about certain things.",
                    "label": 0
                },
                {
                    "sent": "There might be typos in there.",
                    "label": 0
                },
                {
                    "sent": "If you think of database and if you think of the task of cleaning a database, right?",
                    "label": 0
                },
                {
                    "sent": "If you're a big company and you have people typing in all the information about customers, it's very likely that there is here and then few typos in there.",
                    "label": 0
                },
                {
                    "sent": "So for example, I had reason troubles with our financial Department 'cause they were typing my last name wrongly, but only with two or little.",
                    "label": 0
                },
                {
                    "sent": "2 little typos, but it took them quite long and what you face is people say, oh, it's in the system, so it's true and I always try to tell them well, it's in the system so it's more likely to be true, but they still don't get it.",
                    "label": 0
                },
                {
                    "sent": "So let's try to work on that.",
                    "label": 0
                },
                {
                    "sent": "Right so then again, we can ask, how do computers systems nowadays deal with that certainty?",
                    "label": 1
                },
                {
                    "sent": "And well, that is all what you know of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About innocence mixture models, hidden Markov models, Bayesian networks, Markov random fields, maximum entropy models, whatever.",
                    "label": 1
                },
                {
                    "sent": "Take your favorite one in this sense, right?",
                    "label": 0
                },
                {
                    "sent": "So we have these two kind of axis in here.",
                    "label": 0
                },
                {
                    "sent": "If you look at what classically in AI people would have done to tackle these.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems right?",
                    "label": 0
                },
                {
                    "sent": "And so given these two axis, you can ask, will traditional AI or UI scale?",
                    "label": 0
                },
                {
                    "sent": "So given this simplified view on it, I hope you agree that this will not work.",
                    "label": 0
                },
                {
                    "sent": "Huawei.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see how you traditionally do it, like with the Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "For example, what does it mean in the traditional view we have something like a table, right?",
                    "label": 0
                },
                {
                    "sent": "So your features you may have class labels in there.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you do whatever you like.",
                    "label": 0
                },
                {
                    "sent": "You can do prediction, you may do.",
                    "label": 0
                },
                {
                    "sent": "Latent variable model or whatever.",
                    "label": 0
                },
                {
                    "sent": "So what you do is essentially if you try to discover a lot of in dependencies within your random variables you have there and then try to get a compact representation of these in dependencies.",
                    "label": 0
                },
                {
                    "sent": "Let's say in terms of the Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "In order to do efficient inference.",
                    "label": 0
                },
                {
                    "sent": "That's what we all know.",
                    "label": 0
                },
                {
                    "sent": "However, in many cases, like I was showing or you already with the world Wide Mind project or the text runner system, we know that we are not having just a single table, or indeed if you go for a universal table then this would be.",
                    "label": 0
                },
                {
                    "sent": "Very large table with a lot of null values in there, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not a good idea.",
                    "label": 0
                },
                {
                    "sent": "That's already what database theory tells you, so if you go for any kind of normal form then you would have not a single table but several tables with relations between them.",
                    "label": 0
                },
                {
                    "sent": "So how do we do right?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yet another example I was taking from Definion near is something at least I had to learn.",
                    "label": 0
                },
                {
                    "sent": "When I was PhD student, so as everybody we're going to submit a paper to a conference and then you know your supervisor.",
                    "label": 1
                },
                {
                    "sent": "If he has no time, just tells you yeah yeah, good papers get a so they get accepted, right?",
                    "label": 0
                },
                {
                    "sent": "He's not talking a lot about you.",
                    "label": 1
                },
                {
                    "sent": "The about the real rule there.",
                    "label": 1
                },
                {
                    "sent": "Or he may say good papers sometimes get ID and so on.",
                    "label": 0
                },
                {
                    "sent": "So he's giving you just these rules.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So from that you build up your little structure in a Bayesian network like you have quality of the paper.",
                    "label": 0
                },
                {
                    "sent": "How difficult is certain conferences and you try to predict the grade of your paper to see whether it's worthwhile submitting?",
                    "label": 0
                },
                {
                    "sent": "Is it clear that it gets accepted or not, right?",
                    "label": 0
                },
                {
                    "sent": "And then we have the condition probability distribution attached here and the overall joint distribution factorizes according to the graphs.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, but this is not the full story.",
                    "label": 0
                },
                {
                    "sent": "Again because I mean, as you know, here we don't have just UI here, but next week or actually with things, there's even an overlap here.",
                    "label": 0
                },
                {
                    "sent": "There's each key, right?",
                    "label": 0
                },
                {
                    "sent": "So let's see what that means.",
                    "label": 0
                },
                {
                    "sent": "So we are talking here about one paper submitted to UI or we think about the grade at UI.",
                    "label": 0
                },
                {
                    "sent": "We can think about the difficulty of AI and about the quality of paper one.",
                    "label": 0
                },
                {
                    "sent": "So let's say you get back in a here.",
                    "label": 0
                },
                {
                    "sent": "That means without any additional information evidence while getting into UI is not so difficult.",
                    "label": 0
                },
                {
                    "sent": "Some assumption here OK?",
                    "label": 0
                },
                {
                    "sent": "Of course, apriori the quality of the paper is pretty high, but that else would then sorry.",
                    "label": 0
                },
                {
                    "sent": "That also means then if we would submit this paper or another paper to UI, let's say the paper 2, then apriori without knowing anything about the paper, we should also expect that it gets accepted at UI.",
                    "label": 0
                },
                {
                    "sent": "That's definitely wrong.",
                    "label": 0
                },
                {
                    "sent": "So when I was talking to, my supervisor was actually the opposite away around, so he never got something accepted at UI.",
                    "label": 0
                },
                {
                    "sent": "And so I was scared by you.",
                    "label": 0
                },
                {
                    "sent": "I I tried to submit I got in right?",
                    "label": 0
                },
                {
                    "sent": "So it's the opposite.",
                    "label": 0
                },
                {
                    "sent": "But anyhow, you see that you should take all these different conferences into account.",
                    "label": 0
                },
                {
                    "sent": "So then we get additional evidence.",
                    "label": 0
                },
                {
                    "sent": "Let's say we got rejected that actually tells us to explain the whole setting that the paper to the quality was rather low without really knowing it.",
                    "label": 0
                },
                {
                    "sent": "And so we also know.",
                    "label": 0
                },
                {
                    "sent": "Now if I want to submit this paper now to each card, because I want to get it published, I should expect to get it rejected as well, right?",
                    "label": 0
                },
                {
                    "sent": "So what I want to tell you again, we should not try to think of random variables just as a single table or just as rows or columns in a single table.",
                    "label": 0
                },
                {
                    "sent": "But we should think of different families or groups of random variables where we may have relations between them or not.",
                    "label": 0
                },
                {
                    "sent": "So just think of a global state with more compact.",
                    "label": 0
                },
                {
                    "sent": "More interesting scale.",
                    "label": 0
                },
                {
                    "sent": "So in any case.",
                    "label": 0
                },
                {
                    "sent": "Independency is nice, we should try to get it independency.",
                    "label": 0
                },
                {
                    "sent": "But we should ask about it independency at the right level.",
                    "label": 0
                },
                {
                    "sent": "So independency at the object level might not be the right thing to do for silver applications.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, now I gave you time.",
                    "label": 0
                },
                {
                    "sent": "Will it scale?",
                    "label": 0
                },
                {
                    "sent": "I mean, I also told you the answer already, but if you now think again about it, but just treating them explicitly in a sense I mean exclusiveness saying either probability or we deal with logic, then certainly we can say no right?",
                    "label": 0
                },
                {
                    "sent": "So scaling up the environment?",
                    "label": 1
                },
                {
                    "sent": "Will inevitably overtake the resource of traditional AI or UI architectures.",
                    "label": 0
                },
                {
                    "sent": "If you agree on what I say.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the motivation for many people, so actually started.",
                    "label": 0
                },
                {
                    "sent": "Well, if you think about it, it started already quite early in the AI community, so Niels Nielsen maybe was the first coining the term probabilistic logic, but it took off early 90s to some extent, and then really around 2000 that people started to work on statistical relational learning.",
                    "label": 0
                },
                {
                    "sent": "Now about 11 years later, actually turns out that we realize it's not just statistical relational learning, but many tasks we typically face in AI where very well.",
                    "label": 0
                },
                {
                    "sent": "Fit under the umbrella of dealing with.",
                    "label": 0
                },
                {
                    "sent": "Probability, complexity and learning together and actually many tasks solutions become easier than in the traditional case, but more about that later.",
                    "label": 0
                },
                {
                    "sent": "So what we are facing now, what we're trying to do is to set up so to say the study and design of intelligence agent that act in noisy world composed of objects and relations among the objects that also very well fits.",
                    "label": 1
                },
                {
                    "sent": "For example the tutorial this morning about causality.",
                    "label": 0
                },
                {
                    "sent": "So note that ultimately we really want to have acting things right ultimately.",
                    "label": 0
                },
                {
                    "sent": "So it's not just probabilities but a little bit more than.",
                    "label": 0
                },
                {
                    "sent": "Probabilities you may even talk about utilities.",
                    "label": 0
                },
                {
                    "sent": "You also have to maybe realize your robot acting in the world and so on and so on, so it's really covering the whole spectrum here.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why?",
                    "label": 0
                },
                {
                    "sent": "Well, if you look at the conferences at AI conferences, what you typically see if different fields like common sense, natural languages, robotics, vision.",
                    "label": 0
                },
                {
                    "sent": "If you at all, C is still, for example, division people at the AI conferences, so they actually specialize and have their own very good conferences.",
                    "label": 0
                },
                {
                    "sent": "And why?",
                    "label": 0
                },
                {
                    "sent": "Because if you talk to them, they always tell you, oh, I want to know how to solve robotics or I want to know how to solve computer vision.",
                    "label": 1
                },
                {
                    "sent": "But is that actually true?",
                    "label": 0
                },
                {
                    "sent": "I mean, sure, they want to solve their particular problems, but actually what I feel is true is that we all face the similar problem.",
                    "label": 1
                },
                {
                    "sent": "Maybe we somehow have domain knowledge, language knowledge, or robot knowledge, or object knowledge about optics and objects.",
                    "label": 0
                },
                {
                    "sent": "But in a sense, if you look in the end, if you look at what they publish, the inference algorithms are often quite similar.",
                    "label": 0
                },
                {
                    "sent": "So the vision we have here is that we can abstract to some extent.",
                    "label": 0
                },
                {
                    "sent": "I mean you still need to be an expert in the field.",
                    "label": 1
                },
                {
                    "sent": "But to abstract to some extent, into something like each one is working on what really makes their trust different.",
                    "label": 0
                },
                {
                    "sent": "Where they are the real experts, but we share a common toolbox and inference and learning task here so that we benefit cross fertilize each other right.",
                    "label": 0
                },
                {
                    "sent": "So whatever the NLP guy is developing, the computer vision guy can use directly and actually that's happening.",
                    "label": 0
                },
                {
                    "sent": "One of the in my opinion, very interesting.",
                    "label": 0
                },
                {
                    "sent": "Cross benefits are topic models right?",
                    "label": 0
                },
                {
                    "sent": "So they started as something like just to understand what's going on in the collection of text documents.",
                    "label": 0
                },
                {
                    "sent": "But then if you look at CVP RCB the recent years you find a lot of topic models over visual words where people were inspired by the standard words in a text trying to identify what that means in an image coming up with ideas.",
                    "label": 0
                },
                {
                    "sent": "So this is something like patches and then they try to learn topic models about this visual words which is then for impact on computer on machine learning where there's a lot about dictionary learning which means how can we extract or learn these words automatically.",
                    "label": 0
                },
                {
                    "sent": "So if we can somehow their combined process, I guess we can make much more progress than just working independently of each other.",
                    "label": 0
                },
                {
                    "sent": "That's the vision.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Setting the stage a little bit motivating.",
                    "label": 0
                },
                {
                    "sent": "Why we're interested in combining logic and probabilities.",
                    "label": 0
                },
                {
                    "sent": "Why this tutorial?",
                    "label": 0
                },
                {
                    "sent": "Well, it's very active.",
                    "label": 0
                },
                {
                    "sent": "There's really a lot going on, and its multidisciplinary because as argued there, a lot of subdiscipline of AI involved, unfortunately, having said that, they still speak different languages.",
                    "label": 0
                },
                {
                    "sent": "Right, so we manage now maybe within 10 years to almost have agreed on.",
                    "label": 0
                },
                {
                    "sent": "Few languages, let's say 2 languages in a sense, within the statistical relational learning community, and actually this year, I guess we managed to also abstract from these languages and really identify some of the core problems.",
                    "label": 0
                },
                {
                    "sent": "But then if you talk to other people, they still have their own language, right?",
                    "label": 0
                },
                {
                    "sent": "And we have now two chances we can try to learn their languages, go there, solve their problems, show that they should be interested.",
                    "label": 0
                },
                {
                    "sent": "That's what we will do.",
                    "label": 0
                },
                {
                    "sent": "But this tutorials also to show you maybe it's a benefit for you as well.",
                    "label": 0
                },
                {
                    "sent": "So just try to jump on the train.",
                    "label": 0
                },
                {
                    "sent": "So if you say.",
                    "label": 0
                },
                {
                    "sent": "So why should you be interested?",
                    "label": 0
                },
                {
                    "sent": "Well, of course it advertisement here, but I guess it's really a success story and not because I'm defining that as a success story.",
                    "label": 0
                },
                {
                    "sent": "Actually, one should say petrol showed it a success story several times, but it's also if you look at many of these papers.",
                    "label": 0
                },
                {
                    "sent": "These papers got best paper awards at the conference as the values of the specific subdiscipline, right?",
                    "label": 0
                },
                {
                    "sent": "So they have been best paper about, for example, in NLP, right?",
                    "label": 0
                },
                {
                    "sent": "And so why should they give you in best paper what it?",
                    "label": 0
                },
                {
                    "sent": "There's nothing.",
                    "label": 0
                },
                {
                    "sent": "At all interesting for them.",
                    "label": 0
                },
                {
                    "sent": "So there is really something going on in bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "There's a lot going on because you have these experimental design issues and you can ask about relation between different experiments.",
                    "label": 0
                },
                {
                    "sent": "An if you make use of that, the group I definitely color has shown a lot improvements there as well within the planning community there's a lot going on.",
                    "label": 0
                },
                {
                    "sent": "Currently there was a follow up champion at the probabilistic competition which was using planning competition which was using a relational probabilistic approach, right?",
                    "label": 0
                },
                {
                    "sent": "So they are.",
                    "label": 0
                },
                {
                    "sent": "Lot of subfields where people start to see.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, there is really a benefit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So after this motivation, I hope you got a little bit excited.",
                    "label": 0
                },
                {
                    "sent": "At least let's.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give a short overview of what statistical relational learning is.",
                    "label": 0
                },
                {
                    "sent": "So next to these two few applications I've shown you the text runner stuff already and this kind of Toy Story about submitting papers there.",
                    "label": 0
                },
                {
                    "sent": "Of course, really a lot of applications up to date, so they have been in LP and information extraction alot.",
                    "label": 1
                },
                {
                    "sent": "I think one of the main home bases so to say for applications is link prediction.",
                    "label": 1
                },
                {
                    "sent": "So within machine learning all the collective classification tasks.",
                    "label": 0
                },
                {
                    "sent": "Can quite often be nicely represented within a statistical relational learning framework within social network analysis.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of work going on.",
                    "label": 0
                },
                {
                    "sent": "Think of mixed members and so on.",
                    "label": 0
                },
                {
                    "sent": "You can view them as relational models in robot mapping.",
                    "label": 0
                },
                {
                    "sent": "There has been some work by, for example, Petro again, but also by detail folks.",
                    "label": 0
                },
                {
                    "sent": "We're trying to work on that currently in activity recognition, and Sebastian Truman had once one paper on using Markov logic to explain activities.",
                    "label": 0
                },
                {
                    "sent": "Computation biology.",
                    "label": 0
                },
                {
                    "sent": "There's a lot going on.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So let's have a very brief look at them, and particularly this first example petrol will talk much more about that later where you also don't understand how you can model this kind of problem using.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of logic networks.",
                    "label": 0
                },
                {
                    "sent": "So in information extraction, for example, assume you have certain references.",
                    "label": 0
                },
                {
                    "sent": "Let's say you crawled the net or whatever, right?",
                    "label": 0
                },
                {
                    "sent": "So first task is to understand that these are different papers or they all denote Pape.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then next to that you may ask, oh, this is a paper.",
                    "label": 0
                },
                {
                    "sent": "So then I know it talks about authors title and venue like hear about prog zingler petrol, memory, efficient inference in relational domains is the title and to play 06 is a conference right?",
                    "label": 1
                },
                {
                    "sent": "And you do that for all these.",
                    "label": 0
                },
                {
                    "sent": "Diff.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Papers.",
                    "label": 0
                },
                {
                    "sent": "Well then you start realizing that zingler, P and probes Engler are actually the same persons right?",
                    "label": 0
                },
                {
                    "sent": "Although from the evidence you got, they're looking different.",
                    "label": 0
                },
                {
                    "sent": "But if you do some reasoning, for example by making use of that title here, there are two papers with the same title.",
                    "label": 0
                },
                {
                    "sent": "You expect that the two authors are identical, so you see how to take if you take relations into account, you get a better prediction on other thoughts, right?",
                    "label": 0
                },
                {
                    "sent": "And then so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So you see that information extraction.",
                    "label": 0
                },
                {
                    "sent": "NLP tasks definitely.",
                    "label": 0
                },
                {
                    "sent": "We can benefit by making use of relations by solving tasks jointly by making use of all the knowledge available or all the information available, right?",
                    "label": 0
                },
                {
                    "sent": "So relations are really at the harder.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Entity resolution.",
                    "label": 0
                },
                {
                    "sent": "Well, in the early days I did something on, for example, gene localization.",
                    "label": 1
                },
                {
                    "sent": "If you do just a collective inference where you say the location of 1 gene also depends on genes with therapist similar function, then with a rather very simple kind of graphical relational model you can be better than the KDD Cup winner at that time, right?",
                    "label": 0
                },
                {
                    "sent": "So I'm not saying this is the best model nowadays for this kind of task, but those days when you were just making use of the.",
                    "label": 0
                },
                {
                    "sent": "Relations between the different genes you can improve quite a lot.",
                    "label": 0
                },
                {
                    "sent": "What is currently going on in lot in robotics?",
                    "label": 0
                },
                {
                    "sent": "Is this collective influence?",
                    "label": 0
                },
                {
                    "sent": "If you want to do semantic labeling so you get these pointclouds and you would like to know is that tree is that wall is the ground floor, can my robot move there or not?",
                    "label": 0
                },
                {
                    "sent": "And then definitely you know that at least locally you should have something like a smoothness assumption that one pixel or voxel in terms of his class label is not so much different from the neighboring ones, right?",
                    "label": 0
                },
                {
                    "sent": "If you walk out of the building, you're not expecting that the road is all of a sudden stopping and then for the next millimeter.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "It's a tree.",
                    "label": 0
                },
                {
                    "sent": "And then there's again a road.",
                    "label": 0
                },
                {
                    "sent": "And then there's a hole.",
                    "label": 0
                },
                {
                    "sent": "And then there's a tree.",
                    "label": 0
                },
                {
                    "sent": "And then there's now you expect somehow, because there is a kind of manifold that locally everything is rather smooth and it's not changing radically.",
                    "label": 0
                },
                {
                    "sent": "So again, you make use.",
                    "label": 0
                },
                {
                    "sent": "You say if my neighboring pixel is a Rd, I'm more likely to be a Rd segment as well.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's also interesting because you may view here relations as constraints, so it's not like you just use the relations to propagate with.",
                    "label": 0
                },
                {
                    "sent": "You may also see it as a constraint or another one where we did some work with tossed in your concern.",
                    "label": 0
                },
                {
                    "sent": "For example on web search.",
                    "label": 0
                },
                {
                    "sent": "So definitely if I do a query I get certain documents back and you can think about preference relations in there.",
                    "label": 0
                },
                {
                    "sent": "But definitely you also have similar to reference relations between documents here and you know that one is relevant.",
                    "label": 0
                },
                {
                    "sent": "Then similar ones should be relevant as well.",
                    "label": 0
                },
                {
                    "sent": "You may also use the similarity relations to ask completely different interesting questions, like if you want to have diversification in here, then you know if I present that document I should not present that one anymore.",
                    "label": 0
                },
                {
                    "sent": "So people are using relations all the time already, even if they don't give it innate statistical relational learning.",
                    "label": 0
                },
                {
                    "sent": "But somehow they do it already and we just want to go one step ahead, make it more explicit, right?",
                    "label": 0
                },
                {
                    "sent": "And of course you can show that relation approaches really outperform standard approaches.",
                    "label": 1
                },
                {
                    "sent": "Of course, you have to pay the price.",
                    "label": 0
                },
                {
                    "sent": "That may be learning and also your inference is taking a little bit more complex sometimes, but it really pays off doing so.",
                    "label": 0
                },
                {
                    "sent": "Let me skip that one.",
                    "label": 0
                },
                {
                    "sent": "I think we can also skip that one.",
                    "label": 0
                },
                {
                    "sent": "So topic model there a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relational extension for that, but you may also use these kinds of techniques to develop within statistical relational learning with another task.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at how at least my student spent their spare time, they really watch YouTube a lot, right?",
                    "label": 0
                },
                {
                    "sent": "So well, actually not only their spare time also while working.",
                    "label": 0
                },
                {
                    "sent": "And so they but also YouTube is facing this problem of information broadcasting, right?",
                    "label": 0
                },
                {
                    "sent": "So you want to have the latest movie question is where can I download it?",
                    "label": 0
                },
                {
                    "sent": "What is the best policy to download it if one of my friends is already having some parts of a longer movie then I may be downloaded it there and then download the other pass from somebody.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so on, so.",
                    "label": 0
                },
                {
                    "sent": "Many of these big companies nowadays are really facing this problem of having complex networks and trying to broadcast information in there, or I think in the last tutorial we saw this one problem as well with the Microsoft.",
                    "label": 0
                },
                {
                    "sent": "How's it called service update assistant?",
                    "label": 0
                },
                {
                    "sent": "It's the same problem, right?",
                    "label": 0
                },
                {
                    "sent": "So Microsoft doesn't want to send everybody an email because this is really putting a lot of traffic.",
                    "label": 0
                },
                {
                    "sent": "So instead you would like to have it more in a peer to peer network like style.",
                    "label": 0
                },
                {
                    "sent": "So you send it to 1 computer, they send it to the next one.",
                    "label": 0
                },
                {
                    "sent": "So to keep the load for the traffic quite low.",
                    "label": 0
                },
                {
                    "sent": "So here petrol will talk a little bit more about lifted inference for C, But we were able to show that lifted inference techniques are faster than belief propagation perceived right?",
                    "label": 1
                },
                {
                    "sent": "So you can really show that even in task where belief propagation algorithms already quite fast and really used, you can make use of redundancies in there to speed up computation.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yet another one recent application we were able to show with relational techniques you can better predict heart attack diseases than any of the propositional systems, and here we are now talking really about money, because cardiovascular diseases really costs already the EU several billions in a year, right?",
                    "label": 0
                },
                {
                    "sent": "And so if we can somehow get that down, that's good, but you see something else which is important about relational systems.",
                    "label": 0
                },
                {
                    "sent": "If they are sometimes more declarative, better to understand.",
                    "label": 0
                },
                {
                    "sent": "So what you see here is.",
                    "label": 0
                },
                {
                    "sent": "Kind of local conditional probability table, which is now making use of abstractions so variables the lower parts are here variables, so you test whether a person is female or male and what you, well, you can't see here, but you have to trust me, they may the female subtree is much smaller than their male subtree, and that's actually what many medical doctors know, but they never had the chance to validate that using their propositional techniques.",
                    "label": 0
                },
                {
                    "sent": "So it's known that female.",
                    "label": 0
                },
                {
                    "sent": "Females are women have had much lower risk for heart attacks than males, right?",
                    "label": 0
                },
                {
                    "sent": "And so on and so on, and so it's very easy to sit down with the expert with a medical expert.",
                    "label": 0
                },
                {
                    "sent": "That's what we did here and here, under or she understood.",
                    "label": 0
                },
                {
                    "sent": "Right, so we were even able under submission to get new insights about hot topics.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm trying to illustrate interesting applications.",
                    "label": 0
                },
                {
                    "sent": "I think it already popped up that we should ask the questions.",
                    "label": 0
                },
                {
                    "sent": "What are relations, right?",
                    "label": 0
                },
                {
                    "sent": "Because there might be different ideas about what relations are.",
                    "label": 0
                },
                {
                    "sent": "So if you look into the literature, for example, zooming Ghahramani work on just on relational Gaussian process, is there.",
                    "label": 0
                },
                {
                    "sent": "The idea is that relations just provide additional correlations.",
                    "label": 1
                },
                {
                    "sent": "Right, so and closely related to that, you only still have only one table, but some additional knowledge, and you put that into your covariance function.",
                    "label": 0
                },
                {
                    "sent": "Or you try to come up with a distance function between graphs or you still say each graph is a single object in a sense, and you define some relations over.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another view which I'm using a lot to come up with lifted inference algorithms again.",
                    "label": 0
                },
                {
                    "sent": "Be confused now, petrol will tell you what lifted inference is later to some example is that you can view relations or so to say the science about symmetries redundancies because you try to make use that certain entities behave almost the same or identical, even if so, they're interchangeable.",
                    "label": 0
                },
                {
                    "sent": "I can do my inference only for one of them, and then I can use the same inference for the other ones.",
                    "label": 0
                },
                {
                    "sent": "Some other people have viewed relations as hypergraphs or the full fledged one where you really take your language.",
                    "label": 0
                },
                {
                    "sent": "Your favorite logical language and try to attach weights or uncertainty.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So because you have so many different views on that.",
                    "label": 0
                },
                {
                    "sent": "It's not so surprising that there's also this.",
                    "label": 0
                },
                {
                    "sent": "What is called SRL alphabets, which maybe also explains why it's not easy catching up sometimes to other people.",
                    "label": 0
                },
                {
                    "sent": "So let's say everything more or less started in the early 90s with people by the harsh aniak and all these people around the Yurei community.",
                    "label": 0
                },
                {
                    "sent": "But then actually it developed into a beast, in essence, right?",
                    "label": 0
                },
                {
                    "sent": "And I apologize for all the ones I have not listed on the slides at some point I was really not interested anymore.",
                    "label": 0
                },
                {
                    "sent": "And writing down all the different languages people came up with, or approaches, right?",
                    "label": 0
                },
                {
                    "sent": "So there is really an Alpha.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Soup, but there are few key dimensions we can distinguish.",
                    "label": 0
                },
                {
                    "sent": "Many of these approaches, so like in UI, there's kind of directed models or undirected models.",
                    "label": 0
                },
                {
                    "sent": "So for example directly at once, like where are they relation ull.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic relational models by Harvey Leaser.",
                    "label": 0
                },
                {
                    "sent": "Then we have stochastic logic programs block relational Bayesian network problem, relational dependency networks, and so on.",
                    "label": 0
                },
                {
                    "sent": "And I'm directed Markov logic networks, relational Markov networks, infinite hidden relational models and.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to illustrate a little bit, so here's the abstract alot right?",
                    "label": 0
                },
                {
                    "sent": "So there are many details.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to not present you.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more shining every state stay at the abstract level, so here's how it looks like.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a Bayesian network as you know it, you have rather something like a rule graph.",
                    "label": 1
                },
                {
                    "sent": "Rule graphs is similar to Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "It tells you something about the predicates which are, so to say containers or placeholders for a lot of instances in there.",
                    "label": 0
                },
                {
                    "sent": "Now, instead of just having a condition probability distribution.",
                    "label": 0
                },
                {
                    "sent": "We have something like a Bayesian network fragment in there, right?",
                    "label": 0
                },
                {
                    "sent": "So for example here picking up the submission or the paper grading problem.",
                    "label": 0
                },
                {
                    "sent": "You say whether a paper has a high quality depends on whether the author is smart or not.",
                    "label": 1
                },
                {
                    "sent": "So you use a placeholder paper to link the author of a certain paper with weather.",
                    "label": 0
                },
                {
                    "sent": "She was he was smart, right?",
                    "label": 0
                },
                {
                    "sent": "So it's the main difference.",
                    "label": 0
                },
                {
                    "sent": "Instead of just having one single.",
                    "label": 0
                },
                {
                    "sent": "Fragmente like in a conditional probability table.",
                    "label": 1
                },
                {
                    "sent": "You now have such a say a macro for several of these fragments.",
                    "label": 0
                },
                {
                    "sent": "You can now instantiate where plugging in here any person and any author.",
                    "label": 0
                },
                {
                    "sent": "Any paper here in any any author there.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's exactly what you do.",
                    "label": 0
                },
                {
                    "sent": "Imagine you're a few constants or really objects.",
                    "label": 0
                },
                {
                    "sent": "We want to talk about, so you can now put in all these placeholders the different values, and then you come up with.",
                    "label": 0
                },
                {
                    "sent": "So to say, Bayesian network fragments.",
                    "label": 0
                },
                {
                    "sent": "Right, so for example here if you have paper one and paper two, you get these two little different.",
                    "label": 0
                },
                {
                    "sent": "Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "Right, So what this means is if you.",
                    "label": 0
                },
                {
                    "sent": "I mean this way.",
                    "label": 0
                },
                {
                    "sent": "Going back this would mean you have only one paper.",
                    "label": 0
                },
                {
                    "sent": "Now assume we have two papers.",
                    "label": 1
                },
                {
                    "sent": "You get a larger network if you have three papers.",
                    "label": 0
                },
                {
                    "sent": "You will get more copies of that it is growing depending on how many objects you have, right?",
                    "label": 0
                },
                {
                    "sent": "So we can deal with a variable number of object this way, so we were making use of ideas from logic.",
                    "label": 1
                },
                {
                    "sent": "Combining that with idea from UI coming up with something like a Bayesian network.",
                    "label": 1
                },
                {
                    "sent": "Depending on the context, depending on the domain.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, but now you can of course also ask what happens if we Additionally or instead have something like Bob was also the author or coauthor of paper one?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Then we have a problem because we have only specified partial knowledge about one of the authors, so to say.",
                    "label": 0
                },
                {
                    "sent": "So we get multiple links in there.",
                    "label": 0
                },
                {
                    "sent": "We have somehow to combine and then many people have.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Post to use.",
                    "label": 0
                },
                {
                    "sent": "Something like aggregation functions, right?",
                    "label": 0
                },
                {
                    "sent": "So you take all your knowledge in there.",
                    "label": 0
                },
                {
                    "sent": "Let's say you take the mean, the average Max.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the mean, the median Max.",
                    "label": 0
                },
                {
                    "sent": "Or you may use noisy or noisy and whatever fits well.",
                    "label": 0
                },
                {
                    "sent": "Your model at that point.",
                    "label": 0
                },
                {
                    "sent": "This way from a set of conditional probability distribution which are not only specifying part of this sub problem, you get a complete conditional probability distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So that's one idea.",
                    "label": 0
                },
                {
                    "sent": "One problem, of course is that it's acyclic.",
                    "label": 0
                },
                {
                    "sent": "So if we want to learn this model, they still assume that you do not have a cycle.",
                    "label": 0
                },
                {
                    "sent": "We have to check that it's a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applicated so to overcome that, for example, Jennifer Neville and David Jensen came up with the idea of lifting relational dependency networks.",
                    "label": 0
                },
                {
                    "sent": "It's very easy.",
                    "label": 0
                },
                {
                    "sent": "You stay with the same idea now only you accept that there are cycles.",
                    "label": 0
                },
                {
                    "sent": "Because of that you cannot have exact inference anymore, but instead you skip sample or mean field approximations or whatever to do inference or to do lower right?",
                    "label": 0
                },
                {
                    "sent": "So works as well directly liftable by just saying OK, we have macros.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are insincere.",
                    "label": 0
                },
                {
                    "sent": "Alright, that works.",
                    "label": 0
                },
                {
                    "sent": "Really very same way you except now that you have cycles run everything on the ground.",
                    "label": 0
                },
                {
                    "sent": "In network you are happy.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the other hand, you can also go for the undirected world.",
                    "label": 0
                },
                {
                    "sent": "You will see much more about that in the second part.",
                    "label": 0
                },
                {
                    "sent": "But as an illustration here already, in a sense you have rules like here if an author of the paper is smart, then the quality is high.",
                    "label": 0
                },
                {
                    "sent": "If the quality is higher off the paper, then it gets accepted.",
                    "label": 0
                },
                {
                    "sent": "Of course, that's not all true.",
                    "label": 0
                },
                {
                    "sent": "That's why you place weights on them, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not always true, but in some possible worlds it's true.",
                    "label": 0
                },
                {
                    "sent": "In some it's not true.",
                    "label": 0
                },
                {
                    "sent": "So you get a distribution over it.",
                    "label": 0
                },
                {
                    "sent": "How do you do that?",
                    "label": 0
                },
                {
                    "sent": "Again, these placeholders here you put in all of these different constants, and by that you get the set of nodes or random variables.",
                    "label": 0
                },
                {
                    "sent": "You can talk about and then depending on in which ground I mean, after you replace these placeholders, this constant.",
                    "label": 0
                },
                {
                    "sent": "If two nodes are in one of these ground clauses, then you draw a line between them, so you can just view each of these rules as a macro again, right?",
                    "label": 0
                },
                {
                    "sent": "So like before we just instantiate them, you get a big big network.",
                    "label": 0
                },
                {
                    "sent": "You do whatever you like with it, inference, learning or whatever.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So another dimension.",
                    "label": 0
                },
                {
                    "sent": "Sense, so here I was using directed undirected mainly for focusing on this macro view, right?",
                    "label": 0
                },
                {
                    "sent": "So each of these rules is something like a macro that you're instantiating.",
                    "label": 0
                },
                {
                    "sent": "There are also other approaches which more look into what is called proof.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In logic.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go into all the details here, but let me give you an idea here and actually the idea is to show you that this is essentially against something else so you can express as a distribution over possible worlds.",
                    "label": 0
                },
                {
                    "sent": "So here's the logic so called prob.",
                    "label": 0
                },
                {
                    "sent": "Log by Luke the Rats group in Lewiston, so again you have somehow logics rules in there telling you hear that they support if there's an edge between two nodes or there's an edge between two nodes and apart from the one node.",
                    "label": 0
                },
                {
                    "sent": "The other node, right?",
                    "label": 0
                },
                {
                    "sent": "And then you have probabilities attached to the edges so it's something like a uncertain network here, right?",
                    "label": 0
                },
                {
                    "sent": "And so one way of defining some entry or to define a probabilistic program over it is now to view these.",
                    "label": 0
                },
                {
                    "sent": "Probabilities you have here as a sampling distribution, so you sample.",
                    "label": 0
                },
                {
                    "sent": "Whether you want to have this effect into your program or not, so put it differently, these ones are giving a distribution over programs, and so you sample these programs.",
                    "label": 1
                },
                {
                    "sent": "You asked whether your query is true, and this gives you then because probabilities were attached or you only sample the programs to a certain probability, gives you a distribution over the program, right?",
                    "label": 1
                },
                {
                    "sent": "In a certain way, again, you can turn that Elsa into possible worlds.",
                    "label": 0
                },
                {
                    "sent": "That's a UI paper.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 1
                },
                {
                    "sent": "I don't know when is the talk.",
                    "label": 0
                },
                {
                    "sent": "Is it on Friday Saturday?",
                    "label": 0
                },
                {
                    "sent": "I don't know or poster Sunday, so you see that you can also turn that into something like an undirected model as well, and that there are benefits in doing so.",
                    "label": 0
                },
                {
                    "sent": "Maybe also disadvantages, but the point here is that many of these programs are getting too large.",
                    "label": 0
                },
                {
                    "sent": "If you really sample all these different programs, so it may be better to not think of the programs what you want to store.",
                    "label": 0
                },
                {
                    "sent": "But you break these programs up into little pieces, and essentially that's the proof, right?",
                    "label": 0
                },
                {
                    "sent": "Essentially, I mean I'm happy to clarify that offline, but for what we want to talk here, essentially there would be something like.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are other dimensions, like there's some work on parametric and nonparametric.",
                    "label": 0
                },
                {
                    "sent": "So for example here the infinite hidden relational model that was also.",
                    "label": 0
                },
                {
                    "sent": "So I mean there was invented in parable by Josh Tenenbaum's, child's camp and folk art race in Java due so they try to combine nonparametric models and interconnect them using relations.",
                    "label": 0
                },
                {
                    "sent": "So you can do that if you like.",
                    "label": 0
                },
                {
                    "sent": "Or another dimension is closed world assumption.",
                    "label": 0
                },
                {
                    "sent": "So we know what we're talking about.",
                    "label": 0
                },
                {
                    "sent": "We know the number of objects and the objects in there.",
                    "label": 0
                },
                {
                    "sent": "Or something like an open world assumption where you don't know how many objects you have about which objects we can talk at all.",
                    "label": 0
                },
                {
                    "sent": "And there for example, you have non parametric block and block as well and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So these are all these different dimension.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then you can go on further.",
                    "label": 0
                },
                {
                    "sent": "Just to show you there is an alphabet soup, but there are dimensions.",
                    "label": 0
                },
                {
                    "sent": "We know why we did it.",
                    "label": 0
                },
                {
                    "sent": "And it's really covering all the work.",
                    "label": 0
                },
                {
                    "sent": "All the work in AI.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you like ocean processes, there are nowadays relation abortion process is if you like reinforcement learning, there are quite a lot of approaches to relational reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "There are even extensions to partially observable MDP switch our relational and where you can show there is a benefit in doing that for some cases we can even show that you can use lifted or relational technique to solve.",
                    "label": 0
                },
                {
                    "sent": "Innocence more efficiently in a certain other sense, none more efficient for now, but at least you can solve linear equations more efficiently than ground versions using belief propagation, organ belief, propagation.",
                    "label": 0
                },
                {
                    "sent": "There you can speed up a lot.",
                    "label": 0
                },
                {
                    "sent": "That's UI a, each kind next week.",
                    "label": 0
                },
                {
                    "sent": "There other papers at each card this year about common filters where you can show you can do common filtering else in a relational world, and you can also show again that you speed up, at least if you use caution.",
                    "label": 0
                },
                {
                    "sent": "Belief propagation for common filtering.",
                    "label": 0
                },
                {
                    "sent": "There's interesting work by Lisa.",
                    "label": 0
                },
                {
                    "sent": "Good tour going into declarative information networks to really say revolutionalize in this sense.",
                    "label": 1
                },
                {
                    "sent": "What is Google all about?",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "I hope you started to worry at some point here.",
                    "label": 0
                },
                {
                    "sent": "'cause all these names got.",
                    "label": 0
                },
                {
                    "sent": "I mean, how can I ever keep up with that?",
                    "label": 0
                },
                {
                    "sent": "Should we worry about?",
                    "label": 0
                },
                {
                    "sent": "No, I did this exercise.",
                    "label": 0
                },
                {
                    "sent": "Actually it's very much like the first proceedings of UI.",
                    "label": 1
                },
                {
                    "sent": "So I checked the very first 2 proceedings of UI.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you do that, you get all these different names in there.",
                    "label": 0
                },
                {
                    "sent": "There were even papers talking about what based theory missed about, and whether that's interesting to handle uncertainty or not, right?",
                    "label": 0
                },
                {
                    "sent": "So I like to say we wanted.",
                    "label": 0
                },
                {
                    "sent": "So to say at this stage slightly behind that already, so we already a little bit further give us a little bit more time and I guess we have one big idea about what we.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talking about right and why well for you I we know that also somehow boiled down nowadays to something like factor graphs, right?",
                    "label": 1
                },
                {
                    "sent": "If we abstract from certain details, right?",
                    "label": 0
                },
                {
                    "sent": "So we have random variables we affect this between the random variables and so together they somehow give you a factorization of a joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Normalize or not directed.",
                    "label": 0
                },
                {
                    "sent": "Whatever it doesn't care you abstract.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So we abstract also whether it's directed or not.",
                    "label": 0
                },
                {
                    "sent": "You can all fit that into this vector graph representation.",
                    "label": 0
                },
                {
                    "sent": "Similar.",
                    "label": 0
                },
                {
                    "sent": "This boiling down process also happened in SRL.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look nowadays at the literature, what you see is a lot about something like lifted factor graphs or parameterized factor graphs.",
                    "label": 0
                },
                {
                    "sent": "There's another field on Bedarra metrics circuits and waited, and essentially if you think about it, they all interconnect, right.",
                    "label": 0
                },
                {
                    "sent": "So awaited CNF is nothing else than affecter graph and essentially Petra.",
                    "label": 0
                },
                {
                    "sent": "I'm abstracting here Lil bit affecter graph is also something like an arithmetic circuit, let's see.",
                    "label": 0
                },
                {
                    "sent": "But the best paper work this year UI is slightly different there indeed, but on a high level we also boiled down to a certain agreement on what is underlying.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I just would like to give you a little bit of a flavor there.",
                    "label": 0
                },
                {
                    "sent": "So this is a factor graph and this is something like promised a factor graph, right?",
                    "label": 0
                },
                {
                    "sent": "What is the difference?",
                    "label": 0
                },
                {
                    "sent": "Well here it's all about attending a workshop and you're wondering whether it will start a series or whether it will be popular.",
                    "label": 0
                },
                {
                    "sent": "And so you ask whether certain people will attend yes or no.",
                    "label": 0
                },
                {
                    "sent": "And because you assume they're all identical here, we're not writing down the factors or potentials each time.",
                    "label": 0
                },
                {
                    "sent": "But instead we say for all people.",
                    "label": 0
                },
                {
                    "sent": "There is a factor between popular and attends X for that person right now.",
                    "label": 0
                },
                {
                    "sent": "Again, if we instantiate it here, you get this part here similar there, right?",
                    "label": 0
                },
                {
                    "sent": "So instead of having this more complex representation here, which is even depending on the number of people, you have a constant complexity representation here where you abstract it and now you give me the number of people or the people, and I can instantiate dependently on that.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me speak that it's not so important.",
                    "label": 0
                },
                {
                    "sent": "Point is and again petrol will give you more details about that.",
                    "label": 0
                },
                {
                    "sent": "You can now make use of this abstraction here so they're lifted inference techniques that never actually built up this ground model anymore, so you just do inference so to say on the lifted on the abstract level so that you get even answers which just depend on the domain size so that you get a constant answer no matter how many.",
                    "label": 0
                },
                {
                    "sent": "How many people there are at all?",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the other one, the weighted CNF, is coming from the fact that already each it's well known from the satisfiability community people have to starting work on not just using satisfiability problems, but where you wait the different.",
                    "label": 0
                },
                {
                    "sent": "CLOSE is in there.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, you're giving costs for it, and you want to find this assignment with a minimal cost, right?",
                    "label": 0
                },
                {
                    "sent": "And of course they can also be viewed as a factor graph, so this is a little bit more from the logic with a little bit of probabilities in there.",
                    "label": 0
                },
                {
                    "sent": "If you have only one weight per clause, this is very natural.",
                    "label": 0
                },
                {
                    "sent": "If you want to have a factor or power factor associated with your claws, then maybe the power factor is a little bit more interesting, but in the end you can convert between them, right?",
                    "label": 0
                },
                {
                    "sent": "You can always transform between them and again, there's a lot of redundancy in there that you can make use at inference time or learning time.",
                    "label": 0
                },
                {
                    "sent": "To speed up.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So given that, So what is the current state of the state is the filling, so it is good to combine probabilities and logic because it gives you more compact models because they are more complex.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you can do infringe much faster because you can stay at the abstract level.",
                    "label": 0
                },
                {
                    "sent": "Similar for learning to some extent, right?",
                    "label": 0
                },
                {
                    "sent": "Because you are an abstract level, you do not have to learn each rule for each person again and then in the long run.",
                    "label": 0
                },
                {
                    "sent": "You converge and you say, yeah, I know they're essentially identical, so you can place the additional constraint and test.",
                    "label": 0
                },
                {
                    "sent": "There's this rule for all people because of that.",
                    "label": 0
                },
                {
                    "sent": "Well, not in any complexity theory, it's faster, but it may be first in learning because instead of bottom up trying to figure out all these little potentials are identical, you go top down and if there's a lot of regularity then you're actually faster by testing only few of these abstract rules and how have people down there.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, they use what is known in logic in learning these kinds of logical models that called inductive logic programming where you want to find these kinds of logic programs.",
                    "label": 0
                },
                {
                    "sent": "Given few examples like certain molecules are Muta, genic yes or no and these molecules are complex beasts, right?",
                    "label": 0
                },
                {
                    "sent": "So you can have as many atoms in there to describe that.",
                    "label": 0
                },
                {
                    "sent": "So here you may have 10 here.",
                    "label": 0
                },
                {
                    "sent": "You may have 100 atoms, so you have applied flexibly representation and you want given that you want to learn these kinds of programs so various.",
                    "label": 0
                },
                {
                    "sent": "Very similar, you can now learn these probabilistic relational models, at least if you're happy to stay with this vanilla.",
                    "label": 0
                },
                {
                    "sent": "Very basic learning algorithm, how would you do it?",
                    "label": 0
                },
                {
                    "sent": "Let's have an example at a classical algorithm.",
                    "label": 0
                },
                {
                    "sent": "IOP inductive logic programming, not integer linear programming algorithm foiled by Quinlan.",
                    "label": 0
                },
                {
                    "sent": "How is he doing that?",
                    "label": 0
                },
                {
                    "sent": "Well, very easily you start with no rule and then you start really in a dumb way in a systematic way.",
                    "label": 0
                },
                {
                    "sent": "You just refine what you have there.",
                    "label": 0
                },
                {
                    "sent": "You place all possible predicates you can think of in there and test them.",
                    "label": 0
                },
                {
                    "sent": "Right, so for example, you may test Atom xac, which means there's a certain type of Atom certain other tepin Atom in the molecule, so another one, and then using one whatever heuristic you have a scoring function.",
                    "label": 0
                },
                {
                    "sent": "So in our case we may use likelihood, turns out will not work.",
                    "label": 0
                },
                {
                    "sent": "In most cases.",
                    "label": 0
                },
                {
                    "sent": "You may use pseudo likelihood or conditional likelihood, but you score them and then in the greedy fashion Hill climbing you select the best one, try to refine.",
                    "label": 0
                },
                {
                    "sent": "You do that as long as you can get better at some point you stop.",
                    "label": 0
                },
                {
                    "sent": "You found your first rule and then you Start learning the next rule and then you Start learning the next food.",
                    "label": 0
                },
                {
                    "sent": "So essentially, this idea of IOP of a systematic search in your space of hypothesis.",
                    "label": 0
                },
                {
                    "sent": "This is also done in this era.",
                    "label": 0
                },
                {
                    "sent": "Of course, they're much more improved versions nowadays, but that's the vanilla.",
                    "label": 0
                },
                {
                    "sent": "Right, the only difference is that in Contra.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To IOP where we were using.",
                    "label": 0
                },
                {
                    "sent": "Hope you saw it that he was zero in once and you were essentially looking at this junction.",
                    "label": 0
                },
                {
                    "sent": "You're now dealing with probabilities here, right?",
                    "label": 0
                },
                {
                    "sent": "So because you assume independency, let's say among your molecules among your examples.",
                    "label": 0
                },
                {
                    "sent": "So you just have a product there and then you get something like that.",
                    "label": 0
                },
                {
                    "sent": "So that's the main difference because you have let's say your marker logic network used the Markov logic network to do scoring, that's the main difference on this very high level of abstraction.",
                    "label": 0
                },
                {
                    "sent": "So this concludes with almost no delay.",
                    "label": 0
                },
                {
                    "sent": "The first part.",
                    "label": 0
                },
                {
                    "sent": "So the idea now is that many of these things that may be confused now, hopefully refined by doing the same exercise, but now with this concrete specific example, namely Markov logic networks.",
                    "label": 0
                },
                {
                    "sent": "And that's what petrol will do.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so Christian just gave you a very broadview of the field of statistical rationally.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal now is to get more concrete.",
                    "label": 0
                },
                {
                    "sent": "Pick one specific language and look at the representation.",
                    "label": 0
                },
                {
                    "sent": "Look at some inference and learning algorithms and look at some concrete applications and then I will conclude with a little bit of.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discussion.",
                    "label": 0
                },
                {
                    "sent": "So the way I'm going to do this is I'm going to start from logic and build towards being able to do logic probabilistic Lee and I will do it with.",
                    "label": 0
                },
                {
                    "sent": "You know first with the representation, then the inference, then the learning.",
                    "label": 0
                },
                {
                    "sent": "So let's start with logic and the simplest form of logic is propositional logic, and I assume that everybody here is already more or less familiar with this, so this will be fairly brief.",
                    "label": 0
                },
                {
                    "sent": "But in propositional logic we start with atoms.",
                    "label": 1
                },
                {
                    "sent": "Atoms are symbols that represent propositions about the world, and there are the true or false and then out of those.",
                    "label": 0
                },
                {
                    "sent": "Small sentences will build larger sentences in logic using connectives like negation, conjunction, disjunction, and so forth.",
                    "label": 0
                },
                {
                    "sent": "And then all of our knowledge, we call it a knowledge base.",
                    "label": 0
                },
                {
                    "sent": "It's a set of formulas that state what we know or what we've learned, and then we call a world or possible world or interpretation a truth assignment to all the atoms that we can form by by doing this truth assignment I have just stated everything that I could possibly know about the world.",
                    "label": 1
                },
                {
                    "sent": "What we're going to be interested in here is probability distributions over truth assignments to possible worlds.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we're going to be making use of is the fact that every knowledge base can be converted into a standard form called CNF.",
                    "label": 1
                },
                {
                    "sent": "For conjunctive normal form, a CNF is a conjunction of clause is a clause is a disjunction of literals, and the literal is an Atom or its negation.",
                    "label": 1
                },
                {
                    "sent": "So I can take my knowledge base, turn the crank and turn into CNF, and this makes a lot of things easier.",
                    "label": 0
                },
                {
                    "sent": "And usually the way we do things in logic is by asking entailment queries.",
                    "label": 0
                },
                {
                    "sent": "Typically in logic, what we want to do is we want to answer questions of the form given.",
                    "label": 0
                },
                {
                    "sent": "This knowledge base is the following query entailed.",
                    "label": 0
                },
                {
                    "sent": "Given what I know about this patient, does she have breast cancer or does she not have breast cancer?",
                    "label": 0
                },
                {
                    "sent": "Or maybe I just don't know.",
                    "label": 0
                },
                {
                    "sent": "So usually these are the kinds of questions that were true.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Answer.",
                    "label": 0
                },
                {
                    "sent": "Now, propositional logic is very nice and simple.",
                    "label": 0
                },
                {
                    "sent": "It's also not a very compact way to say most things or a very transparent way.",
                    "label": 0
                },
                {
                    "sent": "The same most things.",
                    "label": 0
                },
                {
                    "sent": "What people use most of the time is first order logic and in first order logic we have all this structure that we just saw from propositional logic.",
                    "label": 0
                },
                {
                    "sent": "But in addition, the atoms now have internal structure.",
                    "label": 0
                },
                {
                    "sent": "An Atom is no longer just a symbol, but it's a predicate symbol followed by a list of arguments, which can be either variables or constants.",
                    "label": 0
                },
                {
                    "sent": "So for example, friends and above is a predicate, Anna friends is the predicate symbol.",
                    "label": 0
                },
                {
                    "sent": "This is an Atom, friends and X is an Atom, Anna is a constant and X is a variable.",
                    "label": 0
                },
                {
                    "sent": "And we're going to call a ground Atom an Atom where all the arguments are constant.",
                    "label": 1
                },
                {
                    "sent": "So for example, friends and X is not around that and.",
                    "label": 0
                },
                {
                    "sent": "But if I replace X by Bob friends and above, now is a ground Atom.",
                    "label": 0
                },
                {
                    "sent": "And again, we're going to be concerned a lot with ground items here.",
                    "label": 0
                },
                {
                    "sent": "And now in addition to atoms not having internal structure, we have two new connectors which are the quantifiers, the universal quantifier and the existential quantifier.",
                    "label": 0
                },
                {
                    "sent": "So for example, if I write for all for all X friends an X, this means that N is friends with everybody, and if I right there exists an X such that trends and X.",
                    "label": 0
                },
                {
                    "sent": "This means that Anna has at least one friend.",
                    "label": 0
                },
                {
                    "sent": "And in this talk I'm going to focus on finite domains and her interpretations.",
                    "label": 0
                },
                {
                    "sent": "If you don't know what this means, don't worry about it 'cause it won't matter if you know what this means, then you know what I'm saying.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way we're going to extend 1st order logic to handle uncertainties by combining it with Markov networks.",
                    "label": 0
                },
                {
                    "sent": "So here's just a couple, and I know everybody here knows about Markov networks, but here's a couple of slides just to get you know the basics and the terminology down, so Markov network is an undirected graphical model.",
                    "label": 0
                },
                {
                    "sent": "There's a node for every variable, and there's an arc between any two variables that have a direct dependence on each other, and the arcs are undirected.",
                    "label": 0
                },
                {
                    "sent": "2 two sets of nodes are conditionally independent given a third set.",
                    "label": 0
                },
                {
                    "sent": "If when I remove the third set from the graph, the remaining graphs are separated.",
                    "label": 0
                },
                {
                    "sent": "So independency is just graph separation, it's very simple.",
                    "label": 0
                },
                {
                    "sent": "So the graph tells me what conditional independence is, hold the parameters of the distribution are specified using what are called potential functions.",
                    "label": 0
                },
                {
                    "sent": "There's a potential function for every click in the graph, a click being a completely connected set of notes, so there's one click here which is asthma, cancer cough and my one click that smoking cancer and potentials can take on any non negative real values so they don't.",
                    "label": 0
                },
                {
                    "sent": "They can be more than one.",
                    "label": 0
                },
                {
                    "sent": "So assuming that all the variable variables are Boolean, here's the potential table for smoking cancer or possible potential table and you'll see that what it's really doing is just assigning a lower value to the state, smoking true and cancer false.",
                    "label": 0
                },
                {
                    "sent": "And the way to think intuitively about potentials is that the value of a potential at a state represents how compatible the values in that state are.",
                    "label": 0
                },
                {
                    "sent": "For example, because smoking causes cancer, having smoking and not having cancer, these two things are less compatible than the others.",
                    "label": 0
                },
                {
                    "sent": "And that's why this has a lower value.",
                    "label": 0
                },
                {
                    "sent": "Another probability of a complete state is just the product of the click potentials for the for the states of the clicks in that state.",
                    "label": 0
                },
                {
                    "sent": "Of course, there's a little problem here, which is that with all this reason that we have the sum of the probabilities of all states is not necessarily one.",
                    "label": 0
                },
                {
                    "sent": "And so we need, you know the partition function, which is just the sum over all of the states of the product for that state.",
                    "label": 0
                },
                {
                    "sent": "And by dividing by the partition function, we cannot guarantee that this sum is not the partition function.",
                    "label": 0
                },
                {
                    "sent": "Of course looks very innocent, but it's really at the core of everything.",
                    "label": 0
                },
                {
                    "sent": "We're going to try to do.",
                    "label": 0
                },
                {
                    "sent": "If the partition function was easy to handle, life would be good since it's not easy to handle, we're going to need to be smart about dealing with it, so we'll be hearing a lot more about.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another thing to notice here is that this representation is simple, but it doesn't scale because you need the size of the potential is exponential in the number of variables in it.",
                    "label": 0
                },
                {
                    "sent": "So if I want to talk about dependencies among a lot of variables, this doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "But then I can do something else.",
                    "label": 0
                },
                {
                    "sent": "Which is, you know, which is an equivalent form that of a log linear model instead of a product of factors.",
                    "label": 0
                },
                {
                    "sent": "I have an exponentiated sum of weighted features.",
                    "label": 0
                },
                {
                    "sent": "And it's easy to see that I can convert 1 from to the other just by having a feature for defining each possible state of of the clique and having a weight.",
                    "label": 0
                },
                {
                    "sent": "That's the log of the value of the potential right, so I can convert from.",
                    "label": 0
                },
                {
                    "sent": "I can convert from that form to this form.",
                    "label": 0
                },
                {
                    "sent": "For example, if I define a feature, that's one.",
                    "label": 0
                },
                {
                    "sent": "If you don't smoke or have cancer in 0 otherwise, and give that feature weight of .51, then this represents the same as the previous model.",
                    "label": 0
                },
                {
                    "sent": "But it does so more compact with.",
                    "label": 0
                },
                {
                    "sent": "In this case, I only see you know, went from 4 states to 1 feature, but imagine having a click with 10 variables and two to the 10 states.",
                    "label": 0
                },
                {
                    "sent": "As long as I know that there's a small number of features that are the relevant ones, I can just define the model compactly using those features and we're going to be making good use of that.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Alright, so we would like to have probabilistic knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "Knowledge base is where the formulas that we stated I know longer just true or false, but have probability this is a longstanding question.",
                    "label": 0
                },
                {
                    "sent": "Goes back to the 19th century.",
                    "label": 0
                },
                {
                    "sent": "In AI it started, you know.",
                    "label": 0
                },
                {
                    "sent": "At least in the 80s with Nelson's work.",
                    "label": 0
                },
                {
                    "sent": "And so let's think about how we would do this right?",
                    "label": 0
                },
                {
                    "sent": "The simplest way to have a probabilistic knowledge base on the face of it is just to assign probabilities to formulas.",
                    "label": 0
                },
                {
                    "sent": "Instead of saying that you know smoking causes cancer, I would say that with probability .8, smoking causes cancer.",
                    "label": 0
                },
                {
                    "sent": "This is a very natural thing to do, but you immediately run into a very big problem.",
                    "label": 0
                },
                {
                    "sent": "Their problem is that if I just ate a bunch of formulas and their probabilities, they may not be consistent.",
                    "label": 1
                },
                {
                    "sent": "I cannot, for example, say that the probability of Ace .8 and the probability of A&B is .9.",
                    "label": 0
                },
                {
                    "sent": "The probability of envy has to be small in the probability of a.",
                    "label": 0
                },
                {
                    "sent": "So the first thing you have to do is check that the formulas and the probabilities are consistent and their algorithms for doing this.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately there you know this is an intractable problem.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose, though, that you know you had somehow guarantee that all the formulas were consistent.",
                    "label": 0
                },
                {
                    "sent": "In particular, the formula probabilities are learned from data.",
                    "label": 0
                },
                {
                    "sent": "Those probabilities are guaranteed to be consistent, which is a very good thing, but now you still have another problem.",
                    "label": 0
                },
                {
                    "sent": "The next problem that you have is that just stating the probabilities of a set of formulas does not completely define a distribution.",
                    "label": 0
                },
                {
                    "sent": "Right just because I see that you know this formula holds with this probability in that former husband probability that does not give me enough information to now go and compute the probability of an arbitrary third formula.",
                    "label": 0
                },
                {
                    "sent": "And this is something that caused a lot of confusion in the early days.",
                    "label": 0
                },
                {
                    "sent": "There's a fairly generally accepted way to get around it, which is to make the maximum entropy assumption.",
                    "label": 0
                },
                {
                    "sent": "This is just to assume that my model my distribution contains no information other than modest stated in the formulas and you know why.",
                    "label": 0
                },
                {
                    "sent": "This is an assumption that is hard to refuse, because if you know something else about the distribution, then give me the formula and the probability that goes with it.",
                    "label": 0
                },
                {
                    "sent": "And once you're done then we will back will be back to the maximum entropy assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we in addition to the farmers and their probabilities, we make the maximum entropy assumption.",
                    "label": 0
                },
                {
                    "sent": "Now we do have a completely defined joint distribution over all my items.",
                    "label": 0
                },
                {
                    "sent": "And now we can answer all questions that we want, compute the probabilities of any formula that comes along OK. Now when we do that, when we solve the maximum entropy problem using constrained optimization with LaGrange multipliers, what you get is instead of having probabilities associated with the formulas you have weights.",
                    "label": 0
                },
                {
                    "sent": "So now what we get is a set of formulas and their weights.",
                    "label": 1
                },
                {
                    "sent": "And the weights are in fact the LaGrange multipliers.",
                    "label": 0
                },
                {
                    "sent": "So if I want to if I want to set up a probabilistic knowledge base, there's two ways I can do it.",
                    "label": 1
                },
                {
                    "sent": "I can start with a set of formulas in their probabilities and then verify that the forms are consistent and make the maximum entropy assumption.",
                    "label": 0
                },
                {
                    "sent": "Or I can directly define the probabilistic knowledge base via set of formulas in their weights.",
                    "label": 0
                },
                {
                    "sent": "These two things are equivalent.",
                    "label": 0
                },
                {
                    "sent": "This one is a lot easier because you can specify any weights that you want and not worry about consistency or about incompleteness.",
                    "label": 0
                },
                {
                    "sent": "The potential disadvantage of this is that you know to semis at least the the weights of formulas don't have as intuitive and meaning as the probabilities of the formulas.",
                    "label": 0
                },
                {
                    "sent": "But if I have the formulas and weights, I can also go back and compute the probabilities of the formula.",
                    "label": 0
                },
                {
                    "sent": "So these two things really are equivalent OK?",
                    "label": 0
                },
                {
                    "sent": "And in the same way, now that I have, let's say that I have a set of formulas in their weights as we just saw a certain farmers and their weights is equivalent to a set of formulas in their potentials.",
                    "label": 0
                },
                {
                    "sent": "I can define for each formula A corresponding potential.",
                    "label": 0
                },
                {
                    "sent": "And the potential has one value in the formulas true and one following the formula is false.",
                    "label": 0
                },
                {
                    "sent": "And in fact, because of the of the partition function, because of the normalization, we actually have too many degrees of freedom here, so we can just make a convention that we're going to let the potential corresponding to formula be one.",
                    "label": 0
                },
                {
                    "sent": "If the formula is true and befire I for formula FI otherwise.",
                    "label": 0
                },
                {
                    "sent": "This is a nice feature that we recover the deterministic case when this fire goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So if I is 0, then basically what we have is potentials that are one when the formula is true and zero on the formula is false and now we recover logic as a special case.",
                    "label": 0
                },
                {
                    "sent": "And of course, the interesting case is going to be when you know the formulas have non zero FIS, which means that when the formula is false, you're probably doesn't go to zero, it just becomes lower.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, what is the probability of a world going to be?",
                    "label": 0
                },
                {
                    "sent": "Well, it's a log in your model, so it's a normalized product over all the formulas but not overstated.",
                    "label": 0
                },
                {
                    "sent": "1st order formula can have many groundings in the world, and each of those groundings might be true or false.",
                    "label": 0
                },
                {
                    "sent": "Like you know, if I have the formula smokes S, it might be true for N and it might be false for Bob.",
                    "label": 0
                },
                {
                    "sent": "And now what I need to do for each formula is the product of all of those.",
                    "label": 0
                },
                {
                    "sent": "But notice that by our convention for the groundings that are true, the value of the potential is just one.",
                    "label": 0
                },
                {
                    "sent": "So I just have a big product of ones and I can ignore it.",
                    "label": 0
                },
                {
                    "sent": "So it ended the day.",
                    "label": 0
                },
                {
                    "sent": "What I need is for each formula the fire of the formula raised to the number of worlds in which it's false.",
                    "label": 0
                },
                {
                    "sent": "If I had made the opposite convention, it would be raised to the number of words in which it's true, but you know it's going to be convenient for us to define things in this way.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is our basic.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Representation for probabilistic knowledge basis to make things a little bit more formal.",
                    "label": 0
                },
                {
                    "sent": "What is Markov logic?",
                    "label": 0
                },
                {
                    "sent": "Markov logic is just, you know.",
                    "label": 0
                },
                {
                    "sent": "A Markov logic network, as we call a probabilistic knowledge base, is just a set of pairs FW where F is a formula in first order logic and W is its weight.",
                    "label": 1
                },
                {
                    "sent": "And again, there we can be a real number.",
                    "label": 0
                },
                {
                    "sent": "It can be positive or negative, so this is the syntax.",
                    "label": 0
                },
                {
                    "sent": "What is the semantics?",
                    "label": 1
                },
                {
                    "sent": "The semantics is that a Markov logic network defines a Markov network as follows.",
                    "label": 1
                },
                {
                    "sent": "This Markov network is going to have one node for each grounding of each predicate in the MLM.",
                    "label": 0
                },
                {
                    "sent": "So we take all the predicates that appearing in the knowledge base and all the constants.",
                    "label": 0
                },
                {
                    "sent": "Put the contents is all possible ways.",
                    "label": 0
                },
                {
                    "sent": "There's arguments to the predicates and that gives me a large number of ground atoms, and this is going to be the world that my knowledge base is over, and then the Markov networks link to have one feature for each grounding of each formula.",
                    "label": 0
                },
                {
                    "sent": "So the formula smokes, X is going to have a grounding smokes.",
                    "label": 0
                },
                {
                    "sent": "Anna smokes balls.",
                    "label": 0
                },
                {
                    "sent": "Most child smokes Diane etc etc.",
                    "label": 0
                },
                {
                    "sent": "And that would be another very interesting feature, but we'll see more interesting ones pretty soon, so there's going to be a feature for each grounding of each formula.",
                    "label": 0
                },
                {
                    "sent": "All the groundings of a particular formula inherited sweet.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, what I do and I state a formula with the wait is that I'm compact with specifying a very large number, potentially even an infinite number of features in a ground Markov network, OK?",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example that will hopefully make things clearer and more concrete.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose that I have two objects in my world, Ann and Bob and two predicates, friends and happy so happy scenery.",
                    "label": 0
                },
                {
                    "sent": "Bob could be happy or unhappy.",
                    "label": 0
                },
                {
                    "sent": "Anne, Anne, and Bob could be friends or not friends, and there's other groundings of these predicates.",
                    "label": 0
                },
                {
                    "sent": "But let us just consider these two.",
                    "label": 0
                },
                {
                    "sent": "OK, so with these two predicates in their groundings, there are only four possible worlds, right?",
                    "label": 0
                },
                {
                    "sent": "There's the world where N is not friends with Bob and Bob is not happy.",
                    "label": 0
                },
                {
                    "sent": "There is the world where there not friends, but he still happy and so forth.",
                    "label": 0
                },
                {
                    "sent": "And now if I give you a standard formula in logic, what this does?",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is make this?",
                    "label": 0
                },
                {
                    "sent": "For example, here's the formula, not friends and above or happy Bob.",
                    "label": 1
                },
                {
                    "sent": "Right, this is just the causal form for friends and above implies happy Bob I. I'm saying that if Anna is friends with Bob then Bob is happy.",
                    "label": 0
                },
                {
                    "sent": "And the effect of this is simply to make this world here impossible.",
                    "label": 0
                },
                {
                    "sent": "This is, this is the only world of all the four that violates this formula.",
                    "label": 0
                },
                {
                    "sent": "Because you know, friend above are friends with Bob is not happy.",
                    "label": 0
                },
                {
                    "sent": "So this world goes, you know, goes away is excluded.",
                    "label": 0
                },
                {
                    "sent": "And only these three possible worlds are left and logic.",
                    "label": 0
                },
                {
                    "sent": "If you think about it, is really just the calculus of what happens when you make some words impossible.",
                    "label": 0
                },
                {
                    "sent": "And then you want you want to know about the others.",
                    "label": 0
                },
                {
                    "sent": "Now what we want to do here of course, is to have statements like the probability that not friends and above are happy.",
                    "label": 0
                },
                {
                    "sent": "Bob is .8.",
                    "label": 0
                },
                {
                    "sent": "What does this do?",
                    "label": 0
                },
                {
                    "sent": "What this does is that this world now doesn't become impossible, is just becomes less probable than the other three worlds.",
                    "label": 0
                },
                {
                    "sent": "How much less probable?",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All is going to depend on this probability.",
                    "label": 0
                },
                {
                    "sent": "In particular, let's say that we do start with this knowledge base and we make them the maximum entropy assumption and we turn the crank.",
                    "label": 0
                },
                {
                    "sent": "We do the optimization what?",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the potentials that we get out right again following our convention that when a formula is true, its potential value is one we get that the potential for the formula is 1 and the potential for the negation is .75.",
                    "label": 0
                },
                {
                    "sent": "So each of these and this is only one formula.",
                    "label": 0
                },
                {
                    "sent": "This knowledge base.",
                    "label": 0
                },
                {
                    "sent": "So my unnormalized probability is just one in each of these three words in this .75 in this world, why's it .75 here?",
                    "label": 0
                },
                {
                    "sent": "Because remember I want the probability of this to be .8.",
                    "label": 0
                },
                {
                    "sent": "And 1 + 1 + 1 IE 3 / 3 + .7.",
                    "label": 1
                },
                {
                    "sent": "3 + .75 is 3 /, 3.75 is .8.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So basically this.",
                    "label": 0
                },
                {
                    "sent": "With the maximum entropy assumption is equivalent to this.",
                    "label": 0
                },
                {
                    "sent": "OK. And now, of course we can, instead of having things in potential form, we can put it in.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In weight form right all I have to do is from this.",
                    "label": 0
                },
                {
                    "sent": "From this potential there's only one.",
                    "label": 0
                },
                {
                    "sent": "In this case, compute the corresponding weight.",
                    "label": 0
                },
                {
                    "sent": "The weight is just going to be the log odds between the world where the where the formula is true in the world where the formula is false, so the weight of this formula is going to be 29.",
                    "label": 0
                },
                {
                    "sent": "So equivalent to the two things that we saw before, we could just say that we're going to have an MLN with this formula in it with the weight of .29 and all these things are saying exactly the same, OK?",
                    "label": 0
                },
                {
                    "sent": "By the way, if you have questions at any point, just let me know.",
                    "label": 0
                },
                {
                    "sent": "OK. And how about we take a 10 minute break now and restart it?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "20 and then we'll do the inference.",
                    "label": 0
                },
                {
                    "sent": "Learning applications in discussion.",
                    "label": 0
                }
            ]
        }
    }
}