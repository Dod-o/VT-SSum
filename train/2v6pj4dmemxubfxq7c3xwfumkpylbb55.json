{
    "id": "2v6pj4dmemxubfxq7c3xwfumkpylbb55",
    "title": "Spotlights 3",
    "info": {
        "published": "Dec. 3, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_spotlights1/",
    "segmentation": [
        [
            "What is the nature of mental representations?",
            "I explored this problem by comparing learning times across domains.",
            "If two domains are isomorphic but lead to different learning phenomena, this suggests that different representations might be involved in the two cases.",
            "In the poster tonight, I'll focus on the difference between domains two and three on the slide.",
            "Both domains include eight items, and both rely on exactly the same features, but I'll show some evidence that people think about these domains in rather different ways.",
            "Please come to post a 33 to see one theory about why these differences might arise.",
            "The second spotlight will be fast image deconvolution using Hyper Laplacian priors."
        ],
        [
            "The heavy tailed distributions of gradients in natural scenes have proven effective priors for range of problems, such as denoising, deblurring, and super resolution.",
            "These distributions are well modeled by a hyper Laplacian.",
            "In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use Hyper Laplacian priors.",
            "We adopt an alternating minimization scheme where one of the two phases is a nonconvex problem separable over pixels.",
            "This per pixel sub problem may be solved with a look up table.",
            "Our approach is able to deconvolve a 1 megapixel image in less than 3 seconds, achieving comperable quality to existing methods such as iterative, reweighted least squares that take approximately 20 minutes.",
            "Our method is quite general, an may be extended to related image processing problems beyond the deconvolution application demonstrated.",
            "Please visit our poster at W 19.",
            "OK, next one efficient large scale distributed training of conditional maximum entropy models."
        ],
        [
            "Training maximum entropy training.",
            "Maximum entropy models on large large amounts of data requires significant computational resources both time and also in some cases cost.",
            "Methods for using a distributed network or data center of typically had very large network costs in addition to the CPU costs.",
            "We're going to propose a network efficient method for training maximum maximum entropy models, which relies on partitioning the data into disjoint subsets.",
            "Training the model separately in all of the disjoint subsets, then constructing a weight mixture from these disjoint sets, we show that training in this way achieves not only significantly greater.",
            "Efficiencies and network usage and equal accuracy to traditional distributed gradient, but also has very appealing convergence bounds which make it comparable to exact distributed gradient.",
            "Next spotlight that driven approach to modeling choice."
        ],
        [
            "Suppose your retailer Ann you have N potential products to offer to customers.",
            "However, you have only a limited shelf capacity, let's say C. So the goal now is to determine the optimal optimal assortment of products to offer to customers in order to maximize the expected revenue.",
            "Such optimization typically requires a choice model.",
            "We propose to model customer choices follows each customer who's arriving has a preference list or ranking of all the products.",
            "Consciously or subconsciously in mind and then offered a particular assortment, the customer chooses to purchase the topmost or the most preferred of the available choices.",
            "Now, suppose the suppose the entire population can be divided into K customer types, where each customer type corresponds to one particular permutation of the preference list.",
            "Now the distribution which Maps to each permutation, the fraction of customers that belong to the corresponding customer type will determine the choice model completely.",
            "Now in this setup, usually it is very easy to collect partial data by offering assortments of a small size.",
            "So we have designed a lot of efficient algorithms to learn the underlying choice model when we have only partial data available.",
            "We also test our method using sales data, DVD sales data from Amazon.com.",
            "For more details, please visit our poster.",
            "The Nicer Spotlight statistical analysis of semi supervised learning the limit of infinite unlabeled data."
        ],
        [
            "So for semi supervised learning method for supervised learning method, you'd expect that as you have more and more data, things become better until they converge to some sensible solution that depends on the data distribution.",
            "And this is in fact the case for Laplacian graph based semi supervised learning in one dimension.",
            "But if you use or familiar with the graph based semi supervised learning which is by far the most popular semi supervised learning method is according to Google you might want to be aware that in dimensions higher than one.",
            "This is no longer true, and the limit the problem is imposed and we get a pretty flat and not informative solution.",
            "So I want to see Ryan also talk to us about maybe how to fix this, or whether this how this comes up in practice and will be happy to see the poster."
        ],
        [
            "Nessa Spotlight region based segmentation an object detection.",
            "So in this work we tackle the problem of for this 16 understanding Mystic scene understanding is a recently revived approach in computer vision that attempts to model multiple aspects of the scene at the same time.",
            "In our work.",
            "In particular we look at multi class image segmentation as well as object detection and we define a coherent probability model that models both models, pixels regions an objects or within the same framework.",
            "We show how standard state of the art object detectors can be adapted to be used within our framework.",
            "One important property of our framework is that we explain every single pixel in the scene, so this allows us to provide alternative explanations for pixels which improves precision for the object detection task and also avoids hacky post processing such as non maximal suppression.",
            "We show state of our results on the street scenes data set and if you want to hear more about this work, please come to post.",
            "21"
        ],
        [
            "Then I spotlight is anomaly detection with score functions based on nearest neighbors graph.",
            "In this work, we started falling anomaly detection problem given in monitoring data you want to clear with the test sample is anomalous or not.",
            "Our admin based on ranking scheme according to Kenya's neighbor distances.",
            "Roughly speaking, the rank of a test point X can be understood.",
            "His estimation after.",
            "Volume of small level, so continue acts which is shown in the bottom left and we did declare test samples normalized of the rank is small.",
            "The algorithm is isn't optimal in controlling for slam and minimize the misdirection rate.",
            "It is also other nice properties such as computation efficiency and adapts to the interdimensional data.",
            "The optimality property of our element can be illustrated in the right figure which shows the empirical distribution of the rankings.",
            "For anonymous data.",
            "The distribution curve is monotonic decreasing.",
            "On the other hand, for the normal trend for normal data.",
            "The district curve is syntactically uniform, therefore simple stretching scheme.",
            "We give us optimal performance.",
            "Our simulation without it also opt is also promising.",
            "The situation here is based on their benchmark bernadetta.",
            "We can see that our admin is performing better than the popular one class SVM.",
            "If interest in my talk, interesting work, please come to my poster at W. 57."
        ],
        [
            "Getting the next spotlight adaptive regularization of weight vectors.",
            "Hey so I'm Alex.",
            "Last year my coauthors Marchinko be introduced, confidence weighted learning and confidence weighted.",
            "Learning is an online learning technique where in addition to maintaining just the weight vector, the parameter vector, you also maintain a distribution over weight vectors, and this distribution is sort of suggested represent your confidence in those values, and it turns out that in situations where, for example, some features are very common and some are very rare.",
            "This kind of confidence information can speed convergence, an improve your performance.",
            "So this year we're introducing Arrow, which is sort of a new formulation for online learning with confidence.",
            "Arrow is just as easy to implement as CW, but it has actually vastly improved performance in the presence of label noise, and it also permits us to prove a mistake bound that doesn't assume separability.",
            "So we have some nice results on natural language, an OCR data and also our code is freely available.",
            "So if you're interested in learning more, please stop by W7 or just at the end of the Hall behind the elevators."
        ],
        [
            "So then there's a spotlight.",
            "Is adaptive regularization for transductive support vector machine.",
            "Totaling problems in similar learning do only Brother Unlimited Health and 2nd is West versus underlying assumption in similar learning.",
            "In this work we regard data as a regularization term for simple simple learning, take turns out USM an example.",
            "Yesterday I'm very no regularization term in conduct SVM.",
            "Horrible data are used as regularization term.",
            "So we ask question can we have adaptive controllable regularization term?",
            "So in this work we provide solution we try to learn, Dacian of supervised and partly regularised and fully supervised classifiers with our manifold smoothness.",
            "For more information please with our poster at W 5 to 8.",
            "Thank you."
        ],
        [
            "Spotlight is variational Gaussian process factor analysis for battling special temporal data.",
            "Hello, in this work we consider modeling of spatial temporal data.",
            "Which can be a measurements available taken in different special occasions at a different time instances, and we summarized the data in the form of a matrix and which you can have many missing values because the measurements can be taken irregularly, both in space and in time, and the goal one of the goals is to reconstruct the missing values in this matrix.",
            "And in our approach we model the data is a linear combination of the hidden factors, and each factor is characterized by the spatial pattern and.",
            "Temporal signal and we propose to use Gaussian process is to describe both the spatial patterns and the temporal signals of each hidden factor.",
            "In this way we can.",
            "Impose such properties of the hidden factors as temporal smoothness or spatial smoothness and we in our experiments we show that the model is able to capture the most significant spatial and temporal correlations from very irregular data and we show how to apply this model to perform historical reconstructions of sea surface temperatures.",
            "Welcome to the poster."
        ],
        [
            "Spotlight is predicting the optimal spacing of a study.",
            "A multi scale context model of memory.",
            "When students are required to learn fat, such as foreign language vocabulary.",
            "Slide oh, I have to do it.",
            "Cheesy conference when students are required to learn facts such as foreign language vocabulary, they often cram before an exam.",
            "Sadly, psychologists have known for a long time that to obtain durable memories, it's better to distribute your study over multiple well spaced sessions.",
            "There's a complex relationship between the spacing of study shown on the abscissa abscissa of these graphs and recall accuracy following a retention interval shown on the ordinate.",
            "Each graph corresponds to a different retention interval.",
            "The relationship depends on the individual learner, the materials being learned, the manners.",
            "Study in the retention interval.",
            "We've developed a cognitive model that predicts memory strength, overtime, the model parameters are constrained via data that's easy to collect.",
            "Specifically the time course of forgetting after a single study session, and then the model is able to predict data that's costly to collect.",
            "Recall following an arbitrary study schedule.",
            "Our current goal is to use the model to optimize human study for long-term retention.",
            "Two reasons to come to the poster, high ratio of pictures to equations and free gummy brains.",
            "As Bob."
        ],
        [
            "Type fast graph Laplacian regularizer, kernel learning via semidefinite quadratic linear programming.",
            "Hi good afternoon.",
            "Well down many problems in machine learning which can be formulated and scarf qualification regularizer concerning such as nonlinear dimensionality reduction and the constraint clustering.",
            "Well by doing so you really need to solve a convex quadratic some definite program in previous machine learning papers they solve this program by training a project turn into a.",
            "Leaving matches inequality constraint.",
            "While this this approach is very time consuming because the constraint is very large and it's not very easy to process the numerically.",
            "Actually this is not the best formulation we can hope for.",
            "It can be we we observe that it can be formulated as an instance of some definite quadratic linear program by simply turning the quadratic term into a second order constraint in the set of linear constraints.",
            "These constraints are much easier.",
            "These to be processed numerically.",
            "Therefore the getting speedup is very substantial.",
            "For example, 1M is falling.",
            "It takes a modern day for the previous approach to solve the problem, while in our formulation it only takes about 2 minutes.",
            "And so this is, our work is very simple.",
            "If you like it, please come to our poster Dublin 60.",
            "Thank you.",
            "An the last spot."
        ],
        [
            "Later, the session learning brain connectivity of Alzheimer disease from Norma Jean data.",
            "OK.",
            "In this paper we studied brain connectivity pattern for asthmatics is using sparse inverse covariance estimation.",
            "The recent studies have shown that.",
            "This is closely related to functional connectivity.",
            "Bring between different provisions.",
            "So in this study, three different subject subjects groups are studied, including 80 patients.",
            "Patients with mild cognitive impairment and low marking truth, so we can represent the connectivity as a matrix.",
            "Where the Paxil correspond to the connection between corresponding point raising by, well, the wetsel means that there's no flexing established.",
            "Yeah, this in the brain can be divided into several paths called lobe.",
            "We have also have also studied the connection between different nodes and the collection within each lobe.",
            "In particular, we have proved very interesting property of sparse inverse covariance estimation called monotone property.",
            "Then we can represent the connectivity as a tree structure using a sequence of parameter values.",
            "So we can get an order for the strings of taxing between different brain regions.",
            "I'll post ideas established Sunday, so welcome to our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is the nature of mental representations?",
                    "label": 0
                },
                {
                    "sent": "I explored this problem by comparing learning times across domains.",
                    "label": 0
                },
                {
                    "sent": "If two domains are isomorphic but lead to different learning phenomena, this suggests that different representations might be involved in the two cases.",
                    "label": 0
                },
                {
                    "sent": "In the poster tonight, I'll focus on the difference between domains two and three on the slide.",
                    "label": 0
                },
                {
                    "sent": "Both domains include eight items, and both rely on exactly the same features, but I'll show some evidence that people think about these domains in rather different ways.",
                    "label": 0
                },
                {
                    "sent": "Please come to post a 33 to see one theory about why these differences might arise.",
                    "label": 0
                },
                {
                    "sent": "The second spotlight will be fast image deconvolution using Hyper Laplacian priors.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The heavy tailed distributions of gradients in natural scenes have proven effective priors for range of problems, such as denoising, deblurring, and super resolution.",
                    "label": 0
                },
                {
                    "sent": "These distributions are well modeled by a hyper Laplacian.",
                    "label": 0
                },
                {
                    "sent": "In this paper we describe a deconvolution approach that is several orders of magnitude faster than existing techniques that use Hyper Laplacian priors.",
                    "label": 1
                },
                {
                    "sent": "We adopt an alternating minimization scheme where one of the two phases is a nonconvex problem separable over pixels.",
                    "label": 0
                },
                {
                    "sent": "This per pixel sub problem may be solved with a look up table.",
                    "label": 1
                },
                {
                    "sent": "Our approach is able to deconvolve a 1 megapixel image in less than 3 seconds, achieving comperable quality to existing methods such as iterative, reweighted least squares that take approximately 20 minutes.",
                    "label": 0
                },
                {
                    "sent": "Our method is quite general, an may be extended to related image processing problems beyond the deconvolution application demonstrated.",
                    "label": 0
                },
                {
                    "sent": "Please visit our poster at W 19.",
                    "label": 0
                },
                {
                    "sent": "OK, next one efficient large scale distributed training of conditional maximum entropy models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training maximum entropy training.",
                    "label": 0
                },
                {
                    "sent": "Maximum entropy models on large large amounts of data requires significant computational resources both time and also in some cases cost.",
                    "label": 0
                },
                {
                    "sent": "Methods for using a distributed network or data center of typically had very large network costs in addition to the CPU costs.",
                    "label": 0
                },
                {
                    "sent": "We're going to propose a network efficient method for training maximum maximum entropy models, which relies on partitioning the data into disjoint subsets.",
                    "label": 0
                },
                {
                    "sent": "Training the model separately in all of the disjoint subsets, then constructing a weight mixture from these disjoint sets, we show that training in this way achieves not only significantly greater.",
                    "label": 0
                },
                {
                    "sent": "Efficiencies and network usage and equal accuracy to traditional distributed gradient, but also has very appealing convergence bounds which make it comparable to exact distributed gradient.",
                    "label": 1
                },
                {
                    "sent": "Next spotlight that driven approach to modeling choice.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Suppose your retailer Ann you have N potential products to offer to customers.",
                    "label": 0
                },
                {
                    "sent": "However, you have only a limited shelf capacity, let's say C. So the goal now is to determine the optimal optimal assortment of products to offer to customers in order to maximize the expected revenue.",
                    "label": 0
                },
                {
                    "sent": "Such optimization typically requires a choice model.",
                    "label": 1
                },
                {
                    "sent": "We propose to model customer choices follows each customer who's arriving has a preference list or ranking of all the products.",
                    "label": 0
                },
                {
                    "sent": "Consciously or subconsciously in mind and then offered a particular assortment, the customer chooses to purchase the topmost or the most preferred of the available choices.",
                    "label": 0
                },
                {
                    "sent": "Now, suppose the suppose the entire population can be divided into K customer types, where each customer type corresponds to one particular permutation of the preference list.",
                    "label": 0
                },
                {
                    "sent": "Now the distribution which Maps to each permutation, the fraction of customers that belong to the corresponding customer type will determine the choice model completely.",
                    "label": 0
                },
                {
                    "sent": "Now in this setup, usually it is very easy to collect partial data by offering assortments of a small size.",
                    "label": 0
                },
                {
                    "sent": "So we have designed a lot of efficient algorithms to learn the underlying choice model when we have only partial data available.",
                    "label": 1
                },
                {
                    "sent": "We also test our method using sales data, DVD sales data from Amazon.com.",
                    "label": 0
                },
                {
                    "sent": "For more details, please visit our poster.",
                    "label": 0
                },
                {
                    "sent": "The Nicer Spotlight statistical analysis of semi supervised learning the limit of infinite unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for semi supervised learning method for supervised learning method, you'd expect that as you have more and more data, things become better until they converge to some sensible solution that depends on the data distribution.",
                    "label": 0
                },
                {
                    "sent": "And this is in fact the case for Laplacian graph based semi supervised learning in one dimension.",
                    "label": 0
                },
                {
                    "sent": "But if you use or familiar with the graph based semi supervised learning which is by far the most popular semi supervised learning method is according to Google you might want to be aware that in dimensions higher than one.",
                    "label": 0
                },
                {
                    "sent": "This is no longer true, and the limit the problem is imposed and we get a pretty flat and not informative solution.",
                    "label": 0
                },
                {
                    "sent": "So I want to see Ryan also talk to us about maybe how to fix this, or whether this how this comes up in practice and will be happy to see the poster.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nessa Spotlight region based segmentation an object detection.",
                    "label": 1
                },
                {
                    "sent": "So in this work we tackle the problem of for this 16 understanding Mystic scene understanding is a recently revived approach in computer vision that attempts to model multiple aspects of the scene at the same time.",
                    "label": 0
                },
                {
                    "sent": "In our work.",
                    "label": 0
                },
                {
                    "sent": "In particular we look at multi class image segmentation as well as object detection and we define a coherent probability model that models both models, pixels regions an objects or within the same framework.",
                    "label": 1
                },
                {
                    "sent": "We show how standard state of the art object detectors can be adapted to be used within our framework.",
                    "label": 0
                },
                {
                    "sent": "One important property of our framework is that we explain every single pixel in the scene, so this allows us to provide alternative explanations for pixels which improves precision for the object detection task and also avoids hacky post processing such as non maximal suppression.",
                    "label": 0
                },
                {
                    "sent": "We show state of our results on the street scenes data set and if you want to hear more about this work, please come to post.",
                    "label": 0
                },
                {
                    "sent": "21",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then I spotlight is anomaly detection with score functions based on nearest neighbors graph.",
                    "label": 0
                },
                {
                    "sent": "In this work, we started falling anomaly detection problem given in monitoring data you want to clear with the test sample is anomalous or not.",
                    "label": 1
                },
                {
                    "sent": "Our admin based on ranking scheme according to Kenya's neighbor distances.",
                    "label": 1
                },
                {
                    "sent": "Roughly speaking, the rank of a test point X can be understood.",
                    "label": 0
                },
                {
                    "sent": "His estimation after.",
                    "label": 0
                },
                {
                    "sent": "Volume of small level, so continue acts which is shown in the bottom left and we did declare test samples normalized of the rank is small.",
                    "label": 1
                },
                {
                    "sent": "The algorithm is isn't optimal in controlling for slam and minimize the misdirection rate.",
                    "label": 0
                },
                {
                    "sent": "It is also other nice properties such as computation efficiency and adapts to the interdimensional data.",
                    "label": 0
                },
                {
                    "sent": "The optimality property of our element can be illustrated in the right figure which shows the empirical distribution of the rankings.",
                    "label": 0
                },
                {
                    "sent": "For anonymous data.",
                    "label": 0
                },
                {
                    "sent": "The distribution curve is monotonic decreasing.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, for the normal trend for normal data.",
                    "label": 0
                },
                {
                    "sent": "The district curve is syntactically uniform, therefore simple stretching scheme.",
                    "label": 0
                },
                {
                    "sent": "We give us optimal performance.",
                    "label": 0
                },
                {
                    "sent": "Our simulation without it also opt is also promising.",
                    "label": 0
                },
                {
                    "sent": "The situation here is based on their benchmark bernadetta.",
                    "label": 0
                },
                {
                    "sent": "We can see that our admin is performing better than the popular one class SVM.",
                    "label": 0
                },
                {
                    "sent": "If interest in my talk, interesting work, please come to my poster at W. 57.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Getting the next spotlight adaptive regularization of weight vectors.",
                    "label": 1
                },
                {
                    "sent": "Hey so I'm Alex.",
                    "label": 1
                },
                {
                    "sent": "Last year my coauthors Marchinko be introduced, confidence weighted learning and confidence weighted.",
                    "label": 0
                },
                {
                    "sent": "Learning is an online learning technique where in addition to maintaining just the weight vector, the parameter vector, you also maintain a distribution over weight vectors, and this distribution is sort of suggested represent your confidence in those values, and it turns out that in situations where, for example, some features are very common and some are very rare.",
                    "label": 0
                },
                {
                    "sent": "This kind of confidence information can speed convergence, an improve your performance.",
                    "label": 1
                },
                {
                    "sent": "So this year we're introducing Arrow, which is sort of a new formulation for online learning with confidence.",
                    "label": 0
                },
                {
                    "sent": "Arrow is just as easy to implement as CW, but it has actually vastly improved performance in the presence of label noise, and it also permits us to prove a mistake bound that doesn't assume separability.",
                    "label": 0
                },
                {
                    "sent": "So we have some nice results on natural language, an OCR data and also our code is freely available.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in learning more, please stop by W7 or just at the end of the Hall behind the elevators.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then there's a spotlight.",
                    "label": 0
                },
                {
                    "sent": "Is adaptive regularization for transductive support vector machine.",
                    "label": 1
                },
                {
                    "sent": "Totaling problems in similar learning do only Brother Unlimited Health and 2nd is West versus underlying assumption in similar learning.",
                    "label": 0
                },
                {
                    "sent": "In this work we regard data as a regularization term for simple simple learning, take turns out USM an example.",
                    "label": 1
                },
                {
                    "sent": "Yesterday I'm very no regularization term in conduct SVM.",
                    "label": 1
                },
                {
                    "sent": "Horrible data are used as regularization term.",
                    "label": 1
                },
                {
                    "sent": "So we ask question can we have adaptive controllable regularization term?",
                    "label": 0
                },
                {
                    "sent": "So in this work we provide solution we try to learn, Dacian of supervised and partly regularised and fully supervised classifiers with our manifold smoothness.",
                    "label": 0
                },
                {
                    "sent": "For more information please with our poster at W 5 to 8.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spotlight is variational Gaussian process factor analysis for battling special temporal data.",
                    "label": 1
                },
                {
                    "sent": "Hello, in this work we consider modeling of spatial temporal data.",
                    "label": 0
                },
                {
                    "sent": "Which can be a measurements available taken in different special occasions at a different time instances, and we summarized the data in the form of a matrix and which you can have many missing values because the measurements can be taken irregularly, both in space and in time, and the goal one of the goals is to reconstruct the missing values in this matrix.",
                    "label": 0
                },
                {
                    "sent": "And in our approach we model the data is a linear combination of the hidden factors, and each factor is characterized by the spatial pattern and.",
                    "label": 0
                },
                {
                    "sent": "Temporal signal and we propose to use Gaussian process is to describe both the spatial patterns and the temporal signals of each hidden factor.",
                    "label": 0
                },
                {
                    "sent": "In this way we can.",
                    "label": 0
                },
                {
                    "sent": "Impose such properties of the hidden factors as temporal smoothness or spatial smoothness and we in our experiments we show that the model is able to capture the most significant spatial and temporal correlations from very irregular data and we show how to apply this model to perform historical reconstructions of sea surface temperatures.",
                    "label": 1
                },
                {
                    "sent": "Welcome to the poster.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spotlight is predicting the optimal spacing of a study.",
                    "label": 1
                },
                {
                    "sent": "A multi scale context model of memory.",
                    "label": 0
                },
                {
                    "sent": "When students are required to learn fat, such as foreign language vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Slide oh, I have to do it.",
                    "label": 0
                },
                {
                    "sent": "Cheesy conference when students are required to learn facts such as foreign language vocabulary, they often cram before an exam.",
                    "label": 0
                },
                {
                    "sent": "Sadly, psychologists have known for a long time that to obtain durable memories, it's better to distribute your study over multiple well spaced sessions.",
                    "label": 0
                },
                {
                    "sent": "There's a complex relationship between the spacing of study shown on the abscissa abscissa of these graphs and recall accuracy following a retention interval shown on the ordinate.",
                    "label": 0
                },
                {
                    "sent": "Each graph corresponds to a different retention interval.",
                    "label": 0
                },
                {
                    "sent": "The relationship depends on the individual learner, the materials being learned, the manners.",
                    "label": 0
                },
                {
                    "sent": "Study in the retention interval.",
                    "label": 0
                },
                {
                    "sent": "We've developed a cognitive model that predicts memory strength, overtime, the model parameters are constrained via data that's easy to collect.",
                    "label": 0
                },
                {
                    "sent": "Specifically the time course of forgetting after a single study session, and then the model is able to predict data that's costly to collect.",
                    "label": 0
                },
                {
                    "sent": "Recall following an arbitrary study schedule.",
                    "label": 0
                },
                {
                    "sent": "Our current goal is to use the model to optimize human study for long-term retention.",
                    "label": 0
                },
                {
                    "sent": "Two reasons to come to the poster, high ratio of pictures to equations and free gummy brains.",
                    "label": 0
                },
                {
                    "sent": "As Bob.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Type fast graph Laplacian regularizer, kernel learning via semidefinite quadratic linear programming.",
                    "label": 1
                },
                {
                    "sent": "Hi good afternoon.",
                    "label": 0
                },
                {
                    "sent": "Well down many problems in machine learning which can be formulated and scarf qualification regularizer concerning such as nonlinear dimensionality reduction and the constraint clustering.",
                    "label": 0
                },
                {
                    "sent": "Well by doing so you really need to solve a convex quadratic some definite program in previous machine learning papers they solve this program by training a project turn into a.",
                    "label": 0
                },
                {
                    "sent": "Leaving matches inequality constraint.",
                    "label": 0
                },
                {
                    "sent": "While this this approach is very time consuming because the constraint is very large and it's not very easy to process the numerically.",
                    "label": 0
                },
                {
                    "sent": "Actually this is not the best formulation we can hope for.",
                    "label": 0
                },
                {
                    "sent": "It can be we we observe that it can be formulated as an instance of some definite quadratic linear program by simply turning the quadratic term into a second order constraint in the set of linear constraints.",
                    "label": 1
                },
                {
                    "sent": "These constraints are much easier.",
                    "label": 0
                },
                {
                    "sent": "These to be processed numerically.",
                    "label": 0
                },
                {
                    "sent": "Therefore the getting speedup is very substantial.",
                    "label": 0
                },
                {
                    "sent": "For example, 1M is falling.",
                    "label": 0
                },
                {
                    "sent": "It takes a modern day for the previous approach to solve the problem, while in our formulation it only takes about 2 minutes.",
                    "label": 0
                },
                {
                    "sent": "And so this is, our work is very simple.",
                    "label": 0
                },
                {
                    "sent": "If you like it, please come to our poster Dublin 60.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "An the last spot.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Later, the session learning brain connectivity of Alzheimer disease from Norma Jean data.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 1
                },
                {
                    "sent": "In this paper we studied brain connectivity pattern for asthmatics is using sparse inverse covariance estimation.",
                    "label": 0
                },
                {
                    "sent": "The recent studies have shown that.",
                    "label": 0
                },
                {
                    "sent": "This is closely related to functional connectivity.",
                    "label": 0
                },
                {
                    "sent": "Bring between different provisions.",
                    "label": 0
                },
                {
                    "sent": "So in this study, three different subject subjects groups are studied, including 80 patients.",
                    "label": 0
                },
                {
                    "sent": "Patients with mild cognitive impairment and low marking truth, so we can represent the connectivity as a matrix.",
                    "label": 0
                },
                {
                    "sent": "Where the Paxil correspond to the connection between corresponding point raising by, well, the wetsel means that there's no flexing established.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this in the brain can be divided into several paths called lobe.",
                    "label": 0
                },
                {
                    "sent": "We have also have also studied the connection between different nodes and the collection within each lobe.",
                    "label": 0
                },
                {
                    "sent": "In particular, we have proved very interesting property of sparse inverse covariance estimation called monotone property.",
                    "label": 0
                },
                {
                    "sent": "Then we can represent the connectivity as a tree structure using a sequence of parameter values.",
                    "label": 0
                },
                {
                    "sent": "So we can get an order for the strings of taxing between different brain regions.",
                    "label": 0
                },
                {
                    "sent": "I'll post ideas established Sunday, so welcome to our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}