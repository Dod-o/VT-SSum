{
    "id": "eoqzfl3zgo2vjtnmlbwl7sowopd4ultz",
    "title": "Regularized Linear Regression: A Precise Analysis of the Estimation Error",
    "info": {
        "author": [
            "Christos Thrampoulidis, California Institute of Technology (Caltech)"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_thrampoulidis_estimation_error/",
    "segmentation": [
        [
            "So my name is Christos and this is joint work with Summit and marking my advisor, Professor Bakasi, be done at Caltech.",
            "So the title is regularised linear regression, a precise analysis of the estimation error so?"
        ],
        [
            "Pretty standard setting.",
            "We want to estimate an unknown vector X, not from noisy linear observations, so we get to see y = a X, not 8 * 6, Not plaisier is the measure matrix is the noise vector, and so I, uh, white class of algorithms which would give such an estimate is described by this convex minimization algorithm which tries to minimize the loss function of the residual Wyman.",
            "So X + a regularizer so.",
            "L The loss function which we assume it's a convex function.",
            "It could be some norm or the Huber loss or more fun stuff.",
            "The regularizer function is typically associated with with information we have on the on the structure of.",
            "It's not, so if it's a sparse vector we might want to use the L1 norm for a for a low rank matrix we might want to use nuclear norm and so on.",
            "So we call this the regression optimization algorithm and.",
            "It includes popular instances, right?",
            "Like the last so group lasso, square root law, so least absolute deviations may be the noisy sparse we want to use the L1 norm as the loss function, and so fundamental question is."
        ],
        [
            "How does this class of element performs right?",
            "And and we're measuring performance in terms of the L2 norm of the of the error, ex at minus X squared.",
            "OK, so so this class of algorithms have been out for like more than 20 years.",
            "They are very popular and of course this question has been.",
            "Answered in the literature in the past, but quite surprisingly, even even for the case where the the Matrix A is Gaussian has interest.",
            "I'd be Gaussian, which is the most simple assumption one could start with the results that have been out have been mostly order wise in nature, OK and so."
        ],
        [
            "This work we we.",
            "We propose a framework under the assumption that the that the measurement matrix says I ate the standard normal entry sent in the high dimensional regime where the problem dimensions both grow to Infinity in a proportional manner.",
            "We propose a framework for the precise characterization of the error.",
            "So this is a recipe, and it's generally in the sense that in principle it can be applied for any convex loss function, and any convex regularizer, and this precise in the sense that.",
            "It does not include.",
            "It gives you precise results, no unknown constants and in comparison to some too.",
            "He sent the work on precise results on the law, so it's through the AMP framework.",
            "The approximate message passing framework.",
            "It's it's.",
            "It's somehow a more natural approach and more general and so."
        ],
        [
            "What is the main tool so sewer main I'm our main tool is what we call the convex Gaussian Min Max theorem.",
            "So for those of you have heard of the Gaussian Min Max theorem, it's a result proved by Gordon in 1988, so it's a classical result.",
            "It's a, it's a result, it's a Gaussian comparison inequality and its classical use is on proving high probability lower bound on the minimum singular value of a Gaussian iid matrix.",
            "And So what we saw here is that.",
            "When you combine these results with additional convexity assumptions, it's tight.",
            "So what this what this theorem does is it takes a problem primary optimization problem, which is a min Max problem and it's hard to analyze.",
            "It writes an auxiliary optimization which is simpler, and it says that you can do the analysis for the optimization instead.",
            "OK, and so no time to show the precise formulation of the theory but the pics."
        ],
        [
            "Here is like this.",
            "So we start with the regression optimization.",
            "We write the loss function in using the Fenchel conjugate so that we get we get it in the form of the primary optimization that Gordon steering required.",
            "OK, so we get a min Max problem and then we apply our main theorem.",
            "The convex, Gaussian, Min, Max theorem.",
            "Everything is convex in our case since our assumptions, we get an auxiliary optimization for different instantiations of the loss function and regularizer function.",
            "Dogzilla optimization is different and there's a problem specific analysis.",
            "Which gives you the prediction for the error.",
            "So this is the framework.",
            "This is a recipe in principle.",
            "Again, it's general in the sense that it just requires convexity of the loss function in the regularizer."
        ],
        [
            "And just to to close with an example of proof by picture that results are precise, here's an example where we have an unknown matrix X note of dimension 45 * 45.",
            "It's low rank.",
            "We are using the square root lasso with the nuclear norm and you can see that the prediction is on top of the simulations and also this framework in cases where we know the rank.",
            "For example, we can predict that we can say things about the optimal tuning.",
            "And I lost it."
        ],
        [
            "Sample when using the least absolute deviations and many more in the poster, it's still out there.",
            "I'm happy to talk to you more about it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So my name is Christos and this is joint work with Summit and marking my advisor, Professor Bakasi, be done at Caltech.",
                    "label": 0
                },
                {
                    "sent": "So the title is regularised linear regression, a precise analysis of the estimation error so?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Pretty standard setting.",
                    "label": 0
                },
                {
                    "sent": "We want to estimate an unknown vector X, not from noisy linear observations, so we get to see y = a X, not 8 * 6, Not plaisier is the measure matrix is the noise vector, and so I, uh, white class of algorithms which would give such an estimate is described by this convex minimization algorithm which tries to minimize the loss function of the residual Wyman.",
                    "label": 0
                },
                {
                    "sent": "So X + a regularizer so.",
                    "label": 0
                },
                {
                    "sent": "L The loss function which we assume it's a convex function.",
                    "label": 0
                },
                {
                    "sent": "It could be some norm or the Huber loss or more fun stuff.",
                    "label": 0
                },
                {
                    "sent": "The regularizer function is typically associated with with information we have on the on the structure of.",
                    "label": 0
                },
                {
                    "sent": "It's not, so if it's a sparse vector we might want to use the L1 norm for a for a low rank matrix we might want to use nuclear norm and so on.",
                    "label": 0
                },
                {
                    "sent": "So we call this the regression optimization algorithm and.",
                    "label": 0
                },
                {
                    "sent": "It includes popular instances, right?",
                    "label": 1
                },
                {
                    "sent": "Like the last so group lasso, square root law, so least absolute deviations may be the noisy sparse we want to use the L1 norm as the loss function, and so fundamental question is.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How does this class of element performs right?",
                    "label": 0
                },
                {
                    "sent": "And and we're measuring performance in terms of the L2 norm of the of the error, ex at minus X squared.",
                    "label": 0
                },
                {
                    "sent": "OK, so so this class of algorithms have been out for like more than 20 years.",
                    "label": 0
                },
                {
                    "sent": "They are very popular and of course this question has been.",
                    "label": 0
                },
                {
                    "sent": "Answered in the literature in the past, but quite surprisingly, even even for the case where the the Matrix A is Gaussian has interest.",
                    "label": 0
                },
                {
                    "sent": "I'd be Gaussian, which is the most simple assumption one could start with the results that have been out have been mostly order wise in nature, OK and so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This work we we.",
                    "label": 0
                },
                {
                    "sent": "We propose a framework under the assumption that the that the measurement matrix says I ate the standard normal entry sent in the high dimensional regime where the problem dimensions both grow to Infinity in a proportional manner.",
                    "label": 0
                },
                {
                    "sent": "We propose a framework for the precise characterization of the error.",
                    "label": 1
                },
                {
                    "sent": "So this is a recipe, and it's generally in the sense that in principle it can be applied for any convex loss function, and any convex regularizer, and this precise in the sense that.",
                    "label": 0
                },
                {
                    "sent": "It does not include.",
                    "label": 0
                },
                {
                    "sent": "It gives you precise results, no unknown constants and in comparison to some too.",
                    "label": 0
                },
                {
                    "sent": "He sent the work on precise results on the law, so it's through the AMP framework.",
                    "label": 0
                },
                {
                    "sent": "The approximate message passing framework.",
                    "label": 0
                },
                {
                    "sent": "It's it's.",
                    "label": 0
                },
                {
                    "sent": "It's somehow a more natural approach and more general and so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is the main tool so sewer main I'm our main tool is what we call the convex Gaussian Min Max theorem.",
                    "label": 0
                },
                {
                    "sent": "So for those of you have heard of the Gaussian Min Max theorem, it's a result proved by Gordon in 1988, so it's a classical result.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's a result, it's a Gaussian comparison inequality and its classical use is on proving high probability lower bound on the minimum singular value of a Gaussian iid matrix.",
                    "label": 1
                },
                {
                    "sent": "And So what we saw here is that.",
                    "label": 0
                },
                {
                    "sent": "When you combine these results with additional convexity assumptions, it's tight.",
                    "label": 0
                },
                {
                    "sent": "So what this what this theorem does is it takes a problem primary optimization problem, which is a min Max problem and it's hard to analyze.",
                    "label": 0
                },
                {
                    "sent": "It writes an auxiliary optimization which is simpler, and it says that you can do the analysis for the optimization instead.",
                    "label": 0
                },
                {
                    "sent": "OK, and so no time to show the precise formulation of the theory but the pics.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is like this.",
                    "label": 0
                },
                {
                    "sent": "So we start with the regression optimization.",
                    "label": 1
                },
                {
                    "sent": "We write the loss function in using the Fenchel conjugate so that we get we get it in the form of the primary optimization that Gordon steering required.",
                    "label": 1
                },
                {
                    "sent": "OK, so we get a min Max problem and then we apply our main theorem.",
                    "label": 0
                },
                {
                    "sent": "The convex, Gaussian, Min, Max theorem.",
                    "label": 0
                },
                {
                    "sent": "Everything is convex in our case since our assumptions, we get an auxiliary optimization for different instantiations of the loss function and regularizer function.",
                    "label": 1
                },
                {
                    "sent": "Dogzilla optimization is different and there's a problem specific analysis.",
                    "label": 0
                },
                {
                    "sent": "Which gives you the prediction for the error.",
                    "label": 0
                },
                {
                    "sent": "So this is the framework.",
                    "label": 0
                },
                {
                    "sent": "This is a recipe in principle.",
                    "label": 0
                },
                {
                    "sent": "Again, it's general in the sense that it just requires convexity of the loss function in the regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just to to close with an example of proof by picture that results are precise, here's an example where we have an unknown matrix X note of dimension 45 * 45.",
                    "label": 0
                },
                {
                    "sent": "It's low rank.",
                    "label": 0
                },
                {
                    "sent": "We are using the square root lasso with the nuclear norm and you can see that the prediction is on top of the simulations and also this framework in cases where we know the rank.",
                    "label": 0
                },
                {
                    "sent": "For example, we can predict that we can say things about the optimal tuning.",
                    "label": 0
                },
                {
                    "sent": "And I lost it.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample when using the least absolute deviations and many more in the poster, it's still out there.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to talk to you more about it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}