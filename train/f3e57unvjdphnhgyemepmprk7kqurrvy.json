{
    "id": "f3e57unvjdphnhgyemepmprk7kqurrvy",
    "title": "Merck Molecular Activity Challenge",
    "info": {
        "author": [
            "George E. Dahl, Department of Computer Science, University of Toronto"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Bioinformatics"
        ]
    },
    "url": "http://videolectures.net/nips2012_dahl_activity/",
    "segmentation": [
        [
            "So I'm going to be talking about the Merck Molecular Activity Challenge and our team solution to it, which made heavy use of dropout.",
            "The team is listed here.",
            "I collaborate with these wonderful people.",
            "And they were very."
        ],
        [
            "Awful, so one way to make new drugs is to identify a medically interesting protein.",
            "Measure the activity of many many candidate molecules towards that target protein in the wet lab or something and then work really hard with medical chemists to make drugs from any highly active molecules.",
            "But for this to work, we need activity measurements.",
            "For millions, possibly of candidate molecules, for each target, I mean we can their commercial libraries where you can buy millions of different molecules, and so we'd at least like to check the ones that we can buy instead of having to synthesize.",
            "And we need a way to predict the activity from chemical structure without running assays."
        ],
        [
            "So Merck spends billions of dollars developing new drugs, and so they set up a competition where they take one tiny, tiny little piece of their drug discovery pipeline, and they sort of make a little competition about it and what they did is they released 15.",
            "Related data sets, or I sometimes call them tasks altogether.",
            "Where you predict molecular activity.",
            "Each task is a regression problem where you predict the activity of molecules towards one particular target.",
            "So one task, one data set, one target.",
            "You're only given the chemical structure feature vectors for candidate molecules and not really any information about the targets.",
            "And the molecules and input features sometimes occur in multiple tasks.",
            "So you're given global ID's for the inputs and they get repeated across different tasks.",
            "And also the activity labels have different unknown units in these different tasks."
        ],
        [
            "And so let me just dive right into the data that they actually released.",
            "They have input features that are non negative integers.",
            "Presumably there counts of some sort of substructure, but we're not told really anything about them, and about 3 to 10% are non zero and the number of features in the data set ranges from about 4000 to 9000 or so.",
            "And if you union all the different unique features across the 15 tasks.",
            "You'll get about 11,000 unique features.",
            "The number of training cases for a given tasks between 18137 thousand.",
            "So in many of the tasks you have more input dimensions.",
            "Then you have training cases for that task.",
            "So normally we expect very large neural networks to overfit in these conditions and not give good results even if you do careful early stopping."
        ],
        [
            "And so the scoring metric for.",
            "The competition was just an average of a correlation with the true activity for each task, so it's an equally weighted average over the 15 tasks of essentially the percentage of variance you explain where you have the true activity and your predicted activity, and then the means of the truth in the predictions.",
            "And so that.",
            "This on a per case basis you'd much rather get the cases from the small datasets correct, since the average doesn't take into account the variable sizes of the datasets.",
            "And the training set contains molecules that were assayed before some certain date and then the test set everything after that, and so they actually have very different distributions between the training set and the test set.",
            "So it's very easy to sort of overfit the training set.",
            "Because, but really, it's it's what can you really do since they have different distributions.",
            "And so."
        ],
        [
            "Our solution.",
            "Like almost all solutions to these sorts of competitions ultimately involved in sambol and we just did very, very simple in sampling, and I'm not going to really go into, we just died.",
            "Completely even averages of the predictions of a couple of models for each data set.",
            "And we picked those individually on a per data set basis and someone are in sambol included neural Nets of various types that will go into Gaussian process regression models and even some decision trees.",
            "However, the neural Nets trained with dropout were the only completely indispensable part of our solution and drop out is the technique that let us.",
            "Train these Nets in an effective way on this."
        ],
        [
            "Data.",
            "So the first sort of neural net we used was the most basic simple neural net.",
            "It ignores the potential relationship between these different tasks, and justice is for a single task.",
            "Ignore all the other tasks.",
            "It's a regression problem, just training her on that we use one or two hidden layers.",
            "Different sorts of nonlinearities.",
            "And for the small datasets especially, and even for the moderately larger ones, dropout was essential to avoid overfitting.",
            "And an example of that is without dropout, I couldn't train really a neural net with more than 30 hidden units.",
            "In a single hidden layer, affectively.",
            "Without sort of disastrous overfitting even when I tried to do early stopping but with dropout I could train neural Nets with 500,000 or even more units and just get better results.",
            "So these are the simplest neural Nets that we used."
        ],
        [
            "So we also used multi task neural Nets and so basically there would be one output unit.",
            "I only shown 5 here for each of the tasks.",
            "And so we only observe one of these at a time for a particular training case.",
            "And we also have one input for each unique input across all the tasks.",
            "So we have 11,000 inputs, whereas a particular task might only use 4000 or 5000 of those.",
            "And since we don't really know the units of the outputs, I standardize them to have mean zero unit variance.",
            "Probably we could have reverse engineered exactly what Wet Lab experiment they did to get those numbers and done something even smarter, but that would have required actual knowledge about biochemistry.",
            "Although some people in my team had that knowledge, I certainly did not.",
            "And we trained on mini batches of cases, so we trained these Nets with cast agreeing to send over small mini batches where each mini batch was guaranteed to include an equal number of cases from each of the 15 tasks.",
            "So by doing that we optimize what we hope is something close to the evaluation metric, since if you just lump all the data together.",
            "You're you'll have a neural network that focuses on the largest of the datasets, 'cause it sees the most training data from that.",
            "OK, so."
        ],
        [
            "Basically, to go over exactly how training would work, I've highlighted.",
            "So suppose that one training case we have, we have this activity observed, and then we observe these two inputs and we don't observe.",
            "Are these two so the highlighted pink weights are the ones that will get updated when we train on this case and we just don't update the others?",
            "And in this way we will train sort of slices or columns of this network repeatedly."
        ],
        [
            "So how will this work on the competition?",
            "Well, once we made our best ensemble that we could make with our equally weighted averages of a handful of models, including even some gradient boosted decision trees, we got.",
            "Of course the 1st place result and the validation scores I'm reporting here, where the public leaderboard.",
            "So during the competition.",
            "You could make some missions and get 25% of the test data.",
            "It would always be the same 25% for everyone, but that gives you an incentive to show your progress during the competition.",
            "Since you get bits of information about the test distribution and so the prizes not awarded based on this because we don't want people mining the leaderboard and then exploiting it, and you have a limited number of submissions.",
            "So you can't always do all the careful experiments you'd like to do.",
            "If you were doing a paper or something, but you can sort of a BLT, different models in your ensembles and tweak your.",
            "Your models.",
            "To improve on the leaderboard.",
            "And that's what I'm referring to is validation here, so these are squared numbers show that are best ensemble.",
            "Um?",
            "Got got these numbers and they were very close on the test set which was good.",
            "We were very worried about overfitting the leaderboard data so we only did a very few number of submissions compared to any of the other top teams.",
            "And if you remove the trees, you get slightly worse results 'cause they, although they're not very good individually, they produce radically different predictions than some of our other models, so they're very good to combine with them.",
            "And if you only use neural networks, did know.",
            "Like since ambling just use single task neural Nets and multi task neural Nets trained with dropout and picked the whatever neural network best on each of the 15 datasets and built a submission out of that, we would have still won first place.",
            "And that wouldn't have done any serious model averaging.",
            "Ann just would have used dropout and neural Nets of various sorts.",
            "And so this was a very exciting result since.",
            "Normally the sort of neural Nets that we use at this scale with, like you know, 5000 units in a layer or something in a couple of different layers on just 1000 training cases would just overfit hopelessly.",
            "But also we see from this is that if we just use the Gaussian process regression, we've gotten about 10th place and if we used whatever our internal cross validation said was the best on each individual data set, so would either be a GP or a neural net.",
            "Typically we would have gotten 7th place, which shows that our internal cross validation slightly overestimated the performance of the GPS, which is a little unfortunate.",
            "But when we have a very limited budget of submissions, it's hard to tease out these things during the actual competition.",
            "And of course, since the gains here.",
            "Are you know substantial over the just using the neural Nets?",
            "Even the simple averaging we used is quite helpful."
        ],
        [
            "So.",
            "A few conclusions from this dropout lets us train larger neural Nets on more inputs with fewer training cases, which is wonderful for these sorts of mid sized data sets and even small ones, and it helps a lot when there are more input dimensions than training cases.",
            "Which is the case here, and Furthermore with the multi task neural Nets.",
            "I think doing dropout in the input also helped make it robust to only observing some of the possible inputs.",
            "So that's a sort of ancillary benefit and the single and multi task neural Nets with dropout.",
            "Suffice to get first place in the competition.",
            "And that's all I have to say.",
            "So if there's any questions, please ask.",
            "Thank you, thank you very much.",
            "So if you had used dropout for the Netflix competition, would you have one first place?",
            "Well, we probably have to ask my team a trust that since he participated in the competition, but I have honestly no idea.",
            "You go back and try.",
            "It probably have the data right?",
            "Yeah.",
            "Any other questions?",
            "So would you please use the speaker microphone?",
            "Public microphone, anybody else who has questions?",
            "Would you mind lining up at the microphones right?",
            "So supposedly Mark is very interested in how you did this.",
            "What can you tell them about features or?",
            "Like how can you unfold the black box that is drop out?",
            "OK, well we didn't do any feature engineering that wasn't done by Merck before us, so we just use their features.",
            "We actually, you know, sometimes we took the log of them, but that's it, and so they know what the features are.",
            "We have no idea what they are, so their chemists can figure out what that means.",
            "But as far as the neural Nets themselves go.",
            "Um, if you want to see, sort of why it predicted.",
            "Some particular activity you can sort of backtrack through the network and look at the sensitivity to various input features, but since there are a lot of them, I'm not not sure how productive that would be.",
            "You could also visualize.",
            "The different types of molecules with T sne.",
            "So Lawrence Vondra Martin made a bunch of very informative TC plots for this competition in a related visualization competition, and you can see sort of clusters of similar molecules because a lot of these molecules are made by finding something that looks sort of promising and then sort of Hill climbing around it with lots of chemistry knowledge.",
            "How did the company react?",
            "Because they are squares are quite low.",
            "I mean, although their first place but like they don't really explain lot of difference.",
            "How did the company react to that?",
            "Well I think they were very pleased because they had an internal benchmark as well and their internal benchmark was also on the leaderboard and it was well below the 1st place entry.",
            "But in terms of whether this kind of intervention is can speed up their pipelines?",
            "I mean basically the benchmark.",
            "But no, I'm not an expert on drug discovery, but the people I've talked with Merck.",
            "I suspect they will actually use it in their drug discovery pipeline, and they seem very excited about it.",
            "Wow, thanks.",
            "I have two questions.",
            "What kind of covariance functions did you use for your GPS and you put a lot of effort in optimizing them is my first question, and then the second one is what intuitions you draw from the fact that actually doing equal model averaging was better than neural Nets with power.",
            "So the GPS we tried squared exponential neural net covariance functions, rational quadratic covariance functions, and probably some attorn ones as well.",
            "Russ trained the GPS, so he might be able to give more insight on that and they were the hyper parameters of the covariance functions for the GPS, where sort of optimized on the marginal likelihood.",
            "And so.",
            "I believe Russ tried his best to make good GPS and.",
            "I I didn't do that part of it so and they did work quite well, especially on some of the datasets.",
            "So on some datasets they work better than others and they were very helpful in our average.",
            "As far as your second question, I'm sorry that was.",
            "I got model averaging yes, so I think I was a little surprised that the we couldn't get a benefit from doing more sophisticated model averaging.",
            "We did explore it a little bit, but to really do that carefully you need a lot of leaderboard submissions and we were very concerned about overfitting the leaderboard public data.",
            "And since the validation the cross validation.",
            "On the training set, it's from a different distribution than the test set.",
            "We were very worried about tuning it too much there, so we were able to get benefits on the training set with more specific model averaging, but which probably would not surprise anyone.",
            "So I think really if there's more data, or if we had more freedom to sort of poke around with it, maybe there would be a benefit.",
            "OK, next question.",
            "So the dropout rate is 1/2.",
            "Don't you think that this could be related to the imbalance between the number of cases and the number of features?",
            "Um, so actually not all of our Nets used .5 for the dropout rate in this.",
            "So we tried 25% and 50% and so I don't think there's a direct relationship between the number of.",
            "The number of input features and the best dropout rate.",
            "I think it depends in a relatively complicated way on the size and capacity of the neural Nets that you train and how long you train them.",
            "OK, last question over there yeah so you talked about instead of taking lots of samples to average together models at Test time, just multiplying all the weights by 1/2.",
            "I'm wondering what happens with the performance if instead we just take one sample and use that to predict, does that do much worse than doing the dividing the weights by half thing?",
            "I haven't tried that explicitly, but from my experiences with with the training error that's measured with the stochastic version, it would probably be worse.",
            "And it wouldn't really provide any advantage that I can see.",
            "If you did that because it wouldn't really be that much faster.",
            "Right, well, but if you're like if you only care about, say, binary predictions, than a single sample could actually give you a reasonably good estimate of the probability.",
            "Maybe why not just use all the neurons?",
            "I guess alright OK, thanks.",
            "I've got a quick last question.",
            "Maybe Sammy could summary could already set up his laptop for the next talk so you have quite a bit of covariate shift between your training and your test set, right?",
            "And did you do do anything about that?",
            "Where basically did you take that into account to maybe adapt to different distributions?",
            "Because obviously if they were temporally aligned, they would have started with the more obvious molecules first employee learn something in the process that there would have been some nontrivial temporal dynamics in it.",
            "Did you exploit that?",
            "So although it was not against the rules?",
            "The organizer of the competition expressed as very strong preference that we not use the distribution of test inputs in our solution, so we decided to respect that.",
            "Although other teams claimed to get a large benefit from it, OK?",
            "OK, so then thank you very much.",
            "Again, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to be talking about the Merck Molecular Activity Challenge and our team solution to it, which made heavy use of dropout.",
                    "label": 1
                },
                {
                    "sent": "The team is listed here.",
                    "label": 0
                },
                {
                    "sent": "I collaborate with these wonderful people.",
                    "label": 0
                },
                {
                    "sent": "And they were very.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Awful, so one way to make new drugs is to identify a medically interesting protein.",
                    "label": 1
                },
                {
                    "sent": "Measure the activity of many many candidate molecules towards that target protein in the wet lab or something and then work really hard with medical chemists to make drugs from any highly active molecules.",
                    "label": 1
                },
                {
                    "sent": "But for this to work, we need activity measurements.",
                    "label": 0
                },
                {
                    "sent": "For millions, possibly of candidate molecules, for each target, I mean we can their commercial libraries where you can buy millions of different molecules, and so we'd at least like to check the ones that we can buy instead of having to synthesize.",
                    "label": 0
                },
                {
                    "sent": "And we need a way to predict the activity from chemical structure without running assays.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So Merck spends billions of dollars developing new drugs, and so they set up a competition where they take one tiny, tiny little piece of their drug discovery pipeline, and they sort of make a little competition about it and what they did is they released 15.",
                    "label": 0
                },
                {
                    "sent": "Related data sets, or I sometimes call them tasks altogether.",
                    "label": 0
                },
                {
                    "sent": "Where you predict molecular activity.",
                    "label": 0
                },
                {
                    "sent": "Each task is a regression problem where you predict the activity of molecules towards one particular target.",
                    "label": 1
                },
                {
                    "sent": "So one task, one data set, one target.",
                    "label": 1
                },
                {
                    "sent": "You're only given the chemical structure feature vectors for candidate molecules and not really any information about the targets.",
                    "label": 1
                },
                {
                    "sent": "And the molecules and input features sometimes occur in multiple tasks.",
                    "label": 0
                },
                {
                    "sent": "So you're given global ID's for the inputs and they get repeated across different tasks.",
                    "label": 0
                },
                {
                    "sent": "And also the activity labels have different unknown units in these different tasks.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so let me just dive right into the data that they actually released.",
                    "label": 0
                },
                {
                    "sent": "They have input features that are non negative integers.",
                    "label": 0
                },
                {
                    "sent": "Presumably there counts of some sort of substructure, but we're not told really anything about them, and about 3 to 10% are non zero and the number of features in the data set ranges from about 4000 to 9000 or so.",
                    "label": 1
                },
                {
                    "sent": "And if you union all the different unique features across the 15 tasks.",
                    "label": 1
                },
                {
                    "sent": "You'll get about 11,000 unique features.",
                    "label": 0
                },
                {
                    "sent": "The number of training cases for a given tasks between 18137 thousand.",
                    "label": 1
                },
                {
                    "sent": "So in many of the tasks you have more input dimensions.",
                    "label": 0
                },
                {
                    "sent": "Then you have training cases for that task.",
                    "label": 0
                },
                {
                    "sent": "So normally we expect very large neural networks to overfit in these conditions and not give good results even if you do careful early stopping.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so the scoring metric for.",
                    "label": 0
                },
                {
                    "sent": "The competition was just an average of a correlation with the true activity for each task, so it's an equally weighted average over the 15 tasks of essentially the percentage of variance you explain where you have the true activity and your predicted activity, and then the means of the truth in the predictions.",
                    "label": 0
                },
                {
                    "sent": "And so that.",
                    "label": 0
                },
                {
                    "sent": "This on a per case basis you'd much rather get the cases from the small datasets correct, since the average doesn't take into account the variable sizes of the datasets.",
                    "label": 0
                },
                {
                    "sent": "And the training set contains molecules that were assayed before some certain date and then the test set everything after that, and so they actually have very different distributions between the training set and the test set.",
                    "label": 1
                },
                {
                    "sent": "So it's very easy to sort of overfit the training set.",
                    "label": 0
                },
                {
                    "sent": "Because, but really, it's it's what can you really do since they have different distributions.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our solution.",
                    "label": 0
                },
                {
                    "sent": "Like almost all solutions to these sorts of competitions ultimately involved in sambol and we just did very, very simple in sampling, and I'm not going to really go into, we just died.",
                    "label": 0
                },
                {
                    "sent": "Completely even averages of the predictions of a couple of models for each data set.",
                    "label": 0
                },
                {
                    "sent": "And we picked those individually on a per data set basis and someone are in sambol included neural Nets of various types that will go into Gaussian process regression models and even some decision trees.",
                    "label": 0
                },
                {
                    "sent": "However, the neural Nets trained with dropout were the only completely indispensable part of our solution and drop out is the technique that let us.",
                    "label": 1
                },
                {
                    "sent": "Train these Nets in an effective way on this.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "So the first sort of neural net we used was the most basic simple neural net.",
                    "label": 0
                },
                {
                    "sent": "It ignores the potential relationship between these different tasks, and justice is for a single task.",
                    "label": 0
                },
                {
                    "sent": "Ignore all the other tasks.",
                    "label": 0
                },
                {
                    "sent": "It's a regression problem, just training her on that we use one or two hidden layers.",
                    "label": 0
                },
                {
                    "sent": "Different sorts of nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "And for the small datasets especially, and even for the moderately larger ones, dropout was essential to avoid overfitting.",
                    "label": 1
                },
                {
                    "sent": "And an example of that is without dropout, I couldn't train really a neural net with more than 30 hidden units.",
                    "label": 0
                },
                {
                    "sent": "In a single hidden layer, affectively.",
                    "label": 0
                },
                {
                    "sent": "Without sort of disastrous overfitting even when I tried to do early stopping but with dropout I could train neural Nets with 500,000 or even more units and just get better results.",
                    "label": 0
                },
                {
                    "sent": "So these are the simplest neural Nets that we used.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we also used multi task neural Nets and so basically there would be one output unit.",
                    "label": 1
                },
                {
                    "sent": "I only shown 5 here for each of the tasks.",
                    "label": 0
                },
                {
                    "sent": "And so we only observe one of these at a time for a particular training case.",
                    "label": 1
                },
                {
                    "sent": "And we also have one input for each unique input across all the tasks.",
                    "label": 1
                },
                {
                    "sent": "So we have 11,000 inputs, whereas a particular task might only use 4000 or 5000 of those.",
                    "label": 0
                },
                {
                    "sent": "And since we don't really know the units of the outputs, I standardize them to have mean zero unit variance.",
                    "label": 1
                },
                {
                    "sent": "Probably we could have reverse engineered exactly what Wet Lab experiment they did to get those numbers and done something even smarter, but that would have required actual knowledge about biochemistry.",
                    "label": 0
                },
                {
                    "sent": "Although some people in my team had that knowledge, I certainly did not.",
                    "label": 0
                },
                {
                    "sent": "And we trained on mini batches of cases, so we trained these Nets with cast agreeing to send over small mini batches where each mini batch was guaranteed to include an equal number of cases from each of the 15 tasks.",
                    "label": 0
                },
                {
                    "sent": "So by doing that we optimize what we hope is something close to the evaluation metric, since if you just lump all the data together.",
                    "label": 0
                },
                {
                    "sent": "You're you'll have a neural network that focuses on the largest of the datasets, 'cause it sees the most training data from that.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically, to go over exactly how training would work, I've highlighted.",
                    "label": 0
                },
                {
                    "sent": "So suppose that one training case we have, we have this activity observed, and then we observe these two inputs and we don't observe.",
                    "label": 0
                },
                {
                    "sent": "Are these two so the highlighted pink weights are the ones that will get updated when we train on this case and we just don't update the others?",
                    "label": 0
                },
                {
                    "sent": "And in this way we will train sort of slices or columns of this network repeatedly.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how will this work on the competition?",
                    "label": 0
                },
                {
                    "sent": "Well, once we made our best ensemble that we could make with our equally weighted averages of a handful of models, including even some gradient boosted decision trees, we got.",
                    "label": 0
                },
                {
                    "sent": "Of course the 1st place result and the validation scores I'm reporting here, where the public leaderboard.",
                    "label": 0
                },
                {
                    "sent": "So during the competition.",
                    "label": 0
                },
                {
                    "sent": "You could make some missions and get 25% of the test data.",
                    "label": 0
                },
                {
                    "sent": "It would always be the same 25% for everyone, but that gives you an incentive to show your progress during the competition.",
                    "label": 0
                },
                {
                    "sent": "Since you get bits of information about the test distribution and so the prizes not awarded based on this because we don't want people mining the leaderboard and then exploiting it, and you have a limited number of submissions.",
                    "label": 0
                },
                {
                    "sent": "So you can't always do all the careful experiments you'd like to do.",
                    "label": 0
                },
                {
                    "sent": "If you were doing a paper or something, but you can sort of a BLT, different models in your ensembles and tweak your.",
                    "label": 0
                },
                {
                    "sent": "Your models.",
                    "label": 0
                },
                {
                    "sent": "To improve on the leaderboard.",
                    "label": 0
                },
                {
                    "sent": "And that's what I'm referring to is validation here, so these are squared numbers show that are best ensemble.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Got got these numbers and they were very close on the test set which was good.",
                    "label": 0
                },
                {
                    "sent": "We were very worried about overfitting the leaderboard data so we only did a very few number of submissions compared to any of the other top teams.",
                    "label": 0
                },
                {
                    "sent": "And if you remove the trees, you get slightly worse results 'cause they, although they're not very good individually, they produce radically different predictions than some of our other models, so they're very good to combine with them.",
                    "label": 0
                },
                {
                    "sent": "And if you only use neural networks, did know.",
                    "label": 0
                },
                {
                    "sent": "Like since ambling just use single task neural Nets and multi task neural Nets trained with dropout and picked the whatever neural network best on each of the 15 datasets and built a submission out of that, we would have still won first place.",
                    "label": 1
                },
                {
                    "sent": "And that wouldn't have done any serious model averaging.",
                    "label": 0
                },
                {
                    "sent": "Ann just would have used dropout and neural Nets of various sorts.",
                    "label": 0
                },
                {
                    "sent": "And so this was a very exciting result since.",
                    "label": 0
                },
                {
                    "sent": "Normally the sort of neural Nets that we use at this scale with, like you know, 5000 units in a layer or something in a couple of different layers on just 1000 training cases would just overfit hopelessly.",
                    "label": 0
                },
                {
                    "sent": "But also we see from this is that if we just use the Gaussian process regression, we've gotten about 10th place and if we used whatever our internal cross validation said was the best on each individual data set, so would either be a GP or a neural net.",
                    "label": 0
                },
                {
                    "sent": "Typically we would have gotten 7th place, which shows that our internal cross validation slightly overestimated the performance of the GPS, which is a little unfortunate.",
                    "label": 0
                },
                {
                    "sent": "But when we have a very limited budget of submissions, it's hard to tease out these things during the actual competition.",
                    "label": 0
                },
                {
                    "sent": "And of course, since the gains here.",
                    "label": 0
                },
                {
                    "sent": "Are you know substantial over the just using the neural Nets?",
                    "label": 0
                },
                {
                    "sent": "Even the simple averaging we used is quite helpful.",
                    "label": 1
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A few conclusions from this dropout lets us train larger neural Nets on more inputs with fewer training cases, which is wonderful for these sorts of mid sized data sets and even small ones, and it helps a lot when there are more input dimensions than training cases.",
                    "label": 1
                },
                {
                    "sent": "Which is the case here, and Furthermore with the multi task neural Nets.",
                    "label": 0
                },
                {
                    "sent": "I think doing dropout in the input also helped make it robust to only observing some of the possible inputs.",
                    "label": 0
                },
                {
                    "sent": "So that's a sort of ancillary benefit and the single and multi task neural Nets with dropout.",
                    "label": 1
                },
                {
                    "sent": "Suffice to get first place in the competition.",
                    "label": 0
                },
                {
                    "sent": "And that's all I have to say.",
                    "label": 0
                },
                {
                    "sent": "So if there's any questions, please ask.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So if you had used dropout for the Netflix competition, would you have one first place?",
                    "label": 0
                },
                {
                    "sent": "Well, we probably have to ask my team a trust that since he participated in the competition, but I have honestly no idea.",
                    "label": 0
                },
                {
                    "sent": "You go back and try.",
                    "label": 0
                },
                {
                    "sent": "It probably have the data right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Any other questions?",
                    "label": 0
                },
                {
                    "sent": "So would you please use the speaker microphone?",
                    "label": 0
                },
                {
                    "sent": "Public microphone, anybody else who has questions?",
                    "label": 0
                },
                {
                    "sent": "Would you mind lining up at the microphones right?",
                    "label": 0
                },
                {
                    "sent": "So supposedly Mark is very interested in how you did this.",
                    "label": 0
                },
                {
                    "sent": "What can you tell them about features or?",
                    "label": 0
                },
                {
                    "sent": "Like how can you unfold the black box that is drop out?",
                    "label": 0
                },
                {
                    "sent": "OK, well we didn't do any feature engineering that wasn't done by Merck before us, so we just use their features.",
                    "label": 0
                },
                {
                    "sent": "We actually, you know, sometimes we took the log of them, but that's it, and so they know what the features are.",
                    "label": 0
                },
                {
                    "sent": "We have no idea what they are, so their chemists can figure out what that means.",
                    "label": 0
                },
                {
                    "sent": "But as far as the neural Nets themselves go.",
                    "label": 0
                },
                {
                    "sent": "Um, if you want to see, sort of why it predicted.",
                    "label": 0
                },
                {
                    "sent": "Some particular activity you can sort of backtrack through the network and look at the sensitivity to various input features, but since there are a lot of them, I'm not not sure how productive that would be.",
                    "label": 0
                },
                {
                    "sent": "You could also visualize.",
                    "label": 0
                },
                {
                    "sent": "The different types of molecules with T sne.",
                    "label": 0
                },
                {
                    "sent": "So Lawrence Vondra Martin made a bunch of very informative TC plots for this competition in a related visualization competition, and you can see sort of clusters of similar molecules because a lot of these molecules are made by finding something that looks sort of promising and then sort of Hill climbing around it with lots of chemistry knowledge.",
                    "label": 0
                },
                {
                    "sent": "How did the company react?",
                    "label": 0
                },
                {
                    "sent": "Because they are squares are quite low.",
                    "label": 0
                },
                {
                    "sent": "I mean, although their first place but like they don't really explain lot of difference.",
                    "label": 0
                },
                {
                    "sent": "How did the company react to that?",
                    "label": 0
                },
                {
                    "sent": "Well I think they were very pleased because they had an internal benchmark as well and their internal benchmark was also on the leaderboard and it was well below the 1st place entry.",
                    "label": 0
                },
                {
                    "sent": "But in terms of whether this kind of intervention is can speed up their pipelines?",
                    "label": 0
                },
                {
                    "sent": "I mean basically the benchmark.",
                    "label": 0
                },
                {
                    "sent": "But no, I'm not an expert on drug discovery, but the people I've talked with Merck.",
                    "label": 0
                },
                {
                    "sent": "I suspect they will actually use it in their drug discovery pipeline, and they seem very excited about it.",
                    "label": 0
                },
                {
                    "sent": "Wow, thanks.",
                    "label": 0
                },
                {
                    "sent": "I have two questions.",
                    "label": 0
                },
                {
                    "sent": "What kind of covariance functions did you use for your GPS and you put a lot of effort in optimizing them is my first question, and then the second one is what intuitions you draw from the fact that actually doing equal model averaging was better than neural Nets with power.",
                    "label": 0
                },
                {
                    "sent": "So the GPS we tried squared exponential neural net covariance functions, rational quadratic covariance functions, and probably some attorn ones as well.",
                    "label": 0
                },
                {
                    "sent": "Russ trained the GPS, so he might be able to give more insight on that and they were the hyper parameters of the covariance functions for the GPS, where sort of optimized on the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "I believe Russ tried his best to make good GPS and.",
                    "label": 0
                },
                {
                    "sent": "I I didn't do that part of it so and they did work quite well, especially on some of the datasets.",
                    "label": 0
                },
                {
                    "sent": "So on some datasets they work better than others and they were very helpful in our average.",
                    "label": 0
                },
                {
                    "sent": "As far as your second question, I'm sorry that was.",
                    "label": 0
                },
                {
                    "sent": "I got model averaging yes, so I think I was a little surprised that the we couldn't get a benefit from doing more sophisticated model averaging.",
                    "label": 0
                },
                {
                    "sent": "We did explore it a little bit, but to really do that carefully you need a lot of leaderboard submissions and we were very concerned about overfitting the leaderboard public data.",
                    "label": 0
                },
                {
                    "sent": "And since the validation the cross validation.",
                    "label": 0
                },
                {
                    "sent": "On the training set, it's from a different distribution than the test set.",
                    "label": 0
                },
                {
                    "sent": "We were very worried about tuning it too much there, so we were able to get benefits on the training set with more specific model averaging, but which probably would not surprise anyone.",
                    "label": 0
                },
                {
                    "sent": "So I think really if there's more data, or if we had more freedom to sort of poke around with it, maybe there would be a benefit.",
                    "label": 0
                },
                {
                    "sent": "OK, next question.",
                    "label": 0
                },
                {
                    "sent": "So the dropout rate is 1/2.",
                    "label": 0
                },
                {
                    "sent": "Don't you think that this could be related to the imbalance between the number of cases and the number of features?",
                    "label": 0
                },
                {
                    "sent": "Um, so actually not all of our Nets used .5 for the dropout rate in this.",
                    "label": 0
                },
                {
                    "sent": "So we tried 25% and 50% and so I don't think there's a direct relationship between the number of.",
                    "label": 0
                },
                {
                    "sent": "The number of input features and the best dropout rate.",
                    "label": 0
                },
                {
                    "sent": "I think it depends in a relatively complicated way on the size and capacity of the neural Nets that you train and how long you train them.",
                    "label": 0
                },
                {
                    "sent": "OK, last question over there yeah so you talked about instead of taking lots of samples to average together models at Test time, just multiplying all the weights by 1/2.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering what happens with the performance if instead we just take one sample and use that to predict, does that do much worse than doing the dividing the weights by half thing?",
                    "label": 0
                },
                {
                    "sent": "I haven't tried that explicitly, but from my experiences with with the training error that's measured with the stochastic version, it would probably be worse.",
                    "label": 0
                },
                {
                    "sent": "And it wouldn't really provide any advantage that I can see.",
                    "label": 0
                },
                {
                    "sent": "If you did that because it wouldn't really be that much faster.",
                    "label": 0
                },
                {
                    "sent": "Right, well, but if you're like if you only care about, say, binary predictions, than a single sample could actually give you a reasonably good estimate of the probability.",
                    "label": 0
                },
                {
                    "sent": "Maybe why not just use all the neurons?",
                    "label": 0
                },
                {
                    "sent": "I guess alright OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "I've got a quick last question.",
                    "label": 0
                },
                {
                    "sent": "Maybe Sammy could summary could already set up his laptop for the next talk so you have quite a bit of covariate shift between your training and your test set, right?",
                    "label": 0
                },
                {
                    "sent": "And did you do do anything about that?",
                    "label": 0
                },
                {
                    "sent": "Where basically did you take that into account to maybe adapt to different distributions?",
                    "label": 0
                },
                {
                    "sent": "Because obviously if they were temporally aligned, they would have started with the more obvious molecules first employee learn something in the process that there would have been some nontrivial temporal dynamics in it.",
                    "label": 0
                },
                {
                    "sent": "Did you exploit that?",
                    "label": 0
                },
                {
                    "sent": "So although it was not against the rules?",
                    "label": 0
                },
                {
                    "sent": "The organizer of the competition expressed as very strong preference that we not use the distribution of test inputs in our solution, so we decided to respect that.",
                    "label": 0
                },
                {
                    "sent": "Although other teams claimed to get a large benefit from it, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so then thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Again, thank you.",
                    "label": 0
                }
            ]
        }
    }
}