{
    "id": "3cjhs45cmqehb6qsioea5aekoa6eedaf",
    "title": "Variational Gaussian Process",
    "info": {
        "author": [
            "Dustin Tran, Department of Computer Science, Columbia University"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_tran_variational_gaussian/",
    "segmentation": [
        [
            "OK, so Dustin I'm presenting this paper.",
            "This is joint work with just running out and apply."
        ],
        [
            "OK, so first I just want to show you something success stories recently with the general modeling Neil gave some of the greatest examples but also just motivate coming from our own work.",
            "So in this case where using a deep generative model to solve the classical task of topic modeling which is used in information retrieval to try to uncover global or thematic information that's shared across documents.",
            "And in this case we're using a three layer generative model, which you can just think of as a neural network with no inputs whatsoever.",
            "The outputs of this are the data, so we're trying to do unsupervised learning, and instead of having deterministic hidden units were using hidden variables, so these are random variables.",
            "In this case we're using Boson random variables, because we're going to model count data.",
            "And then obviously we have distributions over the weights of these things.",
            "The top layer of this is the first layer in this generative process.",
            "The second layer is inner product of the previous layers, random variables and the weights of the first layer, and so on and so forth.",
            "So using this generative model we've captured sort of this hierarchy of low level features.",
            "The high level features the first layer in this case, which was used to apply to.",
            "166 thousand articles on New York Times data set is uncovering government and politics and then below these low level features we get higher level ones.",
            "In this case being the different branches in the government and so on and so forth."
        ],
        [
            "OK. And in this task it's very similar, is ultimately the same architecture, but rather than using Python random variables as the hidden variables, they are Gaussian random variables.",
            "The task here is that we're going to feed this model feed into this model a bunch of static images.",
            "So it's going to try to generate these images and the task at Test time is that we're going to randomly throw away some of the pixels in this image, and we want to try to generate and completes this image."
        ],
        [
            "And we can think a bit more ambitiously about how we can use these generative models in this case.",
            "The data set is the same, so we're still using static images.",
            "But now we're trying to think more causally about the generative process.",
            "So here we've used a sequential generation procedure.",
            "In this case, the model is draw, which has recently inspired a lot of the recent papers on building attention and incorporating these processes in order to create complex.",
            "Image generation models and this model and previous variant related variants of this has given us sort of the state of the arts as you will on certain unsupervised learning tasks.",
            "OK, So what has really enabled?",
            "The use of these D trend of?"
        ],
        [
            "Rules I would really argue that some of the recent advances in variable inference has led us to think about these things in a renewed light.",
            "Specifically, it was the things that Neil mentioned, so we have stochastic variational inference which allows us to scale these procedures to massive datasets.",
            "We have black box variational inference, which allows us to use these procedures for general probability models, and we have work from chemo and willing that only allows us to amortize inference.",
            "Such that the computation does not scale any longer with the size of the datasets.",
            "OK, and.",
            "Interestingly enough, one of the main assumptions that we've been using in a lot of these various elements procedures is specifying a rich family approximating distributions that's used to approximate the posterior, and surprisingly, we've just used the typical fully factorized distribution that tries to capture sort of the marginal information presence in the hidden variables, but no correlation across the hidden variables, and we've reached sort of the state of the art on these things.",
            "Using such simple distributions fundamentally as our procedure.",
            "So the question really want to solve is how can we build expressive variational families in this black box framework which will adapt to the model complexity hands.",
            "Whether you give the procedure a five layer generative model or you give the procedure something very simple, such as Bayesian linear regression."
        ],
        [
            "OK, so first let's set up some notation were given some data set X and we have assumed that the researcher or the practitioner has specified this joint probability model, which is just a joint distribution over the data and the hidden variables.",
            "This encapsulates a represents the generative models that I've shown for motivation and also these simple distributions that are typically used in social Sciences, economics and so on and so forth and then."
        ],
        [
            "All of the procedure is to infer a posterior distribution, which is the distribution of the hidden variables conditional on this datasets, and this captures the hidden or distributed representations that are used to also do things like supervised learning.",
            "One shot learning and so on."
        ],
        [
            "OK.",
            "So very small inference.",
            "What it does procedurally is just that you want to deposit some family of distributions.",
            "Here I'm denoting this bike.",
            "You said it's indexed by a set of parameters Lambda, and we're going to minimize a divergent measure information information theoretic notion that tries to formalize the closeness between this approximating family and the posterior distribution.",
            "It turns out, for tractability reasons that minimizing this scale is equivalent to maximizing this lower bound to the log likelihood.",
            "And here I've written it as the way in which you connect to autoencoders, where you going to maximize this expected log likelihood, or you're going to minimize the negative reconstruction error, and then you have this additional penalty term.",
            "An additional penalty term is just the scale regularizer between the family of distributions that are used for the approximation an the model.",
            "Prior and here typically the procedure that's used for positing this family of distributions is just a fully factorized distribution that factorizes across all of the hidden variables for the posterior.",
            "OK, so we really want to construct a more expressive failing distributions that goes beyond this very naive approximation that seems to work well in practice, but maybe we can do better."
        ],
        [
            "So first, let's think about what this character would do.",
            "This character is Thomas Bayes, not actually Thomas Bayes, but he is going to think about how are you going to build an expressive model for the data distribution.",
            "So you want this data distribution P of X.",
            "You want a rich family that will capture this distribution, and so we're going to use latent variables."
        ],
        [
            "OK, and and obviously we're going to IC Newell smiling there.",
            "This is cartoon of Neil and he is going to do a similar procedure like he did in his thesis, which you're now going to specify a rich family for QZ.",
            "And how would we do this?",
            "Well now we just introduce latent variables so we have a hierarchal model for the variational distribution."
        ],
        [
            "OK, so here is the formula for a higher correlational model based on its density form, so I have a prior over the parameters of this variational distribution Q of Lambda and I've chosen the likelihood the probability of observing the posterior latent variables given these parameters as fully factorized.",
            "So I've still use a mean field approximation as the underlying likelihood in the hierarchal version model this is done for scalability reasons, but it still enables.",
            "Correlation by having this prior that couples the mean field parameters and just this very simple construction, unifies a lot of the expressive approximations that have been considered in the literature, namely mixtures of mean field distributions.",
            "You have structured factorizations.",
            "We have MCMC."
        ],
        [
            "And ultimately, the expressiveness of this hierarchal virtual model is determined by the prior distribution, however complex this prior distribution is will give us more powerful or more expressive correlations between the mean field parameters.",
            "OK, so.",
            "We're motivated by choosing this prior based on Gaussian processes, so here we're just going to set up the basic notation of this, so we're going to consider that we have a set of M source target pairs or inputs and outputs.",
            "These are multi dimensional inputs and outputs, so they're both D dimensional and now you want to learn this vector valued function that takes SN as inputs and outputs tiene, and we're going to learn a random nonlinear mapping a distribution over these nonlinear mappings.",
            "Using Gaussian processes, the typical construction that's used for this procedure is by decomposing this vector valued function into scalar valued functions, each of which go from the D dimensional input to a 1 dimensional output.",
            "So this is fully factorized, P of F is just the product of Gaussian processes over each of these dimensions, and what's nice about this procedure is that given the data given, the set of inputs and output pairs, we now.",
            "Have a conditional distribution conditional on this data set which interpolates between all of these inputs and outputs.",
            "So this is just noise for regression, so there is no noise in how the outputs are formed.",
            "OK, so when are we going to use this construction to build our variational prior the prior over the mean field parameters?"
        ],
        [
            "So this is a density form.",
            "It looks rather obtuse, but let's just stare at this little bit.",
            "We still have the likelihood, which is this mean field factorization.",
            "Now we have this prior distribution.",
            "This prior is given by two components, one is the Gaussian process conditional on a hypothetical data sets will talk about this later, and we have latent inputs into this Gaussian process.",
            "So the inputs are into the drug draws, which are these random linear mappings.",
            "The output of the GP draws are the mean field parameters.",
            "So if you just look at this thing a little bit so we can see that this variational Gaussian process is just an ensemble of mean field distributions, so the weights of these mean field of these individual mean field distributions are given by a Bayesian nonparametric prior.",
            "And evaluating these GP draws at the same input C. In this is correlation between the outputs.",
            "And the parameters of this variation Gaussian process are the data itself, because rather than observing data in the latent variable space, we're going to imagine a bunch of hypothetical data and want to learn where to situate these data points, just that it incurs the random nonlinear mappings the GPS draws at certain inputs and output pairs."
        ],
        [
            "So we can also just think of this in terms of the general process.",
            "So first we're going to draw the lead inputs, see from a standard normal, so just family status, simple distribution.",
            "We're going to draw a nonlinear mapping condition on this fake datasets, and then we're going to draw mean field samples, approximately series samples conditional on the output of this GP draw.",
            "So maybe you did not sign into it."
        ],
        [
            "But we have this cool theorem which says that.",
            "Given the variation Gaussian process, we can universally approximate any posterior distribution.",
            "So if you give me more and more parameters, if you feed the variation Gaussian process more and more data, it will converge to the posterior.",
            "And this gives."
        ],
        [
            "Nicely, some statistical and computational tradeoff in inference algorithm, so we can grow the datasets.",
            "This makes the inference algorithm slower, but enables richer approximation to the posterior.",
            "So we've chosen only one family of distributions received.",
            "The very strong calcium process and we just sat this set of the size of the datasets."
        ],
        [
            "I won't go into the infra saglam too much, this is just the.",
            "The lower bound in which we've derived the intuition behind this using variational autoencoders, is that it's just like the minimizing this negative reconstruction error and now have a sum of 2 Cal terms.",
            "You have the original Cal term that's typically used in various non coders and then you have an additional one that is based on the variational prior the prior over C&F and you know XR redistribution.",
            "So you're using a hierarchical variational inference procedure, a sense innocence where you have an auxiliary distribution.",
            "That's used for the inference.",
            "For inferring the.",
            "Variational posterior if you will."
        ],
        [
            "OK.",
            "So here is a simple example that I took from.",
            "OK, so this is using a Bayesian neural network.",
            "It is trying to learn a neural network where the weights of this are given by a normal prior and it's trying to interpolate functions that go in between all of these inputs and outputs.",
            "So this is just one dimensional and you can notice that it fits very well the.",
            "The inside of this portion and then the outside represents all the uncertainty.",
            "In other words, the function doesn't know very well how to predict regions outside of the support, and so there is very high uncertainty and this is the Gaussian process draws.",
            "OK."
        ],
        [
            "And.",
            "Here is a very standard example or standard experiment that's used in unsupervised learning.",
            "With generative models, you're showing to try to generate images for MNIST.",
            "I've categorized this according to four different types of models.",
            "The first one is just the baselines, be second one is the deeply in Gaussian model is just a generative model in which we have Gaussian random variables as the hidden units in the one layer case where you have 1 hidden layer for the deeply in Gaussian model.",
            "We've used the very strong Gaussian process to infer this model and does the best similarly, if you use 2 hidden layers, the variance in process performs other competing methods normally, like the importance weighted autoencoders, Hamiltonian, virtual and friends normalizing flows, and we've also gotten state of the art using the various non Gaussian process to infer draw which is the sequential generative mechanism that was used in the previous slide.",
            "OK."
        ],
        [
            "So what have we done?",
            "As summary?",
            "We've introduced the framework of original models.",
            "It's this, I smell paper.",
            "We've just got published.",
            "We've developed the verification process, which is a universal approximator, and we've derived scalable inference for it."
        ],
        [
            "And finally, I just want to plug that.",
            "We've released software for this as well as General Black Box inference and deep generative modeling.",
            "It's open source software and we think that this will help enable researchers in both and also practitioners to get their feet wet with this procedure.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so Dustin I'm presenting this paper.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with just running out and apply.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first I just want to show you something success stories recently with the general modeling Neil gave some of the greatest examples but also just motivate coming from our own work.",
                    "label": 0
                },
                {
                    "sent": "So in this case where using a deep generative model to solve the classical task of topic modeling which is used in information retrieval to try to uncover global or thematic information that's shared across documents.",
                    "label": 0
                },
                {
                    "sent": "And in this case we're using a three layer generative model, which you can just think of as a neural network with no inputs whatsoever.",
                    "label": 0
                },
                {
                    "sent": "The outputs of this are the data, so we're trying to do unsupervised learning, and instead of having deterministic hidden units were using hidden variables, so these are random variables.",
                    "label": 0
                },
                {
                    "sent": "In this case we're using Boson random variables, because we're going to model count data.",
                    "label": 0
                },
                {
                    "sent": "And then obviously we have distributions over the weights of these things.",
                    "label": 0
                },
                {
                    "sent": "The top layer of this is the first layer in this generative process.",
                    "label": 0
                },
                {
                    "sent": "The second layer is inner product of the previous layers, random variables and the weights of the first layer, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So using this generative model we've captured sort of this hierarchy of low level features.",
                    "label": 0
                },
                {
                    "sent": "The high level features the first layer in this case, which was used to apply to.",
                    "label": 0
                },
                {
                    "sent": "166 thousand articles on New York Times data set is uncovering government and politics and then below these low level features we get higher level ones.",
                    "label": 0
                },
                {
                    "sent": "In this case being the different branches in the government and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. And in this task it's very similar, is ultimately the same architecture, but rather than using Python random variables as the hidden variables, they are Gaussian random variables.",
                    "label": 0
                },
                {
                    "sent": "The task here is that we're going to feed this model feed into this model a bunch of static images.",
                    "label": 0
                },
                {
                    "sent": "So it's going to try to generate these images and the task at Test time is that we're going to randomly throw away some of the pixels in this image, and we want to try to generate and completes this image.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can think a bit more ambitiously about how we can use these generative models in this case.",
                    "label": 0
                },
                {
                    "sent": "The data set is the same, so we're still using static images.",
                    "label": 0
                },
                {
                    "sent": "But now we're trying to think more causally about the generative process.",
                    "label": 0
                },
                {
                    "sent": "So here we've used a sequential generation procedure.",
                    "label": 0
                },
                {
                    "sent": "In this case, the model is draw, which has recently inspired a lot of the recent papers on building attention and incorporating these processes in order to create complex.",
                    "label": 0
                },
                {
                    "sent": "Image generation models and this model and previous variant related variants of this has given us sort of the state of the arts as you will on certain unsupervised learning tasks.",
                    "label": 0
                },
                {
                    "sent": "OK, So what has really enabled?",
                    "label": 0
                },
                {
                    "sent": "The use of these D trend of?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rules I would really argue that some of the recent advances in variable inference has led us to think about these things in a renewed light.",
                    "label": 0
                },
                {
                    "sent": "Specifically, it was the things that Neil mentioned, so we have stochastic variational inference which allows us to scale these procedures to massive datasets.",
                    "label": 0
                },
                {
                    "sent": "We have black box variational inference, which allows us to use these procedures for general probability models, and we have work from chemo and willing that only allows us to amortize inference.",
                    "label": 0
                },
                {
                    "sent": "Such that the computation does not scale any longer with the size of the datasets.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "Interestingly enough, one of the main assumptions that we've been using in a lot of these various elements procedures is specifying a rich family approximating distributions that's used to approximate the posterior, and surprisingly, we've just used the typical fully factorized distribution that tries to capture sort of the marginal information presence in the hidden variables, but no correlation across the hidden variables, and we've reached sort of the state of the art on these things.",
                    "label": 0
                },
                {
                    "sent": "Using such simple distributions fundamentally as our procedure.",
                    "label": 0
                },
                {
                    "sent": "So the question really want to solve is how can we build expressive variational families in this black box framework which will adapt to the model complexity hands.",
                    "label": 1
                },
                {
                    "sent": "Whether you give the procedure a five layer generative model or you give the procedure something very simple, such as Bayesian linear regression.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first let's set up some notation were given some data set X and we have assumed that the researcher or the practitioner has specified this joint probability model, which is just a joint distribution over the data and the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "This encapsulates a represents the generative models that I've shown for motivation and also these simple distributions that are typically used in social Sciences, economics and so on and so forth and then.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All of the procedure is to infer a posterior distribution, which is the distribution of the hidden variables conditional on this datasets, and this captures the hidden or distributed representations that are used to also do things like supervised learning.",
                    "label": 0
                },
                {
                    "sent": "One shot learning and so on.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So very small inference.",
                    "label": 0
                },
                {
                    "sent": "What it does procedurally is just that you want to deposit some family of distributions.",
                    "label": 1
                },
                {
                    "sent": "Here I'm denoting this bike.",
                    "label": 0
                },
                {
                    "sent": "You said it's indexed by a set of parameters Lambda, and we're going to minimize a divergent measure information information theoretic notion that tries to formalize the closeness between this approximating family and the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "It turns out, for tractability reasons that minimizing this scale is equivalent to maximizing this lower bound to the log likelihood.",
                    "label": 1
                },
                {
                    "sent": "And here I've written it as the way in which you connect to autoencoders, where you going to maximize this expected log likelihood, or you're going to minimize the negative reconstruction error, and then you have this additional penalty term.",
                    "label": 0
                },
                {
                    "sent": "An additional penalty term is just the scale regularizer between the family of distributions that are used for the approximation an the model.",
                    "label": 0
                },
                {
                    "sent": "Prior and here typically the procedure that's used for positing this family of distributions is just a fully factorized distribution that factorizes across all of the hidden variables for the posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, so we really want to construct a more expressive failing distributions that goes beyond this very naive approximation that seems to work well in practice, but maybe we can do better.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first, let's think about what this character would do.",
                    "label": 0
                },
                {
                    "sent": "This character is Thomas Bayes, not actually Thomas Bayes, but he is going to think about how are you going to build an expressive model for the data distribution.",
                    "label": 0
                },
                {
                    "sent": "So you want this data distribution P of X.",
                    "label": 0
                },
                {
                    "sent": "You want a rich family that will capture this distribution, and so we're going to use latent variables.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and and obviously we're going to IC Newell smiling there.",
                    "label": 0
                },
                {
                    "sent": "This is cartoon of Neil and he is going to do a similar procedure like he did in his thesis, which you're now going to specify a rich family for QZ.",
                    "label": 1
                },
                {
                    "sent": "And how would we do this?",
                    "label": 1
                },
                {
                    "sent": "Well now we just introduce latent variables so we have a hierarchal model for the variational distribution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here is the formula for a higher correlational model based on its density form, so I have a prior over the parameters of this variational distribution Q of Lambda and I've chosen the likelihood the probability of observing the posterior latent variables given these parameters as fully factorized.",
                    "label": 0
                },
                {
                    "sent": "So I've still use a mean field approximation as the underlying likelihood in the hierarchal version model this is done for scalability reasons, but it still enables.",
                    "label": 0
                },
                {
                    "sent": "Correlation by having this prior that couples the mean field parameters and just this very simple construction, unifies a lot of the expressive approximations that have been considered in the literature, namely mixtures of mean field distributions.",
                    "label": 0
                },
                {
                    "sent": "You have structured factorizations.",
                    "label": 0
                },
                {
                    "sent": "We have MCMC.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And ultimately, the expressiveness of this hierarchal virtual model is determined by the prior distribution, however complex this prior distribution is will give us more powerful or more expressive correlations between the mean field parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We're motivated by choosing this prior based on Gaussian processes, so here we're just going to set up the basic notation of this, so we're going to consider that we have a set of M source target pairs or inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "These are multi dimensional inputs and outputs, so they're both D dimensional and now you want to learn this vector valued function that takes SN as inputs and outputs tiene, and we're going to learn a random nonlinear mapping a distribution over these nonlinear mappings.",
                    "label": 1
                },
                {
                    "sent": "Using Gaussian processes, the typical construction that's used for this procedure is by decomposing this vector valued function into scalar valued functions, each of which go from the D dimensional input to a 1 dimensional output.",
                    "label": 0
                },
                {
                    "sent": "So this is fully factorized, P of F is just the product of Gaussian processes over each of these dimensions, and what's nice about this procedure is that given the data given, the set of inputs and output pairs, we now.",
                    "label": 0
                },
                {
                    "sent": "Have a conditional distribution conditional on this data set which interpolates between all of these inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "So this is just noise for regression, so there is no noise in how the outputs are formed.",
                    "label": 0
                },
                {
                    "sent": "OK, so when are we going to use this construction to build our variational prior the prior over the mean field parameters?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a density form.",
                    "label": 0
                },
                {
                    "sent": "It looks rather obtuse, but let's just stare at this little bit.",
                    "label": 0
                },
                {
                    "sent": "We still have the likelihood, which is this mean field factorization.",
                    "label": 0
                },
                {
                    "sent": "Now we have this prior distribution.",
                    "label": 0
                },
                {
                    "sent": "This prior is given by two components, one is the Gaussian process conditional on a hypothetical data sets will talk about this later, and we have latent inputs into this Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So the inputs are into the drug draws, which are these random linear mappings.",
                    "label": 0
                },
                {
                    "sent": "The output of the GP draws are the mean field parameters.",
                    "label": 1
                },
                {
                    "sent": "So if you just look at this thing a little bit so we can see that this variational Gaussian process is just an ensemble of mean field distributions, so the weights of these mean field of these individual mean field distributions are given by a Bayesian nonparametric prior.",
                    "label": 1
                },
                {
                    "sent": "And evaluating these GP draws at the same input C. In this is correlation between the outputs.",
                    "label": 0
                },
                {
                    "sent": "And the parameters of this variation Gaussian process are the data itself, because rather than observing data in the latent variable space, we're going to imagine a bunch of hypothetical data and want to learn where to situate these data points, just that it incurs the random nonlinear mappings the GPS draws at certain inputs and output pairs.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can also just think of this in terms of the general process.",
                    "label": 0
                },
                {
                    "sent": "So first we're going to draw the lead inputs, see from a standard normal, so just family status, simple distribution.",
                    "label": 0
                },
                {
                    "sent": "We're going to draw a nonlinear mapping condition on this fake datasets, and then we're going to draw mean field samples, approximately series samples conditional on the output of this GP draw.",
                    "label": 0
                },
                {
                    "sent": "So maybe you did not sign into it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we have this cool theorem which says that.",
                    "label": 0
                },
                {
                    "sent": "Given the variation Gaussian process, we can universally approximate any posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "So if you give me more and more parameters, if you feed the variation Gaussian process more and more data, it will converge to the posterior.",
                    "label": 0
                },
                {
                    "sent": "And this gives.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nicely, some statistical and computational tradeoff in inference algorithm, so we can grow the datasets.",
                    "label": 0
                },
                {
                    "sent": "This makes the inference algorithm slower, but enables richer approximation to the posterior.",
                    "label": 0
                },
                {
                    "sent": "So we've chosen only one family of distributions received.",
                    "label": 0
                },
                {
                    "sent": "The very strong calcium process and we just sat this set of the size of the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I won't go into the infra saglam too much, this is just the.",
                    "label": 0
                },
                {
                    "sent": "The lower bound in which we've derived the intuition behind this using variational autoencoders, is that it's just like the minimizing this negative reconstruction error and now have a sum of 2 Cal terms.",
                    "label": 1
                },
                {
                    "sent": "You have the original Cal term that's typically used in various non coders and then you have an additional one that is based on the variational prior the prior over C&F and you know XR redistribution.",
                    "label": 1
                },
                {
                    "sent": "So you're using a hierarchical variational inference procedure, a sense innocence where you have an auxiliary distribution.",
                    "label": 0
                },
                {
                    "sent": "That's used for the inference.",
                    "label": 0
                },
                {
                    "sent": "For inferring the.",
                    "label": 0
                },
                {
                    "sent": "Variational posterior if you will.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here is a simple example that I took from.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is using a Bayesian neural network.",
                    "label": 1
                },
                {
                    "sent": "It is trying to learn a neural network where the weights of this are given by a normal prior and it's trying to interpolate functions that go in between all of these inputs and outputs.",
                    "label": 0
                },
                {
                    "sent": "So this is just one dimensional and you can notice that it fits very well the.",
                    "label": 0
                },
                {
                    "sent": "The inside of this portion and then the outside represents all the uncertainty.",
                    "label": 0
                },
                {
                    "sent": "In other words, the function doesn't know very well how to predict regions outside of the support, and so there is very high uncertainty and this is the Gaussian process draws.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here is a very standard example or standard experiment that's used in unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "With generative models, you're showing to try to generate images for MNIST.",
                    "label": 0
                },
                {
                    "sent": "I've categorized this according to four different types of models.",
                    "label": 0
                },
                {
                    "sent": "The first one is just the baselines, be second one is the deeply in Gaussian model is just a generative model in which we have Gaussian random variables as the hidden units in the one layer case where you have 1 hidden layer for the deeply in Gaussian model.",
                    "label": 0
                },
                {
                    "sent": "We've used the very strong Gaussian process to infer this model and does the best similarly, if you use 2 hidden layers, the variance in process performs other competing methods normally, like the importance weighted autoencoders, Hamiltonian, virtual and friends normalizing flows, and we've also gotten state of the art using the various non Gaussian process to infer draw which is the sequential generative mechanism that was used in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what have we done?",
                    "label": 0
                },
                {
                    "sent": "As summary?",
                    "label": 0
                },
                {
                    "sent": "We've introduced the framework of original models.",
                    "label": 1
                },
                {
                    "sent": "It's this, I smell paper.",
                    "label": 0
                },
                {
                    "sent": "We've just got published.",
                    "label": 0
                },
                {
                    "sent": "We've developed the verification process, which is a universal approximator, and we've derived scalable inference for it.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, I just want to plug that.",
                    "label": 1
                },
                {
                    "sent": "We've released software for this as well as General Black Box inference and deep generative modeling.",
                    "label": 1
                },
                {
                    "sent": "It's open source software and we think that this will help enable researchers in both and also practitioners to get their feet wet with this procedure.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}