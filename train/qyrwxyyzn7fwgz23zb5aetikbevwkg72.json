{
    "id": "qyrwxyyzn7fwgz23zb5aetikbevwkg72",
    "title": "Spectral Clustering with Inconsistent Advice",
    "info": {
        "author": [
            "Tom Coleman, The University of Melbourne"
        ],
        "published": "Aug. 4, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/icml08_coleman_scwia/",
    "segmentation": [
        [
            "Hey, I'm well, I'm Tom Coleman and I'm going to be talking.",
            "About spatial clustering with inconsistent advice.",
            "This is joint work with James Saunderson and my supervisor Tony work were from the University of Melbourne, so it seems like this is the Australian talk then.",
            "So."
        ],
        [
            "When am I going to talk about?",
            "Well, to begin with, I'll tell you about what the problem is and you have to forgive me.",
            "I'm going to be very brief.",
            "But hopefully it should make sense to all of you.",
            "Then I'll tell you about our solution to the problem.",
            "And finally, I'll talk you through some experiments that we did relating to the problem.",
            "So just to begin with the problem.",
            "Basically we have two problems here.",
            "We have spectral clustering with inconsistent advice, so we have the first hand clustering problems and the second hand advice.",
            "And I'm going to talk to you about."
        ],
        [
            "OK, so to begin with."
        ],
        [
            "Clustering."
        ],
        [
            "So basically what we want to do is in this case.",
            "So."
        ],
        [
            "We want to form 2 clusters in this case, so we're only doing a binary clustering problem."
        ],
        [
            "And we just want to separate our data into."
        ],
        [
            "Two clusters this is a very meaningless little."
        ],
        [
            "Demonstration here and the information that we."
        ],
        [
            "I have is this affinity information."
        ],
        [
            "So we."
        ],
        [
            "I actually have features of the data, we just have affinity's which represents how much we want to place.",
            "No Dino Jade in the same class."
        ],
        [
            "So if it's a high affinity, they want to be in the same cluster, so we just have a matrix."
        ],
        [
            "And then how are we going to use this matrix to sort of rate a clustering and to say?",
            "Which class?"
        ],
        [
            "Ring is the best.",
            "If all we know is affinity."
        ],
        [
            "What we could do is just talk about the cut the cut cost, which is just the amount of affinity that is being separated by a clustering.",
            "Remembering we're only taking two clusters."
        ],
        [
            "But this has a problem 'cause the best clustering to do is just to put everything in one cluster and then you have no cut cost.",
            "Obviously this isn't what we want, so we need to somehow."
        ],
        [
            "Now change this measure to make big clusters bad and the way we're going to do that.",
            "Well, one way to do that is with the ratio cut, so we just did."
        ],
        [
            "I'd buy the size.",
            "Another way is the normalized cut and this involves not the size of the clusters, but the amount of affinity within the cluster is not going to talk about normalized color, though it is what we're interested in.",
            "I'm going to talk about ratio cost, 'cause it's slightly easier and it will make things easier for everyone.",
            "So that's.",
            "This is a clustering positive problem.",
            "We want to minimize the ratio cut.",
            "Oh, and the the best clustering for the cut costs where you put everything in one cluster.",
            "I'm going to refer to as the trivial solution.",
            "That's what we want one."
        ],
        [
            "Thing we want to avoid.",
            "The second problem that we're going to we're looking at here sort of simultaneously, is advice, so we might know something about more than."
        ],
        [
            "Affinity's we might know we might have some advice, but two points should be put in the same cluster or the two points should be."
        ],
        [
            "Put in different clusters.",
            "However, this advice might not be correct, so one way to look at it is to say that this advice is a constraint, as in our cluster has to satisfy the advice we need to make sure we do it, but we're looking at it as the advice is error prone, prone.",
            "Maybe it's the result of some experiments.",
            "I just can't move.",
            "And so maybe maybe we don't have to follow the."
        ],
        [
            "Plus it might be wrong, and if you sort of follow that through, it could also be."
        ],
        [
            "Inconsistent, so here's an example of some advice.",
            "A plus representing these two points should be in the same cluster, and a minus representing they should be in a different cluster.",
            "And this advice can't be satisfied, because what were the two plus is basically say all the points should be in the one cluster and the minus says the opposite.",
            "It says that the green and the blue point should be in different clusters.",
            "There's no point no way to satisfy all that advice.",
            "If you wanted to cluster these points, you'd have to violate one of these pieces of advice.",
            "There's three ways to do that, corresponding with the three ways we could violate the advice.",
            "Notice in this case that one of these.",
            "One of these cases where we don't satisfy the minus advice we put all the points in the one cluster, and that's not very good, so I guess we'll say that there's two solutions to."
        ],
        [
            "Probably.",
            "OK, so if we only had advice and we didn't have any affinity would be solving what we call the two correlation clustering prob."
        ],
        [
            "And that."
        ],
        [
            "Basically you have a whole lot of advice you want to form 2 clusters, and you basically want to violate as little as advice."
        ],
        [
            "Possible, so here's one clustering."
        ],
        [
            "You can see that the one plus edge."
        ],
        [
            "The crosses the clusters is violated, and 1 minus edges inside the clusters vial."
        ],
        [
            "So we've got a cost of two, and that's what the two CC cost is.",
            "Ask us to minimize that, so that's one."
        ],
        [
            "Problem we could look at as well.",
            "We're interested in both problems and solving them in some sensible."
        ],
        [
            "Why at the same time so in the left hand side, we've got our cut which deals with affinity's ratio cut on the right hand side.",
            "We've got two CC which is kind of like a graph problem."
        ],
        [
            "So how are we going to solve it?",
            "Well, we're going to base the solution on spectral clustering, which is a well known solution for solving the our cut problem.",
            "And then we're going to show how to constrain the spectrals solution, and we're going to use subspaces to do that.",
            "So then we're going to look for a way to convert.",
            "UH2CC problem or solutions to socc problem to subspaces.",
            "And then we're going to see how we."
        ],
        [
            "Generate those subspaces.",
            "So I'm going to give you a whirlwind tour of spectral clustering and leave out all the important details and only leave in the details that I care about so.",
            "If you haven't seen spectral clustering before, this might go right over your head, but if you have, hopefully you'll take the point that I'm interested in out, so recall that this is the ratio cut costs, just the amount of affinity across the gap."
        ],
        [
            "Divided by the size of the clusters.",
            "We can rewrite this in if we rewrite our clustering as a vector where each coordinate represents one of our data points.",
            "Ann has a plus or minus one value saying which cluster it's in.",
            "We can rewrite the our cut cost in this way.",
            "And basically what we have is distances between points representing which will be 0 if they are in the same cluster and one if they are in different clusters.",
            "So we do a coordinatewise sort of function of this vector and down here I've just got kind of like a picture of what's going on here.",
            "So suppose that there were three data points, which is obviously not very many.",
            "There are eight possible solutions if you represent a solution this way corresponding to the two different choices you have for each data point.",
            "And here's the combinatorial problem, an obviously it's exponential in the number of data points, and we're not going to be able to solve it by brute force or anything like.",
            "So how are we going to solve it?",
            "Well, let's just have a look at this.",
            "Our cut formula here.",
            "The important points to take from this formula are to begin with.",
            "It's defined for not just the coordinates of this vector being plus or minus ones.",
            "They could be anything and it would still make sense.",
            "Well, it would make sense for almost any, except for ones where VI is equal to Vijay in every single case, in which case there be easier on the top and the bottom, and we wouldn't be very happy.",
            "But that corresponds to the scenario that I was talking about before, which I called the trivial solution where we've given every coordinate which corresponds to every data point the same value which were saying they should all be in the same cluster.",
            "We're not allowed to do that.",
            "The other thing to notice is that in a sense this is scaling invariant.",
            "If we multiply this vector by any scalar so that each coordinate changes by the same scalar, then it's not going to change because all that matters here is a distance divided by distance, so that scale is going to fall out so."
        ],
        [
            "If we do relax the thing and let our value take any real value, not just these eight points.",
            "Then why we're going to get a whole lot of potential solutions?",
            "And because of this scaling invariant any point on a line.",
            "So the whole line is going to have the same ratio cut costs, so we may as well just not worry bout lines and just think about vectors of norm one.",
            "That's normally what we do with spectral clustering.",
            "Why is this an advantage?",
            "To do it this way, I mean, we still got like a huge dimensional problem here to find lines with relaxed solutions to the problem.",
            "We haven't got anywhere yet will be."
        ],
        [
            "Have because of some nice linear algebra, so I just vaguely mentioned this Rayleigh Ritz theorem, which basically says if we have a problem in this form Now, this isn't quite."
        ],
        [
            "The same form as before.",
            "We have distances here."
        ],
        [
            "In actual fact it is because if we let a here be a graph Laplacian then it is actually the same.",
            "So here's one of the details that I'm just going to not talk about at all.",
            "If we do have a problem with this form, we can solve it."
        ],
        [
            "And the way we can solve it is with an eigenvector.",
            "And the solution of this problem is going to be given by the smallest eigenvector of a.",
            "This is the Rayleigh Ritz theorem."
        ],
        [
            "The other important point to note is that the cost for a of this function is going to be the eigenvalue.",
            "That's why we're interested in the smallest eigenvector.",
            "By smallest I mean eigenvector with smaller study."
        ],
        [
            "Further, we can extend this really rich theorem.",
            "If we constrain the vector to be within some subspace rather than within the whole of RM.",
            "Then we can still solve the problem and we still get an eigenvector and the cost is still related to an eigenvalue.",
            "So this is what I call the extended rally with theorem and this is basic."
        ],
        [
            "What spectral clustering is all about?",
            "Cause spectral clustering is saying let's minimize this archive function which I'm telling you and you're going to take on faith that looks like one of those earlier things such that V is in the nontrivial subspace, so all the V coordinates are the same, which is a subspace.",
            "So we can apply the extended Rayleigh Ritz theorem and just get an eigenvector.",
            "And it turns out to be the second smallest eigenvalue of the local graph Laplacian, great.",
            "Now, the reason that we like this or a reason that we like this is that finding eigenvectors and values is something we can do pretty well.",
            "There's been a lot of work done in that."
        ],
        [
            "So that's great.",
            "Generally, we might ask a more more general spectral clustering problem, because we can see pretty much straight away that we can do this.",
            "Which is that?",
            "Let's minimize our cut so that V is in still in the nontrivial subspace.",
            "But it's also in some other subspace S. And this is where we're going to get our constraints from, because this S is going to somehow represent our advice."
        ],
        [
            "So this is.",
            "This is still as I said, it's still going to be an eigenvector, and it's and it's basically going to be the best spectral clustering solution in."
        ],
        [
            "Which is good.",
            "So this is our main idea.",
            "We're going to turn the 2CC problem into a subspace somehow and allow that to influence our spectral clustering.",
            "So."
        ],
        [
            "With this kind of you in mind.",
            "There's kind of what what what's called the subspace trick my some previous authors.",
            "Kind of in a way might seem like it comes out kind of.",
            "Obviously when you're looking at things this way, which is just."
        ],
        [
            "If you have one piece of advice, say V1, so the first data point should be in the same cluster as V2, or will just turn that into the subspace where V1 is the same as V2.",
            "If they should be in different clusters, will turn that into the subspace where everyone is the negative of V2.",
            "I haven't talked at all about how we turn these vectors in RN into actual solutions to the problem because we can't just give a vector of the solution, we need a combinatorial solution.",
            "This is a rounding issue and I'm just going to.",
            "Completely brushed that off but.",
            "Assuming most roundings, this V1 equals V1 equals V2 is going to ensure that they are in the same cluster.",
            "Whatever rounding you use and most roundings, V1 is negative, V2 mean they'll probably be in the same in different clusters, so this will work.",
            "But why is it?"
        ],
        [
            "No good for our problem well.",
            "So so we could take so we could take all our piece of advice which are edges on this kind of two CC graph, turn them all into subspaces, take the intersection of all those subspaces and say that's our S. That's what."
        ],
        [
            "Going to search inside.",
            "This is going, so here's an example of this.",
            "So this picture is very esoteric, but.",
            "The idea here is going to be that the green subspaces the nontrivial subspace, the red plane, is the subspace where V1 equals V2.",
            "We're looking in the intersection because we've only got 3 coordinates, is just going to be a line, and this is going to be in for spectral clustering solution.",
            "So in this case, spectral clustering is not going to do anything, it's just going to find that line, and that's going to be the line V1 equals V2 and their different from V3, because of course if the if the three was equal to them.",
            "Then we'd be in the trivial subspace, but we're in a nontrivial subspace."
        ],
        [
            "So great.",
            "So.",
            "Well, why can't we do?",
            "Why can't we just do the subspace trick?",
            "Well, the reason is that we have inconsistent advice.",
            "So if you have three pieces of inconsistent advice, the three subspaces that they generate are going to intersect at 0.",
            "There's going to be no.",
            "Real spectral clustering solutions in the intersection that's that follows pretty obviously from the fact that they are inconsistent.",
            "So we've got to do something a little bit smarter and method one is basic."
        ],
        [
            "Kind of the most obvious thing to do.",
            "And the most obvious thing to do is just, well, look.",
            "We've got inconsistent advice.",
            "Let's just get rid of some of the advice.",
            "So we'll solve the two correlation clustering prob."
        ],
        [
            "So here's the two correlation clustering problem from before, and we know that if we delete the two red lines, we're going to have something."
        ],
        [
            "Consistent so bang, let's delete them and then we can just plug this straight into a subspace, find a subspace that works for that and then solve 2 CC in this case.",
            "So this whole spectral in this case because it's fully specified every single node is involved.",
            "Spectral is not going to be able to do anything.",
            "It's going to have to follow the two CC solution.",
            "So this."
        ],
        [
            "This method one.",
            "And but there's a problem with method one.",
            "Well, the first problem is that two CC is a NP complete problem, so finding the best 2CC solution isn't realistic.",
            "But there's a bigger problem I guess, and that's the they may not just be one best solution to two CC.",
            "Now we have to kind of specify our best solution when we start deleting edges.",
            "An this little example, which is the same triangle from before but with some affinity is now made up.",
            "We just solve 2CC without even looking at the affinity's.",
            "We may very well choose something we have two.",
            "Remember we had two alternatives before, but without going into detail one of them is going to have a lot better affinity.",
            "Kind of our cut solution than the other.",
            "But if we just do 2CC without looking at the affinity's, we're going to pick the worst one.",
            "Or we might pick the worst one so that."
        ],
        [
            "Maybe not such a good idea.",
            "A slightly better idea, we call method two, is maybe let's find all the best solutions of two CC.",
            "So in the previous case there would have been two of them, and each of them could be a subspace.",
            "So it can be converted by a conversion technique into a subspace subspace trick.",
            "Each of them can be a subspace.",
            "And now let's search inside all of those subspaces so we can take a bigger subspace that contains both those subspaces."
        ],
        [
            "We can do that, but there's a potential problem here that we haven't really even talked about yet, which is basically that.",
            "We're talking about a buying advice and trying to obey the advice as well as possible, but we know the advice is inconsistent and that can only happen if the advice is not perfect.",
            "So why do we have to follow the advice as well as possible?",
            "I mean maybe the advice isn't very good.",
            "So the idea here is maybe we should."
        ],
        [
            "Not just look at the best to see solutions in Method 3.",
            "Maybe we should look at some two SQL solutions that whilst not being the best, are close to the bear.",
            "And so method three kind of really vaguely is.",
            "Let's find all clusterings that have two CC cost within F factor of F of the best known cost and will make a subspace that contains all of them.",
            "And then we'll search inside that subspace with the spectral solution.",
            "But there's a problem here.",
            "I mean, we can't even solve 2 CC, let alone find all the best solutions and all the best solutions."
        ],
        [
            "But in some factor.",
            "So how are we going to do this?",
            "So the way we're going to do this is using a spectral relaxation of two CC.",
            "So basically you can express to the two CC costs in a similar way to the archive cost an.",
            "You can do some magic, which I will not do now and get a matrix similar to the graph Laplacian before, which represents the two CC problem."
        ],
        [
            "And you can find solutions for two CC in a relaxed way."
        ],
        [
            "OK.",
            "In the same way, by finding eigenvectors of a matrix and the eigenvalue again is going to represent the eigenvalue of the eigenvector is going to be the two CC cost.",
            "So if we order all the eigenvectors of this matrix, which I haven't even told you what it is, we order all the eigenvectors of this matrix by eigenvalue.",
            "Then we know that the first one with the lowest eigenvalue is the best to relax to CCS 2CC solution.",
            "There is and as we sort of go to the right we're getting slowly worse in 2C."
        ],
        [
            "A solution.",
            "So if we have a factor of F."
        ],
        [
            "We can take all the look at all the eigenvalues in order and take them up to F times the best and then we can take all those corresponding eigenvectors which correspond to subspaces.",
            "We can take the intersection of all those subspaces and we can do spectral inside that subspace and this is going to be alright.",
            "This is our algorithm.",
            "So just to."
        ],
        [
            "Run through the whole thing from the beginning, so this is our method."
        ],
        [
            "Free.",
            "First we have to choose the F factor.",
            "I haven't said how to do that.",
            "Then we're going to use the relaxed version of two CC to convert the advice into a subspace that satisfies two CC up to F. Then we're going to solve spectral clustering inside S. Finally, we're going around again.",
            "I haven't said how we're going around, but that's fine.",
            "That's there.",
            "Almost a separate issue.",
            "So this is our solution method."
        ],
        [
            "Now let's have a look at."
        ],
        [
            "How we do?",
            "So what we took with some standard UCI by?"
        ],
        [
            "Classification data sets.",
            "We took the class labels away.",
            "One point to note is that the cluster sizes aren't particularly uniform of the two clusters.",
            "One might be quite a bit bigger in a way this is a good thing because it makes the ratio cut spectral problem a little bit well, that's actually end cut that we're doing spectral problem a little bit harder.",
            "We want both problems to be hard, so that combining them we can do better than each of them alone.",
            "I mean, if each of them alone does very well, then there's not much point in coming."
        ],
        [
            "Then we generated by synthetically angest probabilistically, so we decided we want some advice between two points.",
            "We look at the class labels and with some probability we independently either.",
            "Go with the correct advice that we flip."
        ],
        [
            "OK, and there's two types of advice that we did, corresponding to kind of to the ways that we know about how to solve to see one of them is complete advice where we have a piece of advice between every pair of points, so sort of order N squared advice.",
            "And there are some heuristic algorithms to solve.",
            "This is certainly not going to be sold by any by the method that we use."
        ],
        [
            "All the other advice, so the other advice is dense advice.",
            "We take small groups of data points and we generate really dense advice within those groups.",
            "We don't generate any other advice.",
            "Those groups, or those blocks of data points can then be from a 2CC's perspective.",
            "Can be considered by themselves and we can use high powered approximation algorithms to solve to CC on them and hopefully get pretty good to see see."
        ],
        [
            "Solution.",
            "So here's a picture of.",
            "Some results.",
            "Um?",
            "On the Y axis we have the accuracy, which is just how close we get to the actual labeling.",
            "And the points to note of interest here is the solid red line is spectral clustering alone.",
            "So just throw the advice away.",
            "Let's not care about the the advice and the dashed red line is corresponding to complete some method method, one which is basically completely solved to CC as well as possible and then use that in the spectral clustering.",
            "So in this case where it's dense advice, when we have blocks of advice, those blocks are going to be completely specified by the two CC solution.",
            "And the blue dots here represent our solution so.",
            "The blue dot on the left hand axis.",
            "Is when we choose an F value of one, we take only the best eigenvector eigenvector for the two CC relax problem as F increases were taking more and more eigenvectors with making the space that spectral allowed to search in to get bigger and bigger but have worse and worse two CC costs.",
            "So as we as we ask them to kind of Infinity when F when the F value gets as large as possible, that means the whole nontrivial subspace are.",
            "Unsurprisingly, we end up on the red line.",
            "And you can see that we actually do a lot better than either of the solutions alone by combining them.",
            "And for a large range of F."
        ],
        [
            "I live.",
            "Here's another one with the complete advice.",
            "The interesting.",
            "The reason I'm showing in this picture.",
            "And the point of interest is that the spectral, I mean the two CC solution which is the dashed red line alone, is doing terribly here.",
            ".5 is the worst you can do, really.",
            "And you can say that even though the advice by itself is pretty much useless, well, you think so.",
            "Given the two CC.",
            "Cost when we actually combine it with spectral.",
            "We actually for some ranges of F get as high as we did before.",
            "So in this picture in the solid red line is the same in both cases, 'cause we're throwing the advice away.",
            "The dashed red line is heaps lower here, yet we're able to achieve pretty similar height at some."
        ],
        [
            "Point of F. So just to summarize.",
            "We found some problems.",
            "Well, we've shown that there are some problems where a combination of advice and spectral clustering is better than each technique."
        ],
        [
            "So.",
            "And we've we've demonstrated an approach in order to combine them, and basically that's by turning advice into subspaces and using multiple solutions to the two CC problem to generate multiple subspaces."
        ],
        [
            "We've shown kind of 1 algorithm to use."
        ],
        [
            "But I mean really, you could use other way if you could have some other way some other algorithm that generated sort of a chain of subspaces corresponding to two CC solutions, you could plug that in the two CC spectral two CC algorithm was the natural thing to use, but maybe."
        ],
        [
            "There is other things that could work.",
            "Also, we haven't.",
            "As I said, I didn't say anything about how to choose F, although we did see there were big ranges of F worth."
        ],
        [
            "We're going well.",
            "And is there an extension to more than two clusters is really odd?"
        ],
        [
            "This question to ask finally I just like to acknowledge in Davidson for suggesting this work to us.",
            "Try.",
            "Laugh.",
            "Right, right I mean.",
            "I. I guess I guess the issue here is the inconsistency of the advice.",
            "I mean, if you take a.",
            "A semi supervised clustering problem and you generate the advice that you that you can infer from your supervised examples.",
            "You're going to get consistent advisor if basically it's going to be constraints, not advice.",
            "So, so it's not really a fair comparison to the algorithm which is designed to work in the case where the advice is potentially incorrect.",
            "Yeah, I was particularly thinking about one of the papers by Jean Bushy, and Stella Zoo was kind of very similar setup.",
            "Yeah, I mean, what's the relationship there?",
            "So this I'm not sure what the specific paper is, so normalized cuts and quality constraints.",
            "Right?",
            "I think I think in that case they are actually.",
            "Changing the affinity matrix using the advice.",
            "Which.",
            "Which is, yeah, I mean, I guess it's different because it's kind of trading the advice and the Infinity kind of.",
            "Almost trading the advice as Affinity, which I guess has some problems because.",
            "As.",
            "The advice isn't, for instance, is not metric in this case, and so you have to do some kind of fudge ING to make it work, and for instance, if the affinity gets over one, you just cut it off at one and it's not particularly nice.",
            "In my opinion, this please."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, I'm well, I'm Tom Coleman and I'm going to be talking.",
                    "label": 0
                },
                {
                    "sent": "About spatial clustering with inconsistent advice.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with James Saunderson and my supervisor Tony work were from the University of Melbourne, so it seems like this is the Australian talk then.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When am I going to talk about?",
                    "label": 0
                },
                {
                    "sent": "Well, to begin with, I'll tell you about what the problem is and you have to forgive me.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be very brief.",
                    "label": 0
                },
                {
                    "sent": "But hopefully it should make sense to all of you.",
                    "label": 0
                },
                {
                    "sent": "Then I'll tell you about our solution to the problem.",
                    "label": 1
                },
                {
                    "sent": "And finally, I'll talk you through some experiments that we did relating to the problem.",
                    "label": 0
                },
                {
                    "sent": "So just to begin with the problem.",
                    "label": 1
                },
                {
                    "sent": "Basically we have two problems here.",
                    "label": 0
                },
                {
                    "sent": "We have spectral clustering with inconsistent advice, so we have the first hand clustering problems and the second hand advice.",
                    "label": 1
                },
                {
                    "sent": "And I'm going to talk to you about.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to begin with.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clustering.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what we want to do is in this case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to form 2 clusters in this case, so we're only doing a binary clustering problem.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we just want to separate our data into.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two clusters this is a very meaningless little.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Demonstration here and the information that we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have is this affinity information.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I actually have features of the data, we just have affinity's which represents how much we want to place.",
                    "label": 0
                },
                {
                    "sent": "No Dino Jade in the same class.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if it's a high affinity, they want to be in the same cluster, so we just have a matrix.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then how are we going to use this matrix to sort of rate a clustering and to say?",
                    "label": 0
                },
                {
                    "sent": "Which class?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ring is the best.",
                    "label": 0
                },
                {
                    "sent": "If all we know is affinity.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we could do is just talk about the cut the cut cost, which is just the amount of affinity that is being separated by a clustering.",
                    "label": 0
                },
                {
                    "sent": "Remembering we're only taking two clusters.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this has a problem 'cause the best clustering to do is just to put everything in one cluster and then you have no cut cost.",
                    "label": 0
                },
                {
                    "sent": "Obviously this isn't what we want, so we need to somehow.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now change this measure to make big clusters bad and the way we're going to do that.",
                    "label": 0
                },
                {
                    "sent": "Well, one way to do that is with the ratio cut, so we just did.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'd buy the size.",
                    "label": 0
                },
                {
                    "sent": "Another way is the normalized cut and this involves not the size of the clusters, but the amount of affinity within the cluster is not going to talk about normalized color, though it is what we're interested in.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about ratio cost, 'cause it's slightly easier and it will make things easier for everyone.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "This is a clustering positive problem.",
                    "label": 1
                },
                {
                    "sent": "We want to minimize the ratio cut.",
                    "label": 0
                },
                {
                    "sent": "Oh, and the the best clustering for the cut costs where you put everything in one cluster.",
                    "label": 1
                },
                {
                    "sent": "I'm going to refer to as the trivial solution.",
                    "label": 0
                },
                {
                    "sent": "That's what we want one.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing we want to avoid.",
                    "label": 0
                },
                {
                    "sent": "The second problem that we're going to we're looking at here sort of simultaneously, is advice, so we might know something about more than.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Affinity's we might know we might have some advice, but two points should be put in the same cluster or the two points should be.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Put in different clusters.",
                    "label": 0
                },
                {
                    "sent": "However, this advice might not be correct, so one way to look at it is to say that this advice is a constraint, as in our cluster has to satisfy the advice we need to make sure we do it, but we're looking at it as the advice is error prone, prone.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's the result of some experiments.",
                    "label": 0
                },
                {
                    "sent": "I just can't move.",
                    "label": 0
                },
                {
                    "sent": "And so maybe maybe we don't have to follow the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Plus it might be wrong, and if you sort of follow that through, it could also be.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inconsistent, so here's an example of some advice.",
                    "label": 0
                },
                {
                    "sent": "A plus representing these two points should be in the same cluster, and a minus representing they should be in a different cluster.",
                    "label": 1
                },
                {
                    "sent": "And this advice can't be satisfied, because what were the two plus is basically say all the points should be in the one cluster and the minus says the opposite.",
                    "label": 0
                },
                {
                    "sent": "It says that the green and the blue point should be in different clusters.",
                    "label": 0
                },
                {
                    "sent": "There's no point no way to satisfy all that advice.",
                    "label": 0
                },
                {
                    "sent": "If you wanted to cluster these points, you'd have to violate one of these pieces of advice.",
                    "label": 0
                },
                {
                    "sent": "There's three ways to do that, corresponding with the three ways we could violate the advice.",
                    "label": 0
                },
                {
                    "sent": "Notice in this case that one of these.",
                    "label": 0
                },
                {
                    "sent": "One of these cases where we don't satisfy the minus advice we put all the points in the one cluster, and that's not very good, so I guess we'll say that there's two solutions to.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probably.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we only had advice and we didn't have any affinity would be solving what we call the two correlation clustering prob.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically you have a whole lot of advice you want to form 2 clusters, and you basically want to violate as little as advice.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possible, so here's one clustering.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can see that the one plus edge.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The crosses the clusters is violated, and 1 minus edges inside the clusters vial.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we've got a cost of two, and that's what the two CC cost is.",
                    "label": 0
                },
                {
                    "sent": "Ask us to minimize that, so that's one.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem we could look at as well.",
                    "label": 0
                },
                {
                    "sent": "We're interested in both problems and solving them in some sensible.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why at the same time so in the left hand side, we've got our cut which deals with affinity's ratio cut on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "We've got two CC which is kind of like a graph problem.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how are we going to solve it?",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to base the solution on spectral clustering, which is a well known solution for solving the our cut problem.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to show how to constrain the spectrals solution, and we're going to use subspaces to do that.",
                    "label": 0
                },
                {
                    "sent": "So then we're going to look for a way to convert.",
                    "label": 1
                },
                {
                    "sent": "UH2CC problem or solutions to socc problem to subspaces.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to see how we.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generate those subspaces.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give you a whirlwind tour of spectral clustering and leave out all the important details and only leave in the details that I care about so.",
                    "label": 1
                },
                {
                    "sent": "If you haven't seen spectral clustering before, this might go right over your head, but if you have, hopefully you'll take the point that I'm interested in out, so recall that this is the ratio cut costs, just the amount of affinity across the gap.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Divided by the size of the clusters.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite this in if we rewrite our clustering as a vector where each coordinate represents one of our data points.",
                    "label": 0
                },
                {
                    "sent": "Ann has a plus or minus one value saying which cluster it's in.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite the our cut cost in this way.",
                    "label": 0
                },
                {
                    "sent": "And basically what we have is distances between points representing which will be 0 if they are in the same cluster and one if they are in different clusters.",
                    "label": 0
                },
                {
                    "sent": "So we do a coordinatewise sort of function of this vector and down here I've just got kind of like a picture of what's going on here.",
                    "label": 0
                },
                {
                    "sent": "So suppose that there were three data points, which is obviously not very many.",
                    "label": 0
                },
                {
                    "sent": "There are eight possible solutions if you represent a solution this way corresponding to the two different choices you have for each data point.",
                    "label": 0
                },
                {
                    "sent": "And here's the combinatorial problem, an obviously it's exponential in the number of data points, and we're not going to be able to solve it by brute force or anything like.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to solve it?",
                    "label": 0
                },
                {
                    "sent": "Well, let's just have a look at this.",
                    "label": 0
                },
                {
                    "sent": "Our cut formula here.",
                    "label": 0
                },
                {
                    "sent": "The important points to take from this formula are to begin with.",
                    "label": 0
                },
                {
                    "sent": "It's defined for not just the coordinates of this vector being plus or minus ones.",
                    "label": 0
                },
                {
                    "sent": "They could be anything and it would still make sense.",
                    "label": 0
                },
                {
                    "sent": "Well, it would make sense for almost any, except for ones where VI is equal to Vijay in every single case, in which case there be easier on the top and the bottom, and we wouldn't be very happy.",
                    "label": 0
                },
                {
                    "sent": "But that corresponds to the scenario that I was talking about before, which I called the trivial solution where we've given every coordinate which corresponds to every data point the same value which were saying they should all be in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "We're not allowed to do that.",
                    "label": 0
                },
                {
                    "sent": "The other thing to notice is that in a sense this is scaling invariant.",
                    "label": 0
                },
                {
                    "sent": "If we multiply this vector by any scalar so that each coordinate changes by the same scalar, then it's not going to change because all that matters here is a distance divided by distance, so that scale is going to fall out so.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we do relax the thing and let our value take any real value, not just these eight points.",
                    "label": 0
                },
                {
                    "sent": "Then why we're going to get a whole lot of potential solutions?",
                    "label": 0
                },
                {
                    "sent": "And because of this scaling invariant any point on a line.",
                    "label": 0
                },
                {
                    "sent": "So the whole line is going to have the same ratio cut costs, so we may as well just not worry bout lines and just think about vectors of norm one.",
                    "label": 0
                },
                {
                    "sent": "That's normally what we do with spectral clustering.",
                    "label": 0
                },
                {
                    "sent": "Why is this an advantage?",
                    "label": 0
                },
                {
                    "sent": "To do it this way, I mean, we still got like a huge dimensional problem here to find lines with relaxed solutions to the problem.",
                    "label": 0
                },
                {
                    "sent": "We haven't got anywhere yet will be.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have because of some nice linear algebra, so I just vaguely mentioned this Rayleigh Ritz theorem, which basically says if we have a problem in this form Now, this isn't quite.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The same form as before.",
                    "label": 0
                },
                {
                    "sent": "We have distances here.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In actual fact it is because if we let a here be a graph Laplacian then it is actually the same.",
                    "label": 0
                },
                {
                    "sent": "So here's one of the details that I'm just going to not talk about at all.",
                    "label": 1
                },
                {
                    "sent": "If we do have a problem with this form, we can solve it.",
                    "label": 1
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the way we can solve it is with an eigenvector.",
                    "label": 0
                },
                {
                    "sent": "And the solution of this problem is going to be given by the smallest eigenvector of a.",
                    "label": 1
                },
                {
                    "sent": "This is the Rayleigh Ritz theorem.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other important point to note is that the cost for a of this function is going to be the eigenvalue.",
                    "label": 1
                },
                {
                    "sent": "That's why we're interested in the smallest eigenvector.",
                    "label": 1
                },
                {
                    "sent": "By smallest I mean eigenvector with smaller study.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Further, we can extend this really rich theorem.",
                    "label": 1
                },
                {
                    "sent": "If we constrain the vector to be within some subspace rather than within the whole of RM.",
                    "label": 0
                },
                {
                    "sent": "Then we can still solve the problem and we still get an eigenvector and the cost is still related to an eigenvalue.",
                    "label": 1
                },
                {
                    "sent": "So this is what I call the extended rally with theorem and this is basic.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What spectral clustering is all about?",
                    "label": 1
                },
                {
                    "sent": "Cause spectral clustering is saying let's minimize this archive function which I'm telling you and you're going to take on faith that looks like one of those earlier things such that V is in the nontrivial subspace, so all the V coordinates are the same, which is a subspace.",
                    "label": 1
                },
                {
                    "sent": "So we can apply the extended Rayleigh Ritz theorem and just get an eigenvector.",
                    "label": 1
                },
                {
                    "sent": "And it turns out to be the second smallest eigenvalue of the local graph Laplacian, great.",
                    "label": 0
                },
                {
                    "sent": "Now, the reason that we like this or a reason that we like this is that finding eigenvectors and values is something we can do pretty well.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of work done in that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's great.",
                    "label": 0
                },
                {
                    "sent": "Generally, we might ask a more more general spectral clustering problem, because we can see pretty much straight away that we can do this.",
                    "label": 1
                },
                {
                    "sent": "Which is that?",
                    "label": 1
                },
                {
                    "sent": "Let's minimize our cut so that V is in still in the nontrivial subspace.",
                    "label": 1
                },
                {
                    "sent": "But it's also in some other subspace S. And this is where we're going to get our constraints from, because this S is going to somehow represent our advice.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is still as I said, it's still going to be an eigenvector, and it's and it's basically going to be the best spectral clustering solution in.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is good.",
                    "label": 0
                },
                {
                    "sent": "So this is our main idea.",
                    "label": 1
                },
                {
                    "sent": "We're going to turn the 2CC problem into a subspace somehow and allow that to influence our spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With this kind of you in mind.",
                    "label": 0
                },
                {
                    "sent": "There's kind of what what what's called the subspace trick my some previous authors.",
                    "label": 1
                },
                {
                    "sent": "Kind of in a way might seem like it comes out kind of.",
                    "label": 0
                },
                {
                    "sent": "Obviously when you're looking at things this way, which is just.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you have one piece of advice, say V1, so the first data point should be in the same cluster as V2, or will just turn that into the subspace where V1 is the same as V2.",
                    "label": 1
                },
                {
                    "sent": "If they should be in different clusters, will turn that into the subspace where everyone is the negative of V2.",
                    "label": 0
                },
                {
                    "sent": "I haven't talked at all about how we turn these vectors in RN into actual solutions to the problem because we can't just give a vector of the solution, we need a combinatorial solution.",
                    "label": 1
                },
                {
                    "sent": "This is a rounding issue and I'm just going to.",
                    "label": 0
                },
                {
                    "sent": "Completely brushed that off but.",
                    "label": 0
                },
                {
                    "sent": "Assuming most roundings, this V1 equals V1 equals V2 is going to ensure that they are in the same cluster.",
                    "label": 0
                },
                {
                    "sent": "Whatever rounding you use and most roundings, V1 is negative, V2 mean they'll probably be in the same in different clusters, so this will work.",
                    "label": 0
                },
                {
                    "sent": "But why is it?",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No good for our problem well.",
                    "label": 0
                },
                {
                    "sent": "So so we could take so we could take all our piece of advice which are edges on this kind of two CC graph, turn them all into subspaces, take the intersection of all those subspaces and say that's our S. That's what.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going to search inside.",
                    "label": 0
                },
                {
                    "sent": "This is going, so here's an example of this.",
                    "label": 1
                },
                {
                    "sent": "So this picture is very esoteric, but.",
                    "label": 0
                },
                {
                    "sent": "The idea here is going to be that the green subspaces the nontrivial subspace, the red plane, is the subspace where V1 equals V2.",
                    "label": 1
                },
                {
                    "sent": "We're looking in the intersection because we've only got 3 coordinates, is just going to be a line, and this is going to be in for spectral clustering solution.",
                    "label": 0
                },
                {
                    "sent": "So in this case, spectral clustering is not going to do anything, it's just going to find that line, and that's going to be the line V1 equals V2 and their different from V3, because of course if the if the three was equal to them.",
                    "label": 0
                },
                {
                    "sent": "Then we'd be in the trivial subspace, but we're in a nontrivial subspace.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So great.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Well, why can't we do?",
                    "label": 0
                },
                {
                    "sent": "Why can't we just do the subspace trick?",
                    "label": 0
                },
                {
                    "sent": "Well, the reason is that we have inconsistent advice.",
                    "label": 1
                },
                {
                    "sent": "So if you have three pieces of inconsistent advice, the three subspaces that they generate are going to intersect at 0.",
                    "label": 0
                },
                {
                    "sent": "There's going to be no.",
                    "label": 1
                },
                {
                    "sent": "Real spectral clustering solutions in the intersection that's that follows pretty obviously from the fact that they are inconsistent.",
                    "label": 1
                },
                {
                    "sent": "So we've got to do something a little bit smarter and method one is basic.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of the most obvious thing to do.",
                    "label": 1
                },
                {
                    "sent": "And the most obvious thing to do is just, well, look.",
                    "label": 0
                },
                {
                    "sent": "We've got inconsistent advice.",
                    "label": 0
                },
                {
                    "sent": "Let's just get rid of some of the advice.",
                    "label": 1
                },
                {
                    "sent": "So we'll solve the two correlation clustering prob.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the two correlation clustering problem from before, and we know that if we delete the two red lines, we're going to have something.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consistent so bang, let's delete them and then we can just plug this straight into a subspace, find a subspace that works for that and then solve 2 CC in this case.",
                    "label": 0
                },
                {
                    "sent": "So this whole spectral in this case because it's fully specified every single node is involved.",
                    "label": 0
                },
                {
                    "sent": "Spectral is not going to be able to do anything.",
                    "label": 0
                },
                {
                    "sent": "It's going to have to follow the two CC solution.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This method one.",
                    "label": 0
                },
                {
                    "sent": "And but there's a problem with method one.",
                    "label": 0
                },
                {
                    "sent": "Well, the first problem is that two CC is a NP complete problem, so finding the best 2CC solution isn't realistic.",
                    "label": 0
                },
                {
                    "sent": "But there's a bigger problem I guess, and that's the they may not just be one best solution to two CC.",
                    "label": 0
                },
                {
                    "sent": "Now we have to kind of specify our best solution when we start deleting edges.",
                    "label": 0
                },
                {
                    "sent": "An this little example, which is the same triangle from before but with some affinity is now made up.",
                    "label": 0
                },
                {
                    "sent": "We just solve 2CC without even looking at the affinity's.",
                    "label": 0
                },
                {
                    "sent": "We may very well choose something we have two.",
                    "label": 0
                },
                {
                    "sent": "Remember we had two alternatives before, but without going into detail one of them is going to have a lot better affinity.",
                    "label": 0
                },
                {
                    "sent": "Kind of our cut solution than the other.",
                    "label": 0
                },
                {
                    "sent": "But if we just do 2CC without looking at the affinity's, we're going to pick the worst one.",
                    "label": 0
                },
                {
                    "sent": "Or we might pick the worst one so that.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe not such a good idea.",
                    "label": 0
                },
                {
                    "sent": "A slightly better idea, we call method two, is maybe let's find all the best solutions of two CC.",
                    "label": 1
                },
                {
                    "sent": "So in the previous case there would have been two of them, and each of them could be a subspace.",
                    "label": 0
                },
                {
                    "sent": "So it can be converted by a conversion technique into a subspace subspace trick.",
                    "label": 1
                },
                {
                    "sent": "Each of them can be a subspace.",
                    "label": 0
                },
                {
                    "sent": "And now let's search inside all of those subspaces so we can take a bigger subspace that contains both those subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can do that, but there's a potential problem here that we haven't really even talked about yet, which is basically that.",
                    "label": 0
                },
                {
                    "sent": "We're talking about a buying advice and trying to obey the advice as well as possible, but we know the advice is inconsistent and that can only happen if the advice is not perfect.",
                    "label": 0
                },
                {
                    "sent": "So why do we have to follow the advice as well as possible?",
                    "label": 0
                },
                {
                    "sent": "I mean maybe the advice isn't very good.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is maybe we should.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not just look at the best to see solutions in Method 3.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should look at some two SQL solutions that whilst not being the best, are close to the bear.",
                    "label": 0
                },
                {
                    "sent": "And so method three kind of really vaguely is.",
                    "label": 1
                },
                {
                    "sent": "Let's find all clusterings that have two CC cost within F factor of F of the best known cost and will make a subspace that contains all of them.",
                    "label": 1
                },
                {
                    "sent": "And then we'll search inside that subspace with the spectral solution.",
                    "label": 0
                },
                {
                    "sent": "But there's a problem here.",
                    "label": 0
                },
                {
                    "sent": "I mean, we can't even solve 2 CC, let alone find all the best solutions and all the best solutions.",
                    "label": 1
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in some factor.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to do this?",
                    "label": 0
                },
                {
                    "sent": "So the way we're going to do this is using a spectral relaxation of two CC.",
                    "label": 1
                },
                {
                    "sent": "So basically you can express to the two CC costs in a similar way to the archive cost an.",
                    "label": 0
                },
                {
                    "sent": "You can do some magic, which I will not do now and get a matrix similar to the graph Laplacian before, which represents the two CC problem.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can find solutions for two CC in a relaxed way.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In the same way, by finding eigenvectors of a matrix and the eigenvalue again is going to represent the eigenvalue of the eigenvector is going to be the two CC cost.",
                    "label": 1
                },
                {
                    "sent": "So if we order all the eigenvectors of this matrix, which I haven't even told you what it is, we order all the eigenvectors of this matrix by eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Then we know that the first one with the lowest eigenvalue is the best to relax to CCS 2CC solution.",
                    "label": 1
                },
                {
                    "sent": "There is and as we sort of go to the right we're getting slowly worse in 2C.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A solution.",
                    "label": 0
                },
                {
                    "sent": "So if we have a factor of F.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can take all the look at all the eigenvalues in order and take them up to F times the best and then we can take all those corresponding eigenvectors which correspond to subspaces.",
                    "label": 1
                },
                {
                    "sent": "We can take the intersection of all those subspaces and we can do spectral inside that subspace and this is going to be alright.",
                    "label": 0
                },
                {
                    "sent": "This is our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So just to.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run through the whole thing from the beginning, so this is our method.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Free.",
                    "label": 0
                },
                {
                    "sent": "First we have to choose the F factor.",
                    "label": 1
                },
                {
                    "sent": "I haven't said how to do that.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to use the relaxed version of two CC to convert the advice into a subspace that satisfies two CC up to F. Then we're going to solve spectral clustering inside S. Finally, we're going around again.",
                    "label": 1
                },
                {
                    "sent": "I haven't said how we're going around, but that's fine.",
                    "label": 0
                },
                {
                    "sent": "That's there.",
                    "label": 0
                },
                {
                    "sent": "Almost a separate issue.",
                    "label": 1
                },
                {
                    "sent": "So this is our solution method.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's have a look at.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How we do?",
                    "label": 0
                },
                {
                    "sent": "So what we took with some standard UCI by?",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification data sets.",
                    "label": 0
                },
                {
                    "sent": "We took the class labels away.",
                    "label": 1
                },
                {
                    "sent": "One point to note is that the cluster sizes aren't particularly uniform of the two clusters.",
                    "label": 0
                },
                {
                    "sent": "One might be quite a bit bigger in a way this is a good thing because it makes the ratio cut spectral problem a little bit well, that's actually end cut that we're doing spectral problem a little bit harder.",
                    "label": 0
                },
                {
                    "sent": "We want both problems to be hard, so that combining them we can do better than each of them alone.",
                    "label": 0
                },
                {
                    "sent": "I mean, if each of them alone does very well, then there's not much point in coming.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we generated by synthetically angest probabilistically, so we decided we want some advice between two points.",
                    "label": 0
                },
                {
                    "sent": "We look at the class labels and with some probability we independently either.",
                    "label": 0
                },
                {
                    "sent": "Go with the correct advice that we flip.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and there's two types of advice that we did, corresponding to kind of to the ways that we know about how to solve to see one of them is complete advice where we have a piece of advice between every pair of points, so sort of order N squared advice.",
                    "label": 0
                },
                {
                    "sent": "And there are some heuristic algorithms to solve.",
                    "label": 0
                },
                {
                    "sent": "This is certainly not going to be sold by any by the method that we use.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the other advice, so the other advice is dense advice.",
                    "label": 0
                },
                {
                    "sent": "We take small groups of data points and we generate really dense advice within those groups.",
                    "label": 0
                },
                {
                    "sent": "We don't generate any other advice.",
                    "label": 0
                },
                {
                    "sent": "Those groups, or those blocks of data points can then be from a 2CC's perspective.",
                    "label": 0
                },
                {
                    "sent": "Can be considered by themselves and we can use high powered approximation algorithms to solve to CC on them and hopefully get pretty good to see see.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "So here's a picture of.",
                    "label": 0
                },
                {
                    "sent": "Some results.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "On the Y axis we have the accuracy, which is just how close we get to the actual labeling.",
                    "label": 0
                },
                {
                    "sent": "And the points to note of interest here is the solid red line is spectral clustering alone.",
                    "label": 1
                },
                {
                    "sent": "So just throw the advice away.",
                    "label": 0
                },
                {
                    "sent": "Let's not care about the the advice and the dashed red line is corresponding to complete some method method, one which is basically completely solved to CC as well as possible and then use that in the spectral clustering.",
                    "label": 1
                },
                {
                    "sent": "So in this case where it's dense advice, when we have blocks of advice, those blocks are going to be completely specified by the two CC solution.",
                    "label": 0
                },
                {
                    "sent": "And the blue dots here represent our solution so.",
                    "label": 0
                },
                {
                    "sent": "The blue dot on the left hand axis.",
                    "label": 1
                },
                {
                    "sent": "Is when we choose an F value of one, we take only the best eigenvector eigenvector for the two CC relax problem as F increases were taking more and more eigenvectors with making the space that spectral allowed to search in to get bigger and bigger but have worse and worse two CC costs.",
                    "label": 0
                },
                {
                    "sent": "So as we as we ask them to kind of Infinity when F when the F value gets as large as possible, that means the whole nontrivial subspace are.",
                    "label": 0
                },
                {
                    "sent": "Unsurprisingly, we end up on the red line.",
                    "label": 0
                },
                {
                    "sent": "And you can see that we actually do a lot better than either of the solutions alone by combining them.",
                    "label": 0
                },
                {
                    "sent": "And for a large range of F.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I live.",
                    "label": 0
                },
                {
                    "sent": "Here's another one with the complete advice.",
                    "label": 0
                },
                {
                    "sent": "The interesting.",
                    "label": 0
                },
                {
                    "sent": "The reason I'm showing in this picture.",
                    "label": 0
                },
                {
                    "sent": "And the point of interest is that the spectral, I mean the two CC solution which is the dashed red line alone, is doing terribly here.",
                    "label": 0
                },
                {
                    "sent": ".5 is the worst you can do, really.",
                    "label": 0
                },
                {
                    "sent": "And you can say that even though the advice by itself is pretty much useless, well, you think so.",
                    "label": 0
                },
                {
                    "sent": "Given the two CC.",
                    "label": 0
                },
                {
                    "sent": "Cost when we actually combine it with spectral.",
                    "label": 0
                },
                {
                    "sent": "We actually for some ranges of F get as high as we did before.",
                    "label": 0
                },
                {
                    "sent": "So in this picture in the solid red line is the same in both cases, 'cause we're throwing the advice away.",
                    "label": 0
                },
                {
                    "sent": "The dashed red line is heaps lower here, yet we're able to achieve pretty similar height at some.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point of F. So just to summarize.",
                    "label": 0
                },
                {
                    "sent": "We found some problems.",
                    "label": 0
                },
                {
                    "sent": "Well, we've shown that there are some problems where a combination of advice and spectral clustering is better than each technique.",
                    "label": 1
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And we've we've demonstrated an approach in order to combine them, and basically that's by turning advice into subspaces and using multiple solutions to the two CC problem to generate multiple subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've shown kind of 1 algorithm to use.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I mean really, you could use other way if you could have some other way some other algorithm that generated sort of a chain of subspaces corresponding to two CC solutions, you could plug that in the two CC spectral two CC algorithm was the natural thing to use, but maybe.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is other things that could work.",
                    "label": 0
                },
                {
                    "sent": "Also, we haven't.",
                    "label": 0
                },
                {
                    "sent": "As I said, I didn't say anything about how to choose F, although we did see there were big ranges of F worth.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going well.",
                    "label": 0
                },
                {
                    "sent": "And is there an extension to more than two clusters is really odd?",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This question to ask finally I just like to acknowledge in Davidson for suggesting this work to us.",
                    "label": 0
                },
                {
                    "sent": "Try.",
                    "label": 0
                },
                {
                    "sent": "Laugh.",
                    "label": 0
                },
                {
                    "sent": "Right, right I mean.",
                    "label": 0
                },
                {
                    "sent": "I. I guess I guess the issue here is the inconsistency of the advice.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you take a.",
                    "label": 0
                },
                {
                    "sent": "A semi supervised clustering problem and you generate the advice that you that you can infer from your supervised examples.",
                    "label": 0
                },
                {
                    "sent": "You're going to get consistent advisor if basically it's going to be constraints, not advice.",
                    "label": 0
                },
                {
                    "sent": "So, so it's not really a fair comparison to the algorithm which is designed to work in the case where the advice is potentially incorrect.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was particularly thinking about one of the papers by Jean Bushy, and Stella Zoo was kind of very similar setup.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, what's the relationship there?",
                    "label": 0
                },
                {
                    "sent": "So this I'm not sure what the specific paper is, so normalized cuts and quality constraints.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I think I think in that case they are actually.",
                    "label": 0
                },
                {
                    "sent": "Changing the affinity matrix using the advice.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "Which is, yeah, I mean, I guess it's different because it's kind of trading the advice and the Infinity kind of.",
                    "label": 0
                },
                {
                    "sent": "Almost trading the advice as Affinity, which I guess has some problems because.",
                    "label": 0
                },
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "The advice isn't, for instance, is not metric in this case, and so you have to do some kind of fudge ING to make it work, and for instance, if the affinity gets over one, you just cut it off at one and it's not particularly nice.",
                    "label": 0
                },
                {
                    "sent": "In my opinion, this please.",
                    "label": 0
                }
            ]
        }
    }
}