{
    "id": "dtnicioanlxzxvmpl4ulp4t4kx36nvqe",
    "title": "Low-rank modeling",
    "info": {
        "author": [
            "Emmanuel Candes, Department of Statistics, Stanford University"
        ],
        "published": "Oct. 12, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Collaborative Filtering"
        ]
    },
    "url": "http://videolectures.net/mlss2011_candes_lowrank/",
    "segmentation": [
        [
            "OK, So what I decided to tell you about is about some work.",
            "The talk is going to be a bit conceptual, not very technical, but conceptual about some recent work about the theory of low rank modeling and so in the first part of the talk I'm going to talk a lot about what we have learned through the modeling.",
            "With the low rank matrices and then and the second part I will show you what I believe are kind of exciting applications in computer vision."
        ],
        [
            "OK so I will say that in my field of research is an explosion of research on the theory of low rank modeling.",
            "When I grew up.",
            "I'll tell you a little story to start with.",
            "When I grew up, I worked on sparsity an it was weird because nobody was working on sparsity and now it seems that if you try to publish a paper, at least in my field without the word sparsity in the abstract, good luck.",
            "But I can see the same stuff happening now, not use sparse models but using low rank models.",
            "And that's what I would like to discuss.",
            "Some of the works I'm going to present is ours and some of it is not."
        ],
        [
            "OK, so I'm just going to talk about two very simple things.",
            "One is matrix completion and the other one is ways of doing dimensionality reduction in a very robust way, which is I think important for people working in machine learning."
        ],
        [
            "OK, so let's talk with the first topic."
        ],
        [
            "Matrix completion if you do not know the Netflix Prize, check the name of their summer school again, so I assume that everybody is familiar with the Netflix price.",
            "Everybody knows what is.",
            "Do we have people who don't know what is the Netflix price?",
            "No kidding.",
            "You're not serious, are you?",
            "You are serious.",
            "I'm so old.",
            "OK, OK, so the Netflix Prize is a major initiative that was launched by a company named Netflix.",
            "Netflix rents movies out to users.",
            "They have about half a million users at the time.",
            "They launched the price and they were renting about 18,000 titles and after you it was a Mail service.",
            "So what you do is you go on the Internet and you say you want to rent a movie.",
            "And then Netflix would email user DVD would watch the movie, we return the DVD, they would Mail user next one and so on and so forth.",
            "But very quickly what they would ask you is via email is what whether or not you like the movies actually ask you to rate the movie you had just seen."
        ],
        [
            "And so users would go out and sparsely enter some entries in a huge database that Netflix was assembling an it's a database in which we have columns which are movies and about their 20,000 of them at the time they were about 500,000 users, so it's a huge data matrix.",
            "And when you see us cross in this data matrix, it's a movie that as user has rated, of course this data matrix is extraordinary, sparsely sampled because users on average random at.",
            "30 to 50 movies.",
            "And so instead of having 18,000 ratings, we have about 30 to 50.",
            "And what?"
        ],
        [
            "Netflix wants if they want to complete the Netflix Matrix.",
            "So what does this mean?",
            "That there are many entries that are not seen that have many movies that have not been rated by users.",
            "And Netflix launched the price.",
            "The Netflix price, where people were to do challenge to make predictions about items or entries of the Matrix you had not seen.",
            "And why wouldn't Netflix wants to do such a thing?",
            "I think the reason is pretty obvious, which is, of course, if you can predict accurately movies that people have not yet seen.",
            "And of course we can build an efficient recommender system and so of course we can generate a lot of cost.",
            "You have lots of customers, lots of happy customers, and so you generate a lot of income.",
            "Now of course, as you can imagine, there's not only Netflix interested in such things, but you know Facebook has the same problems.",
            "Apple sending music has the same problems.",
            "You know everywhere you go in the online Commerce.",
            "People would like to in fear preferences for items based on few preferences.",
            "So there are many such problems.",
            "This is an area called collaborative filtering.",
            "In statistics we have similar problems in which we have questionnaires.",
            "For example, this could be questions.",
            "This could be.",
            "It could be questions.",
            "This could be people being surveyed and some people may not want to answer questions and you would like to know what if they had answered the question.",
            "OK, so for those of you who do not know about the Netflix Prize, essentially Netflix offered $1,000,000 to whomever would be able to come up with a prediction algorithm that was besting their own in-house algorithm by 10%."
        ],
        [
            "Another problem which is not the Netflix price.",
            "What I'm going to try to show you that this matrix completion occurs in many different areas of science and engineering and other things that happens in electrical engineering.",
            "This time is like what you've got is.",
            "We've got points in space with US XJ and what you know is you have partial informations about those pairwise distances, so you have a matrix L which records pairwise distances between points.",
            "I endpoint J, but the problem is you can only see a few of these pairwise distances, perhaps because you have a sensor net, so you have a wireless network where people or sensors can only construct a distance estimate to their nearest neighbors, but not too too sensors that are far away because of power constraints, obviously.",
            "OK, so now you've got this data matrix, which is very large an what you would like to know is you'd like given pairwise local distances can actually recover all distances and why."
        ],
        [
            "Is it important support important?",
            "Because in sensornets we always want to know where stuff is and so local localization of sensors is the same as.",
            "Of course if I can localize the sensors, I can compute all global pairwise distances.",
            "But it goes.",
            "It's a two way St in the sense that if you can't compute all pairwise distances, and of course you know the location of the sensors up to a rigid transformation, and so the problem is from a few entries of this matrix, can I actually recover all global distances so that I can locate the sensors?",
            "So it's another problem in which we have entries of a matrix of a very diff."
        ],
        [
            "Kinda obviously the Netflix data Matrix, where there are lots of missing entries and we'd like to in fear the value of those missing entries.",
            "OK."
        ],
        [
            "There are many other problems.",
            "A great friend of my leaving is going to talk next week about convex optimization.",
            "Perhaps they show it will show you applications of matrix completion in control and system identification.",
            "You have exciting applications in quantum state tomography in quantum mechanics, which are fantastic.",
            "They do to David Gross you have problems of this kind in signal processing, of course it's a big problem in machine learning and computer vision.",
            "So this problem of trying to be to have partial information about your large data matrix and try to recover in fear the entire matrix is a problem that comes up all the time."
        ],
        [
            "So what is a matrix completion problem?",
            "The mystery completion problem is something like this.",
            "We've got a data matrix L which I'm going to call L. We observe a subset of its entries.",
            "Very few, perhaps in the Netflix case, it's below a percent and we would like to get some."
        ],
        [
            "Missing entries so very simply stated and everybody would agree that it looks a bit daunting."
        ],
        [
            "OK, so obviously I'm not going to be able to do to guess all the missing entries if I don't make some kind of a structural assumption and the structural assumption we're going to make it that oftentimes the data matrix we see in practice have low rank, approximately low rank.",
            "So now we deal with massive amounts of data, such as a Netflix matrix, but typically really only the dimension of the phenomenon we care about, or that we're looking at is much lower.",
            "Then the ambiance dimension suggests and so in many applications of interest, the matrix wish to recover is not unstructured.",
            "It is structured in the sense that it has low rank or approximately lower."
        ],
        [
            "In fact, it is true of all the applications that I'm familiar with.",
            "And so it is certainly true of the Netflix matrix, and that's why we teach, by the way, things like principal component and factor analysis in statistics.",
            "In fact, if you look at the pieces of the Netflix data matrix and you look at the SVD, you'll see a sharp drop in the singular values after forgot exactly the numerical value, but quite rapidly.",
            "So in that sense, only a few factors capture people preferences for movies.",
            "You know what's the most important factor that determines a lot of variation in their data.",
            "In the Netflix matrix.",
            "An attribute about a movie.",
            "Yes, is there a big Hollywood star in a movie or not?",
            "And that's capturing a lot of the of the of people's preferences.",
            "There are people who are turned down by celebrities and others were not.",
            "There is also the center net matrix or sensornet matrix which records pairwise distances, is very low rank.",
            "For example, is a sensors leave in 2 dimensional space and these matrix has rank two if it leaves the sensors leave in 3D space aranki 3 and so on and so forth.",
            "So we have very low rank and in fact all the problems that we have in computer vision and system identification in quantum state tomography.",
            "In all these problems we have very little information about a huge data matrix, but we can safely assume that the matrix has low rank."
        ],
        [
            "So then I'd like to, oh, by the way, this is an advertising for a conference that will take place in Washington DC.",
            "The Netflix Prize has been claimed by the team by AT&T by Robert Bell and his colleagues.",
            "This gentleman who made a lot of work.",
            "I don't know if you know him, lesser Mikey, who came with even a slightly better prediction.",
            "But 20 minutes later?",
            "And the price has been awarded an then there will be a conference, a public conference in Washington DC telling you a little bit about the history of the Netflix price.",
            "It set up things about the winning algorithms underlying the winning teams and the key ingredient of the AT&T algorithm is low rank.",
            "It starts with trying to find a factorization of this Netflix matrix as lowering and then it has another component.",
            "OK, so if you want to know more about the workings of the Netflix competition, I invite you to Washington on November 4th and I'm also a speaker at that conference because what I find is I heard about the Netflix price on the radio going to Caltech when I was still at Caltech.",
            "I thought that this was a fascinating problem, and so then I thought about this idea of recovering matrices from what seems to be too little information, and I started to think about.",
            "And then I could see a lot of connections between the Netflix price and other people also problems that were floating around.",
            "And so I will explain some of these connections.",
            "OK, but the low rank structure plays an enormous role in a thing.",
            "It's 90% as, as Rob once told me of their solution, right?",
            "It's low rank plus ansambel methods essentially."
        ],
        [
            "OK, so now if we assume that the matrix has low rank.",
            "Then I'd like to ask the question again.",
            "We can think about the low rank decomposition of a matrix via its singular value decomposition, for example, which is a simple factorization of the matrix that everybody knows.",
            "So then we've got M = U, Sigma, V star, and so when I look at the number of degrees of freedom, that is a number of real numbers that I need to specify Knew Sigma and vystar to specify a matrix of rank R of dimension N1 and N2, then we can count the number of.",
            "Real numbers you see in these factors and if you do this properly, you'll see that the number of degrees of freedom in arrancar matrix, which is rectangular.",
            "An one by N 2 is the sum of the dimension times the rank essentially OK.",
            "So if N is 10,000 and one is 10,000 thousand and two is another 10,000, you have a matrix with 100 million entries.",
            "But it does not have a number of degrees of freedom, which is 100 million is essentially 20,000 times a rank.",
            "So if the rank is 20, you know it's a much very small fraction of the ambient dimension OK, and so now we can ask the question again.",
            "Well, do I need to see everything to actually recover a structure that has intrinsically a number of degrees of freedom, which is much lower than the ambient dimension suggests?"
        ],
        [
            "An OK and that's a question."
        ],
        [
            "Of course we need to make this and so that's a conceptual part of the presentation.",
            "We need to make this well posed and to make this well posed.",
            "What we have is, well, it depends what I decide to show you because I could decide to show you all the entries of the Matrix except a row for example.",
            "Alright, so in the case of the Netflix price in the next weeks competition it would be I show you all the entries of so there's a user that has not rated a single item and I claim well, if you've not seen this person, how can you predict his or her preferences?",
            "You can't, right?",
            "So even if you have a rank one matrix and you don't see Aurora column, then recovery is obviously not possible."
        ],
        [
            "So we assume and you know it's a strong modeling assumption.",
            "We're going to assume that, at least for the purpose of this talk, is that.",
            "Well, let's assume that the entries that are revealed to you have been selected at random, not because I believe in probability stick sampling, but because I'm interested in discussing what happens in most cases, perhaps not in all cases, but in most cases."
        ],
        [
            "OK, so we have a random subset of entries of a low rank matrix and I say well.",
            "I don't know.",
            "It's still a tough problem and the reason it's a tough problem is because well, here is an example of a low rank matrix.",
            "Right, so it's a very low rank matrix or matrix of rank one.",
            "And if I show you I don't know in Netflix shows you .4% of the entries or something ridiculous like this.",
            "If I show you even here, 10% of the entries 90% of the time you're going to actually miss this one, and so you're going to see only zeros, and you're going to tell me that the matrix is.",
            "It has low rank.",
            "You've only seen zeros.",
            "Zero and then you would make a big mistake.",
            "Now of course, this matrix is very special.",
            "For example, in the case of the Netflix matrix.",
            "I certainly do not believe that it looks like this because it would look like this if and only if nobody likes anything except that there is a guy that likes a movie.",
            "That is not the way it looks or in the sensor net application.",
            "It would say all the sensors are the exactly the same location and one is far away.",
            "Right, which is not reality.",
            "So this matrix is very peculiar."
        ],
        [
            "Here's another example of a matrix which obviously cannot be recovered from a small set of entries for the same reason is like I don't know if I show you 10% of the entries, this matrix has rank two.",
            "Obviously if I show you 10% of the entries you're going to miss with very high probability you're going to miss one of these four entries.",
            "And then what do you do?",
            "You can't recover them, it's impossible."
        ],
        [
            "To me, the protonic particle, the most basic example of a low rank matrix that you cannot recover.",
            "Something like this.",
            "What I've got here is I've got a data matrix, will have a rose, which is a vector X.",
            "It's a rank one matrix obviously, and then everything else is 0 and then you sample these matrix at random.",
            "I show you even half of the data and then of course you're going to miss half of the entries upstairs.",
            "And how can you know what they are?",
            "You will not.",
            "So what are we learning through these things?",
            "We're learning that somehow to be able to complete the matrix.",
            "Like I think this is a good example to complete the matrix.",
            "If I have rows that are orthogonal to other rows, that if I have singular rose.",
            "That orthogonal to everybody else, there is no way on Earth that I can set in motion the principle of collaborative filtering.",
            "If somebody is rating movies at random such that this person's ratings are orthogonal to everybody else.",
            "How can I learn his rating from other people like this person?",
            "There are none."
        ],
        [
            "OK, so another way to say what I just said is that if somehow they are singular rows and columns, that is what it means in terms of the SVD is that if some singular vectors or if you would like to speak like a statistician if some principal components happen to be very sparse.",
            "Like in this case because look, we see a singular vector.",
            "It's E one.",
            "It's extremely sparse, then vector like this create a single row, a singular row, then matrix completion is obviously impossible."
        ],
        [
            "Here we have in the previous example we have very sparse singular vectors supported by the 1st two basis vectors and matrix recovery is impossible.",
            "So if somehow the principal components of this data matrix are sparse, you're in big trouble.",
            "An I believe from my conversation with.",
            "I forgot whom that there are some.",
            "Well, in the Netflix case, there are some movies that are very hard to predict exactly.",
            "For this reason.",
            "For example, good example is Napoleon dynamite like nobody knows whether they like or not.",
            "These movies and the rating seems to be a bit random."
        ],
        [
            "OK, alright, So what we're going to do a mathematically is we're going to try to quantify the degree with which the singular vectors are sparse, and we're going to do this geometrically, and that's almost my my most technical slide here.",
            "Which is that we're going to quantify the degree of correlation between the basis vectors and the column and the row space of your matrix.",
            "And so we're going to introduce a very simple definition.",
            "We have our low rank matrix, which we think of it as we are.",
            "Is singular value decomposition, and we're going to introduce a coherence, and coherence is simply what I'm going to do is.",
            "I'm going to look at this basis vector EI.",
            "I'm going to project them onto the column space of the Matrix U and I'm going to look at the norm of these vectors.",
            "Now if the column space is orthogonal to EI then.",
            "Of course the norm is 0 if the column space contains EI is enormous one.",
            "And So what I'm going to do is, I'm going to look at the maximum of this projection, and I'm going to introduce a parameter mu, which qualifies the degree with which the column space is aligned with the basis vector by being essentially the maximum norm of this projection, renormalized by the dimension on which on the space you projecting onto on the rank divided by Zambian dimension.",
            "OK, and so I'm going to skip the condition below.",
            "I'm going to look at focus at the top condition.",
            "The condition said that if mu is small then it seems like the column, the column space, and aerospace of your matrix are not well aligned with the coordinate axis.",
            "If mu is large then the column space and the row space are well aligned with the coordinate axis and if it's large venue in a situation that looks like this.",
            "Here's a situation where mu is maximum because as you can see the column space contains.",
            "E1, so it's actually contains.",
            "This space is vector.",
            "OK, so another way to think about this is to say, well, we want a small mu that is we don't want a singular.",
            "Sparse singular vectors.",
            "OK, and so from now on we can think about singular vectors are being not too sparse.",
            "An mu essentially qualifies the sparsity of this singular vectors."
        ],
        [
            "OK, so the 1st result we've got in this field is something that has an information theoretic nature.",
            "It says that essentially there is a fundamental role played by this coherence.",
            "And this role is that the number is the number of samples you get to see from this huge data matrix is smaller than this parameter, mu, which essentially qualifies a correlation between the basis vectors and the column and row space times.",
            "So here we have an N by N matrix times the number of degrees of freedom.",
            "Because we've seen that the number of degrees of freedom is essentially dimension times a rank.",
            "Times a log factor.",
            "Then you a number of samples that is smaller than this quantity, then no method whatsoever will work.",
            "So that's in a way you know it's like a lower bound on what you can do, so it really exemplifies a fundamental role played by the coherence.",
            "It said that you need to sample the matrix.",
            "You need to get a number of entries which is proportional to coherence.",
            "And in fact, this number of entries need to be proportional to this geometric parameters equation stands the degrees of freedom's times a log factor, and if you sample at a number of entries which is below this number, then nothing will work.",
            "No algorithm on their schoolwork.",
            "OK, so we're going to assume from now on the coherence is small, so it's order one, and so when the coherence is small, that is when we have non sparse principal components then what the theorem says that for any method to work I need to see a number of entries which is the dimension of the matrix times a rank which we think about as a really number of degrees of freedom in a matrix times a log factor.",
            "Alright."
        ],
        [
            "Now.",
            "OK, so then we know what we cannot do, and then what is it that we can do?",
            "Well, well, so now you give me samples at random from a low rank matrix.",
            "Maybe the coherence is the singular vectors are not crazy, so something can be done.",
            "What is it that I'm going to do?",
            "Well, what I would want to do is that I would want to minimize.",
            "I would say, well, you know I'm looking for a rank low rank matrix.",
            "Why don't I just try this?",
            "I'm going to try to find a matrix with minimum rank that fits the data I have just.",
            "Observed.",
            "And that's a very good idea.",
            "That's what you should be doing.",
            "The problem is, as I'm sure most of you know, I do not have the right to talk about this becausw.",
            "It's a NP hard problem, so NP hard.",
            "I don't even know what that means.",
            "I mean, I should not say this because I'm videotaped I know what I I know what is NP hard.",
            "This is a problem with tape lectures.",
            "You cannot really speak the way you want to speak.",
            "Um?",
            "I'm NP hard now.",
            "I know what that means.",
            "But what it means practically, it means in this case that I cannot solve a problem when the dimension of the matrix is 10 by 10, because the best algorithms we know to actually minimize rank under equality constraints.",
            "Is double exponential in N?",
            "So maybe you are very hard on a supercomputer and you can do N = 10, While what that means you cannot do any calls 11.",
            "OK, so we cannot do it, so we cannot talk about it."
        ],
        [
            "So what we're going to do instead is we're going to minimize another norm, which is in some sense a proxy for the rank.",
            "And that Norm is, well, we're going to minimize the sum of the singular values of the matrix, right?",
            "That's the norm.",
            "It's called the nuclear norm.",
            "It has nothing to do is nuclear physics.",
            "It's actually a harmless norm.",
            "You minimize the sum of the singular values subject to the same constraints.",
            "Now do people know what is the nuclear norm?",
            "Is it the first time you see it or not?",
            "Because if you have seen it many times, and I I'd be happy to skip.",
            "It is some of the singular values of a matrix.",
            "So first of all, I'm sure everybody knows what is the L1 norm.",
            "And why do we use the L1 norm?",
            "Because it's a good proxy for.",
            "L 0 here it's the same thing.",
            "It's the L1 norm of the singular value, so it's a good proxy for the L0 norm of the singular value.",
            "But what is the L0 norm of the singular value?",
            "It's a rank, so the L1 norm is a good proxy for the rank.",
            "Now there's one way which is much more sophisticated to see it, and I'm going to try to draw a picture and that picture is this, which is that the way I think about there is a one norm personally is that I look at sparse vectors.",
            "Alright, so we scored an 8 plus or minus one so here they are.",
            "So I have four of them because I can only draw in the plane and what the L1 ball is.",
            "It's another way to see that it says tightest convex relaxation of an LO problem where the L1 ball is it actually the tightest convex bodies that contain sparse vectors.",
            "And that's why we use the L1 norm because it's smallest convex body that contains very sparse vectors.",
            "Of no one.",
            "The nuclear norm is exactly like this.",
            "Now I cannot draw unfortunately, but the nuclear norm is exactly like this.",
            "Now I can look at well what are very low rank matrices.",
            "While these are rank one matrices, we say Norm lesson 1 and I say what's a convex body is a smallest convex body that contains them all, and that's a nuclear ball.",
            "Cancel in that sense, the nuclear norm is the tightest relaxation of the of the rank functional.",
            "Just like the L1 is a tireless relaxation of the LO function.",
            "Now, it's not obvious that the sum of singular values is a norm, and I don't know if I have slides about this.",
            "No, I don't have flights about this.",
            "What I'll say though, is that the nuclear norm, the L1 norm for those of you know the L1 norm, is a dual 2Z.",
            "An Infinity norm and the Elder do alter the nuclear norm is going to be the DeWalt tool.",
            "The maximum I think value, which is the usual matrix norms as operator norm and So what?",
            "I claim that the nuclear norm X star.",
            "Is actually is a supremum over all matrices of norm?",
            "Usual normal lesson one of you dot X.",
            "Yes, it's exactly like what you have for the L1 norm and now of course this is not obvious that this is a case, but if you say this is that this is true, then it's obvious that the nuclear norm is unknown because it's a dual norm.",
            "OK, um, alright, so we have these proxy for the rank and so we're going to minimize this proxy instead.",
            "Now what you'll see with leave in an this I'm pretty sure you'll show you this is that this is a nice convex optimization program.",
            "Why because while you minimize the Norman norm is convex, you have linear equality constraints are convex set, so you have a nice convex optimization program.",
            "And I.",
            "It's actually a semidefinite program.",
            "For those of you know about semidefinite programming and our field, which I'm not going to have time to discuss too much about, our field has spent years trying to develop algorithms that solve this thing efficiently.",
            "So for example, now we can solve problems in which you have matrices, which are I don't know, have a billion entries in in a few minutes on a desktop PC.",
            "OK."
        ],
        [
            "OK, so this is normal.",
            "Interestingly enough, has been used before.",
            "We used it essentially in control by mess dahiana papavasiliou polos and buyback and Andrea, Anne Marie and Fazel champions.",
            "The use of her norm.",
            "In 2002, Anne for matrix problems, he was actually introduced by Rasht, Salem and Parrillo."
        ],
        [
            "OK, so now the result is like this.",
            "You have a result that I find a bit surprising when you think about it.",
            "OK, so now we have a result.",
            "We've got our matrix completion problem.",
            "We see very few entries, perhaps point 1%.",
            "There are many entries we do not see.",
            "In fact, the huge majority of entries we do not get to see.",
            "And how do we going to guess the entries by just minimizing this nuclear norm subject to equality constraints?",
            "And uh, result due to myself and Terence Tao shows the following, which is well, we're going to have a matrix of rank R. You're going to show me a random set of size M and what we were able to show is that if the number of centuries is greater than the number of degrees of freedom times a log factor where we showed that the power of the log is less than six and sometimes equals to two, then this semidefinite programs is simple.",
            "Optimization program magically recovers all the missing entries.",
            "With no error whatsoever.",
            "Right, so you've got very few entries that are visible.",
            "You try to guess the other entries by minimizing this simple convex functional, and then there's this surprising result that occurs, which is that this solution at hot when you look at it, it has filled in all the missing values exactly with no error whatsoever.",
            "OK David Gross at whom I mentioned earlier, has the best result known to date.",
            "You was involved to improve on this power of log, and we're showing and managed to show that.",
            "Well, the the number of entries is greater than coherence times the degrees of freedom time.",
            "Log N squared exact matrix completion occurs by convex optimization."
        ],
        [
            "Alright, so there is a lot of work.",
            "As I mentioned going on in this literature, so matrix completion as a field is a field that is exploding really rapidly.",
            "People are discovering new applications all the time at this are discussing new algorithms for solving these very large scale problem because like if you think about the matrix with a billion entries, you have to solve a problem with a billion unknowns.",
            "This is not something that you can take lightly and so there's a lot of activity on the theoretical front.",
            "On the applications front end on the algorithmic front and so and so, a lot of people mentioned on this slide have done some very important work.",
            "For example, I would like to mention the work of my colleague Andrea Montanari an of his grad students who have shown a different algorithm, not based on convex optimization, that also can fill in the missing entries.",
            "Always, no error.",
            "Alright."
        ],
        [
            "OK, so why does this work?",
            "Why is it that magically it it works?",
            "I don't know why it works really.",
            "I can prove it, but I will try to make a picture.",
            "So the first picture I have to show you is why does L1 work well?",
            "Why is it that when you minimize the L1 norm, recover sparse signal?",
            "And then I'm going to try to draw an analogy.",
            "So what you have is when you solve an L1 problem, you recover a sparse solution.",
            "And why is this because?",
            "What you say is like.",
            "Let's say you have a sparse vector which is here, and it's sparse because this code is 0 and this one is not zero.",
            "And then you take one measurements.",
            "I'd just make one measurement and this vector, and so we all know from middle school that, well, this gives you the equation of a line like this.",
            "And then you say alright.",
            "Well, if I were to find on this line a point with minimum L2 norm, well I would get this thing which is not good.",
            "So in in low dimension it looks close, but in high dimension is a disaster.",
            "If I use the L1 norm instead, so this is the L2 ball as we all know.",
            "The L1 ball is like this as we've shown.",
            "And so if I use the L1 norm instead, I'm going to say what's the point on DSL one?",
            "On this line that has minimum L1 norm while well so well, I'm going to do is I'm going to grow this thing until it becomes tangent.",
            "And so the point where the L1 ball is tangent is this, but it's tension at this vertex, and so it recovers.",
            "A sparse solution actually.",
            "So this I show you that L1, at least on this example, L1 optimization works.",
            "And why is it it's because the L1 ball is very pointy.",
            "Add add on sparse vectors."
        ],
        [
            "For matrix completion, there's something similar that occurs which is at this.",
            "Now we have the nuclear ball, which is pictorially represented by this cylinder.",
            "So here you've got this set of two by two matrices, which are symmetric, because I can only draw in sometimes.",
            "When I'm brave, I draw in 3D.",
            "So here we have three parameters XYZ.",
            "We have a symmetric matrix and I'm going which have nuclear norm less than one.",
            "And what happens is that the low rank matrices are extreme points of this convex body.",
            "And now we have our feasible set, which is huge dimensional space.",
            "But it actually happens to be tangent to this very pointy nuclear ball, because the nuclear ball is pointy at low rank solutions.",
            "OK, so this is very quick.",
            "Geometric representation of why this work.",
            "Then it's because of the pointiness of the nuclear norm or the nuclear ball at low rank solution that all of a sudden there's many ways you can have your feasible set and maintain this tension property."
        ],
        [
            "But of course this is not a proof an I don't claim that it is.",
            "OK, so now of course you say, well, you know you can recover from a subset of entries.",
            "Can you recover from other type of information?",
            "And of course yes you can.",
            "An now by now there's a whole theory which is being developed to show very precisely that now you've got information about the matrix not given by revealing entries, but by revealing coefficients.",
            "So now I give you all the kind of linear information about the matrix of interest L. So I give you inner product.",
            "So before I was giving you an inner product between a matrix L&EIEJ star which essentially reveals Ellijay.",
            "But now you can say, well, these matrices don't have to be.",
            "The matrix is full of zeros and ones somewhere that just pick an element.",
            "You can have anything you want and so now there is this general theory of saying, well, you know now I have linear information about a matrix linear information about a matrix of this kind.",
            "I'm going to find among all those matrices, obeying these constraints that with minimum L1 norm an what I'm trying to say here is that if I have incoherence between the sensing matrices and the column space and the row space of the matrix.",
            "Everything should work, and indeed it works."
        ],
        [
            "And that's we've done a piece of it with Ben.",
            "Rushed, and."
        ],
        [
            "Because of time.",
            "Maybe I'll skip this part."
        ],
        [
            "But"
        ],
        [
            "Please big con."
        ],
        [
            "Tribu Shun is due to David Gross, who who."
        ],
        [
            "Explain and show exactly what kind of coherence you need.",
            "An when.",
            "Can you expect things to work, so I'm not going to go through the detail, I'm just going to tell you that it's available and it's out there."
        ],
        [
            "What's interesting about the contribution of David Gross is that is a physicist, a quantum physicist, and when he realises that there are lots of problems in quantum mechanics that are matrix completion problem and so I will just mention what it is.",
            "Quickly, so in quantum mechanics you have a system and quantum mechanically.",
            "If you have a case being 1/2 system quantum mechanically, it's represented by a matrix L, which is called a density matrix.",
            "The problem is, as you know, as you may not know, the dimensioning grows very quickly that the power of quantum computing.",
            "It grows exponentially in the number of particles you have, and so if you have a system with K particles you've got a dimension which is expansion of exponential in the number of particles.",
            "So perhaps without going into too much into details that will try to explain what people try to do in this field so people prepare quantum states.",
            "Like they prepare quantum systems.",
            "And then how do they know that what they prepared is what they thought they prepared?",
            "How do they measure that what they thought they prepare in the lab is actually what they thought they prepared.",
            "They have to take measurements.",
            "And what do you do when you take a measurement on a quantum system?",
            "You destroy it while you modify it completely.",
            "So because you're going to have to make a lot of measurements, why?",
            "Because a number of degrees of freedom is a matrix which is ambion San Square.",
            "It's huge, an end is exponential in the number of particles you're going to have to prepare a lot of quantum states.",
            "Now a lot of the states that these guys prepare are actually low rank or approximately low rank.",
            "And So what this theory says is that with far fewer number of measurement that you thought were possible, you can actually guess the quantum state of the system, because you're going to use a low rank structure of the system to do it OK. And So what David Rose did was to use the tools of matrix completion to actually show that you could solve this problem and he did much more."
        ],
        [
            "OK, so these are problems like this in quantum mechanics and people are actually using these techniques to actually interfere quantum states from quantum state tomography measurements."
        ],
        [
            "OK, so I'm done with matrix completion in the second part of the talk I will talk a little bit about robust PCA, but this part is of course much shorter because we've introduced most of what we need now."
        ],
        [
            "OK, so first of all it is if you add noise to matrix you could say yeah, it's interesting to recover a matrix which is exactly low rank from knows less data, but in practice you know I'm not exactly low rank, plus I have noisy data like the ratings are contaminated with noise.",
            "It would be a pity if the whole theory we could not accommodate in exact measurements and So what you do when you have an exact measurement you do what you always do.",
            "You don't enforce equality constraints.",
            "Of course.",
            "Now your data.",
            "Why is inexact?",
            "So you're going to.",
            "Only enforce an approximate fit to the data and among all.",
            "Low rank on all matrix L compatible with data.",
            "Are you going to pick the one with minimum nuclear?"
        ],
        [
            "And when you do this, things work.",
            "That is, you get an error which is proportional to the noise level and another way of saying this is that when matrix completion from noiseless data occurs, then if you have a smaller amount of noise, the error will be small as well."
        ],
        [
            "It's a big field of research, again, by no means.",
            "We are the only one to work on this.",
            "There are many statisticians, including Martin Wainwright, who will be here.",
            "I think next week or tomorrow.",
            "I forgot.",
            "Who has done a lot of important work in this area, right?",
            "So we can actually recover matrices stably from from now."
        ],
        [
            "The data.",
            "OK, so now I would like to come back to this matrix problem to motivate the second part of the talk and the reason I started to work on this is because the first time I gave a talk about matrix completion there was a gentleman in the audience who raised his hand and said, you know, imagine there's something you forget and the thing you forget is that there is lots of bogus ratings in this data matrix.",
            "And he said, I don't know why.",
            "But there are people who enter ratings that have nothing to do with their own preferences.",
            "And so, isn't it dangerous that you use these ratings to actually predict all the ratings when these ratings are completely bogus?",
            "And I said yes it is.",
            "It is very dangerous.",
            "I did not know this.",
            "So what can I do?",
            "And so now we have a slightly more complicated problem where we have actually partial over observation about the low rank matrix, the Netflix matrix.",
            "But we also have a sparse error term on top of it which corresponds to entries that are completely bogus.",
            "OK, and so I need to deal with this.",
            "And so the problem is now I don't want to make the noise the matrix completion robust with every small additive noise like I want to make it robust with heavy very bad stuff that could happen."
        ],
        [
            "Now, this motivates us to introduce a problem that I like a lot, which is a blind deconvolution problem which I'm going to state again as simply as I can.",
            "In this problem, we've got a data matrix which is available to me.",
            "It's available to the statistician or the machine learner, and it's a sum of a low rank matrix and a sparse matrix, right?",
            "So what that says?",
            "Is that what you see is, you see, is a sum of a low rank matrix and the sparse matrix.",
            "So what do I mean by this?",
            "It means that there's a low rank matrix that you cannot observe because some entries have been corrupted.",
            "Again, the corruptions is exactly carried by the Z term.",
            "The only thing you do is you observe the sum of these two mate."
        ],
        [
            "And what I'm claiming it wouldn't be nice to recover Le accurately.",
            "Again, this seems impossible and it would be great to do it, because if I could."
        ],
        [
            "Wait, I could I could detect the bogus rating and apply matrix completion to the good ratings and complete my matrix accurately.",
            "OK, take."
        ],
        [
            "Be great, but why would be great?",
            "It would be great essentially becausw.",
            "How do we do dimensionality reduction?",
            "So typically what we all learn is principal component analysis and I'm sure everybody knows what it is.",
            "What you have is you've got a data matrix M which is a low rank matrix plus.",
            "A perturbation, and So what we think of this is if the points we have a low rank matrix and we look at the columns.",
            "Of this matrix.",
            "And what we think about when we do principal component that we look at the column vectors and we say, well, you know in fact this column vectors believe in very high dimension, but they actually clustered along a low dimensional space.",
            "Again, the goal is to recover this low dimensional space and the way you do this is by principal component analysis, as I'm sure you all know.",
            "So what you do is, you say, well, how am I going to look at the recovers or lower dimensional structure in this high dimensional data set while I'm going to try to find a matrix which has small rank which is as close as possible to the data matrix you gave me and it looks like a horrible communist combinatorial problem because we have a rank constraint, which is absolutely unfriendly.",
            "But this is the only hard.",
            "Combinatorially, problem that I know how to solve because I know the solution and the solution is just calculate the SVD, dumps the small singular values and you're done OK."
        ],
        [
            "And that's what we all learn too.",
            "Every undergraduate taking a course in statistics at Stanford."
        ],
        [
            "What we don't teach so much is that this procedure is extremely sensitive to what we call outliers.",
            "So because again I can only draw in two dimensions.",
            "We've got points lying around a nice one dimensional space.",
            "And so when I fit PCA, it finds his line obviously, and that's great.",
            "But suppose that this data point was incorrectly recorded.",
            "Like for example, let's suppose that the Y value of this point was incorrectly labeled or recorded like a sensor failed, or somebody who typed their numbers in the computer made a mistake."
        ],
        [
            "Or something like this.",
            "At this point we happen here for example, so there's a change of scale.",
            "The other points have not moved.",
            "This point has moved in and I fit PC again.",
            "And what is the first principle component this?",
            "Has nothing to do with the low dimensional structure, So what I'm saying is that I have this huge data matrix.",
            "There is an entry somewhere that is corrupted and everything breaks down or could potentially breakdown.",
            "And that's highly."
        ],
        [
            "Problematic becausw.",
            "Because gross errors occur all the time and they occur because in bioinformatics applications you've got sensor failures in image processing, you have occlusion in image data.",
            "Some pixels are completely occluded, so it's a gross error.",
            "In web data analysis I've shown you that people tamper with data and enter bogus ratings, and so it's important to make lower dimensionality reduction techniques such as PCA robust to outliers of this kind."
        ],
        [
            "And if we talk about face recognition, face recognition, when I look at faces of people, there is some theory that says that if you take a phase under varying illumination, when you look at this face in pixel space, they lie around a 9 dimensional space.",
            "It's called the harmonic plain, but the problem is that."
        ],
        [
            "In a lot of the data we have to deal with this phase of images are going to be extremely corrupted.",
            "So for example, one day somebody decided to go out with sunglasses or you might be interested in faces that you retrieve off the Internet.",
            "And of course, because of characters superimposed on the face, you've got extreme and severe corruptions.",
            "You could have missing pixels and so on.",
            "So raw data are very corrupted.",
            "They have missing blocks and classical PCA technique would breakdown because.",
            "I want to try to apply PCA with people who wear glasses.",
            "People with characters on on their foreheads, and so I'm going to get garbage in, garbage out, and so the problem that we pose now is how can you recover the low dimensional structure when you have data of this kind?",
            "OK, so so again the problem.",
            "As you can imagine is is very, very simply stated I've."
        ],
        [
            "But this problems I've got this low dimensional structure.",
            "I've got this sparse structure.",
            "I want to recover or back out the low dimensional structure.",
            "OK, so I want to perform this blind deconvolution right and it's it looks strange.",
            "But to perform this because you some give me the sum of two numbers, how can I know each of the numbers right?",
            "I mean just looks impossible.",
            "OK."
        ],
        [
            "So can I actually separate the two components?",
            "So now we've got our matrix sandwiches.",
            "A low rank matrix plus a sparse term, which I think about as a sparse impulse noise and I'm saying, well, can you actually separate the two components?",
            "Let's say, well, wait a minute.",
            "We have the same problems as we had before.",
            "If you give me a low rank matrix which is both low rank and sparse, how can I know what the lowering component and the sparse components are?",
            "I just can't right?",
            "So if you give me this matrix, the lowering component could be this and the sparse component would be 0 or vice versa.",
            "Lauren component would be 0 and the sparse component would be this or even the lowering component might be half of this and the sparse component might be, and they're all good solutions.",
            "So we don't want the low rank matrix to be sparse, otherwise the problems makes no sense whatsoever, and so we're going to assume that little run component has the same condition as before.",
            "This incoherence condition as before, right?",
            "So roughly, the singular vectors are not sparse."
        ],
        [
            "OK, now I say well, is this enough?",
            "Now you give me a matrix, which is a nice low rank matrix with singular vectors that are not sparse.",
            "And I said, well, now you can corrupt it and can you correct it in such a way that I cannot recover their own component and you say yes, and how can I do this?",
            "Yeah, correct one row.",
            "Take my ratings and corrupt them all of them.",
            "And I'm going to be in big trouble.",
            "So it's a very good answer."
        ],
        [
            "What you do is you give me a matrix E I'm you give me a matrix to prepare a nice matrix Li look at the first column of L. Put - I add them up, you see zero.",
            "In doing this I did not increase the rank.",
            "If anything I decrease the rank.",
            "I erased all information there was your bank account information here, it's gone and there's no way you can retrieve it."
        ],
        [
            "Alright, so to avoid this kind of things, the model we're going to consider looks at model, in which the entries you can corrupt are actually randomly distributed.",
            "OK, So what you can do in this game is you can take a subset of entries, corrupt them, and then this subset is selected at random and then you can do anything you want.",
            "But I will not let you, when I can let you, but with very low probability I can actually let you corrupt an entire column.",
            "Because you select the entries at random."
        ],
        [
            "OK, so now we go back to this thing where now we've got this deconvolution problems blind deconvolution problem.",
            "We know nothing about Li, don't know it's rank.",
            "I know nothing about EI.",
            "Don't know which entries have been corrupted that always in magnitude of the corruption is how many they are.",
            "I know nothing."
        ],
        [
            "OK, to minimize we're going to use the same approach that we've used all along, which is a user convex optimization approach where it's OK.",
            "There are many solutions to this problem, but I'm going to pick a special one and one that minimizes this simple objective functional.",
            "I'm going to try to make the lowering component as low rank as possible by using the nuclear norm.",
            "And then I'm going to.",
            "I'm going to try to make this sparse component as sparse as possible.",
            "I would like to use is your number I can, so I'm going to use your OneNote.",
            "Alright then, so we gonna use.",
            "This optimization algorithm, which was also proposed by genres occurrence on Gabby Parrillo wilske abit before us in a very important paper.",
            "OK, so then the nuclear norm is what we have.",
            "The L1 norm is.",
            "I just see the norm as the matrix as a gigantic vector and it's some of the absolute values.",
            "OK, and the main second main result of this talk to show you a little bit what things are possible?",
            "Is that if we have a learning component which is as before?",
            "If I have a set of entries that has been corrupted at random locations of positive fractions of entries that have been corrupted.",
            "Random locations and I solve this problem with Lambda equals one over square root of the dimension, so there's no cross validation or anything of this nature.",
            "I just take a Lambda equals one over square root of the dim."
        ],
        [
            "Mission.",
            "Then the theorem says that with very high probability and the probabilities over distribution of the locations of the errors, then this solution is exactly it is I recover the lowering component exactly and then recovers sparse component exactly.",
            "So if he says something that when I look at it, I've almost find impossible.",
            "I prepare a low rank matrix.",
            "I give it to you.",
            "You look at a lot of entries.",
            "You corrupt them.",
            "You give it back to me when I'm not to meet with somebody else because me I already know what I prepared.",
            "You solve this optimization problem that depends on nothing, so use of nuclear norm plus one over square root of dimension times one normal V and you will find exactly what you did.",
            "All the entries you've touched on, how you touch them, and so on.",
            "So you recover the low rank structure.",
            "And it's an exact recovery that has nothing to do.",
            "Its exact, no matter what the magnitude of the low rank component is.",
            "And no matter what the magnitude of this parse error, there is.",
            "No tuning parameter is in this thing, it just.",
            "Just the way it is."
        ],
        [
            "OK, so we've seen 2 examples.",
            "One where we have missing data, the other one is more complicated because now we have corruption.",
            "So in the missing data world it's good because what you've got is you've got an entry and then you have to predict missing entries.",
            "But you know these entries in good in the corrupted world, you never know when you look at the data point, whether it's a good data point or bad data point.",
            "You have to decide this.",
            "Yes, yeah here for those of you who know about coding theory is like you have eurasiers here.",
            "An errors here and it's much harder to correct errors than to deal with the Rangers."
        ],
        [
            "OK, so it's information theoretically harder so if you look at this.",
            "So what we've done is we've run the algorithms.",
            "What you see on the right hand side is you see the rank of the low rank component divided by the dimension and on the Y axis Y axis you see the sparsity.",
            "That is the number of the fraction of entries that have been corrupted.",
            "And then each pixel on this image is a lot of experiment.",
            "A great pixel is an experiment for which the method works half the time.",
            "Awide pick SolidWorks 100% of the time of BLACKPINK SolidWorks 0% of the time.",
            "What you see here is you seen in a very sharp phase transition between a regime where the algorithm is perfect and they were assuming which it fails miserably.",
            "OK, but the regime in which in perfect is very interesting.",
            "So let's be in this white region.",
            "For example is a rank is a total dimension divided by 10 soarin over 10.",
            "What this plot indicates is that can corrupt 30% of the entries.",
            "And it's harmless.",
            "OK, if the rank decreases to 5% an over 20, I can corrupt almost half of the entries and it still harmless OK?",
            "In the Matrix completion world, the region where things go better is much larger, because it's here now.",
            "This is a fraction of missing entries, and so the right region is much larger because it's much easier to deal with missing entries and with corrupted entries."
        ],
        [
            "OK.",
            "So I'm running a bit out of time because I want to show you some numerical results.",
            "I want to mention this very important paper by Alan Wilsky and his colleagues introducing deterministic results.",
            "The very beautiful piece of work and just like as matrix completion, is this robust PCA generates a lot of interest and then since then alot of people in machine learning and statistics have been able to even improve on these results.",
            "There's a lot of activity at the moment by several teams around the world getting better and better results."
        ],
        [
            "OK, in fact, the last result I want to mention, but I don't want to take too much of your time says, well, you could tie them together.",
            "You could have both missing and corrupted."
        ],
        [
            "Trees and you run the optimization algorithm and you back out the the lower end components.",
            "If you show me a very few entries of a low rank matrix of which you've corrupted positive fraction, it doesn't matter.",
            "Will recover the entire matrix with no error.",
            "OK, in all of this I can do by convex programming by semidefinite programs."
        ],
        [
            "OK, we can deal with small noise, so again you could say well, but you know you're not exactly low rank plus parceiro rank plus plus plus my nose is are things are going to be OK."
        ],
        [
            "A and you say yes, things are OK because the when you run these algorithms with relaxed constraint you're going to get an error as if you did not have the big corruptions that make your life so delicate."
        ],
        [
            "OK, so they are computational issues.",
            "I see that I've been going on for a bit of time, so these algorithms there are many.",
            "A lot of work implementing these algorithms.",
            "Um?",
            "And so there's a lot of activity.",
            "On this, for example, if you have a matrix which is 1000 by 1000 of rank 50 of which occur at 100,000 entries, so you have a million unknown variables.",
            "You know on this laptop it won't run in about 13 seconds."
        ],
        [
            "So my colleague from Microsoft Research because this is joint work with I hope I said it.",
            "It's John work with Emma John Wright who are wonderful researchers I Microsoft Research working in computer vision have implemented all of these IDs, not only on desktops but also on the Redmond high performance clusters, Microsoft Research and they use all of this to do things that are of interest to Microsoft, such as dealing with high demand.",
            "Show me images, videos, audio, text, documents and so on, and so that's the kind of applications I'd like."
        ],
        [
            "To show you now.",
            "OK, so let's look at some application."
        ],
        [
            "Can I take 5 minutes to show applications or I'm completely out of time?",
            "5 minutes OK. Alright, OK so the first application we looked at is actually video surveillance and here is the problem is you want to detect separate foreground from background.",
            "And so we think about this as a separation problem as a low rank plus sparse separation problem.",
            "What you do is just stores the images as columns of your huge data matrix and of course the background is going to be extremely low rank because the background is not changing from frame to frame.",
            "It's highly correlated, so you're going to identify this with a low rank.",
            "The foreground, like for example, these people moving about this airport hopefully are going to be picked up by the sparse component.",
            "And so the idea is, why don't we just apply robust PCA to this thing to do video surveillance?"
        ],
        [
            "Grandson when we do this, it works surprisingly well.",
            "So here you've got 3 frames in the airport.",
            "And here you see the lowering component which looks like background.",
            "Then you have this sparse component, which is of course the difference between the frame and the lower end components.",
            "This looks at people moving about the airport and you see this is applied out of the box with Lambda equals one over Scooter, then no other information and it actually perfectly separates lowering from background.",
            "You could say well, what is this person in the background while this person is not moving ever so as far as I'm concerned it's a statue.",
            "And so when you look at the method by competing algorithms, it is getting confused by you.",
            "See you see mess here and mess up appearing up here.",
            "And so it's the results are not as clean.",
            "OK, now there are people who have coded these things in real time and so if you go on YouTube you'll see videos of people moving about a subway and the algorithm actually tracking all the people moving about the subway in real time."
        ],
        [
            "Here's a more interesting example.",
            "Here you have a lobby and what's interesting about this example is the illumination is changing overtime.",
            "And so of course, this is the rank is not one of the background is dark because elimination is changing and so the algorithm does not get confused at all.",
            "Like here you see the number of lights is changing overtime and so the algorithm is exactly picking this person when he walks in an not picking anything else when elsewhere.",
            "Whereas if you look at all their algorithms in the field, they seem to be confused by the separation problem."
        ],
        [
            "OK, we've used this to repair movies, so that's a kind of very simple application.",
            "All movies have all kinds of speckles, and so you're going to apply the low rank plus parts separation.",
            "The low rank will be the clean movies.",
            "Ascept sparse part will be the speckles."
        ],
        [
            "So when you look at stuff like this, you see speckles that completely disappear here."
        ],
        [
            "Um?"
        ],
        [
            "No lines that disappear on the right hand side.",
            "Lines that disappear."
        ],
        [
            "On the right hand side, lines go."
        ],
        [
            "Play speckled go."
        ],
        [
            "Where lines go away and so on.",
            "OK, So what you have to be careful is to tune the Lambda parameters right so that actors do not disappear as well."
        ],
        [
            "OK, so now my next set of application has to do with face recognition, so I will preface this with this.",
            "I said from when I talk to E. Here's what he tells me, he says.",
            "You know a man who will face recognition in a controlled environment is pretty much solved.",
            "That is if we have pictures of people align and behind, there's a nice white wall and have clean pictures of everybody, and I'll show you a new picture.",
            "I say who is this you will find with this person is the problem is that now we have to deal with the fact that data are absolutely non ideal.",
            "So we see people under non ideal conditions and so that's why face recognition is extremely difficult still.",
            "So the idea of this second part is to use this algorithm as a cleaning tool to kind of do the dimensionality reduction or get back to a set of data sets on which face recognition is essentially a solved problem.",
            "OK, so how are we going to do this?",
            "We're going to put faces of people as a columns of a data matrix within applies is the algorithm and the algorithm will remove all the defects we see in faces of images that renders a recognition problematic."
        ],
        [
            "So if we apply it to three images like this, then what we get is we get, you know, a clean face in the middle and then we get all these non the ideologies like cast shadows, specular points that show up in the sparse component."
        ],
        [
            "In practice, to make it work, though, it is not true that image of faces will be exactly low rank because to be low rank they have to be well aligned.",
            "And so the next idea is to say which is due to E. My colleague, and now I have not much to do with the slide you're about to see.",
            "The next idea is to say well.",
            "It's only after alignment that you have this low rank plus sparse structure, so we have to find the alignment as."
        ],
        [
            "Well.",
            "And so the idea is quite simple.",
            "He says, well, let's try to find another alignment that is a transformation said after transformation.",
            "My data set is as lowering plus sparse as possible.",
            "OK, and we can adjust.",
            "Now we leave the convex world because the fact that we have to deal find this deformation is not convex, but we're going to try to do things anyway."
        ],
        [
            "Alright, so just to show you in principle the interest of these methods, let's suppose that I look at a building from a different angle.",
            "And then in front of it I haven't resigned some some shots and other shots.",
            "It's not.",
            "And now I'm going to try to apply this, find the deformation so that I have deformation.",
            "I'm low rank and and low rank, plus sparse as possible, and I'm going to show you the results here.",
            "The result is is away.",
            "You should transform the images.",
            "This is allowing component which is a building unoccluded and then the sparse component, which is of course this tree or cleaning the building.",
            "And so it seems to work quite well in practice."
        ],
        [
            "So people have done Ian, his colleagues have done a lot of work, and again it's not mine anymore.",
            "What you see is you take people's faces of people.",
            "These faces are major line as you can see and then they are corrupted, so I don't see if you see these patches that are in front of the eyes or nose and so on.",
            "And so you're going to try to align them, and so you run this algorithm by trying to find, for each image deformation so that as a whole you've got as much lowering plus power structure as possible.",
            "OK, and so you see the output of the data matrix.",
            "Of the lowering component and is, as I actually perfectly or almost perfectly aligns with images, and of course removed those occlusion that include eyes, nose, mouth and so on and so forth."
        ],
        [
            "Here is a thing that I find the most.",
            "I don't know actually almost check it before I can talk about it, but here we So what they've done is called.",
            "It took a pictures of a guy on the clu's.",
            "Conditions, so it's the same person, but here is wearing sunglasses.",
            "Here is they've got, is has something in front of his mouth.",
            "He has incentive in front of mouth.",
            "He's wearing a hat or HIG.",
            "And so on.",
            "And then you apply his algorithm.",
            "We say we are going doing face recognition on something like this is going to be very difficult, so let's use this as a preprocessing step.",
            "So you use this as a preprocessing step.",
            "You do the alignment, the low rank and sparse component simultaneously, and what you get out of these cells are low rank component and that's a sparse component.",
            "And so you see all these non idealities sunglasses issue here.",
            "Then you impute the eyes.",
            "The hats showing here you know you have removed the hard.",
            "You remove the wig, you remove the hand in front of your mouse and so now perhaps you can feed this to a face recognition algorithm."
        ],
        [
            "As you all know, if you do, for example, if you retrieve images off the Internet annual and you do averaging or faces to get the typical average phase before you do alignment an repairing, you're going to get extraordinary blurred pictures.",
            "It's very hard to recognize people, but if you were to apply these alignments in repairing in the sense that you perform this lowering plus sparse decomposition.",
            "Beef."
        ],
        [
            "Or you take averages.",
            "Then all of a sudden everything gets much much clearer because you align the right stuff.",
            "You average the right stuff."
        ],
        [
            "Of course you can use this for other applications as well.",
            "You can say, well, you know this is a hand digit recognition problem.",
            "You know this is a way people could write the number 3.",
            "You could say, well, what if I were to apply these techniques before actually trying to attempt to understand what this character is?",
            "And if you do this and you look at the deformation and the lowering component, you would essentially get a clean picture over three that perhaps is easy to recognize."
        ],
        [
            "My last set of applications and then I'll be done is trying to do the same thing, but instead of trying to do it on a bunch of images to do it on a single image.",
            "So of course, the world that we see is tilted is tilted because we see it often times at an angle, and the question is can we undo this tilt?",
            "Can we undo this perspective automatically?"
        ],
        [
            "And so the idea of again of Imaan of his colleagues at Microsoft Research is to say, well, you know you've got a texture like this.",
            "Maybe it's been corrupted by this pattern that is superimposed on top of the texture.",
            "But what if I were to try to find a deformation such that after deformation I'm as low rank plus sparse as possible?",
            "OK, and so they've carried out this research program and I'll show you some of their."
        ],
        [
            "Notes.",
            "Alright, so you've got a texture that looks like this.",
            "It has several components.",
            "It has of course you see that you know it's not straight and then it has a gradient and the idea would be that perhaps by applying these techniques you would actually find the right deformation so that you straighten out the texture you correct for these non idealities and so somehow you can re align objects."
        ],
        [
            "So they've done this and um, and with great empirical success, they've achieved good success.",
            "So here is you.",
            "See a picture of a checkerboard.",
            "So now a checkerboard has rank two.",
            "Well, I think I gave the answer a checkerboard.",
            "If you look at straight will have rank 2.",
            "OK, so it's not surprising that when I see a checkerboard, added perspective angle and I apply this algorithm, it's training it because it finds a rank two matrix that it likes a lot.",
            "When you see all the kinds of structures are texture like this, it's trading them as well and so they've looked apply this algorithm to a lot of textures that they see from my perspective angle and each time it seems to kind of correctly identify as a perspective angle."
        ],
        [
            "OK, so if you applied this to synthetic stuff which is for example textures of images of this kind, and in this case of course is not difficult to see that you're going to do well, because this is rank one here it's rank two, so you do well."
        ],
        [
            "What's more surprising is that in a lot of applications they seem to do quite well, so so they have taken images of signs in Beijing parks.",
            "And So what you see is you see this sign with Chinese character at an angle, you apply the algorithm and over certain it's training the sign.",
            "You can use this with bar code.",
            "It will straighten it up.",
            "You see with pages of text it straightens it out.",
            "Chinese character becomes trade and so now perhaps you can.",
            "You can run recognition."
        ],
        [
            "Algorithms?",
            "Because here is a very interesting example where you see a Chinese.",
            "So you see, a Chinese saying that we know the characters are viewed as a perspective.",
            "You apply the algorithm, it's back up here, you see a.",
            "And you see the characters.",
            "And now you can recognize them easily.",
            "And of course you can straighten other types of structures, which I'm not sure is a good idea."
        ],
        [
            "OK. Because you can find the perspective angle, you can actually find the geometry of these things and so by working out this decomposition, you know when I give you a picture like this, you know how do you find the deformation that makes this thing as lowering plus parses possible.",
            "It gives you the angle at which you see things.",
            "So actually now you can recover the orientations of things on a single."
        ],
        [
            "Image.",
            "And so you can do augmented reality if you want it too.",
            "So what you do is because you know you're going to you partition your image into sub images and then you're going to apply this algorithm and each time because you can straighten out these textures, you know exactly the angle, the orientation of these buildings, and because you know the orientation of these buildings, it's not going to be hard for you to actually paste an augmented reality in any way you want."
        ],
        [
            "I'm lost, maybe I'll spend 30 seconds on this and I'll close a lost application that they've worked out, which I find kind of nice is to do web document corpus analysis, which I'm sure you're familiar with is I give you a bunch of documents.",
            "Can you identify which topic it is?",
            "Here again, this is the the suggestion is to use these low rank plus sparse model.",
            "So what you use as a as a data matrix you use a frequency of words occurring in each document.",
            "And what you could say is, well, documents are written in English, so there is a, you know the English language.",
            "Same words you know words will occur within the same frequency and so you have the low rank background model which is used in classical solutions to this problem.",
            "But then suppose that you actually say, well, you know I'm going to have this data matrix of frequency occurrences, but I'm going to try to model this as a lower rank.",
            "The background of the English language plus a sparse component.",
            "Now what is our is going to be the sparse component?",
            "This is going to be a key words.",
            "These are going to single out those words that occur at a frequency which is very unlike in any other document."
        ],
        [
            "And when you do this and you apply it to, you know data sets like this.",
            "What it does, it's it spits out.",
            "You look at the estimate and see which words.",
            "Appear in the sparse component.",
            "You know, and these words tell you much about what this article is about.",
            "OK, so it's parses a document and find those words that have a frequency of occurrence, which is very unlike other words and so then it's not difficult from this to kind of in fear what the topic might be."
        ],
        [
            "OK so I spoke for much too long even though the night is still young and so I thank you for your attention.",
            "Give examples of where this stuff can work and give a theoretical limit about where it might not work, but in practical applications, what are the limits of this that we tried?",
            "Things where it's not a good idea?",
            "Or can you give us some intuition about?",
            "Yeah, so well when the lowering structure is not so low rank anymore when the data set is not well approximated by a low rank, then sinks tends to break down quite badly.",
            "So when you don't have real underlying low rank structure then the sparse components becomes uninformative.",
            "Solo and component becomes an informative and you don't do well, but we've seen that this method is surprisingly robust because what we're trying to do here is robust statistics.",
            "We try to fit low rank models in the presence of small noise, Gaussian noise.",
            "And bad corruptions, and so because we use methods of robust statistics, we tend to have fairly robust results, but if we're not in a regime where the rank is low enough, things are not going to work well.",
            "Yeah.",
            "So all these pictures that you showed of this person but where?"
        ],
        [
            "Yes.",
            "Of course."
        ],
        [
            "You'd have a segmentation problem on top of it, but still.",
            "Yeah, but still you wouldn't go in the database and try to represent it as low rank.",
            "So what you'd have is you'd have, you know, I think it's a very good point what you do, but still you have this long structure becausw all of a sudden what you'd have is you'd have a lot of these low dimensional planes right, corresponding to different people, and as long as you can correctly identify your low dimensional plane would be fine.",
            "And I think you should be able to do this, but you're completely right in a way.",
            "This is supervised learning if you will.",
            "It's not completely unsupervised yet.",
            "What we should do next time I come back is we have several people in the database and all of a sudden we want to correct them.",
            "And not use the wrong people to correct, you know, not use Paul to correct Peter.",
            "Write an here.",
            "I have only pictures of Paul and so I completely agree with this.",
            "But I think they've done it and I think it works OK."
        ],
        [
            "Yeah, we don't understand this.",
            "We John Wright who was a postdoc of E at Microsoft Research, did this simulation a send me the plot?",
            "I said it's wrong, you know, redo it, he read it.",
            "We got it the same way we did it.",
            "We got it the same and we could not understand and we've done it 1000 times and we always get the same.",
            "Now here is be careful because here it means that the corruption I corrupted this in a very strange way.",
            "I corrupted with assigned pattern that exactly matches a low rank pattern.",
            "And what we see is that when the rank decreases, we do worse.",
            "And I'm not sure I understand it and that we've redone the simulation 1000 times because we could not believe it.",
            "And always we use the first one who picks on this.",
            "Which is an that I don't understand why.",
            "Absolutely not.",
            "But we've redone it, we've done it many, many times.",
            "Because I thought it's weird.",
            "In the random case, in the coherent case see here is the sign of S is the same as a sign of L?",
            "I don't know.",
            "Enjoy.",
            "Yes, this I don't know how far they've taken it.",
            "What they've done is that they've."
        ],
        [
            "I don't know, I cannot answer this question.",
            "What they've seen is I've seen that suppose I hide the title and I look at this.",
            "The keywords highlighted by in S or the sparse term.",
            "Often their words on the title, so they've been able to do some correlation between the frequency of words being picked up blindly with words on the titles that were hidden.",
            "But I don't know.",
            "I don't think they've been all the way to actually classification.",
            "These people they work on so many things.",
            "I don't know.",
            "I mean, he has like a team of people they work on so many things that it's hard for me to keep up with what they do.",
            "Perhaps some of you are Microsoft Research.",
            "Nobody's from Microsoft Research in Beijing.",
            "Sorry.",
            "Straightening examples.",
            "We are.",
            "Yes.",
            "Yes, absolutely.",
            "No, no.",
            "I mean this is done with.",
            "This is done for like a certain kind of structures and I don't think it would work for other kind of structured structures that do not have a bit of geometry built in of the kind you discuss that would not work well.",
            "There is some structure there.",
            "I don't think I think the thing that you have to be worried about when you do think you said you don't want overparameterized stuff because we already kind of as you know, sometimes things break down so you don't want to put too many unknowns, and like which deformation would make the structure you know 'cause you can try to find for more complicated.",
            "If your morphism like you know.",
            "But why is it that my views are projective and not warps more complicated warps when you do this you start to have too many parameters so you have to be careful about a few things.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what I decided to tell you about is about some work.",
                    "label": 0
                },
                {
                    "sent": "The talk is going to be a bit conceptual, not very technical, but conceptual about some recent work about the theory of low rank modeling and so in the first part of the talk I'm going to talk a lot about what we have learned through the modeling.",
                    "label": 1
                },
                {
                    "sent": "With the low rank matrices and then and the second part I will show you what I believe are kind of exciting applications in computer vision.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I will say that in my field of research is an explosion of research on the theory of low rank modeling.",
                    "label": 0
                },
                {
                    "sent": "When I grew up.",
                    "label": 0
                },
                {
                    "sent": "I'll tell you a little story to start with.",
                    "label": 0
                },
                {
                    "sent": "When I grew up, I worked on sparsity an it was weird because nobody was working on sparsity and now it seems that if you try to publish a paper, at least in my field without the word sparsity in the abstract, good luck.",
                    "label": 0
                },
                {
                    "sent": "But I can see the same stuff happening now, not use sparse models but using low rank models.",
                    "label": 0
                },
                {
                    "sent": "And that's what I would like to discuss.",
                    "label": 0
                },
                {
                    "sent": "Some of the works I'm going to present is ours and some of it is not.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm just going to talk about two very simple things.",
                    "label": 0
                },
                {
                    "sent": "One is matrix completion and the other one is ways of doing dimensionality reduction in a very robust way, which is I think important for people working in machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's talk with the first topic.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix completion if you do not know the Netflix Prize, check the name of their summer school again, so I assume that everybody is familiar with the Netflix price.",
                    "label": 0
                },
                {
                    "sent": "Everybody knows what is.",
                    "label": 0
                },
                {
                    "sent": "Do we have people who don't know what is the Netflix price?",
                    "label": 0
                },
                {
                    "sent": "No kidding.",
                    "label": 0
                },
                {
                    "sent": "You're not serious, are you?",
                    "label": 0
                },
                {
                    "sent": "You are serious.",
                    "label": 0
                },
                {
                    "sent": "I'm so old.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, so the Netflix Prize is a major initiative that was launched by a company named Netflix.",
                    "label": 0
                },
                {
                    "sent": "Netflix rents movies out to users.",
                    "label": 0
                },
                {
                    "sent": "They have about half a million users at the time.",
                    "label": 1
                },
                {
                    "sent": "They launched the price and they were renting about 18,000 titles and after you it was a Mail service.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you go on the Internet and you say you want to rent a movie.",
                    "label": 0
                },
                {
                    "sent": "And then Netflix would email user DVD would watch the movie, we return the DVD, they would Mail user next one and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "But very quickly what they would ask you is via email is what whether or not you like the movies actually ask you to rate the movie you had just seen.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so users would go out and sparsely enter some entries in a huge database that Netflix was assembling an it's a database in which we have columns which are movies and about their 20,000 of them at the time they were about 500,000 users, so it's a huge data matrix.",
                    "label": 0
                },
                {
                    "sent": "And when you see us cross in this data matrix, it's a movie that as user has rated, of course this data matrix is extraordinary, sparsely sampled because users on average random at.",
                    "label": 0
                },
                {
                    "sent": "30 to 50 movies.",
                    "label": 0
                },
                {
                    "sent": "And so instead of having 18,000 ratings, we have about 30 to 50.",
                    "label": 0
                },
                {
                    "sent": "And what?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Netflix wants if they want to complete the Netflix Matrix.",
                    "label": 1
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "That there are many entries that are not seen that have many movies that have not been rated by users.",
                    "label": 0
                },
                {
                    "sent": "And Netflix launched the price.",
                    "label": 0
                },
                {
                    "sent": "The Netflix price, where people were to do challenge to make predictions about items or entries of the Matrix you had not seen.",
                    "label": 0
                },
                {
                    "sent": "And why wouldn't Netflix wants to do such a thing?",
                    "label": 0
                },
                {
                    "sent": "I think the reason is pretty obvious, which is, of course, if you can predict accurately movies that people have not yet seen.",
                    "label": 0
                },
                {
                    "sent": "And of course we can build an efficient recommender system and so of course we can generate a lot of cost.",
                    "label": 0
                },
                {
                    "sent": "You have lots of customers, lots of happy customers, and so you generate a lot of income.",
                    "label": 0
                },
                {
                    "sent": "Now of course, as you can imagine, there's not only Netflix interested in such things, but you know Facebook has the same problems.",
                    "label": 0
                },
                {
                    "sent": "Apple sending music has the same problems.",
                    "label": 0
                },
                {
                    "sent": "You know everywhere you go in the online Commerce.",
                    "label": 0
                },
                {
                    "sent": "People would like to in fear preferences for items based on few preferences.",
                    "label": 0
                },
                {
                    "sent": "So there are many such problems.",
                    "label": 0
                },
                {
                    "sent": "This is an area called collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "In statistics we have similar problems in which we have questionnaires.",
                    "label": 0
                },
                {
                    "sent": "For example, this could be questions.",
                    "label": 0
                },
                {
                    "sent": "This could be.",
                    "label": 0
                },
                {
                    "sent": "It could be questions.",
                    "label": 0
                },
                {
                    "sent": "This could be people being surveyed and some people may not want to answer questions and you would like to know what if they had answered the question.",
                    "label": 0
                },
                {
                    "sent": "OK, so for those of you who do not know about the Netflix Prize, essentially Netflix offered $1,000,000 to whomever would be able to come up with a prediction algorithm that was besting their own in-house algorithm by 10%.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another problem which is not the Netflix price.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to try to show you that this matrix completion occurs in many different areas of science and engineering and other things that happens in electrical engineering.",
                    "label": 0
                },
                {
                    "sent": "This time is like what you've got is.",
                    "label": 0
                },
                {
                    "sent": "We've got points in space with US XJ and what you know is you have partial informations about those pairwise distances, so you have a matrix L which records pairwise distances between points.",
                    "label": 0
                },
                {
                    "sent": "I endpoint J, but the problem is you can only see a few of these pairwise distances, perhaps because you have a sensor net, so you have a wireless network where people or sensors can only construct a distance estimate to their nearest neighbors, but not too too sensors that are far away because of power constraints, obviously.",
                    "label": 1
                },
                {
                    "sent": "OK, so now you've got this data matrix, which is very large an what you would like to know is you'd like given pairwise local distances can actually recover all distances and why.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it important support important?",
                    "label": 0
                },
                {
                    "sent": "Because in sensornets we always want to know where stuff is and so local localization of sensors is the same as.",
                    "label": 0
                },
                {
                    "sent": "Of course if I can localize the sensors, I can compute all global pairwise distances.",
                    "label": 0
                },
                {
                    "sent": "But it goes.",
                    "label": 0
                },
                {
                    "sent": "It's a two way St in the sense that if you can't compute all pairwise distances, and of course you know the location of the sensors up to a rigid transformation, and so the problem is from a few entries of this matrix, can I actually recover all global distances so that I can locate the sensors?",
                    "label": 0
                },
                {
                    "sent": "So it's another problem in which we have entries of a matrix of a very diff.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kinda obviously the Netflix data Matrix, where there are lots of missing entries and we'd like to in fear the value of those missing entries.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many other problems.",
                    "label": 1
                },
                {
                    "sent": "A great friend of my leaving is going to talk next week about convex optimization.",
                    "label": 0
                },
                {
                    "sent": "Perhaps they show it will show you applications of matrix completion in control and system identification.",
                    "label": 1
                },
                {
                    "sent": "You have exciting applications in quantum state tomography in quantum mechanics, which are fantastic.",
                    "label": 0
                },
                {
                    "sent": "They do to David Gross you have problems of this kind in signal processing, of course it's a big problem in machine learning and computer vision.",
                    "label": 1
                },
                {
                    "sent": "So this problem of trying to be to have partial information about your large data matrix and try to recover in fear the entire matrix is a problem that comes up all the time.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is a matrix completion problem?",
                    "label": 0
                },
                {
                    "sent": "The mystery completion problem is something like this.",
                    "label": 0
                },
                {
                    "sent": "We've got a data matrix L which I'm going to call L. We observe a subset of its entries.",
                    "label": 1
                },
                {
                    "sent": "Very few, perhaps in the Netflix case, it's below a percent and we would like to get some.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Missing entries so very simply stated and everybody would agree that it looks a bit daunting.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so obviously I'm not going to be able to do to guess all the missing entries if I don't make some kind of a structural assumption and the structural assumption we're going to make it that oftentimes the data matrix we see in practice have low rank, approximately low rank.",
                    "label": 0
                },
                {
                    "sent": "So now we deal with massive amounts of data, such as a Netflix matrix, but typically really only the dimension of the phenomenon we care about, or that we're looking at is much lower.",
                    "label": 0
                },
                {
                    "sent": "Then the ambiance dimension suggests and so in many applications of interest, the matrix wish to recover is not unstructured.",
                    "label": 0
                },
                {
                    "sent": "It is structured in the sense that it has low rank or approximately lower.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In fact, it is true of all the applications that I'm familiar with.",
                    "label": 0
                },
                {
                    "sent": "And so it is certainly true of the Netflix matrix, and that's why we teach, by the way, things like principal component and factor analysis in statistics.",
                    "label": 1
                },
                {
                    "sent": "In fact, if you look at the pieces of the Netflix data matrix and you look at the SVD, you'll see a sharp drop in the singular values after forgot exactly the numerical value, but quite rapidly.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, only a few factors capture people preferences for movies.",
                    "label": 0
                },
                {
                    "sent": "You know what's the most important factor that determines a lot of variation in their data.",
                    "label": 0
                },
                {
                    "sent": "In the Netflix matrix.",
                    "label": 0
                },
                {
                    "sent": "An attribute about a movie.",
                    "label": 0
                },
                {
                    "sent": "Yes, is there a big Hollywood star in a movie or not?",
                    "label": 0
                },
                {
                    "sent": "And that's capturing a lot of the of the of people's preferences.",
                    "label": 0
                },
                {
                    "sent": "There are people who are turned down by celebrities and others were not.",
                    "label": 0
                },
                {
                    "sent": "There is also the center net matrix or sensornet matrix which records pairwise distances, is very low rank.",
                    "label": 1
                },
                {
                    "sent": "For example, is a sensors leave in 2 dimensional space and these matrix has rank two if it leaves the sensors leave in 3D space aranki 3 and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "So we have very low rank and in fact all the problems that we have in computer vision and system identification in quantum state tomography.",
                    "label": 0
                },
                {
                    "sent": "In all these problems we have very little information about a huge data matrix, but we can safely assume that the matrix has low rank.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then I'd like to, oh, by the way, this is an advertising for a conference that will take place in Washington DC.",
                    "label": 0
                },
                {
                    "sent": "The Netflix Prize has been claimed by the team by AT&T by Robert Bell and his colleagues.",
                    "label": 0
                },
                {
                    "sent": "This gentleman who made a lot of work.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you know him, lesser Mikey, who came with even a slightly better prediction.",
                    "label": 0
                },
                {
                    "sent": "But 20 minutes later?",
                    "label": 0
                },
                {
                    "sent": "And the price has been awarded an then there will be a conference, a public conference in Washington DC telling you a little bit about the history of the Netflix price.",
                    "label": 0
                },
                {
                    "sent": "It set up things about the winning algorithms underlying the winning teams and the key ingredient of the AT&T algorithm is low rank.",
                    "label": 0
                },
                {
                    "sent": "It starts with trying to find a factorization of this Netflix matrix as lowering and then it has another component.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you want to know more about the workings of the Netflix competition, I invite you to Washington on November 4th and I'm also a speaker at that conference because what I find is I heard about the Netflix price on the radio going to Caltech when I was still at Caltech.",
                    "label": 0
                },
                {
                    "sent": "I thought that this was a fascinating problem, and so then I thought about this idea of recovering matrices from what seems to be too little information, and I started to think about.",
                    "label": 0
                },
                {
                    "sent": "And then I could see a lot of connections between the Netflix price and other people also problems that were floating around.",
                    "label": 0
                },
                {
                    "sent": "And so I will explain some of these connections.",
                    "label": 0
                },
                {
                    "sent": "OK, but the low rank structure plays an enormous role in a thing.",
                    "label": 0
                },
                {
                    "sent": "It's 90% as, as Rob once told me of their solution, right?",
                    "label": 0
                },
                {
                    "sent": "It's low rank plus ansambel methods essentially.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now if we assume that the matrix has low rank.",
                    "label": 0
                },
                {
                    "sent": "Then I'd like to ask the question again.",
                    "label": 0
                },
                {
                    "sent": "We can think about the low rank decomposition of a matrix via its singular value decomposition, for example, which is a simple factorization of the matrix that everybody knows.",
                    "label": 0
                },
                {
                    "sent": "So then we've got M = U, Sigma, V star, and so when I look at the number of degrees of freedom, that is a number of real numbers that I need to specify Knew Sigma and vystar to specify a matrix of rank R of dimension N1 and N2, then we can count the number of.",
                    "label": 1
                },
                {
                    "sent": "Real numbers you see in these factors and if you do this properly, you'll see that the number of degrees of freedom in arrancar matrix, which is rectangular.",
                    "label": 0
                },
                {
                    "sent": "An one by N 2 is the sum of the dimension times the rank essentially OK.",
                    "label": 0
                },
                {
                    "sent": "So if N is 10,000 and one is 10,000 thousand and two is another 10,000, you have a matrix with 100 million entries.",
                    "label": 0
                },
                {
                    "sent": "But it does not have a number of degrees of freedom, which is 100 million is essentially 20,000 times a rank.",
                    "label": 0
                },
                {
                    "sent": "So if the rank is 20, you know it's a much very small fraction of the ambient dimension OK, and so now we can ask the question again.",
                    "label": 0
                },
                {
                    "sent": "Well, do I need to see everything to actually recover a structure that has intrinsically a number of degrees of freedom, which is much lower than the ambient dimension suggests?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An OK and that's a question.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course we need to make this and so that's a conceptual part of the presentation.",
                    "label": 0
                },
                {
                    "sent": "We need to make this well posed and to make this well posed.",
                    "label": 0
                },
                {
                    "sent": "What we have is, well, it depends what I decide to show you because I could decide to show you all the entries of the Matrix except a row for example.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in the case of the Netflix price in the next weeks competition it would be I show you all the entries of so there's a user that has not rated a single item and I claim well, if you've not seen this person, how can you predict his or her preferences?",
                    "label": 0
                },
                {
                    "sent": "You can't, right?",
                    "label": 0
                },
                {
                    "sent": "So even if you have a rank one matrix and you don't see Aurora column, then recovery is obviously not possible.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we assume and you know it's a strong modeling assumption.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that, at least for the purpose of this talk, is that.",
                    "label": 0
                },
                {
                    "sent": "Well, let's assume that the entries that are revealed to you have been selected at random, not because I believe in probability stick sampling, but because I'm interested in discussing what happens in most cases, perhaps not in all cases, but in most cases.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have a random subset of entries of a low rank matrix and I say well.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's still a tough problem and the reason it's a tough problem is because well, here is an example of a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Right, so it's a very low rank matrix or matrix of rank one.",
                    "label": 0
                },
                {
                    "sent": "And if I show you I don't know in Netflix shows you .4% of the entries or something ridiculous like this.",
                    "label": 0
                },
                {
                    "sent": "If I show you even here, 10% of the entries 90% of the time you're going to actually miss this one, and so you're going to see only zeros, and you're going to tell me that the matrix is.",
                    "label": 0
                },
                {
                    "sent": "It has low rank.",
                    "label": 0
                },
                {
                    "sent": "You've only seen zeros.",
                    "label": 0
                },
                {
                    "sent": "Zero and then you would make a big mistake.",
                    "label": 0
                },
                {
                    "sent": "Now of course, this matrix is very special.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case of the Netflix matrix.",
                    "label": 0
                },
                {
                    "sent": "I certainly do not believe that it looks like this because it would look like this if and only if nobody likes anything except that there is a guy that likes a movie.",
                    "label": 0
                },
                {
                    "sent": "That is not the way it looks or in the sensor net application.",
                    "label": 0
                },
                {
                    "sent": "It would say all the sensors are the exactly the same location and one is far away.",
                    "label": 0
                },
                {
                    "sent": "Right, which is not reality.",
                    "label": 0
                },
                {
                    "sent": "So this matrix is very peculiar.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another example of a matrix which obviously cannot be recovered from a small set of entries for the same reason is like I don't know if I show you 10% of the entries, this matrix has rank two.",
                    "label": 1
                },
                {
                    "sent": "Obviously if I show you 10% of the entries you're going to miss with very high probability you're going to miss one of these four entries.",
                    "label": 0
                },
                {
                    "sent": "And then what do you do?",
                    "label": 0
                },
                {
                    "sent": "You can't recover them, it's impossible.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To me, the protonic particle, the most basic example of a low rank matrix that you cannot recover.",
                    "label": 0
                },
                {
                    "sent": "Something like this.",
                    "label": 0
                },
                {
                    "sent": "What I've got here is I've got a data matrix, will have a rose, which is a vector X.",
                    "label": 0
                },
                {
                    "sent": "It's a rank one matrix obviously, and then everything else is 0 and then you sample these matrix at random.",
                    "label": 0
                },
                {
                    "sent": "I show you even half of the data and then of course you're going to miss half of the entries upstairs.",
                    "label": 0
                },
                {
                    "sent": "And how can you know what they are?",
                    "label": 0
                },
                {
                    "sent": "You will not.",
                    "label": 0
                },
                {
                    "sent": "So what are we learning through these things?",
                    "label": 0
                },
                {
                    "sent": "We're learning that somehow to be able to complete the matrix.",
                    "label": 0
                },
                {
                    "sent": "Like I think this is a good example to complete the matrix.",
                    "label": 0
                },
                {
                    "sent": "If I have rows that are orthogonal to other rows, that if I have singular rose.",
                    "label": 0
                },
                {
                    "sent": "That orthogonal to everybody else, there is no way on Earth that I can set in motion the principle of collaborative filtering.",
                    "label": 0
                },
                {
                    "sent": "If somebody is rating movies at random such that this person's ratings are orthogonal to everybody else.",
                    "label": 0
                },
                {
                    "sent": "How can I learn his rating from other people like this person?",
                    "label": 0
                },
                {
                    "sent": "There are none.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so another way to say what I just said is that if somehow they are singular rows and columns, that is what it means in terms of the SVD is that if some singular vectors or if you would like to speak like a statistician if some principal components happen to be very sparse.",
                    "label": 0
                },
                {
                    "sent": "Like in this case because look, we see a singular vector.",
                    "label": 0
                },
                {
                    "sent": "It's E one.",
                    "label": 0
                },
                {
                    "sent": "It's extremely sparse, then vector like this create a single row, a singular row, then matrix completion is obviously impossible.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we have in the previous example we have very sparse singular vectors supported by the 1st two basis vectors and matrix recovery is impossible.",
                    "label": 0
                },
                {
                    "sent": "So if somehow the principal components of this data matrix are sparse, you're in big trouble.",
                    "label": 0
                },
                {
                    "sent": "An I believe from my conversation with.",
                    "label": 0
                },
                {
                    "sent": "I forgot whom that there are some.",
                    "label": 0
                },
                {
                    "sent": "Well, in the Netflix case, there are some movies that are very hard to predict exactly.",
                    "label": 0
                },
                {
                    "sent": "For this reason.",
                    "label": 0
                },
                {
                    "sent": "For example, good example is Napoleon dynamite like nobody knows whether they like or not.",
                    "label": 0
                },
                {
                    "sent": "These movies and the rating seems to be a bit random.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, alright, So what we're going to do a mathematically is we're going to try to quantify the degree with which the singular vectors are sparse, and we're going to do this geometrically, and that's almost my my most technical slide here.",
                    "label": 0
                },
                {
                    "sent": "Which is that we're going to quantify the degree of correlation between the basis vectors and the column and the row space of your matrix.",
                    "label": 0
                },
                {
                    "sent": "And so we're going to introduce a very simple definition.",
                    "label": 0
                },
                {
                    "sent": "We have our low rank matrix, which we think of it as we are.",
                    "label": 0
                },
                {
                    "sent": "Is singular value decomposition, and we're going to introduce a coherence, and coherence is simply what I'm going to do is.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at this basis vector EI.",
                    "label": 0
                },
                {
                    "sent": "I'm going to project them onto the column space of the Matrix U and I'm going to look at the norm of these vectors.",
                    "label": 0
                },
                {
                    "sent": "Now if the column space is orthogonal to EI then.",
                    "label": 0
                },
                {
                    "sent": "Of course the norm is 0 if the column space contains EI is enormous one.",
                    "label": 0
                },
                {
                    "sent": "And So what I'm going to do is, I'm going to look at the maximum of this projection, and I'm going to introduce a parameter mu, which qualifies the degree with which the column space is aligned with the basis vector by being essentially the maximum norm of this projection, renormalized by the dimension on which on the space you projecting onto on the rank divided by Zambian dimension.",
                    "label": 0
                },
                {
                    "sent": "OK, and so I'm going to skip the condition below.",
                    "label": 0
                },
                {
                    "sent": "I'm going to look at focus at the top condition.",
                    "label": 0
                },
                {
                    "sent": "The condition said that if mu is small then it seems like the column, the column space, and aerospace of your matrix are not well aligned with the coordinate axis.",
                    "label": 0
                },
                {
                    "sent": "If mu is large then the column space and the row space are well aligned with the coordinate axis and if it's large venue in a situation that looks like this.",
                    "label": 0
                },
                {
                    "sent": "Here's a situation where mu is maximum because as you can see the column space contains.",
                    "label": 0
                },
                {
                    "sent": "E1, so it's actually contains.",
                    "label": 0
                },
                {
                    "sent": "This space is vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so another way to think about this is to say, well, we want a small mu that is we don't want a singular.",
                    "label": 0
                },
                {
                    "sent": "Sparse singular vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, and so from now on we can think about singular vectors are being not too sparse.",
                    "label": 0
                },
                {
                    "sent": "An mu essentially qualifies the sparsity of this singular vectors.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the 1st result we've got in this field is something that has an information theoretic nature.",
                    "label": 0
                },
                {
                    "sent": "It says that essentially there is a fundamental role played by this coherence.",
                    "label": 1
                },
                {
                    "sent": "And this role is that the number is the number of samples you get to see from this huge data matrix is smaller than this parameter, mu, which essentially qualifies a correlation between the basis vectors and the column and row space times.",
                    "label": 0
                },
                {
                    "sent": "So here we have an N by N matrix times the number of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Because we've seen that the number of degrees of freedom is essentially dimension times a rank.",
                    "label": 0
                },
                {
                    "sent": "Times a log factor.",
                    "label": 1
                },
                {
                    "sent": "Then you a number of samples that is smaller than this quantity, then no method whatsoever will work.",
                    "label": 0
                },
                {
                    "sent": "So that's in a way you know it's like a lower bound on what you can do, so it really exemplifies a fundamental role played by the coherence.",
                    "label": 0
                },
                {
                    "sent": "It said that you need to sample the matrix.",
                    "label": 0
                },
                {
                    "sent": "You need to get a number of entries which is proportional to coherence.",
                    "label": 0
                },
                {
                    "sent": "And in fact, this number of entries need to be proportional to this geometric parameters equation stands the degrees of freedom's times a log factor, and if you sample at a number of entries which is below this number, then nothing will work.",
                    "label": 0
                },
                {
                    "sent": "No algorithm on their schoolwork.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to assume from now on the coherence is small, so it's order one, and so when the coherence is small, that is when we have non sparse principal components then what the theorem says that for any method to work I need to see a number of entries which is the dimension of the matrix times a rank which we think about as a really number of degrees of freedom in a matrix times a log factor.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we know what we cannot do, and then what is it that we can do?",
                    "label": 0
                },
                {
                    "sent": "Well, well, so now you give me samples at random from a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Maybe the coherence is the singular vectors are not crazy, so something can be done.",
                    "label": 0
                },
                {
                    "sent": "What is it that I'm going to do?",
                    "label": 0
                },
                {
                    "sent": "Well, what I would want to do is that I would want to minimize.",
                    "label": 0
                },
                {
                    "sent": "I would say, well, you know I'm looking for a rank low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "Why don't I just try this?",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to find a matrix with minimum rank that fits the data I have just.",
                    "label": 0
                },
                {
                    "sent": "Observed.",
                    "label": 0
                },
                {
                    "sent": "And that's a very good idea.",
                    "label": 0
                },
                {
                    "sent": "That's what you should be doing.",
                    "label": 0
                },
                {
                    "sent": "The problem is, as I'm sure most of you know, I do not have the right to talk about this becausw.",
                    "label": 0
                },
                {
                    "sent": "It's a NP hard problem, so NP hard.",
                    "label": 0
                },
                {
                    "sent": "I don't even know what that means.",
                    "label": 0
                },
                {
                    "sent": "I mean, I should not say this because I'm videotaped I know what I I know what is NP hard.",
                    "label": 0
                },
                {
                    "sent": "This is a problem with tape lectures.",
                    "label": 0
                },
                {
                    "sent": "You cannot really speak the way you want to speak.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm NP hard now.",
                    "label": 0
                },
                {
                    "sent": "I know what that means.",
                    "label": 0
                },
                {
                    "sent": "But what it means practically, it means in this case that I cannot solve a problem when the dimension of the matrix is 10 by 10, because the best algorithms we know to actually minimize rank under equality constraints.",
                    "label": 0
                },
                {
                    "sent": "Is double exponential in N?",
                    "label": 0
                },
                {
                    "sent": "So maybe you are very hard on a supercomputer and you can do N = 10, While what that means you cannot do any calls 11.",
                    "label": 0
                },
                {
                    "sent": "OK, so we cannot do it, so we cannot talk about it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're going to do instead is we're going to minimize another norm, which is in some sense a proxy for the rank.",
                    "label": 0
                },
                {
                    "sent": "And that Norm is, well, we're going to minimize the sum of the singular values of the matrix, right?",
                    "label": 0
                },
                {
                    "sent": "That's the norm.",
                    "label": 0
                },
                {
                    "sent": "It's called the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "It has nothing to do is nuclear physics.",
                    "label": 0
                },
                {
                    "sent": "It's actually a harmless norm.",
                    "label": 0
                },
                {
                    "sent": "You minimize the sum of the singular values subject to the same constraints.",
                    "label": 1
                },
                {
                    "sent": "Now do people know what is the nuclear norm?",
                    "label": 0
                },
                {
                    "sent": "Is it the first time you see it or not?",
                    "label": 0
                },
                {
                    "sent": "Because if you have seen it many times, and I I'd be happy to skip.",
                    "label": 0
                },
                {
                    "sent": "It is some of the singular values of a matrix.",
                    "label": 0
                },
                {
                    "sent": "So first of all, I'm sure everybody knows what is the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "And why do we use the L1 norm?",
                    "label": 0
                },
                {
                    "sent": "Because it's a good proxy for.",
                    "label": 0
                },
                {
                    "sent": "L 0 here it's the same thing.",
                    "label": 0
                },
                {
                    "sent": "It's the L1 norm of the singular value, so it's a good proxy for the L0 norm of the singular value.",
                    "label": 0
                },
                {
                    "sent": "But what is the L0 norm of the singular value?",
                    "label": 0
                },
                {
                    "sent": "It's a rank, so the L1 norm is a good proxy for the rank.",
                    "label": 0
                },
                {
                    "sent": "Now there's one way which is much more sophisticated to see it, and I'm going to try to draw a picture and that picture is this, which is that the way I think about there is a one norm personally is that I look at sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we scored an 8 plus or minus one so here they are.",
                    "label": 0
                },
                {
                    "sent": "So I have four of them because I can only draw in the plane and what the L1 ball is.",
                    "label": 0
                },
                {
                    "sent": "It's another way to see that it says tightest convex relaxation of an LO problem where the L1 ball is it actually the tightest convex bodies that contain sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "And that's why we use the L1 norm because it's smallest convex body that contains very sparse vectors.",
                    "label": 0
                },
                {
                    "sent": "Of no one.",
                    "label": 0
                },
                {
                    "sent": "The nuclear norm is exactly like this.",
                    "label": 0
                },
                {
                    "sent": "Now I cannot draw unfortunately, but the nuclear norm is exactly like this.",
                    "label": 0
                },
                {
                    "sent": "Now I can look at well what are very low rank matrices.",
                    "label": 0
                },
                {
                    "sent": "While these are rank one matrices, we say Norm lesson 1 and I say what's a convex body is a smallest convex body that contains them all, and that's a nuclear ball.",
                    "label": 0
                },
                {
                    "sent": "Cancel in that sense, the nuclear norm is the tightest relaxation of the of the rank functional.",
                    "label": 1
                },
                {
                    "sent": "Just like the L1 is a tireless relaxation of the LO function.",
                    "label": 0
                },
                {
                    "sent": "Now, it's not obvious that the sum of singular values is a norm, and I don't know if I have slides about this.",
                    "label": 0
                },
                {
                    "sent": "No, I don't have flights about this.",
                    "label": 0
                },
                {
                    "sent": "What I'll say though, is that the nuclear norm, the L1 norm for those of you know the L1 norm, is a dual 2Z.",
                    "label": 0
                },
                {
                    "sent": "An Infinity norm and the Elder do alter the nuclear norm is going to be the DeWalt tool.",
                    "label": 0
                },
                {
                    "sent": "The maximum I think value, which is the usual matrix norms as operator norm and So what?",
                    "label": 0
                },
                {
                    "sent": "I claim that the nuclear norm X star.",
                    "label": 0
                },
                {
                    "sent": "Is actually is a supremum over all matrices of norm?",
                    "label": 0
                },
                {
                    "sent": "Usual normal lesson one of you dot X.",
                    "label": 0
                },
                {
                    "sent": "Yes, it's exactly like what you have for the L1 norm and now of course this is not obvious that this is a case, but if you say this is that this is true, then it's obvious that the nuclear norm is unknown because it's a dual norm.",
                    "label": 0
                },
                {
                    "sent": "OK, um, alright, so we have these proxy for the rank and so we're going to minimize this proxy instead.",
                    "label": 0
                },
                {
                    "sent": "Now what you'll see with leave in an this I'm pretty sure you'll show you this is that this is a nice convex optimization program.",
                    "label": 0
                },
                {
                    "sent": "Why because while you minimize the Norman norm is convex, you have linear equality constraints are convex set, so you have a nice convex optimization program.",
                    "label": 0
                },
                {
                    "sent": "And I.",
                    "label": 0
                },
                {
                    "sent": "It's actually a semidefinite program.",
                    "label": 0
                },
                {
                    "sent": "For those of you know about semidefinite programming and our field, which I'm not going to have time to discuss too much about, our field has spent years trying to develop algorithms that solve this thing efficiently.",
                    "label": 0
                },
                {
                    "sent": "So for example, now we can solve problems in which you have matrices, which are I don't know, have a billion entries in in a few minutes on a desktop PC.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is normal.",
                    "label": 0
                },
                {
                    "sent": "Interestingly enough, has been used before.",
                    "label": 0
                },
                {
                    "sent": "We used it essentially in control by mess dahiana papavasiliou polos and buyback and Andrea, Anne Marie and Fazel champions.",
                    "label": 0
                },
                {
                    "sent": "The use of her norm.",
                    "label": 0
                },
                {
                    "sent": "In 2002, Anne for matrix problems, he was actually introduced by Rasht, Salem and Parrillo.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now the result is like this.",
                    "label": 0
                },
                {
                    "sent": "You have a result that I find a bit surprising when you think about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we have a result.",
                    "label": 0
                },
                {
                    "sent": "We've got our matrix completion problem.",
                    "label": 0
                },
                {
                    "sent": "We see very few entries, perhaps point 1%.",
                    "label": 0
                },
                {
                    "sent": "There are many entries we do not see.",
                    "label": 0
                },
                {
                    "sent": "In fact, the huge majority of entries we do not get to see.",
                    "label": 0
                },
                {
                    "sent": "And how do we going to guess the entries by just minimizing this nuclear norm subject to equality constraints?",
                    "label": 0
                },
                {
                    "sent": "And uh, result due to myself and Terence Tao shows the following, which is well, we're going to have a matrix of rank R. You're going to show me a random set of size M and what we were able to show is that if the number of centuries is greater than the number of degrees of freedom times a log factor where we showed that the power of the log is less than six and sometimes equals to two, then this semidefinite programs is simple.",
                    "label": 1
                },
                {
                    "sent": "Optimization program magically recovers all the missing entries.",
                    "label": 0
                },
                {
                    "sent": "With no error whatsoever.",
                    "label": 0
                },
                {
                    "sent": "Right, so you've got very few entries that are visible.",
                    "label": 0
                },
                {
                    "sent": "You try to guess the other entries by minimizing this simple convex functional, and then there's this surprising result that occurs, which is that this solution at hot when you look at it, it has filled in all the missing values exactly with no error whatsoever.",
                    "label": 0
                },
                {
                    "sent": "OK David Gross at whom I mentioned earlier, has the best result known to date.",
                    "label": 0
                },
                {
                    "sent": "You was involved to improve on this power of log, and we're showing and managed to show that.",
                    "label": 0
                },
                {
                    "sent": "Well, the the number of entries is greater than coherence times the degrees of freedom time.",
                    "label": 0
                },
                {
                    "sent": "Log N squared exact matrix completion occurs by convex optimization.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so there is a lot of work.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned going on in this literature, so matrix completion as a field is a field that is exploding really rapidly.",
                    "label": 0
                },
                {
                    "sent": "People are discovering new applications all the time at this are discussing new algorithms for solving these very large scale problem because like if you think about the matrix with a billion entries, you have to solve a problem with a billion unknowns.",
                    "label": 0
                },
                {
                    "sent": "This is not something that you can take lightly and so there's a lot of activity on the theoretical front.",
                    "label": 0
                },
                {
                    "sent": "On the applications front end on the algorithmic front and so and so, a lot of people mentioned on this slide have done some very important work.",
                    "label": 0
                },
                {
                    "sent": "For example, I would like to mention the work of my colleague Andrea Montanari an of his grad students who have shown a different algorithm, not based on convex optimization, that also can fill in the missing entries.",
                    "label": 0
                },
                {
                    "sent": "Always, no error.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so why does this work?",
                    "label": 0
                },
                {
                    "sent": "Why is it that magically it it works?",
                    "label": 0
                },
                {
                    "sent": "I don't know why it works really.",
                    "label": 0
                },
                {
                    "sent": "I can prove it, but I will try to make a picture.",
                    "label": 0
                },
                {
                    "sent": "So the first picture I have to show you is why does L1 work well?",
                    "label": 0
                },
                {
                    "sent": "Why is it that when you minimize the L1 norm, recover sparse signal?",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to try to draw an analogy.",
                    "label": 0
                },
                {
                    "sent": "So what you have is when you solve an L1 problem, you recover a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "And why is this because?",
                    "label": 0
                },
                {
                    "sent": "What you say is like.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a sparse vector which is here, and it's sparse because this code is 0 and this one is not zero.",
                    "label": 0
                },
                {
                    "sent": "And then you take one measurements.",
                    "label": 0
                },
                {
                    "sent": "I'd just make one measurement and this vector, and so we all know from middle school that, well, this gives you the equation of a line like this.",
                    "label": 0
                },
                {
                    "sent": "And then you say alright.",
                    "label": 0
                },
                {
                    "sent": "Well, if I were to find on this line a point with minimum L2 norm, well I would get this thing which is not good.",
                    "label": 0
                },
                {
                    "sent": "So in in low dimension it looks close, but in high dimension is a disaster.",
                    "label": 0
                },
                {
                    "sent": "If I use the L1 norm instead, so this is the L2 ball as we all know.",
                    "label": 0
                },
                {
                    "sent": "The L1 ball is like this as we've shown.",
                    "label": 0
                },
                {
                    "sent": "And so if I use the L1 norm instead, I'm going to say what's the point on DSL one?",
                    "label": 0
                },
                {
                    "sent": "On this line that has minimum L1 norm while well so well, I'm going to do is I'm going to grow this thing until it becomes tangent.",
                    "label": 0
                },
                {
                    "sent": "And so the point where the L1 ball is tangent is this, but it's tension at this vertex, and so it recovers.",
                    "label": 0
                },
                {
                    "sent": "A sparse solution actually.",
                    "label": 0
                },
                {
                    "sent": "So this I show you that L1, at least on this example, L1 optimization works.",
                    "label": 0
                },
                {
                    "sent": "And why is it it's because the L1 ball is very pointy.",
                    "label": 0
                },
                {
                    "sent": "Add add on sparse vectors.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For matrix completion, there's something similar that occurs which is at this.",
                    "label": 0
                },
                {
                    "sent": "Now we have the nuclear ball, which is pictorially represented by this cylinder.",
                    "label": 0
                },
                {
                    "sent": "So here you've got this set of two by two matrices, which are symmetric, because I can only draw in sometimes.",
                    "label": 0
                },
                {
                    "sent": "When I'm brave, I draw in 3D.",
                    "label": 0
                },
                {
                    "sent": "So here we have three parameters XYZ.",
                    "label": 0
                },
                {
                    "sent": "We have a symmetric matrix and I'm going which have nuclear norm less than one.",
                    "label": 0
                },
                {
                    "sent": "And what happens is that the low rank matrices are extreme points of this convex body.",
                    "label": 0
                },
                {
                    "sent": "And now we have our feasible set, which is huge dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But it actually happens to be tangent to this very pointy nuclear ball, because the nuclear ball is pointy at low rank solutions.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is very quick.",
                    "label": 0
                },
                {
                    "sent": "Geometric representation of why this work.",
                    "label": 0
                },
                {
                    "sent": "Then it's because of the pointiness of the nuclear norm or the nuclear ball at low rank solution that all of a sudden there's many ways you can have your feasible set and maintain this tension property.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But of course this is not a proof an I don't claim that it is.",
                    "label": 0
                },
                {
                    "sent": "OK, so now of course you say, well, you know you can recover from a subset of entries.",
                    "label": 0
                },
                {
                    "sent": "Can you recover from other type of information?",
                    "label": 0
                },
                {
                    "sent": "And of course yes you can.",
                    "label": 0
                },
                {
                    "sent": "An now by now there's a whole theory which is being developed to show very precisely that now you've got information about the matrix not given by revealing entries, but by revealing coefficients.",
                    "label": 0
                },
                {
                    "sent": "So now I give you all the kind of linear information about the matrix of interest L. So I give you inner product.",
                    "label": 0
                },
                {
                    "sent": "So before I was giving you an inner product between a matrix L&EIEJ star which essentially reveals Ellijay.",
                    "label": 0
                },
                {
                    "sent": "But now you can say, well, these matrices don't have to be.",
                    "label": 0
                },
                {
                    "sent": "The matrix is full of zeros and ones somewhere that just pick an element.",
                    "label": 0
                },
                {
                    "sent": "You can have anything you want and so now there is this general theory of saying, well, you know now I have linear information about a matrix linear information about a matrix of this kind.",
                    "label": 0
                },
                {
                    "sent": "I'm going to find among all those matrices, obeying these constraints that with minimum L1 norm an what I'm trying to say here is that if I have incoherence between the sensing matrices and the column space and the row space of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Everything should work, and indeed it works.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's we've done a piece of it with Ben.",
                    "label": 0
                },
                {
                    "sent": "Rushed, and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of time.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll skip this part.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Please big con.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tribu Shun is due to David Gross, who who.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explain and show exactly what kind of coherence you need.",
                    "label": 0
                },
                {
                    "sent": "An when.",
                    "label": 0
                },
                {
                    "sent": "Can you expect things to work, so I'm not going to go through the detail, I'm just going to tell you that it's available and it's out there.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's interesting about the contribution of David Gross is that is a physicist, a quantum physicist, and when he realises that there are lots of problems in quantum mechanics that are matrix completion problem and so I will just mention what it is.",
                    "label": 0
                },
                {
                    "sent": "Quickly, so in quantum mechanics you have a system and quantum mechanically.",
                    "label": 0
                },
                {
                    "sent": "If you have a case being 1/2 system quantum mechanically, it's represented by a matrix L, which is called a density matrix.",
                    "label": 1
                },
                {
                    "sent": "The problem is, as you know, as you may not know, the dimensioning grows very quickly that the power of quantum computing.",
                    "label": 0
                },
                {
                    "sent": "It grows exponentially in the number of particles you have, and so if you have a system with K particles you've got a dimension which is expansion of exponential in the number of particles.",
                    "label": 0
                },
                {
                    "sent": "So perhaps without going into too much into details that will try to explain what people try to do in this field so people prepare quantum states.",
                    "label": 0
                },
                {
                    "sent": "Like they prepare quantum systems.",
                    "label": 0
                },
                {
                    "sent": "And then how do they know that what they prepared is what they thought they prepared?",
                    "label": 0
                },
                {
                    "sent": "How do they measure that what they thought they prepare in the lab is actually what they thought they prepared.",
                    "label": 0
                },
                {
                    "sent": "They have to take measurements.",
                    "label": 0
                },
                {
                    "sent": "And what do you do when you take a measurement on a quantum system?",
                    "label": 0
                },
                {
                    "sent": "You destroy it while you modify it completely.",
                    "label": 0
                },
                {
                    "sent": "So because you're going to have to make a lot of measurements, why?",
                    "label": 0
                },
                {
                    "sent": "Because a number of degrees of freedom is a matrix which is ambion San Square.",
                    "label": 0
                },
                {
                    "sent": "It's huge, an end is exponential in the number of particles you're going to have to prepare a lot of quantum states.",
                    "label": 1
                },
                {
                    "sent": "Now a lot of the states that these guys prepare are actually low rank or approximately low rank.",
                    "label": 1
                },
                {
                    "sent": "And So what this theory says is that with far fewer number of measurement that you thought were possible, you can actually guess the quantum state of the system, because you're going to use a low rank structure of the system to do it OK. And So what David Rose did was to use the tools of matrix completion to actually show that you could solve this problem and he did much more.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so these are problems like this in quantum mechanics and people are actually using these techniques to actually interfere quantum states from quantum state tomography measurements.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm done with matrix completion in the second part of the talk I will talk a little bit about robust PCA, but this part is of course much shorter because we've introduced most of what we need now.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first of all it is if you add noise to matrix you could say yeah, it's interesting to recover a matrix which is exactly low rank from knows less data, but in practice you know I'm not exactly low rank, plus I have noisy data like the ratings are contaminated with noise.",
                    "label": 0
                },
                {
                    "sent": "It would be a pity if the whole theory we could not accommodate in exact measurements and So what you do when you have an exact measurement you do what you always do.",
                    "label": 0
                },
                {
                    "sent": "You don't enforce equality constraints.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "Now your data.",
                    "label": 0
                },
                {
                    "sent": "Why is inexact?",
                    "label": 0
                },
                {
                    "sent": "So you're going to.",
                    "label": 0
                },
                {
                    "sent": "Only enforce an approximate fit to the data and among all.",
                    "label": 0
                },
                {
                    "sent": "Low rank on all matrix L compatible with data.",
                    "label": 0
                },
                {
                    "sent": "Are you going to pick the one with minimum nuclear?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you do this, things work.",
                    "label": 0
                },
                {
                    "sent": "That is, you get an error which is proportional to the noise level and another way of saying this is that when matrix completion from noiseless data occurs, then if you have a smaller amount of noise, the error will be small as well.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's a big field of research, again, by no means.",
                    "label": 0
                },
                {
                    "sent": "We are the only one to work on this.",
                    "label": 0
                },
                {
                    "sent": "There are many statisticians, including Martin Wainwright, who will be here.",
                    "label": 0
                },
                {
                    "sent": "I think next week or tomorrow.",
                    "label": 0
                },
                {
                    "sent": "I forgot.",
                    "label": 0
                },
                {
                    "sent": "Who has done a lot of important work in this area, right?",
                    "label": 0
                },
                {
                    "sent": "So we can actually recover matrices stably from from now.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The data.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I would like to come back to this matrix problem to motivate the second part of the talk and the reason I started to work on this is because the first time I gave a talk about matrix completion there was a gentleman in the audience who raised his hand and said, you know, imagine there's something you forget and the thing you forget is that there is lots of bogus ratings in this data matrix.",
                    "label": 0
                },
                {
                    "sent": "And he said, I don't know why.",
                    "label": 0
                },
                {
                    "sent": "But there are people who enter ratings that have nothing to do with their own preferences.",
                    "label": 1
                },
                {
                    "sent": "And so, isn't it dangerous that you use these ratings to actually predict all the ratings when these ratings are completely bogus?",
                    "label": 0
                },
                {
                    "sent": "And I said yes it is.",
                    "label": 0
                },
                {
                    "sent": "It is very dangerous.",
                    "label": 0
                },
                {
                    "sent": "I did not know this.",
                    "label": 0
                },
                {
                    "sent": "So what can I do?",
                    "label": 0
                },
                {
                    "sent": "And so now we have a slightly more complicated problem where we have actually partial over observation about the low rank matrix, the Netflix matrix.",
                    "label": 1
                },
                {
                    "sent": "But we also have a sparse error term on top of it which corresponds to entries that are completely bogus.",
                    "label": 0
                },
                {
                    "sent": "OK, and so I need to deal with this.",
                    "label": 0
                },
                {
                    "sent": "And so the problem is now I don't want to make the noise the matrix completion robust with every small additive noise like I want to make it robust with heavy very bad stuff that could happen.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, this motivates us to introduce a problem that I like a lot, which is a blind deconvolution problem which I'm going to state again as simply as I can.",
                    "label": 0
                },
                {
                    "sent": "In this problem, we've got a data matrix which is available to me.",
                    "label": 1
                },
                {
                    "sent": "It's available to the statistician or the machine learner, and it's a sum of a low rank matrix and a sparse matrix, right?",
                    "label": 0
                },
                {
                    "sent": "So what that says?",
                    "label": 0
                },
                {
                    "sent": "Is that what you see is, you see, is a sum of a low rank matrix and the sparse matrix.",
                    "label": 0
                },
                {
                    "sent": "So what do I mean by this?",
                    "label": 0
                },
                {
                    "sent": "It means that there's a low rank matrix that you cannot observe because some entries have been corrupted.",
                    "label": 0
                },
                {
                    "sent": "Again, the corruptions is exactly carried by the Z term.",
                    "label": 0
                },
                {
                    "sent": "The only thing you do is you observe the sum of these two mate.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what I'm claiming it wouldn't be nice to recover Le accurately.",
                    "label": 0
                },
                {
                    "sent": "Again, this seems impossible and it would be great to do it, because if I could.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wait, I could I could detect the bogus rating and apply matrix completion to the good ratings and complete my matrix accurately.",
                    "label": 0
                },
                {
                    "sent": "OK, take.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be great, but why would be great?",
                    "label": 0
                },
                {
                    "sent": "It would be great essentially becausw.",
                    "label": 0
                },
                {
                    "sent": "How do we do dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "So typically what we all learn is principal component analysis and I'm sure everybody knows what it is.",
                    "label": 0
                },
                {
                    "sent": "What you have is you've got a data matrix M which is a low rank matrix plus.",
                    "label": 0
                },
                {
                    "sent": "A perturbation, and So what we think of this is if the points we have a low rank matrix and we look at the columns.",
                    "label": 0
                },
                {
                    "sent": "Of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And what we think about when we do principal component that we look at the column vectors and we say, well, you know in fact this column vectors believe in very high dimension, but they actually clustered along a low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Again, the goal is to recover this low dimensional space and the way you do this is by principal component analysis, as I'm sure you all know.",
                    "label": 0
                },
                {
                    "sent": "So what you do is, you say, well, how am I going to look at the recovers or lower dimensional structure in this high dimensional data set while I'm going to try to find a matrix which has small rank which is as close as possible to the data matrix you gave me and it looks like a horrible communist combinatorial problem because we have a rank constraint, which is absolutely unfriendly.",
                    "label": 0
                },
                {
                    "sent": "But this is the only hard.",
                    "label": 0
                },
                {
                    "sent": "Combinatorially, problem that I know how to solve because I know the solution and the solution is just calculate the SVD, dumps the small singular values and you're done OK.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's what we all learn too.",
                    "label": 0
                },
                {
                    "sent": "Every undergraduate taking a course in statistics at Stanford.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we don't teach so much is that this procedure is extremely sensitive to what we call outliers.",
                    "label": 0
                },
                {
                    "sent": "So because again I can only draw in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "We've got points lying around a nice one dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And so when I fit PCA, it finds his line obviously, and that's great.",
                    "label": 0
                },
                {
                    "sent": "But suppose that this data point was incorrectly recorded.",
                    "label": 0
                },
                {
                    "sent": "Like for example, let's suppose that the Y value of this point was incorrectly labeled or recorded like a sensor failed, or somebody who typed their numbers in the computer made a mistake.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or something like this.",
                    "label": 0
                },
                {
                    "sent": "At this point we happen here for example, so there's a change of scale.",
                    "label": 0
                },
                {
                    "sent": "The other points have not moved.",
                    "label": 0
                },
                {
                    "sent": "This point has moved in and I fit PC again.",
                    "label": 0
                },
                {
                    "sent": "And what is the first principle component this?",
                    "label": 0
                },
                {
                    "sent": "Has nothing to do with the low dimensional structure, So what I'm saying is that I have this huge data matrix.",
                    "label": 0
                },
                {
                    "sent": "There is an entry somewhere that is corrupted and everything breaks down or could potentially breakdown.",
                    "label": 1
                },
                {
                    "sent": "And that's highly.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problematic becausw.",
                    "label": 0
                },
                {
                    "sent": "Because gross errors occur all the time and they occur because in bioinformatics applications you've got sensor failures in image processing, you have occlusion in image data.",
                    "label": 1
                },
                {
                    "sent": "Some pixels are completely occluded, so it's a gross error.",
                    "label": 0
                },
                {
                    "sent": "In web data analysis I've shown you that people tamper with data and enter bogus ratings, and so it's important to make lower dimensionality reduction techniques such as PCA robust to outliers of this kind.",
                    "label": 1
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we talk about face recognition, face recognition, when I look at faces of people, there is some theory that says that if you take a phase under varying illumination, when you look at this face in pixel space, they lie around a 9 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "It's called the harmonic plain, but the problem is that.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In a lot of the data we have to deal with this phase of images are going to be extremely corrupted.",
                    "label": 0
                },
                {
                    "sent": "So for example, one day somebody decided to go out with sunglasses or you might be interested in faces that you retrieve off the Internet.",
                    "label": 0
                },
                {
                    "sent": "And of course, because of characters superimposed on the face, you've got extreme and severe corruptions.",
                    "label": 0
                },
                {
                    "sent": "You could have missing pixels and so on.",
                    "label": 0
                },
                {
                    "sent": "So raw data are very corrupted.",
                    "label": 1
                },
                {
                    "sent": "They have missing blocks and classical PCA technique would breakdown because.",
                    "label": 1
                },
                {
                    "sent": "I want to try to apply PCA with people who wear glasses.",
                    "label": 0
                },
                {
                    "sent": "People with characters on on their foreheads, and so I'm going to get garbage in, garbage out, and so the problem that we pose now is how can you recover the low dimensional structure when you have data of this kind?",
                    "label": 0
                },
                {
                    "sent": "OK, so so again the problem.",
                    "label": 0
                },
                {
                    "sent": "As you can imagine is is very, very simply stated I've.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this problems I've got this low dimensional structure.",
                    "label": 0
                },
                {
                    "sent": "I've got this sparse structure.",
                    "label": 0
                },
                {
                    "sent": "I want to recover or back out the low dimensional structure.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to perform this blind deconvolution right and it's it looks strange.",
                    "label": 0
                },
                {
                    "sent": "But to perform this because you some give me the sum of two numbers, how can I know each of the numbers right?",
                    "label": 0
                },
                {
                    "sent": "I mean just looks impossible.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So can I actually separate the two components?",
                    "label": 0
                },
                {
                    "sent": "So now we've got our matrix sandwiches.",
                    "label": 0
                },
                {
                    "sent": "A low rank matrix plus a sparse term, which I think about as a sparse impulse noise and I'm saying, well, can you actually separate the two components?",
                    "label": 0
                },
                {
                    "sent": "Let's say, well, wait a minute.",
                    "label": 0
                },
                {
                    "sent": "We have the same problems as we had before.",
                    "label": 0
                },
                {
                    "sent": "If you give me a low rank matrix which is both low rank and sparse, how can I know what the lowering component and the sparse components are?",
                    "label": 1
                },
                {
                    "sent": "I just can't right?",
                    "label": 0
                },
                {
                    "sent": "So if you give me this matrix, the lowering component could be this and the sparse component would be 0 or vice versa.",
                    "label": 0
                },
                {
                    "sent": "Lauren component would be 0 and the sparse component would be this or even the lowering component might be half of this and the sparse component might be, and they're all good solutions.",
                    "label": 0
                },
                {
                    "sent": "So we don't want the low rank matrix to be sparse, otherwise the problems makes no sense whatsoever, and so we're going to assume that little run component has the same condition as before.",
                    "label": 0
                },
                {
                    "sent": "This incoherence condition as before, right?",
                    "label": 0
                },
                {
                    "sent": "So roughly, the singular vectors are not sparse.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now I say well, is this enough?",
                    "label": 0
                },
                {
                    "sent": "Now you give me a matrix, which is a nice low rank matrix with singular vectors that are not sparse.",
                    "label": 1
                },
                {
                    "sent": "And I said, well, now you can corrupt it and can you correct it in such a way that I cannot recover their own component and you say yes, and how can I do this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, correct one row.",
                    "label": 0
                },
                {
                    "sent": "Take my ratings and corrupt them all of them.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to be in big trouble.",
                    "label": 0
                },
                {
                    "sent": "So it's a very good answer.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you do is you give me a matrix E I'm you give me a matrix to prepare a nice matrix Li look at the first column of L. Put - I add them up, you see zero.",
                    "label": 1
                },
                {
                    "sent": "In doing this I did not increase the rank.",
                    "label": 0
                },
                {
                    "sent": "If anything I decrease the rank.",
                    "label": 0
                },
                {
                    "sent": "I erased all information there was your bank account information here, it's gone and there's no way you can retrieve it.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so to avoid this kind of things, the model we're going to consider looks at model, in which the entries you can corrupt are actually randomly distributed.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you can do in this game is you can take a subset of entries, corrupt them, and then this subset is selected at random and then you can do anything you want.",
                    "label": 0
                },
                {
                    "sent": "But I will not let you, when I can let you, but with very low probability I can actually let you corrupt an entire column.",
                    "label": 0
                },
                {
                    "sent": "Because you select the entries at random.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we go back to this thing where now we've got this deconvolution problems blind deconvolution problem.",
                    "label": 0
                },
                {
                    "sent": "We know nothing about Li, don't know it's rank.",
                    "label": 0
                },
                {
                    "sent": "I know nothing about EI.",
                    "label": 0
                },
                {
                    "sent": "Don't know which entries have been corrupted that always in magnitude of the corruption is how many they are.",
                    "label": 0
                },
                {
                    "sent": "I know nothing.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, to minimize we're going to use the same approach that we've used all along, which is a user convex optimization approach where it's OK.",
                    "label": 0
                },
                {
                    "sent": "There are many solutions to this problem, but I'm going to pick a special one and one that minimizes this simple objective functional.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to make the lowering component as low rank as possible by using the nuclear norm.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to make this sparse component as sparse as possible.",
                    "label": 0
                },
                {
                    "sent": "I would like to use is your number I can, so I'm going to use your OneNote.",
                    "label": 0
                },
                {
                    "sent": "Alright then, so we gonna use.",
                    "label": 0
                },
                {
                    "sent": "This optimization algorithm, which was also proposed by genres occurrence on Gabby Parrillo wilske abit before us in a very important paper.",
                    "label": 0
                },
                {
                    "sent": "OK, so then the nuclear norm is what we have.",
                    "label": 0
                },
                {
                    "sent": "The L1 norm is.",
                    "label": 0
                },
                {
                    "sent": "I just see the norm as the matrix as a gigantic vector and it's some of the absolute values.",
                    "label": 0
                },
                {
                    "sent": "OK, and the main second main result of this talk to show you a little bit what things are possible?",
                    "label": 0
                },
                {
                    "sent": "Is that if we have a learning component which is as before?",
                    "label": 0
                },
                {
                    "sent": "If I have a set of entries that has been corrupted at random locations of positive fractions of entries that have been corrupted.",
                    "label": 0
                },
                {
                    "sent": "Random locations and I solve this problem with Lambda equals one over square root of the dimension, so there's no cross validation or anything of this nature.",
                    "label": 0
                },
                {
                    "sent": "I just take a Lambda equals one over square root of the dim.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mission.",
                    "label": 0
                },
                {
                    "sent": "Then the theorem says that with very high probability and the probabilities over distribution of the locations of the errors, then this solution is exactly it is I recover the lowering component exactly and then recovers sparse component exactly.",
                    "label": 0
                },
                {
                    "sent": "So if he says something that when I look at it, I've almost find impossible.",
                    "label": 0
                },
                {
                    "sent": "I prepare a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "I give it to you.",
                    "label": 0
                },
                {
                    "sent": "You look at a lot of entries.",
                    "label": 0
                },
                {
                    "sent": "You corrupt them.",
                    "label": 0
                },
                {
                    "sent": "You give it back to me when I'm not to meet with somebody else because me I already know what I prepared.",
                    "label": 0
                },
                {
                    "sent": "You solve this optimization problem that depends on nothing, so use of nuclear norm plus one over square root of dimension times one normal V and you will find exactly what you did.",
                    "label": 0
                },
                {
                    "sent": "All the entries you've touched on, how you touch them, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you recover the low rank structure.",
                    "label": 0
                },
                {
                    "sent": "And it's an exact recovery that has nothing to do.",
                    "label": 0
                },
                {
                    "sent": "Its exact, no matter what the magnitude of the low rank component is.",
                    "label": 0
                },
                {
                    "sent": "And no matter what the magnitude of this parse error, there is.",
                    "label": 0
                },
                {
                    "sent": "No tuning parameter is in this thing, it just.",
                    "label": 0
                },
                {
                    "sent": "Just the way it is.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we've seen 2 examples.",
                    "label": 0
                },
                {
                    "sent": "One where we have missing data, the other one is more complicated because now we have corruption.",
                    "label": 0
                },
                {
                    "sent": "So in the missing data world it's good because what you've got is you've got an entry and then you have to predict missing entries.",
                    "label": 0
                },
                {
                    "sent": "But you know these entries in good in the corrupted world, you never know when you look at the data point, whether it's a good data point or bad data point.",
                    "label": 0
                },
                {
                    "sent": "You have to decide this.",
                    "label": 0
                },
                {
                    "sent": "Yes, yeah here for those of you who know about coding theory is like you have eurasiers here.",
                    "label": 0
                },
                {
                    "sent": "An errors here and it's much harder to correct errors than to deal with the Rangers.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so it's information theoretically harder so if you look at this.",
                    "label": 0
                },
                {
                    "sent": "So what we've done is we've run the algorithms.",
                    "label": 0
                },
                {
                    "sent": "What you see on the right hand side is you see the rank of the low rank component divided by the dimension and on the Y axis Y axis you see the sparsity.",
                    "label": 0
                },
                {
                    "sent": "That is the number of the fraction of entries that have been corrupted.",
                    "label": 0
                },
                {
                    "sent": "And then each pixel on this image is a lot of experiment.",
                    "label": 1
                },
                {
                    "sent": "A great pixel is an experiment for which the method works half the time.",
                    "label": 0
                },
                {
                    "sent": "Awide pick SolidWorks 100% of the time of BLACKPINK SolidWorks 0% of the time.",
                    "label": 0
                },
                {
                    "sent": "What you see here is you seen in a very sharp phase transition between a regime where the algorithm is perfect and they were assuming which it fails miserably.",
                    "label": 0
                },
                {
                    "sent": "OK, but the regime in which in perfect is very interesting.",
                    "label": 0
                },
                {
                    "sent": "So let's be in this white region.",
                    "label": 0
                },
                {
                    "sent": "For example is a rank is a total dimension divided by 10 soarin over 10.",
                    "label": 0
                },
                {
                    "sent": "What this plot indicates is that can corrupt 30% of the entries.",
                    "label": 0
                },
                {
                    "sent": "And it's harmless.",
                    "label": 0
                },
                {
                    "sent": "OK, if the rank decreases to 5% an over 20, I can corrupt almost half of the entries and it still harmless OK?",
                    "label": 1
                },
                {
                    "sent": "In the Matrix completion world, the region where things go better is much larger, because it's here now.",
                    "label": 0
                },
                {
                    "sent": "This is a fraction of missing entries, and so the right region is much larger because it's much easier to deal with missing entries and with corrupted entries.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm running a bit out of time because I want to show you some numerical results.",
                    "label": 0
                },
                {
                    "sent": "I want to mention this very important paper by Alan Wilsky and his colleagues introducing deterministic results.",
                    "label": 1
                },
                {
                    "sent": "The very beautiful piece of work and just like as matrix completion, is this robust PCA generates a lot of interest and then since then alot of people in machine learning and statistics have been able to even improve on these results.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of activity at the moment by several teams around the world getting better and better results.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, in fact, the last result I want to mention, but I don't want to take too much of your time says, well, you could tie them together.",
                    "label": 0
                },
                {
                    "sent": "You could have both missing and corrupted.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trees and you run the optimization algorithm and you back out the the lower end components.",
                    "label": 0
                },
                {
                    "sent": "If you show me a very few entries of a low rank matrix of which you've corrupted positive fraction, it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Will recover the entire matrix with no error.",
                    "label": 0
                },
                {
                    "sent": "OK, in all of this I can do by convex programming by semidefinite programs.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we can deal with small noise, so again you could say well, but you know you're not exactly low rank plus parceiro rank plus plus plus my nose is are things are going to be OK.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A and you say yes, things are OK because the when you run these algorithms with relaxed constraint you're going to get an error as if you did not have the big corruptions that make your life so delicate.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so they are computational issues.",
                    "label": 0
                },
                {
                    "sent": "I see that I've been going on for a bit of time, so these algorithms there are many.",
                    "label": 0
                },
                {
                    "sent": "A lot of work implementing these algorithms.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And so there's a lot of activity.",
                    "label": 0
                },
                {
                    "sent": "On this, for example, if you have a matrix which is 1000 by 1000 of rank 50 of which occur at 100,000 entries, so you have a million unknown variables.",
                    "label": 0
                },
                {
                    "sent": "You know on this laptop it won't run in about 13 seconds.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So my colleague from Microsoft Research because this is joint work with I hope I said it.",
                    "label": 0
                },
                {
                    "sent": "It's John work with Emma John Wright who are wonderful researchers I Microsoft Research working in computer vision have implemented all of these IDs, not only on desktops but also on the Redmond high performance clusters, Microsoft Research and they use all of this to do things that are of interest to Microsoft, such as dealing with high demand.",
                    "label": 0
                },
                {
                    "sent": "Show me images, videos, audio, text, documents and so on, and so that's the kind of applications I'd like.",
                    "label": 1
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To show you now.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's look at some application.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can I take 5 minutes to show applications or I'm completely out of time?",
                    "label": 0
                },
                {
                    "sent": "5 minutes OK. Alright, OK so the first application we looked at is actually video surveillance and here is the problem is you want to detect separate foreground from background.",
                    "label": 0
                },
                {
                    "sent": "And so we think about this as a separation problem as a low rank plus sparse separation problem.",
                    "label": 0
                },
                {
                    "sent": "What you do is just stores the images as columns of your huge data matrix and of course the background is going to be extremely low rank because the background is not changing from frame to frame.",
                    "label": 0
                },
                {
                    "sent": "It's highly correlated, so you're going to identify this with a low rank.",
                    "label": 1
                },
                {
                    "sent": "The foreground, like for example, these people moving about this airport hopefully are going to be picked up by the sparse component.",
                    "label": 1
                },
                {
                    "sent": "And so the idea is, why don't we just apply robust PCA to this thing to do video surveillance?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Grandson when we do this, it works surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "So here you've got 3 frames in the airport.",
                    "label": 0
                },
                {
                    "sent": "And here you see the lowering component which looks like background.",
                    "label": 0
                },
                {
                    "sent": "Then you have this sparse component, which is of course the difference between the frame and the lower end components.",
                    "label": 0
                },
                {
                    "sent": "This looks at people moving about the airport and you see this is applied out of the box with Lambda equals one over Scooter, then no other information and it actually perfectly separates lowering from background.",
                    "label": 0
                },
                {
                    "sent": "You could say well, what is this person in the background while this person is not moving ever so as far as I'm concerned it's a statue.",
                    "label": 0
                },
                {
                    "sent": "And so when you look at the method by competing algorithms, it is getting confused by you.",
                    "label": 0
                },
                {
                    "sent": "See you see mess here and mess up appearing up here.",
                    "label": 0
                },
                {
                    "sent": "And so it's the results are not as clean.",
                    "label": 0
                },
                {
                    "sent": "OK, now there are people who have coded these things in real time and so if you go on YouTube you'll see videos of people moving about a subway and the algorithm actually tracking all the people moving about the subway in real time.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's a more interesting example.",
                    "label": 0
                },
                {
                    "sent": "Here you have a lobby and what's interesting about this example is the illumination is changing overtime.",
                    "label": 0
                },
                {
                    "sent": "And so of course, this is the rank is not one of the background is dark because elimination is changing and so the algorithm does not get confused at all.",
                    "label": 0
                },
                {
                    "sent": "Like here you see the number of lights is changing overtime and so the algorithm is exactly picking this person when he walks in an not picking anything else when elsewhere.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you look at all their algorithms in the field, they seem to be confused by the separation problem.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we've used this to repair movies, so that's a kind of very simple application.",
                    "label": 0
                },
                {
                    "sent": "All movies have all kinds of speckles, and so you're going to apply the low rank plus parts separation.",
                    "label": 0
                },
                {
                    "sent": "The low rank will be the clean movies.",
                    "label": 0
                },
                {
                    "sent": "Ascept sparse part will be the speckles.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when you look at stuff like this, you see speckles that completely disappear here.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No lines that disappear on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "Lines that disappear.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the right hand side, lines go.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Play speckled go.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where lines go away and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, So what you have to be careful is to tune the Lambda parameters right so that actors do not disappear as well.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now my next set of application has to do with face recognition, so I will preface this with this.",
                    "label": 0
                },
                {
                    "sent": "I said from when I talk to E. Here's what he tells me, he says.",
                    "label": 0
                },
                {
                    "sent": "You know a man who will face recognition in a controlled environment is pretty much solved.",
                    "label": 0
                },
                {
                    "sent": "That is if we have pictures of people align and behind, there's a nice white wall and have clean pictures of everybody, and I'll show you a new picture.",
                    "label": 0
                },
                {
                    "sent": "I say who is this you will find with this person is the problem is that now we have to deal with the fact that data are absolutely non ideal.",
                    "label": 0
                },
                {
                    "sent": "So we see people under non ideal conditions and so that's why face recognition is extremely difficult still.",
                    "label": 0
                },
                {
                    "sent": "So the idea of this second part is to use this algorithm as a cleaning tool to kind of do the dimensionality reduction or get back to a set of data sets on which face recognition is essentially a solved problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so how are we going to do this?",
                    "label": 0
                },
                {
                    "sent": "We're going to put faces of people as a columns of a data matrix within applies is the algorithm and the algorithm will remove all the defects we see in faces of images that renders a recognition problematic.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we apply it to three images like this, then what we get is we get, you know, a clean face in the middle and then we get all these non the ideologies like cast shadows, specular points that show up in the sparse component.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In practice, to make it work, though, it is not true that image of faces will be exactly low rank because to be low rank they have to be well aligned.",
                    "label": 0
                },
                {
                    "sent": "And so the next idea is to say which is due to E. My colleague, and now I have not much to do with the slide you're about to see.",
                    "label": 0
                },
                {
                    "sent": "The next idea is to say well.",
                    "label": 0
                },
                {
                    "sent": "It's only after alignment that you have this low rank plus sparse structure, so we have to find the alignment as.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "And so the idea is quite simple.",
                    "label": 0
                },
                {
                    "sent": "He says, well, let's try to find another alignment that is a transformation said after transformation.",
                    "label": 0
                },
                {
                    "sent": "My data set is as lowering plus sparse as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, and we can adjust.",
                    "label": 0
                },
                {
                    "sent": "Now we leave the convex world because the fact that we have to deal find this deformation is not convex, but we're going to try to do things anyway.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so just to show you in principle the interest of these methods, let's suppose that I look at a building from a different angle.",
                    "label": 0
                },
                {
                    "sent": "And then in front of it I haven't resigned some some shots and other shots.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to try to apply this, find the deformation so that I have deformation.",
                    "label": 0
                },
                {
                    "sent": "I'm low rank and and low rank, plus sparse as possible, and I'm going to show you the results here.",
                    "label": 0
                },
                {
                    "sent": "The result is is away.",
                    "label": 0
                },
                {
                    "sent": "You should transform the images.",
                    "label": 0
                },
                {
                    "sent": "This is allowing component which is a building unoccluded and then the sparse component, which is of course this tree or cleaning the building.",
                    "label": 0
                },
                {
                    "sent": "And so it seems to work quite well in practice.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So people have done Ian, his colleagues have done a lot of work, and again it's not mine anymore.",
                    "label": 0
                },
                {
                    "sent": "What you see is you take people's faces of people.",
                    "label": 0
                },
                {
                    "sent": "These faces are major line as you can see and then they are corrupted, so I don't see if you see these patches that are in front of the eyes or nose and so on.",
                    "label": 0
                },
                {
                    "sent": "And so you're going to try to align them, and so you run this algorithm by trying to find, for each image deformation so that as a whole you've got as much lowering plus power structure as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, and so you see the output of the data matrix.",
                    "label": 0
                },
                {
                    "sent": "Of the lowering component and is, as I actually perfectly or almost perfectly aligns with images, and of course removed those occlusion that include eyes, nose, mouth and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here is a thing that I find the most.",
                    "label": 0
                },
                {
                    "sent": "I don't know actually almost check it before I can talk about it, but here we So what they've done is called.",
                    "label": 0
                },
                {
                    "sent": "It took a pictures of a guy on the clu's.",
                    "label": 0
                },
                {
                    "sent": "Conditions, so it's the same person, but here is wearing sunglasses.",
                    "label": 0
                },
                {
                    "sent": "Here is they've got, is has something in front of his mouth.",
                    "label": 0
                },
                {
                    "sent": "He has incentive in front of mouth.",
                    "label": 0
                },
                {
                    "sent": "He's wearing a hat or HIG.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And then you apply his algorithm.",
                    "label": 0
                },
                {
                    "sent": "We say we are going doing face recognition on something like this is going to be very difficult, so let's use this as a preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "So you use this as a preprocessing step.",
                    "label": 0
                },
                {
                    "sent": "You do the alignment, the low rank and sparse component simultaneously, and what you get out of these cells are low rank component and that's a sparse component.",
                    "label": 0
                },
                {
                    "sent": "And so you see all these non idealities sunglasses issue here.",
                    "label": 0
                },
                {
                    "sent": "Then you impute the eyes.",
                    "label": 0
                },
                {
                    "sent": "The hats showing here you know you have removed the hard.",
                    "label": 0
                },
                {
                    "sent": "You remove the wig, you remove the hand in front of your mouse and so now perhaps you can feed this to a face recognition algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you all know, if you do, for example, if you retrieve images off the Internet annual and you do averaging or faces to get the typical average phase before you do alignment an repairing, you're going to get extraordinary blurred pictures.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to recognize people, but if you were to apply these alignments in repairing in the sense that you perform this lowering plus sparse decomposition.",
                    "label": 0
                },
                {
                    "sent": "Beef.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or you take averages.",
                    "label": 0
                },
                {
                    "sent": "Then all of a sudden everything gets much much clearer because you align the right stuff.",
                    "label": 0
                },
                {
                    "sent": "You average the right stuff.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course you can use this for other applications as well.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, you know this is a hand digit recognition problem.",
                    "label": 0
                },
                {
                    "sent": "You know this is a way people could write the number 3.",
                    "label": 0
                },
                {
                    "sent": "You could say, well, what if I were to apply these techniques before actually trying to attempt to understand what this character is?",
                    "label": 0
                },
                {
                    "sent": "And if you do this and you look at the deformation and the lowering component, you would essentially get a clean picture over three that perhaps is easy to recognize.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My last set of applications and then I'll be done is trying to do the same thing, but instead of trying to do it on a bunch of images to do it on a single image.",
                    "label": 0
                },
                {
                    "sent": "So of course, the world that we see is tilted is tilted because we see it often times at an angle, and the question is can we undo this tilt?",
                    "label": 1
                },
                {
                    "sent": "Can we undo this perspective automatically?",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the idea of again of Imaan of his colleagues at Microsoft Research is to say, well, you know you've got a texture like this.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's been corrupted by this pattern that is superimposed on top of the texture.",
                    "label": 0
                },
                {
                    "sent": "But what if I were to try to find a deformation such that after deformation I'm as low rank plus sparse as possible?",
                    "label": 0
                },
                {
                    "sent": "OK, and so they've carried out this research program and I'll show you some of their.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Notes.",
                    "label": 0
                },
                {
                    "sent": "Alright, so you've got a texture that looks like this.",
                    "label": 0
                },
                {
                    "sent": "It has several components.",
                    "label": 0
                },
                {
                    "sent": "It has of course you see that you know it's not straight and then it has a gradient and the idea would be that perhaps by applying these techniques you would actually find the right deformation so that you straighten out the texture you correct for these non idealities and so somehow you can re align objects.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they've done this and um, and with great empirical success, they've achieved good success.",
                    "label": 0
                },
                {
                    "sent": "So here is you.",
                    "label": 0
                },
                {
                    "sent": "See a picture of a checkerboard.",
                    "label": 0
                },
                {
                    "sent": "So now a checkerboard has rank two.",
                    "label": 0
                },
                {
                    "sent": "Well, I think I gave the answer a checkerboard.",
                    "label": 0
                },
                {
                    "sent": "If you look at straight will have rank 2.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not surprising that when I see a checkerboard, added perspective angle and I apply this algorithm, it's training it because it finds a rank two matrix that it likes a lot.",
                    "label": 0
                },
                {
                    "sent": "When you see all the kinds of structures are texture like this, it's trading them as well and so they've looked apply this algorithm to a lot of textures that they see from my perspective angle and each time it seems to kind of correctly identify as a perspective angle.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if you applied this to synthetic stuff which is for example textures of images of this kind, and in this case of course is not difficult to see that you're going to do well, because this is rank one here it's rank two, so you do well.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What's more surprising is that in a lot of applications they seem to do quite well, so so they have taken images of signs in Beijing parks.",
                    "label": 0
                },
                {
                    "sent": "And So what you see is you see this sign with Chinese character at an angle, you apply the algorithm and over certain it's training the sign.",
                    "label": 0
                },
                {
                    "sent": "You can use this with bar code.",
                    "label": 0
                },
                {
                    "sent": "It will straighten it up.",
                    "label": 0
                },
                {
                    "sent": "You see with pages of text it straightens it out.",
                    "label": 0
                },
                {
                    "sent": "Chinese character becomes trade and so now perhaps you can.",
                    "label": 0
                },
                {
                    "sent": "You can run recognition.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "Because here is a very interesting example where you see a Chinese.",
                    "label": 0
                },
                {
                    "sent": "So you see, a Chinese saying that we know the characters are viewed as a perspective.",
                    "label": 0
                },
                {
                    "sent": "You apply the algorithm, it's back up here, you see a.",
                    "label": 0
                },
                {
                    "sent": "And you see the characters.",
                    "label": 0
                },
                {
                    "sent": "And now you can recognize them easily.",
                    "label": 0
                },
                {
                    "sent": "And of course you can straighten other types of structures, which I'm not sure is a good idea.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Because you can find the perspective angle, you can actually find the geometry of these things and so by working out this decomposition, you know when I give you a picture like this, you know how do you find the deformation that makes this thing as lowering plus parses possible.",
                    "label": 0
                },
                {
                    "sent": "It gives you the angle at which you see things.",
                    "label": 0
                },
                {
                    "sent": "So actually now you can recover the orientations of things on a single.",
                    "label": 1
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image.",
                    "label": 0
                },
                {
                    "sent": "And so you can do augmented reality if you want it too.",
                    "label": 0
                },
                {
                    "sent": "So what you do is because you know you're going to you partition your image into sub images and then you're going to apply this algorithm and each time because you can straighten out these textures, you know exactly the angle, the orientation of these buildings, and because you know the orientation of these buildings, it's not going to be hard for you to actually paste an augmented reality in any way you want.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm lost, maybe I'll spend 30 seconds on this and I'll close a lost application that they've worked out, which I find kind of nice is to do web document corpus analysis, which I'm sure you're familiar with is I give you a bunch of documents.",
                    "label": 0
                },
                {
                    "sent": "Can you identify which topic it is?",
                    "label": 0
                },
                {
                    "sent": "Here again, this is the the suggestion is to use these low rank plus sparse model.",
                    "label": 0
                },
                {
                    "sent": "So what you use as a as a data matrix you use a frequency of words occurring in each document.",
                    "label": 0
                },
                {
                    "sent": "And what you could say is, well, documents are written in English, so there is a, you know the English language.",
                    "label": 0
                },
                {
                    "sent": "Same words you know words will occur within the same frequency and so you have the low rank background model which is used in classical solutions to this problem.",
                    "label": 0
                },
                {
                    "sent": "But then suppose that you actually say, well, you know I'm going to have this data matrix of frequency occurrences, but I'm going to try to model this as a lower rank.",
                    "label": 0
                },
                {
                    "sent": "The background of the English language plus a sparse component.",
                    "label": 0
                },
                {
                    "sent": "Now what is our is going to be the sparse component?",
                    "label": 0
                },
                {
                    "sent": "This is going to be a key words.",
                    "label": 0
                },
                {
                    "sent": "These are going to single out those words that occur at a frequency which is very unlike in any other document.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you do this and you apply it to, you know data sets like this.",
                    "label": 0
                },
                {
                    "sent": "What it does, it's it spits out.",
                    "label": 0
                },
                {
                    "sent": "You look at the estimate and see which words.",
                    "label": 0
                },
                {
                    "sent": "Appear in the sparse component.",
                    "label": 0
                },
                {
                    "sent": "You know, and these words tell you much about what this article is about.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's parses a document and find those words that have a frequency of occurrence, which is very unlike other words and so then it's not difficult from this to kind of in fear what the topic might be.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so I spoke for much too long even though the night is still young and so I thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "Give examples of where this stuff can work and give a theoretical limit about where it might not work, but in practical applications, what are the limits of this that we tried?",
                    "label": 0
                },
                {
                    "sent": "Things where it's not a good idea?",
                    "label": 0
                },
                {
                    "sent": "Or can you give us some intuition about?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so well when the lowering structure is not so low rank anymore when the data set is not well approximated by a low rank, then sinks tends to break down quite badly.",
                    "label": 0
                },
                {
                    "sent": "So when you don't have real underlying low rank structure then the sparse components becomes uninformative.",
                    "label": 0
                },
                {
                    "sent": "Solo and component becomes an informative and you don't do well, but we've seen that this method is surprisingly robust because what we're trying to do here is robust statistics.",
                    "label": 0
                },
                {
                    "sent": "We try to fit low rank models in the presence of small noise, Gaussian noise.",
                    "label": 1
                },
                {
                    "sent": "And bad corruptions, and so because we use methods of robust statistics, we tend to have fairly robust results, but if we're not in a regime where the rank is low enough, things are not going to work well.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So all these pictures that you showed of this person but where?",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You'd have a segmentation problem on top of it, but still.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but still you wouldn't go in the database and try to represent it as low rank.",
                    "label": 1
                },
                {
                    "sent": "So what you'd have is you'd have, you know, I think it's a very good point what you do, but still you have this long structure becausw all of a sudden what you'd have is you'd have a lot of these low dimensional planes right, corresponding to different people, and as long as you can correctly identify your low dimensional plane would be fine.",
                    "label": 0
                },
                {
                    "sent": "And I think you should be able to do this, but you're completely right in a way.",
                    "label": 0
                },
                {
                    "sent": "This is supervised learning if you will.",
                    "label": 0
                },
                {
                    "sent": "It's not completely unsupervised yet.",
                    "label": 0
                },
                {
                    "sent": "What we should do next time I come back is we have several people in the database and all of a sudden we want to correct them.",
                    "label": 0
                },
                {
                    "sent": "And not use the wrong people to correct, you know, not use Paul to correct Peter.",
                    "label": 0
                },
                {
                    "sent": "Write an here.",
                    "label": 0
                },
                {
                    "sent": "I have only pictures of Paul and so I completely agree with this.",
                    "label": 0
                },
                {
                    "sent": "But I think they've done it and I think it works OK.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, we don't understand this.",
                    "label": 0
                },
                {
                    "sent": "We John Wright who was a postdoc of E at Microsoft Research, did this simulation a send me the plot?",
                    "label": 0
                },
                {
                    "sent": "I said it's wrong, you know, redo it, he read it.",
                    "label": 0
                },
                {
                    "sent": "We got it the same way we did it.",
                    "label": 0
                },
                {
                    "sent": "We got it the same and we could not understand and we've done it 1000 times and we always get the same.",
                    "label": 0
                },
                {
                    "sent": "Now here is be careful because here it means that the corruption I corrupted this in a very strange way.",
                    "label": 0
                },
                {
                    "sent": "I corrupted with assigned pattern that exactly matches a low rank pattern.",
                    "label": 0
                },
                {
                    "sent": "And what we see is that when the rank decreases, we do worse.",
                    "label": 0
                },
                {
                    "sent": "And I'm not sure I understand it and that we've redone the simulation 1000 times because we could not believe it.",
                    "label": 0
                },
                {
                    "sent": "And always we use the first one who picks on this.",
                    "label": 0
                },
                {
                    "sent": "Which is an that I don't understand why.",
                    "label": 0
                },
                {
                    "sent": "Absolutely not.",
                    "label": 0
                },
                {
                    "sent": "But we've redone it, we've done it many, many times.",
                    "label": 0
                },
                {
                    "sent": "Because I thought it's weird.",
                    "label": 0
                },
                {
                    "sent": "In the random case, in the coherent case see here is the sign of S is the same as a sign of L?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Enjoy.",
                    "label": 0
                },
                {
                    "sent": "Yes, this I don't know how far they've taken it.",
                    "label": 0
                },
                {
                    "sent": "What they've done is that they've.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't know, I cannot answer this question.",
                    "label": 0
                },
                {
                    "sent": "What they've seen is I've seen that suppose I hide the title and I look at this.",
                    "label": 0
                },
                {
                    "sent": "The keywords highlighted by in S or the sparse term.",
                    "label": 0
                },
                {
                    "sent": "Often their words on the title, so they've been able to do some correlation between the frequency of words being picked up blindly with words on the titles that were hidden.",
                    "label": 0
                },
                {
                    "sent": "But I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't think they've been all the way to actually classification.",
                    "label": 0
                },
                {
                    "sent": "These people they work on so many things.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I mean, he has like a team of people they work on so many things that it's hard for me to keep up with what they do.",
                    "label": 0
                },
                {
                    "sent": "Perhaps some of you are Microsoft Research.",
                    "label": 0
                },
                {
                    "sent": "Nobody's from Microsoft Research in Beijing.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Straightening examples.",
                    "label": 0
                },
                {
                    "sent": "We are.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, absolutely.",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "I mean this is done with.",
                    "label": 0
                },
                {
                    "sent": "This is done for like a certain kind of structures and I don't think it would work for other kind of structured structures that do not have a bit of geometry built in of the kind you discuss that would not work well.",
                    "label": 0
                },
                {
                    "sent": "There is some structure there.",
                    "label": 0
                },
                {
                    "sent": "I don't think I think the thing that you have to be worried about when you do think you said you don't want overparameterized stuff because we already kind of as you know, sometimes things break down so you don't want to put too many unknowns, and like which deformation would make the structure you know 'cause you can try to find for more complicated.",
                    "label": 0
                },
                {
                    "sent": "If your morphism like you know.",
                    "label": 0
                },
                {
                    "sent": "But why is it that my views are projective and not warps more complicated warps when you do this you start to have too many parameters so you have to be careful about a few things.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}