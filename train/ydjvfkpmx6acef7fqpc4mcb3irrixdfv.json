{
    "id": "ydjvfkpmx6acef7fqpc4mcb3irrixdfv",
    "title": "Association Mapping of Traits over Time Using Gaussian Processes",
    "info": {
        "author": [
            "Oliver Stegle, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Feb. 15, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Computational Biology",
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_stegle_amto/",
    "segmentation": [
        [
            "OK, so finally set up.",
            "So I'm going to talk about.",
            "Actually, it's quite a line of the talk from previously heard by the previous speaker, which is Association mapping and the definite tools that we can look at look in this application at and here we may be looking at one of these modules that we actually heard about, which is for special case that we found interesting, namely that we have phenotypes observed overtime and what we can do to specifically address the properties of such data."
        ],
        [
            "So let me briefly review those things.",
            "The basic idea physician studies.",
            "This can be very brief because we heard already about those things.",
            "So here at the top we basically have genetic information for number of individuals and snips specifically.",
            "We have variable positional loci.",
            "There's these guys in Gray, which would be snips and we're interested in which one of those snips have an effect on a phenotype.",
            "And the problem is quite well established for."
        ],
        [
            "Univariate phenotypes is?",
            "I would call them.",
            "Let's say if there's a single phenotypic variable, just one of them.",
            "There are many methods for identifying OK.",
            "Yes, this snip, which is at least correlated with phenotype.",
            "And then we may be hypothesized causal relationship and their methods like linear or logistic regression or statistical dependencies H six more kernel methods that can be used for this specific task."
        ],
        [
            "The problem gets a little bit more interesting.",
            "If multivariate phenotypes we don't have a single phenotype, but we have a whole chain of EM, let's say 1, two or or other sort of structure that we could have for instance in equal studies.",
            "The previous also mentioned.",
            "We have many thousands 10s of thousands of variables, namely for every one of the genes that is observed.",
            "And this case is it's a little bit more tricky.",
            "What to do?"
        ],
        [
            "I claim that as of two interesting things that we could add or learn from certain multivariate phenotypes.",
            "One thing that's previous worker to become 2008 we're interested in.",
            "We just say, Oh yes, we can maybe identify confounding factors simply because of the idea that one confounding factor is similar effect of any one of those phenotypes that we observe, and hence we might be able to identify that factor in their prep, improve statistical power."
        ],
        [
            "Another approach is to say, well, let's just look really at the dependent structure among these phenotypic variables, right?",
            "So say how they are independent, whether their correlations."
        ],
        [
            "They have structure within the phenotype and there are two approaches I want to mention.",
            "Any one of them that is nice and be by Kim.",
            "Quite very nice work which looks at a correlation graph between phenotypes so we can form a network between phenotypes that we can pre estimate and they study how this correlation can be useful in a multivariate regression setting to you.",
            "Better predictions in this specific context."
        ],
        [
            "That's exactly where this talks fit in name.",
            "We were looking at time series.",
            "We're assuming that we have one phenotype that observed overtime.",
            "So here we have time series for different individuals, and we're looking at how these time series evolve and trying to exploit this specific structure.",
            "Because time series are obviously sort of dependent one time still depends on the last one.",
            "We have smoothness and we want to study how this can be used."
        ],
        [
            "That leads into the outline after the motivation that I've been giving you."
        ],
        [
            "King, about a Gaussian process model for this task, where we first look, how can we deal with a single regulator if you just say, Oh yeah, let's assume we just had one snip, and that's what we want to solve.",
            "Then how we extend it to multiple regulators?",
            "And then I've experiment simulated data in real data, followed by some conclusions."
        ],
        [
            "OK, so very briefly to where we were before, so our setting is we have snips at the top genetic information for a number of individuals and a time series phenotype at the bottom.",
            "And we can think about these phenotypes.",
            "For example just to motivate the little bit we could have gene expression level time series.",
            "That would be one observation, or what will be later interesting.",
            "Sort of clinical measurements, right?",
            "So that will be actually studying in the application."
        ],
        [
            "And.",
            "The traditional traditional approach would be to say, well, we consider the variables that every one time point time, .1 time .2 every one of those slides is completely independent.",
            "Neglecting that there's a time correlation between the."
        ],
        [
            "And the question here is, can we count for the time structure?",
            "Can we improve the model by doing something more smart?"
        ],
        [
            "So what I'm now doing in the first step is I'm simplifying the problem a little bit and saying, well, let's just assume they were just one step, right?",
            "That's very many traditional approaches to that.",
            "Anyway.",
            "They do independent tests person, so let's start out with that."
        ],
        [
            "And the nice story is that for a single sniper regulator, we actually we're back to a well known problem I2 sample problem.",
            "So here this is another context we could say, OK, we could.",
            "We could label all the individuals which have the minor or major allele frequency in green and the other ones in red or in other contexts we could say we have two conditions or treatment control condition and we label the ones in green and the other ones in red.",
            "And now we're looking at is there sort of is 2 distributions that we're observing?",
            "Or is it 1 distribution here?",
            "In this case it would clearly say it's maybe one dynamics that gives rise to both of these time series.",
            "Well, this case here we have a 2 sample case where they sort of sparking off behavior that we can identify."
        ],
        [
            "OK, so that's a little bit of previous work that we've been involved with, namely, how can we resolve or how can we resolve?"
        ],
        [
            "This problem and the basic idea is we can treat this problem as a as a model comparison.",
            "We can compare these two hypothesis.",
            "Is there a single underlying dynamics or two underlying dynamics by saying, well, let's just compare two models.",
            "One both phenotype."
        ],
        [
            "Time series groups are explained by a single process that describes both dynamics and the alternative is that we have independent dynamics for both these processes that describe that."
        ],
        [
            "Potential behavior.",
            "And then.",
            "Just to recap, so here we have the red guys.",
            "These are our condition one.",
            "The individuals we have a number of observations or he assumed that we are replicates for every one of those conditions.",
            "So these are the treatment guys.",
            "These are the control groups and now the shared."
        ],
        [
            "Model which is say well we just fit a single dynamic on top of it.",
            "We just try to explain both datasets by a single dynamics.",
            "Here we're using a Gaussian process model and that includes nicely the belief that this particular function value time T1 depends on time 22 etc etc.",
            "By some smoothness assumption that's built into the model.",
            "And."
        ],
        [
            "Some more details.",
            "For instance, we could assume noise levels that depend on the condition, but that's really just a detail of the."
        ],
        [
            "Point and the alternative model.",
            "The independent model just follows complete discrete right here.",
            "The only difference is that we have two process, one for the dependent independent model.",
            "Sorry, one for the treatment and one for the control case of the model.",
            "So these are the two models that we would like to compare and then everything else is a matter of."
        ],
        [
            "Doing Bayesian inference, for example, we could say we compare the probability of the data under each of those model, namely the dependent on the independent model, and this log score.",
            "Ratio then tells us how likely it is that they were famous.",
            "Actually, the independent model over the shared model."
        ],
        [
            "And this specific case just to motivate that a little further, we actually see that the independent model is really just describing the data.",
            "The observation that we have here in the condition a separately from the data in the condition B.",
            "So these distributions completely factorized well in the shared model.",
            "They all fall in one bucket, and we describe both datasets with one single model, and hence with one underlying dynamics.",
            "In this in this context."
        ],
        [
            "And now."
        ],
        [
            "I want to go.",
            "Very briefly into how this sort of technically could work, so this is just.",
            "Standard for those who know about Gaussian processes, but I want to briefly to recap on these.",
            "So let's assume we have one of those datasets, so here both conditions A&B and we're looking at the shared model.",
            "So remember, the shared model explains both data with one single underlying dynamics and then the posterior over the latent function values.",
            "This is the process dynamics that we want to cover is just proportional to the prior times likelihood.",
            "And here, important for us is."
        ],
        [
            "That the likelihood is the prior is really driven by a kernel function that sits in here actually covariance matrix that is built from a kernel function that determines how do observations at time input T&T prime covary.",
            "In other words, how smoothly are they related and the assumption that we are going to make here for this purpose is to say, well, they just Taylor off exponentially.",
            "So we just have an exponential decay, meaning that the further time points are apart from each other, the less are they cover.",
            "Do they covary and hence the more they sort of closer to independence?",
            "It's the one critical bit and we need to keep that in mind for later."
        ],
        [
            "And the second bit is just the likelihood term here, and the likelihood is we have both conditions in the game.",
            "So we have a product of both conditions A&B, with all the replicates and then with all the time points.",
            "And here we really just relate the observations to the process model.",
            "That is sort of the latent thing that underlies all this.",
            "And here parameters could be thought of something like a noise model that relates those two data set."
        ],
        [
            "It's already set that certain number of parameters, so one parameter again in the prior distribution is.",
            "In particular this link scale parameter, which is very important, and this one here is an amplitude.",
            "And obviously the noise parameters also improve."
        ],
        [
            "That is in the game.",
            "And the important bit is if you know it will choose Gosha noise, then everything is closed form.",
            "But there are other choices within Gaussian process.",
            "I don't have time to jump into this, but using approximate inference we can also consider robust noise MoD."
        ],
        [
            "And now actually we can just play the game.",
            "So here we have one set of time series.",
            "Again treatment control and we can first fit the shared model.",
            "So here the black line is the mean prediction and here we have error bars to this one day."
        ],
        [
            "Set and now we could fit the independent model, which obviously is a much better."
        ],
        [
            "In this case, and yes, we get it.",
            "You get a very positive log likelihood ratio score, which favors the independent model over the shared model by quite a significant mark."
        ],
        [
            "And this sort of idea we applied to differential gene expression.",
            "So here we can really just test.",
            "You know how likely is it that any gene is differentially expressed and which is actually exactly the same question asking whether a single snip has a causal influence on a set of time series.",
            "So that's a very similar link, and that work very well in this case."
        ],
        [
            "So after the sort of recap, the question is, how can we generalize that with the interesting case?",
            "In the interesting cases that we don't have a single marker, but we have a very large number of markers and putative regulators that could have an influence on the phenome on those time series that we're trying to model.",
            "And here if you just would group up everything into sort of different data groups, it would just be not tractable at all, right?",
            "We would have two to the end different groupings of all these different markers and it would just completely breakdown."
        ],
        [
            "And the solution that we propose is to say, well, we could also softly group those.",
            "In a way, softly by defining a suitable covariance function.",
            "And that's sort of the next step that I would like to."
        ],
        [
            "Through So what we're now doing is to say instead of just saying the similarity between any two observations is purely dependent on the time scale on how far they are apart in time.",
            "Space for all the data in one big bucket, and say the covariation or the smoothness in the space of observations depends not only on how far they are important, I'm space, but also how far are the corresponding individuals genetically apart.",
            "But any observation is determined by a time point within the time series for specific individual.",
            "And that individual has a genetic background that we can all have also access to.",
            "So we propose to say, well, let's say the covariation between an individual with genotype S at anytime point T and another individual as parameter.",
            "Monte Prime is a product of two covariates and functions or two kernels, one defined on the genetics and one on the time on the time scale.",
            "And for the time scale, we can assume exactly the same kernel is before the squared exponential kernel that says close things in time are closer together in sort of covariant space, and for the genetics we would like to sort of define a kernel that groups closely related individuals in this genetic sense together, and those which are not closely related, not together.",
            "In other words, the covariation should be close to 0.",
            "And now."
        ],
        [
            "And now we can see how we can actually make make this work and do feature selection.",
            "So here we were using a very basic aid covariance function were just saying the covariance function in this snip space is just a sum overall snips.",
            "So here's some overall snips, and for every sniper, look at the squared distance.",
            "Let's assume those guys could be just 01, so this term is 01, and then we divided by length scale parameters.",
            "So those steps which are irrelevant.",
            "We have a very very large links get parameter.",
            "In other words, these elements don't attribute or don't account to that some and those things which are relevant to very short link scale parameter and hence will define whether two individuals are genetically related in the sense of the relevant snips and relevant for predicting the phenotype that we're interested in.",
            "Right?",
            "Because there's no don't need to be all snips, this is typically a subset."
        ],
        [
            "OK, and here is some intuition how that would work.",
            "I mean here just plotted a sample covariance function.",
            "No discovery function is specially quite a large structure, it's individuals times time point.",
            "We have one entry per individual end time point and it actually has a sort of block like structure.",
            "So here any one of those little squares is 1 individual, so within one individual genetic covariances one.",
            "So we just boiled down to the time covariance and we get this sort of tailoring of exponential on the diagonal.",
            "And then also Taylors off left and right.",
            "That's just the normal covariance function and then here we assume that the genetic covariance function would assign that.",
            "All these guys have a cross correlation of 1, so they are all genetically completely identical, and those are identical and they don't.",
            "Interfere at all.",
            "So that's the special case of this covariance function corresponding to the previous two sample case that I introduced in the first part of my talk.",
            "But really, the point is, this reference function could really softly intervene between any of the things that could be part of the individual, correlating also in the genetic space.",
            "Or we could have more than two groups, etc etc, etc.",
            "So some sort of clustering that we're doing here of individual."
        ],
        [
            "In this genetic sense.",
            "So how do we select features?",
            "Well, the main point is appointed is that these length scale parameters determine which regulators player, alright, short length scales means a relevant parameter largely."
        ],
        [
            "Scale means there irrelevant, and we could just follow the basin rule and say we want to maximize the posterior probability of these parameters given the data and the model.",
            "And that's just the probability of data under the parameters in the model times in prior.",
            "The problem is that's quite an expensive procedure, and it's expensive because we have to invert for every one of those evaluations.",
            "If you want to evaluate that function for some gradient calculation etc, we need to invert and J * T James T size matrix, and that doesn't scale all that well.",
            "So the solution that we found quite practice."
        ],
        [
            "So for this application is to revert to some greedy approximation and the greedy approximation."
        ],
        [
            "That you reason is a very classical active set selection, so we just say we have a set of snips that do play a role at all, and every iteration we first select a snip to include in the active set by looking at the gradient.",
            "So we're looking at the gradient of the log likelihood with respect to all of these length scale parameters, and the one that might have the biggest influences included in the active set."
        ],
        [
            "And then the second step is to just use the current active set and optimize those parameters of the current active set.",
            "That is sort of feasible because we assume that in any one big datasets, only very few snips play a role, let's say 5, let's say 10, but not.",
            "That many that this sort of could breakdown.",
            "And then we iteratively reduces repeat this procedure until we don't make a strong improvement anymore."
        ],
        [
            "OK, and that already leads me to experiment so."
        ],
        [
            "We applied that too because first of all we wanted to see how well can we recover the causal snips, and for that we can only Roberta simulations because we need to have some some ground truth data.",
            "And here we look really at this active sets of which snips landed in the active set, where they causal, whether not causal.",
            "And we compared to statistically pens testing.",
            "Here we use H sick with forward feature selection, and we also considered L1 regularised regression.",
            "So simply lawsuit and the simulations that we at least got very competitive performance.",
            "OK, we seem to be able to recover.",
            "Sort of.",
            "What is the underlying structure?"
        ],
        [
            "And then we went on to some real data, and that's maybe that's interesting.",
            "Here we got interesting data from colleagues in Munich and.",
            "They studied quite a significant number of depressed patients, so these are 360 depressed patients.",
            "They're all depressed when they come into treatment.",
            "Then they are treated.",
            "For a number of eight weeks and every week during the treatment, they basically.",
            "Produce a score that summarizes their depressive Ness right.",
            "A high Hamilton score as they called here means high depression level Low Hamilton score low Depression is a very complicated score, but basically there's some quantitative expression of how depressed patients are overtime and for anyone individuals.",
            "We had 30,000 steps and hear.",
            "The question was to understand.",
            "What genetic determination within the individuals causes that some individuals respond to the treatment and others don't?",
            "Right because you want to know whether this treatment will work for these individuals or not, because otherwise you could help them.",
            "Maybe otherwise or try different medicine etc etc."
        ],
        [
            "So yeah, really, the goals were identified which snips are relevant for this point, and also, and that's maybe a different setting to the normalization mapping to make predictions early on you want at day one predict whether you may be able to help the patient or not, and if you maybe try for two weeks of treatment and then improve your prediction, that still helpful because you still save six weeks or 10 weeks, etc etc.",
            "So these are these two goals and we try to sort of answer both of them."
        ],
        [
            "And here's some 1st results.",
            "What we're seeing here on the Y axis is some root mean squared error on a test set of the individual.",
            "So we leave out a few individuals for testing an as a function of the week.",
            "So these are eight weeks and the black line here is just a simple benchmark where we say, well, let's just in the training set, calculate the mean expression depression level if all the individuals and use that as a predictor.",
            "Very naive predictor, and that need predictor variable at the beginning because the patients are all depressed when they come in, otherwise there wouldn't be.",
            "Put into hospital and then later on it's getting a very, very bad predictor because half of the individuals gets better and the other half basically stays depressed or even gets worse.",
            "And on this sort of scale, our model, which just uses a single snip.",
            "This ginowan that is making just predictions from the from the Geno type of the single snip.",
            "The best snip already significantly decreases this predictive error here, and if we include more snip, it gets better, but not all that much.",
            "So basically the first Nip already tells half the story, and that's also consistent to other studies on this data set, so that she just seems to be the case for this response.",
            "And the second."
        ],
        [
            "We were interested in can we make use of early responses in the data.",
            "So here we say, well, let's just leave the patients the test patients for two weeks in treatment and use there.",
            "Their first two data points in this test set to predict their future, so this is this green line here, so this doesn't use any genetics, it just uses the first 2 weeks of treatment to predict the remainder of the time series and that is already still better than the mean model as expected.",
            "But the genetic still adds a lot.",
            "And then we can also combine.",
            "We can say let's use the genotype and the first 2 weeks of treatment response and here we can see that in the beginning we gained significantly over the models and then after a large number of weeks, in this case, six weeks.",
            "It seems like the first 2 weeks don't tell anything anymore about the outcome and it reverts back to the genetic model."
        ],
        [
            "OK, so here maybe to look at the individual scale.",
            "So this was all average across all tests and videos.",
            "That's just one individuals.",
            "How this could look.",
            "So here we have these two first data points that are also included in the training and these are predicted means and standard error bars of the Gaussian process.",
            "These are complete total test points here and we see we have actually getting quite nice fit for quite a few individuals.",
            "So this looks very very promising in.",
            "This works for about half of the individuals and four."
        ],
        [
            "Only for the other ones it doesn't quite work, so if we if we look at the last time point, this is the total prediction after 8 weeks and we just make a histogram of individuals to see quite a few of them are predicted extremely well and for others it doesn't work and it's ongoing work to investigate what happens here.",
            "It looks like they got different medicines and there are all sorts of other variables that have not been taken into account yet, so there's a large scattered between India."
        ],
        [
            "OK, and that is basically it."
        ],
        [
            "Let me just conclude.",
            "We propose discussion process model, which is quite efficient.",
            "At least it's a pliable, applicable to like large scale genetic datasets.",
            "It can detect associations and also is very good for prediction.",
            "Interestingly, on this combined setting where we combine some phenotypes to predict others within the time series, which is maybe a little bit of a normal normal setting.",
            "And the."
        ],
        [
            "The point here was also to make greedy selection tricks work on the setting that have been well established.",
            "Also in L1 type models and.",
            "Could be applied to Gaussian processes."
        ],
        [
            "And there are lots of future directions.",
            "For example, we interesting alternative covariance functions.",
            "This is just one choice.",
            "There might be others that work even better, and we also interested in sparse approximations to discussion processes, because the limiting factor is this big matrix which is J * 2 T individuals times time steps.",
            "And for upcoming datasets.",
            "We still need to do some work to make this.",
            "Applicable to those data as well.",
            "Thank you.",
            "So it is not very good for basic computer.",
            "It works very well with innovative with outside of the data properly I was just heading out for other color function that would not be local differences somehow or you go along with his parents when I show everywhere.",
            "I mean here we use just the square exponential everywhere which which seems to be a quite reasonable first time for the time.",
            "It definitely works very well for the genetics.",
            "I agree that this is not the ideal setting yet.",
            "We need probably something that is much more discreet in saying this snip has an effect, has no effect, and then sort of when it has an effect modulate, maybe with the strength of the effect, or maybe even leave it with this group type of setting.",
            "Where I agree.",
            "And that is the first point that we note here.",
            "There's more work to be done.",
            "That's a good point.",
            "In your greedy selection you re learning the language scales of the whole set of steps at every iteration.",
            "Or you just learn like tail for the new one.",
            "We re learn the length scales of all snips in the active set, because that small.",
            "I mean here we just went up to 25 or something, so that's more.",
            "And then once that's settled we look at the gradient of all snips.",
            "So that's the expense you have to do that for all snips.",
            "But that's a parallel operation, so that runs well on the cluster on many cores, and that's why this is applicable at all.",
            "Questions.",
            "I have one thing 'cause I tend to be skeptical about.",
            "Feature selection based on my own experience.",
            "Have you compared a model with you identified small instead of snips to a folder model using all its members?",
            "So I mean.",
            "For the for this specific task, right?",
            "If you if you don't select any features and you just make them sort of equally equally relevant, then basically the genetic differences average completely out and you don't, and this specific model it wouldn't work at all for this specific.",
            "This specific case, generally speaking.",
            "I'm less convinced that this model isn't very excellent.",
            "Feature selector.",
            "I mean we we had this comparison experiment simulation that I showed you where we compared to H seek and linear regression and those answers were sort of in a comparable regime.",
            "But the interesting aspect of this is a very good sort of prediction method that also can tweak half of the data being in the prediction set, others not in the predictions.",
            "That is very flexible in that in that sort of setting.",
            "So that's maybe my answer.",
            "To that question.",
            "But yes, you need to select features, otherwise you wouldn't get any answer here.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so finally set up.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's quite a line of the talk from previously heard by the previous speaker, which is Association mapping and the definite tools that we can look at look in this application at and here we may be looking at one of these modules that we actually heard about, which is for special case that we found interesting, namely that we have phenotypes observed overtime and what we can do to specifically address the properties of such data.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me briefly review those things.",
                    "label": 0
                },
                {
                    "sent": "The basic idea physician studies.",
                    "label": 0
                },
                {
                    "sent": "This can be very brief because we heard already about those things.",
                    "label": 0
                },
                {
                    "sent": "So here at the top we basically have genetic information for number of individuals and snips specifically.",
                    "label": 0
                },
                {
                    "sent": "We have variable positional loci.",
                    "label": 0
                },
                {
                    "sent": "There's these guys in Gray, which would be snips and we're interested in which one of those snips have an effect on a phenotype.",
                    "label": 0
                },
                {
                    "sent": "And the problem is quite well established for.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Univariate phenotypes is?",
                    "label": 0
                },
                {
                    "sent": "I would call them.",
                    "label": 0
                },
                {
                    "sent": "Let's say if there's a single phenotypic variable, just one of them.",
                    "label": 0
                },
                {
                    "sent": "There are many methods for identifying OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, this snip, which is at least correlated with phenotype.",
                    "label": 0
                },
                {
                    "sent": "And then we may be hypothesized causal relationship and their methods like linear or logistic regression or statistical dependencies H six more kernel methods that can be used for this specific task.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem gets a little bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "If multivariate phenotypes we don't have a single phenotype, but we have a whole chain of EM, let's say 1, two or or other sort of structure that we could have for instance in equal studies.",
                    "label": 0
                },
                {
                    "sent": "The previous also mentioned.",
                    "label": 0
                },
                {
                    "sent": "We have many thousands 10s of thousands of variables, namely for every one of the genes that is observed.",
                    "label": 0
                },
                {
                    "sent": "And this case is it's a little bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "What to do?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I claim that as of two interesting things that we could add or learn from certain multivariate phenotypes.",
                    "label": 0
                },
                {
                    "sent": "One thing that's previous worker to become 2008 we're interested in.",
                    "label": 0
                },
                {
                    "sent": "We just say, Oh yes, we can maybe identify confounding factors simply because of the idea that one confounding factor is similar effect of any one of those phenotypes that we observe, and hence we might be able to identify that factor in their prep, improve statistical power.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another approach is to say, well, let's just look really at the dependent structure among these phenotypic variables, right?",
                    "label": 0
                },
                {
                    "sent": "So say how they are independent, whether their correlations.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They have structure within the phenotype and there are two approaches I want to mention.",
                    "label": 0
                },
                {
                    "sent": "Any one of them that is nice and be by Kim.",
                    "label": 0
                },
                {
                    "sent": "Quite very nice work which looks at a correlation graph between phenotypes so we can form a network between phenotypes that we can pre estimate and they study how this correlation can be useful in a multivariate regression setting to you.",
                    "label": 0
                },
                {
                    "sent": "Better predictions in this specific context.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's exactly where this talks fit in name.",
                    "label": 0
                },
                {
                    "sent": "We were looking at time series.",
                    "label": 0
                },
                {
                    "sent": "We're assuming that we have one phenotype that observed overtime.",
                    "label": 0
                },
                {
                    "sent": "So here we have time series for different individuals, and we're looking at how these time series evolve and trying to exploit this specific structure.",
                    "label": 0
                },
                {
                    "sent": "Because time series are obviously sort of dependent one time still depends on the last one.",
                    "label": 0
                },
                {
                    "sent": "We have smoothness and we want to study how this can be used.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That leads into the outline after the motivation that I've been giving you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "King, about a Gaussian process model for this task, where we first look, how can we deal with a single regulator if you just say, Oh yeah, let's assume we just had one snip, and that's what we want to solve.",
                    "label": 1
                },
                {
                    "sent": "Then how we extend it to multiple regulators?",
                    "label": 0
                },
                {
                    "sent": "And then I've experiment simulated data in real data, followed by some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so very briefly to where we were before, so our setting is we have snips at the top genetic information for a number of individuals and a time series phenotype at the bottom.",
                    "label": 0
                },
                {
                    "sent": "And we can think about these phenotypes.",
                    "label": 0
                },
                {
                    "sent": "For example just to motivate the little bit we could have gene expression level time series.",
                    "label": 0
                },
                {
                    "sent": "That would be one observation, or what will be later interesting.",
                    "label": 0
                },
                {
                    "sent": "Sort of clinical measurements, right?",
                    "label": 0
                },
                {
                    "sent": "So that will be actually studying in the application.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The traditional traditional approach would be to say, well, we consider the variables that every one time point time, .1 time .2 every one of those slides is completely independent.",
                    "label": 0
                },
                {
                    "sent": "Neglecting that there's a time correlation between the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the question here is, can we count for the time structure?",
                    "label": 0
                },
                {
                    "sent": "Can we improve the model by doing something more smart?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm now doing in the first step is I'm simplifying the problem a little bit and saying, well, let's just assume they were just one step, right?",
                    "label": 0
                },
                {
                    "sent": "That's very many traditional approaches to that.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "They do independent tests person, so let's start out with that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the nice story is that for a single sniper regulator, we actually we're back to a well known problem I2 sample problem.",
                    "label": 1
                },
                {
                    "sent": "So here this is another context we could say, OK, we could.",
                    "label": 0
                },
                {
                    "sent": "We could label all the individuals which have the minor or major allele frequency in green and the other ones in red or in other contexts we could say we have two conditions or treatment control condition and we label the ones in green and the other ones in red.",
                    "label": 0
                },
                {
                    "sent": "And now we're looking at is there sort of is 2 distributions that we're observing?",
                    "label": 0
                },
                {
                    "sent": "Or is it 1 distribution here?",
                    "label": 0
                },
                {
                    "sent": "In this case it would clearly say it's maybe one dynamics that gives rise to both of these time series.",
                    "label": 0
                },
                {
                    "sent": "Well, this case here we have a 2 sample case where they sort of sparking off behavior that we can identify.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's a little bit of previous work that we've been involved with, namely, how can we resolve or how can we resolve?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem and the basic idea is we can treat this problem as a as a model comparison.",
                    "label": 1
                },
                {
                    "sent": "We can compare these two hypothesis.",
                    "label": 1
                },
                {
                    "sent": "Is there a single underlying dynamics or two underlying dynamics by saying, well, let's just compare two models.",
                    "label": 0
                },
                {
                    "sent": "One both phenotype.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time series groups are explained by a single process that describes both dynamics and the alternative is that we have independent dynamics for both these processes that describe that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Potential behavior.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Just to recap, so here we have the red guys.",
                    "label": 0
                },
                {
                    "sent": "These are our condition one.",
                    "label": 0
                },
                {
                    "sent": "The individuals we have a number of observations or he assumed that we are replicates for every one of those conditions.",
                    "label": 0
                },
                {
                    "sent": "So these are the treatment guys.",
                    "label": 0
                },
                {
                    "sent": "These are the control groups and now the shared.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Model which is say well we just fit a single dynamic on top of it.",
                    "label": 0
                },
                {
                    "sent": "We just try to explain both datasets by a single dynamics.",
                    "label": 0
                },
                {
                    "sent": "Here we're using a Gaussian process model and that includes nicely the belief that this particular function value time T1 depends on time 22 etc etc.",
                    "label": 1
                },
                {
                    "sent": "By some smoothness assumption that's built into the model.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some more details.",
                    "label": 0
                },
                {
                    "sent": "For instance, we could assume noise levels that depend on the condition, but that's really just a detail of the.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point and the alternative model.",
                    "label": 0
                },
                {
                    "sent": "The independent model just follows complete discrete right here.",
                    "label": 1
                },
                {
                    "sent": "The only difference is that we have two process, one for the dependent independent model.",
                    "label": 0
                },
                {
                    "sent": "Sorry, one for the treatment and one for the control case of the model.",
                    "label": 0
                },
                {
                    "sent": "So these are the two models that we would like to compare and then everything else is a matter of.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing Bayesian inference, for example, we could say we compare the probability of the data under each of those model, namely the dependent on the independent model, and this log score.",
                    "label": 0
                },
                {
                    "sent": "Ratio then tells us how likely it is that they were famous.",
                    "label": 0
                },
                {
                    "sent": "Actually, the independent model over the shared model.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this specific case just to motivate that a little further, we actually see that the independent model is really just describing the data.",
                    "label": 0
                },
                {
                    "sent": "The observation that we have here in the condition a separately from the data in the condition B.",
                    "label": 0
                },
                {
                    "sent": "So these distributions completely factorized well in the shared model.",
                    "label": 0
                },
                {
                    "sent": "They all fall in one bucket, and we describe both datasets with one single model, and hence with one underlying dynamics.",
                    "label": 0
                },
                {
                    "sent": "In this in this context.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to go.",
                    "label": 0
                },
                {
                    "sent": "Very briefly into how this sort of technically could work, so this is just.",
                    "label": 0
                },
                {
                    "sent": "Standard for those who know about Gaussian processes, but I want to briefly to recap on these.",
                    "label": 0
                },
                {
                    "sent": "So let's assume we have one of those datasets, so here both conditions A&B and we're looking at the shared model.",
                    "label": 1
                },
                {
                    "sent": "So remember, the shared model explains both data with one single underlying dynamics and then the posterior over the latent function values.",
                    "label": 1
                },
                {
                    "sent": "This is the process dynamics that we want to cover is just proportional to the prior times likelihood.",
                    "label": 0
                },
                {
                    "sent": "And here, important for us is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That the likelihood is the prior is really driven by a kernel function that sits in here actually covariance matrix that is built from a kernel function that determines how do observations at time input T&T prime covary.",
                    "label": 0
                },
                {
                    "sent": "In other words, how smoothly are they related and the assumption that we are going to make here for this purpose is to say, well, they just Taylor off exponentially.",
                    "label": 0
                },
                {
                    "sent": "So we just have an exponential decay, meaning that the further time points are apart from each other, the less are they cover.",
                    "label": 0
                },
                {
                    "sent": "Do they covary and hence the more they sort of closer to independence?",
                    "label": 0
                },
                {
                    "sent": "It's the one critical bit and we need to keep that in mind for later.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second bit is just the likelihood term here, and the likelihood is we have both conditions in the game.",
                    "label": 0
                },
                {
                    "sent": "So we have a product of both conditions A&B, with all the replicates and then with all the time points.",
                    "label": 0
                },
                {
                    "sent": "And here we really just relate the observations to the process model.",
                    "label": 0
                },
                {
                    "sent": "That is sort of the latent thing that underlies all this.",
                    "label": 0
                },
                {
                    "sent": "And here parameters could be thought of something like a noise model that relates those two data set.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's already set that certain number of parameters, so one parameter again in the prior distribution is.",
                    "label": 0
                },
                {
                    "sent": "In particular this link scale parameter, which is very important, and this one here is an amplitude.",
                    "label": 0
                },
                {
                    "sent": "And obviously the noise parameters also improve.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is in the game.",
                    "label": 0
                },
                {
                    "sent": "And the important bit is if you know it will choose Gosha noise, then everything is closed form.",
                    "label": 0
                },
                {
                    "sent": "But there are other choices within Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to jump into this, but using approximate inference we can also consider robust noise MoD.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now actually we can just play the game.",
                    "label": 0
                },
                {
                    "sent": "So here we have one set of time series.",
                    "label": 0
                },
                {
                    "sent": "Again treatment control and we can first fit the shared model.",
                    "label": 1
                },
                {
                    "sent": "So here the black line is the mean prediction and here we have error bars to this one day.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Set and now we could fit the independent model, which obviously is a much better.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, and yes, we get it.",
                    "label": 0
                },
                {
                    "sent": "You get a very positive log likelihood ratio score, which favors the independent model over the shared model by quite a significant mark.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this sort of idea we applied to differential gene expression.",
                    "label": 1
                },
                {
                    "sent": "So here we can really just test.",
                    "label": 0
                },
                {
                    "sent": "You know how likely is it that any gene is differentially expressed and which is actually exactly the same question asking whether a single snip has a causal influence on a set of time series.",
                    "label": 0
                },
                {
                    "sent": "So that's a very similar link, and that work very well in this case.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after the sort of recap, the question is, how can we generalize that with the interesting case?",
                    "label": 1
                },
                {
                    "sent": "In the interesting cases that we don't have a single marker, but we have a very large number of markers and putative regulators that could have an influence on the phenome on those time series that we're trying to model.",
                    "label": 0
                },
                {
                    "sent": "And here if you just would group up everything into sort of different data groups, it would just be not tractable at all, right?",
                    "label": 1
                },
                {
                    "sent": "We would have two to the end different groupings of all these different markers and it would just completely breakdown.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the solution that we propose is to say, well, we could also softly group those.",
                    "label": 0
                },
                {
                    "sent": "In a way, softly by defining a suitable covariance function.",
                    "label": 1
                },
                {
                    "sent": "And that's sort of the next step that I would like to.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through So what we're now doing is to say instead of just saying the similarity between any two observations is purely dependent on the time scale on how far they are apart in time.",
                    "label": 0
                },
                {
                    "sent": "Space for all the data in one big bucket, and say the covariation or the smoothness in the space of observations depends not only on how far they are important, I'm space, but also how far are the corresponding individuals genetically apart.",
                    "label": 0
                },
                {
                    "sent": "But any observation is determined by a time point within the time series for specific individual.",
                    "label": 0
                },
                {
                    "sent": "And that individual has a genetic background that we can all have also access to.",
                    "label": 0
                },
                {
                    "sent": "So we propose to say, well, let's say the covariation between an individual with genotype S at anytime point T and another individual as parameter.",
                    "label": 0
                },
                {
                    "sent": "Monte Prime is a product of two covariates and functions or two kernels, one defined on the genetics and one on the time on the time scale.",
                    "label": 0
                },
                {
                    "sent": "And for the time scale, we can assume exactly the same kernel is before the squared exponential kernel that says close things in time are closer together in sort of covariant space, and for the genetics we would like to sort of define a kernel that groups closely related individuals in this genetic sense together, and those which are not closely related, not together.",
                    "label": 0
                },
                {
                    "sent": "In other words, the covariation should be close to 0.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now we can see how we can actually make make this work and do feature selection.",
                    "label": 1
                },
                {
                    "sent": "So here we were using a very basic aid covariance function were just saying the covariance function in this snip space is just a sum overall snips.",
                    "label": 1
                },
                {
                    "sent": "So here's some overall snips, and for every sniper, look at the squared distance.",
                    "label": 0
                },
                {
                    "sent": "Let's assume those guys could be just 01, so this term is 01, and then we divided by length scale parameters.",
                    "label": 0
                },
                {
                    "sent": "So those steps which are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "We have a very very large links get parameter.",
                    "label": 0
                },
                {
                    "sent": "In other words, these elements don't attribute or don't account to that some and those things which are relevant to very short link scale parameter and hence will define whether two individuals are genetically related in the sense of the relevant snips and relevant for predicting the phenotype that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Because there's no don't need to be all snips, this is typically a subset.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and here is some intuition how that would work.",
                    "label": 0
                },
                {
                    "sent": "I mean here just plotted a sample covariance function.",
                    "label": 0
                },
                {
                    "sent": "No discovery function is specially quite a large structure, it's individuals times time point.",
                    "label": 0
                },
                {
                    "sent": "We have one entry per individual end time point and it actually has a sort of block like structure.",
                    "label": 0
                },
                {
                    "sent": "So here any one of those little squares is 1 individual, so within one individual genetic covariances one.",
                    "label": 0
                },
                {
                    "sent": "So we just boiled down to the time covariance and we get this sort of tailoring of exponential on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "And then also Taylors off left and right.",
                    "label": 0
                },
                {
                    "sent": "That's just the normal covariance function and then here we assume that the genetic covariance function would assign that.",
                    "label": 1
                },
                {
                    "sent": "All these guys have a cross correlation of 1, so they are all genetically completely identical, and those are identical and they don't.",
                    "label": 0
                },
                {
                    "sent": "Interfere at all.",
                    "label": 0
                },
                {
                    "sent": "So that's the special case of this covariance function corresponding to the previous two sample case that I introduced in the first part of my talk.",
                    "label": 0
                },
                {
                    "sent": "But really, the point is, this reference function could really softly intervene between any of the things that could be part of the individual, correlating also in the genetic space.",
                    "label": 1
                },
                {
                    "sent": "Or we could have more than two groups, etc etc, etc.",
                    "label": 0
                },
                {
                    "sent": "So some sort of clustering that we're doing here of individual.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this genetic sense.",
                    "label": 0
                },
                {
                    "sent": "So how do we select features?",
                    "label": 0
                },
                {
                    "sent": "Well, the main point is appointed is that these length scale parameters determine which regulators player, alright, short length scales means a relevant parameter largely.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scale means there irrelevant, and we could just follow the basin rule and say we want to maximize the posterior probability of these parameters given the data and the model.",
                    "label": 0
                },
                {
                    "sent": "And that's just the probability of data under the parameters in the model times in prior.",
                    "label": 0
                },
                {
                    "sent": "The problem is that's quite an expensive procedure, and it's expensive because we have to invert for every one of those evaluations.",
                    "label": 0
                },
                {
                    "sent": "If you want to evaluate that function for some gradient calculation etc, we need to invert and J * T James T size matrix, and that doesn't scale all that well.",
                    "label": 0
                },
                {
                    "sent": "So the solution that we found quite practice.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for this application is to revert to some greedy approximation and the greedy approximation.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you reason is a very classical active set selection, so we just say we have a set of snips that do play a role at all, and every iteration we first select a snip to include in the active set by looking at the gradient.",
                    "label": 0
                },
                {
                    "sent": "So we're looking at the gradient of the log likelihood with respect to all of these length scale parameters, and the one that might have the biggest influences included in the active set.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the second step is to just use the current active set and optimize those parameters of the current active set.",
                    "label": 0
                },
                {
                    "sent": "That is sort of feasible because we assume that in any one big datasets, only very few snips play a role, let's say 5, let's say 10, but not.",
                    "label": 0
                },
                {
                    "sent": "That many that this sort of could breakdown.",
                    "label": 0
                },
                {
                    "sent": "And then we iteratively reduces repeat this procedure until we don't make a strong improvement anymore.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and that already leads me to experiment so.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We applied that too because first of all we wanted to see how well can we recover the causal snips, and for that we can only Roberta simulations because we need to have some some ground truth data.",
                    "label": 0
                },
                {
                    "sent": "And here we look really at this active sets of which snips landed in the active set, where they causal, whether not causal.",
                    "label": 0
                },
                {
                    "sent": "And we compared to statistically pens testing.",
                    "label": 0
                },
                {
                    "sent": "Here we use H sick with forward feature selection, and we also considered L1 regularised regression.",
                    "label": 0
                },
                {
                    "sent": "So simply lawsuit and the simulations that we at least got very competitive performance.",
                    "label": 0
                },
                {
                    "sent": "OK, we seem to be able to recover.",
                    "label": 1
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "What is the underlying structure?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we went on to some real data, and that's maybe that's interesting.",
                    "label": 0
                },
                {
                    "sent": "Here we got interesting data from colleagues in Munich and.",
                    "label": 1
                },
                {
                    "sent": "They studied quite a significant number of depressed patients, so these are 360 depressed patients.",
                    "label": 1
                },
                {
                    "sent": "They're all depressed when they come into treatment.",
                    "label": 0
                },
                {
                    "sent": "Then they are treated.",
                    "label": 0
                },
                {
                    "sent": "For a number of eight weeks and every week during the treatment, they basically.",
                    "label": 0
                },
                {
                    "sent": "Produce a score that summarizes their depressive Ness right.",
                    "label": 0
                },
                {
                    "sent": "A high Hamilton score as they called here means high depression level Low Hamilton score low Depression is a very complicated score, but basically there's some quantitative expression of how depressed patients are overtime and for anyone individuals.",
                    "label": 0
                },
                {
                    "sent": "We had 30,000 steps and hear.",
                    "label": 0
                },
                {
                    "sent": "The question was to understand.",
                    "label": 0
                },
                {
                    "sent": "What genetic determination within the individuals causes that some individuals respond to the treatment and others don't?",
                    "label": 0
                },
                {
                    "sent": "Right because you want to know whether this treatment will work for these individuals or not, because otherwise you could help them.",
                    "label": 0
                },
                {
                    "sent": "Maybe otherwise or try different medicine etc etc.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So yeah, really, the goals were identified which snips are relevant for this point, and also, and that's maybe a different setting to the normalization mapping to make predictions early on you want at day one predict whether you may be able to help the patient or not, and if you maybe try for two weeks of treatment and then improve your prediction, that still helpful because you still save six weeks or 10 weeks, etc etc.",
                    "label": 0
                },
                {
                    "sent": "So these are these two goals and we try to sort of answer both of them.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's some 1st results.",
                    "label": 0
                },
                {
                    "sent": "What we're seeing here on the Y axis is some root mean squared error on a test set of the individual.",
                    "label": 0
                },
                {
                    "sent": "So we leave out a few individuals for testing an as a function of the week.",
                    "label": 0
                },
                {
                    "sent": "So these are eight weeks and the black line here is just a simple benchmark where we say, well, let's just in the training set, calculate the mean expression depression level if all the individuals and use that as a predictor.",
                    "label": 0
                },
                {
                    "sent": "Very naive predictor, and that need predictor variable at the beginning because the patients are all depressed when they come in, otherwise there wouldn't be.",
                    "label": 0
                },
                {
                    "sent": "Put into hospital and then later on it's getting a very, very bad predictor because half of the individuals gets better and the other half basically stays depressed or even gets worse.",
                    "label": 0
                },
                {
                    "sent": "And on this sort of scale, our model, which just uses a single snip.",
                    "label": 0
                },
                {
                    "sent": "This ginowan that is making just predictions from the from the Geno type of the single snip.",
                    "label": 0
                },
                {
                    "sent": "The best snip already significantly decreases this predictive error here, and if we include more snip, it gets better, but not all that much.",
                    "label": 0
                },
                {
                    "sent": "So basically the first Nip already tells half the story, and that's also consistent to other studies on this data set, so that she just seems to be the case for this response.",
                    "label": 0
                },
                {
                    "sent": "And the second.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were interested in can we make use of early responses in the data.",
                    "label": 0
                },
                {
                    "sent": "So here we say, well, let's just leave the patients the test patients for two weeks in treatment and use there.",
                    "label": 0
                },
                {
                    "sent": "Their first two data points in this test set to predict their future, so this is this green line here, so this doesn't use any genetics, it just uses the first 2 weeks of treatment to predict the remainder of the time series and that is already still better than the mean model as expected.",
                    "label": 0
                },
                {
                    "sent": "But the genetic still adds a lot.",
                    "label": 0
                },
                {
                    "sent": "And then we can also combine.",
                    "label": 0
                },
                {
                    "sent": "We can say let's use the genotype and the first 2 weeks of treatment response and here we can see that in the beginning we gained significantly over the models and then after a large number of weeks, in this case, six weeks.",
                    "label": 0
                },
                {
                    "sent": "It seems like the first 2 weeks don't tell anything anymore about the outcome and it reverts back to the genetic model.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here maybe to look at the individual scale.",
                    "label": 0
                },
                {
                    "sent": "So this was all average across all tests and videos.",
                    "label": 0
                },
                {
                    "sent": "That's just one individuals.",
                    "label": 0
                },
                {
                    "sent": "How this could look.",
                    "label": 0
                },
                {
                    "sent": "So here we have these two first data points that are also included in the training and these are predicted means and standard error bars of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "These are complete total test points here and we see we have actually getting quite nice fit for quite a few individuals.",
                    "label": 0
                },
                {
                    "sent": "So this looks very very promising in.",
                    "label": 0
                },
                {
                    "sent": "This works for about half of the individuals and four.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Only for the other ones it doesn't quite work, so if we if we look at the last time point, this is the total prediction after 8 weeks and we just make a histogram of individuals to see quite a few of them are predicted extremely well and for others it doesn't work and it's ongoing work to investigate what happens here.",
                    "label": 0
                },
                {
                    "sent": "It looks like they got different medicines and there are all sorts of other variables that have not been taken into account yet, so there's a large scattered between India.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and that is basically it.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me just conclude.",
                    "label": 0
                },
                {
                    "sent": "We propose discussion process model, which is quite efficient.",
                    "label": 0
                },
                {
                    "sent": "At least it's a pliable, applicable to like large scale genetic datasets.",
                    "label": 0
                },
                {
                    "sent": "It can detect associations and also is very good for prediction.",
                    "label": 0
                },
                {
                    "sent": "Interestingly, on this combined setting where we combine some phenotypes to predict others within the time series, which is maybe a little bit of a normal normal setting.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The point here was also to make greedy selection tricks work on the setting that have been well established.",
                    "label": 0
                },
                {
                    "sent": "Also in L1 type models and.",
                    "label": 0
                },
                {
                    "sent": "Could be applied to Gaussian processes.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are lots of future directions.",
                    "label": 1
                },
                {
                    "sent": "For example, we interesting alternative covariance functions.",
                    "label": 1
                },
                {
                    "sent": "This is just one choice.",
                    "label": 0
                },
                {
                    "sent": "There might be others that work even better, and we also interested in sparse approximations to discussion processes, because the limiting factor is this big matrix which is J * 2 T individuals times time steps.",
                    "label": 0
                },
                {
                    "sent": "And for upcoming datasets.",
                    "label": 0
                },
                {
                    "sent": "We still need to do some work to make this.",
                    "label": 0
                },
                {
                    "sent": "Applicable to those data as well.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So it is not very good for basic computer.",
                    "label": 0
                },
                {
                    "sent": "It works very well with innovative with outside of the data properly I was just heading out for other color function that would not be local differences somehow or you go along with his parents when I show everywhere.",
                    "label": 0
                },
                {
                    "sent": "I mean here we use just the square exponential everywhere which which seems to be a quite reasonable first time for the time.",
                    "label": 0
                },
                {
                    "sent": "It definitely works very well for the genetics.",
                    "label": 0
                },
                {
                    "sent": "I agree that this is not the ideal setting yet.",
                    "label": 0
                },
                {
                    "sent": "We need probably something that is much more discreet in saying this snip has an effect, has no effect, and then sort of when it has an effect modulate, maybe with the strength of the effect, or maybe even leave it with this group type of setting.",
                    "label": 0
                },
                {
                    "sent": "Where I agree.",
                    "label": 0
                },
                {
                    "sent": "And that is the first point that we note here.",
                    "label": 0
                },
                {
                    "sent": "There's more work to be done.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "In your greedy selection you re learning the language scales of the whole set of steps at every iteration.",
                    "label": 0
                },
                {
                    "sent": "Or you just learn like tail for the new one.",
                    "label": 0
                },
                {
                    "sent": "We re learn the length scales of all snips in the active set, because that small.",
                    "label": 0
                },
                {
                    "sent": "I mean here we just went up to 25 or something, so that's more.",
                    "label": 0
                },
                {
                    "sent": "And then once that's settled we look at the gradient of all snips.",
                    "label": 0
                },
                {
                    "sent": "So that's the expense you have to do that for all snips.",
                    "label": 0
                },
                {
                    "sent": "But that's a parallel operation, so that runs well on the cluster on many cores, and that's why this is applicable at all.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "I have one thing 'cause I tend to be skeptical about.",
                    "label": 0
                },
                {
                    "sent": "Feature selection based on my own experience.",
                    "label": 0
                },
                {
                    "sent": "Have you compared a model with you identified small instead of snips to a folder model using all its members?",
                    "label": 0
                },
                {
                    "sent": "So I mean.",
                    "label": 0
                },
                {
                    "sent": "For the for this specific task, right?",
                    "label": 0
                },
                {
                    "sent": "If you if you don't select any features and you just make them sort of equally equally relevant, then basically the genetic differences average completely out and you don't, and this specific model it wouldn't work at all for this specific.",
                    "label": 0
                },
                {
                    "sent": "This specific case, generally speaking.",
                    "label": 0
                },
                {
                    "sent": "I'm less convinced that this model isn't very excellent.",
                    "label": 0
                },
                {
                    "sent": "Feature selector.",
                    "label": 0
                },
                {
                    "sent": "I mean we we had this comparison experiment simulation that I showed you where we compared to H seek and linear regression and those answers were sort of in a comparable regime.",
                    "label": 0
                },
                {
                    "sent": "But the interesting aspect of this is a very good sort of prediction method that also can tweak half of the data being in the prediction set, others not in the predictions.",
                    "label": 0
                },
                {
                    "sent": "That is very flexible in that in that sort of setting.",
                    "label": 0
                },
                {
                    "sent": "So that's maybe my answer.",
                    "label": 0
                },
                {
                    "sent": "To that question.",
                    "label": 0
                },
                {
                    "sent": "But yes, you need to select features, otherwise you wouldn't get any answer here.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}