{
    "id": "34yusk6oy3grkogaxfnft3zoutmtco5r",
    "title": "Introduction to Hidden Markov Models",
    "info": {
        "author": [
            "Antonio Art\u00e9s Rodr\u00edguez, Department of Signal Theory and Communications, Carlos III University of Madrid"
        ],
        "published": "Feb. 17, 2015",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Machine Learning",
            "Top->Medicine"
        ]
    },
    "url": "http://videolectures.net/mlpmsummerschool2014_artes_rodriguez_models/",
    "segmentation": [
        [
            "Good morning everyone.",
            "Yeah, this luxury SUV sharp into that show.",
            "Cool a hidden Markov model.",
            "Oldest son, we put those tools.",
            "Which part of you will be or have been through the commercial model or something for that they Angels of this of this storm is to explain a little bit the inside so they may not bring in a Theatre model model and focus late with the model and hope to modify the emission probabilities and so on.",
            "Most of the of the of the result regarding hidden Markov model are now more than four years so.",
            "But on the other hand, there is some a recent result of a show about the scientific community.",
            "Are a US interest in some aspect of the hidden Markov model results so they."
        ],
        [
            "So I know the top will be this one, so I will introduce what is Marco Process and what is hidden Markov models and some of the application of his American models.",
            "I will introduce after that the main inference method in feeding market models one is the forward backward algorithm and the other is 1.",
            "One problem that is not so easy.",
            "There is training the hidden Markov model that is due to the turmoil.",
            "The parameter of the hidden Markov model from a bunch of samples.",
            "After that I will continue with a slight modification of variation on classical hmm.",
            "So first we'll incorporate from Gaussian to Mr. Regression emission probabilities we will present.",
            "Toy example, but it is a thing that is illustrative or whole.",
            "Can we transform a hidden Markov model that is basically an unsupervised model to a supervised one by completing label?",
            "A little bit about auto receive a HMM.",
            "Another generalization of hmm.",
            "At the end I will cover two recent family of metal.",
            "Traces so two of the main problems of the hidden Markov model.",
            "First, in the infinite hidden Markov model that use a Bayesian nonparametric method in order to determine the number of even a state can be also extended to any other fixed parameter of the hidden Markov model.",
            "Well, finally I will cover the spectrum learning of hmm.",
            "Spectral learning is 1 method that has been recently proposed.",
            "Some years ago, and contrary to the rest of the of the method as not the local maximize the likelihood, so is assume local convergence.",
            "So the model parameter.",
            "So let's proceed."
        ],
        [
            "Now with Mark about hidden Markov model."
        ],
        [
            "In the medical model, we are dealing with sequence.",
            "If you try to analyze the genre PDF of a sequence Y from 1:15, one thing that you can do is to decompose using the chain rule.",
            "This joint PDF of the idea of the first observation, multiplied by the conditional PDF of the second observation, even the first one, and so on, and so forth until the last one that is.",
            "Conditional probability of the last sample condition to the rest of the sun.",
            "The Markel Sumption.",
            "Is that some of those sample here are irrelevant.",
            "In order to determine this conditional PDF.",
            "So the most simple case is the so called 1st order Markov process, in which there is a.",
            "All sample except the last one are irrelevant for the demining.",
            "The PDF of a given sample.",
            "So you can represent this here mathematically or graphically here.",
            "In which they you can.",
            "Represent this joint PDF an directed graph directed graphical model, where you can easily verify that condition.",
            "For example to 1 sample to multi.",
            "All the sample in this side of the chain and all the sample in this sense of the chains are conditionally independent.",
            "Let's say it is all you need to know.",
            "Where are they given in some respect to the past, it's just the last sample.",
            "This is the.",
            "Conditional independent of Markov process, this first order Markov process can be extended to 2nd order or further or whatever, just by incorporating more sample in this pedia.",
            "This is a more complex and can deal with more complex relationship with Mark among a sample, but OK most of the time or the most uses the first server and one simplification.",
            "And it is a war is when OK all these conditional PDF.",
            "In principle could be different depending on the on the.",
            "In some time T, but when all these conditional PDF are exactly the same, the Markov processes collect on machine.",
            "OK, but this is how morose, So what is a hidden?"
        ],
        [
            "Local process or sometime you are not able to observe alias the state of the of the of the process.",
            "Typical there is some solution preparation or there is some observation, noise, etc.",
            "In fact, the Markov chain is hidden is represented here by South, represented a state of the update of the chain, and you are only able to observe.",
            "And this is a sample Y.",
            "There is a dissipation of S. And a well you can decompose the young video of the observation and it changes state.",
            "As for the.",
            "Their operation is conditionally independent given the state at this time against the Markov property fall from continuity, do it in chains.",
            "Here we come on this thing with between the nature of the of the space S if the state as our discrete, so we call it is called the hidden Markov model, but.",
            "The PDF of S could be a continuous, so in that case it is called Anna status too small.",
            "For example, an auto racing model can be stated as a Markov process at the state space model in which you have.",
            "Authorizing model order two, the state is formed by the last two sample.",
            "I mean, this is an honest obey this this rule, but in this talk we will say.",
            "Concentrate on hidden Markov model so."
        ],
        [
            "Here you can find an example taken from an excellent review from 1986 or what is a Markov chain on hidden Markov model.",
            "Imagine that you observe multiple coin tosses.",
            "You outcome will be a painting.",
            "Take a sitter and you want to explain this.",
            "So one of the models that you can consider for modeling this observation is a simple one coin model.",
            "In which you only have one coin.",
            "With a certain probability, while you flip the coin, worth the time and with certain probability there is the head that is independent if they respect to the previous previous of.",
            "But you can consider more refinement model with a.",
            "You can see there at 2 two coin model.",
            "Let's say you have for example a fair coin and pay coins.",
            "I'm not biased Korea, so.",
            "Did this take an model?",
            "Where is a coin?",
            "Are you used to?",
            "Are you flipping?",
            "So you can define the probability of observing ahead when you are using.",
            "The first coin as a P1.",
            "Undertail using the first coin as one 1 -- B one.",
            "And with the other coin, the problems of Serena tail is different.",
            "Kind of different communities there to buy P2 and you can.",
            "From a time to time you can either.",
            "A change from one point to another, or use the same point for the.",
            "So this is represented here.",
            "If you are using the first coin.",
            "An A in the next time you will be you will use the same coin with probability A1, one with probability A12.",
            "You want change from going one 2.2 and so on so forth.",
            "And assist with this probability song with this Troy ilities.",
            "All the model is properly defined and is fully defined.",
            "You don't need more OK."
        ],
        [
            "Well, if we want to generalize this that we consider the the more general case or the hidden Markov model you can find here the model.",
            "So we denote as the hidden state sequence of length T capital T and they are capital Y possible stay.",
            "I consider them or one general case in which the observation.",
            "RA.",
            "Real Bachelor of Hunting was a vector sequence and the sequence is obviously from Y one and Y capital T. The model also is defined by the state transition probabilities, that is, the probability that we defined here.",
            "That is, the probability of you are at instant T in a state.",
            "Why?",
            "Ay, ay ay convert percent probability, and then the next install time, and you will be at Stage AE an Additionally to assist ratio estate transition probabilities.",
            "You have also the observation emission probabilities, that is, the probability that relate South with the observation.",
            "What in this case?",
            "This observation and this initial probability depend or are dependent on the state of the chain.",
            "There is that you have a lot of parimeter.",
            "And you have as much parameter at this state is capital Y.",
            "In the case of the observation are discrete and it is with an alphabet of, for example J, single day observation.",
            "Emission probabilities can be put aside matching of size capital I capital J.",
            "Also, you need the initial state probability definition.",
            "Bye.",
            "All these parameters of the model AB Empire with another feature is involved.",
            "I mean OK. Well.",
            "This model."
        ],
        [
            "I think you think enough huge amount of different application.",
            "Perhaps one of the first application was automatic experience and privilege.",
            "In that application, the state S correspond to phoneme or war and why their service sequence to feature a structure for Mystic cinema.",
            "Could be there Mail, a coefficient or energy?",
            "Whatever, whatever model use or you have another possible application is activity recognition, in which as the state will respond to the activity that is subjective performance.",
            "That person is performing that is changing time to time.",
            "And why correspond to the feature structure from video sequence, for example, or from a combination of sensor in a sentence or a separate one more into the?",
            "Copy of this IPM.",
            "The application of a hidden Markov model in gravitational biology, it help also it has also while a area and you can find something for example in the program of game streaming, filing in which South correspond to the location of the gene.",
            "An AY to the DNA negative.",
            "Offer a sample to the problem or protein sequence alignment with S corresponded state can be binary is says if the if this observation corresponding matching lighting consists of sequence.",
            "And why is the sequence of amino acid.",
            "There is much more different applications.",
            "Well."
        ],
        [
            "After presenting the.",
            "Yeah, the model.",
            "Basically we have to play with that model we got.",
            "We can consider."
        ],
        [
            "Different inference problem regarding to hidden Markov model.",
            "The 1st and most simple problem.",
            "Is simple given the observation and the model parameter is to determine the posterior of the offset of the observation or the probability of the observation, method or likeness or not.",
            "The problem with this for calculating this likelihood is that for calculating it, basically you have to take into account that the dependencies through the hidden estate.",
            "So one of the things that you can do is to fill in the joint PDF of the observation on their state and then marginalized over the state.",
            "But the problem is that if the number of spaces, capital Y and a length of the sequence capital T, the number of possible sequence is a capital I capital T, there is an exponential growth with the length of the signal.",
            "That is a huge amount so.",
            "We need to simplify this problem.",
            "One way of simplifying is if you are able to calculate now, then join PDF of their service equal respect to the whole.",
            "Hidden sequence, but just the last sequence to the lost a state.",
            "One thing that you can do.",
            "Is that if you only have to marginalized over the possible state that is, this is only assume of capital Y there.",
            "And there is an algorithm we will present shortly that is the forward algorithm that allows you to compute this quantity in order of AI to 2 * T. So there is a we have changed from exponential complexity, depends exponentially on the length of the sequence to exponential complexity that the length that depends linearly with the length of the sequence.",
            "OK, this is the first problem.",
            "More open problem is OK is not to determine only the likelihood, but there may the Altima.",
            "But they they win, which you can define the optimal Earth can be different than there is different ways of defining this optimum.",
            "For example, the one possibility is to determine the posterior probability of the state at instant T. Even all the observation.",
            "Now select the maximum perform at maximum posterior.",
            "OK, OK this is performed with forward backward right.",
            "Or you can directly want to select the maximum likelihood sequence.",
            "Let's say the sequence that maximize this like little, and they can be obtained by the material they reordering this slightly different on the forward backward algorithm that is used to determine the posterior probability of the sequence.",
            "But it has the same order of magnitude.",
            "In fact, the algorithm is only 1/2 of the computational environment of the forward backward algorithm, because in the former, by what you have to perform.",
            "Two buses for forward and one backward and will give it a real going.",
            "You only need to perform one path forward pass and the complexity of each class is similar.",
            "The only difference is that in the forward backward perform some and in the Viterbi algorithm to perform the substitute is some for maximum.",
            "But the hardest problem, the hardest problem, all this problem.",
            "Ask install in their vinning of the 70s.",
            "The first problem was solved.",
            "Focus on.",
            "But there's a problem that remain in against the hardest.",
            "One is to determine the model parameter that maximize the likelihood or the posterior of the state will begin on slide to this problem.",
            "After that first we will go to treat jointly.",
            "The forward and backward album."
        ],
        [
            "Single slide.",
            "OK.",
            "The forward backward algorithm is the algorithm that allows you to calculate the posterior of the state given the observation.",
            "I removed the dependency of the model parameters for pricing.",
            "So.",
            "This is color gamati.",
            "OK, and for doing this we can decompose first is to transform this conditional PDF into the joint divided by the marginal of the of the observation and after that will be by the observation in two part.",
            "There is a considering that the given the state.",
            "Instant DA, they all sample from C + 1 to the end of the sequence.",
            "Capital TR, conditionally independent to the.",
            "Also, the sequence from Eastern want Winston T, so we divide this.",
            "We will calculate this in two different past.",
            "This part in which you want to estimate the joint probability of the State instant tea.",
            "Given all the observation up, listen, think you will calculate and using the forward incursion and the other one.",
            "This is the probability of the likelihood of the observation for instant plus one descend on the sequence even the state of instant tea is calculated using the backward in question.",
            "The forward equation is obtained simply by operating this.",
            "Probability, and you can.",
            "They can both, or basically, if you're interested, I can do in the black bar, but well, it is.",
            "Depends on your.",
            "So basically the operation is a very simple is that a Jew?",
            "You express this probability of the probability of a white from one to TST&S T -- 1 marginalized over S T -- 1.",
            "As you play a little bit with the equation as you obtain this recursion.",
            "Therefore, the expression of this one that is covered the Alpha respect to the previous, the Alpha in the previous instance.",
            "So you can recursively first to calculate all their fault of the initial Alpha.",
            "That is simply the initial probability of the chain multiplied by the likelihood of the first observation, and then recursively to calculate all the office.",
            "Remember that this Alpha, the full Alpha, can be a star noninteractive arena match with in a metric in which you can.",
            "A you have to restore the Alpha value for each possible value of the state Y from one to capital Y from all the time instead of from one to capital T. And you can do a similar thing with."
        ],
        [
            "For calculating the beta.",
            "You obtain the bug or recursion.",
            "With a similar transformation you you consider the bagger question.",
            "That is, the probability of AY T + 1 + 1 is capital T + 1.",
            "It doesn't exist, so you assume an uniform distribution.",
            "It is well, it is not normalized, but it doesn't need to be normalized.",
            "And after that you proceed from T Bob.",
            "Work until the initial state using this reflection.",
            "Basically you propagate the bidders this probability using the a state transition matrix.",
            "Are weighted by the likelihood of their service segment.",
            "So at the end for calculating the.",
            "Beta on the gamma.",
            "You only have to perform forward pass and about where bus and follow this on.",
            "Combining this descent Alpha you obtain the gamma.",
            "While there is a hard part, the hard part is to obtain the margin of the observation.",
            "OK, but this is not.",
            "This is not a problem because you know that this is a probability.",
            "So you simply have to normalize among wild indigo.",
            "OK, and this normalization factor is the inverse of the marginal, so is Additionally is easy way for calculating the marginal on the full of certain sequence.",
            "OK, so clear."
        ],
        [
            "OK, let's go now to the third inference problem.",
            "That is the hardest one.",
            "A.",
            "Basically you have.",
            "In most of the cases, you don't have a single sequence.",
            "You have a bunch of SQL.",
            "Let's consider that you have and different SQL.",
            "And the first thing that you can do is to express the joint distribution of SRY.",
            "And that is in that form, so it can be the compose.",
            "You can assume that the end sequence are independent, so you can.",
            "There is a joint PDF.",
            "Is the problem of the let's say this is the.",
            "Yeah, at the PDF of the estate you propagate from AS 1 to S capital.",
            "TS1 is a private set.",
            "The initial initial probabilities, and this is given by the transition probability matrix and multiply by the likelihood of the of the observation.",
            "Why OK?",
            "Basically they have looked at.",
            "The problem here is that you only have the sequence Y 7 sequence, but you don't have no idea about what it always is.",
            "So.",
            "The first method for solve this inference problem is to apply EM algorithm.",
            "Let's say is to consider.",
            "As with, they argue hidden variable and performing an expectation maximization algorithm.",
            "In fact it was proposed but bound.",
            "In the beginning of the 70s, before this algorithm was called AM.",
            "It is a also known as about Welch algorithm, and it is also known in the communication literature at the base ejr because there is other guys that propose in the beginning of the 70s this for decoding convolutional codes.",
            "So this is one thing with the bound wells method is fully a maximum likelihood that, let's say do try to maximize this likely wouldn't even stick test.",
            "But you can also follow a Bayesian approach to the inference of the model, and you have basically the two different ways of doing Bashan.",
            "Friends.",
            "One is to use MCMC method.",
            "And the simplest one is the Gibbs sampler.",
            "The force gives sampler for the.",
            "A SMN.",
            "My knowledge is likely sample web.",
            "In a 1993.",
            "You can also use the variational Bayes was first proposed by Mikey in a 90 day one.",
            "It is seven 1997.",
            "So let's process first with the bottom well child point, and then let's I give you a flavor or whole are the base an inference and I will follow.",
            "Mainly they go.",
            "I will explain mainly focused on this website because it is the one that will be also used.",
            "For the basin and permitting.",
            "OK."
        ],
        [
            "Wow.",
            "The formula can be argued, beware, but OK. Local match well probably will be there and you have not seen during the past week so many Sigma Sandpiper Capital five as in this reply but OK this is not bad.",
            "It's not necessarily about.",
            "OK, this is a very simple.",
            "This is the joint distribution of some wire like in the previous slide.",
            "I just stay.",
            "The The law of this PDF for calculating the log likelihood.",
            "OK and.",
            "Assuming all her, I will try to you have to express.",
            "Sample the probability of a.",
            "The transition from one state to another, even all the observation for doing this.",
            "I will use this indication indicator function.",
            "This indicator function will be one if a given all the observation.",
            "The first state is the state I. OK, and so I transfer this term into this and by doing this I perform a summation over the whole state.",
            "This transition probability there is present here.",
            "I'm Esis press as the indicator function of doing a transition for from State Y. Este J awaited by the lowering of the transition probability matrix.",
            "And this term.",
            "Corresponds to this one, and if we rearrange an if instead of ordering first by the number of sequence, we rearranged all these symmetries and the first one is respect to this state.",
            "We'll think this further.",
            "OK, using this formula we are able now to perform the expectation and straighten mathematician instead of being an algorithm."
        ],
        [
            "So this formula is the same here.",
            "OK, and I have to perform the aspect asianna step.",
            "So first is the assist as expectation of the state.",
            "The initial state.",
            "Is not a statement instant one, given or taken value of Y?",
            "This is actually when when you take this dictation, this is exactly the gamma.",
            "The posterior probability of the initial state given all the observation.",
            "And this part is they correspond to the state transition.",
            "When you perform this a spectation, well, you need to define a new quantity that is called to a slight margin up.",
            "The seeds of an entity that has a 2A in it for a instant E is exactly the approach the joint probability given the whole sequence of B at instant T at the same J at instant T -- 1.",
            "Understand why?",
            "Remember that the government are they are the conditional and kind of stress.",
            "Big can be expressed also in the term of the Alpha, beta and the.",
            "A. I think transition probability and the likelihood of the observation.",
            "And the last one that is the likelihood of the observation is also dependent on one day, So you have all the separate units for all the information that you need for performing the aspect Tatian step of the.",
            "After that you have to maximize, so remember that.",
            "Here.",
            "You have the sum of the gammas in the in the initial state, so you."
        ],
        [
            "Do they given this a gamma?",
            "The the maximum likelihood estimation of the probability is simply.",
            "This gamma is normalized.",
            "Also, there is a maximum likelihood estimation of the state transition element AI, so IA is simply the normalized.",
            "Yeah, then normalize so of the truth lies margin.",
            "And if you consider Gaussian emission probabilities is like this, just like estimation of the mean and variance of Gaussian of a Gaussian.",
            "But remember that is a Gaussian for each state and all the observation are weighted by the posterior probability of the state, even the whole.",
            "Observation for the rest is rather simple.",
            "Even the if the expression are, you can see that less complicated."
        ],
        [
            "OK.",
            "This is the classical one.",
            "The inbound works and it can be modified a slightly different way is to perform based based on inference for HMM.",
            "So for performing based on important, first you have to define private.",
            "You have to have the prior over the parimeter and remember that the parameter are the state transition matrix.",
            "The emission probability parameter.",
            "Kill it still.",
            "OK, so the most number one prior is a for the state transition.",
            "Matrix.",
            "A is to assume independent rogue.",
            "Remember that this is each row represent the probability from going from one given a state to all there.",
            "So this is a probability and you can use our prior at the distribution.",
            "For the rest, I'm mainly for the observation.",
            "If possible, you can use a conjugate prior because you know you have to for calculating the posterior you have to perform some approximation OK, But there is a lot of literature that gives you in the case your prior doesn't fit or it doesn't.",
            "Is a conjugate prior you can perform also a certain cost, but easily so.",
            "If you are discrete observation.",
            "A prior that gives you a conjugate prior could be also do the distribution, and if you're a observation, the prior of your observational are normal or Gaussian.",
            "One of the conjugate prior is to use normal for the mean animal wishe for the mattress.",
            "For the covariance matrix.",
            "So when you have said this prior after that, you have to perform."
        ],
        [
            "The inference that so in different method.",
            "Basically you have two option.",
            "As I have said.",
            "What is the example I don't know?",
            "Are you familiar with the gift sampler or you know what is a gift sampler and the rest of them?",
            "OK, for the rest, well I give sampler is some in the simplest MCMC method.",
            "Respect to the all the variable that you have to imperil to infant.",
            "Basically a do calculate the posterior.",
            "Of this, of each of the quantity given in the rest, and you run or obtain a sample.",
            "And you do this sequentially with all your parameters.",
            "I'm doing it interactively.",
            "You obtain the solution, but remember that the the Infra Smith the Basin method.",
            "Either you use a GIF sampler or operational base.",
            "They even don't send a boy that problem of the bound and works algorithm is that they have local maximum into likelihood of they have local Maxima into the into the posterior.",
            "So for opinion, the posterior of each parameter given the rest it is you have to obtain the posterior of AST given the observed sequence.",
            "And the rest of the state.",
            "Does this from time is not one to one time instead of capital T?",
            "And if also you have to calculate the posterior of a given the rest that the only condition that you need is to respect to S and is very simple because you have only up to count for doing this and you have to calculate the posterior build given the rest.",
            "But depends only.",
            "Or why it S and is independent of the of the rest of para meter and also pick the most challenging part here.",
            "Maybe if you use a conjugate prior for the observation.",
            "Is the calculation of the posterior of each of their stay at a given instant T even the rest of the parameter in pulling the rest of the state?",
            "The rest of the time itself, but this can be obtained easily or computationally cheap using an algorithm that is similar to the forward backward algorithm and is it is called the forward filtering backwards sampling.",
            "It is basically because you have to perform forward forward path that in mind the Alpha.",
            "And after that in the power path, instead of being a. Posterior probability you just something and it is a well you can.",
            "It is fully explained in this in this book.",
            "OK, and it is computationally simple if you don't have this algorithm, they forfeit embark on something.",
            "This is the more challenging part.",
            "OK. One alternative approaches to use operational base when they say you construct the evidence lower bound.",
            "This sort of able.",
            "That is, assuming independence among the parimeter.",
            "Let's say you assume that the.",
            "This distribution.",
            "Or in this distribution?",
            "A there is a the.",
            "Here.",
            "OK, yeah you assume that the A it doesn't depend on as they be in this independent.",
            "Why on earth and is independent and you perform an algorithm that is similar to OEM and you obtain also compares to the solution.",
            "OK, any questions to here.",
            "OK, those are the classic elemental.",
            "You can find them in any textbook.",
            "A there you can modify if you are not able to find exactly the model you are looking for in a textbook or in an article, you can modify your model as you can modify the algorithm is."
        ],
        [
            "So."
        ],
        [
            "Yeah.",
            "We want, oh we will do it for different cases.",
            "Imagine for example and this is.",
            "You can find this this example also in any textbook.",
            "A that you can instead of using a Gaussian emission probability your service and are more complex than this.",
            "You want to model as a Gaussian mixture model again.",
            "OK so.",
            "It is very easy.",
            "The only thing you have to do is OK here you have in a importable the things that change this respect to the Gaussian page.",
            "So here you have a mix of K component.",
            "So when you calculate this indicator function you have this step a probability you have some.",
            "Over K here over the component for performing the eastep, you only have to do the same thing as a like in the previous one.",
            "That is exactly exactly is obtaining the.",
            "The gamma that we obtained previously but multiply or weighted by the.",
            "A probability of each component of a combination probability of their of their of image."
        ],
        [
            "For performing the end step.",
            "For maximizing, that is exactly the same."
        ],
        [
            "It.",
            "Except that like here, remember that, let's say it is here.",
            "You can have the gamma.",
            "Then there are sort of the forward backward, but they're different things here.",
            "Is that you think a different sequence for a mixture components and its mixture component is weighted by the likelihood of this component is like instead of having now metrics for representing all the gamma.",
            "There is a of the size of capital YY Capital T you have now capital y * K by the number of components and these are capital costs on.",
            "You can or you can rent in a tensor if you if you like."
        ],
        [
            "So I'm guessing there when you have obtained in this way to gamma weighted posterior respect to the each component of the of the of the mixer, the thing you have to do is exactly the same the same as previously as a using the Gaussian observation, but now using the new yoga thing.",
            "I'm not weighted by the likelihood of each component of the of the optimization.",
            "So this is a simple modification, but you can also modify."
        ],
        [
            "The model.",
            "For example, for including label.",
            "As I said previously, they are the HMM is an unsupervised method for modeling the observation.",
            "But sometimes you have or you want to use HTML so classifier to classify things.",
            "So one naive way of doing this there is some more elaborate ways is considered, for example that they a label of each sample corresponding oh are also a given by the state.",
            "And you have a legal emission probabilities, but usually you can.",
            "You can specify previously.",
            "Let's say for example that if you have for example, two different label binary label OK and you have for it, for example 5 estate.",
            "You can specify matrix D. You can specify which state model each observation.",
            "OK, and this is the label sequence, so you can use the same empower.",
            "The only thing that appear is a new observation.",
            "The language of the label F so.",
            "It has a.",
            "This observation is independent condition on the state.",
            "So when you consider the full PDF of this table selection on the label, it's only multiplying.",
            "Also buy a term that is the probability of the label given the state."
        ],
        [
            "You can introduce this.",
            "Into the log likelihood.",
            "The thing is that the new term here I do have the dependency with the label here, here and here."
        ],
        [
            "Then this is the Alpha and the beta of the former board of the power forward.",
            "Backward algorithm is almost the same except that.",
            "When performing the forward step, when Angie, multiplied by the transition.",
            "Let's say it is here.",
            "And if you suppress the fire in green, do you think the Origonal 1?",
            "So the only thing that changed is that after performing the summation of the alphas in the previous sign instant given by the state transition probability, proficient is multiplied by the like it.",
            "But the likelihood of the observation at the likelihood.",
            "Of delay.",
            "And this is exactly the same for the thing in the beta.",
            "So it's only adding a parameter.",
            "Here, and you obtain a new Alpha and beta."
        ],
        [
            "Beautiful.",
            "You in the game as your painting to a slice marginal so exactly the same, it's just at that range you have the likelihood you multiply that since also the likelihood the obvious by the likelihood of the label of.",
            "Remember that if this light include is a one or zero, it means that some part of the sequence are used to train or to determine the perimeter.",
            "Certain estate in order to over is like finally you obtain.",
            "I still don't see a magic that is a block diagonal matrix.",
            "OK."
        ],
        [
            "Hey, this is for training, but for the operation you don't have the label.",
            "But you can obtain the label as a function of the state of the hidden estate.",
            "But it is a pretty easy to consider or to remove the label.",
            "Becausw is simply that if you don't have the label."
        ],
        [
            "In the previous step, you don't have to put the don't have to put this step.",
            "So for running the algorithm for determining the estimation of the posterior probability of the of the state, you don't have to consider the level.",
            "You do exactly the same action without delay.",
            "But you have called it the information about the labeling to this state into the station, systematic and into the initial product.",
            "OK."
        ],
        [
            "So.",
            "A.",
            "As you can see.",
            "Dealing with missing values in HTML is pretty easy.",
            "It is basically is that when you have to perform."
        ],
        [
            "The forward or the backward step.",
            "There is a you don't have information about this.",
            "So you have.",
            "This is simply one.",
            "For all the possible states.",
            "OK, so there is not like other infant algorithms problem.",
            "When you have missing values with even more communities.",
            "3D sequel DLC."
        ],
        [
            "OK, I will be leaving soon.",
            "Probably yes at the priority and you can even.",
            "Complicated.",
            "More your model and imagine, for example that.",
            "You're a.",
            "Your observation of you model survey shun is very simple or simply that OK you can.",
            "You want to express by the stage one different knowledge on label or where you want but you have another short 10 dependencies.",
            "And and this certain dependencies, for example, can be included in the observation.",
            "And you can define for."
        ],
        [
            "Sample and autoregressive hidden.",
            "So you can complicate further than normal.",
            "So this is also is pretty easy to perform saying you have to slightly modify the forward backward algorithm in order to include the prediction.",
            "Basically first you have to perform the prediction and after that you have to calculate the likelihood but.",
            "Using also OEM algorithm, here is the form that adopt the expectation of step.",
            "Add and as a result of the optimization step you could obtain or do obtain the covariance matrix that it is the magic that you need to design the linear regressor from a YT minus 12 white.",
            "You can complete it, or you can make model as complicated as you need it.",
            "And you have you don't have to deal with many.",
            "It.",
            "God of other algorithm like a heater model and but you have to.",
            "The only thing you have to do is to rewrite the equation according to your model and you obtain version in another.",
            "OK."
        ],
        [
            "Organization of the HMM there.",
            "For example a hidden semi Markov model OK?",
            "The Markov model is very simple, but as several limitation one of them is that as you have on a state transition matrix.",
            "A big time.",
            "The probability of staying in a given state.",
            "Indicate exponential it is just cause the state transition probability independent.",
            "Nothing in just a parameter.",
            "This could be not or could not fit exactly your data, so you can do a hidden semi Markov model in which the state state transition probability depends not only.",
            "Or not even number but depends also on the day of the time you stay in this in this state.",
            "So it's a slight modification.",
            "Or you can have an.",
            "Also, an input is like the control input to stay the same model.",
            "And you have also the in front of it or for example, instead of making the observation more and more complex, you can do a hierarchical hmm, let's say in which you define top level and its state it developed in a lower level as another full achievement defining hierarchically or.",
            "A.",
            "You can make also factorial hmm.",
            "A factorial hmm is simple and simply when the number of the state is very very big, difference becomes more and more complicated.",
            "So one thing that you can do that you can do is to factorize this thing.",
            "Let's say for example in not hidden Markov model.",
            "Let's say for example that you are trying to model for example that.",
            "Evolution of a disease.",
            "OK, and there is change in the disease and you can you want for example, to code into the state all the information about the the dictation even patient or something.",
            "It's a or is, gender, etc.",
            "But if you if you want to call more and more information.",
            "The number of the state role is one of the bigger, the other value other whatever.",
            "So one thing that you can do is to define independent change for each of the parameter for the eight, for the ginger etc.",
            "They join it all together into a single emission probability.",
            "Or it's an emission probability that depends on all the change.",
            "These correspond of this is a factorial hmm?",
            "OK?",
            "Or there is another situation in which a you have more than one hmm but they are not fully independent and you are able to model the dependencies among them.",
            "This is called a couple engineer.",
            "There is also improving.",
            "OK, any questions you know.",
            "Yeah so.",
            "Function.",
            "Medication are set up.",
            "What happens if not is to form simple, tractable or?",
            "As a truly impeccable uof change language press half you are not, or you're a you don't control the final solution.",
            "So for example, if each of the images that you have discrete observation OK, well you have this kind of operation into the label.",
            "Let's say imagine that all the use of case in which is, which in which?",
            "It's wrong, you have our address here.",
            "But aside and state for legal.",
            "OK, but imagine that you have a distribution or that Rd that is not one of the results.",
            "You have some certainty about this state, or you cannot infer the label given the state you have a posterior probability.",
            "Office State of the little story of the label given this state.",
            "But not a single one, but you can do it.",
            "And then you can define.",
            "Semi supervised training.",
            "In which you are a, you are sure about certain labels.",
            "You are not sure about, certainly, but you can have both.",
            "The uncertainty that you have overdue label.",
            "And this gives you a model English.",
            "Finally, you obtain the posterior probability of the label given the state.",
            "OK. Yeah.",
            "Welcome back even sending Michael.",
            "No, I just.",
            "Understand the students semi Markov model.",
            "So what is the same market model?",
            "This city market.",
            "Yeah, yeah.",
            "It was not useful for OK. And if you have an ignition probability, very sticks right there parameter, what are the 8?",
            "Hey.",
            "The time that you stay in a given state decay exponentially with time.",
            "But the your your reality.",
            "But your data and it doesn't fit with that model because there is.",
            "For example, you're saying that to the same state.",
            "For a very long time are you changed for another state?",
            "And in one state you are.",
            "Imagine, for example, that one state is represent an established a situation, another a taxation.",
            "That ain't the time that you remain, or the probability of that you remain the given state using the standard model is the same, But the game exponentially with time.",
            "But making the transition probability.",
            "Major depending on the state.",
            "You can also modify this.",
            "OK. More question.",
            "OK."
        ],
        [
            "So let's go to the.",
            "Final part."
        ],
        [
            "Well, as I said.",
            "The HMM, apart from the limitation of the model that can be seen or etc, they have set down.",
            "Problem that is 101.",
            "One of them is the model selection.",
            "Let's say if you are not sure about the number of hidden estate.",
            "And you have to do a model selection instead, OK?",
            "A the thing that is usually do is OK, you use your favorite complexity measure.",
            "You use the peak of the account information pretty regularly.",
            "Description length or a the.",
            "Log likelihood of day of the convention.",
            "Whatever criteria an annual include, like in some of them, some penalty term for complexity and basically went in for the most common solution is that you train different hmm for different value of capital Y of the hidden state.",
            "Can you apply this measure?",
            "I just select the proper way.",
            "OK, but this is too expensive an the best thing to do probably is not to do a proper model and model selection to our model.",
            "Leveraging or to fit the model to your training to your training data.",
            "So our solution is using.",
            "Number use in Bayesian parametric technique until define the circle infinite hidden Markov model.",
            "The agent, the infinite hidden Markov model, is that assume that the number of states initially is initially in.",
            "But you will.",
            "They use or you will assign transition probabilities different from zero to one state according to your data.",
            "OK, so in that case you don't have to do a proper model selection.",
            "And this is one of the problem and I will go after that.",
            "So some detail of this.",
            "The infinite hidden Markov model.",
            "And the other is the local maximum of the likelihoods.",
            "Let's say a. Aiden if you use a log likelihood algorithm to fit the model like the bottom world salary, or if you use a Bayesian techniques.",
            "Due to the presence of the hidden estate, you likely could have fixed.",
            "So you can be a star, you local maximum of this like.",
            "There is no way in principle that assure you that you are in the global maximum of the like, so they're useful strategy is to reinitialize.",
            "A autorun they there are several times.",
            "And according to your criteria, could be the likelihood.",
            "Respect to that, you select the back, but recently they are asking proposed.",
            "Method that avoids this problem and if they said on a moment matching are useful.",
            "Respect on learning of hmm.",
            "So will first today.",
            "Infinity the marker model an after something to the spectral length and of hidden."
        ],
        [
            "Well.",
            "For a building with the infinite hidden Markov model, let's go first in more detail into the Bayesian solution.",
            "For the third in current problem.",
            "OK so I said previously in basically method you have to put a prior and you have to define or you have to calculate the posterior of the parameter.",
            "OK so for example if you have based on that email with discrete observation and a single sequence and you define a supplier.",
            "I did let's pray on a.",
            "For the role of the transition matrix and each transition metric to be independent, OK, and you can use for example.",
            "So you can use this symmetric dinner.",
            "Let this is Alpha divided by the number of the state by vector one.",
            "Symmetric.",
            "For the Salvation, if you have more information about the state of the State representative of the relationship between the state and the.",
            "Observation you can use this knowledge to coordinate a sub vector parameter D in order to define dizziness prior for the growth of the beamer.",
            "The observation map.",
            "So one thing that you have to do is.",
            "And when you have, for example using the, give some.",
            "Let's say in the sampler to sample sequentially all the variable that you have.",
            "So you have to calculate the posterior.",
            "Then you have to calculate the posterior of the each of the roles of the transition metric given the rest of the parameter, and they each of the roles of the selection metric given the rest of the family.",
            "So basically the things that you do is a you have.",
            "A. I stay sequence.",
            "You simply count the number of transition for and stay white with AJ.",
            "This is NYA.",
            "Are you far with this object?",
            "I do the same respect to the observation.",
            "I do form the vector A.",
            "And after that, well, that the prior is conjugate to the likelihood.",
            "So the posterior is also added.",
            "It's left.",
            "This is that reflect this.",
            "The empirical distribution plus the price.",
            "Obviously if you have more than more than the prior has less influence because this is not normalized OK, and the and the same exactly the same respect to the observation logics.",
            "And one thing that you got there that you can do is using a gift sampler.",
            "Samples from the distribution and it's pretty easy.",
            "OK, but going from."
        ],
        [
            "Is through the infinite hidden Markov model the first thing that you can do?",
            "Is to consider a number of a state that is infinite.",
            "OK, after that OK will we?",
            "Will I need to define what is on a stick breaking process?",
            "There is something related with an English process.",
            "OK, that is OK. You for you are a obtaining a distribution.",
            "It's title given one single parameter, so in this single parameter gamma tell you is how to divide the interval between zero and one.",
            "And it is in the 1st.",
            "And the first interval is proportional to the.",
            "Is given by bit beta distribution with parameter cannot so proportional to be so the second one is 1 minus.",
            "The portion you have ascended to the first one, and you're a Jew, and multiply it by a new breed of parameters and you continue until Infinity and this is the this is like the division of of.",
            "Of the interval between zero and one in pieces, each time a smaller and smaller.",
            "And this gives you the basis for defining a distribution over Infinite State.",
            "So.",
            "A.",
            "Respect to the model, but you have to change from the Bashan.",
            "Hmm, is that OK?",
            "Observation would remain the same.",
            "OK, but here.",
            "There is a hierarchical model in which the prior for your A for each of the role of the transition matrix is also on a stick breaking process that depends.",
            "And another variable, but this is on a stick breaking process.",
            "Why do you need this hierarchical definition?",
            "Do you know this hierarchical definition becausw?",
            "All all of the roof of the transition matrix.",
            "Ask to have was to have the same primer and to have the number of a occupied state.",
            "The state with English transition probability that estate is greater than 0 is not new.",
            "OK, this format junior hierarchical data that process.",
            "For example, if this is not in one situation in which you don't need this hierarchical during that process.",
            "Is that instead of performing inference over a hmm you to refine?",
            "Submission model with an infinite number of components.",
            "In that case, the prior of this component it don't depend on nothing.",
            "So you don't need this hierarchical construction that you need here for the HMM, so after that you have defined, also calculate the positive.",
            "So let's define capital K and the number of active state.",
            "Let's say the number of the state in which there is a new probability of transition.",
            "I will define.",
            "A super high as the state transition probability to those states plus the K plus one is the sum of all of them.",
            "That is principle is there.",
            "OK, and a exactly respect to the asylum.",
            "That is the distribution that define the fryer for all the rows of the transition matrix.",
            "Define the first K and after that you make the sum of all of this.",
            "So in order to resample, it is easier to obtain the.",
            "A the posterior of the state transition probability.",
            "This is is like.",
            "Previous one OK.",
            "Math here.",
            "You have the fryer.",
            "That is, this part will be playing by Alpha.",
            "Their empirical distribution.",
            "OK, so the B is exactly the same.",
            "The most complicated part is to obtain the posterior.",
            "All the excitement of the distribution that define the prior of each row of the transition match.",
            "OK, and this is a all this is.",
            "Can be interpreted as a model.",
            "They have different interpretation.",
            "One of them is to use him on hierarchical, polio, polio, polio.",
            "The things perform like this OK is a.",
            "Think about this, so maybe that's a.",
            "Here, here there is a non low probability.",
            "That there is some transition to state K plus one.",
            "So in that in that case Joe is like if you do another resample.",
            "Liver sample is exactly the same of your sample.",
            "The number of.",
            "Fiction, but you have from the state.",
            "While twisting J.",
            "Exactly is like you for each one of those transitions, you flip a coin, flip a coin in which you have.",
            "A probability or is there no need of parameter Alpha multiplied by excellent J OK?",
            "So of accepting or not?",
            "Each one of those problems.",
            "So with this you obtain this new code 0.",
            "I do know this thing.",
            "So you come all of this.",
            "If you perform a similar operation like this, but with this resample transition.",
            "With the resampled transition you construct.",
            "They vector see that they say the empirical distribution of each of the transition and gamma parameter they have parameter is the parameter that control the board of Annual state of the currency of Nu Estate.",
            "Let's say if you put a high value on gamma, the number of a occupied state grows very fast.",
            "As does the number of training samples.",
            "If the parameter gamma is very low.",
            "The number of hidden estate grow slowly.",
            "With George trend sample, so you think this?",
            "Probably to see you perform an audio thing.",
            "The posterior of the side of the define the prior over.",
            "It's a role that transition match.",
            "OK, the difficult part here.",
            "Over to the classical give sampler for the HMM is that you cannot use here in the forward 50 backwards sampling algorithm.",
            "So sampling into the hidden state.",
            "Could be expensive and it is a difficult.",
            "And if you don't consider the dependency number mistake, then beings of the chain.",
            "It will be very, very slow, so there is a plethora of different algorithms for performing inference here.",
            "I think one of the simplest."
        ],
        [
            "More useful is.",
            "A the the one proposed by Jurgen Bangle is to use our being sampling.",
            "There being something is just.",
            "Use an aux variable that make the number of the state field you have the numbers of dating.",
            "You can use the former featuring black or something.",
            "OK. And so there is a different a lot of variation respect to this.",
            "For example, you can also many use nonparametric model for the emission probabilities.",
            "Emily talks us out of work on this on this part.",
            "Hold to determine the order of the number of the mixture component for the observation probabilities.",
            "The order of the fine.",
            "Auto racing, observation, model etc.",
            "So that you can communicate."
        ],
        [
            "And the last thing I don't know this.",
            "Yes, running lights.",
            "OK, well this is a. I will explain how funny well I did not give no give all the details OK and there is this this technique of it as being proposed some years ago the 1st Paper of hearing the Lucas on a something like this.",
            "The thing is that they use a different model for representing the.",
            "A operation on the for the.",
            "This is a new observation model.",
            "This observation model was taking from automata theory and etc.",
            "So basically the things that you do is you use.",
            "You perform the same marginalization OK, but using a product of matrices?",
            "And you are not able to determine the true one, but you are able to determine.",
            "An affine transformation of this, and this is fitted by calculating by estimating the moment the probability of a given state.",
            "The probability of the transition, and this tensor there.",
            "Is there a probability of going from one state to another, and 12 is the 3rd order moment and you see you the compose this matrix using an SVD."
        ],
        [
            "I look up find the equivalent.",
            "The nice message is that this method, compared to the rest of the method interest no local minima.",
            "But it's performed well only if you have a low out of sample, because it's a moment that can clean it.",
            "OK, and that's all.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everyone.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this luxury SUV sharp into that show.",
                    "label": 0
                },
                {
                    "sent": "Cool a hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "Oldest son, we put those tools.",
                    "label": 0
                },
                {
                    "sent": "Which part of you will be or have been through the commercial model or something for that they Angels of this of this storm is to explain a little bit the inside so they may not bring in a Theatre model model and focus late with the model and hope to modify the emission probabilities and so on.",
                    "label": 0
                },
                {
                    "sent": "Most of the of the of the result regarding hidden Markov model are now more than four years so.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, there is some a recent result of a show about the scientific community.",
                    "label": 0
                },
                {
                    "sent": "Are a US interest in some aspect of the hidden Markov model results so they.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I know the top will be this one, so I will introduce what is Marco Process and what is hidden Markov models and some of the application of his American models.",
                    "label": 0
                },
                {
                    "sent": "I will introduce after that the main inference method in feeding market models one is the forward backward algorithm and the other is 1.",
                    "label": 0
                },
                {
                    "sent": "One problem that is not so easy.",
                    "label": 0
                },
                {
                    "sent": "There is training the hidden Markov model that is due to the turmoil.",
                    "label": 1
                },
                {
                    "sent": "The parameter of the hidden Markov model from a bunch of samples.",
                    "label": 0
                },
                {
                    "sent": "After that I will continue with a slight modification of variation on classical hmm.",
                    "label": 1
                },
                {
                    "sent": "So first we'll incorporate from Gaussian to Mr. Regression emission probabilities we will present.",
                    "label": 0
                },
                {
                    "sent": "Toy example, but it is a thing that is illustrative or whole.",
                    "label": 0
                },
                {
                    "sent": "Can we transform a hidden Markov model that is basically an unsupervised model to a supervised one by completing label?",
                    "label": 0
                },
                {
                    "sent": "A little bit about auto receive a HMM.",
                    "label": 0
                },
                {
                    "sent": "Another generalization of hmm.",
                    "label": 0
                },
                {
                    "sent": "At the end I will cover two recent family of metal.",
                    "label": 1
                },
                {
                    "sent": "Traces so two of the main problems of the hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "First, in the infinite hidden Markov model that use a Bayesian nonparametric method in order to determine the number of even a state can be also extended to any other fixed parameter of the hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Well, finally I will cover the spectrum learning of hmm.",
                    "label": 0
                },
                {
                    "sent": "Spectral learning is 1 method that has been recently proposed.",
                    "label": 0
                },
                {
                    "sent": "Some years ago, and contrary to the rest of the of the method as not the local maximize the likelihood, so is assume local convergence.",
                    "label": 0
                },
                {
                    "sent": "So the model parameter.",
                    "label": 0
                },
                {
                    "sent": "So let's proceed.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now with Mark about hidden Markov model.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the medical model, we are dealing with sequence.",
                    "label": 0
                },
                {
                    "sent": "If you try to analyze the genre PDF of a sequence Y from 1:15, one thing that you can do is to decompose using the chain rule.",
                    "label": 0
                },
                {
                    "sent": "This joint PDF of the idea of the first observation, multiplied by the conditional PDF of the second observation, even the first one, and so on, and so forth until the last one that is.",
                    "label": 0
                },
                {
                    "sent": "Conditional probability of the last sample condition to the rest of the sun.",
                    "label": 0
                },
                {
                    "sent": "The Markel Sumption.",
                    "label": 0
                },
                {
                    "sent": "Is that some of those sample here are irrelevant.",
                    "label": 0
                },
                {
                    "sent": "In order to determine this conditional PDF.",
                    "label": 0
                },
                {
                    "sent": "So the most simple case is the so called 1st order Markov process, in which there is a.",
                    "label": 0
                },
                {
                    "sent": "All sample except the last one are irrelevant for the demining.",
                    "label": 0
                },
                {
                    "sent": "The PDF of a given sample.",
                    "label": 0
                },
                {
                    "sent": "So you can represent this here mathematically or graphically here.",
                    "label": 0
                },
                {
                    "sent": "In which they you can.",
                    "label": 0
                },
                {
                    "sent": "Represent this joint PDF an directed graph directed graphical model, where you can easily verify that condition.",
                    "label": 0
                },
                {
                    "sent": "For example to 1 sample to multi.",
                    "label": 0
                },
                {
                    "sent": "All the sample in this side of the chain and all the sample in this sense of the chains are conditionally independent.",
                    "label": 0
                },
                {
                    "sent": "Let's say it is all you need to know.",
                    "label": 0
                },
                {
                    "sent": "Where are they given in some respect to the past, it's just the last sample.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                },
                {
                    "sent": "Conditional independent of Markov process, this first order Markov process can be extended to 2nd order or further or whatever, just by incorporating more sample in this pedia.",
                    "label": 1
                },
                {
                    "sent": "This is a more complex and can deal with more complex relationship with Mark among a sample, but OK most of the time or the most uses the first server and one simplification.",
                    "label": 0
                },
                {
                    "sent": "And it is a war is when OK all these conditional PDF.",
                    "label": 0
                },
                {
                    "sent": "In principle could be different depending on the on the.",
                    "label": 0
                },
                {
                    "sent": "In some time T, but when all these conditional PDF are exactly the same, the Markov processes collect on machine.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is how morose, So what is a hidden?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Local process or sometime you are not able to observe alias the state of the of the of the process.",
                    "label": 0
                },
                {
                    "sent": "Typical there is some solution preparation or there is some observation, noise, etc.",
                    "label": 0
                },
                {
                    "sent": "In fact, the Markov chain is hidden is represented here by South, represented a state of the update of the chain, and you are only able to observe.",
                    "label": 0
                },
                {
                    "sent": "And this is a sample Y.",
                    "label": 0
                },
                {
                    "sent": "There is a dissipation of S. And a well you can decompose the young video of the observation and it changes state.",
                    "label": 0
                },
                {
                    "sent": "As for the.",
                    "label": 0
                },
                {
                    "sent": "Their operation is conditionally independent given the state at this time against the Markov property fall from continuity, do it in chains.",
                    "label": 0
                },
                {
                    "sent": "Here we come on this thing with between the nature of the of the space S if the state as our discrete, so we call it is called the hidden Markov model, but.",
                    "label": 1
                },
                {
                    "sent": "The PDF of S could be a continuous, so in that case it is called Anna status too small.",
                    "label": 0
                },
                {
                    "sent": "For example, an auto racing model can be stated as a Markov process at the state space model in which you have.",
                    "label": 0
                },
                {
                    "sent": "Authorizing model order two, the state is formed by the last two sample.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is an honest obey this this rule, but in this talk we will say.",
                    "label": 0
                },
                {
                    "sent": "Concentrate on hidden Markov model so.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here you can find an example taken from an excellent review from 1986 or what is a Markov chain on hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Imagine that you observe multiple coin tosses.",
                    "label": 0
                },
                {
                    "sent": "You outcome will be a painting.",
                    "label": 0
                },
                {
                    "sent": "Take a sitter and you want to explain this.",
                    "label": 0
                },
                {
                    "sent": "So one of the models that you can consider for modeling this observation is a simple one coin model.",
                    "label": 0
                },
                {
                    "sent": "In which you only have one coin.",
                    "label": 0
                },
                {
                    "sent": "With a certain probability, while you flip the coin, worth the time and with certain probability there is the head that is independent if they respect to the previous previous of.",
                    "label": 0
                },
                {
                    "sent": "But you can consider more refinement model with a.",
                    "label": 0
                },
                {
                    "sent": "You can see there at 2 two coin model.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have for example a fair coin and pay coins.",
                    "label": 0
                },
                {
                    "sent": "I'm not biased Korea, so.",
                    "label": 0
                },
                {
                    "sent": "Did this take an model?",
                    "label": 0
                },
                {
                    "sent": "Where is a coin?",
                    "label": 0
                },
                {
                    "sent": "Are you used to?",
                    "label": 0
                },
                {
                    "sent": "Are you flipping?",
                    "label": 0
                },
                {
                    "sent": "So you can define the probability of observing ahead when you are using.",
                    "label": 0
                },
                {
                    "sent": "The first coin as a P1.",
                    "label": 0
                },
                {
                    "sent": "Undertail using the first coin as one 1 -- B one.",
                    "label": 0
                },
                {
                    "sent": "And with the other coin, the problems of Serena tail is different.",
                    "label": 0
                },
                {
                    "sent": "Kind of different communities there to buy P2 and you can.",
                    "label": 0
                },
                {
                    "sent": "From a time to time you can either.",
                    "label": 0
                },
                {
                    "sent": "A change from one point to another, or use the same point for the.",
                    "label": 0
                },
                {
                    "sent": "So this is represented here.",
                    "label": 0
                },
                {
                    "sent": "If you are using the first coin.",
                    "label": 0
                },
                {
                    "sent": "An A in the next time you will be you will use the same coin with probability A1, one with probability A12.",
                    "label": 0
                },
                {
                    "sent": "You want change from going one 2.2 and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And assist with this probability song with this Troy ilities.",
                    "label": 0
                },
                {
                    "sent": "All the model is properly defined and is fully defined.",
                    "label": 0
                },
                {
                    "sent": "You don't need more OK.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, if we want to generalize this that we consider the the more general case or the hidden Markov model you can find here the model.",
                    "label": 0
                },
                {
                    "sent": "So we denote as the hidden state sequence of length T capital T and they are capital Y possible stay.",
                    "label": 1
                },
                {
                    "sent": "I consider them or one general case in which the observation.",
                    "label": 0
                },
                {
                    "sent": "RA.",
                    "label": 0
                },
                {
                    "sent": "Real Bachelor of Hunting was a vector sequence and the sequence is obviously from Y one and Y capital T. The model also is defined by the state transition probabilities, that is, the probability that we defined here.",
                    "label": 1
                },
                {
                    "sent": "That is, the probability of you are at instant T in a state.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 0
                },
                {
                    "sent": "Ay, ay ay convert percent probability, and then the next install time, and you will be at Stage AE an Additionally to assist ratio estate transition probabilities.",
                    "label": 1
                },
                {
                    "sent": "You have also the observation emission probabilities, that is, the probability that relate South with the observation.",
                    "label": 0
                },
                {
                    "sent": "What in this case?",
                    "label": 0
                },
                {
                    "sent": "This observation and this initial probability depend or are dependent on the state of the chain.",
                    "label": 0
                },
                {
                    "sent": "There is that you have a lot of parimeter.",
                    "label": 0
                },
                {
                    "sent": "And you have as much parameter at this state is capital Y.",
                    "label": 0
                },
                {
                    "sent": "In the case of the observation are discrete and it is with an alphabet of, for example J, single day observation.",
                    "label": 1
                },
                {
                    "sent": "Emission probabilities can be put aside matching of size capital I capital J.",
                    "label": 1
                },
                {
                    "sent": "Also, you need the initial state probability definition.",
                    "label": 0
                },
                {
                    "sent": "Bye.",
                    "label": 0
                },
                {
                    "sent": "All these parameters of the model AB Empire with another feature is involved.",
                    "label": 0
                },
                {
                    "sent": "I mean OK. Well.",
                    "label": 0
                },
                {
                    "sent": "This model.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think you think enough huge amount of different application.",
                    "label": 0
                },
                {
                    "sent": "Perhaps one of the first application was automatic experience and privilege.",
                    "label": 0
                },
                {
                    "sent": "In that application, the state S correspond to phoneme or war and why their service sequence to feature a structure for Mystic cinema.",
                    "label": 0
                },
                {
                    "sent": "Could be there Mail, a coefficient or energy?",
                    "label": 0
                },
                {
                    "sent": "Whatever, whatever model use or you have another possible application is activity recognition, in which as the state will respond to the activity that is subjective performance.",
                    "label": 0
                },
                {
                    "sent": "That person is performing that is changing time to time.",
                    "label": 0
                },
                {
                    "sent": "And why correspond to the feature structure from video sequence, for example, or from a combination of sensor in a sentence or a separate one more into the?",
                    "label": 1
                },
                {
                    "sent": "Copy of this IPM.",
                    "label": 0
                },
                {
                    "sent": "The application of a hidden Markov model in gravitational biology, it help also it has also while a area and you can find something for example in the program of game streaming, filing in which South correspond to the location of the gene.",
                    "label": 1
                },
                {
                    "sent": "An AY to the DNA negative.",
                    "label": 0
                },
                {
                    "sent": "Offer a sample to the problem or protein sequence alignment with S corresponded state can be binary is says if the if this observation corresponding matching lighting consists of sequence.",
                    "label": 0
                },
                {
                    "sent": "And why is the sequence of amino acid.",
                    "label": 0
                },
                {
                    "sent": "There is much more different applications.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After presenting the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the model.",
                    "label": 0
                },
                {
                    "sent": "Basically we have to play with that model we got.",
                    "label": 0
                },
                {
                    "sent": "We can consider.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different inference problem regarding to hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "The 1st and most simple problem.",
                    "label": 0
                },
                {
                    "sent": "Is simple given the observation and the model parameter is to determine the posterior of the offset of the observation or the probability of the observation, method or likeness or not.",
                    "label": 0
                },
                {
                    "sent": "The problem with this for calculating this likelihood is that for calculating it, basically you have to take into account that the dependencies through the hidden estate.",
                    "label": 0
                },
                {
                    "sent": "So one of the things that you can do is to fill in the joint PDF of the observation on their state and then marginalized over the state.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that if the number of spaces, capital Y and a length of the sequence capital T, the number of possible sequence is a capital I capital T, there is an exponential growth with the length of the signal.",
                    "label": 0
                },
                {
                    "sent": "That is a huge amount so.",
                    "label": 0
                },
                {
                    "sent": "We need to simplify this problem.",
                    "label": 0
                },
                {
                    "sent": "One way of simplifying is if you are able to calculate now, then join PDF of their service equal respect to the whole.",
                    "label": 0
                },
                {
                    "sent": "Hidden sequence, but just the last sequence to the lost a state.",
                    "label": 0
                },
                {
                    "sent": "One thing that you can do.",
                    "label": 0
                },
                {
                    "sent": "Is that if you only have to marginalized over the possible state that is, this is only assume of capital Y there.",
                    "label": 0
                },
                {
                    "sent": "And there is an algorithm we will present shortly that is the forward algorithm that allows you to compute this quantity in order of AI to 2 * T. So there is a we have changed from exponential complexity, depends exponentially on the length of the sequence to exponential complexity that the length that depends linearly with the length of the sequence.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the first problem.",
                    "label": 0
                },
                {
                    "sent": "More open problem is OK is not to determine only the likelihood, but there may the Altima.",
                    "label": 0
                },
                {
                    "sent": "But they they win, which you can define the optimal Earth can be different than there is different ways of defining this optimum.",
                    "label": 0
                },
                {
                    "sent": "For example, the one possibility is to determine the posterior probability of the state at instant T. Even all the observation.",
                    "label": 0
                },
                {
                    "sent": "Now select the maximum perform at maximum posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, OK this is performed with forward backward right.",
                    "label": 0
                },
                {
                    "sent": "Or you can directly want to select the maximum likelihood sequence.",
                    "label": 0
                },
                {
                    "sent": "Let's say the sequence that maximize this like little, and they can be obtained by the material they reordering this slightly different on the forward backward algorithm that is used to determine the posterior probability of the sequence.",
                    "label": 0
                },
                {
                    "sent": "But it has the same order of magnitude.",
                    "label": 0
                },
                {
                    "sent": "In fact, the algorithm is only 1/2 of the computational environment of the forward backward algorithm, because in the former, by what you have to perform.",
                    "label": 0
                },
                {
                    "sent": "Two buses for forward and one backward and will give it a real going.",
                    "label": 0
                },
                {
                    "sent": "You only need to perform one path forward pass and the complexity of each class is similar.",
                    "label": 0
                },
                {
                    "sent": "The only difference is that in the forward backward perform some and in the Viterbi algorithm to perform the substitute is some for maximum.",
                    "label": 0
                },
                {
                    "sent": "But the hardest problem, the hardest problem, all this problem.",
                    "label": 0
                },
                {
                    "sent": "Ask install in their vinning of the 70s.",
                    "label": 0
                },
                {
                    "sent": "The first problem was solved.",
                    "label": 0
                },
                {
                    "sent": "Focus on.",
                    "label": 0
                },
                {
                    "sent": "But there's a problem that remain in against the hardest.",
                    "label": 0
                },
                {
                    "sent": "One is to determine the model parameter that maximize the likelihood or the posterior of the state will begin on slide to this problem.",
                    "label": 0
                },
                {
                    "sent": "After that first we will go to treat jointly.",
                    "label": 0
                },
                {
                    "sent": "The forward and backward album.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Single slide.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The forward backward algorithm is the algorithm that allows you to calculate the posterior of the state given the observation.",
                    "label": 0
                },
                {
                    "sent": "I removed the dependency of the model parameters for pricing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is color gamati.",
                    "label": 0
                },
                {
                    "sent": "OK, and for doing this we can decompose first is to transform this conditional PDF into the joint divided by the marginal of the of the observation and after that will be by the observation in two part.",
                    "label": 0
                },
                {
                    "sent": "There is a considering that the given the state.",
                    "label": 0
                },
                {
                    "sent": "Instant DA, they all sample from C + 1 to the end of the sequence.",
                    "label": 0
                },
                {
                    "sent": "Capital TR, conditionally independent to the.",
                    "label": 0
                },
                {
                    "sent": "Also, the sequence from Eastern want Winston T, so we divide this.",
                    "label": 0
                },
                {
                    "sent": "We will calculate this in two different past.",
                    "label": 0
                },
                {
                    "sent": "This part in which you want to estimate the joint probability of the State instant tea.",
                    "label": 0
                },
                {
                    "sent": "Given all the observation up, listen, think you will calculate and using the forward incursion and the other one.",
                    "label": 0
                },
                {
                    "sent": "This is the probability of the likelihood of the observation for instant plus one descend on the sequence even the state of instant tea is calculated using the backward in question.",
                    "label": 0
                },
                {
                    "sent": "The forward equation is obtained simply by operating this.",
                    "label": 0
                },
                {
                    "sent": "Probability, and you can.",
                    "label": 0
                },
                {
                    "sent": "They can both, or basically, if you're interested, I can do in the black bar, but well, it is.",
                    "label": 0
                },
                {
                    "sent": "Depends on your.",
                    "label": 0
                },
                {
                    "sent": "So basically the operation is a very simple is that a Jew?",
                    "label": 0
                },
                {
                    "sent": "You express this probability of the probability of a white from one to TST&S T -- 1 marginalized over S T -- 1.",
                    "label": 0
                },
                {
                    "sent": "As you play a little bit with the equation as you obtain this recursion.",
                    "label": 0
                },
                {
                    "sent": "Therefore, the expression of this one that is covered the Alpha respect to the previous, the Alpha in the previous instance.",
                    "label": 0
                },
                {
                    "sent": "So you can recursively first to calculate all their fault of the initial Alpha.",
                    "label": 0
                },
                {
                    "sent": "That is simply the initial probability of the chain multiplied by the likelihood of the first observation, and then recursively to calculate all the office.",
                    "label": 0
                },
                {
                    "sent": "Remember that this Alpha, the full Alpha, can be a star noninteractive arena match with in a metric in which you can.",
                    "label": 0
                },
                {
                    "sent": "A you have to restore the Alpha value for each possible value of the state Y from one to capital Y from all the time instead of from one to capital T. And you can do a similar thing with.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For calculating the beta.",
                    "label": 0
                },
                {
                    "sent": "You obtain the bug or recursion.",
                    "label": 0
                },
                {
                    "sent": "With a similar transformation you you consider the bagger question.",
                    "label": 0
                },
                {
                    "sent": "That is, the probability of AY T + 1 + 1 is capital T + 1.",
                    "label": 0
                },
                {
                    "sent": "It doesn't exist, so you assume an uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "It is well, it is not normalized, but it doesn't need to be normalized.",
                    "label": 0
                },
                {
                    "sent": "And after that you proceed from T Bob.",
                    "label": 0
                },
                {
                    "sent": "Work until the initial state using this reflection.",
                    "label": 0
                },
                {
                    "sent": "Basically you propagate the bidders this probability using the a state transition matrix.",
                    "label": 0
                },
                {
                    "sent": "Are weighted by the likelihood of their service segment.",
                    "label": 0
                },
                {
                    "sent": "So at the end for calculating the.",
                    "label": 0
                },
                {
                    "sent": "Beta on the gamma.",
                    "label": 0
                },
                {
                    "sent": "You only have to perform forward pass and about where bus and follow this on.",
                    "label": 0
                },
                {
                    "sent": "Combining this descent Alpha you obtain the gamma.",
                    "label": 0
                },
                {
                    "sent": "While there is a hard part, the hard part is to obtain the margin of the observation.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is not.",
                    "label": 0
                },
                {
                    "sent": "This is not a problem because you know that this is a probability.",
                    "label": 0
                },
                {
                    "sent": "So you simply have to normalize among wild indigo.",
                    "label": 0
                },
                {
                    "sent": "OK, and this normalization factor is the inverse of the marginal, so is Additionally is easy way for calculating the marginal on the full of certain sequence.",
                    "label": 0
                },
                {
                    "sent": "OK, so clear.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's go now to the third inference problem.",
                    "label": 1
                },
                {
                    "sent": "That is the hardest one.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Basically you have.",
                    "label": 0
                },
                {
                    "sent": "In most of the cases, you don't have a single sequence.",
                    "label": 0
                },
                {
                    "sent": "You have a bunch of SQL.",
                    "label": 0
                },
                {
                    "sent": "Let's consider that you have and different SQL.",
                    "label": 1
                },
                {
                    "sent": "And the first thing that you can do is to express the joint distribution of SRY.",
                    "label": 0
                },
                {
                    "sent": "And that is in that form, so it can be the compose.",
                    "label": 0
                },
                {
                    "sent": "You can assume that the end sequence are independent, so you can.",
                    "label": 0
                },
                {
                    "sent": "There is a joint PDF.",
                    "label": 0
                },
                {
                    "sent": "Is the problem of the let's say this is the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, at the PDF of the estate you propagate from AS 1 to S capital.",
                    "label": 0
                },
                {
                    "sent": "TS1 is a private set.",
                    "label": 0
                },
                {
                    "sent": "The initial initial probabilities, and this is given by the transition probability matrix and multiply by the likelihood of the of the observation.",
                    "label": 0
                },
                {
                    "sent": "Why OK?",
                    "label": 0
                },
                {
                    "sent": "Basically they have looked at.",
                    "label": 0
                },
                {
                    "sent": "The problem here is that you only have the sequence Y 7 sequence, but you don't have no idea about what it always is.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The first method for solve this inference problem is to apply EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "Let's say is to consider.",
                    "label": 0
                },
                {
                    "sent": "As with, they argue hidden variable and performing an expectation maximization algorithm.",
                    "label": 0
                },
                {
                    "sent": "In fact it was proposed but bound.",
                    "label": 0
                },
                {
                    "sent": "In the beginning of the 70s, before this algorithm was called AM.",
                    "label": 0
                },
                {
                    "sent": "It is a also known as about Welch algorithm, and it is also known in the communication literature at the base ejr because there is other guys that propose in the beginning of the 70s this for decoding convolutional codes.",
                    "label": 0
                },
                {
                    "sent": "So this is one thing with the bound wells method is fully a maximum likelihood that, let's say do try to maximize this likely wouldn't even stick test.",
                    "label": 0
                },
                {
                    "sent": "But you can also follow a Bayesian approach to the inference of the model, and you have basically the two different ways of doing Bashan.",
                    "label": 0
                },
                {
                    "sent": "Friends.",
                    "label": 1
                },
                {
                    "sent": "One is to use MCMC method.",
                    "label": 0
                },
                {
                    "sent": "And the simplest one is the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "The force gives sampler for the.",
                    "label": 0
                },
                {
                    "sent": "A SMN.",
                    "label": 0
                },
                {
                    "sent": "My knowledge is likely sample web.",
                    "label": 0
                },
                {
                    "sent": "In a 1993.",
                    "label": 0
                },
                {
                    "sent": "You can also use the variational Bayes was first proposed by Mikey in a 90 day one.",
                    "label": 0
                },
                {
                    "sent": "It is seven 1997.",
                    "label": 0
                },
                {
                    "sent": "So let's process first with the bottom well child point, and then let's I give you a flavor or whole are the base an inference and I will follow.",
                    "label": 0
                },
                {
                    "sent": "Mainly they go.",
                    "label": 0
                },
                {
                    "sent": "I will explain mainly focused on this website because it is the one that will be also used.",
                    "label": 0
                },
                {
                    "sent": "For the basin and permitting.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "The formula can be argued, beware, but OK. Local match well probably will be there and you have not seen during the past week so many Sigma Sandpiper Capital five as in this reply but OK this is not bad.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily about.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a very simple.",
                    "label": 0
                },
                {
                    "sent": "This is the joint distribution of some wire like in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "I just stay.",
                    "label": 0
                },
                {
                    "sent": "The The law of this PDF for calculating the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK and.",
                    "label": 0
                },
                {
                    "sent": "Assuming all her, I will try to you have to express.",
                    "label": 0
                },
                {
                    "sent": "Sample the probability of a.",
                    "label": 0
                },
                {
                    "sent": "The transition from one state to another, even all the observation for doing this.",
                    "label": 0
                },
                {
                    "sent": "I will use this indication indicator function.",
                    "label": 0
                },
                {
                    "sent": "This indicator function will be one if a given all the observation.",
                    "label": 0
                },
                {
                    "sent": "The first state is the state I. OK, and so I transfer this term into this and by doing this I perform a summation over the whole state.",
                    "label": 0
                },
                {
                    "sent": "This transition probability there is present here.",
                    "label": 0
                },
                {
                    "sent": "I'm Esis press as the indicator function of doing a transition for from State Y. Este J awaited by the lowering of the transition probability matrix.",
                    "label": 0
                },
                {
                    "sent": "And this term.",
                    "label": 0
                },
                {
                    "sent": "Corresponds to this one, and if we rearrange an if instead of ordering first by the number of sequence, we rearranged all these symmetries and the first one is respect to this state.",
                    "label": 0
                },
                {
                    "sent": "We'll think this further.",
                    "label": 0
                },
                {
                    "sent": "OK, using this formula we are able now to perform the expectation and straighten mathematician instead of being an algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this formula is the same here.",
                    "label": 0
                },
                {
                    "sent": "OK, and I have to perform the aspect asianna step.",
                    "label": 0
                },
                {
                    "sent": "So first is the assist as expectation of the state.",
                    "label": 0
                },
                {
                    "sent": "The initial state.",
                    "label": 0
                },
                {
                    "sent": "Is not a statement instant one, given or taken value of Y?",
                    "label": 0
                },
                {
                    "sent": "This is actually when when you take this dictation, this is exactly the gamma.",
                    "label": 0
                },
                {
                    "sent": "The posterior probability of the initial state given all the observation.",
                    "label": 0
                },
                {
                    "sent": "And this part is they correspond to the state transition.",
                    "label": 0
                },
                {
                    "sent": "When you perform this a spectation, well, you need to define a new quantity that is called to a slight margin up.",
                    "label": 0
                },
                {
                    "sent": "The seeds of an entity that has a 2A in it for a instant E is exactly the approach the joint probability given the whole sequence of B at instant T at the same J at instant T -- 1.",
                    "label": 0
                },
                {
                    "sent": "Understand why?",
                    "label": 0
                },
                {
                    "sent": "Remember that the government are they are the conditional and kind of stress.",
                    "label": 0
                },
                {
                    "sent": "Big can be expressed also in the term of the Alpha, beta and the.",
                    "label": 0
                },
                {
                    "sent": "A. I think transition probability and the likelihood of the observation.",
                    "label": 0
                },
                {
                    "sent": "And the last one that is the likelihood of the observation is also dependent on one day, So you have all the separate units for all the information that you need for performing the aspect Tatian step of the.",
                    "label": 0
                },
                {
                    "sent": "After that you have to maximize, so remember that.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You have the sum of the gammas in the in the initial state, so you.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do they given this a gamma?",
                    "label": 0
                },
                {
                    "sent": "The the maximum likelihood estimation of the probability is simply.",
                    "label": 0
                },
                {
                    "sent": "This gamma is normalized.",
                    "label": 0
                },
                {
                    "sent": "Also, there is a maximum likelihood estimation of the state transition element AI, so IA is simply the normalized.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then normalize so of the truth lies margin.",
                    "label": 0
                },
                {
                    "sent": "And if you consider Gaussian emission probabilities is like this, just like estimation of the mean and variance of Gaussian of a Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But remember that is a Gaussian for each state and all the observation are weighted by the posterior probability of the state, even the whole.",
                    "label": 0
                },
                {
                    "sent": "Observation for the rest is rather simple.",
                    "label": 0
                },
                {
                    "sent": "Even the if the expression are, you can see that less complicated.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This is the classical one.",
                    "label": 0
                },
                {
                    "sent": "The inbound works and it can be modified a slightly different way is to perform based based on inference for HMM.",
                    "label": 0
                },
                {
                    "sent": "So for performing based on important, first you have to define private.",
                    "label": 0
                },
                {
                    "sent": "You have to have the prior over the parimeter and remember that the parameter are the state transition matrix.",
                    "label": 0
                },
                {
                    "sent": "The emission probability parameter.",
                    "label": 0
                },
                {
                    "sent": "Kill it still.",
                    "label": 0
                },
                {
                    "sent": "OK, so the most number one prior is a for the state transition.",
                    "label": 0
                },
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "A is to assume independent rogue.",
                    "label": 0
                },
                {
                    "sent": "Remember that this is each row represent the probability from going from one given a state to all there.",
                    "label": 0
                },
                {
                    "sent": "So this is a probability and you can use our prior at the distribution.",
                    "label": 0
                },
                {
                    "sent": "For the rest, I'm mainly for the observation.",
                    "label": 0
                },
                {
                    "sent": "If possible, you can use a conjugate prior because you know you have to for calculating the posterior you have to perform some approximation OK, But there is a lot of literature that gives you in the case your prior doesn't fit or it doesn't.",
                    "label": 0
                },
                {
                    "sent": "Is a conjugate prior you can perform also a certain cost, but easily so.",
                    "label": 0
                },
                {
                    "sent": "If you are discrete observation.",
                    "label": 0
                },
                {
                    "sent": "A prior that gives you a conjugate prior could be also do the distribution, and if you're a observation, the prior of your observational are normal or Gaussian.",
                    "label": 0
                },
                {
                    "sent": "One of the conjugate prior is to use normal for the mean animal wishe for the mattress.",
                    "label": 0
                },
                {
                    "sent": "For the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So when you have said this prior after that, you have to perform.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The inference that so in different method.",
                    "label": 0
                },
                {
                    "sent": "Basically you have two option.",
                    "label": 0
                },
                {
                    "sent": "As I have said.",
                    "label": 0
                },
                {
                    "sent": "What is the example I don't know?",
                    "label": 0
                },
                {
                    "sent": "Are you familiar with the gift sampler or you know what is a gift sampler and the rest of them?",
                    "label": 0
                },
                {
                    "sent": "OK, for the rest, well I give sampler is some in the simplest MCMC method.",
                    "label": 0
                },
                {
                    "sent": "Respect to the all the variable that you have to imperil to infant.",
                    "label": 0
                },
                {
                    "sent": "Basically a do calculate the posterior.",
                    "label": 0
                },
                {
                    "sent": "Of this, of each of the quantity given in the rest, and you run or obtain a sample.",
                    "label": 0
                },
                {
                    "sent": "And you do this sequentially with all your parameters.",
                    "label": 0
                },
                {
                    "sent": "I'm doing it interactively.",
                    "label": 0
                },
                {
                    "sent": "You obtain the solution, but remember that the the Infra Smith the Basin method.",
                    "label": 0
                },
                {
                    "sent": "Either you use a GIF sampler or operational base.",
                    "label": 0
                },
                {
                    "sent": "They even don't send a boy that problem of the bound and works algorithm is that they have local maximum into likelihood of they have local Maxima into the into the posterior.",
                    "label": 0
                },
                {
                    "sent": "So for opinion, the posterior of each parameter given the rest it is you have to obtain the posterior of AST given the observed sequence.",
                    "label": 0
                },
                {
                    "sent": "And the rest of the state.",
                    "label": 1
                },
                {
                    "sent": "Does this from time is not one to one time instead of capital T?",
                    "label": 0
                },
                {
                    "sent": "And if also you have to calculate the posterior of a given the rest that the only condition that you need is to respect to S and is very simple because you have only up to count for doing this and you have to calculate the posterior build given the rest.",
                    "label": 0
                },
                {
                    "sent": "But depends only.",
                    "label": 0
                },
                {
                    "sent": "Or why it S and is independent of the of the rest of para meter and also pick the most challenging part here.",
                    "label": 0
                },
                {
                    "sent": "Maybe if you use a conjugate prior for the observation.",
                    "label": 0
                },
                {
                    "sent": "Is the calculation of the posterior of each of their stay at a given instant T even the rest of the parameter in pulling the rest of the state?",
                    "label": 0
                },
                {
                    "sent": "The rest of the time itself, but this can be obtained easily or computationally cheap using an algorithm that is similar to the forward backward algorithm and is it is called the forward filtering backwards sampling.",
                    "label": 0
                },
                {
                    "sent": "It is basically because you have to perform forward forward path that in mind the Alpha.",
                    "label": 0
                },
                {
                    "sent": "And after that in the power path, instead of being a. Posterior probability you just something and it is a well you can.",
                    "label": 0
                },
                {
                    "sent": "It is fully explained in this in this book.",
                    "label": 0
                },
                {
                    "sent": "OK, and it is computationally simple if you don't have this algorithm, they forfeit embark on something.",
                    "label": 0
                },
                {
                    "sent": "This is the more challenging part.",
                    "label": 0
                },
                {
                    "sent": "OK. One alternative approaches to use operational base when they say you construct the evidence lower bound.",
                    "label": 1
                },
                {
                    "sent": "This sort of able.",
                    "label": 1
                },
                {
                    "sent": "That is, assuming independence among the parimeter.",
                    "label": 0
                },
                {
                    "sent": "Let's say you assume that the.",
                    "label": 0
                },
                {
                    "sent": "This distribution.",
                    "label": 0
                },
                {
                    "sent": "Or in this distribution?",
                    "label": 0
                },
                {
                    "sent": "A there is a the.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah you assume that the A it doesn't depend on as they be in this independent.",
                    "label": 0
                },
                {
                    "sent": "Why on earth and is independent and you perform an algorithm that is similar to OEM and you obtain also compares to the solution.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions to here.",
                    "label": 0
                },
                {
                    "sent": "OK, those are the classic elemental.",
                    "label": 0
                },
                {
                    "sent": "You can find them in any textbook.",
                    "label": 0
                },
                {
                    "sent": "A there you can modify if you are not able to find exactly the model you are looking for in a textbook or in an article, you can modify your model as you can modify the algorithm is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We want, oh we will do it for different cases.",
                    "label": 0
                },
                {
                    "sent": "Imagine for example and this is.",
                    "label": 0
                },
                {
                    "sent": "You can find this this example also in any textbook.",
                    "label": 0
                },
                {
                    "sent": "A that you can instead of using a Gaussian emission probability your service and are more complex than this.",
                    "label": 0
                },
                {
                    "sent": "You want to model as a Gaussian mixture model again.",
                    "label": 0
                },
                {
                    "sent": "OK so.",
                    "label": 0
                },
                {
                    "sent": "It is very easy.",
                    "label": 0
                },
                {
                    "sent": "The only thing you have to do is OK here you have in a importable the things that change this respect to the Gaussian page.",
                    "label": 0
                },
                {
                    "sent": "So here you have a mix of K component.",
                    "label": 0
                },
                {
                    "sent": "So when you calculate this indicator function you have this step a probability you have some.",
                    "label": 0
                },
                {
                    "sent": "Over K here over the component for performing the eastep, you only have to do the same thing as a like in the previous one.",
                    "label": 0
                },
                {
                    "sent": "That is exactly exactly is obtaining the.",
                    "label": 0
                },
                {
                    "sent": "The gamma that we obtained previously but multiply or weighted by the.",
                    "label": 0
                },
                {
                    "sent": "A probability of each component of a combination probability of their of their of image.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For performing the end step.",
                    "label": 0
                },
                {
                    "sent": "For maximizing, that is exactly the same.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "Except that like here, remember that, let's say it is here.",
                    "label": 0
                },
                {
                    "sent": "You can have the gamma.",
                    "label": 0
                },
                {
                    "sent": "Then there are sort of the forward backward, but they're different things here.",
                    "label": 0
                },
                {
                    "sent": "Is that you think a different sequence for a mixture components and its mixture component is weighted by the likelihood of this component is like instead of having now metrics for representing all the gamma.",
                    "label": 0
                },
                {
                    "sent": "There is a of the size of capital YY Capital T you have now capital y * K by the number of components and these are capital costs on.",
                    "label": 0
                },
                {
                    "sent": "You can or you can rent in a tensor if you if you like.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm guessing there when you have obtained in this way to gamma weighted posterior respect to the each component of the of the of the mixer, the thing you have to do is exactly the same the same as previously as a using the Gaussian observation, but now using the new yoga thing.",
                    "label": 0
                },
                {
                    "sent": "I'm not weighted by the likelihood of each component of the of the optimization.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple modification, but you can also modify.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model.",
                    "label": 0
                },
                {
                    "sent": "For example, for including label.",
                    "label": 0
                },
                {
                    "sent": "As I said previously, they are the HMM is an unsupervised method for modeling the observation.",
                    "label": 0
                },
                {
                    "sent": "But sometimes you have or you want to use HTML so classifier to classify things.",
                    "label": 0
                },
                {
                    "sent": "So one naive way of doing this there is some more elaborate ways is considered, for example that they a label of each sample corresponding oh are also a given by the state.",
                    "label": 0
                },
                {
                    "sent": "And you have a legal emission probabilities, but usually you can.",
                    "label": 0
                },
                {
                    "sent": "You can specify previously.",
                    "label": 0
                },
                {
                    "sent": "Let's say for example that if you have for example, two different label binary label OK and you have for it, for example 5 estate.",
                    "label": 0
                },
                {
                    "sent": "You can specify matrix D. You can specify which state model each observation.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is the label sequence, so you can use the same empower.",
                    "label": 0
                },
                {
                    "sent": "The only thing that appear is a new observation.",
                    "label": 0
                },
                {
                    "sent": "The language of the label F so.",
                    "label": 0
                },
                {
                    "sent": "It has a.",
                    "label": 0
                },
                {
                    "sent": "This observation is independent condition on the state.",
                    "label": 0
                },
                {
                    "sent": "So when you consider the full PDF of this table selection on the label, it's only multiplying.",
                    "label": 0
                },
                {
                    "sent": "Also buy a term that is the probability of the label given the state.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can introduce this.",
                    "label": 0
                },
                {
                    "sent": "Into the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "The thing is that the new term here I do have the dependency with the label here, here and here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this is the Alpha and the beta of the former board of the power forward.",
                    "label": 0
                },
                {
                    "sent": "Backward algorithm is almost the same except that.",
                    "label": 0
                },
                {
                    "sent": "When performing the forward step, when Angie, multiplied by the transition.",
                    "label": 0
                },
                {
                    "sent": "Let's say it is here.",
                    "label": 0
                },
                {
                    "sent": "And if you suppress the fire in green, do you think the Origonal 1?",
                    "label": 0
                },
                {
                    "sent": "So the only thing that changed is that after performing the summation of the alphas in the previous sign instant given by the state transition probability, proficient is multiplied by the like it.",
                    "label": 0
                },
                {
                    "sent": "But the likelihood of the observation at the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Of delay.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly the same for the thing in the beta.",
                    "label": 0
                },
                {
                    "sent": "So it's only adding a parameter.",
                    "label": 0
                },
                {
                    "sent": "Here, and you obtain a new Alpha and beta.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Beautiful.",
                    "label": 0
                },
                {
                    "sent": "You in the game as your painting to a slice marginal so exactly the same, it's just at that range you have the likelihood you multiply that since also the likelihood the obvious by the likelihood of the label of.",
                    "label": 0
                },
                {
                    "sent": "Remember that if this light include is a one or zero, it means that some part of the sequence are used to train or to determine the perimeter.",
                    "label": 0
                },
                {
                    "sent": "Certain estate in order to over is like finally you obtain.",
                    "label": 0
                },
                {
                    "sent": "I still don't see a magic that is a block diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey, this is for training, but for the operation you don't have the label.",
                    "label": 0
                },
                {
                    "sent": "But you can obtain the label as a function of the state of the hidden estate.",
                    "label": 0
                },
                {
                    "sent": "But it is a pretty easy to consider or to remove the label.",
                    "label": 0
                },
                {
                    "sent": "Becausw is simply that if you don't have the label.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the previous step, you don't have to put the don't have to put this step.",
                    "label": 0
                },
                {
                    "sent": "So for running the algorithm for determining the estimation of the posterior probability of the of the state, you don't have to consider the level.",
                    "label": 0
                },
                {
                    "sent": "You do exactly the same action without delay.",
                    "label": 0
                },
                {
                    "sent": "But you have called it the information about the labeling to this state into the station, systematic and into the initial product.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "As you can see.",
                    "label": 0
                },
                {
                    "sent": "Dealing with missing values in HTML is pretty easy.",
                    "label": 0
                },
                {
                    "sent": "It is basically is that when you have to perform.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The forward or the backward step.",
                    "label": 0
                },
                {
                    "sent": "There is a you don't have information about this.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "This is simply one.",
                    "label": 0
                },
                {
                    "sent": "For all the possible states.",
                    "label": 0
                },
                {
                    "sent": "OK, so there is not like other infant algorithms problem.",
                    "label": 0
                },
                {
                    "sent": "When you have missing values with even more communities.",
                    "label": 0
                },
                {
                    "sent": "3D sequel DLC.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I will be leaving soon.",
                    "label": 0
                },
                {
                    "sent": "Probably yes at the priority and you can even.",
                    "label": 0
                },
                {
                    "sent": "Complicated.",
                    "label": 0
                },
                {
                    "sent": "More your model and imagine, for example that.",
                    "label": 0
                },
                {
                    "sent": "You're a.",
                    "label": 0
                },
                {
                    "sent": "Your observation of you model survey shun is very simple or simply that OK you can.",
                    "label": 0
                },
                {
                    "sent": "You want to express by the stage one different knowledge on label or where you want but you have another short 10 dependencies.",
                    "label": 0
                },
                {
                    "sent": "And and this certain dependencies, for example, can be included in the observation.",
                    "label": 1
                },
                {
                    "sent": "And you can define for.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample and autoregressive hidden.",
                    "label": 0
                },
                {
                    "sent": "So you can complicate further than normal.",
                    "label": 0
                },
                {
                    "sent": "So this is also is pretty easy to perform saying you have to slightly modify the forward backward algorithm in order to include the prediction.",
                    "label": 0
                },
                {
                    "sent": "Basically first you have to perform the prediction and after that you have to calculate the likelihood but.",
                    "label": 0
                },
                {
                    "sent": "Using also OEM algorithm, here is the form that adopt the expectation of step.",
                    "label": 0
                },
                {
                    "sent": "Add and as a result of the optimization step you could obtain or do obtain the covariance matrix that it is the magic that you need to design the linear regressor from a YT minus 12 white.",
                    "label": 0
                },
                {
                    "sent": "You can complete it, or you can make model as complicated as you need it.",
                    "label": 0
                },
                {
                    "sent": "And you have you don't have to deal with many.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "God of other algorithm like a heater model and but you have to.",
                    "label": 0
                },
                {
                    "sent": "The only thing you have to do is to rewrite the equation according to your model and you obtain version in another.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Organization of the HMM there.",
                    "label": 0
                },
                {
                    "sent": "For example a hidden semi Markov model OK?",
                    "label": 0
                },
                {
                    "sent": "The Markov model is very simple, but as several limitation one of them is that as you have on a state transition matrix.",
                    "label": 0
                },
                {
                    "sent": "A big time.",
                    "label": 0
                },
                {
                    "sent": "The probability of staying in a given state.",
                    "label": 0
                },
                {
                    "sent": "Indicate exponential it is just cause the state transition probability independent.",
                    "label": 0
                },
                {
                    "sent": "Nothing in just a parameter.",
                    "label": 0
                },
                {
                    "sent": "This could be not or could not fit exactly your data, so you can do a hidden semi Markov model in which the state state transition probability depends not only.",
                    "label": 0
                },
                {
                    "sent": "Or not even number but depends also on the day of the time you stay in this in this state.",
                    "label": 0
                },
                {
                    "sent": "So it's a slight modification.",
                    "label": 0
                },
                {
                    "sent": "Or you can have an.",
                    "label": 0
                },
                {
                    "sent": "Also, an input is like the control input to stay the same model.",
                    "label": 0
                },
                {
                    "sent": "And you have also the in front of it or for example, instead of making the observation more and more complex, you can do a hierarchical hmm, let's say in which you define top level and its state it developed in a lower level as another full achievement defining hierarchically or.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "You can make also factorial hmm.",
                    "label": 1
                },
                {
                    "sent": "A factorial hmm is simple and simply when the number of the state is very very big, difference becomes more and more complicated.",
                    "label": 0
                },
                {
                    "sent": "So one thing that you can do that you can do is to factorize this thing.",
                    "label": 0
                },
                {
                    "sent": "Let's say for example in not hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "Let's say for example that you are trying to model for example that.",
                    "label": 0
                },
                {
                    "sent": "Evolution of a disease.",
                    "label": 0
                },
                {
                    "sent": "OK, and there is change in the disease and you can you want for example, to code into the state all the information about the the dictation even patient or something.",
                    "label": 0
                },
                {
                    "sent": "It's a or is, gender, etc.",
                    "label": 0
                },
                {
                    "sent": "But if you if you want to call more and more information.",
                    "label": 0
                },
                {
                    "sent": "The number of the state role is one of the bigger, the other value other whatever.",
                    "label": 0
                },
                {
                    "sent": "So one thing that you can do is to define independent change for each of the parameter for the eight, for the ginger etc.",
                    "label": 0
                },
                {
                    "sent": "They join it all together into a single emission probability.",
                    "label": 0
                },
                {
                    "sent": "Or it's an emission probability that depends on all the change.",
                    "label": 0
                },
                {
                    "sent": "These correspond of this is a factorial hmm?",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "Or there is another situation in which a you have more than one hmm but they are not fully independent and you are able to model the dependencies among them.",
                    "label": 0
                },
                {
                    "sent": "This is called a couple engineer.",
                    "label": 0
                },
                {
                    "sent": "There is also improving.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions you know.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Function.",
                    "label": 0
                },
                {
                    "sent": "Medication are set up.",
                    "label": 0
                },
                {
                    "sent": "What happens if not is to form simple, tractable or?",
                    "label": 0
                },
                {
                    "sent": "As a truly impeccable uof change language press half you are not, or you're a you don't control the final solution.",
                    "label": 0
                },
                {
                    "sent": "So for example, if each of the images that you have discrete observation OK, well you have this kind of operation into the label.",
                    "label": 0
                },
                {
                    "sent": "Let's say imagine that all the use of case in which is, which in which?",
                    "label": 0
                },
                {
                    "sent": "It's wrong, you have our address here.",
                    "label": 0
                },
                {
                    "sent": "But aside and state for legal.",
                    "label": 0
                },
                {
                    "sent": "OK, but imagine that you have a distribution or that Rd that is not one of the results.",
                    "label": 0
                },
                {
                    "sent": "You have some certainty about this state, or you cannot infer the label given the state you have a posterior probability.",
                    "label": 0
                },
                {
                    "sent": "Office State of the little story of the label given this state.",
                    "label": 0
                },
                {
                    "sent": "But not a single one, but you can do it.",
                    "label": 0
                },
                {
                    "sent": "And then you can define.",
                    "label": 0
                },
                {
                    "sent": "Semi supervised training.",
                    "label": 0
                },
                {
                    "sent": "In which you are a, you are sure about certain labels.",
                    "label": 0
                },
                {
                    "sent": "You are not sure about, certainly, but you can have both.",
                    "label": 0
                },
                {
                    "sent": "The uncertainty that you have overdue label.",
                    "label": 0
                },
                {
                    "sent": "And this gives you a model English.",
                    "label": 0
                },
                {
                    "sent": "Finally, you obtain the posterior probability of the label given the state.",
                    "label": 0
                },
                {
                    "sent": "OK. Yeah.",
                    "label": 0
                },
                {
                    "sent": "Welcome back even sending Michael.",
                    "label": 0
                },
                {
                    "sent": "No, I just.",
                    "label": 0
                },
                {
                    "sent": "Understand the students semi Markov model.",
                    "label": 0
                },
                {
                    "sent": "So what is the same market model?",
                    "label": 0
                },
                {
                    "sent": "This city market.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "It was not useful for OK. And if you have an ignition probability, very sticks right there parameter, what are the 8?",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "The time that you stay in a given state decay exponentially with time.",
                    "label": 0
                },
                {
                    "sent": "But the your your reality.",
                    "label": 0
                },
                {
                    "sent": "But your data and it doesn't fit with that model because there is.",
                    "label": 0
                },
                {
                    "sent": "For example, you're saying that to the same state.",
                    "label": 0
                },
                {
                    "sent": "For a very long time are you changed for another state?",
                    "label": 0
                },
                {
                    "sent": "And in one state you are.",
                    "label": 0
                },
                {
                    "sent": "Imagine, for example, that one state is represent an established a situation, another a taxation.",
                    "label": 0
                },
                {
                    "sent": "That ain't the time that you remain, or the probability of that you remain the given state using the standard model is the same, But the game exponentially with time.",
                    "label": 0
                },
                {
                    "sent": "But making the transition probability.",
                    "label": 0
                },
                {
                    "sent": "Major depending on the state.",
                    "label": 0
                },
                {
                    "sent": "You can also modify this.",
                    "label": 0
                },
                {
                    "sent": "OK. More question.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's go to the.",
                    "label": 0
                },
                {
                    "sent": "Final part.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, as I said.",
                    "label": 0
                },
                {
                    "sent": "The HMM, apart from the limitation of the model that can be seen or etc, they have set down.",
                    "label": 0
                },
                {
                    "sent": "Problem that is 101.",
                    "label": 0
                },
                {
                    "sent": "One of them is the model selection.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you are not sure about the number of hidden estate.",
                    "label": 0
                },
                {
                    "sent": "And you have to do a model selection instead, OK?",
                    "label": 0
                },
                {
                    "sent": "A the thing that is usually do is OK, you use your favorite complexity measure.",
                    "label": 1
                },
                {
                    "sent": "You use the peak of the account information pretty regularly.",
                    "label": 0
                },
                {
                    "sent": "Description length or a the.",
                    "label": 0
                },
                {
                    "sent": "Log likelihood of day of the convention.",
                    "label": 0
                },
                {
                    "sent": "Whatever criteria an annual include, like in some of them, some penalty term for complexity and basically went in for the most common solution is that you train different hmm for different value of capital Y of the hidden state.",
                    "label": 0
                },
                {
                    "sent": "Can you apply this measure?",
                    "label": 0
                },
                {
                    "sent": "I just select the proper way.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is too expensive an the best thing to do probably is not to do a proper model and model selection to our model.",
                    "label": 0
                },
                {
                    "sent": "Leveraging or to fit the model to your training to your training data.",
                    "label": 0
                },
                {
                    "sent": "So our solution is using.",
                    "label": 0
                },
                {
                    "sent": "Number use in Bayesian parametric technique until define the circle infinite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "The agent, the infinite hidden Markov model, is that assume that the number of states initially is initially in.",
                    "label": 0
                },
                {
                    "sent": "But you will.",
                    "label": 0
                },
                {
                    "sent": "They use or you will assign transition probabilities different from zero to one state according to your data.",
                    "label": 0
                },
                {
                    "sent": "OK, so in that case you don't have to do a proper model selection.",
                    "label": 0
                },
                {
                    "sent": "And this is one of the problem and I will go after that.",
                    "label": 0
                },
                {
                    "sent": "So some detail of this.",
                    "label": 1
                },
                {
                    "sent": "The infinite hidden Markov model.",
                    "label": 0
                },
                {
                    "sent": "And the other is the local maximum of the likelihoods.",
                    "label": 0
                },
                {
                    "sent": "Let's say a. Aiden if you use a log likelihood algorithm to fit the model like the bottom world salary, or if you use a Bayesian techniques.",
                    "label": 0
                },
                {
                    "sent": "Due to the presence of the hidden estate, you likely could have fixed.",
                    "label": 0
                },
                {
                    "sent": "So you can be a star, you local maximum of this like.",
                    "label": 0
                },
                {
                    "sent": "There is no way in principle that assure you that you are in the global maximum of the like, so they're useful strategy is to reinitialize.",
                    "label": 0
                },
                {
                    "sent": "A autorun they there are several times.",
                    "label": 0
                },
                {
                    "sent": "And according to your criteria, could be the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Respect to that, you select the back, but recently they are asking proposed.",
                    "label": 0
                },
                {
                    "sent": "Method that avoids this problem and if they said on a moment matching are useful.",
                    "label": 0
                },
                {
                    "sent": "Respect on learning of hmm.",
                    "label": 0
                },
                {
                    "sent": "So will first today.",
                    "label": 0
                },
                {
                    "sent": "Infinity the marker model an after something to the spectral length and of hidden.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "For a building with the infinite hidden Markov model, let's go first in more detail into the Bayesian solution.",
                    "label": 1
                },
                {
                    "sent": "For the third in current problem.",
                    "label": 0
                },
                {
                    "sent": "OK so I said previously in basically method you have to put a prior and you have to define or you have to calculate the posterior of the parameter.",
                    "label": 0
                },
                {
                    "sent": "OK so for example if you have based on that email with discrete observation and a single sequence and you define a supplier.",
                    "label": 0
                },
                {
                    "sent": "I did let's pray on a.",
                    "label": 0
                },
                {
                    "sent": "For the role of the transition matrix and each transition metric to be independent, OK, and you can use for example.",
                    "label": 0
                },
                {
                    "sent": "So you can use this symmetric dinner.",
                    "label": 0
                },
                {
                    "sent": "Let this is Alpha divided by the number of the state by vector one.",
                    "label": 0
                },
                {
                    "sent": "Symmetric.",
                    "label": 0
                },
                {
                    "sent": "For the Salvation, if you have more information about the state of the State representative of the relationship between the state and the.",
                    "label": 0
                },
                {
                    "sent": "Observation you can use this knowledge to coordinate a sub vector parameter D in order to define dizziness prior for the growth of the beamer.",
                    "label": 0
                },
                {
                    "sent": "The observation map.",
                    "label": 0
                },
                {
                    "sent": "So one thing that you have to do is.",
                    "label": 0
                },
                {
                    "sent": "And when you have, for example using the, give some.",
                    "label": 0
                },
                {
                    "sent": "Let's say in the sampler to sample sequentially all the variable that you have.",
                    "label": 0
                },
                {
                    "sent": "So you have to calculate the posterior.",
                    "label": 0
                },
                {
                    "sent": "Then you have to calculate the posterior of the each of the roles of the transition metric given the rest of the parameter, and they each of the roles of the selection metric given the rest of the family.",
                    "label": 0
                },
                {
                    "sent": "So basically the things that you do is a you have.",
                    "label": 0
                },
                {
                    "sent": "A. I stay sequence.",
                    "label": 0
                },
                {
                    "sent": "You simply count the number of transition for and stay white with AJ.",
                    "label": 0
                },
                {
                    "sent": "This is NYA.",
                    "label": 0
                },
                {
                    "sent": "Are you far with this object?",
                    "label": 0
                },
                {
                    "sent": "I do the same respect to the observation.",
                    "label": 0
                },
                {
                    "sent": "I do form the vector A.",
                    "label": 0
                },
                {
                    "sent": "And after that, well, that the prior is conjugate to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So the posterior is also added.",
                    "label": 0
                },
                {
                    "sent": "It's left.",
                    "label": 0
                },
                {
                    "sent": "This is that reflect this.",
                    "label": 0
                },
                {
                    "sent": "The empirical distribution plus the price.",
                    "label": 0
                },
                {
                    "sent": "Obviously if you have more than more than the prior has less influence because this is not normalized OK, and the and the same exactly the same respect to the observation logics.",
                    "label": 0
                },
                {
                    "sent": "And one thing that you got there that you can do is using a gift sampler.",
                    "label": 0
                },
                {
                    "sent": "Samples from the distribution and it's pretty easy.",
                    "label": 0
                },
                {
                    "sent": "OK, but going from.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is through the infinite hidden Markov model the first thing that you can do?",
                    "label": 1
                },
                {
                    "sent": "Is to consider a number of a state that is infinite.",
                    "label": 0
                },
                {
                    "sent": "OK, after that OK will we?",
                    "label": 0
                },
                {
                    "sent": "Will I need to define what is on a stick breaking process?",
                    "label": 0
                },
                {
                    "sent": "There is something related with an English process.",
                    "label": 0
                },
                {
                    "sent": "OK, that is OK. You for you are a obtaining a distribution.",
                    "label": 0
                },
                {
                    "sent": "It's title given one single parameter, so in this single parameter gamma tell you is how to divide the interval between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And it is in the 1st.",
                    "label": 0
                },
                {
                    "sent": "And the first interval is proportional to the.",
                    "label": 0
                },
                {
                    "sent": "Is given by bit beta distribution with parameter cannot so proportional to be so the second one is 1 minus.",
                    "label": 0
                },
                {
                    "sent": "The portion you have ascended to the first one, and you're a Jew, and multiply it by a new breed of parameters and you continue until Infinity and this is the this is like the division of of.",
                    "label": 0
                },
                {
                    "sent": "Of the interval between zero and one in pieces, each time a smaller and smaller.",
                    "label": 0
                },
                {
                    "sent": "And this gives you the basis for defining a distribution over Infinite State.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "Respect to the model, but you have to change from the Bashan.",
                    "label": 0
                },
                {
                    "sent": "Hmm, is that OK?",
                    "label": 0
                },
                {
                    "sent": "Observation would remain the same.",
                    "label": 0
                },
                {
                    "sent": "OK, but here.",
                    "label": 0
                },
                {
                    "sent": "There is a hierarchical model in which the prior for your A for each of the role of the transition matrix is also on a stick breaking process that depends.",
                    "label": 0
                },
                {
                    "sent": "And another variable, but this is on a stick breaking process.",
                    "label": 0
                },
                {
                    "sent": "Why do you need this hierarchical definition?",
                    "label": 0
                },
                {
                    "sent": "Do you know this hierarchical definition becausw?",
                    "label": 0
                },
                {
                    "sent": "All all of the roof of the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "Ask to have was to have the same primer and to have the number of a occupied state.",
                    "label": 0
                },
                {
                    "sent": "The state with English transition probability that estate is greater than 0 is not new.",
                    "label": 0
                },
                {
                    "sent": "OK, this format junior hierarchical data that process.",
                    "label": 0
                },
                {
                    "sent": "For example, if this is not in one situation in which you don't need this hierarchical during that process.",
                    "label": 0
                },
                {
                    "sent": "Is that instead of performing inference over a hmm you to refine?",
                    "label": 0
                },
                {
                    "sent": "Submission model with an infinite number of components.",
                    "label": 0
                },
                {
                    "sent": "In that case, the prior of this component it don't depend on nothing.",
                    "label": 0
                },
                {
                    "sent": "So you don't need this hierarchical construction that you need here for the HMM, so after that you have defined, also calculate the positive.",
                    "label": 0
                },
                {
                    "sent": "So let's define capital K and the number of active state.",
                    "label": 0
                },
                {
                    "sent": "Let's say the number of the state in which there is a new probability of transition.",
                    "label": 0
                },
                {
                    "sent": "I will define.",
                    "label": 0
                },
                {
                    "sent": "A super high as the state transition probability to those states plus the K plus one is the sum of all of them.",
                    "label": 0
                },
                {
                    "sent": "That is principle is there.",
                    "label": 0
                },
                {
                    "sent": "OK, and a exactly respect to the asylum.",
                    "label": 0
                },
                {
                    "sent": "That is the distribution that define the fryer for all the rows of the transition matrix.",
                    "label": 0
                },
                {
                    "sent": "Define the first K and after that you make the sum of all of this.",
                    "label": 0
                },
                {
                    "sent": "So in order to resample, it is easier to obtain the.",
                    "label": 0
                },
                {
                    "sent": "A the posterior of the state transition probability.",
                    "label": 0
                },
                {
                    "sent": "This is is like.",
                    "label": 0
                },
                {
                    "sent": "Previous one OK.",
                    "label": 0
                },
                {
                    "sent": "Math here.",
                    "label": 0
                },
                {
                    "sent": "You have the fryer.",
                    "label": 0
                },
                {
                    "sent": "That is, this part will be playing by Alpha.",
                    "label": 0
                },
                {
                    "sent": "Their empirical distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so the B is exactly the same.",
                    "label": 0
                },
                {
                    "sent": "The most complicated part is to obtain the posterior.",
                    "label": 0
                },
                {
                    "sent": "All the excitement of the distribution that define the prior of each row of the transition match.",
                    "label": 0
                },
                {
                    "sent": "OK, and this is a all this is.",
                    "label": 0
                },
                {
                    "sent": "Can be interpreted as a model.",
                    "label": 0
                },
                {
                    "sent": "They have different interpretation.",
                    "label": 0
                },
                {
                    "sent": "One of them is to use him on hierarchical, polio, polio, polio.",
                    "label": 0
                },
                {
                    "sent": "The things perform like this OK is a.",
                    "label": 0
                },
                {
                    "sent": "Think about this, so maybe that's a.",
                    "label": 0
                },
                {
                    "sent": "Here, here there is a non low probability.",
                    "label": 0
                },
                {
                    "sent": "That there is some transition to state K plus one.",
                    "label": 0
                },
                {
                    "sent": "So in that in that case Joe is like if you do another resample.",
                    "label": 0
                },
                {
                    "sent": "Liver sample is exactly the same of your sample.",
                    "label": 0
                },
                {
                    "sent": "The number of.",
                    "label": 0
                },
                {
                    "sent": "Fiction, but you have from the state.",
                    "label": 0
                },
                {
                    "sent": "While twisting J.",
                    "label": 0
                },
                {
                    "sent": "Exactly is like you for each one of those transitions, you flip a coin, flip a coin in which you have.",
                    "label": 0
                },
                {
                    "sent": "A probability or is there no need of parameter Alpha multiplied by excellent J OK?",
                    "label": 0
                },
                {
                    "sent": "So of accepting or not?",
                    "label": 0
                },
                {
                    "sent": "Each one of those problems.",
                    "label": 0
                },
                {
                    "sent": "So with this you obtain this new code 0.",
                    "label": 0
                },
                {
                    "sent": "I do know this thing.",
                    "label": 0
                },
                {
                    "sent": "So you come all of this.",
                    "label": 0
                },
                {
                    "sent": "If you perform a similar operation like this, but with this resample transition.",
                    "label": 0
                },
                {
                    "sent": "With the resampled transition you construct.",
                    "label": 0
                },
                {
                    "sent": "They vector see that they say the empirical distribution of each of the transition and gamma parameter they have parameter is the parameter that control the board of Annual state of the currency of Nu Estate.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you put a high value on gamma, the number of a occupied state grows very fast.",
                    "label": 0
                },
                {
                    "sent": "As does the number of training samples.",
                    "label": 0
                },
                {
                    "sent": "If the parameter gamma is very low.",
                    "label": 0
                },
                {
                    "sent": "The number of hidden estate grow slowly.",
                    "label": 0
                },
                {
                    "sent": "With George trend sample, so you think this?",
                    "label": 0
                },
                {
                    "sent": "Probably to see you perform an audio thing.",
                    "label": 0
                },
                {
                    "sent": "The posterior of the side of the define the prior over.",
                    "label": 0
                },
                {
                    "sent": "It's a role that transition match.",
                    "label": 0
                },
                {
                    "sent": "OK, the difficult part here.",
                    "label": 0
                },
                {
                    "sent": "Over to the classical give sampler for the HMM is that you cannot use here in the forward 50 backwards sampling algorithm.",
                    "label": 0
                },
                {
                    "sent": "So sampling into the hidden state.",
                    "label": 0
                },
                {
                    "sent": "Could be expensive and it is a difficult.",
                    "label": 0
                },
                {
                    "sent": "And if you don't consider the dependency number mistake, then beings of the chain.",
                    "label": 0
                },
                {
                    "sent": "It will be very, very slow, so there is a plethora of different algorithms for performing inference here.",
                    "label": 0
                },
                {
                    "sent": "I think one of the simplest.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More useful is.",
                    "label": 0
                },
                {
                    "sent": "A the the one proposed by Jurgen Bangle is to use our being sampling.",
                    "label": 0
                },
                {
                    "sent": "There being something is just.",
                    "label": 0
                },
                {
                    "sent": "Use an aux variable that make the number of the state field you have the numbers of dating.",
                    "label": 0
                },
                {
                    "sent": "You can use the former featuring black or something.",
                    "label": 0
                },
                {
                    "sent": "OK. And so there is a different a lot of variation respect to this.",
                    "label": 0
                },
                {
                    "sent": "For example, you can also many use nonparametric model for the emission probabilities.",
                    "label": 0
                },
                {
                    "sent": "Emily talks us out of work on this on this part.",
                    "label": 0
                },
                {
                    "sent": "Hold to determine the order of the number of the mixture component for the observation probabilities.",
                    "label": 0
                },
                {
                    "sent": "The order of the fine.",
                    "label": 0
                },
                {
                    "sent": "Auto racing, observation, model etc.",
                    "label": 0
                },
                {
                    "sent": "So that you can communicate.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the last thing I don't know this.",
                    "label": 0
                },
                {
                    "sent": "Yes, running lights.",
                    "label": 0
                },
                {
                    "sent": "OK, well this is a. I will explain how funny well I did not give no give all the details OK and there is this this technique of it as being proposed some years ago the 1st Paper of hearing the Lucas on a something like this.",
                    "label": 0
                },
                {
                    "sent": "The thing is that they use a different model for representing the.",
                    "label": 0
                },
                {
                    "sent": "A operation on the for the.",
                    "label": 0
                },
                {
                    "sent": "This is a new observation model.",
                    "label": 0
                },
                {
                    "sent": "This observation model was taking from automata theory and etc.",
                    "label": 0
                },
                {
                    "sent": "So basically the things that you do is you use.",
                    "label": 0
                },
                {
                    "sent": "You perform the same marginalization OK, but using a product of matrices?",
                    "label": 0
                },
                {
                    "sent": "And you are not able to determine the true one, but you are able to determine.",
                    "label": 0
                },
                {
                    "sent": "An affine transformation of this, and this is fitted by calculating by estimating the moment the probability of a given state.",
                    "label": 0
                },
                {
                    "sent": "The probability of the transition, and this tensor there.",
                    "label": 0
                },
                {
                    "sent": "Is there a probability of going from one state to another, and 12 is the 3rd order moment and you see you the compose this matrix using an SVD.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I look up find the equivalent.",
                    "label": 0
                },
                {
                    "sent": "The nice message is that this method, compared to the rest of the method interest no local minima.",
                    "label": 0
                },
                {
                    "sent": "But it's performed well only if you have a low out of sample, because it's a moment that can clean it.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's all.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}