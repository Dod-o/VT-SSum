{
    "id": "lufymbfsbuhsk3jgonqjctfnco4pb2ed",
    "title": "Generalization Error under Covariate Shift Input-Dependent Estimation of Generalization Error under Covariate Shift",
    "info": {
        "author": [
            "Klaus-Robert M\u00fcller, Department of Software Engineering and Theoretical Computer Science, Technische Universit\u00e4t Berlin"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "October 2005",
        "category": [
            "Top->Computer Science->Machine Learning->Statistical Learning"
        ]
    },
    "url": "http://videolectures.net/mcslw04_muller_geucs/",
    "segmentation": [
        [
            "Think it's the third time here or fourth time.",
            "I don't remember quite.",
            "So the.",
            "Some disclaimer in the beginning, so when thinking what to talk about here, I decided to talk about model selection from a rather classical perspective.",
            "But when thinking about what talking, I also thought it would be nice to to also make some connections to more recent work that that I'm interested in.",
            "And sometimes you start on one end, which is very theoretical, which is this work and you end up at something being very practical which will be brain computer interfacing in the end.",
            "So you can be.",
            "I hope you're now curious what I'm going to talk about.",
            "So this is joint work with Masashi Sugiyama, who should actually be the guy presenting this, because it's mostly his work.",
            "The part, so we have we have for 10.",
            "For 10 years I'm interested in model selection and.",
            "The first pieces of work I've done together with with Amari and with the School of University of Tokyo.",
            "And then since maybe six or seven years I've been working closely together with massage, so Yama.",
            "These two guys that is color, dot and product Pradeep Shenoy.",
            "He's originally from University of Washington right now visiting.",
            "They have been doing the BCI work.",
            "OK, so.",
            "First, a very general gentle introduction to modern selection problems.",
            "Then the covariate shift problem.",
            "So for those of you who are not familiar with this expression, covariate shift by that I mean the training and the test distribution come from are not the same.",
            "OK then we come to some applications, regression, brain, computer interfacing and I conclude.",
            "OK, so in the regression problem.",
            "We know we all know that, so there's some training examples.",
            "And so X is and wise.",
            "And there's some noise and we would like to to learn a function F hat.",
            "And we hope to be approximating the true function in some way as good as possible.",
            "For example, if we use a linear regression model, so we have P basis functions in some space.",
            "Um?",
            "And this would be.",
            "They could be linear functions.",
            "They could be RBF, so whatever you wish, then the Ridge regression scenario is as you see here.",
            "We would like to estimate these parameter values Alpha.",
            "That are hidden in this function, and we would also like to.",
            "Trade off.",
            "To some smoothness."
        ],
        [
            "And the accuracy of our prediction.",
            "The usual kind of regularization.",
            "So it's clear if we don't regularize at all."
        ],
        [
            "Uninstall too complex so the green function that we learn if we just regularize right.",
            "Then maybe it's like this and if you were too simple and it's not what we intend to do.",
            "So in the end.",
            "The choice of the model, which is in this sense here.",
            "The choice of the regularization parameter, Lambda.",
            "Very strongly influences the model that we learn.",
            "OK, so."
        ],
        [
            "So now if we talk about ideal Model selection then we would like to get the right model.",
            "Minimizes the generalization error, so if we would have access to the two generalization error, then we just do an Arg min.",
            "This function of Lambda and pick the right Lambda and that's it.",
            "So that would be our choice for the best model.",
            "Of course, we don't know the generalization error.",
            "Anne.",
            "So we cannot directly calculate it."
        ],
        [
            "Except right now I'm not wearing my base in head, so if we're basing then the world is different.",
            "So we're disclaiming the Bayesian part here.",
            "So.",
            "Now we.",
            "The next possible step is that we estimate this generalization error, and then we do an augment over this estimated generalization error.",
            "And of course we would like this function as to be as good and close to the true generalization error as possible."
        ],
        [
            "Now.",
            "There are some classical ways.",
            "And so in expectation, say the difference between this estimated generalization error and the true one.",
            "Goes to 0 or is zero and so to say this again, I would like to point on.",
            "On this here the expectation.",
            "And so where to say interested in the typical performance?",
            "So these two curves, the two generalization error and the estimated generalization error?",
            "They should not be too far apart.",
            "In expectation and again, the literature gives us archive this information criterion that has some asymptotic consistency and so on.",
            "So this list is very long.",
            "There's also another way of looking at that which.",
            "Says, well, let's upper bound the generalization error.",
            "Say in the VC spirit.",
            "Span bond concentration bonds and.",
            "You name it this many possibilities, and we've already heard people mention several of these possibilities that I've not on this slide.",
            "And in this case."
        ],
        [
            "We are interested in giving worst case bound on the true generalization error and hope that this is not not to lose."
        ],
        [
            "And of course by using entropy numbers, covering numbers, whatever fashionable and tighter methods we have, we can get closer and closer to the truth.",
            "So these are typical.",
            "Of ways to look at that.",
            "Now clearly we have to say what is the generalization measure?",
            "For example, we could take a square loss and integrate over it, and then this would be called risk.",
            "But we could also have the Qu\u00e9bec labeler divergent.",
            "This is just.",
            "I think this is well known to all of you.",
            "Anne."
        ],
        [
            "So.",
            "In the first option that I had on my slide, we might need a very large number of training examples.",
            "To come close to the truth, so this would be we would need some asymptotic exotic approximations, so which most likely doesn't work for few data points.",
            "So if I have a carcass information criterium, it works very well.",
            "If I have many data points.",
            "But if I have few of them, then it doesn't work well and it's clear that if you look at your data, there's no sign popping up that says asymptotic starts here.",
            "So therefore.",
            "Um, yeah we need.",
            "We would actually get like to get rid of this asymptotic point.",
            "The other point is that I would like to mention is.",
            "That typically we integrate over the probability distribution.",
            "And from that probability distribution, the training examples are drawn, but this cannot necessarily be used for transduction, so we might be much more interested in not.",
            "Just single points, so assessing generalization error for a single point.",
            "So."
        ],
        [
            "OK, and for what I'm going to talk about.",
            "Anne.",
            "We would like to get a quite accurate estimate of the generalization error for small or finite samples, so non asymptotically we would like to be able to estimate that inducts transaction errors at any point of our interest.",
            "So we do active learning then, so data point.",
            "And here we need our estimate.",
            "And we would like to make use of unlabeled samples.",
            "OK, so the way."
        ],
        [
            "We approach this and it's been also on slides.",
            "Of previous speakers is we have some Hilbert space, say functional Hilbert space, H and.",
            "We have some true function and some estimated function.",
            "Both are in this Hilbert space and we take the Hilbert.",
            "The difference of the Hilbert space norm difference between the.",
            "Function estimate in the true function.",
            "So if this is zero, then if it's a properly behaving Hilbert space then these guys should coincide."
        ],
        [
            "OK, so now I will make a short detour about.",
            "Um?",
            "Who the so called subspace information criterium that that was originally put forward by Masashi Sugiyama and Ogawa?",
            "And before I come to this covariate shift scenario?",
            "OK.",
            "So.",
            "Here's a so we're typically interested in some.",
            "In a typical performance, so we take the expectation over this Hilbert space Norm, but now this is very important and therefore it's written in green.",
            "We take the expectation over the noise.",
            "So.",
            "Um?",
            "It's not over the input data points, so this is a data dependent thing.",
            "And because we do it like that, taking the expectation of the noise, not of the data, we can actually do not assume anything about the training or test set distribution, or from the same probability distribution.",
            "So this is advantage in active learning and also advantages in cases where they come from different distributions.",
            "Now."
        ],
        [
            "Let's talk a little bit about this Hilbert space difference and do we do something very typical?",
            "Choose a bias variance decomposition.",
            "So this expectation in order to compute this, we have a bias term and the variance term and the question is how can we actually estimate them properly.",
            "And we start by.",
            "Um?",
            "Trying to estimate the bias.",
            "So there's always some cartoons down here that tried to give you an impression what, what the heck we want and what we are doing with these formulas and the variance here is the variance due to the noise, OK?"
        ],
        [
            "OK, so that is due to Sugiyama Agava.",
            "So.",
            "Imagine we have some linear operator Xu that gives an unbiased estimator F at U of FF.",
            "So this means taking this average with respect to noise.",
            "We get F. So that means that in this whole somber.",
            "The mean would be here.",
            "This equation says and this is just writing down this operator times.",
            "Why are the labels?",
            "Where is the noise because FF is based on your data, yes.",
            "So remember there was this this.",
            "Thing that that.",
            "That we have A5 plus epsilon OK. And so that is, you know.",
            "OK, so."
        ],
        [
            "So this expectation is the conditional access which you used.",
            "That's why exactly.",
            "Yeah.",
            "So I see that you are with me and so.",
            "And it's so the thing that we do now is we would like to get this estimate over the bias and for this we make some rough estimate and so we could just.",
            "Take the distance of this guy.",
            "And this guy instead of the distance between the means there will be some error in this.",
            "OK, so let's let's write this down.",
            "So this is the bias term.",
            "And.",
            "So this will now be F -- F U and basically this formula.",
            "And Interestingly this operator or this scalar product in expectation over the noise will vanish.",
            "And the Sigma would be just this.",
            "The variance with respect to the noise, OK?"
        ],
        [
            "So basically what we have is now we have.",
            "An distance that we can calculate.",
            "Because we have this F hat and then unbiased estimator.",
            "I have not been talking about how we get it, but that's another story.",
            "And that's that's basically our bias estimator, and that's the variance estimator, and so one can prove that this so called subspace information criteria is an unbiased estimator of the generalization error with finite samples.",
            "Anne.",
            "That can be computed like this.",
            "Are you estimating that yes, so so there's two things that we need to do?",
            "So getting the UN."
        ],
        [
            "Biased estimator and getting an estimate of the Sigma squared.",
            "But you know this is this can be done like."
        ],
        [
            "OK, now.",
            "Let's go to this covariate shift setting.",
            "OK. We will use similar geometric techniques and you will get to recognize some of the formulas.",
            "Now again.",
            "We have noise term, additional noise term, but the training, the training examples.",
            "They come from the different distribution than the test examples.",
            "And so.",
            "So the way we measure the generalization error is again written down here."
        ],
        [
            "OK.",
            "So typically these two distributions coincide, and here we say they don't coincide.",
            "And then the quest."
        ],
        [
            "Is whether it's.",
            "Sorry.",
            "Yes.",
            "That's why it.",
            "So for example, I mean this is a very simple example, so we have a regression problem.",
            "These blue dots are from the training distribution and the black dots are from the test distribution.",
            "So.",
            "Clearly the densities are different.",
            "And as we haven't seen these guys.",
            "So we could let me see.",
            "So so we could ask ourselves, is this a realistic scenario and I'm I would say yes.",
            "It's quite often a realistic scenario.",
            "It's the scenario of extrapolation or active learning.",
            "And in fact, if you think about it, it's also the situation of of imbalanced.",
            "And of classification with imbalanced data because you say for some some microarray data or some data from Ken's cancer patients you would have balanced training data set where you have equally many cancer and non cancer patients.",
            "But in real life you have maybe 1% cancer patients and 99% healthy.",
            "So if you do model selection so to say based on only the training set itself, you might make a very lousy model.",
            "OK, so if we."
        ],
        [
            "Do ordinary least squares fitting.",
            "Then say in this very simple example we have we fit a line OK and this is what we fit.",
            "And it's a very reasonable estimate for the data that is given.",
            "Of course, this is not a very good estimate if we want to extrapolate, we would rather have this one.",
            "So Interestingly, so this whole modeling ordinary least squares fitting is asymptotically unbiased.",
            "If the model is correct, of course, if the training and test distribution are different, then the model is not correct.",
            "So this is in fact asymptotically biased for such misspecified specified models.",
            "So that's one possibility."
        ],
        [
            "Get one possibility to get around in this least squares fitting.",
            "Which is taking a ratio, so we're just doing weighted.",
            "Squares fitting and for the waiting we use the weighting of the density of the training and the tests distribution.",
            "Before you were just using the random from the error.",
            "That's right.",
            "So so we so before we were not considering this term at all.",
            "And now we are.",
            "We are considering it assuming at the moment there these distributions are known, and of course later on they will not be known to us.",
            "But the interesting thing is that if we include this then we can also prove that this is asymptotically unbiased, even if the models are misspecified.",
            "But because the model is unspecified, so it doesn't make sense to talk about the bias or placement of the model, but you're talking about the estimate of the.",
            "Prediction error yes.",
            "So, so you're what I'm what I'm talking about is is if I'm doing extrapolation, even if the training and test set distribution are not the same, then I'm asymptotically getting the right model.",
            "Even though this training and test set distribution is not the same, using this kind of thing.",
            "Being the best model in your class, yes.",
            "Possibly interesting, important something that's I was actually going to mention this.",
            "It has some.",
            "I mean it has a lot of links, so important sampling is 1 possible link.",
            "In fact, if you look at old neural networks papers, there are several papers that try to deal with this unbalanced classification problem and basically what those people have done is is important sampling or some weighted waiting.",
            "Sorry once once.",
            "Yes, yes there are some.",
            "Maybe I just didn't, yes.",
            "If you end a line, that's right.",
            "No, it's.",
            "Trying to find the best linear models in it.",
            "What Alex said, what I would have answered as well is is that if you, if you're realizable then it's OK, I will.",
            "I mean, we will still in the beginning.",
            "So maybe one thing to mention is these estimators, although asymptotically unbiased, can have a large variance.",
            "And so this, in practice, this is quite bad, so there's ways to reduce."
        ],
        [
            "The variance.",
            "And this is also due to see modiolus and then we basically put a Lambda here and so in the end we are again at.",
            "Model selection.",
            "Now we put the Lambda.",
            "In the exponent.",
            "And again, we can see if we have no.",
            "If we have the ordinary least squares model, then we're back here.",
            "So that's the original model and basically this the appropriate choice of Lambda weights, the importance.",
            "But it's still we have to choose it appropriately and we have to find a proper measure to do that."
        ],
        [
            "Um?",
            "So it's clear that.",
            "So so imagine we do some for this simple example, we make a plot of the true generalization error as a function of Lambda down here, and we look at the cross validation error.",
            "That's the black line here.",
            "This is the true generalization error.",
            "The dashed line is the true generalization error, and this is the proposed estimator that we will have on the next couple of slides.",
            "So you see that I mean it's.",
            "Quite clear cross validation using only your training data won't help much.",
            "So."
        ],
        [
            "No.",
            "OK, we assume some IID noise with mean zero and variance.",
            "We have regressed.",
            "Typical linear regression models.",
            "We have this weighted Lambda weighted least squares, and now the Alpha determined.",
            "These alphas are determined by some.",
            "Matrix L, which is defined here and the Lambda is.",
            "Is as an exponent of this matrix.",
            "And the matrix D. Basically, if you have ordinary least squares, it's like that.",
            "And D is is just the ratio of these.",
            "Two probabilities and the diagonal of it.",
            "OK, so again at bias variance decomposition.",
            "Anne."
        ],
        [
            "OK, so here's the difference that we would like to estimate.",
            "This is quite accessible.",
            "The norm of the Sky.",
            "This is a concept which is ignored, but this one is interesting, so we would like to estimate that.",
            "And."
        ],
        [
            "For this we decompose the function F in.",
            "And that's this is a technique also that has been used in this subspace.",
            "Information criteria.",
            "We decompose it into a subspace.",
            "Where D lives.",
            "That is, in the span of these basis functions and some orthogonal part that is orthogonal to the basis functions.",
            "OK so F would be here.",
            "G is in this plane of the span of the basis functions and R is orthogonal to this.",
            "And then we can essentially.",
            "Compose set was."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think it's the third time here or fourth time.",
                    "label": 0
                },
                {
                    "sent": "I don't remember quite.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Some disclaimer in the beginning, so when thinking what to talk about here, I decided to talk about model selection from a rather classical perspective.",
                    "label": 0
                },
                {
                    "sent": "But when thinking about what talking, I also thought it would be nice to to also make some connections to more recent work that that I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "And sometimes you start on one end, which is very theoretical, which is this work and you end up at something being very practical which will be brain computer interfacing in the end.",
                    "label": 0
                },
                {
                    "sent": "So you can be.",
                    "label": 0
                },
                {
                    "sent": "I hope you're now curious what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with Masashi Sugiyama, who should actually be the guy presenting this, because it's mostly his work.",
                    "label": 0
                },
                {
                    "sent": "The part, so we have we have for 10.",
                    "label": 0
                },
                {
                    "sent": "For 10 years I'm interested in model selection and.",
                    "label": 0
                },
                {
                    "sent": "The first pieces of work I've done together with with Amari and with the School of University of Tokyo.",
                    "label": 0
                },
                {
                    "sent": "And then since maybe six or seven years I've been working closely together with massage, so Yama.",
                    "label": 0
                },
                {
                    "sent": "These two guys that is color, dot and product Pradeep Shenoy.",
                    "label": 1
                },
                {
                    "sent": "He's originally from University of Washington right now visiting.",
                    "label": 0
                },
                {
                    "sent": "They have been doing the BCI work.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "First, a very general gentle introduction to modern selection problems.",
                    "label": 0
                },
                {
                    "sent": "Then the covariate shift problem.",
                    "label": 0
                },
                {
                    "sent": "So for those of you who are not familiar with this expression, covariate shift by that I mean the training and the test distribution come from are not the same.",
                    "label": 0
                },
                {
                    "sent": "OK then we come to some applications, regression, brain, computer interfacing and I conclude.",
                    "label": 0
                },
                {
                    "sent": "OK, so in the regression problem.",
                    "label": 0
                },
                {
                    "sent": "We know we all know that, so there's some training examples.",
                    "label": 0
                },
                {
                    "sent": "And so X is and wise.",
                    "label": 0
                },
                {
                    "sent": "And there's some noise and we would like to to learn a function F hat.",
                    "label": 0
                },
                {
                    "sent": "And we hope to be approximating the true function in some way as good as possible.",
                    "label": 0
                },
                {
                    "sent": "For example, if we use a linear regression model, so we have P basis functions in some space.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And this would be.",
                    "label": 0
                },
                {
                    "sent": "They could be linear functions.",
                    "label": 0
                },
                {
                    "sent": "They could be RBF, so whatever you wish, then the Ridge regression scenario is as you see here.",
                    "label": 0
                },
                {
                    "sent": "We would like to estimate these parameter values Alpha.",
                    "label": 0
                },
                {
                    "sent": "That are hidden in this function, and we would also like to.",
                    "label": 0
                },
                {
                    "sent": "Trade off.",
                    "label": 0
                },
                {
                    "sent": "To some smoothness.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the accuracy of our prediction.",
                    "label": 0
                },
                {
                    "sent": "The usual kind of regularization.",
                    "label": 0
                },
                {
                    "sent": "So it's clear if we don't regularize at all.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uninstall too complex so the green function that we learn if we just regularize right.",
                    "label": 0
                },
                {
                    "sent": "Then maybe it's like this and if you were too simple and it's not what we intend to do.",
                    "label": 0
                },
                {
                    "sent": "So in the end.",
                    "label": 0
                },
                {
                    "sent": "The choice of the model, which is in this sense here.",
                    "label": 0
                },
                {
                    "sent": "The choice of the regularization parameter, Lambda.",
                    "label": 0
                },
                {
                    "sent": "Very strongly influences the model that we learn.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now if we talk about ideal Model selection then we would like to get the right model.",
                    "label": 0
                },
                {
                    "sent": "Minimizes the generalization error, so if we would have access to the two generalization error, then we just do an Arg min.",
                    "label": 0
                },
                {
                    "sent": "This function of Lambda and pick the right Lambda and that's it.",
                    "label": 0
                },
                {
                    "sent": "So that would be our choice for the best model.",
                    "label": 0
                },
                {
                    "sent": "Of course, we don't know the generalization error.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So we cannot directly calculate it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Except right now I'm not wearing my base in head, so if we're basing then the world is different.",
                    "label": 0
                },
                {
                    "sent": "So we're disclaiming the Bayesian part here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Now we.",
                    "label": 0
                },
                {
                    "sent": "The next possible step is that we estimate this generalization error, and then we do an augment over this estimated generalization error.",
                    "label": 0
                },
                {
                    "sent": "And of course we would like this function as to be as good and close to the true generalization error as possible.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "There are some classical ways.",
                    "label": 0
                },
                {
                    "sent": "And so in expectation, say the difference between this estimated generalization error and the true one.",
                    "label": 0
                },
                {
                    "sent": "Goes to 0 or is zero and so to say this again, I would like to point on.",
                    "label": 0
                },
                {
                    "sent": "On this here the expectation.",
                    "label": 0
                },
                {
                    "sent": "And so where to say interested in the typical performance?",
                    "label": 0
                },
                {
                    "sent": "So these two curves, the two generalization error and the estimated generalization error?",
                    "label": 0
                },
                {
                    "sent": "They should not be too far apart.",
                    "label": 0
                },
                {
                    "sent": "In expectation and again, the literature gives us archive this information criterion that has some asymptotic consistency and so on.",
                    "label": 0
                },
                {
                    "sent": "So this list is very long.",
                    "label": 0
                },
                {
                    "sent": "There's also another way of looking at that which.",
                    "label": 0
                },
                {
                    "sent": "Says, well, let's upper bound the generalization error.",
                    "label": 0
                },
                {
                    "sent": "Say in the VC spirit.",
                    "label": 0
                },
                {
                    "sent": "Span bond concentration bonds and.",
                    "label": 0
                },
                {
                    "sent": "You name it this many possibilities, and we've already heard people mention several of these possibilities that I've not on this slide.",
                    "label": 0
                },
                {
                    "sent": "And in this case.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We are interested in giving worst case bound on the true generalization error and hope that this is not not to lose.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And of course by using entropy numbers, covering numbers, whatever fashionable and tighter methods we have, we can get closer and closer to the truth.",
                    "label": 0
                },
                {
                    "sent": "So these are typical.",
                    "label": 0
                },
                {
                    "sent": "Of ways to look at that.",
                    "label": 0
                },
                {
                    "sent": "Now clearly we have to say what is the generalization measure?",
                    "label": 0
                },
                {
                    "sent": "For example, we could take a square loss and integrate over it, and then this would be called risk.",
                    "label": 0
                },
                {
                    "sent": "But we could also have the Qu\u00e9bec labeler divergent.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                },
                {
                    "sent": "I think this is well known to all of you.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the first option that I had on my slide, we might need a very large number of training examples.",
                    "label": 0
                },
                {
                    "sent": "To come close to the truth, so this would be we would need some asymptotic exotic approximations, so which most likely doesn't work for few data points.",
                    "label": 0
                },
                {
                    "sent": "So if I have a carcass information criterium, it works very well.",
                    "label": 0
                },
                {
                    "sent": "If I have many data points.",
                    "label": 0
                },
                {
                    "sent": "But if I have few of them, then it doesn't work well and it's clear that if you look at your data, there's no sign popping up that says asymptotic starts here.",
                    "label": 0
                },
                {
                    "sent": "So therefore.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah we need.",
                    "label": 0
                },
                {
                    "sent": "We would actually get like to get rid of this asymptotic point.",
                    "label": 0
                },
                {
                    "sent": "The other point is that I would like to mention is.",
                    "label": 0
                },
                {
                    "sent": "That typically we integrate over the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And from that probability distribution, the training examples are drawn, but this cannot necessarily be used for transduction, so we might be much more interested in not.",
                    "label": 0
                },
                {
                    "sent": "Just single points, so assessing generalization error for a single point.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and for what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We would like to get a quite accurate estimate of the generalization error for small or finite samples, so non asymptotically we would like to be able to estimate that inducts transaction errors at any point of our interest.",
                    "label": 0
                },
                {
                    "sent": "So we do active learning then, so data point.",
                    "label": 0
                },
                {
                    "sent": "And here we need our estimate.",
                    "label": 0
                },
                {
                    "sent": "And we would like to make use of unlabeled samples.",
                    "label": 0
                },
                {
                    "sent": "OK, so the way.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We approach this and it's been also on slides.",
                    "label": 0
                },
                {
                    "sent": "Of previous speakers is we have some Hilbert space, say functional Hilbert space, H and.",
                    "label": 0
                },
                {
                    "sent": "We have some true function and some estimated function.",
                    "label": 0
                },
                {
                    "sent": "Both are in this Hilbert space and we take the Hilbert.",
                    "label": 0
                },
                {
                    "sent": "The difference of the Hilbert space norm difference between the.",
                    "label": 0
                },
                {
                    "sent": "Function estimate in the true function.",
                    "label": 0
                },
                {
                    "sent": "So if this is zero, then if it's a properly behaving Hilbert space then these guys should coincide.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now I will make a short detour about.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Who the so called subspace information criterium that that was originally put forward by Masashi Sugiyama and Ogawa?",
                    "label": 0
                },
                {
                    "sent": "And before I come to this covariate shift scenario?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's a so we're typically interested in some.",
                    "label": 0
                },
                {
                    "sent": "In a typical performance, so we take the expectation over this Hilbert space Norm, but now this is very important and therefore it's written in green.",
                    "label": 0
                },
                {
                    "sent": "We take the expectation over the noise.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's not over the input data points, so this is a data dependent thing.",
                    "label": 0
                },
                {
                    "sent": "And because we do it like that, taking the expectation of the noise, not of the data, we can actually do not assume anything about the training or test set distribution, or from the same probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is advantage in active learning and also advantages in cases where they come from different distributions.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's talk a little bit about this Hilbert space difference and do we do something very typical?",
                    "label": 0
                },
                {
                    "sent": "Choose a bias variance decomposition.",
                    "label": 0
                },
                {
                    "sent": "So this expectation in order to compute this, we have a bias term and the variance term and the question is how can we actually estimate them properly.",
                    "label": 0
                },
                {
                    "sent": "And we start by.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Trying to estimate the bias.",
                    "label": 0
                },
                {
                    "sent": "So there's always some cartoons down here that tried to give you an impression what, what the heck we want and what we are doing with these formulas and the variance here is the variance due to the noise, OK?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that is due to Sugiyama Agava.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Imagine we have some linear operator Xu that gives an unbiased estimator F at U of FF.",
                    "label": 0
                },
                {
                    "sent": "So this means taking this average with respect to noise.",
                    "label": 0
                },
                {
                    "sent": "We get F. So that means that in this whole somber.",
                    "label": 0
                },
                {
                    "sent": "The mean would be here.",
                    "label": 0
                },
                {
                    "sent": "This equation says and this is just writing down this operator times.",
                    "label": 0
                },
                {
                    "sent": "Why are the labels?",
                    "label": 0
                },
                {
                    "sent": "Where is the noise because FF is based on your data, yes.",
                    "label": 0
                },
                {
                    "sent": "So remember there was this this.",
                    "label": 0
                },
                {
                    "sent": "Thing that that.",
                    "label": 0
                },
                {
                    "sent": "That we have A5 plus epsilon OK. And so that is, you know.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this expectation is the conditional access which you used.",
                    "label": 0
                },
                {
                    "sent": "That's why exactly.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I see that you are with me and so.",
                    "label": 0
                },
                {
                    "sent": "And it's so the thing that we do now is we would like to get this estimate over the bias and for this we make some rough estimate and so we could just.",
                    "label": 0
                },
                {
                    "sent": "Take the distance of this guy.",
                    "label": 0
                },
                {
                    "sent": "And this guy instead of the distance between the means there will be some error in this.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's let's write this down.",
                    "label": 0
                },
                {
                    "sent": "So this is the bias term.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So this will now be F -- F U and basically this formula.",
                    "label": 0
                },
                {
                    "sent": "And Interestingly this operator or this scalar product in expectation over the noise will vanish.",
                    "label": 0
                },
                {
                    "sent": "And the Sigma would be just this.",
                    "label": 0
                },
                {
                    "sent": "The variance with respect to the noise, OK?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically what we have is now we have.",
                    "label": 0
                },
                {
                    "sent": "An distance that we can calculate.",
                    "label": 0
                },
                {
                    "sent": "Because we have this F hat and then unbiased estimator.",
                    "label": 0
                },
                {
                    "sent": "I have not been talking about how we get it, but that's another story.",
                    "label": 0
                },
                {
                    "sent": "And that's that's basically our bias estimator, and that's the variance estimator, and so one can prove that this so called subspace information criteria is an unbiased estimator of the generalization error with finite samples.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "That can be computed like this.",
                    "label": 0
                },
                {
                    "sent": "Are you estimating that yes, so so there's two things that we need to do?",
                    "label": 0
                },
                {
                    "sent": "So getting the UN.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Biased estimator and getting an estimate of the Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "But you know this is this can be done like.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now.",
                    "label": 0
                },
                {
                    "sent": "Let's go to this covariate shift setting.",
                    "label": 0
                },
                {
                    "sent": "OK. We will use similar geometric techniques and you will get to recognize some of the formulas.",
                    "label": 0
                },
                {
                    "sent": "Now again.",
                    "label": 0
                },
                {
                    "sent": "We have noise term, additional noise term, but the training, the training examples.",
                    "label": 0
                },
                {
                    "sent": "They come from the different distribution than the test examples.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So the way we measure the generalization error is again written down here.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So typically these two distributions coincide, and here we say they don't coincide.",
                    "label": 0
                },
                {
                    "sent": "And then the quest.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is whether it's.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "That's why it.",
                    "label": 0
                },
                {
                    "sent": "So for example, I mean this is a very simple example, so we have a regression problem.",
                    "label": 0
                },
                {
                    "sent": "These blue dots are from the training distribution and the black dots are from the test distribution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Clearly the densities are different.",
                    "label": 0
                },
                {
                    "sent": "And as we haven't seen these guys.",
                    "label": 0
                },
                {
                    "sent": "So we could let me see.",
                    "label": 0
                },
                {
                    "sent": "So so we could ask ourselves, is this a realistic scenario and I'm I would say yes.",
                    "label": 0
                },
                {
                    "sent": "It's quite often a realistic scenario.",
                    "label": 0
                },
                {
                    "sent": "It's the scenario of extrapolation or active learning.",
                    "label": 0
                },
                {
                    "sent": "And in fact, if you think about it, it's also the situation of of imbalanced.",
                    "label": 0
                },
                {
                    "sent": "And of classification with imbalanced data because you say for some some microarray data or some data from Ken's cancer patients you would have balanced training data set where you have equally many cancer and non cancer patients.",
                    "label": 0
                },
                {
                    "sent": "But in real life you have maybe 1% cancer patients and 99% healthy.",
                    "label": 0
                },
                {
                    "sent": "So if you do model selection so to say based on only the training set itself, you might make a very lousy model.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do ordinary least squares fitting.",
                    "label": 0
                },
                {
                    "sent": "Then say in this very simple example we have we fit a line OK and this is what we fit.",
                    "label": 0
                },
                {
                    "sent": "And it's a very reasonable estimate for the data that is given.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is not a very good estimate if we want to extrapolate, we would rather have this one.",
                    "label": 0
                },
                {
                    "sent": "So Interestingly, so this whole modeling ordinary least squares fitting is asymptotically unbiased.",
                    "label": 0
                },
                {
                    "sent": "If the model is correct, of course, if the training and test distribution are different, then the model is not correct.",
                    "label": 0
                },
                {
                    "sent": "So this is in fact asymptotically biased for such misspecified specified models.",
                    "label": 0
                },
                {
                    "sent": "So that's one possibility.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get one possibility to get around in this least squares fitting.",
                    "label": 0
                },
                {
                    "sent": "Which is taking a ratio, so we're just doing weighted.",
                    "label": 0
                },
                {
                    "sent": "Squares fitting and for the waiting we use the weighting of the density of the training and the tests distribution.",
                    "label": 0
                },
                {
                    "sent": "Before you were just using the random from the error.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "So so we so before we were not considering this term at all.",
                    "label": 0
                },
                {
                    "sent": "And now we are.",
                    "label": 0
                },
                {
                    "sent": "We are considering it assuming at the moment there these distributions are known, and of course later on they will not be known to us.",
                    "label": 0
                },
                {
                    "sent": "But the interesting thing is that if we include this then we can also prove that this is asymptotically unbiased, even if the models are misspecified.",
                    "label": 0
                },
                {
                    "sent": "But because the model is unspecified, so it doesn't make sense to talk about the bias or placement of the model, but you're talking about the estimate of the.",
                    "label": 0
                },
                {
                    "sent": "Prediction error yes.",
                    "label": 0
                },
                {
                    "sent": "So, so you're what I'm what I'm talking about is is if I'm doing extrapolation, even if the training and test set distribution are not the same, then I'm asymptotically getting the right model.",
                    "label": 0
                },
                {
                    "sent": "Even though this training and test set distribution is not the same, using this kind of thing.",
                    "label": 0
                },
                {
                    "sent": "Being the best model in your class, yes.",
                    "label": 0
                },
                {
                    "sent": "Possibly interesting, important something that's I was actually going to mention this.",
                    "label": 0
                },
                {
                    "sent": "It has some.",
                    "label": 0
                },
                {
                    "sent": "I mean it has a lot of links, so important sampling is 1 possible link.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you look at old neural networks papers, there are several papers that try to deal with this unbalanced classification problem and basically what those people have done is is important sampling or some weighted waiting.",
                    "label": 0
                },
                {
                    "sent": "Sorry once once.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes there are some.",
                    "label": 0
                },
                {
                    "sent": "Maybe I just didn't, yes.",
                    "label": 0
                },
                {
                    "sent": "If you end a line, that's right.",
                    "label": 0
                },
                {
                    "sent": "No, it's.",
                    "label": 0
                },
                {
                    "sent": "Trying to find the best linear models in it.",
                    "label": 0
                },
                {
                    "sent": "What Alex said, what I would have answered as well is is that if you, if you're realizable then it's OK, I will.",
                    "label": 0
                },
                {
                    "sent": "I mean, we will still in the beginning.",
                    "label": 0
                },
                {
                    "sent": "So maybe one thing to mention is these estimators, although asymptotically unbiased, can have a large variance.",
                    "label": 0
                },
                {
                    "sent": "And so this, in practice, this is quite bad, so there's ways to reduce.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The variance.",
                    "label": 0
                },
                {
                    "sent": "And this is also due to see modiolus and then we basically put a Lambda here and so in the end we are again at.",
                    "label": 0
                },
                {
                    "sent": "Model selection.",
                    "label": 0
                },
                {
                    "sent": "Now we put the Lambda.",
                    "label": 0
                },
                {
                    "sent": "In the exponent.",
                    "label": 0
                },
                {
                    "sent": "And again, we can see if we have no.",
                    "label": 0
                },
                {
                    "sent": "If we have the ordinary least squares model, then we're back here.",
                    "label": 0
                },
                {
                    "sent": "So that's the original model and basically this the appropriate choice of Lambda weights, the importance.",
                    "label": 0
                },
                {
                    "sent": "But it's still we have to choose it appropriately and we have to find a proper measure to do that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So it's clear that.",
                    "label": 0
                },
                {
                    "sent": "So so imagine we do some for this simple example, we make a plot of the true generalization error as a function of Lambda down here, and we look at the cross validation error.",
                    "label": 0
                },
                {
                    "sent": "That's the black line here.",
                    "label": 0
                },
                {
                    "sent": "This is the true generalization error.",
                    "label": 0
                },
                {
                    "sent": "The dashed line is the true generalization error, and this is the proposed estimator that we will have on the next couple of slides.",
                    "label": 0
                },
                {
                    "sent": "So you see that I mean it's.",
                    "label": 0
                },
                {
                    "sent": "Quite clear cross validation using only your training data won't help much.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "OK, we assume some IID noise with mean zero and variance.",
                    "label": 0
                },
                {
                    "sent": "We have regressed.",
                    "label": 0
                },
                {
                    "sent": "Typical linear regression models.",
                    "label": 0
                },
                {
                    "sent": "We have this weighted Lambda weighted least squares, and now the Alpha determined.",
                    "label": 0
                },
                {
                    "sent": "These alphas are determined by some.",
                    "label": 0
                },
                {
                    "sent": "Matrix L, which is defined here and the Lambda is.",
                    "label": 0
                },
                {
                    "sent": "Is as an exponent of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And the matrix D. Basically, if you have ordinary least squares, it's like that.",
                    "label": 0
                },
                {
                    "sent": "And D is is just the ratio of these.",
                    "label": 0
                },
                {
                    "sent": "Two probabilities and the diagonal of it.",
                    "label": 0
                },
                {
                    "sent": "OK, so again at bias variance decomposition.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's the difference that we would like to estimate.",
                    "label": 0
                },
                {
                    "sent": "This is quite accessible.",
                    "label": 0
                },
                {
                    "sent": "The norm of the Sky.",
                    "label": 0
                },
                {
                    "sent": "This is a concept which is ignored, but this one is interesting, so we would like to estimate that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this we decompose the function F in.",
                    "label": 0
                },
                {
                    "sent": "And that's this is a technique also that has been used in this subspace.",
                    "label": 0
                },
                {
                    "sent": "Information criteria.",
                    "label": 0
                },
                {
                    "sent": "We decompose it into a subspace.",
                    "label": 0
                },
                {
                    "sent": "Where D lives.",
                    "label": 0
                },
                {
                    "sent": "That is, in the span of these basis functions and some orthogonal part that is orthogonal to the basis functions.",
                    "label": 0
                },
                {
                    "sent": "OK so F would be here.",
                    "label": 0
                },
                {
                    "sent": "G is in this plane of the span of the basis functions and R is orthogonal to this.",
                    "label": 0
                },
                {
                    "sent": "And then we can essentially.",
                    "label": 0
                },
                {
                    "sent": "Compose set was.",
                    "label": 0
                }
            ]
        }
    }
}