{
    "id": "bhofkdsfp6xxaryepndia5cfdieojcqu",
    "title": "Inductive Regularized Learning of Kernel Functions",
    "info": {
        "author": [
            "Prateek Jain, Nuance Communications, Inc."
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nips2010_jain_irl/",
    "segmentation": [
        [
            "Good evening everyone.",
            "This is joint work with Brian Kulis at UC Berkeley and Energy Dillon at UT Austin.",
            "So in this work we consider the problem of kernel function learning."
        ],
        [
            "For the given task at hand.",
            "So we assume that the task can be specified using labels or pairwise distance similarity, dissimilarity constraints, or some other linear constraints over similarity as well as distance distance functions.",
            "So, and we want that the learned kernel function should be generalizable to new unseen points with reasonable generalization error.",
            "So for this work on the kernel, learning the existing approaches can be divided into 2 broad categories.",
            "One is parametric approaches where the learned kernel function is assumed to be restricted to a fixed set of parametric family, and then the goal is to learn parameters for the given task.",
            "And the nonparametric approaches where you learn a nonparametric kernel matrix over your training points.",
            "But the problem with this approach is that OK, it cannot be generalized to new points easily.",
            "So our approach tries to."
        ],
        [
            "Mine both of these existing work.",
            "So first what we do is we learn a kernel matrix over only the training points.",
            "And the.",
            "Kernel matrix is learned using a very generic optimization problem.",
            "So here in this slide we show this optimization problem where we want to learn matrix kW and these functions F are our objective regulation function and the side information is specified using these constraints GIS.",
            "And assuming FNG else to be convex, we can learn matrix kW using standard convex optimization procedure.",
            "But the problem is that generalization to new points is not easy.",
            "However, we show that if function F is a spectral function, that is, it only depends on eigenvalues of its input.",
            "Then our optimization problem implicitly learns a linear transformation kernel, which is given by X transpose WY, and it is parameterized by a positive definite matrix W. And this is actually just a counterpart to modernize distance function, and that's why we can use generalization L, known for modern abyss distance learning.",
            "And use them to guarantee generation error for our kernel functions.",
            "So a direct result of our approach is that we can."
        ],
        [
            "Show cancellation for most of the existing model based metric learning methods.",
            "Furthermore, we can apply our method to the task of semi supervised kernel dimension reduction.",
            "We also want to extend or embed new data points to our learning low dimensional embedding.",
            "So we learn a lot of kernel function using different regulations and different.",
            "Constraints and apply them to standard benchmark datasets, and we observe that learn kernel functions are pretty good for classification and low dimensional embedding.",
            "So please visit our poster at T55 for further details.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good evening everyone.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Brian Kulis at UC Berkeley and Energy Dillon at UT Austin.",
                    "label": 1
                },
                {
                    "sent": "So in this work we consider the problem of kernel function learning.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the given task at hand.",
                    "label": 1
                },
                {
                    "sent": "So we assume that the task can be specified using labels or pairwise distance similarity, dissimilarity constraints, or some other linear constraints over similarity as well as distance distance functions.",
                    "label": 0
                },
                {
                    "sent": "So, and we want that the learned kernel function should be generalizable to new unseen points with reasonable generalization error.",
                    "label": 1
                },
                {
                    "sent": "So for this work on the kernel, learning the existing approaches can be divided into 2 broad categories.",
                    "label": 0
                },
                {
                    "sent": "One is parametric approaches where the learned kernel function is assumed to be restricted to a fixed set of parametric family, and then the goal is to learn parameters for the given task.",
                    "label": 0
                },
                {
                    "sent": "And the nonparametric approaches where you learn a nonparametric kernel matrix over your training points.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this approach is that OK, it cannot be generalized to new points easily.",
                    "label": 0
                },
                {
                    "sent": "So our approach tries to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mine both of these existing work.",
                    "label": 0
                },
                {
                    "sent": "So first what we do is we learn a kernel matrix over only the training points.",
                    "label": 1
                },
                {
                    "sent": "And the.",
                    "label": 1
                },
                {
                    "sent": "Kernel matrix is learned using a very generic optimization problem.",
                    "label": 0
                },
                {
                    "sent": "So here in this slide we show this optimization problem where we want to learn matrix kW and these functions F are our objective regulation function and the side information is specified using these constraints GIS.",
                    "label": 0
                },
                {
                    "sent": "And assuming FNG else to be convex, we can learn matrix kW using standard convex optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that generalization to new points is not easy.",
                    "label": 0
                },
                {
                    "sent": "However, we show that if function F is a spectral function, that is, it only depends on eigenvalues of its input.",
                    "label": 0
                },
                {
                    "sent": "Then our optimization problem implicitly learns a linear transformation kernel, which is given by X transpose WY, and it is parameterized by a positive definite matrix W. And this is actually just a counterpart to modernize distance function, and that's why we can use generalization L, known for modern abyss distance learning.",
                    "label": 0
                },
                {
                    "sent": "And use them to guarantee generation error for our kernel functions.",
                    "label": 0
                },
                {
                    "sent": "So a direct result of our approach is that we can.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Show cancellation for most of the existing model based metric learning methods.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, we can apply our method to the task of semi supervised kernel dimension reduction.",
                    "label": 1
                },
                {
                    "sent": "We also want to extend or embed new data points to our learning low dimensional embedding.",
                    "label": 1
                },
                {
                    "sent": "So we learn a lot of kernel function using different regulations and different.",
                    "label": 0
                },
                {
                    "sent": "Constraints and apply them to standard benchmark datasets, and we observe that learn kernel functions are pretty good for classification and low dimensional embedding.",
                    "label": 0
                },
                {
                    "sent": "So please visit our poster at T55 for further details.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}