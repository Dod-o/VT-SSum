{
    "id": "s4n7ks2rci6yzybuf6bqnyynvw4rwhk3",
    "title": "Boosting with Structural Sparsity",
    "info": {
        "author": [
            "John Duchi, Department of Electrical Engineering and Computer Sciences, UC Berkeley"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Boosting"
        ]
    },
    "url": "http://videolectures.net/icml09_duchi_bwss/",
    "segmentation": [
        [
            "Sorry, hold on one second alright you guys ready yet?",
            "'cause I am.",
            "Alright, so I'm John duchi.",
            "I'm going to be presenting some work that I've been doing at Google with your arm singer.",
            "And I'm going to be talking about boosting where we impose some sparsity constraints in our boosting."
        ],
        [
            "Process.",
            "So a brief outline of the talk.",
            "I'm going to start off by going through a tiny bit of what boosting does and and it's kind of relationship with coordinate descent methods which most of you probably know.",
            "Then I'll be talking about the losses that we use and how we incorporate sparsity, promoting penalties into them.",
            "I'll be focusing on one particular loss just for concreteness, that we're getting.",
            "This is applicable, applicable to a lot more things.",
            "Go through one tiny little bit of theory which shouldn't be too frightening at all.",
            "Now I'll go through some experiments and then there's a lot of open.",
            "Quite a lot of questions.",
            "We still have a lot of open problems."
        ],
        [
            "So.",
            "Starting off what what we give in this.",
            "Basically how I want you to think of the boosting process for this talk is as a sort of generalized coordinate descent procedure, or just this coordinate descent.",
            "So the view of boosting is coordinate descent was popularized by Jerry Friedman, Trevor Hastie, and Rob Tibshirani and 2000 or so.",
            "Basically, the game is that we have some loss function L and we're going to be adding features that sort of minimize some sort of upper bound on the loss.",
            "So we just want to add features that do well on the loss function.",
            "And you know this is this is sort of a nice way of looking at it.",
            "These coordinate descent methods are sort of pretty efficient because we don't have to deal with any hashing or second order information, which when you've got lots of features can be pretty efficient or can really slow things down if you have to do that.",
            "Yeah, and this is how I sort of want you guys to think about the boosting ideas in this paper.",
            "Also, we're going to be the framework I'm working in is totally corrective boosting, in which you induce a feature, and then you cycle back over all of the features that you already have in your model and re optimize the weights for them.",
            "So just keep that in mind that we're not just inducing features and moving on.",
            "We induce a feature, we re optimize the weights and then keep going OK."
        ],
        [
            "So.",
            "What I'm going to give in this talk is a family of coordinate descent methods, and these coordinate descent methods are going to be applicable to multiclass logistic regression, binary logistic regression, and just standard sort of linear regression.",
            "Except with logistic type losses.",
            "And also I'm going to have extensions two different mixed norm regularizers.",
            "So."
        ],
        [
            "This is the loss function that I'm going to spend this entire talk focusing on.",
            "Just because it's going to make things nice and concrete.",
            "So this is the multi class logistic loss.",
            "We have a weight vector or weight matrix.",
            "W&W is a sequence of columns which are the weights for each individual class.",
            "We have K classes, so we learn a weight vector for each of them.",
            "This is slightly different than the general set up for multiclass boosting in general and multiclass boosting, so we assume sort of every class gets the same features XC we're just representing.",
            "These expected vectors in the standard set up for multiclass boosting, you actually each feature vector actually has a function, both of the original features and of the class that you're looking at.",
            "So we think this is sort of a slightly more natural framework to look at it, just you have your weight matrix and it's there's a weight vector for every class, so the bounds we actually give on this are new in the boosting literature.",
            "Now what we're going to add to this?"
        ],
        [
            "This loss function is mixed norm regularization, so our goal in this is basically we want to get sparsity, but we want to get sparsity over entire rows of our weight matrix, 'cause we want to basically remove features from the boosting process.",
            "So this is just what mixed norm regularization is.",
            "If you've probably seen it before, so the L1LP norm of a matrix is we take the P norm of every row in the matrix and then we just sum them up like L1.",
            "So a quick example of this, just so you guys get a feel of what we."
        ],
        [
            "Doing is, let's consider L1L2 regularization now, not L, not L2 squared, just an L2 norm.",
            "So this is the contour of the L2 norm in two dimensions, and what you see is there's actually a singularity at zero.",
            "It's not smooth, it's actually singularity."
        ],
        [
            "So.",
            "The subgradient set of the two norm when W = 0 is actually the set of vectors that have two norm less than one.",
            "So what does this mean?",
            "So let's go."
        ],
        [
            "Our loss function if the loss function evaluated at zero is less or equal to Lambda.",
            "The two norm of it rather than of its gradient then."
        ],
        [
            "N 0 is actually the minimizer of the loss plus L2 regularization so.",
            "L1L2 is what I'm going to focus on, and that's actually going to give us the row sparsity that we want, but we can also do this for other P norms.",
            "This is just our sort of motivating examples so."
        ],
        [
            "And back to the loss.",
            "This is what we get.",
            "We just add in this regularizer, OK?",
            "So.",
            "That's the that's the basic framework.",
            "The set up that we're going to be going through so."
        ],
        [
            "Now the standard game in boosting is that you take your loss function and you upper bounded by something.",
            "So in Adaboost we do exponential upper bounds.",
            "For this paper we have exponential upper bounds, but I'm going, but we also have quadratic upper bounds.",
            "I'm going to focus on those 'cause I think there are a little more little clearer, a little easier to talk about, so the basic idea is that we just take our loss.",
            "We say what happens if we go move in one coordinate direction?",
            "Well, we can upper bound it by a simple quadratic, basically using a Taylor approximation."
        ],
        [
            "So just as a quick illustration, here's log 1 + E to the minus X.",
            "You take the derivative and the 2nd order, and we sort of lower bound."
        ],
        [
            "2nd order approximation and this is what you get.",
            "The Red line is just the 2nd order approximation.",
            "We also have.",
            "We can also do it for multiple."
        ],
        [
            "And it's at the same time.",
            "So here's your logistic function in two dimensions.",
            "Log 1 + y to the one transpose X."
        ],
        [
            "And then we can put this type of quadratic on it.",
            "You see it always upper bounds it, although it kind of goes up in places where you might not like it too, but.",
            "Still an upper bound."
        ],
        [
            "So back to the bound so.",
            "The starting point is this Taylor expansion, and we do a quadratic one, so we're guaranteed an upper bound."
        ],
        [
            "Just by doing this basically for the Matrix case, we can.",
            "Look at we update one rule of our matrix which corresponds to updating all of the weights for one feature and then we get this quadratic upper bound now.",
            "The point of this is that this quadratic upper bound is going to be fairly easy to minimize, and we're going to be able to handle nonsmooth regularizers with it.",
            "We can also do this for the entire matrix instead of just doing coordinate steps we can parallelize it across many machines, and things like that, and those types of algorithms are fairly well known.",
            "I think Mike Collins and Rob Shapiro did some work on that in the early 2000s.",
            "So this is the bound to Taylor expansion.",
            "It's nothing too complicated."
        ],
        [
            "So what's the point of this?",
            "Well, the point of that is that with these type of bounds, we're going to get easy minimization problems, and the updates are going to be really easy, and it's going to allow easy feature induction too.",
            "So if we just set AJ to be this, basically normalizing constant and we replace Delta with the difference between the current weights in the previous weights, then we see we just get this quadratic, which is this gradient term roughly, and then hashing term.",
            "Except there's no more hashing information.",
            "We don't have to deal with it.",
            "And then we add regularization, Lambda times the P norm of W. This is going to be a really simple update.",
            "So what we do all we have to do is minimize this.",
            "People feeling pretty good about this.",
            "OK."
        ],
        [
            "Not too many blank stares.",
            "I like that."
        ],
        [
            "So the update for L2 regularization for the mixed norm L1L2 is just this, and basically so we have the previous role in our matrix WJ, we subtract basically a constant times the gradient and then we multiply it by this term.",
            "Now what is this term really?",
            "It's a lot like soft thresholding for L1 regularization, so we see this a lot in like different coordinate descent methods.",
            "When you've got L1 and it turns out that you also can get it for coordinate ascent methods with these.",
            "Multi class logistic objectives but with L2 regularization and there's similar updates that we give for L1L Infinity when you have block norms that are sort of Infinity regularised or other things.",
            "So this is this is really all we do.",
            "This is just the update to rojae when we look at it and we just iterate this cycle through the features and we're going to minimize the loss."
        ],
        [
            "Now I'm going to take a brief foray back into boosting instead of just the straight coordinate descent flavor that we've been thinking about.",
            "So this is the update.",
            "Just remember this soft soft threshold so.",
            "Now we're doing totally corrective boosting, so we revisit."
        ],
        [
            "Features that we've induced.",
            "So what does this give us?",
            "This is going to give us some type of feature pruning, right?",
            "So imagine we add a feature that, in retrospect, is actually not that informative standard boosting.",
            "You never remove features once you've added them, your model just keeps growing.",
            "But if you look at this carefully, you see, if in retrospect the feature is not that informative, as in like.",
            "Basically the gradient gets sort of small, and then the Lambda penalty kicks in.",
            "What we're going to do is we're going to zero out the weights for that feature.",
            "The entire row of them.",
            "So we can basically just kick that feature out of the model.",
            "So this is sort of a nice effect of this update and we don't have any good solid theory about when this will work when it won't, but I'll show some experiments that suggest it does what we want it to do."
        ],
        [
            "The other thing we can get.",
            "No, evidently that didn't show up on your screen, so the other thing we can get is scoring of new features.",
            "So when we're inducing features, we need some way to score them.",
            "So the scoring of features for this quadratic upper bound that we've got basically looks like this.",
            "You take the feature you compute, so if you're going to do some feature J, you compute its gradient with respect to your current model, and then you just get this thresholding operation and you divide you normalize by the sum of the feature values across the data set.",
            "And So what does this give you?",
            "So this actually gives you kind of a nice stopping criterion for boosting.",
            "So basically it says, if at some point in the boosting process when we're inducing features we get some feature, you know the feature, the weak learner that is returned from whatever we're using to get weak learners is basically non informative.",
            "Then we can just terminate 'cause this will just truncate itself to zero and boom, you're done.",
            "So you get it, get a criterion for stopping boosting, which is kind of nice.",
            "OK. And now."
        ],
        [
            "The obligatory theory slide.",
            "This is all of it.",
            "Then I'm going to do now.",
            "Basically, if the number of base features based hypothesis is finite, then everything I'm talking about is convergent, and this applies to all of the other algorithms.",
            "In our paper.",
            "You know we have exponential bounds, different regularizers blah blah blah, but it all converges to the true.",
            "Minimize your loss, which is what you want so.",
            "But we don't have any convergence rates, so we don't know how it works.",
            "I assume the convergence rate is something like one over epsilon.",
            "Shai has a nice paper that he presented yesterday which gives one over epsilon rates for things really similar to these methods, but I don't know how to prove it for ours.",
            "So if anybody feels like doing that, go get 'em.",
            "OK, so that's that's it for the method."
        ],
        [
            "Now I'm going to go through some experiments just to give you guys sort of a feel for.",
            "What we expect.",
            "You know how these methods perform in practice that they're actually doing what we want.",
            "We set out to do, which is getting sparsity in the models and sort of improving the booting process, helping us avoid overfitting.",
            "I'll start off with binary classification.",
            "I didn't really talk about that, but you can imagine if we can do it for multiclass.",
            "Probably binary is not that much harder.",
            "Actually, it's a lot easier, and then I'll go through the multiclass classification examples and then I'll talk a little bit about what the sort of algorithm looks like across it.",
            "Run so starting."
        ],
        [
            "Binary classification.",
            "So here we compared the L1 regularize booster, totally corrected booster with a boosting process with smooth L1 regularization, which is roughly equivalent to adding an extra positive and negative examples to help keep our weights sort of manageable.",
            "It's pretty well known in the blue string literature and also just L to regularize logistic regression with lots of features tossed in at the beginning.",
            "No sort of feature selection is part of it.",
            "So what we plot on the left?",
            "So this is all with the Reuters, MCV.",
            "One data set.",
            "And this is just for this is the task is to classify an article as either a medical article or an automatical article.",
            "And what we see on the left are the error rates for the different boosting processes on train and test.",
            "So the green line here and the black line above excuse me are the smooth.",
            "Regularised booster and we see that it improves the test set or the training set loss a lot and keeps improving it, but the test set like we could sort of plateaus and it actually the loss starts growing.",
            "The error rate stays pretty much constant.",
            "Yeah, what's up you?",
            "So the smooth regularization is actually the regularizer is log 1 + E to the Wii Plus log 1 + E to the minus Wii.",
            "So it's kind of like a smooth approximation to L1.",
            "It looks a lot like this accepted smooth here, and it's pretty common in the boosting literature I think.",
            "Shy you think it's common in the boosting literature she thinks is common in the boosting literature.",
            "He's the chair of this section, so.",
            "Yeah, so that's the reason we cross validated the regularization parameters and everything, but so the neat thing we see is that the L1 regularize booster, the reason that line stops is because it actually stops inducing features.",
            "So this is these are sort of the iterations of feature induction.",
            "You know, 100 features, 200 features, 300 features blah blah blah, and after it's added 700 features, the L1 regularize booster just terminates itself.",
            "It says I've got enough.",
            "I'm going to stop, so that's kind of neat behavior.",
            "You know, we thought that would happen.",
            "It actually does.",
            "We saw this for a lot of other binary classification problems."
        ],
        [
            "Now I'll talk a bit about the multi class classification results so.",
            "This is on the stat log satellite data set where the task is to classify a Patch of ground as either grass or gravel or trees or some other things.",
            "I don't remember what there's four more tasks.",
            "So on the left when I'm plotting, is the X axis is the number of features actually used, so the number of non zero rows in the matrix.",
            "Basically if an entire row is 0, then we never actually have to use that feature in computation and the Y axis is test loss or coverage and coverage is basically error rate, except you get penalized more for sort of predicting things more incorrectly and penalized less for predicting them less correctly anyway.",
            "So what we see here is this interesting behavior that these mixed norm regularizers.",
            "L1L2 is in red as a function of the actual number of features that you need to use.",
            "They get sort of much better performance in terms of test set loss in coverage then the L1 regularizer.",
            "OK, so the L1 regularizer doesn't give you as much sparsity in terms of feature space Bar City like row sparsity, But if all were considered concerned about is sparsity in the matrix itself, like the number of zeros in the matrix, as in you know if you've got one on element in the entire row, the L1L.",
            "Sort of L1L2 says everything else will be on, but L1 says that's OK in terms of just strict sparsity.",
            "L1 is going to give you a little better performance, but you have to compute more features so."
        ],
        [
            "We did this for many datasets.",
            "I'll just show you one other one.",
            "This is the pen digit handwritten digit classification data set.",
            "So we just need to classify what someone's writing is like, 0 through 9 or maybe a through ZI, don't remember.",
            "I forget what the datasets all are, but we see similar behaviour.",
            "Basically, if you're concerned about reducing the number of features that you actually need to compute, then these mixed norm regularizer seem to give you a lot more bang for your Buck.",
            "Then the L1 regularizer, which is kind of interesting and nice behavior.",
            "And Lastly, I said I'd talk a bit about."
        ],
        [
            "The runtime behavior of these methods and this is this is related to.",
            "How I said that you know these would do induction and then sort of.",
            "In retrospect they would be able to specify remove Hypotheek hypothesis that in retrospect were not informative.",
            "So this is indicative of that.",
            "So what we did here this we just said OK run cyclically through all the features and this is on the MNIST handwritten digit data set and we gave 30,000 features.",
            "So total 10 or 3000 features, so 10,000 total for all the classes.",
            "And on the Landsat data set.",
            "And so we said just cyclically run through all the features over and over and over again.",
            "OK, and so we see at the beginning sort of iterations like zero to 1000, you end up putting a lot of features in your models just the whole whole pile of.",
            "I mean basically these things were getting up to almost totally non zero all the way to non 0.",
            "But then as you keep running it they sort of revisit features and they just add sparsity as they go along.",
            "Oh, and the reason there's four lines instead of just two for the datasets.",
            "One of these is with the exponential bounds that I didn't present the standard Adaboost bounds and the other one is with the gradient based bounds.",
            "But what we see is that basically.",
            "You're removing features as you go along, but your test set loss your test error rates are not increasing at all.",
            "So basically you improve a lot by adding all these features in and then you just sort of.",
            "These methods seem to just spend their time removing features that you don't care about while not increasing your loss rates at all.",
            "So we thought this was really nice behavior.",
            "Seems pretty cool so."
        ],
        [
            "It's.",
            "That's it for my talk."
        ],
        [
            "What I've done is I've given you guys boosting variance for multi class binary multi class in binary logistic regression as well as linear regression.",
            "Even though I didn't talk about that that much."
        ],
        [
            "I.",
            "These methods let us prune features that we've added.",
            "They let us do feature induction through feature scoring and they are probably convergent.",
            "They're going to get to the right answer, and now there's a lot of."
        ],
        [
            "Questions still."
        ],
        [
            "Like I said, we don't have any convergence rates, we don't.",
            "You know, I think they want to epsilon, but I'm not really sure I haven't."
        ],
        [
            "Unable to prove anything, and also there's some questions of consistency and whether we get better generalization by sort of doing this.",
            "Removal of features or terminating the boosting process."
        ],
        [
            "So thanks very much and I'd love to take your questions.",
            "Question.",
            "Yum.",
            "Is yelling at you proposing different than with a group that approximates the regularization path of like the norm?",
            "I guess it.",
            "I mean it depends on what algorithm most of the regularization path algorithms for linear regression, right?",
            "I. I mean so it's.",
            "I guess it's hard for me to say I don't know like which algorithms in particular are you thinking of?",
            "Um?",
            "Well so.",
            "Like for example, like the algorithm or right, right, right?",
            "So I guess most of those I thought were for linear regression, but but yeah, you can adapt them for generalized linear models, but you know they don't really give you like the feature induction properties, right?",
            "Like this you could use with sort of a countably infinite set of features and still do feature scoring, induction, etc like that.",
            "The feature scoring you could do would be with that.",
            "I would assume this runs a little faster, but I didn't have any experiments on real runtime.",
            "Sorry, that's an unsatisfying answer, but.",
            "More questions.",
            "At the beginning you would message not realizing.",
            "Well, uh huh.",
            "Article 2 any energy.",
            "I mean, for example equal to 1.5 or three.",
            "Yeah, so sorry.",
            "I guess it would depend on.",
            "I mean certainly they would be applicable to that and they would be probably convergent.",
            "It just depends on whether or not you can solve this problem for the P or considering and it's really easy with 1, two and Infinity.",
            "You know this sort of standard thing, and 1.5 would probably give you similar performance to want to like L1L2 'cause you're still going to get block sparsity, but we didn't.",
            "We didn't consider it 'cause it didn't seem unimportant.",
            "OK, alright thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry, hold on one second alright you guys ready yet?",
                    "label": 0
                },
                {
                    "sent": "'cause I am.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I'm John duchi.",
                    "label": 1
                },
                {
                    "sent": "I'm going to be presenting some work that I've been doing at Google with your arm singer.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to be talking about boosting where we impose some sparsity constraints in our boosting.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Process.",
                    "label": 0
                },
                {
                    "sent": "So a brief outline of the talk.",
                    "label": 1
                },
                {
                    "sent": "I'm going to start off by going through a tiny bit of what boosting does and and it's kind of relationship with coordinate descent methods which most of you probably know.",
                    "label": 0
                },
                {
                    "sent": "Then I'll be talking about the losses that we use and how we incorporate sparsity, promoting penalties into them.",
                    "label": 0
                },
                {
                    "sent": "I'll be focusing on one particular loss just for concreteness, that we're getting.",
                    "label": 0
                },
                {
                    "sent": "This is applicable, applicable to a lot more things.",
                    "label": 0
                },
                {
                    "sent": "Go through one tiny little bit of theory which shouldn't be too frightening at all.",
                    "label": 0
                },
                {
                    "sent": "Now I'll go through some experiments and then there's a lot of open.",
                    "label": 0
                },
                {
                    "sent": "Quite a lot of questions.",
                    "label": 0
                },
                {
                    "sent": "We still have a lot of open problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Starting off what what we give in this.",
                    "label": 0
                },
                {
                    "sent": "Basically how I want you to think of the boosting process for this talk is as a sort of generalized coordinate descent procedure, or just this coordinate descent.",
                    "label": 0
                },
                {
                    "sent": "So the view of boosting is coordinate descent was popularized by Jerry Friedman, Trevor Hastie, and Rob Tibshirani and 2000 or so.",
                    "label": 0
                },
                {
                    "sent": "Basically, the game is that we have some loss function L and we're going to be adding features that sort of minimize some sort of upper bound on the loss.",
                    "label": 0
                },
                {
                    "sent": "So we just want to add features that do well on the loss function.",
                    "label": 1
                },
                {
                    "sent": "And you know this is this is sort of a nice way of looking at it.",
                    "label": 0
                },
                {
                    "sent": "These coordinate descent methods are sort of pretty efficient because we don't have to deal with any hashing or second order information, which when you've got lots of features can be pretty efficient or can really slow things down if you have to do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and this is how I sort of want you guys to think about the boosting ideas in this paper.",
                    "label": 0
                },
                {
                    "sent": "Also, we're going to be the framework I'm working in is totally corrective boosting, in which you induce a feature, and then you cycle back over all of the features that you already have in your model and re optimize the weights for them.",
                    "label": 0
                },
                {
                    "sent": "So just keep that in mind that we're not just inducing features and moving on.",
                    "label": 0
                },
                {
                    "sent": "We induce a feature, we re optimize the weights and then keep going OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to give in this talk is a family of coordinate descent methods, and these coordinate descent methods are going to be applicable to multiclass logistic regression, binary logistic regression, and just standard sort of linear regression.",
                    "label": 1
                },
                {
                    "sent": "Except with logistic type losses.",
                    "label": 0
                },
                {
                    "sent": "And also I'm going to have extensions two different mixed norm regularizers.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the loss function that I'm going to spend this entire talk focusing on.",
                    "label": 1
                },
                {
                    "sent": "Just because it's going to make things nice and concrete.",
                    "label": 0
                },
                {
                    "sent": "So this is the multi class logistic loss.",
                    "label": 1
                },
                {
                    "sent": "We have a weight vector or weight matrix.",
                    "label": 0
                },
                {
                    "sent": "W&W is a sequence of columns which are the weights for each individual class.",
                    "label": 0
                },
                {
                    "sent": "We have K classes, so we learn a weight vector for each of them.",
                    "label": 0
                },
                {
                    "sent": "This is slightly different than the general set up for multiclass boosting in general and multiclass boosting, so we assume sort of every class gets the same features XC we're just representing.",
                    "label": 0
                },
                {
                    "sent": "These expected vectors in the standard set up for multiclass boosting, you actually each feature vector actually has a function, both of the original features and of the class that you're looking at.",
                    "label": 0
                },
                {
                    "sent": "So we think this is sort of a slightly more natural framework to look at it, just you have your weight matrix and it's there's a weight vector for every class, so the bounds we actually give on this are new in the boosting literature.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to add to this?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This loss function is mixed norm regularization, so our goal in this is basically we want to get sparsity, but we want to get sparsity over entire rows of our weight matrix, 'cause we want to basically remove features from the boosting process.",
                    "label": 0
                },
                {
                    "sent": "So this is just what mixed norm regularization is.",
                    "label": 0
                },
                {
                    "sent": "If you've probably seen it before, so the L1LP norm of a matrix is we take the P norm of every row in the matrix and then we just sum them up like L1.",
                    "label": 0
                },
                {
                    "sent": "So a quick example of this, just so you guys get a feel of what we.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing is, let's consider L1L2 regularization now, not L, not L2 squared, just an L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So this is the contour of the L2 norm in two dimensions, and what you see is there's actually a singularity at zero.",
                    "label": 1
                },
                {
                    "sent": "It's not smooth, it's actually singularity.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The subgradient set of the two norm when W = 0 is actually the set of vectors that have two norm less than one.",
                    "label": 0
                },
                {
                    "sent": "So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "So let's go.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our loss function if the loss function evaluated at zero is less or equal to Lambda.",
                    "label": 0
                },
                {
                    "sent": "The two norm of it rather than of its gradient then.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "N 0 is actually the minimizer of the loss plus L2 regularization so.",
                    "label": 0
                },
                {
                    "sent": "L1L2 is what I'm going to focus on, and that's actually going to give us the row sparsity that we want, but we can also do this for other P norms.",
                    "label": 0
                },
                {
                    "sent": "This is just our sort of motivating examples so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And back to the loss.",
                    "label": 0
                },
                {
                    "sent": "This is what we get.",
                    "label": 0
                },
                {
                    "sent": "We just add in this regularizer, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's the that's the basic framework.",
                    "label": 0
                },
                {
                    "sent": "The set up that we're going to be going through so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the standard game in boosting is that you take your loss function and you upper bounded by something.",
                    "label": 0
                },
                {
                    "sent": "So in Adaboost we do exponential upper bounds.",
                    "label": 0
                },
                {
                    "sent": "For this paper we have exponential upper bounds, but I'm going, but we also have quadratic upper bounds.",
                    "label": 0
                },
                {
                    "sent": "I'm going to focus on those 'cause I think there are a little more little clearer, a little easier to talk about, so the basic idea is that we just take our loss.",
                    "label": 0
                },
                {
                    "sent": "We say what happens if we go move in one coordinate direction?",
                    "label": 0
                },
                {
                    "sent": "Well, we can upper bound it by a simple quadratic, basically using a Taylor approximation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just as a quick illustration, here's log 1 + E to the minus X.",
                    "label": 0
                },
                {
                    "sent": "You take the derivative and the 2nd order, and we sort of lower bound.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2nd order approximation and this is what you get.",
                    "label": 0
                },
                {
                    "sent": "The Red line is just the 2nd order approximation.",
                    "label": 0
                },
                {
                    "sent": "We also have.",
                    "label": 0
                },
                {
                    "sent": "We can also do it for multiple.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's at the same time.",
                    "label": 0
                },
                {
                    "sent": "So here's your logistic function in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "Log 1 + y to the one transpose X.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can put this type of quadratic on it.",
                    "label": 0
                },
                {
                    "sent": "You see it always upper bounds it, although it kind of goes up in places where you might not like it too, but.",
                    "label": 0
                },
                {
                    "sent": "Still an upper bound.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So back to the bound so.",
                    "label": 0
                },
                {
                    "sent": "The starting point is this Taylor expansion, and we do a quadratic one, so we're guaranteed an upper bound.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just by doing this basically for the Matrix case, we can.",
                    "label": 0
                },
                {
                    "sent": "Look at we update one rule of our matrix which corresponds to updating all of the weights for one feature and then we get this quadratic upper bound now.",
                    "label": 0
                },
                {
                    "sent": "The point of this is that this quadratic upper bound is going to be fairly easy to minimize, and we're going to be able to handle nonsmooth regularizers with it.",
                    "label": 0
                },
                {
                    "sent": "We can also do this for the entire matrix instead of just doing coordinate steps we can parallelize it across many machines, and things like that, and those types of algorithms are fairly well known.",
                    "label": 0
                },
                {
                    "sent": "I think Mike Collins and Rob Shapiro did some work on that in the early 2000s.",
                    "label": 0
                },
                {
                    "sent": "So this is the bound to Taylor expansion.",
                    "label": 0
                },
                {
                    "sent": "It's nothing too complicated.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the point of this?",
                    "label": 0
                },
                {
                    "sent": "Well, the point of that is that with these type of bounds, we're going to get easy minimization problems, and the updates are going to be really easy, and it's going to allow easy feature induction too.",
                    "label": 1
                },
                {
                    "sent": "So if we just set AJ to be this, basically normalizing constant and we replace Delta with the difference between the current weights in the previous weights, then we see we just get this quadratic, which is this gradient term roughly, and then hashing term.",
                    "label": 0
                },
                {
                    "sent": "Except there's no more hashing information.",
                    "label": 0
                },
                {
                    "sent": "We don't have to deal with it.",
                    "label": 0
                },
                {
                    "sent": "And then we add regularization, Lambda times the P norm of W. This is going to be a really simple update.",
                    "label": 0
                },
                {
                    "sent": "So what we do all we have to do is minimize this.",
                    "label": 0
                },
                {
                    "sent": "People feeling pretty good about this.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not too many blank stares.",
                    "label": 0
                },
                {
                    "sent": "I like that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the update for L2 regularization for the mixed norm L1L2 is just this, and basically so we have the previous role in our matrix WJ, we subtract basically a constant times the gradient and then we multiply it by this term.",
                    "label": 0
                },
                {
                    "sent": "Now what is this term really?",
                    "label": 0
                },
                {
                    "sent": "It's a lot like soft thresholding for L1 regularization, so we see this a lot in like different coordinate descent methods.",
                    "label": 0
                },
                {
                    "sent": "When you've got L1 and it turns out that you also can get it for coordinate ascent methods with these.",
                    "label": 0
                },
                {
                    "sent": "Multi class logistic objectives but with L2 regularization and there's similar updates that we give for L1L Infinity when you have block norms that are sort of Infinity regularised or other things.",
                    "label": 0
                },
                {
                    "sent": "So this is this is really all we do.",
                    "label": 0
                },
                {
                    "sent": "This is just the update to rojae when we look at it and we just iterate this cycle through the features and we're going to minimize the loss.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm going to take a brief foray back into boosting instead of just the straight coordinate descent flavor that we've been thinking about.",
                    "label": 0
                },
                {
                    "sent": "So this is the update.",
                    "label": 0
                },
                {
                    "sent": "Just remember this soft soft threshold so.",
                    "label": 0
                },
                {
                    "sent": "Now we're doing totally corrective boosting, so we revisit.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Features that we've induced.",
                    "label": 0
                },
                {
                    "sent": "So what does this give us?",
                    "label": 0
                },
                {
                    "sent": "This is going to give us some type of feature pruning, right?",
                    "label": 1
                },
                {
                    "sent": "So imagine we add a feature that, in retrospect, is actually not that informative standard boosting.",
                    "label": 0
                },
                {
                    "sent": "You never remove features once you've added them, your model just keeps growing.",
                    "label": 0
                },
                {
                    "sent": "But if you look at this carefully, you see, if in retrospect the feature is not that informative, as in like.",
                    "label": 0
                },
                {
                    "sent": "Basically the gradient gets sort of small, and then the Lambda penalty kicks in.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is we're going to zero out the weights for that feature.",
                    "label": 0
                },
                {
                    "sent": "The entire row of them.",
                    "label": 0
                },
                {
                    "sent": "So we can basically just kick that feature out of the model.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a nice effect of this update and we don't have any good solid theory about when this will work when it won't, but I'll show some experiments that suggest it does what we want it to do.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other thing we can get.",
                    "label": 0
                },
                {
                    "sent": "No, evidently that didn't show up on your screen, so the other thing we can get is scoring of new features.",
                    "label": 0
                },
                {
                    "sent": "So when we're inducing features, we need some way to score them.",
                    "label": 0
                },
                {
                    "sent": "So the scoring of features for this quadratic upper bound that we've got basically looks like this.",
                    "label": 0
                },
                {
                    "sent": "You take the feature you compute, so if you're going to do some feature J, you compute its gradient with respect to your current model, and then you just get this thresholding operation and you divide you normalize by the sum of the feature values across the data set.",
                    "label": 0
                },
                {
                    "sent": "And So what does this give you?",
                    "label": 0
                },
                {
                    "sent": "So this actually gives you kind of a nice stopping criterion for boosting.",
                    "label": 0
                },
                {
                    "sent": "So basically it says, if at some point in the boosting process when we're inducing features we get some feature, you know the feature, the weak learner that is returned from whatever we're using to get weak learners is basically non informative.",
                    "label": 0
                },
                {
                    "sent": "Then we can just terminate 'cause this will just truncate itself to zero and boom, you're done.",
                    "label": 0
                },
                {
                    "sent": "So you get it, get a criterion for stopping boosting, which is kind of nice.",
                    "label": 0
                },
                {
                    "sent": "OK. And now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The obligatory theory slide.",
                    "label": 0
                },
                {
                    "sent": "This is all of it.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to do now.",
                    "label": 0
                },
                {
                    "sent": "Basically, if the number of base features based hypothesis is finite, then everything I'm talking about is convergent, and this applies to all of the other algorithms.",
                    "label": 1
                },
                {
                    "sent": "In our paper.",
                    "label": 0
                },
                {
                    "sent": "You know we have exponential bounds, different regularizers blah blah blah, but it all converges to the true.",
                    "label": 0
                },
                {
                    "sent": "Minimize your loss, which is what you want so.",
                    "label": 0
                },
                {
                    "sent": "But we don't have any convergence rates, so we don't know how it works.",
                    "label": 0
                },
                {
                    "sent": "I assume the convergence rate is something like one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "Shai has a nice paper that he presented yesterday which gives one over epsilon rates for things really similar to these methods, but I don't know how to prove it for ours.",
                    "label": 0
                },
                {
                    "sent": "So if anybody feels like doing that, go get 'em.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's it for the method.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now I'm going to go through some experiments just to give you guys sort of a feel for.",
                    "label": 0
                },
                {
                    "sent": "What we expect.",
                    "label": 0
                },
                {
                    "sent": "You know how these methods perform in practice that they're actually doing what we want.",
                    "label": 0
                },
                {
                    "sent": "We set out to do, which is getting sparsity in the models and sort of improving the booting process, helping us avoid overfitting.",
                    "label": 0
                },
                {
                    "sent": "I'll start off with binary classification.",
                    "label": 1
                },
                {
                    "sent": "I didn't really talk about that, but you can imagine if we can do it for multiclass.",
                    "label": 0
                },
                {
                    "sent": "Probably binary is not that much harder.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a lot easier, and then I'll go through the multiclass classification examples and then I'll talk a little bit about what the sort of algorithm looks like across it.",
                    "label": 0
                },
                {
                    "sent": "Run so starting.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Binary classification.",
                    "label": 0
                },
                {
                    "sent": "So here we compared the L1 regularize booster, totally corrected booster with a boosting process with smooth L1 regularization, which is roughly equivalent to adding an extra positive and negative examples to help keep our weights sort of manageable.",
                    "label": 0
                },
                {
                    "sent": "It's pretty well known in the blue string literature and also just L to regularize logistic regression with lots of features tossed in at the beginning.",
                    "label": 0
                },
                {
                    "sent": "No sort of feature selection is part of it.",
                    "label": 0
                },
                {
                    "sent": "So what we plot on the left?",
                    "label": 0
                },
                {
                    "sent": "So this is all with the Reuters, MCV.",
                    "label": 0
                },
                {
                    "sent": "One data set.",
                    "label": 0
                },
                {
                    "sent": "And this is just for this is the task is to classify an article as either a medical article or an automatical article.",
                    "label": 0
                },
                {
                    "sent": "And what we see on the left are the error rates for the different boosting processes on train and test.",
                    "label": 0
                },
                {
                    "sent": "So the green line here and the black line above excuse me are the smooth.",
                    "label": 0
                },
                {
                    "sent": "Regularised booster and we see that it improves the test set or the training set loss a lot and keeps improving it, but the test set like we could sort of plateaus and it actually the loss starts growing.",
                    "label": 0
                },
                {
                    "sent": "The error rate stays pretty much constant.",
                    "label": 0
                },
                {
                    "sent": "Yeah, what's up you?",
                    "label": 0
                },
                {
                    "sent": "So the smooth regularization is actually the regularizer is log 1 + E to the Wii Plus log 1 + E to the minus Wii.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of like a smooth approximation to L1.",
                    "label": 0
                },
                {
                    "sent": "It looks a lot like this accepted smooth here, and it's pretty common in the boosting literature I think.",
                    "label": 0
                },
                {
                    "sent": "Shy you think it's common in the boosting literature she thinks is common in the boosting literature.",
                    "label": 0
                },
                {
                    "sent": "He's the chair of this section, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's the reason we cross validated the regularization parameters and everything, but so the neat thing we see is that the L1 regularize booster, the reason that line stops is because it actually stops inducing features.",
                    "label": 0
                },
                {
                    "sent": "So this is these are sort of the iterations of feature induction.",
                    "label": 0
                },
                {
                    "sent": "You know, 100 features, 200 features, 300 features blah blah blah, and after it's added 700 features, the L1 regularize booster just terminates itself.",
                    "label": 0
                },
                {
                    "sent": "It says I've got enough.",
                    "label": 0
                },
                {
                    "sent": "I'm going to stop, so that's kind of neat behavior.",
                    "label": 0
                },
                {
                    "sent": "You know, we thought that would happen.",
                    "label": 0
                },
                {
                    "sent": "It actually does.",
                    "label": 0
                },
                {
                    "sent": "We saw this for a lot of other binary classification problems.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'll talk a bit about the multi class classification results so.",
                    "label": 0
                },
                {
                    "sent": "This is on the stat log satellite data set where the task is to classify a Patch of ground as either grass or gravel or trees or some other things.",
                    "label": 0
                },
                {
                    "sent": "I don't remember what there's four more tasks.",
                    "label": 0
                },
                {
                    "sent": "So on the left when I'm plotting, is the X axis is the number of features actually used, so the number of non zero rows in the matrix.",
                    "label": 0
                },
                {
                    "sent": "Basically if an entire row is 0, then we never actually have to use that feature in computation and the Y axis is test loss or coverage and coverage is basically error rate, except you get penalized more for sort of predicting things more incorrectly and penalized less for predicting them less correctly anyway.",
                    "label": 0
                },
                {
                    "sent": "So what we see here is this interesting behavior that these mixed norm regularizers.",
                    "label": 0
                },
                {
                    "sent": "L1L2 is in red as a function of the actual number of features that you need to use.",
                    "label": 0
                },
                {
                    "sent": "They get sort of much better performance in terms of test set loss in coverage then the L1 regularizer.",
                    "label": 0
                },
                {
                    "sent": "OK, so the L1 regularizer doesn't give you as much sparsity in terms of feature space Bar City like row sparsity, But if all were considered concerned about is sparsity in the matrix itself, like the number of zeros in the matrix, as in you know if you've got one on element in the entire row, the L1L.",
                    "label": 0
                },
                {
                    "sent": "Sort of L1L2 says everything else will be on, but L1 says that's OK in terms of just strict sparsity.",
                    "label": 0
                },
                {
                    "sent": "L1 is going to give you a little better performance, but you have to compute more features so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did this for many datasets.",
                    "label": 0
                },
                {
                    "sent": "I'll just show you one other one.",
                    "label": 0
                },
                {
                    "sent": "This is the pen digit handwritten digit classification data set.",
                    "label": 0
                },
                {
                    "sent": "So we just need to classify what someone's writing is like, 0 through 9 or maybe a through ZI, don't remember.",
                    "label": 0
                },
                {
                    "sent": "I forget what the datasets all are, but we see similar behaviour.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you're concerned about reducing the number of features that you actually need to compute, then these mixed norm regularizer seem to give you a lot more bang for your Buck.",
                    "label": 0
                },
                {
                    "sent": "Then the L1 regularizer, which is kind of interesting and nice behavior.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, I said I'd talk a bit about.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The runtime behavior of these methods and this is this is related to.",
                    "label": 0
                },
                {
                    "sent": "How I said that you know these would do induction and then sort of.",
                    "label": 0
                },
                {
                    "sent": "In retrospect they would be able to specify remove Hypotheek hypothesis that in retrospect were not informative.",
                    "label": 0
                },
                {
                    "sent": "So this is indicative of that.",
                    "label": 0
                },
                {
                    "sent": "So what we did here this we just said OK run cyclically through all the features and this is on the MNIST handwritten digit data set and we gave 30,000 features.",
                    "label": 0
                },
                {
                    "sent": "So total 10 or 3000 features, so 10,000 total for all the classes.",
                    "label": 0
                },
                {
                    "sent": "And on the Landsat data set.",
                    "label": 0
                },
                {
                    "sent": "And so we said just cyclically run through all the features over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "OK, and so we see at the beginning sort of iterations like zero to 1000, you end up putting a lot of features in your models just the whole whole pile of.",
                    "label": 0
                },
                {
                    "sent": "I mean basically these things were getting up to almost totally non zero all the way to non 0.",
                    "label": 0
                },
                {
                    "sent": "But then as you keep running it they sort of revisit features and they just add sparsity as they go along.",
                    "label": 0
                },
                {
                    "sent": "Oh, and the reason there's four lines instead of just two for the datasets.",
                    "label": 0
                },
                {
                    "sent": "One of these is with the exponential bounds that I didn't present the standard Adaboost bounds and the other one is with the gradient based bounds.",
                    "label": 0
                },
                {
                    "sent": "But what we see is that basically.",
                    "label": 0
                },
                {
                    "sent": "You're removing features as you go along, but your test set loss your test error rates are not increasing at all.",
                    "label": 0
                },
                {
                    "sent": "So basically you improve a lot by adding all these features in and then you just sort of.",
                    "label": 0
                },
                {
                    "sent": "These methods seem to just spend their time removing features that you don't care about while not increasing your loss rates at all.",
                    "label": 0
                },
                {
                    "sent": "So we thought this was really nice behavior.",
                    "label": 0
                },
                {
                    "sent": "Seems pretty cool so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "That's it for my talk.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I've done is I've given you guys boosting variance for multi class binary multi class in binary logistic regression as well as linear regression.",
                    "label": 0
                },
                {
                    "sent": "Even though I didn't talk about that that much.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "These methods let us prune features that we've added.",
                    "label": 1
                },
                {
                    "sent": "They let us do feature induction through feature scoring and they are probably convergent.",
                    "label": 0
                },
                {
                    "sent": "They're going to get to the right answer, and now there's a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Questions still.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like I said, we don't have any convergence rates, we don't.",
                    "label": 0
                },
                {
                    "sent": "You know, I think they want to epsilon, but I'm not really sure I haven't.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unable to prove anything, and also there's some questions of consistency and whether we get better generalization by sort of doing this.",
                    "label": 0
                },
                {
                    "sent": "Removal of features or terminating the boosting process.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thanks very much and I'd love to take your questions.",
                    "label": 1
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Yum.",
                    "label": 0
                },
                {
                    "sent": "Is yelling at you proposing different than with a group that approximates the regularization path of like the norm?",
                    "label": 0
                },
                {
                    "sent": "I guess it.",
                    "label": 0
                },
                {
                    "sent": "I mean it depends on what algorithm most of the regularization path algorithms for linear regression, right?",
                    "label": 0
                },
                {
                    "sent": "I. I mean so it's.",
                    "label": 0
                },
                {
                    "sent": "I guess it's hard for me to say I don't know like which algorithms in particular are you thinking of?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Well so.",
                    "label": 0
                },
                {
                    "sent": "Like for example, like the algorithm or right, right, right?",
                    "label": 0
                },
                {
                    "sent": "So I guess most of those I thought were for linear regression, but but yeah, you can adapt them for generalized linear models, but you know they don't really give you like the feature induction properties, right?",
                    "label": 0
                },
                {
                    "sent": "Like this you could use with sort of a countably infinite set of features and still do feature scoring, induction, etc like that.",
                    "label": 0
                },
                {
                    "sent": "The feature scoring you could do would be with that.",
                    "label": 0
                },
                {
                    "sent": "I would assume this runs a little faster, but I didn't have any experiments on real runtime.",
                    "label": 0
                },
                {
                    "sent": "Sorry, that's an unsatisfying answer, but.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "At the beginning you would message not realizing.",
                    "label": 0
                },
                {
                    "sent": "Well, uh huh.",
                    "label": 0
                },
                {
                    "sent": "Article 2 any energy.",
                    "label": 0
                },
                {
                    "sent": "I mean, for example equal to 1.5 or three.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so sorry.",
                    "label": 0
                },
                {
                    "sent": "I guess it would depend on.",
                    "label": 0
                },
                {
                    "sent": "I mean certainly they would be applicable to that and they would be probably convergent.",
                    "label": 0
                },
                {
                    "sent": "It just depends on whether or not you can solve this problem for the P or considering and it's really easy with 1, two and Infinity.",
                    "label": 0
                },
                {
                    "sent": "You know this sort of standard thing, and 1.5 would probably give you similar performance to want to like L1L2 'cause you're still going to get block sparsity, but we didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't consider it 'cause it didn't seem unimportant.",
                    "label": 0
                },
                {
                    "sent": "OK, alright thank you.",
                    "label": 0
                }
            ]
        }
    }
}