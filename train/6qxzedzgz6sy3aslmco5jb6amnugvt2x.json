{
    "id": "6qxzedzgz6sy3aslmco5jb6amnugvt2x",
    "title": "Detecting Errors in Numerical Linked Data using Cross-Checked Outlier Detection",
    "info": {
        "author": [
            "Daniel Fleischhacker, Institut f\u00fcr Informatik, University of Mannheim"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_fleischhacker_detecting_errors/",
    "segmentation": [
        [
            "I'm talking about our work on detecting errors in American link data.",
            "Uh, which is work going on at the University of Mannheim in collaboration with Hiker Powell, Highmore Habra, Lyanna Folker and Kristen Bitzer.",
            "And regarding the motivation."
        ],
        [
            "We all know there is large or large amounts of linked data, so like you see here in the link data cloud, we have many repositories containing numerous different datasets.",
            "Many triples containing data and also, as in most cases, if you have much data than the probabilities that you're somehow introduce errors into these datasets is getting higher and higher, and the more data you have.",
            "For example, if you use some automatic approaches for getting the data, then you might.",
            "Introduced errors by heuristic processing.",
            "So, for example, you rely on a given format of the data you are trying to pass or something, and then you get errors in the final data.",
            "Or just as always, humans do errors.",
            "They could do mistakes introducing.",
            "Then yeah, Iran is well used interlink data sources.",
            "So our approach is that we have a look at this data and try to find problematic data.",
            "Problematic numerical values in our case, which we can try to highlight so that people afterwards can have a closer look at these.",
            "And can say, OK, that's actually an erroneous value.",
            "I should somehow fix it, or it's not an erroneous radio, so.",
            "That's our whole goal, and for this we have the assumption that even though property values might not always have some known probability distribution, they're following, we nevertheless have a typical behavior for most properties.",
            "So for example, the height of persons there, you know that it's in most cases in between half a meter and maybe up to 2 1/2 meter."
        ],
        [
            "For example, if we have a look at the pedia, we all know it's automatically extracted.",
            "And so we also have errors.",
            "So like in this case we have an automatic processing error where something with the brackets that go wrong and we had.",
            "Finally a population values of.",
            "Some million instead of just some thousands.",
            "But also there are errors introduced by humans, like in this case someone changed the population value and some kind of vandalism which would also go into DB pedia.",
            "Or someone did not pay attention to the language differences regarding thousands delimiters and decimal delimiters and copied data over from.",
            "In this case, the Portuguese Wikipedia to the English one.",
            "But also some kind of mixing up units or.",
            "Also, in this case, mixing up centimeters and meters.",
            "Which is all in the end arounds data.",
            "And yeah, that's what we're trying to detect.",
            "The overall framework of our."
        ],
        [
            "Approach is that we take all values for one property.",
            "And then we first work on this data set trying to determine the typical behavior for the values.",
            "And then finding values which do not adhere to it.",
            "So that's outlier detection.",
            "What we perform here then, in the second step, which is the actual error detection we try to.",
            "Reduce the data by filtering out so called natural outliers.",
            "I will come in some minutes to what we are doing here exactly.",
            "And finally we have a list of values with which are potentially around us."
        ],
        [
            "This whole approach has two main challenges.",
            "The first one is that in some cases the behavior of values depends on the type of instances.",
            "So for example, having a look here at this diagram in the upper right you see there are some population counts, but OK, there are two clusters of values.",
            "But All in all, yeah, it looks pretty good because you cannot really recognize some outlying values.",
            "However, if we do not pay attention to the type of the instances.",
            "We might miss some errors.",
            "So for example if we now have a look at the types.",
            "We have villages and cities.",
            "Cities are in blue and in this case we see there are two or there are some city values.",
            "Somewhere in the population count of villagers which are having a look at this city instances only somehow suspicious because there are much too low.",
            "Thus."
        ],
        [
            "Can challenge we are facing is that not all outliers are actual errors.",
            "There are so called natural outliers which are just extraordinary values which are nevertheless correct.",
            "So for example, having a look at the islands of Japan, there are some main islands like Han Chu or key issue who have around some million of inhabitants.",
            "So for example, Honshu is the largest one, about 100 million inhabitants.",
            "And then there are some smaller islands and there is a large amount of really small islands having only some hundreds of inhabitants.",
            "Applying natural, applying outlier detection to this we would find out that honcho is an outlier.",
            "But yeah, actually it's correct.",
            "So we have to somehow deal with this and detect that it's only natural outlier, but not in Iran's value.",
            "As I said, our whole approach."
        ],
        [
            "This is based on outlier detection.",
            "In this first task.",
            "And oddly, detection is basically finding patterns in data that do not conform to the expected normal behavior in our case, because we don't have any labeled data, we have to use unsupervised outlier detection.",
            "So we're trying to find the pattern in the data we have and then identifying values which do not conform to this pattern.",
            "In our special case, we are for example using local outlier factor, which is an approach for getting outlier scores and it's using the neighborhood of of values to find out whether one specific value has a much higher distance from its neighbors than other neighbors.",
            "I'm in."
        ],
        [
            "In this respect, our approach is based on the work, but also we not on power line which has been presented at the SWC, but the main difference is that we identify these two challenges or we are addressing these two challenges.",
            "First, we have a look at efficient method measures to identify meaningful subpopulations, meaning hard to reduce to some classes and some subsets of values, and we are filtering out natural outliers."
        ],
        [
            "Regarding the first challenge, as we already saw in the example before, it's a good idea to have a look at types, for example, so we could restrict the instances and values we are looking at by for example, only having a look at a specific type like only on continents we could have a look at specific properties, so only values which are subject of a property, for example and a matter are considered.",
            "Or we could also very specific have a look at properties and values.",
            "So for example, only instances which are assigned to the United Arabian Emirates using a country property becausw.",
            "For example, if you have a look at the temperature, probably cities lying in the United Arabian Emirates have a higher temperature than other cities.",
            "If we allow combinations of these different restrictions, we arrive at the problem that we have a very high number of possible combinations which make it somehow computationally complex.",
            "Because of."
        ],
        [
            "Yes, we are introducing the subpopulation letters where we start with just all instances for a given property, and then we stepwise restricte instances.",
            "For example, we restrict like.",
            "I'm here OK so like having a look at this first node where we restrict it to continents.",
            "Then we have one restricting to a city so we could only have a look at these instance values.",
            "Then we generate for each of these notes the histogram and we apply several pruning criteria using this histograms.",
            "For example, one of the easiest pruning criterions is just that we have a look at the number of instances which are.",
            "The change in number of instances.",
            "For example, if we do not restrict by any instances, then we don't have to have a look at this node be cause for example, having a look at population then probably restriction to populated plays is not that meaningful.",
            "The second criterion is that we have a look at the number of remaining instances, because if we only have 5 instances, for example, our Lie detection is not meaningful at all.",
            "And finally we have a look at the card back, slightly divergent between the parent node and a new node.",
            "Cause if we assume that if the divergences very low.",
            "Then we just changed, not the distribution of values, but only the number of instances.",
            "For example, in our case, having a look at a city and then only restricting two places starting with a T. So in the name, then it's probably not really changed in the behavior, but only yeah, as I said in number of instances.",
            "Finally, we detect outliers for each unpruned note.",
            "And then we use the scores we have here based on several selection criterion's.",
            "For example, we can just have a look at the most specific restriction for each instance and use this outlier score.",
            "Or we could use some other metrics for finding different outlier scores and rank the values according to these."
        ],
        [
            "Regarding the second, regarding the second challenge, we are actually using one of the main features of linked data, which is we have links.",
            "We have different instances, and many of these instances are interconnected by means of same as links to other datasets, and also these datasets are not always disjoint, but they might also describe, for example, the same properties.",
            "So like if we go to another data set, we might also find data about the population.",
            "So and in our case, if we have a something which is suspicious of being an outlier and we want to check whether it's natural outlier, we start at the instance we follow same as links, we get additional property values.",
            "Like in this example for Hunter we got more values from other datasets and then we have a look at this data and try to find out whether it's an outlier here.",
            "In our case, all values are more or less the same.",
            "So yeah, we conclude that it's only natural outlier cause more or less the data is confirmed by the other datasets.",
            "And so we can leave this out when we're trying to detect errors and say, OK, Hon chew, it's a correct value."
        ],
        [
            "We performed experiments on the DB Pedia 39 in data set in English, and we use the additional language versions of the PDF for this cross checking step.",
            "So for getting additional information about different property values.",
            "We we evaluated on the properties high population, total and elevation.",
            "For seeing how our approach performs.",
            "And forgetting subpopulations, we use the YAGO classes, which are very specific, so we were able to do really get really specific subpopulations.",
            "And after running our approach on this, we manually assessed 100 values for each property.",
            "Regarding the correctness there we had a slight bias towards more highly ranked, well used, because otherwise there the number of wrong values in the Holy Pedia data set is just too low that you can really meaningfully assess them without any bias.",
            "We compared our approaches to the two 2 baseline approaches.",
            "The first one was just using the values determining the median value and then using the distance from the median value as a score.",
            "And the second one was because we models used only the multilingual data for detecting outliers.",
            "We used this as a second baseline.",
            "The results were that we."
        ],
        [
            "In all cases, the approach was proposed by us were better than both baseline approaches, but I have to say that the baseline approaches are really strong in this case, so many errors are detectable by the baselines.",
            "But still we are improving.",
            "Oh, compared to the baselines and also we see that cross checking filters out natural outliers.",
            "The main problem for the filtering step is that the overlap between different language versions.",
            "In our case, for example, is not too high, so as we see for example, by the way, that's the evaluation based on area under the curve.",
            "That's maybe something I should mention.",
            "I'm.",
            "And the Yep, so the problem is that it's cross checking step suffers if there is not enough identical data in different datasets.",
            "As we see for example for the property height, there is not really much improvement becausw we also see the multilingual baseline is just really bad in this case.",
            "On the other side, population total there is much multilingual data as we see from this multilingual baseline and also there we see that we are really improving compared to the non crosschecked approach.",
            "In some cases, like for example in elevation, we see a slight decrease from outlier detection to this cross check version.",
            "But that's that can be traced back to.",
            "It seems that some people are just copying over data, for example from the English Wikipedia to others, and then you like spreading the error through all language versions.",
            "Which finally man makes a bit problematic in this case for this cross checked version.",
            "However, if that's really common pattern, you could think about applying something like a copy detection algorithm for a bit, limiting the effect.",
            "Um, yeah."
        ],
        [
            "And that's thank you."
        ],
        [
            "The villages city example you gave is pretty cool, but there are also cases in real life like consider income.",
            "Yeah, like so if you consider the whole world population you'd get a normal distribution.",
            "If you consider income like within a population in a country, you also get a normal distribution.",
            "So in your original example, you can actually because it's by model you can.",
            "Easy to get these two subclasses, but like how would you get it?",
            "For instance for for income because you get a normal distribution in the beginning and without knowing the country you can't really.",
            "OK, but we are so we're always taking the values for given instances and we hope that these instances have type assertions.",
            "Assertions for example.",
            "Or like we we had this property and value restriction.",
            "So we as long as the data is really is not true.",
            "Sparse for the definition.",
            "So like if we don't know that some instance represents a person from the USA, then we cannot.",
            "Then we cannot do something.",
            "But if the data is there, we could use these restrictions to mallus limit down the instance and the world is well having a look at thank you.",
            "Thanks for the talk, um.",
            "Somewhere you mentioned that you are you're checking some values across multiple resources, so you have example of Honshu that has a population that is very large.",
            "So here's an idea about maybe a supervised learning method and so is it possible to do this kind of checking for all the values and you pick the ones that are consistent across all these different resources and use them as training data?",
            "How about that?",
            "Do you think this is feasible?",
            "You mean just based on the multilingual data and then using supervised approaches?",
            "Or I think it doesn't matter about language, is just just whatever properties you that you check are consistent across all the different different datasets.",
            "Like for example Home show you have checked for four different datasets, you see that is a correct consistent value.",
            "You do this checking on any other values you are you having a data set, you keep the ones that are consistent across all datasets.",
            "Use them as training data.",
            "It might not give you a big data set, but maybe something first.",
            "Start with us as a supervised approach.",
            "Thing that could be interesting yet sure we always also considering including learning in the overall process so that so like having feedback, getting feedback from people and then introducing this for selecting for example, one of these outlier scores or for aggregating them using some weights if we use some classes may be very error prone.",
            "Then we could introduce their own.",
            "Different weights depending on for which subpopulation we detected the problem.",
            "And maybe we could.",
            "We could All in all combine your.",
            "So your approach, your idea with something like this.",
            "So like including feedback also.",
            "It seems that you have many user defined parameters like writs of the bins in the histograms and also some distances.",
            "Have you checked your performance?",
            "Depending on these parameters or how do you choose them?",
            "So we we checked it based on.",
            "We performed several, performed several experiments, and.",
            "So what we for example for the bin sized it was finding something like we had a balance between runtime because if you have smaller bins then you just get larger runtimes because we have much more to compute for KL divergent for example, and on the other side for having a look at pruning.",
            "So how many notes get pruned and how meaningful is this pruning?",
            "So yeah, that's.",
            "One problem in this whole approach, they will have to specify some para meters, but on the other side, yeah.",
            "So maybe if we can include feedback from the outside then that might.",
            "We might be able to use this for getting better para meters, but right now from because it's overall more like an unsupervised approach it we have just set some parameters.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm talking about our work on detecting errors in American link data.",
                    "label": 1
                },
                {
                    "sent": "Uh, which is work going on at the University of Mannheim in collaboration with Hiker Powell, Highmore Habra, Lyanna Folker and Kristen Bitzer.",
                    "label": 0
                },
                {
                    "sent": "And regarding the motivation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We all know there is large or large amounts of linked data, so like you see here in the link data cloud, we have many repositories containing numerous different datasets.",
                    "label": 1
                },
                {
                    "sent": "Many triples containing data and also, as in most cases, if you have much data than the probabilities that you're somehow introduce errors into these datasets is getting higher and higher, and the more data you have.",
                    "label": 0
                },
                {
                    "sent": "For example, if you use some automatic approaches for getting the data, then you might.",
                    "label": 1
                },
                {
                    "sent": "Introduced errors by heuristic processing.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you rely on a given format of the data you are trying to pass or something, and then you get errors in the final data.",
                    "label": 0
                },
                {
                    "sent": "Or just as always, humans do errors.",
                    "label": 0
                },
                {
                    "sent": "They could do mistakes introducing.",
                    "label": 0
                },
                {
                    "sent": "Then yeah, Iran is well used interlink data sources.",
                    "label": 0
                },
                {
                    "sent": "So our approach is that we have a look at this data and try to find problematic data.",
                    "label": 0
                },
                {
                    "sent": "Problematic numerical values in our case, which we can try to highlight so that people afterwards can have a closer look at these.",
                    "label": 0
                },
                {
                    "sent": "And can say, OK, that's actually an erroneous value.",
                    "label": 0
                },
                {
                    "sent": "I should somehow fix it, or it's not an erroneous radio, so.",
                    "label": 0
                },
                {
                    "sent": "That's our whole goal, and for this we have the assumption that even though property values might not always have some known probability distribution, they're following, we nevertheless have a typical behavior for most properties.",
                    "label": 1
                },
                {
                    "sent": "So for example, the height of persons there, you know that it's in most cases in between half a meter and maybe up to 2 1/2 meter.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, if we have a look at the pedia, we all know it's automatically extracted.",
                    "label": 0
                },
                {
                    "sent": "And so we also have errors.",
                    "label": 0
                },
                {
                    "sent": "So like in this case we have an automatic processing error where something with the brackets that go wrong and we had.",
                    "label": 0
                },
                {
                    "sent": "Finally a population values of.",
                    "label": 0
                },
                {
                    "sent": "Some million instead of just some thousands.",
                    "label": 0
                },
                {
                    "sent": "But also there are errors introduced by humans, like in this case someone changed the population value and some kind of vandalism which would also go into DB pedia.",
                    "label": 0
                },
                {
                    "sent": "Or someone did not pay attention to the language differences regarding thousands delimiters and decimal delimiters and copied data over from.",
                    "label": 0
                },
                {
                    "sent": "In this case, the Portuguese Wikipedia to the English one.",
                    "label": 0
                },
                {
                    "sent": "But also some kind of mixing up units or.",
                    "label": 0
                },
                {
                    "sent": "Also, in this case, mixing up centimeters and meters.",
                    "label": 0
                },
                {
                    "sent": "Which is all in the end arounds data.",
                    "label": 0
                },
                {
                    "sent": "And yeah, that's what we're trying to detect.",
                    "label": 0
                },
                {
                    "sent": "The overall framework of our.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach is that we take all values for one property.",
                    "label": 0
                },
                {
                    "sent": "And then we first work on this data set trying to determine the typical behavior for the values.",
                    "label": 0
                },
                {
                    "sent": "And then finding values which do not adhere to it.",
                    "label": 0
                },
                {
                    "sent": "So that's outlier detection.",
                    "label": 0
                },
                {
                    "sent": "What we perform here then, in the second step, which is the actual error detection we try to.",
                    "label": 0
                },
                {
                    "sent": "Reduce the data by filtering out so called natural outliers.",
                    "label": 0
                },
                {
                    "sent": "I will come in some minutes to what we are doing here exactly.",
                    "label": 0
                },
                {
                    "sent": "And finally we have a list of values with which are potentially around us.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This whole approach has two main challenges.",
                    "label": 0
                },
                {
                    "sent": "The first one is that in some cases the behavior of values depends on the type of instances.",
                    "label": 0
                },
                {
                    "sent": "So for example, having a look here at this diagram in the upper right you see there are some population counts, but OK, there are two clusters of values.",
                    "label": 0
                },
                {
                    "sent": "But All in all, yeah, it looks pretty good because you cannot really recognize some outlying values.",
                    "label": 0
                },
                {
                    "sent": "However, if we do not pay attention to the type of the instances.",
                    "label": 0
                },
                {
                    "sent": "We might miss some errors.",
                    "label": 0
                },
                {
                    "sent": "So for example if we now have a look at the types.",
                    "label": 0
                },
                {
                    "sent": "We have villages and cities.",
                    "label": 0
                },
                {
                    "sent": "Cities are in blue and in this case we see there are two or there are some city values.",
                    "label": 0
                },
                {
                    "sent": "Somewhere in the population count of villagers which are having a look at this city instances only somehow suspicious because there are much too low.",
                    "label": 0
                },
                {
                    "sent": "Thus.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can challenge we are facing is that not all outliers are actual errors.",
                    "label": 1
                },
                {
                    "sent": "There are so called natural outliers which are just extraordinary values which are nevertheless correct.",
                    "label": 1
                },
                {
                    "sent": "So for example, having a look at the islands of Japan, there are some main islands like Han Chu or key issue who have around some million of inhabitants.",
                    "label": 0
                },
                {
                    "sent": "So for example, Honshu is the largest one, about 100 million inhabitants.",
                    "label": 0
                },
                {
                    "sent": "And then there are some smaller islands and there is a large amount of really small islands having only some hundreds of inhabitants.",
                    "label": 0
                },
                {
                    "sent": "Applying natural, applying outlier detection to this we would find out that honcho is an outlier.",
                    "label": 0
                },
                {
                    "sent": "But yeah, actually it's correct.",
                    "label": 0
                },
                {
                    "sent": "So we have to somehow deal with this and detect that it's only natural outlier, but not in Iran's value.",
                    "label": 0
                },
                {
                    "sent": "As I said, our whole approach.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is based on outlier detection.",
                    "label": 0
                },
                {
                    "sent": "In this first task.",
                    "label": 0
                },
                {
                    "sent": "And oddly, detection is basically finding patterns in data that do not conform to the expected normal behavior in our case, because we don't have any labeled data, we have to use unsupervised outlier detection.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to find the pattern in the data we have and then identifying values which do not conform to this pattern.",
                    "label": 0
                },
                {
                    "sent": "In our special case, we are for example using local outlier factor, which is an approach for getting outlier scores and it's using the neighborhood of of values to find out whether one specific value has a much higher distance from its neighbors than other neighbors.",
                    "label": 0
                },
                {
                    "sent": "I'm in.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this respect, our approach is based on the work, but also we not on power line which has been presented at the SWC, but the main difference is that we identify these two challenges or we are addressing these two challenges.",
                    "label": 0
                },
                {
                    "sent": "First, we have a look at efficient method measures to identify meaningful subpopulations, meaning hard to reduce to some classes and some subsets of values, and we are filtering out natural outliers.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the first challenge, as we already saw in the example before, it's a good idea to have a look at types, for example, so we could restrict the instances and values we are looking at by for example, only having a look at a specific type like only on continents we could have a look at specific properties, so only values which are subject of a property, for example and a matter are considered.",
                    "label": 0
                },
                {
                    "sent": "Or we could also very specific have a look at properties and values.",
                    "label": 0
                },
                {
                    "sent": "So for example, only instances which are assigned to the United Arabian Emirates using a country property becausw.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a look at the temperature, probably cities lying in the United Arabian Emirates have a higher temperature than other cities.",
                    "label": 0
                },
                {
                    "sent": "If we allow combinations of these different restrictions, we arrive at the problem that we have a very high number of possible combinations which make it somehow computationally complex.",
                    "label": 0
                },
                {
                    "sent": "Because of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, we are introducing the subpopulation letters where we start with just all instances for a given property, and then we stepwise restricte instances.",
                    "label": 0
                },
                {
                    "sent": "For example, we restrict like.",
                    "label": 0
                },
                {
                    "sent": "I'm here OK so like having a look at this first node where we restrict it to continents.",
                    "label": 0
                },
                {
                    "sent": "Then we have one restricting to a city so we could only have a look at these instance values.",
                    "label": 0
                },
                {
                    "sent": "Then we generate for each of these notes the histogram and we apply several pruning criteria using this histograms.",
                    "label": 0
                },
                {
                    "sent": "For example, one of the easiest pruning criterions is just that we have a look at the number of instances which are.",
                    "label": 0
                },
                {
                    "sent": "The change in number of instances.",
                    "label": 0
                },
                {
                    "sent": "For example, if we do not restrict by any instances, then we don't have to have a look at this node be cause for example, having a look at population then probably restriction to populated plays is not that meaningful.",
                    "label": 0
                },
                {
                    "sent": "The second criterion is that we have a look at the number of remaining instances, because if we only have 5 instances, for example, our Lie detection is not meaningful at all.",
                    "label": 0
                },
                {
                    "sent": "And finally we have a look at the card back, slightly divergent between the parent node and a new node.",
                    "label": 0
                },
                {
                    "sent": "Cause if we assume that if the divergences very low.",
                    "label": 0
                },
                {
                    "sent": "Then we just changed, not the distribution of values, but only the number of instances.",
                    "label": 0
                },
                {
                    "sent": "For example, in our case, having a look at a city and then only restricting two places starting with a T. So in the name, then it's probably not really changed in the behavior, but only yeah, as I said in number of instances.",
                    "label": 0
                },
                {
                    "sent": "Finally, we detect outliers for each unpruned note.",
                    "label": 0
                },
                {
                    "sent": "And then we use the scores we have here based on several selection criterion's.",
                    "label": 0
                },
                {
                    "sent": "For example, we can just have a look at the most specific restriction for each instance and use this outlier score.",
                    "label": 0
                },
                {
                    "sent": "Or we could use some other metrics for finding different outlier scores and rank the values according to these.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regarding the second, regarding the second challenge, we are actually using one of the main features of linked data, which is we have links.",
                    "label": 0
                },
                {
                    "sent": "We have different instances, and many of these instances are interconnected by means of same as links to other datasets, and also these datasets are not always disjoint, but they might also describe, for example, the same properties.",
                    "label": 0
                },
                {
                    "sent": "So like if we go to another data set, we might also find data about the population.",
                    "label": 0
                },
                {
                    "sent": "So and in our case, if we have a something which is suspicious of being an outlier and we want to check whether it's natural outlier, we start at the instance we follow same as links, we get additional property values.",
                    "label": 0
                },
                {
                    "sent": "Like in this example for Hunter we got more values from other datasets and then we have a look at this data and try to find out whether it's an outlier here.",
                    "label": 0
                },
                {
                    "sent": "In our case, all values are more or less the same.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we conclude that it's only natural outlier cause more or less the data is confirmed by the other datasets.",
                    "label": 0
                },
                {
                    "sent": "And so we can leave this out when we're trying to detect errors and say, OK, Hon chew, it's a correct value.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We performed experiments on the DB Pedia 39 in data set in English, and we use the additional language versions of the PDF for this cross checking step.",
                    "label": 0
                },
                {
                    "sent": "So for getting additional information about different property values.",
                    "label": 0
                },
                {
                    "sent": "We we evaluated on the properties high population, total and elevation.",
                    "label": 0
                },
                {
                    "sent": "For seeing how our approach performs.",
                    "label": 0
                },
                {
                    "sent": "And forgetting subpopulations, we use the YAGO classes, which are very specific, so we were able to do really get really specific subpopulations.",
                    "label": 0
                },
                {
                    "sent": "And after running our approach on this, we manually assessed 100 values for each property.",
                    "label": 0
                },
                {
                    "sent": "Regarding the correctness there we had a slight bias towards more highly ranked, well used, because otherwise there the number of wrong values in the Holy Pedia data set is just too low that you can really meaningfully assess them without any bias.",
                    "label": 0
                },
                {
                    "sent": "We compared our approaches to the two 2 baseline approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one was just using the values determining the median value and then using the distance from the median value as a score.",
                    "label": 0
                },
                {
                    "sent": "And the second one was because we models used only the multilingual data for detecting outliers.",
                    "label": 0
                },
                {
                    "sent": "We used this as a second baseline.",
                    "label": 0
                },
                {
                    "sent": "The results were that we.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In all cases, the approach was proposed by us were better than both baseline approaches, but I have to say that the baseline approaches are really strong in this case, so many errors are detectable by the baselines.",
                    "label": 0
                },
                {
                    "sent": "But still we are improving.",
                    "label": 0
                },
                {
                    "sent": "Oh, compared to the baselines and also we see that cross checking filters out natural outliers.",
                    "label": 0
                },
                {
                    "sent": "The main problem for the filtering step is that the overlap between different language versions.",
                    "label": 0
                },
                {
                    "sent": "In our case, for example, is not too high, so as we see for example, by the way, that's the evaluation based on area under the curve.",
                    "label": 0
                },
                {
                    "sent": "That's maybe something I should mention.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "And the Yep, so the problem is that it's cross checking step suffers if there is not enough identical data in different datasets.",
                    "label": 0
                },
                {
                    "sent": "As we see for example for the property height, there is not really much improvement becausw we also see the multilingual baseline is just really bad in this case.",
                    "label": 0
                },
                {
                    "sent": "On the other side, population total there is much multilingual data as we see from this multilingual baseline and also there we see that we are really improving compared to the non crosschecked approach.",
                    "label": 0
                },
                {
                    "sent": "In some cases, like for example in elevation, we see a slight decrease from outlier detection to this cross check version.",
                    "label": 0
                },
                {
                    "sent": "But that's that can be traced back to.",
                    "label": 0
                },
                {
                    "sent": "It seems that some people are just copying over data, for example from the English Wikipedia to others, and then you like spreading the error through all language versions.",
                    "label": 0
                },
                {
                    "sent": "Which finally man makes a bit problematic in this case for this cross checked version.",
                    "label": 0
                },
                {
                    "sent": "However, if that's really common pattern, you could think about applying something like a copy detection algorithm for a bit, limiting the effect.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's thank you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The villages city example you gave is pretty cool, but there are also cases in real life like consider income.",
                    "label": 0
                },
                {
                    "sent": "Yeah, like so if you consider the whole world population you'd get a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "If you consider income like within a population in a country, you also get a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So in your original example, you can actually because it's by model you can.",
                    "label": 0
                },
                {
                    "sent": "Easy to get these two subclasses, but like how would you get it?",
                    "label": 0
                },
                {
                    "sent": "For instance for for income because you get a normal distribution in the beginning and without knowing the country you can't really.",
                    "label": 0
                },
                {
                    "sent": "OK, but we are so we're always taking the values for given instances and we hope that these instances have type assertions.",
                    "label": 0
                },
                {
                    "sent": "Assertions for example.",
                    "label": 0
                },
                {
                    "sent": "Or like we we had this property and value restriction.",
                    "label": 0
                },
                {
                    "sent": "So we as long as the data is really is not true.",
                    "label": 0
                },
                {
                    "sent": "Sparse for the definition.",
                    "label": 0
                },
                {
                    "sent": "So like if we don't know that some instance represents a person from the USA, then we cannot.",
                    "label": 0
                },
                {
                    "sent": "Then we cannot do something.",
                    "label": 0
                },
                {
                    "sent": "But if the data is there, we could use these restrictions to mallus limit down the instance and the world is well having a look at thank you.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the talk, um.",
                    "label": 0
                },
                {
                    "sent": "Somewhere you mentioned that you are you're checking some values across multiple resources, so you have example of Honshu that has a population that is very large.",
                    "label": 0
                },
                {
                    "sent": "So here's an idea about maybe a supervised learning method and so is it possible to do this kind of checking for all the values and you pick the ones that are consistent across all these different resources and use them as training data?",
                    "label": 0
                },
                {
                    "sent": "How about that?",
                    "label": 0
                },
                {
                    "sent": "Do you think this is feasible?",
                    "label": 0
                },
                {
                    "sent": "You mean just based on the multilingual data and then using supervised approaches?",
                    "label": 0
                },
                {
                    "sent": "Or I think it doesn't matter about language, is just just whatever properties you that you check are consistent across all the different different datasets.",
                    "label": 0
                },
                {
                    "sent": "Like for example Home show you have checked for four different datasets, you see that is a correct consistent value.",
                    "label": 0
                },
                {
                    "sent": "You do this checking on any other values you are you having a data set, you keep the ones that are consistent across all datasets.",
                    "label": 0
                },
                {
                    "sent": "Use them as training data.",
                    "label": 0
                },
                {
                    "sent": "It might not give you a big data set, but maybe something first.",
                    "label": 0
                },
                {
                    "sent": "Start with us as a supervised approach.",
                    "label": 0
                },
                {
                    "sent": "Thing that could be interesting yet sure we always also considering including learning in the overall process so that so like having feedback, getting feedback from people and then introducing this for selecting for example, one of these outlier scores or for aggregating them using some weights if we use some classes may be very error prone.",
                    "label": 0
                },
                {
                    "sent": "Then we could introduce their own.",
                    "label": 0
                },
                {
                    "sent": "Different weights depending on for which subpopulation we detected the problem.",
                    "label": 0
                },
                {
                    "sent": "And maybe we could.",
                    "label": 0
                },
                {
                    "sent": "We could All in all combine your.",
                    "label": 0
                },
                {
                    "sent": "So your approach, your idea with something like this.",
                    "label": 0
                },
                {
                    "sent": "So like including feedback also.",
                    "label": 0
                },
                {
                    "sent": "It seems that you have many user defined parameters like writs of the bins in the histograms and also some distances.",
                    "label": 0
                },
                {
                    "sent": "Have you checked your performance?",
                    "label": 0
                },
                {
                    "sent": "Depending on these parameters or how do you choose them?",
                    "label": 0
                },
                {
                    "sent": "So we we checked it based on.",
                    "label": 0
                },
                {
                    "sent": "We performed several, performed several experiments, and.",
                    "label": 0
                },
                {
                    "sent": "So what we for example for the bin sized it was finding something like we had a balance between runtime because if you have smaller bins then you just get larger runtimes because we have much more to compute for KL divergent for example, and on the other side for having a look at pruning.",
                    "label": 0
                },
                {
                    "sent": "So how many notes get pruned and how meaningful is this pruning?",
                    "label": 0
                },
                {
                    "sent": "So yeah, that's.",
                    "label": 0
                },
                {
                    "sent": "One problem in this whole approach, they will have to specify some para meters, but on the other side, yeah.",
                    "label": 0
                },
                {
                    "sent": "So maybe if we can include feedback from the outside then that might.",
                    "label": 0
                },
                {
                    "sent": "We might be able to use this for getting better para meters, but right now from because it's overall more like an unsupervised approach it we have just set some parameters.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}