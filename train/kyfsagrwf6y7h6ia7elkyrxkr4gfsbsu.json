{
    "id": "kyfsagrwf6y7h6ia7elkyrxkr4gfsbsu",
    "title": "Adaptive Sequential Bayesian Change-point Detection",
    "info": {
        "author": [
            "Ryan Turner, University of Cambridge"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_turner_asbcpd/",
    "segmentation": [
        [
            "Joint work with Eunice Saatchi and Carl Rasmussen, we all were able to contribute one active to our project.",
            "So first I'm going to talk about some of the.",
            "That's not what I want to talk about.",
            "So what I want to talk about with some of the motivation for some of the things we want our method to achieve."
        ],
        [
            "So we want to handle non stationary time series because as well as anybody in here probably knows any real world Time series you tend to run into this thing where you have a model that works.",
            "And then there's something in the system that changes and then it doesn't work anymore.",
            "So we need to be able to sort of automatically erase that sort of old training data for the old system and recognize that things have changed.",
            "Now we want to avoid making point estimates of the parameters.",
            "Because we assume that maybe some change points might happen frequently enough that an isn't going to grow large, and we're never going to be able to make a reliable estimate, and we want our framework to be modular so we can use it in different contexts.",
            "I'll get back to.",
            "That will become more clear later, exactly our modularity.",
            "We want to be efficient, tractable, so we did without any heavy duty MCMC, and we want to do online applications, and we want to have enough.",
            "Faith that we made probabilistic predictions so that we can actually.",
            "People have something that we can trust with.",
            "And we want to have minimal hand tuning and that was the main contribution over the previous of the work that we've been extending, so this is a pretty good bill of goods here, so to speak.",
            "These two things here were have been faulted by some as being sort of key flaws instead of the copula models and credit default swaps, and that you have these correlation structure in the mortgage market which used to be true, but.",
            "Wasn't anymore.",
            "He also made point estimates of it that way."
        ],
        [
            "So there is a few key ingredients here to our algorithm.",
            "The key terms that we use is the run length.",
            "That's what best when we compute our posteriors, so the run length is the time between the time since the last change point.",
            "Some Cholera regene change the other term even use.",
            "So for instance, here we have.",
            "This is some synthetic data for the illustrate illustrate with from our model, and we had here a changing mean, and this is the run length since the last change point.",
            "And it simply just increments one every time.",
            "So we have underlying predictive model and that's the model that we solve the mission model that you predict from within regime given the parameters within within that receipt.",
            "So in this case here we used was just the Gaussian and the mean the variance change between these routines.",
            "We also had the hazard function which is how which is the how likely change planes to happen at each point in time and each hazard function.",
            "Implying different interarrival distribution on change point and in this synthetic data is just a constant with hazard function, which implies a geometric interarrival distribution."
        ],
        [
            "So there's been previous work as you can see from all the other talks before me, so they then test based approaches with another basic questions that have been retrospective and we want to be on line and the keyword that we build upon is added to mechanic paper from 2007 and they did not address the hyperparameter learning issue.",
            "We've kind of seen from our experiments that it can be sensitive.",
            "The hyperparameters, which is a little bit disturbing if you have to.",
            "And tune them.",
            "And this is kind of like using a Gaussian process regression model and sort of handpicking the hyperparameters in that it's usually not as good as actually learning your hyperparameters."
        ],
        [
            "So this is basically the core of the inference step in the algorithm.",
            "So one of the key parts we work with is this predictive distribution of the next time point.",
            "Given all your past data and we just.",
            "Marginalized out the last well what the run length is.",
            "So for any given run link, it's just a standard posterior predictive operation to get your prediction on the next data.",
            "I guess that was another change.",
            "So then we end together total prediction that is weighted by our post current posterior over the run links and then we also can put in this convenient notation here, which is the we summarize just the X points since since the last change point with that.",
            "So we work with the message passing scheme.",
            "Here that is the joint distribution of the run length and the data which we just expand out with the joint distribution between the run length in the past run Lane we use uh some here, but this is in all cases except for the hypothesis of the current run length is is 0.",
            "This sum is just one term.",
            "So for instance if you're looking at the probability that the current run length is 5, that miss mean the last friend Link was four 'cause.",
            "Other possibility now in the case that you're testing hypothesis that this is a current run length of 0, then there can be any possible pass running because it could be anything that changed back to zero and then we just use the rules of our ability to expand this out into a hazard function and a likelihood or the UPN as we call it and multiply it times the last message.",
            "And then we can normalize that to get our conditional distribution on the run like given the data.",
            "Now this is now in the simple case that use a constant hazard function.",
            "The intuition here is that you just take your previous hypothesis probability of previous houses on all the past run links.",
            "The time since the last change point and reweigh them depending on how well they predicted the current data point and that this gives a slight relating to the hypothesis.",
            "And then he had an cousin hazard.",
            "You can add a little extra."
        ],
        [
            "So the the key contribution here is that we want to do learning to learn the hyperparameters.",
            "So for instance, in the UPN case, for instance, if you you kind of Gaussian with changing the variance that we're going to have to deal with the fact that we need to put a prior on that meeting, that variance and those are going to have two hyperparameters each.",
            "And then we also have high pressures and how they were dealing with five hyperparameters and we don't want to hand pick that hand handset it so.",
            "We maximized maximized the log marginal likelihood, also known as the evidence to do that.",
            "The intuition here is that.",
            "If you want to be even more Bayesian about it and integrate the full posterior.",
            "You can do that, but if it's the post series very peak then we just get revert back to what we're doing here, which is just find the maximum posterior the marginal likelihood.",
            "So we can decompose the marginal like that in this case into the sum of 1 step ahead predictive distributions and the log likelihood, and this is something that is not very hard to do here, because the UPS automatically give us this equity given X. Oh one T -- 1.",
            "Distribution there, so this is the case where the evidence is actually a tractable thing to compute.",
            "So we have this whole message passing scheme and the inputs in those messages are the probabilities from the UPM in the House are function.",
            "So if we just take the derivatives of those, we can just use the chain rule to propagate the derivatives forward through time, just like we're passing messages for your time."
        ],
        [
            "So that's the way we do an after we just, you know, not even mutations just throw that into a constant gradient optimizer to maximize the hyperparameters.",
            "Now we can improve that a little bit more with this idea of pruning.",
            "So a naive implementation to be ordered T squared, because at any point T you have to compare the hypothesis that the that there could be any change point between time zero and T. So that's the half life policies and you and you have to re weigh those.",
            "And everytime septets T squared now in practice that is a very peaked posterior, so you can sort of get rid of the messages that are sort of redundant and get down to the linear scale.",
            "So I mean intuitively, if you're if you've been looking at stock returns for last year and you're looking at change points.",
            "Probably I process it.",
            "There's a change .6 months ago and six months ago minus one day isn't going to be all isn't going to be very much different at that time scale, so you can sort of just keep one of them.",
            "That's that's good enough.",
            "In practice you'll actually see a lot of hypotheses once you go down, and clicking on any of my 20 or something.",
            "And then it's just a waste of time to work with them, and it's highly modular.",
            "We can work on any improving any hazard function and any MoD presence here predict.",
            "If so, you can use the Gaussian process regression.",
            "So so this isn't just restricted to IID data, so you can use like.",
            "Have a linear trend and then you can have that trend change so you cannot change in linear trends or even nonlinear translating Gaussian process case.",
            "Now if you want to be if you want to not have any parametric assumption of the density, this is a module in a fully Bayesian value you really want to do is say process picture or something like that, but to keep it keep complication down you can just use a kernel density estimate.",
            "Probably the same thing.",
            "And you can also cache a lot of things because a lot of these run links complications, while normalization constants that only depend on the run length and the hyperparameters and not data, and that you're going to be doing computation over and over again.",
            "So you can sort of cash a lot of your normally constant speed things up."
        ],
        [
            "So we tried this on the well log data which is used in the original Addams Mackay Paper.",
            "Or they had the hantik parameters.",
            "We use the.",
            "We also generalize things a little bit with the logistic after function, so we don't.",
            "We aren't committing ourselves to a geometric interarrival.",
            "Time of change points.",
            "This is a more flexible distribution.",
            "We use an IID Gaussian DPM and try to catch changes in the mean and the variance.",
            "We have a graph here of the posterior that we got the filtered posterior.",
            "So this is the post here.",
            "Given only the past data.",
            "This is that we we plotted the CDF here in the original Series A lot of PDF, so the transition from black to white here means a peak in the posterior.",
            "We all the red line here corresponds to the comedian.",
            "Listeria, which we found to be a much better summary of posterior than using the mode in the mean.",
            "To give results that looks a lot better."
        ],
        [
            "We also use the 30 industry portfolios data set.",
            "It's been used in some of the retrospective papers and it is daily return since 1963.",
            "I think it is.",
            "And indices for 30 different sectors in the economy.",
            "So this is a case where we've extended the results on multivariate context, so it's not.",
            "Observations you know some cases which are.",
            "I'm looking at the change point where we turn out, decreases in run length.",
            "We shouldn't be surprised at some of the areas that it found aroundlikethe.com bubble burst and the and the Northern Rock bank run, which are not surprisingly as traumatic events.",
            "So then we."
        ],
        [
            "And have some quantitative results.",
            "We compare it against the time independent model, which is just using.",
            "That's a baseline, which is just I considering the ID, which is sort of like a on the returns is sort of like a Brownian motion assumption in the, which is typically reasonable in the industry for Folio case.",
            "So the fix hikers is the Adams is out and I hand picked parameters.",
            "You have learned parameters.",
            "We trained the hyperparameters on 1st 1000 points and then we tested it on the next 3000 points in well, log data.",
            "Here the error bars in key value one second test comparing these predicted like this and then we also on the industry data we used in the comparison.",
            "The independent video CD and the joint chief model and what I mean by that is that in the joint case, we assume that the market follows.",
            "We have one joint normal distribution and at another change point the entire joint changes and you can get a new meeting covariance.",
            "And and all the sectors change at the same time now is in the independent case.",
            "We just do a unitary approach and and and different sectors can change different times and each has its strengths and weaknesses that the joint has.",
            "The strength that you have 30 observations which gives you much more information to infer when the change points are current.",
            "As in the independent case, you only have one point with one dimension, which is less much Internet, which is much less information.",
            "But on the other hand has flexibility that one sector can change, and without another sector of the economy changing.",
            "So the joint approach seems to work a little bit better, but not by a lot, so each has its strengths and weaknesses.",
            "I mean, future work we might want to do is come up with this sort of a PCA sort of approach.",
            "We can sort of land between the two, but we we lose some of the trackability.",
            "Probably by doing that."
        ],
        [
            "And that's just my summer there.",
            "Any question or comment?",
            "Seems to be a big change point in your beta, since we'll."
        ],
        [
            "Where?",
            "Expect that.",
            "Yeah, we kind of expected.",
            "There might be a change point there, but they didn't seem to be one.",
            "I mean, I guess.",
            "With your market was was closed for awhile after that, so I mean maybe.",
            "But I mean it wasn't a completely different change in the market structure after things sort of re stabilized, but it started to hard to know for sure what the you know the full story is, but.",
            "Interesting.",
            "Any other questions or?",
            "I would have won only your last table.",
            "Displayed."
        ],
        [
            "In it.",
            "Did you did you looking?",
            "Is it really something that's?",
            "Is it giving you a lot of information on the prize and there was some?",
            "I mean, he's so much surprising.",
            "For instance, if you compare the.",
            "Depends on what's happening in the daytime.",
            "These are predictive likelihood.",
            "Is it really?",
            "So something something?",
            "Should I conclude from these days?",
            "They did actually.",
            "Because you're not at all.",
            "I mean, if you do this change pointing probably it's also because you expect to send something you change my button.",
            "This measure is absolutely confident.",
            "Yeah, I mean we had we had to pick a because we now have ground truth on the latest dates of the change point.",
            "We had a fix for the quantitative results we needed to pick something at the most most natural ideas.",
            "Think about how well you can predict the future.",
            "And that's like in the industry portfolios case in marketing.",
            "That's that's actually what you're interested in predicting.",
            "What's going to happen.",
            "I mean that."
        ],
        [
            "For that reason is why we also, you know we also put the figures in together quality results to see if if it does match is intuitively what you think it should.",
            "I mean, if whatever whatever that happens to be.",
            "It's pretty, it's also the same issue as in topic model because each method predict the future, but on its own standard because you use it pretty like you.",
            "Exactly like you ever fixed criterion for motivated you each one to predict.",
            "So in some sense, perhaps I don't know that committed to top that have more flexibility that are more parameters are bound to be better based on this trend.",
            "Not necessarily always the case.",
            "For instance, in the well log data we did try using a nonparametric density for prediction and we found that the Gaussian model actually was able to have a higher predictive likelihood, because because the the true density is close enough to Gaussian.",
            "It had the the prior information help the help the Gaussian PM and then the nonparametric density estimate is is comparing all these hypothesis about very bizarre densities that are nowhere if you're nowhere near Gaussian.",
            "But I mean, I mean, it does automatically have this tradeoff between having that something is too simple.",
            "It's a time independent model.",
            "And something which is just too flexible, which never wins anything.",
            "So basically what you said more complex models will not have a better, better, better metric there, because if the next slot is marginal likelihood of randomizing, so you're integrating out the next bulls.",
            "So that means that a more complex model will have a lower, lower lower marginal likelihood be cause of the outcome.",
            "Razor, thanks.",
            "Yeah, you can also cast in those terms.",
            "So any other questions?",
            "OK, then it's not there."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Joint work with Eunice Saatchi and Carl Rasmussen, we all were able to contribute one active to our project.",
                    "label": 1
                },
                {
                    "sent": "So first I'm going to talk about some of the.",
                    "label": 0
                },
                {
                    "sent": "That's not what I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "So what I want to talk about with some of the motivation for some of the things we want our method to achieve.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we want to handle non stationary time series because as well as anybody in here probably knows any real world Time series you tend to run into this thing where you have a model that works.",
                    "label": 0
                },
                {
                    "sent": "And then there's something in the system that changes and then it doesn't work anymore.",
                    "label": 0
                },
                {
                    "sent": "So we need to be able to sort of automatically erase that sort of old training data for the old system and recognize that things have changed.",
                    "label": 0
                },
                {
                    "sent": "Now we want to avoid making point estimates of the parameters.",
                    "label": 1
                },
                {
                    "sent": "Because we assume that maybe some change points might happen frequently enough that an isn't going to grow large, and we're never going to be able to make a reliable estimate, and we want our framework to be modular so we can use it in different contexts.",
                    "label": 0
                },
                {
                    "sent": "I'll get back to.",
                    "label": 0
                },
                {
                    "sent": "That will become more clear later, exactly our modularity.",
                    "label": 0
                },
                {
                    "sent": "We want to be efficient, tractable, so we did without any heavy duty MCMC, and we want to do online applications, and we want to have enough.",
                    "label": 0
                },
                {
                    "sent": "Faith that we made probabilistic predictions so that we can actually.",
                    "label": 0
                },
                {
                    "sent": "People have something that we can trust with.",
                    "label": 0
                },
                {
                    "sent": "And we want to have minimal hand tuning and that was the main contribution over the previous of the work that we've been extending, so this is a pretty good bill of goods here, so to speak.",
                    "label": 0
                },
                {
                    "sent": "These two things here were have been faulted by some as being sort of key flaws instead of the copula models and credit default swaps, and that you have these correlation structure in the mortgage market which used to be true, but.",
                    "label": 0
                },
                {
                    "sent": "Wasn't anymore.",
                    "label": 0
                },
                {
                    "sent": "He also made point estimates of it that way.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a few key ingredients here to our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The key terms that we use is the run length.",
                    "label": 1
                },
                {
                    "sent": "That's what best when we compute our posteriors, so the run length is the time between the time since the last change point.",
                    "label": 1
                },
                {
                    "sent": "Some Cholera regene change the other term even use.",
                    "label": 0
                },
                {
                    "sent": "So for instance, here we have.",
                    "label": 0
                },
                {
                    "sent": "This is some synthetic data for the illustrate illustrate with from our model, and we had here a changing mean, and this is the run length since the last change point.",
                    "label": 0
                },
                {
                    "sent": "And it simply just increments one every time.",
                    "label": 0
                },
                {
                    "sent": "So we have underlying predictive model and that's the model that we solve the mission model that you predict from within regime given the parameters within within that receipt.",
                    "label": 0
                },
                {
                    "sent": "So in this case here we used was just the Gaussian and the mean the variance change between these routines.",
                    "label": 1
                },
                {
                    "sent": "We also had the hazard function which is how which is the how likely change planes to happen at each point in time and each hazard function.",
                    "label": 0
                },
                {
                    "sent": "Implying different interarrival distribution on change point and in this synthetic data is just a constant with hazard function, which implies a geometric interarrival distribution.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there's been previous work as you can see from all the other talks before me, so they then test based approaches with another basic questions that have been retrospective and we want to be on line and the keyword that we build upon is added to mechanic paper from 2007 and they did not address the hyperparameter learning issue.",
                    "label": 1
                },
                {
                    "sent": "We've kind of seen from our experiments that it can be sensitive.",
                    "label": 0
                },
                {
                    "sent": "The hyperparameters, which is a little bit disturbing if you have to.",
                    "label": 0
                },
                {
                    "sent": "And tune them.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of like using a Gaussian process regression model and sort of handpicking the hyperparameters in that it's usually not as good as actually learning your hyperparameters.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is basically the core of the inference step in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So one of the key parts we work with is this predictive distribution of the next time point.",
                    "label": 0
                },
                {
                    "sent": "Given all your past data and we just.",
                    "label": 0
                },
                {
                    "sent": "Marginalized out the last well what the run length is.",
                    "label": 1
                },
                {
                    "sent": "So for any given run link, it's just a standard posterior predictive operation to get your prediction on the next data.",
                    "label": 0
                },
                {
                    "sent": "I guess that was another change.",
                    "label": 0
                },
                {
                    "sent": "So then we end together total prediction that is weighted by our post current posterior over the run links and then we also can put in this convenient notation here, which is the we summarize just the X points since since the last change point with that.",
                    "label": 0
                },
                {
                    "sent": "So we work with the message passing scheme.",
                    "label": 1
                },
                {
                    "sent": "Here that is the joint distribution of the run length and the data which we just expand out with the joint distribution between the run length in the past run Lane we use uh some here, but this is in all cases except for the hypothesis of the current run length is is 0.",
                    "label": 0
                },
                {
                    "sent": "This sum is just one term.",
                    "label": 0
                },
                {
                    "sent": "So for instance if you're looking at the probability that the current run length is 5, that miss mean the last friend Link was four 'cause.",
                    "label": 0
                },
                {
                    "sent": "Other possibility now in the case that you're testing hypothesis that this is a current run length of 0, then there can be any possible pass running because it could be anything that changed back to zero and then we just use the rules of our ability to expand this out into a hazard function and a likelihood or the UPN as we call it and multiply it times the last message.",
                    "label": 0
                },
                {
                    "sent": "And then we can normalize that to get our conditional distribution on the run like given the data.",
                    "label": 0
                },
                {
                    "sent": "Now this is now in the simple case that use a constant hazard function.",
                    "label": 0
                },
                {
                    "sent": "The intuition here is that you just take your previous hypothesis probability of previous houses on all the past run links.",
                    "label": 0
                },
                {
                    "sent": "The time since the last change point and reweigh them depending on how well they predicted the current data point and that this gives a slight relating to the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "And then he had an cousin hazard.",
                    "label": 0
                },
                {
                    "sent": "You can add a little extra.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the the key contribution here is that we want to do learning to learn the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So for instance, in the UPN case, for instance, if you you kind of Gaussian with changing the variance that we're going to have to deal with the fact that we need to put a prior on that meeting, that variance and those are going to have two hyperparameters each.",
                    "label": 0
                },
                {
                    "sent": "And then we also have high pressures and how they were dealing with five hyperparameters and we don't want to hand pick that hand handset it so.",
                    "label": 0
                },
                {
                    "sent": "We maximized maximized the log marginal likelihood, also known as the evidence to do that.",
                    "label": 1
                },
                {
                    "sent": "The intuition here is that.",
                    "label": 0
                },
                {
                    "sent": "If you want to be even more Bayesian about it and integrate the full posterior.",
                    "label": 0
                },
                {
                    "sent": "You can do that, but if it's the post series very peak then we just get revert back to what we're doing here, which is just find the maximum posterior the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we can decompose the marginal like that in this case into the sum of 1 step ahead predictive distributions and the log likelihood, and this is something that is not very hard to do here, because the UPS automatically give us this equity given X. Oh one T -- 1.",
                    "label": 0
                },
                {
                    "sent": "Distribution there, so this is the case where the evidence is actually a tractable thing to compute.",
                    "label": 0
                },
                {
                    "sent": "So we have this whole message passing scheme and the inputs in those messages are the probabilities from the UPM in the House are function.",
                    "label": 1
                },
                {
                    "sent": "So if we just take the derivatives of those, we can just use the chain rule to propagate the derivatives forward through time, just like we're passing messages for your time.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the way we do an after we just, you know, not even mutations just throw that into a constant gradient optimizer to maximize the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Now we can improve that a little bit more with this idea of pruning.",
                    "label": 0
                },
                {
                    "sent": "So a naive implementation to be ordered T squared, because at any point T you have to compare the hypothesis that the that there could be any change point between time zero and T. So that's the half life policies and you and you have to re weigh those.",
                    "label": 1
                },
                {
                    "sent": "And everytime septets T squared now in practice that is a very peaked posterior, so you can sort of get rid of the messages that are sort of redundant and get down to the linear scale.",
                    "label": 0
                },
                {
                    "sent": "So I mean intuitively, if you're if you've been looking at stock returns for last year and you're looking at change points.",
                    "label": 0
                },
                {
                    "sent": "Probably I process it.",
                    "label": 0
                },
                {
                    "sent": "There's a change .6 months ago and six months ago minus one day isn't going to be all isn't going to be very much different at that time scale, so you can sort of just keep one of them.",
                    "label": 0
                },
                {
                    "sent": "That's that's good enough.",
                    "label": 0
                },
                {
                    "sent": "In practice you'll actually see a lot of hypotheses once you go down, and clicking on any of my 20 or something.",
                    "label": 0
                },
                {
                    "sent": "And then it's just a waste of time to work with them, and it's highly modular.",
                    "label": 0
                },
                {
                    "sent": "We can work on any improving any hazard function and any MoD presence here predict.",
                    "label": 1
                },
                {
                    "sent": "If so, you can use the Gaussian process regression.",
                    "label": 1
                },
                {
                    "sent": "So so this isn't just restricted to IID data, so you can use like.",
                    "label": 0
                },
                {
                    "sent": "Have a linear trend and then you can have that trend change so you cannot change in linear trends or even nonlinear translating Gaussian process case.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to be if you want to not have any parametric assumption of the density, this is a module in a fully Bayesian value you really want to do is say process picture or something like that, but to keep it keep complication down you can just use a kernel density estimate.",
                    "label": 0
                },
                {
                    "sent": "Probably the same thing.",
                    "label": 0
                },
                {
                    "sent": "And you can also cache a lot of things because a lot of these run links complications, while normalization constants that only depend on the run length and the hyperparameters and not data, and that you're going to be doing computation over and over again.",
                    "label": 0
                },
                {
                    "sent": "So you can sort of cash a lot of your normally constant speed things up.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we tried this on the well log data which is used in the original Addams Mackay Paper.",
                    "label": 1
                },
                {
                    "sent": "Or they had the hantik parameters.",
                    "label": 0
                },
                {
                    "sent": "We use the.",
                    "label": 1
                },
                {
                    "sent": "We also generalize things a little bit with the logistic after function, so we don't.",
                    "label": 0
                },
                {
                    "sent": "We aren't committing ourselves to a geometric interarrival.",
                    "label": 0
                },
                {
                    "sent": "Time of change points.",
                    "label": 0
                },
                {
                    "sent": "This is a more flexible distribution.",
                    "label": 0
                },
                {
                    "sent": "We use an IID Gaussian DPM and try to catch changes in the mean and the variance.",
                    "label": 1
                },
                {
                    "sent": "We have a graph here of the posterior that we got the filtered posterior.",
                    "label": 0
                },
                {
                    "sent": "So this is the post here.",
                    "label": 0
                },
                {
                    "sent": "Given only the past data.",
                    "label": 0
                },
                {
                    "sent": "This is that we we plotted the CDF here in the original Series A lot of PDF, so the transition from black to white here means a peak in the posterior.",
                    "label": 0
                },
                {
                    "sent": "We all the red line here corresponds to the comedian.",
                    "label": 0
                },
                {
                    "sent": "Listeria, which we found to be a much better summary of posterior than using the mode in the mean.",
                    "label": 0
                },
                {
                    "sent": "To give results that looks a lot better.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also use the 30 industry portfolios data set.",
                    "label": 1
                },
                {
                    "sent": "It's been used in some of the retrospective papers and it is daily return since 1963.",
                    "label": 0
                },
                {
                    "sent": "I think it is.",
                    "label": 0
                },
                {
                    "sent": "And indices for 30 different sectors in the economy.",
                    "label": 0
                },
                {
                    "sent": "So this is a case where we've extended the results on multivariate context, so it's not.",
                    "label": 0
                },
                {
                    "sent": "Observations you know some cases which are.",
                    "label": 1
                },
                {
                    "sent": "I'm looking at the change point where we turn out, decreases in run length.",
                    "label": 0
                },
                {
                    "sent": "We shouldn't be surprised at some of the areas that it found aroundlikethe.com bubble burst and the and the Northern Rock bank run, which are not surprisingly as traumatic events.",
                    "label": 1
                },
                {
                    "sent": "So then we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And have some quantitative results.",
                    "label": 0
                },
                {
                    "sent": "We compare it against the time independent model, which is just using.",
                    "label": 0
                },
                {
                    "sent": "That's a baseline, which is just I considering the ID, which is sort of like a on the returns is sort of like a Brownian motion assumption in the, which is typically reasonable in the industry for Folio case.",
                    "label": 0
                },
                {
                    "sent": "So the fix hikers is the Adams is out and I hand picked parameters.",
                    "label": 0
                },
                {
                    "sent": "You have learned parameters.",
                    "label": 0
                },
                {
                    "sent": "We trained the hyperparameters on 1st 1000 points and then we tested it on the next 3000 points in well, log data.",
                    "label": 0
                },
                {
                    "sent": "Here the error bars in key value one second test comparing these predicted like this and then we also on the industry data we used in the comparison.",
                    "label": 1
                },
                {
                    "sent": "The independent video CD and the joint chief model and what I mean by that is that in the joint case, we assume that the market follows.",
                    "label": 0
                },
                {
                    "sent": "We have one joint normal distribution and at another change point the entire joint changes and you can get a new meeting covariance.",
                    "label": 0
                },
                {
                    "sent": "And and all the sectors change at the same time now is in the independent case.",
                    "label": 0
                },
                {
                    "sent": "We just do a unitary approach and and and different sectors can change different times and each has its strengths and weaknesses that the joint has.",
                    "label": 0
                },
                {
                    "sent": "The strength that you have 30 observations which gives you much more information to infer when the change points are current.",
                    "label": 0
                },
                {
                    "sent": "As in the independent case, you only have one point with one dimension, which is less much Internet, which is much less information.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand has flexibility that one sector can change, and without another sector of the economy changing.",
                    "label": 0
                },
                {
                    "sent": "So the joint approach seems to work a little bit better, but not by a lot, so each has its strengths and weaknesses.",
                    "label": 0
                },
                {
                    "sent": "I mean, future work we might want to do is come up with this sort of a PCA sort of approach.",
                    "label": 0
                },
                {
                    "sent": "We can sort of land between the two, but we we lose some of the trackability.",
                    "label": 0
                },
                {
                    "sent": "Probably by doing that.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's just my summer there.",
                    "label": 0
                },
                {
                    "sent": "Any question or comment?",
                    "label": 0
                },
                {
                    "sent": "Seems to be a big change point in your beta, since we'll.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "Expect that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we kind of expected.",
                    "label": 0
                },
                {
                    "sent": "There might be a change point there, but they didn't seem to be one.",
                    "label": 0
                },
                {
                    "sent": "I mean, I guess.",
                    "label": 0
                },
                {
                    "sent": "With your market was was closed for awhile after that, so I mean maybe.",
                    "label": 0
                },
                {
                    "sent": "But I mean it wasn't a completely different change in the market structure after things sort of re stabilized, but it started to hard to know for sure what the you know the full story is, but.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "Any other questions or?",
                    "label": 0
                },
                {
                    "sent": "I would have won only your last table.",
                    "label": 0
                },
                {
                    "sent": "Displayed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In it.",
                    "label": 0
                },
                {
                    "sent": "Did you did you looking?",
                    "label": 0
                },
                {
                    "sent": "Is it really something that's?",
                    "label": 0
                },
                {
                    "sent": "Is it giving you a lot of information on the prize and there was some?",
                    "label": 0
                },
                {
                    "sent": "I mean, he's so much surprising.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you compare the.",
                    "label": 0
                },
                {
                    "sent": "Depends on what's happening in the daytime.",
                    "label": 0
                },
                {
                    "sent": "These are predictive likelihood.",
                    "label": 0
                },
                {
                    "sent": "Is it really?",
                    "label": 0
                },
                {
                    "sent": "So something something?",
                    "label": 0
                },
                {
                    "sent": "Should I conclude from these days?",
                    "label": 0
                },
                {
                    "sent": "They did actually.",
                    "label": 0
                },
                {
                    "sent": "Because you're not at all.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you do this change pointing probably it's also because you expect to send something you change my button.",
                    "label": 0
                },
                {
                    "sent": "This measure is absolutely confident.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean we had we had to pick a because we now have ground truth on the latest dates of the change point.",
                    "label": 0
                },
                {
                    "sent": "We had a fix for the quantitative results we needed to pick something at the most most natural ideas.",
                    "label": 0
                },
                {
                    "sent": "Think about how well you can predict the future.",
                    "label": 0
                },
                {
                    "sent": "And that's like in the industry portfolios case in marketing.",
                    "label": 0
                },
                {
                    "sent": "That's that's actually what you're interested in predicting.",
                    "label": 0
                },
                {
                    "sent": "What's going to happen.",
                    "label": 0
                },
                {
                    "sent": "I mean that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For that reason is why we also, you know we also put the figures in together quality results to see if if it does match is intuitively what you think it should.",
                    "label": 0
                },
                {
                    "sent": "I mean, if whatever whatever that happens to be.",
                    "label": 0
                },
                {
                    "sent": "It's pretty, it's also the same issue as in topic model because each method predict the future, but on its own standard because you use it pretty like you.",
                    "label": 0
                },
                {
                    "sent": "Exactly like you ever fixed criterion for motivated you each one to predict.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, perhaps I don't know that committed to top that have more flexibility that are more parameters are bound to be better based on this trend.",
                    "label": 0
                },
                {
                    "sent": "Not necessarily always the case.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the well log data we did try using a nonparametric density for prediction and we found that the Gaussian model actually was able to have a higher predictive likelihood, because because the the true density is close enough to Gaussian.",
                    "label": 0
                },
                {
                    "sent": "It had the the prior information help the help the Gaussian PM and then the nonparametric density estimate is is comparing all these hypothesis about very bizarre densities that are nowhere if you're nowhere near Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But I mean, I mean, it does automatically have this tradeoff between having that something is too simple.",
                    "label": 0
                },
                {
                    "sent": "It's a time independent model.",
                    "label": 0
                },
                {
                    "sent": "And something which is just too flexible, which never wins anything.",
                    "label": 0
                },
                {
                    "sent": "So basically what you said more complex models will not have a better, better, better metric there, because if the next slot is marginal likelihood of randomizing, so you're integrating out the next bulls.",
                    "label": 0
                },
                {
                    "sent": "So that means that a more complex model will have a lower, lower lower marginal likelihood be cause of the outcome.",
                    "label": 0
                },
                {
                    "sent": "Razor, thanks.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can also cast in those terms.",
                    "label": 0
                },
                {
                    "sent": "So any other questions?",
                    "label": 0
                },
                {
                    "sent": "OK, then it's not there.",
                    "label": 0
                }
            ]
        }
    }
}