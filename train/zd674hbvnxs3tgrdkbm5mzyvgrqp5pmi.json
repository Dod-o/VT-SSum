{
    "id": "zd674hbvnxs3tgrdkbm5mzyvgrqp5pmi",
    "title": "Web-Scale Extension of RDF Knowledge Bases from Templated Websites",
    "info": {
        "author": [
            "Ricardo Usbeck, Agile Knowledge Engineering and Semantic Web (AKSW), University of Leipzig"
        ],
        "published": "Dec. 19, 2014",
        "recorded": "October 2014",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2014_usbeck_web_scale_extension/",
    "segmentation": [
        [
            "I'm going to present to you wreck switches to DUP title for the web scale extension framework.",
            "I'm going to show you today and this is joint work of Lawrence Buhman Axle normal more metal Mattress Boat but across NZ.",
            "Plumeria audio and Vision Q and why are we doing that?",
            "Well we need to populate the web of data definitely.",
            "And we will do that by using predicate based optimization of those web of data knowledge basis we have."
        ],
        [
            "So for example, I'm sure you all know imdb.com goodreads.com or espnfc.com, which are websites that are heavily templated, and that provide data for movies, books, and sports teams.",
            "We crawled 10,000 pages of them, and you can find more data on rex.aksw.org and just to be in the line with the autos that presented in front of me, I will explain you the whole approach on the running example an.",
            "I'm pretty."
        ],
        [
            "Sure, all of you know the movie Lion King Anne.",
            "If you go to DB pedia.",
            "I would expect to find a relation with the predicate DBO director, which says that Roger Allers in Rob Minkoff, other director of The Lion King.",
            "But in fact."
        ],
        [
            "But this is not true.",
            "DP that does not have that that certain triple.",
            "So we need to populate the knowledge base with that information."
        ],
        [
            "How do we do that?",
            "Well, we had some features in mind when we were designing our X framework.",
            "The first is it has to be extensible.",
            "That means it works on the MIT license, so you can extend our our framework.",
            "We using standard from the we W 3C, which is RDF and sparkle.",
            "We are modular, you can just plug in your kind of algorithm to extend our framework.",
            "We are scalable.",
            "That means we work on web scale data in a linear linear mode.",
            "We are locals.",
            "We have only unsupervised algorithms.",
            "We are highly accurate, which I will show you in the end and we do produce consistent RDF data.",
            "This is important since if you want to populate the web of data, the data that you are generating has to be consistent with the already existing data."
        ],
        [
            "So our X frameworks consists of four layers.",
            "Those four layers are the extraction, storage, induction and generation layer."
        ],
        [
            "And then in the extraction layer we have two main steps.",
            "The first step is we need to identify the domains on the web that contain the data we are looking for.",
            "To do that, we sample from our structured knowledge base which is here.",
            "Triplets which contain our target predicate P and we go to, for example Google and Google for SPN.",
            "Oh, and notice which are the top end domains that contain our sample triples, and then we're going to crawl them after."
        ],
        [
            "We crossed them.",
            "We store the data in a Lucene index for those unstructured web pages and we also have a structured knowledge base base which is virtuoso which contains our already existing knowledge."
        ],
        [
            "In the induction layer, then we sample out of our knowledge base RDF data to generate more examples for those crowd web pages on those crowd web pages, we assume that we have a subject and object for certain predicate P which is our target predicate an for those as you know, we generate X path expressions to extract strings for as in.",
            "Oh, so that we can maximize our precision or recall why unsupervised machine learning."
        ],
        [
            "How does that work?",
            "Well, imagine that you have to top three dumb trees from our websites.",
            "Those three trees look very similar to each other.",
            "And they all have the same structure, but the tree on B does miss one particular path, which is the ratings path and the tree on the that is missing, not information, but that has additional information.",
            "So in the end we can extract 3 different expert expressions to extract S oh pairs of, let's say director.",
            "And the goal of our rapid induction is to combine those.",
            "Extracted experts to one and two extract with a higher precision, high recall.",
            "New data that we don't know before."
        ],
        [
            "So, and what expect us is it extracts strings so strings are not valuable to RDF or two to knowledge base basis at all, so we need to disambiguate those strings to your eyes.",
            "And we do that by using a distance which represented this morning.",
            "This is an approach which is also highly scalable, and which will link to existing knowledge bases.",
            "And when we got those, your eyes for the subject and the object, we just need to consistency check this new generated triple.",
            "Whether it fits in the knowledge base or violates some constraints, thereby we assuming that we have closed word, and we do that checking on consistency.",
            "Why are using a rule based approach which ensures that we are maximally consistent with the already existing data?"
        ],
        [
            "And just not to set up a framework.",
            "We also have to evaluate it and we evaluated our approach on three different knowledge bases or three different websites that that I showed you before and we sent it 100 pages per domain."
        ],
        [
            "For this 100 pages per domain, it was possible to extract the first column number of triplets which are which were extracted by manual reviews.",
            "In there every cost intensive thing or in a cost intensive evaluation process this."
        ],
        [
            "This is that, for example, on ESPN.com we can't.",
            "We were only able to extract 2 triplets.",
            "Why are handcrafted expert fruits and our automatic approach was only able to extract 1.",
            "Tripper so there are domains which are really really hard to to extract knowledge from."
        ],
        [
            "But on the other hand side, imdb.com, where is really thankful to to automatic extraction so our Xbox generation approach Alpha Rex was able to extract 99 as au pairs for a certain predicate which were staring and in the end we were able to extra."
        ],
        [
            "But those numbers of triplets, Mewtwo the knowledge base."
        ],
        [
            "So we are really scalable, which I want to show you just quickly.",
            "That means that we only have to see 5% of the knowledge base or to some 5% of the triplets we want to use in the end to achieve a high F measure."
        ],
        [
            "And you can use our approach via our Maven build system, which is deployed at a CSW."
        ],
        [
            "In the end, Rex is scalable Java Source Open Project, open Source Java project and we want to populate the web of data more with data from those template heavily templated websites or future work will be to extend it to other domains.",
            "So thank you for your attention and I hope we're not late to the coffee break."
        ],
        [
            "When once you find these expats they could become griddle files which could be used by all.",
            "Yeah, we can collaborate.",
            "Just visit our website and we can extend everything.",
            "Thank you.",
            "My question is which mechanism actually are you using to detect the templates in the 1st place?",
            "If you have one?",
            "Yeah Dylan, this is quite simple.",
            "We are now using a domain identifier that's based on Google, so you take a subject and object like say 10,000 triples that are have go director in it and we use the subway in the object and go to Google and say find me websites that F that in it and the most ranked website will be a website which has probably the most database like.",
            "Information, background and is most heavily but but templated.",
            "Alright, that's it for the session.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to present to you wreck switches to DUP title for the web scale extension framework.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show you today and this is joint work of Lawrence Buhman Axle normal more metal Mattress Boat but across NZ.",
                    "label": 0
                },
                {
                    "sent": "Plumeria audio and Vision Q and why are we doing that?",
                    "label": 0
                },
                {
                    "sent": "Well we need to populate the web of data definitely.",
                    "label": 0
                },
                {
                    "sent": "And we will do that by using predicate based optimization of those web of data knowledge basis we have.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, I'm sure you all know imdb.com goodreads.com or espnfc.com, which are websites that are heavily templated, and that provide data for movies, books, and sports teams.",
                    "label": 1
                },
                {
                    "sent": "We crawled 10,000 pages of them, and you can find more data on rex.aksw.org and just to be in the line with the autos that presented in front of me, I will explain you the whole approach on the running example an.",
                    "label": 1
                },
                {
                    "sent": "I'm pretty.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, all of you know the movie Lion King Anne.",
                    "label": 0
                },
                {
                    "sent": "If you go to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "I would expect to find a relation with the predicate DBO director, which says that Roger Allers in Rob Minkoff, other director of The Lion King.",
                    "label": 0
                },
                {
                    "sent": "But in fact.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this is not true.",
                    "label": 0
                },
                {
                    "sent": "DP that does not have that that certain triple.",
                    "label": 0
                },
                {
                    "sent": "So we need to populate the knowledge base with that information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "Well, we had some features in mind when we were designing our X framework.",
                    "label": 0
                },
                {
                    "sent": "The first is it has to be extensible.",
                    "label": 0
                },
                {
                    "sent": "That means it works on the MIT license, so you can extend our our framework.",
                    "label": 0
                },
                {
                    "sent": "We using standard from the we W 3C, which is RDF and sparkle.",
                    "label": 0
                },
                {
                    "sent": "We are modular, you can just plug in your kind of algorithm to extend our framework.",
                    "label": 0
                },
                {
                    "sent": "We are scalable.",
                    "label": 0
                },
                {
                    "sent": "That means we work on web scale data in a linear linear mode.",
                    "label": 0
                },
                {
                    "sent": "We are locals.",
                    "label": 0
                },
                {
                    "sent": "We have only unsupervised algorithms.",
                    "label": 0
                },
                {
                    "sent": "We are highly accurate, which I will show you in the end and we do produce consistent RDF data.",
                    "label": 0
                },
                {
                    "sent": "This is important since if you want to populate the web of data, the data that you are generating has to be consistent with the already existing data.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our X frameworks consists of four layers.",
                    "label": 0
                },
                {
                    "sent": "Those four layers are the extraction, storage, induction and generation layer.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then in the extraction layer we have two main steps.",
                    "label": 1
                },
                {
                    "sent": "The first step is we need to identify the domains on the web that contain the data we are looking for.",
                    "label": 0
                },
                {
                    "sent": "To do that, we sample from our structured knowledge base which is here.",
                    "label": 1
                },
                {
                    "sent": "Triplets which contain our target predicate P and we go to, for example Google and Google for SPN.",
                    "label": 0
                },
                {
                    "sent": "Oh, and notice which are the top end domains that contain our sample triples, and then we're going to crawl them after.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We crossed them.",
                    "label": 0
                },
                {
                    "sent": "We store the data in a Lucene index for those unstructured web pages and we also have a structured knowledge base base which is virtuoso which contains our already existing knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the induction layer, then we sample out of our knowledge base RDF data to generate more examples for those crowd web pages on those crowd web pages, we assume that we have a subject and object for certain predicate P which is our target predicate an for those as you know, we generate X path expressions to extract strings for as in.",
                    "label": 0
                },
                {
                    "sent": "Oh, so that we can maximize our precision or recall why unsupervised machine learning.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How does that work?",
                    "label": 0
                },
                {
                    "sent": "Well, imagine that you have to top three dumb trees from our websites.",
                    "label": 0
                },
                {
                    "sent": "Those three trees look very similar to each other.",
                    "label": 0
                },
                {
                    "sent": "And they all have the same structure, but the tree on B does miss one particular path, which is the ratings path and the tree on the that is missing, not information, but that has additional information.",
                    "label": 0
                },
                {
                    "sent": "So in the end we can extract 3 different expert expressions to extract S oh pairs of, let's say director.",
                    "label": 0
                },
                {
                    "sent": "And the goal of our rapid induction is to combine those.",
                    "label": 0
                },
                {
                    "sent": "Extracted experts to one and two extract with a higher precision, high recall.",
                    "label": 0
                },
                {
                    "sent": "New data that we don't know before.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, and what expect us is it extracts strings so strings are not valuable to RDF or two to knowledge base basis at all, so we need to disambiguate those strings to your eyes.",
                    "label": 0
                },
                {
                    "sent": "And we do that by using a distance which represented this morning.",
                    "label": 1
                },
                {
                    "sent": "This is an approach which is also highly scalable, and which will link to existing knowledge bases.",
                    "label": 1
                },
                {
                    "sent": "And when we got those, your eyes for the subject and the object, we just need to consistency check this new generated triple.",
                    "label": 0
                },
                {
                    "sent": "Whether it fits in the knowledge base or violates some constraints, thereby we assuming that we have closed word, and we do that checking on consistency.",
                    "label": 0
                },
                {
                    "sent": "Why are using a rule based approach which ensures that we are maximally consistent with the already existing data?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just not to set up a framework.",
                    "label": 0
                },
                {
                    "sent": "We also have to evaluate it and we evaluated our approach on three different knowledge bases or three different websites that that I showed you before and we sent it 100 pages per domain.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this 100 pages per domain, it was possible to extract the first column number of triplets which are which were extracted by manual reviews.",
                    "label": 0
                },
                {
                    "sent": "In there every cost intensive thing or in a cost intensive evaluation process this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is that, for example, on ESPN.com we can't.",
                    "label": 0
                },
                {
                    "sent": "We were only able to extract 2 triplets.",
                    "label": 0
                },
                {
                    "sent": "Why are handcrafted expert fruits and our automatic approach was only able to extract 1.",
                    "label": 0
                },
                {
                    "sent": "Tripper so there are domains which are really really hard to to extract knowledge from.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But on the other hand side, imdb.com, where is really thankful to to automatic extraction so our Xbox generation approach Alpha Rex was able to extract 99 as au pairs for a certain predicate which were staring and in the end we were able to extra.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But those numbers of triplets, Mewtwo the knowledge base.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we are really scalable, which I want to show you just quickly.",
                    "label": 0
                },
                {
                    "sent": "That means that we only have to see 5% of the knowledge base or to some 5% of the triplets we want to use in the end to achieve a high F measure.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you can use our approach via our Maven build system, which is deployed at a CSW.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the end, Rex is scalable Java Source Open Project, open Source Java project and we want to populate the web of data more with data from those template heavily templated websites or future work will be to extend it to other domains.",
                    "label": 0
                },
                {
                    "sent": "So thank you for your attention and I hope we're not late to the coffee break.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When once you find these expats they could become griddle files which could be used by all.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we can collaborate.",
                    "label": 0
                },
                {
                    "sent": "Just visit our website and we can extend everything.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "My question is which mechanism actually are you using to detect the templates in the 1st place?",
                    "label": 0
                },
                {
                    "sent": "If you have one?",
                    "label": 0
                },
                {
                    "sent": "Yeah Dylan, this is quite simple.",
                    "label": 0
                },
                {
                    "sent": "We are now using a domain identifier that's based on Google, so you take a subject and object like say 10,000 triples that are have go director in it and we use the subway in the object and go to Google and say find me websites that F that in it and the most ranked website will be a website which has probably the most database like.",
                    "label": 0
                },
                {
                    "sent": "Information, background and is most heavily but but templated.",
                    "label": 0
                },
                {
                    "sent": "Alright, that's it for the session.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}