{
    "id": "6ysa6qhqzg36myeuyfj64xixohau2atm",
    "title": "Monte-Carlo Simulation Balancing",
    "info": {
        "author": [
            "David Silver, Department of Computer Science, University College London"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Monte Carlo Methods"
        ]
    },
    "url": "http://videolectures.net/icml09_silver_mcsb/",
    "segmentation": [
        [
            "Hi everyone, thank you for your patience.",
            "Both just then and also staying right to the end of the final session of the conference.",
            "I'm David Silva from the University of Alberta.",
            "This is joint work with Jerry Tesauro from IBM Research and it's really a pleasure to be here to tell you about this."
        ],
        [
            "So I'm going to begin by giving a little bit of background on Monte Carlo search and what's been happening recently there.",
            "Before I go on to the new."
        ],
        [
            "Tradition of this work.",
            "So in two player games, Monte Carlo searches really actually acquired quite a number of successes.",
            "Now over the years, so beginning with a number of stochastic two player games, so backgammon.",
            "Sarah and others were able to achieve master level in backgammon back in 97 using a simple form of Monte Carlo search, and this success was then repeated in poker and Scrabble and then a few years ago people were able to extend this success to deterministic games such as Go first of all achieving master level in 99 Go and this year now extending those successes to full size go 19 by 19."
        ],
        [
            "Go.",
            "So really what this talk is about is how these successes can be taken even further.",
            "Perhaps looking to the future.",
            "So the key idea of Monte Carlo search is to simulate a number of rollouts from each position using some simulation policy, and it turns out that the performance of Monte Carlo search really depends critically on the particular simulation policy that you use.",
            "And especially in terministic domains, it turns out that it's not just good enough to have a strong simulation policy.",
            "You also need some kind of diversity.",
            "You need stochastic rollouts you need to make sure that you get this nice wide variety of simulations.",
            "And we've seen time and again many different examples where a strong policy improving the quality of the policy directly does not necessarily improve the performance of the overall Monte Carlo player.",
            "So the problem that we're trying to address it with this work is how can we automatically learn a good simulation policy?"
        ],
        [
            "So I'm going to focus on the game of go, but this work could be applied to any two player game.",
            "And in the game of go in particular, we've seen that the simulation policy has a really dramatic effect on the performance of a Monte Carlo player.",
            "In fact, it's the single number one factor determining the overall performance of Monte Carlo player.",
            "So unsurprisingly, people have tried a number of different techniques to construct a simulation player, the most common being handcrafting the simulation policy by trial and error.",
            "So essentially it turns out that our human intuitions, as go players actually don't help very much in this case, because really, our human intuition is trying to.",
            "Tell us how to make the simulation policy stronger, and this, as we've seen, doesn't necessarily make the overall Monte Carlo player stronger.",
            "So what about doing this automatically?",
            "Well, some people have tried Hill climbing methods, such as evolutionary methods or cross entropy methods.",
            "The problem here is that each single evaluation of a Monte Carlo search program is very expensive, so you might need hundreds of games of real games played out by the Monte Carlo program, which each game contains hundreds of positions, and each position may need 10s of thousands of simulations.",
            "Just to be able to get one evaluation point during this kind of optimization process.",
            "So what about the traditional paradigms of supervised learning and reinforcement learning?",
            "Well, these are very successful.",
            "If you want to learn a strong policy.",
            "If you really want to learn a policy that directly performs well in a domain, then you can apply supervised learning or reinforcement learning.",
            "But this again is not really attacking the problem of how can we make our Monte Carlo search stronger, so this talk is all about attacking that that specific problem.",
            "How can we actually do better in a Monte Carlo search by changing choosing an objective function which really is matched up with this?",
            "Thing which we are trying to do so if we really want to achieve good Monte Carlo performance, what objective should we be optimizing?",
            "And the answer just to give you a sneak preview.",
            "The answer is yes, although this is really just focusing on small board goes so far and I'll just touch on the end.",
            "You know perhaps."
        ],
        [
            "How this can be extended?",
            "So OK, I'll just give some background on Monte Carlo search so we have a two player game at the end of the game.",
            "There's some outcomes Ed, so we're going to say there's two players will just call black and white.",
            "If Black wins, then the outcome is going to be zed equals one, and if white wins then Z = 0 and we can then define the value of a position to simply be the expected outcome at the end of the game.",
            "And the key idea here is that we can estimate this this value very simply by Monte Carlo by just performing some simulations and taking the average outcome from those simulation."
        ],
        [
            "So in slightly more detail, the idea is that we start our simulations from the current position S and execute a number of water called rollouts.",
            "From this position, using some simulation policy.",
            "So the simplest example would be that simulation policy is uniform random, so games are played out a rolled out by random from the current position, and that position is evaluated simply by the mean outcome."
        ],
        [
            "Of those simulations.",
            "So here's a little example.",
            "So at the top here you can see the actual current position in the game, and we'd like to know what's the value of this position.",
            "And here we see four different simulations have been rolled out from that position, and in two of them black ended up winning with the outcome of one and two of them.",
            "White ended up winning, so we can just say we can just evaluate this position to have a value of nought .5."
        ],
        [
            "So.",
            "How can this be used in practice?",
            "Well, the simplest approach which was used in backgammon, Scrabble, Poker, which I mentioned the beginning, is simple Monte Carlo search, where every single legal move is evaluated by Monte Carlo simulation, and the idea is simply to select the move with the highest value.",
            "After this, this process is complete."
        ],
        [
            "More recently, this has been extended into to use ideas from minimax search to develop a search tree during simulation.",
            "So the idea here in Monte Carlo Tree search is to start simulations from the current position again, simulate thousands of positions from the current simulation.",
            "But while you're doing this, you can actually build up a search tree that contains all visited positions and every position in that search tree can then be evaluated by Monte Carlo by just the average outcome of all simulations that pass through that position.",
            "And it turns out that with a suitable level of exploration, for example using the UCT algorithm, then this approach will actually converge on the minimax game tree, given enough time in memory."
        ],
        [
            "OK, so that's a little bit of background on Monte Carlo."
        ],
        [
            "Search in general.",
            "So now here's the new idea.",
            "So as I mentioned, if we really want to perform well in this domain, we'd like to be able to in Monte Carlo search, we'd like to be able to find a simulation policy that actually really is effective in Monte Carlo search.",
            "So here's the setup.",
            "We're going to use to define an objective function.",
            "We're going to have some stochastic simulation policy Pi that's parameterized by a vector of weights, Theta.",
            "And the key idea here is that any simulation policy, even if it's uniform random, actually has some bias with respect to the true minimax value.",
            "So here we can see the bias is defined as the mini Max value V star minus the expected outcome.",
            "If you are following this particular simulation policy.",
            "So really this expected outcome.",
            "This tells us what would happen.",
            "This is the Monte Carlo Monte Carlo value.",
            "Given an infinite number of simulations, this is what Monte Carlo will find in the limit.",
            "Is this expected value here?",
            "And so the bias is the difference between.",
            "This Monte Carlo value in the limit and the true mini Max value.",
            "What we'd like is for that to be small.",
            "We would like to find a simulation policy that has bias that's as small as possible over a set of the set of positions which we expect to encounter.",
            "So that's our aim.",
            "We're going to formulate this into an objective function, which is to minimize the mean squared bias, and we're going to try and find parameters that minimize that.",
            "Minimize that objective function.",
            "So the key issue here, which you probably notice Taz that we've included the true minimax value in our objective function, and in something like Go this, is actually pretty difficult to evaluate.",
            "But actually we can make a fairly reasonable approximation using Deep Monte Carlo.",
            "Tree search is so, as I mentioned earlier, Monte Carlo Tree search, given enough computation, will actually find the min and Max value.",
            "So the idea is we simply perform sufficient number of simulations that we actually have a reasonable approximation, and we can construct a training set of positions and values of those positions.",
            "That we can use to then train our simulation policy.",
            "So that's the idea here."
        ],
        [
            "OK, so I'm just going to try and give a bit more intuition behind this idea in the next couple of slides, so I've talked a little bit about strength and I'm going to introduce this idea of balance as well so.",
            "Really, you could imagine that in any position we can think of the there's some true mini Max value of that position, and every time the agent of player actually selects and move the best they can do is actually to maintain that that mini Max value if they play a bad move, they'll actually reduce the mini Max value.",
            "They'll so for example, Black will have some error that would be negative, black would be losing some value with respect to minimax, and if white plays a bad move, white will be increasing the value of the successive position.",
            "And the key idea here is that strong policy will have small errors that these errors will in general be quite low.",
            "That the scale of the error committed by each player will be small.",
            "In general, that's what a strong policy is that not very many errors are committed, or those errors are going to be small in magnitude.",
            "What we want is something a little bit different.",
            "We want to balance policy, which means that the expected total error over the course of the game will be small.",
            "So this means we don't actually mind if if the players make errors, as long as they cancel out over the course of the game.",
            "So we could either do this over the course of the entire game, which is what I'm going to discuss in this presentation, or you could try and balance just two steps at a time.",
            "So this would mean that it's OK if black makes an error as long as white makes an error immediately afterwards that cancels out.",
            "So if you're interested in that idea, then please look at the paper.",
            "I'm just going to focus on the full balancing."
        ],
        [
            "Is here.",
            "So I'll give a little more integration in these diagrams, I hope so.",
            "These diagrams represent it's actually some experiments with an artificial game, and in the start position of this artificial game the minimax value is actually 0.",
            "So the Max value is represented there on the Y axis, and what we have on the X axis.",
            "Here is the move number in the game, and each of these green lines that I hope people can just about make out at the back there.",
            "There's a bunch of green lines.",
            "That starting with the from the current position and each of these green lines represent A roll out from that start position high.",
            "But yeah, the minimax value is the game optimal value if you constructed a search tree such that black is maximizing his value at every move and white is minimizing her value every move, then this is the mini Max value.",
            "So it's not just one min one Max it.",
            "So I'm in a Max kind of iterated over the entire game tree.",
            "OK, so that's what being represented on the Y axis here.",
            "The Mini Max value and each of these green lines represents a trajectory a game that's being rolled out from this position for 100 time steps.",
            "And in this particular example here.",
            "So now we have a whole bunch of different trajectories in this black line.",
            "In the middle represents the mean value of each of these of these 30 different trajectories that have been rolled out.",
            "And in this particular example here, what we have here is both of these players are actually that are rolling out this.",
            "Games are quite strong.",
            "You can see there committing errors, but the magnitude of those errors is quite small, but it turns out the Blacks just a little bit stronger than white in this example.",
            "So the first step here, Black makes a very small error.",
            "White makes a slightly bigger error.",
            "Black makes a small error, white makes a slightly bigger error, and so on.",
            "Throughout this game and the net result over 100 different moves is actually a great deal of bias that the value at the end of these hundred moves, the minimax value of where these rollups end up, is actually very very biased, because Black has played a little bit stronger throughout a large number of moves and White House.",
            "And so if you're actually trying to play Monte Carlo evaluation to this set of trajectories, it's going to give a very big bias here.",
            "The difference between this red line and the mini Max value 0 here.",
            "So there's this big bias here, and that's what we want to reduce.",
            "We don't want to see that bias.",
            "That means that Monte Carlo is giving us the wrong answer.",
            "It's not giving us the minimax value in this city."
        ],
        [
            "So in contrast, what we'd like to see is something a bit more like this, where now we have two very weak players, so both black and white are very weak.",
            "They are committing quite large errors at every step, but those errors that are roughly equal in magnitude.",
            "So over the course of the game, those errors tend to cancel out, and you can see here now, I don't know if you can see the green lines at the back there, but they kind of spread out across this entire graph and you get this very diverse set of rollouts.",
            "And now when you apply Monte Carlo you get this very nice average so that the mean of these rollouts is actually very close to the mini Max value.",
            "So this is what we want.",
            "So the key lesson here actually is that if you're playing a game with a large number of steps that are actually having balanced rollouts is much more important than having strong rollouts."
        ],
        [
            "So how can we turn this into an algorithm that's actually useful in practice?",
            "So I mentioned earlier this objective function, the mean squared bias.",
            "Well here it is in detail here.",
            "So the idea is basically we want to minimize the error between the mini Max value V star and the mean outcome of our rollouts, and we want to minimize this error kind of averaged over all possible positions, so the expected value on the outside there is over all possible positions that we think we might encounter, and now we're simply going to apply gradient descent to try and optimize this objective function.",
            "So here is the gradient of this objective function and it's just unrolled one step there using the chain rule and now you can see when we apply the chain rule we end up with these two terms.",
            "So we have this kind of bias term on the left there and this gradient term on the right there and then we have this outer expectation so we can deal with this our expectation which is the expectation over our training set by using stochastic gradient descent.",
            "So now how would this look if we applied this to?"
        ],
        [
            "State gradient descent algorithm.",
            "Well, we would have some weight update which would be proportional to this bias term multiplied by this gradient term.",
            "So if we look at these.",
            "The main thing to notice here is that this gradient term on the right should perhaps be a little familiar to many of us in the audience.",
            "So actually, if you think of this outcome, for example as a reward reward signal, then this actually is the policy gradient.",
            "This is the gradient of the expected reward, and so policy gradient reinforcement learning algorithms tell us how to calculate this term.",
            "Policy gradient algorithms such as reinforce actor critic, so forth.",
            "These algorithms are designed to actually figure out this policy gradient from experience, so this is something we can actually compute.",
            "And then the left hand turn the bias is this is simply telling us the difference between Monte Carlo estimate and the min and Max value.",
            "So we have these two terms here.",
            "So let's just try and see what this algorithm kind of means.",
            "Slightly more intuitively, when we have this bias tell on the left, and that essentially tells us where the black needs to win more or less, so it may be that the mini Max value tells us that black should be winning 70% of the time, but at Monte Carlo estimate actually ends up winning 60% of the time.",
            "So then this gives us.",
            "This tells us we would like to black to be winning 10% more.",
            "To be giving us a much more accurate evaluation of this position.",
            "Well, how can we win 10% more?",
            "How can we adjust our parameters so as to win 10% more for black?",
            "Well, that's what the policy gradient tells us.",
            "That tells us how can we adjust the parameters so that black will win more, and so the idea is simply to take the product of these terms."
        ],
        [
            "In the algorithm, more concretely, we now contend this into an algorithm, so we have some data set with mini Max values that have been generated by Deep Monte Carlo.",
            "Tree search is and the idea is we're going to have two steps to our rollouts.",
            "Now.",
            "First of all, we're going to roll out one set of simulations to estimate the bias between.",
            "So here we're just going to compute the Monte Carlo value this.",
            "Average value of the outcomes there, and we're going to look at the difference between that average outcome and the.",
            "And the Mini Max value so that estimates the bias and then the second step we're going to use policy gradient algorithm.",
            "So we're just going to use the simplest policy gradient algorithm.",
            "Reinforce here to estimate this policy gradient from a second set of simulations, and it's important that these are independent.",
            "'cause if you look at our objective.",
            "Our algorithm here, which we're trying to estimate.",
            "We actually have a product of expectations, and in order to make sure that product of expectations is unbiased, we need 2 separate sets of simulations to compute the bias and compute the policy gradient, and then we simply update the policy weights by the product of these two terms at every step for every training example.",
            "So at the end of the day, we actually need to commit to some kind of representation for our simulation policy, so a very intuitive and appealing Rep."
        ],
        [
            "Intention of policy is the softmax policy, so now we're going to represent our policy using a vector features of the position and the current and the move, and we're going to compute some preferences of those features, so these preferences tell us how much we would like to be in a particular state with a particular move, how much we would prefer to take that move over other moves, and then the softmax distribution simply favors higher preferences over lower preferences.",
            "So this is a stochastic policy parameterized by our feature vector by a weight vector."
        ],
        [
            "Pizza.",
            "And so now let's apply that to compute."
        ],
        [
            "Go and see what the results look like.",
            "OK, so again, the message of this talk really has been that there are two approaches to learning a simulation policy.",
            "We can either directly try and optimize the strength of that policy, and then hope that it will just perform well in Monte Carlo.",
            "Or you can use this new approach, which is to optimize this balance objective function.",
            "This new objective function and so we compared two methods for optimizing strength with two methods for optimizing balance.",
            "So to optimize strength we tried an apprenticeship learning approach, which essentially is an approach that maximizes the likelihood.",
            "Of the expert data, so it adjusts the parameters so as to make the expert data as likely as possible, and we also tried a policy gradient reinforcement learning methods, so this is using policy gradient this reinforce algorithm, but using it directly just to find the strongest possible to maximize reward to try and find the strongest possible policy.",
            "And we compare these two approaches with two methods for optimizing balancing for optimizing balance.",
            "The policy gradient simulation balancing that I just mentioned and the two step version of this which you can see in the paper."
        ],
        [
            "And we tried this in 5 by 5 and 6 by 6 go.",
            "We parameterized our softmax policy by a set of binary features that I'll explain in just a minute 2000 different features using around 100 unique weights and to generate our approximation to minimax, we generated training values by Deep Monte Carlo Tree Search is using the current World Champion program for wego.",
            "And our training positions were simply drawn from random versus random games, and we used for wego to roll out 10,000 simulations from each position to estimate this vstar value for each of those training positions.",
            "So now we have a big training set containing positions S and training values, V star of S, and we'd like to use that either in our supervised learning approach or in our new simulation balance."
        ],
        [
            "Approach and we're going to compare them.",
            "So those features I mentioned, well, they're very simple, they're just binary features that match a local configuration of stones.",
            "We just used one by one and two by two patterns of stones for all possible locations on the board for all possible configurations of stones, and we shared weights between symmetric shapes."
        ],
        [
            "OK, so here's what happens to the weights during the four different algorithms, and we don't need to spend too long in this.",
            "I don't have a huge amount of time, but really the main thing to take from this is that all four algorithms actually converge on stable solutions, and in each of the cases the values that they converge on a quite similar, but they're subtly different, and so in particular what you can notice here is that this kind of black line at the bottom.",
            "This tells you the value of what's called the dumpling shape.",
            "This is the worst possible shape you can make with a 2 by two shape.",
            "And go players know that this is just a terrible shape and all four algorithms very quickly give this a very large negative weight.",
            "On the other hand, the best shape the turn shape is quickly identified as being having very positive weight by all four of these algorithms, and it's very highly favored in this stochastic mixture.",
            "But the nice thing is that unlike policy gradient reinforcement learning, which actually just keeps going, it will actually become more and more deterministic overtime because it's just trying to become stronger and stronger and stronger.",
            "The new algorithms actually stabilize on this nice balanced mixture.",
            "This nice balance stochastic policy that kind of balances strength with diversity."
        ],
        [
            "OK, if we look at the mean squared error over the data set, we can again see, so the blue line in the green line, which maybe you can't see the green again, but it's somewhat higher correspond to.",
            "So first of all at the top there we have.",
            "What happens if you use uniform random as the simulation policy and it has a very large error in the.",
            "So this is mean squared error if you actually use this in a Monte Carlo simulation, what will your error be over the whole data set and using uniform random you have a very high error using apprenticeship learning or policy gradient learning you get somewhat lower.",
            "Error over the data set, but it has this undesirable policy that actually bounces that it starts to do a little better, but then it starts to do worse and worse and worse because it's just trying to make that policy stronger and stronger and it starts to lose diversity overtime, whereas the simulation balancing methods actually do much better and they continue to do better with additional."
        ],
        [
            "Training.",
            "This is also true in 6 by 6 go only.",
            "It's even more exaggerated and we have reason to believe that this continues to scale up that the advantage of simulation balancing becomes larger with more steps in the game with longer games and."
        ],
        [
            "Four with larger board sizes.",
            "So finally we ran some tournaments between each of the learned policies learned by the four different methods.",
            "We ran a tournament using a simple Monte Carlo search program using each of the learn simulation policies, and we ran a tournament in 5 by 5 go and is 6 by 6 go between each of these programs, and we used each of the simulation policies in two different ways.",
            "We first of all used it directly, so this is using the simulation policy actually to select the moves in the game directly.",
            "And Secondly we used it in a simple Monte Carlo.",
            "Search algorithm and what you can see here is that if you just use the simulation policies directly then it turns out that policy gradient reinforcement learning using the reinforcement learning paradigm as you might hope, actually does the best of all the algorithms that reinforcement learning actually learns to maximize the final outcome.",
            "But if you really care about Monte Carlo performance, then it turns out that using the simulation balancing paradigm actually gets more than two 100K points of benefit over.",
            "Either supervised learning or reinforcement learning paradigms and more than 340 Y lo more than just using uniform random.",
            "So this is really quite a nice benefit, and it's true in both 5 by 5 and 6 by 6 and one nice thing here is, even though we're just using a very simple Monte Carlo search, this very simple procedure using this lens simulation policy is already as strong as very sophisticated programmed new go that's been handcrafted over a period of maybe 10 or 15 years, and this very naive Monte Carlo search already performs as well as new go.",
            "Using this land policy.",
            "So.",
            "There's reason to be optimistic.",
            "We're currently in the process of scaling this up to larger board sizes, and the initial results are promising.",
            "Although it turns out to be somewhat trickier to actually deal with the datasets because the games are much longer and they need to be cleaned up before these algorithms can be applied."
        ],
        [
            "But the overall conclusions are that we can.",
            "Yes, we can learn a good simulation policy for Monte Carlo search and this new paradigm helps us balance the stochasticity of the rollouts so that we can have rollouts that balance strength with Stochastic City.",
            "With diversity we can actually exploit this expert data set much more effectively than just by using supervised learning, and we can optimize more relevant objective function than reinforcement learning does.",
            "And finally with this this approach outperforms prior methods in small board go.",
            "So any questions?"
        ],
        [
            "Yes.",
            "The Max value is not always the best thing to look.",
            "For example in poker.",
            "Your opponent is not playing optimally, and so the question is what is method be possible to extend in a way that that your your simulation method is actually based on the learning features of your apartment so that we would be able to.",
            "I'm so festival I completely agree that there are there are games for which mini Max is not the right solution to be shooting for and poker is the best known example.",
            "However, I think there is always some definition of optimal policy that you could be shooting for so in.",
            "In that sense, it may be the policy that maximally exploits a particular opponent in some way, and it may be possible to apply this same kind of procedure to other definitions of optimal policy then mini Max, but we haven't looked into that and I think it's very interesting.",
            "Other questions.",
            "I was wondering about that.",
            "So you are using policy gradient.",
            "To come up with a good policy.",
            "And you show that that you need a good policy.",
            "That's bad policy that if you use this other mental that's trying to this, trying to approximate the minimums.",
            "But shouldn't be the case that so.",
            "That that if you.",
            "Trying to maximize performance than the value of that policy should be closer to be.",
            "Ultimately it's value.",
            "Or sorry, I'm not sure I follow what you're asking.",
            "OK, OK.",
            "Question.",
            "Is it sufficient to maintain balance?",
            "I mean, I can imagine the difference to very weak players, and they're perfectly balanced.",
            "The trajectory is you might still get very bad baseball mean Imagine 2.",
            "Newbies in the game I counted.",
            "Pretty balanced, said he would get performance.",
            "That would give you good event set correctly, so it depends.",
            "So it depends on your definition of balance.",
            "So the objective function which we defined to be balanced is actually we're defining this objective function relative to the minimax objective.",
            "So what we would define to be balance is something which when you average over it, actually gives you the mini Max value.",
            "So if it so happens that your two very weak players average out in some way that magically gives you mini Max, then we're happy.",
            "This turns out to be a very nice policy to use in simulations.",
            "If it doesn't so in general.",
            "Two completely random players.",
            "If you average over there play, it will be very far from Mini Max.",
            "So in general actually the only way to achieve this balance criterion is to actually have a quite strong policy, and that's why you see actually."
        ],
        [
            "These white plots you see actually that even though there's no strength criterion in this balancing objective function that just by following this objective function actually implicitly helps achieve implicitly, does find a strong policy simply by finding the policy which when you average over it.",
            "It comes as close as possible to minimax.",
            "Java OK good.",
            "So you're trying to show that the optimal I function we start yes and don't shoot for that, right?",
            "So the policy that has the best bathrooms with shoes for that, yes.",
            "That shit is shooting for that as well.",
            "Yes, so this is.",
            "So this is true so.",
            "Exactly so, so the key issue here is.",
            "So the key issue here is.",
            "OK, maybe I can shine on so.",
            "The key issue here is that there is some kind of approximation here, so if you're able to find the optimal policy exactly, then all of these algorithms will find the optimal policy, and you're right.",
            "However, if you're unable to achieve optimal because there's some kind of function approximation necessary in the domain, the domain is too big to achieve optimal, then you have to allocate your resources in some way according to your function approximator.",
            "According to your policy parameterisation.",
            "And now it's very likely to be the case that one of these algorithms will allocate it.",
            "Resource is for that approximation in a very different way to the other.",
            "So what we want to do is allocate our resources for our approximation so that with finite resource is we come up with the best approximation to minimax given those resource limitations.",
            "Given the fact that we acknowledge that we have an approximate policy, and if you try and do policy gradient, it's not doing the same thing it's trying to shoot for the optimal policy directly, and this gives a very different approximation that actually is this key idea of Monte Carlo that actually you can get closer to your objective function.",
            "By averaging over a simple policy, then you can by trying to approximate that policy directly.",
            "OK.",
            "Plane is another way so.",
            "Should go back to the star or star or various other algorithms.",
            "Studied in the 80s.",
            "One of the things they do is often explore.",
            "Alternatives that don't appear currently to be the best alternative.",
            "I don't always expand.",
            "The best thing for the tree shop, and often that's a really good idea because the best leave maybe a certain drawer.",
            "There's no point any further in the second best leave.",
            "Maybe a speculative move that looks bad on the surface, but perhaps it will win.",
            "And if you were exploration policy.",
            "He's always just pursuing what appears to be the best move.",
            "Right then you'll never do that, and so you're wasting awful lot of computation.",
            "So what?",
            "What Monte Carlo is really doing is avoiding just following.",
            "The current best line, which is what you would do if you try to explore using just the optimal policy, is dumb.",
            "If you think that's how you're losing eventually.",
            "Yeah, yeah.",
            "Not just that, we should look at random news.",
            "They have to be both reasonable and potentially high value.",
            "Yes, exactly exactly, and this may vary from position to position as well.",
            "I would also.",
            "Yes.",
            "There's one other part, which is that.",
            "Is often more on thing to do in a tree that doesn't go all the way to terminal positions, so you may also get some benefit from.",
            "Operating information from multiple branches just just because.",
            "Apology.",
            "And maybe one thing I should add here is that so so we tried these experiments in Simple Monte Carlo set, which is to say you simply apply Monte Carlo simulation for every legal action.",
            "Now the natural extension to this is to apply it to Monte Carlo Tree search and really Monte Carlo tree search algorithms such as UCT.",
            "I mean the key idea there is to balance exploration intelligently so that you're not just following the Mini Max line so that you're actually choosing to invest some time in exploration which can help you a lot in the long run.",
            "So maybe that would help to address.",
            "One issue, but.",
            "I'm Csaba, kind of said something quietly behind you whilst you're talking, which is this idea that if you just drive to improve the performance of your policy then you will lose diversity by trying to.",
            "Act optimally and I think this is really a very central issue that we encounter here and what we'd like to do is to there are many positions in go, for example, where there is really an obvious right move, and we would like our Monte Carlo simulations to just take that right, move all the time.",
            "However, there are other positions in which it's really important to maintain diversity, and that the best way to actually find the right action using Monte Carlo is to average over a number of different possibilities, and so really the key idea is to find a way to automatically decide how to balance out.",
            "This issue so as to get at the get at the strong policy in some simple way.",
            "Maybe?",
            "Alright.",
            "Perhaps, perhaps, maybe it's just terminology.",
            "OK. Vincent, that you're having trouble taking this to bigger.",
            "So.",
            "I wouldn't say trouble.",
            "Actually, I think it looks quite promising.",
            "It's so we've tried some initial experiments on 9 by 9 and actually we get very similar looking."
        ],
        [
            "Mean squared error plots.",
            "However, the difficulty on large boards is simply just deciding which part of the simulation, which part of the game do you want to know?",
            "How do you generate your training data?",
            "So in the game of go, there's this huge difference between at the end of the game.",
            "There are many, many moves which almost have no effect on the actual outcome of the game, and if you train too heavily on these positions, it actually means that you're really you're balancing algorithm learns to balance out this kind of relevant data, and so there's really just this careful this.",
            "Slightly tricky issue of just choosing a data set that actually leads to high performance in the long run, so that's kind of what we've just been.",
            "It's just a practical issue, but it seems like the same principles apply and I think I think actually the opposite should be true in the long run, which is it should be the case that for larger boards the advantage of simulation balancing over over over supervised learning reinforcement learning should be extended simply because there are many, many many steps in each game, and therefore this kind of.",
            "It's just like these plots I showed earlier.",
            "The number of steps."
        ],
        [
            "In which the bias.",
            "Can increase over is is very large that the longer this time scale the more the more important is to be unbiased rather than rather than strong.",
            "Thank you very much again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi everyone, thank you for your patience.",
                    "label": 0
                },
                {
                    "sent": "Both just then and also staying right to the end of the final session of the conference.",
                    "label": 0
                },
                {
                    "sent": "I'm David Silva from the University of Alberta.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with Jerry Tesauro from IBM Research and it's really a pleasure to be here to tell you about this.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to begin by giving a little bit of background on Monte Carlo search and what's been happening recently there.",
                    "label": 0
                },
                {
                    "sent": "Before I go on to the new.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tradition of this work.",
                    "label": 0
                },
                {
                    "sent": "So in two player games, Monte Carlo searches really actually acquired quite a number of successes.",
                    "label": 0
                },
                {
                    "sent": "Now over the years, so beginning with a number of stochastic two player games, so backgammon.",
                    "label": 0
                },
                {
                    "sent": "Sarah and others were able to achieve master level in backgammon back in 97 using a simple form of Monte Carlo search, and this success was then repeated in poker and Scrabble and then a few years ago people were able to extend this success to deterministic games such as Go first of all achieving master level in 99 Go and this year now extending those successes to full size go 19 by 19.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go.",
                    "label": 0
                },
                {
                    "sent": "So really what this talk is about is how these successes can be taken even further.",
                    "label": 0
                },
                {
                    "sent": "Perhaps looking to the future.",
                    "label": 0
                },
                {
                    "sent": "So the key idea of Monte Carlo search is to simulate a number of rollouts from each position using some simulation policy, and it turns out that the performance of Monte Carlo search really depends critically on the particular simulation policy that you use.",
                    "label": 0
                },
                {
                    "sent": "And especially in terministic domains, it turns out that it's not just good enough to have a strong simulation policy.",
                    "label": 0
                },
                {
                    "sent": "You also need some kind of diversity.",
                    "label": 0
                },
                {
                    "sent": "You need stochastic rollouts you need to make sure that you get this nice wide variety of simulations.",
                    "label": 0
                },
                {
                    "sent": "And we've seen time and again many different examples where a strong policy improving the quality of the policy directly does not necessarily improve the performance of the overall Monte Carlo player.",
                    "label": 0
                },
                {
                    "sent": "So the problem that we're trying to address it with this work is how can we automatically learn a good simulation policy?",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to focus on the game of go, but this work could be applied to any two player game.",
                    "label": 0
                },
                {
                    "sent": "And in the game of go in particular, we've seen that the simulation policy has a really dramatic effect on the performance of a Monte Carlo player.",
                    "label": 1
                },
                {
                    "sent": "In fact, it's the single number one factor determining the overall performance of Monte Carlo player.",
                    "label": 0
                },
                {
                    "sent": "So unsurprisingly, people have tried a number of different techniques to construct a simulation player, the most common being handcrafting the simulation policy by trial and error.",
                    "label": 0
                },
                {
                    "sent": "So essentially it turns out that our human intuitions, as go players actually don't help very much in this case, because really, our human intuition is trying to.",
                    "label": 0
                },
                {
                    "sent": "Tell us how to make the simulation policy stronger, and this, as we've seen, doesn't necessarily make the overall Monte Carlo player stronger.",
                    "label": 0
                },
                {
                    "sent": "So what about doing this automatically?",
                    "label": 0
                },
                {
                    "sent": "Well, some people have tried Hill climbing methods, such as evolutionary methods or cross entropy methods.",
                    "label": 0
                },
                {
                    "sent": "The problem here is that each single evaluation of a Monte Carlo search program is very expensive, so you might need hundreds of games of real games played out by the Monte Carlo program, which each game contains hundreds of positions, and each position may need 10s of thousands of simulations.",
                    "label": 0
                },
                {
                    "sent": "Just to be able to get one evaluation point during this kind of optimization process.",
                    "label": 1
                },
                {
                    "sent": "So what about the traditional paradigms of supervised learning and reinforcement learning?",
                    "label": 0
                },
                {
                    "sent": "Well, these are very successful.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn a strong policy.",
                    "label": 0
                },
                {
                    "sent": "If you really want to learn a policy that directly performs well in a domain, then you can apply supervised learning or reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "But this again is not really attacking the problem of how can we make our Monte Carlo search stronger, so this talk is all about attacking that that specific problem.",
                    "label": 0
                },
                {
                    "sent": "How can we actually do better in a Monte Carlo search by changing choosing an objective function which really is matched up with this?",
                    "label": 0
                },
                {
                    "sent": "Thing which we are trying to do so if we really want to achieve good Monte Carlo performance, what objective should we be optimizing?",
                    "label": 0
                },
                {
                    "sent": "And the answer just to give you a sneak preview.",
                    "label": 0
                },
                {
                    "sent": "The answer is yes, although this is really just focusing on small board goes so far and I'll just touch on the end.",
                    "label": 0
                },
                {
                    "sent": "You know perhaps.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How this can be extended?",
                    "label": 0
                },
                {
                    "sent": "So OK, I'll just give some background on Monte Carlo search so we have a two player game at the end of the game.",
                    "label": 0
                },
                {
                    "sent": "There's some outcomes Ed, so we're going to say there's two players will just call black and white.",
                    "label": 0
                },
                {
                    "sent": "If Black wins, then the outcome is going to be zed equals one, and if white wins then Z = 0 and we can then define the value of a position to simply be the expected outcome at the end of the game.",
                    "label": 1
                },
                {
                    "sent": "And the key idea here is that we can estimate this this value very simply by Monte Carlo by just performing some simulations and taking the average outcome from those simulation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in slightly more detail, the idea is that we start our simulations from the current position S and execute a number of water called rollouts.",
                    "label": 0
                },
                {
                    "sent": "From this position, using some simulation policy.",
                    "label": 0
                },
                {
                    "sent": "So the simplest example would be that simulation policy is uniform random, so games are played out a rolled out by random from the current position, and that position is evaluated simply by the mean outcome.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of those simulations.",
                    "label": 0
                },
                {
                    "sent": "So here's a little example.",
                    "label": 0
                },
                {
                    "sent": "So at the top here you can see the actual current position in the game, and we'd like to know what's the value of this position.",
                    "label": 1
                },
                {
                    "sent": "And here we see four different simulations have been rolled out from that position, and in two of them black ended up winning with the outcome of one and two of them.",
                    "label": 0
                },
                {
                    "sent": "White ended up winning, so we can just say we can just evaluate this position to have a value of nought .5.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How can this be used in practice?",
                    "label": 0
                },
                {
                    "sent": "Well, the simplest approach which was used in backgammon, Scrabble, Poker, which I mentioned the beginning, is simple Monte Carlo search, where every single legal move is evaluated by Monte Carlo simulation, and the idea is simply to select the move with the highest value.",
                    "label": 0
                },
                {
                    "sent": "After this, this process is complete.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More recently, this has been extended into to use ideas from minimax search to develop a search tree during simulation.",
                    "label": 0
                },
                {
                    "sent": "So the idea here in Monte Carlo Tree search is to start simulations from the current position again, simulate thousands of positions from the current simulation.",
                    "label": 1
                },
                {
                    "sent": "But while you're doing this, you can actually build up a search tree that contains all visited positions and every position in that search tree can then be evaluated by Monte Carlo by just the average outcome of all simulations that pass through that position.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that with a suitable level of exploration, for example using the UCT algorithm, then this approach will actually converge on the minimax game tree, given enough time in memory.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's a little bit of background on Monte Carlo.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Search in general.",
                    "label": 0
                },
                {
                    "sent": "So now here's the new idea.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned, if we really want to perform well in this domain, we'd like to be able to in Monte Carlo search, we'd like to be able to find a simulation policy that actually really is effective in Monte Carlo search.",
                    "label": 0
                },
                {
                    "sent": "So here's the setup.",
                    "label": 0
                },
                {
                    "sent": "We're going to use to define an objective function.",
                    "label": 0
                },
                {
                    "sent": "We're going to have some stochastic simulation policy Pi that's parameterized by a vector of weights, Theta.",
                    "label": 1
                },
                {
                    "sent": "And the key idea here is that any simulation policy, even if it's uniform random, actually has some bias with respect to the true minimax value.",
                    "label": 0
                },
                {
                    "sent": "So here we can see the bias is defined as the mini Max value V star minus the expected outcome.",
                    "label": 0
                },
                {
                    "sent": "If you are following this particular simulation policy.",
                    "label": 0
                },
                {
                    "sent": "So really this expected outcome.",
                    "label": 0
                },
                {
                    "sent": "This tells us what would happen.",
                    "label": 0
                },
                {
                    "sent": "This is the Monte Carlo Monte Carlo value.",
                    "label": 0
                },
                {
                    "sent": "Given an infinite number of simulations, this is what Monte Carlo will find in the limit.",
                    "label": 0
                },
                {
                    "sent": "Is this expected value here?",
                    "label": 0
                },
                {
                    "sent": "And so the bias is the difference between.",
                    "label": 0
                },
                {
                    "sent": "This Monte Carlo value in the limit and the true mini Max value.",
                    "label": 0
                },
                {
                    "sent": "What we'd like is for that to be small.",
                    "label": 0
                },
                {
                    "sent": "We would like to find a simulation policy that has bias that's as small as possible over a set of the set of positions which we expect to encounter.",
                    "label": 0
                },
                {
                    "sent": "So that's our aim.",
                    "label": 0
                },
                {
                    "sent": "We're going to formulate this into an objective function, which is to minimize the mean squared bias, and we're going to try and find parameters that minimize that.",
                    "label": 0
                },
                {
                    "sent": "Minimize that objective function.",
                    "label": 0
                },
                {
                    "sent": "So the key issue here, which you probably notice Taz that we've included the true minimax value in our objective function, and in something like Go this, is actually pretty difficult to evaluate.",
                    "label": 0
                },
                {
                    "sent": "But actually we can make a fairly reasonable approximation using Deep Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "Tree search is so, as I mentioned earlier, Monte Carlo Tree search, given enough computation, will actually find the min and Max value.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we simply perform sufficient number of simulations that we actually have a reasonable approximation, and we can construct a training set of positions and values of those positions.",
                    "label": 0
                },
                {
                    "sent": "That we can use to then train our simulation policy.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea here.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm just going to try and give a bit more intuition behind this idea in the next couple of slides, so I've talked a little bit about strength and I'm going to introduce this idea of balance as well so.",
                    "label": 0
                },
                {
                    "sent": "Really, you could imagine that in any position we can think of the there's some true mini Max value of that position, and every time the agent of player actually selects and move the best they can do is actually to maintain that that mini Max value if they play a bad move, they'll actually reduce the mini Max value.",
                    "label": 0
                },
                {
                    "sent": "They'll so for example, Black will have some error that would be negative, black would be losing some value with respect to minimax, and if white plays a bad move, white will be increasing the value of the successive position.",
                    "label": 0
                },
                {
                    "sent": "And the key idea here is that strong policy will have small errors that these errors will in general be quite low.",
                    "label": 1
                },
                {
                    "sent": "That the scale of the error committed by each player will be small.",
                    "label": 0
                },
                {
                    "sent": "In general, that's what a strong policy is that not very many errors are committed, or those errors are going to be small in magnitude.",
                    "label": 0
                },
                {
                    "sent": "What we want is something a little bit different.",
                    "label": 1
                },
                {
                    "sent": "We want to balance policy, which means that the expected total error over the course of the game will be small.",
                    "label": 1
                },
                {
                    "sent": "So this means we don't actually mind if if the players make errors, as long as they cancel out over the course of the game.",
                    "label": 0
                },
                {
                    "sent": "So we could either do this over the course of the entire game, which is what I'm going to discuss in this presentation, or you could try and balance just two steps at a time.",
                    "label": 0
                },
                {
                    "sent": "So this would mean that it's OK if black makes an error as long as white makes an error immediately afterwards that cancels out.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in that idea, then please look at the paper.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to focus on the full balancing.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is here.",
                    "label": 0
                },
                {
                    "sent": "So I'll give a little more integration in these diagrams, I hope so.",
                    "label": 0
                },
                {
                    "sent": "These diagrams represent it's actually some experiments with an artificial game, and in the start position of this artificial game the minimax value is actually 0.",
                    "label": 0
                },
                {
                    "sent": "So the Max value is represented there on the Y axis, and what we have on the X axis.",
                    "label": 0
                },
                {
                    "sent": "Here is the move number in the game, and each of these green lines that I hope people can just about make out at the back there.",
                    "label": 0
                },
                {
                    "sent": "There's a bunch of green lines.",
                    "label": 0
                },
                {
                    "sent": "That starting with the from the current position and each of these green lines represent A roll out from that start position high.",
                    "label": 0
                },
                {
                    "sent": "But yeah, the minimax value is the game optimal value if you constructed a search tree such that black is maximizing his value at every move and white is minimizing her value every move, then this is the mini Max value.",
                    "label": 0
                },
                {
                    "sent": "So it's not just one min one Max it.",
                    "label": 0
                },
                {
                    "sent": "So I'm in a Max kind of iterated over the entire game tree.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what being represented on the Y axis here.",
                    "label": 0
                },
                {
                    "sent": "The Mini Max value and each of these green lines represents a trajectory a game that's being rolled out from this position for 100 time steps.",
                    "label": 1
                },
                {
                    "sent": "And in this particular example here.",
                    "label": 0
                },
                {
                    "sent": "So now we have a whole bunch of different trajectories in this black line.",
                    "label": 0
                },
                {
                    "sent": "In the middle represents the mean value of each of these of these 30 different trajectories that have been rolled out.",
                    "label": 0
                },
                {
                    "sent": "And in this particular example here, what we have here is both of these players are actually that are rolling out this.",
                    "label": 0
                },
                {
                    "sent": "Games are quite strong.",
                    "label": 0
                },
                {
                    "sent": "You can see there committing errors, but the magnitude of those errors is quite small, but it turns out the Blacks just a little bit stronger than white in this example.",
                    "label": 0
                },
                {
                    "sent": "So the first step here, Black makes a very small error.",
                    "label": 0
                },
                {
                    "sent": "White makes a slightly bigger error.",
                    "label": 0
                },
                {
                    "sent": "Black makes a small error, white makes a slightly bigger error, and so on.",
                    "label": 0
                },
                {
                    "sent": "Throughout this game and the net result over 100 different moves is actually a great deal of bias that the value at the end of these hundred moves, the minimax value of where these rollups end up, is actually very very biased, because Black has played a little bit stronger throughout a large number of moves and White House.",
                    "label": 0
                },
                {
                    "sent": "And so if you're actually trying to play Monte Carlo evaluation to this set of trajectories, it's going to give a very big bias here.",
                    "label": 0
                },
                {
                    "sent": "The difference between this red line and the mini Max value 0 here.",
                    "label": 0
                },
                {
                    "sent": "So there's this big bias here, and that's what we want to reduce.",
                    "label": 0
                },
                {
                    "sent": "We don't want to see that bias.",
                    "label": 0
                },
                {
                    "sent": "That means that Monte Carlo is giving us the wrong answer.",
                    "label": 0
                },
                {
                    "sent": "It's not giving us the minimax value in this city.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in contrast, what we'd like to see is something a bit more like this, where now we have two very weak players, so both black and white are very weak.",
                    "label": 0
                },
                {
                    "sent": "They are committing quite large errors at every step, but those errors that are roughly equal in magnitude.",
                    "label": 0
                },
                {
                    "sent": "So over the course of the game, those errors tend to cancel out, and you can see here now, I don't know if you can see the green lines at the back there, but they kind of spread out across this entire graph and you get this very diverse set of rollouts.",
                    "label": 0
                },
                {
                    "sent": "And now when you apply Monte Carlo you get this very nice average so that the mean of these rollouts is actually very close to the mini Max value.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want.",
                    "label": 0
                },
                {
                    "sent": "So the key lesson here actually is that if you're playing a game with a large number of steps that are actually having balanced rollouts is much more important than having strong rollouts.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can we turn this into an algorithm that's actually useful in practice?",
                    "label": 0
                },
                {
                    "sent": "So I mentioned earlier this objective function, the mean squared bias.",
                    "label": 0
                },
                {
                    "sent": "Well here it is in detail here.",
                    "label": 0
                },
                {
                    "sent": "So the idea is basically we want to minimize the error between the mini Max value V star and the mean outcome of our rollouts, and we want to minimize this error kind of averaged over all possible positions, so the expected value on the outside there is over all possible positions that we think we might encounter, and now we're simply going to apply gradient descent to try and optimize this objective function.",
                    "label": 1
                },
                {
                    "sent": "So here is the gradient of this objective function and it's just unrolled one step there using the chain rule and now you can see when we apply the chain rule we end up with these two terms.",
                    "label": 0
                },
                {
                    "sent": "So we have this kind of bias term on the left there and this gradient term on the right there and then we have this outer expectation so we can deal with this our expectation which is the expectation over our training set by using stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So now how would this look if we applied this to?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "State gradient descent algorithm.",
                    "label": 0
                },
                {
                    "sent": "Well, we would have some weight update which would be proportional to this bias term multiplied by this gradient term.",
                    "label": 1
                },
                {
                    "sent": "So if we look at these.",
                    "label": 0
                },
                {
                    "sent": "The main thing to notice here is that this gradient term on the right should perhaps be a little familiar to many of us in the audience.",
                    "label": 0
                },
                {
                    "sent": "So actually, if you think of this outcome, for example as a reward reward signal, then this actually is the policy gradient.",
                    "label": 0
                },
                {
                    "sent": "This is the gradient of the expected reward, and so policy gradient reinforcement learning algorithms tell us how to calculate this term.",
                    "label": 1
                },
                {
                    "sent": "Policy gradient algorithms such as reinforce actor critic, so forth.",
                    "label": 0
                },
                {
                    "sent": "These algorithms are designed to actually figure out this policy gradient from experience, so this is something we can actually compute.",
                    "label": 0
                },
                {
                    "sent": "And then the left hand turn the bias is this is simply telling us the difference between Monte Carlo estimate and the min and Max value.",
                    "label": 0
                },
                {
                    "sent": "So we have these two terms here.",
                    "label": 0
                },
                {
                    "sent": "So let's just try and see what this algorithm kind of means.",
                    "label": 0
                },
                {
                    "sent": "Slightly more intuitively, when we have this bias tell on the left, and that essentially tells us where the black needs to win more or less, so it may be that the mini Max value tells us that black should be winning 70% of the time, but at Monte Carlo estimate actually ends up winning 60% of the time.",
                    "label": 1
                },
                {
                    "sent": "So then this gives us.",
                    "label": 0
                },
                {
                    "sent": "This tells us we would like to black to be winning 10% more.",
                    "label": 0
                },
                {
                    "sent": "To be giving us a much more accurate evaluation of this position.",
                    "label": 0
                },
                {
                    "sent": "Well, how can we win 10% more?",
                    "label": 0
                },
                {
                    "sent": "How can we adjust our parameters so as to win 10% more for black?",
                    "label": 1
                },
                {
                    "sent": "Well, that's what the policy gradient tells us.",
                    "label": 0
                },
                {
                    "sent": "That tells us how can we adjust the parameters so that black will win more, and so the idea is simply to take the product of these terms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the algorithm, more concretely, we now contend this into an algorithm, so we have some data set with mini Max values that have been generated by Deep Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Tree search is and the idea is we're going to have two steps to our rollouts.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "First of all, we're going to roll out one set of simulations to estimate the bias between.",
                    "label": 1
                },
                {
                    "sent": "So here we're just going to compute the Monte Carlo value this.",
                    "label": 0
                },
                {
                    "sent": "Average value of the outcomes there, and we're going to look at the difference between that average outcome and the.",
                    "label": 0
                },
                {
                    "sent": "And the Mini Max value so that estimates the bias and then the second step we're going to use policy gradient algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we're just going to use the simplest policy gradient algorithm.",
                    "label": 0
                },
                {
                    "sent": "Reinforce here to estimate this policy gradient from a second set of simulations, and it's important that these are independent.",
                    "label": 0
                },
                {
                    "sent": "'cause if you look at our objective.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm here, which we're trying to estimate.",
                    "label": 0
                },
                {
                    "sent": "We actually have a product of expectations, and in order to make sure that product of expectations is unbiased, we need 2 separate sets of simulations to compute the bias and compute the policy gradient, and then we simply update the policy weights by the product of these two terms at every step for every training example.",
                    "label": 1
                },
                {
                    "sent": "So at the end of the day, we actually need to commit to some kind of representation for our simulation policy, so a very intuitive and appealing Rep.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Intention of policy is the softmax policy, so now we're going to represent our policy using a vector features of the position and the current and the move, and we're going to compute some preferences of those features, so these preferences tell us how much we would like to be in a particular state with a particular move, how much we would prefer to take that move over other moves, and then the softmax distribution simply favors higher preferences over lower preferences.",
                    "label": 0
                },
                {
                    "sent": "So this is a stochastic policy parameterized by our feature vector by a weight vector.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pizza.",
                    "label": 0
                },
                {
                    "sent": "And so now let's apply that to compute.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go and see what the results look like.",
                    "label": 0
                },
                {
                    "sent": "OK, so again, the message of this talk really has been that there are two approaches to learning a simulation policy.",
                    "label": 0
                },
                {
                    "sent": "We can either directly try and optimize the strength of that policy, and then hope that it will just perform well in Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Or you can use this new approach, which is to optimize this balance objective function.",
                    "label": 0
                },
                {
                    "sent": "This new objective function and so we compared two methods for optimizing strength with two methods for optimizing balance.",
                    "label": 1
                },
                {
                    "sent": "So to optimize strength we tried an apprenticeship learning approach, which essentially is an approach that maximizes the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Of the expert data, so it adjusts the parameters so as to make the expert data as likely as possible, and we also tried a policy gradient reinforcement learning methods, so this is using policy gradient this reinforce algorithm, but using it directly just to find the strongest possible to maximize reward to try and find the strongest possible policy.",
                    "label": 0
                },
                {
                    "sent": "And we compare these two approaches with two methods for optimizing balancing for optimizing balance.",
                    "label": 0
                },
                {
                    "sent": "The policy gradient simulation balancing that I just mentioned and the two step version of this which you can see in the paper.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we tried this in 5 by 5 and 6 by 6 go.",
                    "label": 0
                },
                {
                    "sent": "We parameterized our softmax policy by a set of binary features that I'll explain in just a minute 2000 different features using around 100 unique weights and to generate our approximation to minimax, we generated training values by Deep Monte Carlo Tree Search is using the current World Champion program for wego.",
                    "label": 1
                },
                {
                    "sent": "And our training positions were simply drawn from random versus random games, and we used for wego to roll out 10,000 simulations from each position to estimate this vstar value for each of those training positions.",
                    "label": 0
                },
                {
                    "sent": "So now we have a big training set containing positions S and training values, V star of S, and we'd like to use that either in our supervised learning approach or in our new simulation balance.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach and we're going to compare them.",
                    "label": 0
                },
                {
                    "sent": "So those features I mentioned, well, they're very simple, they're just binary features that match a local configuration of stones.",
                    "label": 1
                },
                {
                    "sent": "We just used one by one and two by two patterns of stones for all possible locations on the board for all possible configurations of stones, and we shared weights between symmetric shapes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so here's what happens to the weights during the four different algorithms, and we don't need to spend too long in this.",
                    "label": 0
                },
                {
                    "sent": "I don't have a huge amount of time, but really the main thing to take from this is that all four algorithms actually converge on stable solutions, and in each of the cases the values that they converge on a quite similar, but they're subtly different, and so in particular what you can notice here is that this kind of black line at the bottom.",
                    "label": 0
                },
                {
                    "sent": "This tells you the value of what's called the dumpling shape.",
                    "label": 0
                },
                {
                    "sent": "This is the worst possible shape you can make with a 2 by two shape.",
                    "label": 0
                },
                {
                    "sent": "And go players know that this is just a terrible shape and all four algorithms very quickly give this a very large negative weight.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the best shape the turn shape is quickly identified as being having very positive weight by all four of these algorithms, and it's very highly favored in this stochastic mixture.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing is that unlike policy gradient reinforcement learning, which actually just keeps going, it will actually become more and more deterministic overtime because it's just trying to become stronger and stronger and stronger.",
                    "label": 0
                },
                {
                    "sent": "The new algorithms actually stabilize on this nice balanced mixture.",
                    "label": 0
                },
                {
                    "sent": "This nice balance stochastic policy that kind of balances strength with diversity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, if we look at the mean squared error over the data set, we can again see, so the blue line in the green line, which maybe you can't see the green again, but it's somewhat higher correspond to.",
                    "label": 0
                },
                {
                    "sent": "So first of all at the top there we have.",
                    "label": 0
                },
                {
                    "sent": "What happens if you use uniform random as the simulation policy and it has a very large error in the.",
                    "label": 0
                },
                {
                    "sent": "So this is mean squared error if you actually use this in a Monte Carlo simulation, what will your error be over the whole data set and using uniform random you have a very high error using apprenticeship learning or policy gradient learning you get somewhat lower.",
                    "label": 1
                },
                {
                    "sent": "Error over the data set, but it has this undesirable policy that actually bounces that it starts to do a little better, but then it starts to do worse and worse and worse because it's just trying to make that policy stronger and stronger and it starts to lose diversity overtime, whereas the simulation balancing methods actually do much better and they continue to do better with additional.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Training.",
                    "label": 0
                },
                {
                    "sent": "This is also true in 6 by 6 go only.",
                    "label": 0
                },
                {
                    "sent": "It's even more exaggerated and we have reason to believe that this continues to scale up that the advantage of simulation balancing becomes larger with more steps in the game with longer games and.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Four with larger board sizes.",
                    "label": 0
                },
                {
                    "sent": "So finally we ran some tournaments between each of the learned policies learned by the four different methods.",
                    "label": 0
                },
                {
                    "sent": "We ran a tournament using a simple Monte Carlo search program using each of the learn simulation policies, and we ran a tournament in 5 by 5 go and is 6 by 6 go between each of these programs, and we used each of the simulation policies in two different ways.",
                    "label": 0
                },
                {
                    "sent": "We first of all used it directly, so this is using the simulation policy actually to select the moves in the game directly.",
                    "label": 0
                },
                {
                    "sent": "And Secondly we used it in a simple Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Search algorithm and what you can see here is that if you just use the simulation policies directly then it turns out that policy gradient reinforcement learning using the reinforcement learning paradigm as you might hope, actually does the best of all the algorithms that reinforcement learning actually learns to maximize the final outcome.",
                    "label": 0
                },
                {
                    "sent": "But if you really care about Monte Carlo performance, then it turns out that using the simulation balancing paradigm actually gets more than two 100K points of benefit over.",
                    "label": 0
                },
                {
                    "sent": "Either supervised learning or reinforcement learning paradigms and more than 340 Y lo more than just using uniform random.",
                    "label": 0
                },
                {
                    "sent": "So this is really quite a nice benefit, and it's true in both 5 by 5 and 6 by 6 and one nice thing here is, even though we're just using a very simple Monte Carlo search, this very simple procedure using this lens simulation policy is already as strong as very sophisticated programmed new go that's been handcrafted over a period of maybe 10 or 15 years, and this very naive Monte Carlo search already performs as well as new go.",
                    "label": 0
                },
                {
                    "sent": "Using this land policy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "There's reason to be optimistic.",
                    "label": 0
                },
                {
                    "sent": "We're currently in the process of scaling this up to larger board sizes, and the initial results are promising.",
                    "label": 0
                },
                {
                    "sent": "Although it turns out to be somewhat trickier to actually deal with the datasets because the games are much longer and they need to be cleaned up before these algorithms can be applied.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the overall conclusions are that we can.",
                    "label": 0
                },
                {
                    "sent": "Yes, we can learn a good simulation policy for Monte Carlo search and this new paradigm helps us balance the stochasticity of the rollouts so that we can have rollouts that balance strength with Stochastic City.",
                    "label": 1
                },
                {
                    "sent": "With diversity we can actually exploit this expert data set much more effectively than just by using supervised learning, and we can optimize more relevant objective function than reinforcement learning does.",
                    "label": 1
                },
                {
                    "sent": "And finally with this this approach outperforms prior methods in small board go.",
                    "label": 0
                },
                {
                    "sent": "So any questions?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The Max value is not always the best thing to look.",
                    "label": 0
                },
                {
                    "sent": "For example in poker.",
                    "label": 0
                },
                {
                    "sent": "Your opponent is not playing optimally, and so the question is what is method be possible to extend in a way that that your your simulation method is actually based on the learning features of your apartment so that we would be able to.",
                    "label": 0
                },
                {
                    "sent": "I'm so festival I completely agree that there are there are games for which mini Max is not the right solution to be shooting for and poker is the best known example.",
                    "label": 0
                },
                {
                    "sent": "However, I think there is always some definition of optimal policy that you could be shooting for so in.",
                    "label": 0
                },
                {
                    "sent": "In that sense, it may be the policy that maximally exploits a particular opponent in some way, and it may be possible to apply this same kind of procedure to other definitions of optimal policy then mini Max, but we haven't looked into that and I think it's very interesting.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "I was wondering about that.",
                    "label": 0
                },
                {
                    "sent": "So you are using policy gradient.",
                    "label": 0
                },
                {
                    "sent": "To come up with a good policy.",
                    "label": 0
                },
                {
                    "sent": "And you show that that you need a good policy.",
                    "label": 0
                },
                {
                    "sent": "That's bad policy that if you use this other mental that's trying to this, trying to approximate the minimums.",
                    "label": 0
                },
                {
                    "sent": "But shouldn't be the case that so.",
                    "label": 0
                },
                {
                    "sent": "That that if you.",
                    "label": 0
                },
                {
                    "sent": "Trying to maximize performance than the value of that policy should be closer to be.",
                    "label": 0
                },
                {
                    "sent": "Ultimately it's value.",
                    "label": 0
                },
                {
                    "sent": "Or sorry, I'm not sure I follow what you're asking.",
                    "label": 0
                },
                {
                    "sent": "OK, OK.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Is it sufficient to maintain balance?",
                    "label": 0
                },
                {
                    "sent": "I mean, I can imagine the difference to very weak players, and they're perfectly balanced.",
                    "label": 0
                },
                {
                    "sent": "The trajectory is you might still get very bad baseball mean Imagine 2.",
                    "label": 0
                },
                {
                    "sent": "Newbies in the game I counted.",
                    "label": 0
                },
                {
                    "sent": "Pretty balanced, said he would get performance.",
                    "label": 0
                },
                {
                    "sent": "That would give you good event set correctly, so it depends.",
                    "label": 0
                },
                {
                    "sent": "So it depends on your definition of balance.",
                    "label": 0
                },
                {
                    "sent": "So the objective function which we defined to be balanced is actually we're defining this objective function relative to the minimax objective.",
                    "label": 0
                },
                {
                    "sent": "So what we would define to be balance is something which when you average over it, actually gives you the mini Max value.",
                    "label": 0
                },
                {
                    "sent": "So if it so happens that your two very weak players average out in some way that magically gives you mini Max, then we're happy.",
                    "label": 0
                },
                {
                    "sent": "This turns out to be a very nice policy to use in simulations.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't so in general.",
                    "label": 0
                },
                {
                    "sent": "Two completely random players.",
                    "label": 0
                },
                {
                    "sent": "If you average over there play, it will be very far from Mini Max.",
                    "label": 0
                },
                {
                    "sent": "So in general actually the only way to achieve this balance criterion is to actually have a quite strong policy, and that's why you see actually.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These white plots you see actually that even though there's no strength criterion in this balancing objective function that just by following this objective function actually implicitly helps achieve implicitly, does find a strong policy simply by finding the policy which when you average over it.",
                    "label": 0
                },
                {
                    "sent": "It comes as close as possible to minimax.",
                    "label": 0
                },
                {
                    "sent": "Java OK good.",
                    "label": 0
                },
                {
                    "sent": "So you're trying to show that the optimal I function we start yes and don't shoot for that, right?",
                    "label": 0
                },
                {
                    "sent": "So the policy that has the best bathrooms with shoes for that, yes.",
                    "label": 0
                },
                {
                    "sent": "That shit is shooting for that as well.",
                    "label": 0
                },
                {
                    "sent": "Yes, so this is.",
                    "label": 0
                },
                {
                    "sent": "So this is true so.",
                    "label": 0
                },
                {
                    "sent": "Exactly so, so the key issue here is.",
                    "label": 0
                },
                {
                    "sent": "So the key issue here is.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I can shine on so.",
                    "label": 0
                },
                {
                    "sent": "The key issue here is that there is some kind of approximation here, so if you're able to find the optimal policy exactly, then all of these algorithms will find the optimal policy, and you're right.",
                    "label": 0
                },
                {
                    "sent": "However, if you're unable to achieve optimal because there's some kind of function approximation necessary in the domain, the domain is too big to achieve optimal, then you have to allocate your resources in some way according to your function approximator.",
                    "label": 0
                },
                {
                    "sent": "According to your policy parameterisation.",
                    "label": 0
                },
                {
                    "sent": "And now it's very likely to be the case that one of these algorithms will allocate it.",
                    "label": 0
                },
                {
                    "sent": "Resource is for that approximation in a very different way to the other.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is allocate our resources for our approximation so that with finite resource is we come up with the best approximation to minimax given those resource limitations.",
                    "label": 0
                },
                {
                    "sent": "Given the fact that we acknowledge that we have an approximate policy, and if you try and do policy gradient, it's not doing the same thing it's trying to shoot for the optimal policy directly, and this gives a very different approximation that actually is this key idea of Monte Carlo that actually you can get closer to your objective function.",
                    "label": 0
                },
                {
                    "sent": "By averaging over a simple policy, then you can by trying to approximate that policy directly.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Plane is another way so.",
                    "label": 0
                },
                {
                    "sent": "Should go back to the star or star or various other algorithms.",
                    "label": 0
                },
                {
                    "sent": "Studied in the 80s.",
                    "label": 0
                },
                {
                    "sent": "One of the things they do is often explore.",
                    "label": 0
                },
                {
                    "sent": "Alternatives that don't appear currently to be the best alternative.",
                    "label": 0
                },
                {
                    "sent": "I don't always expand.",
                    "label": 0
                },
                {
                    "sent": "The best thing for the tree shop, and often that's a really good idea because the best leave maybe a certain drawer.",
                    "label": 0
                },
                {
                    "sent": "There's no point any further in the second best leave.",
                    "label": 0
                },
                {
                    "sent": "Maybe a speculative move that looks bad on the surface, but perhaps it will win.",
                    "label": 0
                },
                {
                    "sent": "And if you were exploration policy.",
                    "label": 0
                },
                {
                    "sent": "He's always just pursuing what appears to be the best move.",
                    "label": 0
                },
                {
                    "sent": "Right then you'll never do that, and so you're wasting awful lot of computation.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What Monte Carlo is really doing is avoiding just following.",
                    "label": 0
                },
                {
                    "sent": "The current best line, which is what you would do if you try to explore using just the optimal policy, is dumb.",
                    "label": 0
                },
                {
                    "sent": "If you think that's how you're losing eventually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Not just that, we should look at random news.",
                    "label": 0
                },
                {
                    "sent": "They have to be both reasonable and potentially high value.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly exactly, and this may vary from position to position as well.",
                    "label": 0
                },
                {
                    "sent": "I would also.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "There's one other part, which is that.",
                    "label": 0
                },
                {
                    "sent": "Is often more on thing to do in a tree that doesn't go all the way to terminal positions, so you may also get some benefit from.",
                    "label": 0
                },
                {
                    "sent": "Operating information from multiple branches just just because.",
                    "label": 0
                },
                {
                    "sent": "Apology.",
                    "label": 0
                },
                {
                    "sent": "And maybe one thing I should add here is that so so we tried these experiments in Simple Monte Carlo set, which is to say you simply apply Monte Carlo simulation for every legal action.",
                    "label": 0
                },
                {
                    "sent": "Now the natural extension to this is to apply it to Monte Carlo Tree search and really Monte Carlo tree search algorithms such as UCT.",
                    "label": 0
                },
                {
                    "sent": "I mean the key idea there is to balance exploration intelligently so that you're not just following the Mini Max line so that you're actually choosing to invest some time in exploration which can help you a lot in the long run.",
                    "label": 0
                },
                {
                    "sent": "So maybe that would help to address.",
                    "label": 0
                },
                {
                    "sent": "One issue, but.",
                    "label": 0
                },
                {
                    "sent": "I'm Csaba, kind of said something quietly behind you whilst you're talking, which is this idea that if you just drive to improve the performance of your policy then you will lose diversity by trying to.",
                    "label": 0
                },
                {
                    "sent": "Act optimally and I think this is really a very central issue that we encounter here and what we'd like to do is to there are many positions in go, for example, where there is really an obvious right move, and we would like our Monte Carlo simulations to just take that right, move all the time.",
                    "label": 0
                },
                {
                    "sent": "However, there are other positions in which it's really important to maintain diversity, and that the best way to actually find the right action using Monte Carlo is to average over a number of different possibilities, and so really the key idea is to find a way to automatically decide how to balance out.",
                    "label": 0
                },
                {
                    "sent": "This issue so as to get at the get at the strong policy in some simple way.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Perhaps, perhaps, maybe it's just terminology.",
                    "label": 0
                },
                {
                    "sent": "OK. Vincent, that you're having trouble taking this to bigger.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say trouble.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think it looks quite promising.",
                    "label": 0
                },
                {
                    "sent": "It's so we've tried some initial experiments on 9 by 9 and actually we get very similar looking.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mean squared error plots.",
                    "label": 0
                },
                {
                    "sent": "However, the difficulty on large boards is simply just deciding which part of the simulation, which part of the game do you want to know?",
                    "label": 0
                },
                {
                    "sent": "How do you generate your training data?",
                    "label": 0
                },
                {
                    "sent": "So in the game of go, there's this huge difference between at the end of the game.",
                    "label": 0
                },
                {
                    "sent": "There are many, many moves which almost have no effect on the actual outcome of the game, and if you train too heavily on these positions, it actually means that you're really you're balancing algorithm learns to balance out this kind of relevant data, and so there's really just this careful this.",
                    "label": 0
                },
                {
                    "sent": "Slightly tricky issue of just choosing a data set that actually leads to high performance in the long run, so that's kind of what we've just been.",
                    "label": 0
                },
                {
                    "sent": "It's just a practical issue, but it seems like the same principles apply and I think I think actually the opposite should be true in the long run, which is it should be the case that for larger boards the advantage of simulation balancing over over over supervised learning reinforcement learning should be extended simply because there are many, many many steps in each game, and therefore this kind of.",
                    "label": 0
                },
                {
                    "sent": "It's just like these plots I showed earlier.",
                    "label": 0
                },
                {
                    "sent": "The number of steps.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In which the bias.",
                    "label": 0
                },
                {
                    "sent": "Can increase over is is very large that the longer this time scale the more the more important is to be unbiased rather than rather than strong.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much again.",
                    "label": 0
                }
            ]
        }
    }
}