{
    "id": "qt5xo7idrm6wjxingsgesodpkzs6awbx",
    "title": "Dynamic Analysis of Multiagent Q-learning with e-greedy Exploration",
    "info": {
        "author": [
            "Eduardo Rodrigues Gomes, Swinburne University of Technology"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Intelligent Agents",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_gomes_damq/",
    "segmentation": [
        [
            "And basically motivation."
        ],
        [
            "Learning in the last year has become one of the most active researchers in artificial intelligence, and as this mutation based but agent learning basic system find applications in a wide variety of domains, the development of tools to understand and model the expected dynamics of the agents are becoming increasingly important.",
            "And in this paper we tackled the."
        ],
        [
            "Specific case of multi agent learning with grid exploration and there is at least two good reasons why to study."
        ],
        [
            "Is algorithm first?",
            "It's one of the classic algorithms of motivation, replacement, learning.",
            "And also it has been applied with success in several domains and the model of this algorithm for the dynamics operations hasn't been developed for."
        ],
        [
            "So welcome learning is perhaps the most disorder enforcement learning algorithm.",
            "It has a strong theoretical support and convergence guarantees, but."
        ],
        [
            "This only holds when I have a single agent single learner in the prod."
        ],
        [
            "Done.",
            "When it goes to the mutation case."
        ],
        [
            "Then there is actually a lack of theoretical Sparta.",
            "No convergence guarantees at all, and this is basically be cause of the very dynamic environment generated by mutation learners.",
            "And which option is generated?",
            "This so called cooperation affecting which one Asian adapted its status to the others?",
            "And also in the mutation case, the rewards in the state transitions.",
            "They depend on the joint action of other agents, which make it very hard to obtain the expected dynamic."
        ],
        [
            "So in order to study."
        ],
        [
            "Problem some researchers have explored links between reinforcement learning algorithms and evolutionary game theory, in particular, on the replicator dynamics of evolutionary game theory and basically what happens is that most mutation, refreshment, learning algorithms and they share the same principles of the replicator dynamics in which the growth of the probability of playing one of the action is directly proportional to the performance of this action.",
            "I guess the others, so this link has been explored to model them.",
            "Mutation, Killarney with bottom exploration which perhaps the most closest work of ours so."
        ],
        [
            "However, the link cannot be applied in your case where you have the epsilon greedy exploration mechanism cause in this mechanism helps my uniform."
        ],
        [
            "Solution in which the best action was chosen with probability 1 minus epsilon and around with strategies chosen probability epsilon."
        ],
        [
            "So before going to show your mother or talk a little bit more about motivation to learning so it's the natural extension of Q learning to motivation to problems in which each of the agents are placed under Q learning algorithm, they just learn independently so they are not aware of other agents in the environment.",
            "And the as I said before, the rewards, they state transitions depend on the joint election of other agents."
        ],
        [
            "So this is a.",
            "Baseball killer.",
            "Now we know each agent at table for Q virus, and each value represented estimation of how good it is for the agent to take a particular action in each particular states.",
            "So to learn needs to update this table of values and it's updated agent gather experience in the environment.",
            "This is the standard Q learning operating room for learning, where R is the reward for taking a particular action in this state, which the agent receives after taking actions in the environment.",
            "Alpha is the learning rate and gamma is the discount rate for eating it."
        ],
        [
            "Call it on problems.",
            "So as some of the speakers have readmission.",
            "There is this problem of exploration exploitation in a multi agent in learning refinement, learning in general and then here is where the epsilon greedy mechanism comes in place.",
            "This mechanism is one of the strategy of the techniques used to balance to achieve this balance between exploration and exploitation and also said before this mechanism chooses the actions that currently the best with probability 1 minus epsilon and Brando action with probability."
        ],
        [
            "Epson.",
            "So.",
            "What does mechanism actually does it?",
            "Assign probabilities to each of the actions for each state, and we can define us X to be this probability of playing action Wyatt status.",
            "And this is how this is a conditional function which assigns this probability to the to the actions in the states."
        ],
        [
            "So now how we get?"
        ],
        [
            "To model for the algorithm, first build a continuous time version of the Q Learning update below."
        ],
        [
            "After that we analyze the limits of this equation for the single learner case."
        ],
        [
            "And show how this limits change dynamically when you have the mootry learner case."
        ],
        [
            "After that, we study how the grid epsilon greedy mechanism affected the shape of the function."
        ],
        [
            "And finally, based on these observations, we develop a system of difference equation to obtain the expected behavior of the agents.",
            "Ashal talk."
        ],
        [
            "Little bit more on the note."
        ],
        [
            "Nation in this paper.",
            "Consider all the single state scenarios composed of two agents which you actions each.",
            "So far the model is able to."
        ],
        [
            "And the more agents with more actions.",
            "But we haven't extended it yet to most scenarios, and this is perhaps the most important future work through this research when I have only two agents, it's not inconvenient to represent the rule."
        ],
        [
            "Functions as payoff tables for each member of this table.",
            "Here I have one table for the first agent an under the table for the 2nd and each element represented the reward that the agent will be receiving for playing that action, given the actual chosen by the other agent, and also as I have only one state, I can simplify the Q learning updated role so I can avoid the state dependent information.",
            "So now I changed the notation as well.",
            "From what I have shown before, Q AI is the Q value for officia NTE for action Y&R AI is the major award that the agent A receives for playing Action Y.",
            "So now I'll talk about the continuous."
        ],
        [
            "Time is version of the Killarney Updatable."
        ],
        [
            "So.",
            "Actually, what we do, we insert this time step independent notation in this formula.",
            "And what if we move this component to the other side?",
            "We have actually.",
            "Open equation which states the absolute growth in the value of the Q in between two time steps.",
            "You concept steps.",
            "Now I need to obtain account."
        ],
        [
            "No time version for this equation, so I can actually analyze the system based on the limits of this."
        ],
        [
            "And for this I consider Delta T to be."
        ],
        [
            "A small amount of time that elapses between two consecutive time step and then the growth in the Q value will be direct proportional to the amount of time.",
            "We could buy both."
        ],
        [
            "Sides of the equation for this by this Delta T and take the limits of this equation."
        ],
        [
            "Lt goes to 0 actually have this.",
            "Differential like equation which is actually an approximation of the continuous time version of this equation here.",
            "With this differential equation, then we can use."
        ],
        [
            "To solve it, use."
        ],
        [
            "In standard integration and find out a family of equations that actually can solve the differential equation.",
            "We can take the limit."
        ],
        [
            "So for this equation, when time goes to infinite and find out that actually the limit of the Q learning updated role is the reward, which is not surprising.",
            "That's what we want of the Q learning to talk to the rewards.",
            "But what is important?"
        ],
        [
            "Here is that actually you can show that the cue file is.",
            "Then if the person is not learning and using a pure strategy, so he always plays the same action, the Q values will monotonically increase or decrease.",
            "It offers the reward and this is because of this term here, which is actually a monotonic function.",
            "Here I have a constant, so actually.",
            "I can show that this the Q factor will increase or decrease in monotonically and here I have an example.",
            "I thought the direction fields."
        ],
        [
            "The differential equation Alpha is equal to 0.2 and the rewards.",
            "This case is equal to five and then I have some specific solutions when the initial values Q values are 028 and 10 and as you can see, all the curves they converge towards the Cleveland point of the direction field, which is basically the limits of the equations I've shown fan."
        ],
        [
            "So what if the diversity is applying mixes?"
        ],
        [
            "In this case, I can replace the reward.",
            "The absolute reward by the expected rewards here have an example from the Prisoner's dilemma, again.",
            "Then I have placed this in the equation and we solved."
        ],
        [
            "Is my integration again take the limits?",
            "20 goes to it and."
        ],
        [
            "Also, any can show that the Q value then move in expectation towards the expected reward and in the money."
        ],
        [
            "Sonic version.",
            "So."
        ],
        [
            "Now is the perhaps the most important aspect of this work here.",
            "What if you did your service learning?",
            "What happens with the dynamics of the agents?",
            "If you UN playing against learning adversary?",
            "In this case the other side can change its strategy in the process.",
            "This is the result of the learning process, of course, and by changing it, is this true?"
        ],
        [
            "Traditional changes, expected rewards.",
            "So basically I have an example here where they."
        ],
        [
            "Agency is playing this strategy profile.",
            "We change it to this one, then expect a reward for playing action.",
            "One of my agent is equal goes from one point 24.2."
        ],
        [
            "So actually each time the expected will change, they change the limits of the equation and also the direction fields associated with the differential equation."
        ],
        [
            "So it becomes very important to identify who ended changes in the adversary structure will occur, and this is defined by the epsilon greedy mechanism which updates the strategy whenever a new."
        ],
        [
            "Action becomes the one with the highest value.",
            "So to find out who ended, this change will occur we need to find out."
        ],
        [
            "The intersection points in the functions of the, the curves.",
            "Occupiers of the diversity.",
            "Here I have an example for the prisoners dilemma you can see here.",
            "There's intersection points in the curves of the agent one and the results of this intersection point here is the change in direction of the curves of Asian two which were converted to these values at the beginning that time training.",
            "When there is this intersection, the direction of the curves change.",
            "So basically, uh, overall behavior of the agents are dependent on this intersection points, 'cause they will define it to which value is the Q value conversion tool."
        ],
        [
            "So now what are the effects of that?"
        ],
        [
            "Hungry mechanism apart from this, determining the intersection points basically by the gate mechanism, the actions they have different probabilities of being played, so they are the actions are actually the kouvelis update at different speeds and in the MoD."
        ],
        [
            "We simulate this behavior by making the growth in the Q value as direct proportional to the probability that nation will play that strategy.",
            "Once again, we."
        ],
        [
            "We have integrated this function and what you find out here is that this probability it goes at the power term of this exponential term.",
            "And what happens is that it doesn't change the limits of the equation, as you can see here."
        ],
        [
            "The limit of the question is do they expect everyone?",
            "Under but."
        ],
        [
            "It does change the shape of the function.",
            "Here I have an example.",
            "Here is when the X is equal to 0.1 and point is equal to 0.9.",
            "As you can see, the larger the probability that action will be played, the quicker the Q values will evolve towards the clip on point."
        ],
        [
            "So something different."
        ],
        [
            "This is basically the."
        ],
        [
            "Star Wars."
        ],
        [
            "Are the values to which the Q value converter they speeds, which is basically the probability of playing they slash reader among the path that the values you follow to get there to get the expected rewards and intersection points they define whether the Q values will get there.",
            "Because if there are intersection points and maybe they are changing the expected reward, so the Q values we start converging to the new expected words.",
            "So based on this observation."
        ],
        [
            "Define a system of a difference equation."
        ],
        [
            "Which model the expected evolution of the cube."
        ],
        [
            "The status of the agents.",
            "Well, the system they have.",
            "It's only dependency between the equations here, for example, to define the Q values of the first agent decision, he uses the it's only strategy to define the speed of the convergence.",
            "But it also uses the strategy of the opponent, which is Y to define what it is the expected reward.",
            "So there are strong dependence between them.",
            "And I applied this MoD."
        ],
        [
            "Doing some."
        ],
        [
            "Games some."
        ],
        [
            "This game is found in the literature.",
            "Here is an example using the prisoner dilemma.",
            "Of course, the devolution of the values depends also on the initial setup of the values, but with different initial values.",
            "The evolution will be different, so here is for this specific specific configuration, well based on the left side, I have the theoretical the comparison between the theoretical values, which are the key values obtained by the model and the Q values obtained.",
            "He will experimentation with you learning.",
            "And as you can see, the model is able to capture the major trends found in the experiments in the center.",
            "I have the strategy of the agent following by following model and on the right side I have this strategy of the agents obtaining observant in the experiments.",
            "So here is an example."
        ],
        [
            "One individual runs in the graph before they have shown they forgot to mention this.",
            "I have shown here is the statistical median of the experiments over 100 experiments.",
            "Well here I have an example of.",
            "Sorry for them.",
            "For one of the documents, as you can see the.",
            "The behavior is similar to what the model predicts.",
            "However in this case, as it is one more experiment one I have this random component which will basically.",
            "Define accommodation choose this strategy during the learning process.",
            "OK, now here I have another example with the battle of."
        ],
        [
            "Sexist.",
            "So once again, the model is able to capture the major trends found in the experiment.",
            "Here is the."
        ],
        [
            "So one of the individual runs of these experiments."
        ],
        [
            "El.",
            "Here have another example.",
            "This is a game taking."
        ],
        [
            "Phone.",
            "The work of call tools which has worked with the model of a Baltimore nutrition: of Baltimore."
        ],
        [
            "Exploration and I included here just people that are aware with that worker could actually compare the two and this is interesting because actually I have in this example have among first part where the model is able to capture the behavior found in the experiments.",
            "But then in this second part the experiment seems to converge to a specific regulation while the model actually predicts that there will be cyclic adaptation.",
            "No.",
            "This cycle of the adaptations at the end.",
            "Oh how."
        ],
        [
            "And when I am going to, when I look at the individual runs, I actually see that the experiment is actually they have this cyclic behavior and the conversations are shown before it's just.",
            "The result of the application of the update status comedian."
        ],
        [
            "So conclusions of this paper I have."
        ],
        [
            "So the model of the dynamics of motorcycle learning.",
            "Which basically are based on system of differential equation."
        ],
        [
            "And the population in typical games will show its feasibility."
        ],
        [
            "So future works we need to extend the model to multistate scenarios and we also need to develop techniques to help individualization operations behavior.",
            "'cause it's not so.",
            "Intuitive to look at, these graphs like this so we need to workout how to.",
            "To help this visualization.",
            "So that's all I had to talk to."
        ],
        [
            "Determine if the.",
            "Just looking at the expected value.",
            "Yes, basically you can actually do it.",
            "The two ways we can actually track.",
            "Show one of the graphs here.",
            "For example, we can track the action with a higher skill value and then update this strategy based on this one.",
            "But actually they X equation.",
            "This equation is here.",
            "They actually track down the the status of the agents already, so so if you mean I'm not sure about all these things are presented, but all of them are relatively small, being so like if you had a much bigger game coming up with this kind of formulation would be quite difficult.",
            "Oh well, basically if I have more agents, for example, I would need to include a new equation for that agent for the evolution of the values and a new equation for the epsilon greedy algorithm for that agents.",
            "If I have more actions then then I wouldn't need to include more equations.",
            "But in this case yeah, the why here is equation.",
            "So if I have two actions, I have actually two equations like this.",
            "In the model.",
            "See maybe another show here.",
            "Yeah, with more actions I will have more courage here basically.",
            "For each of these actions of these graphs.",
            "Yeah, So what I'm saying is that it's your game is not trivial.",
            "Then you may have a huge number of options.",
            "Coffee.",
            "Continually modeling all of these values and checking routine?",
            "Yeah, that's true.",
            "That's true.",
            "Yeah, but yeah, anyway, it's much faster than running different experiments with Kalani and trying to combine all these experiments.",
            "Then, in the hope to find out a good configuration for the company for the for the for the learning parameters, for example.",
            "So this is where this model could be used with some success.",
            "Bridge.",
            "So you use the language of humor.",
            "But as far as I can tell, if you're not actually getting temporary fix.",
            "Sorry.",
            "Are you?",
            "With the word Q learning used means that you're asking state action values.",
            "It usually means that you're estimating state action values and then you're using temporal difference learning to do that kind of bootstrap.",
            "I was hearing you talk.",
            "It doesn't seem like you're doing bootstrapping.",
            "So with that being natural direction extent, yeah, actually, maybe there's misunderstanding.",
            "I'm just presenting a system of differential equations that can actually model the dynamics of real Killarney in multi agent scenarios.",
            "Given that decisions using the epsilon grid expiration mechanism.",
            "So this is basically mathematical equations.",
            "So I'm not using the Q learning actually.",
            "I just compared the behavior of which I found in Killer Lee using real experimentation.",
            "Colony with the mathematical equations I have derived from the Q learning formulas.",
            "Thanks for anything.",
            "Learning.",
            "It's not, it's not.",
            "The key thing that you're calling Q learning is that really Q learning.",
            "And yes, yes, so you learn.",
            "It usually has some kind of a temporal bootstrapping aspect to it.",
            "Take the values from the next state and use them to back up the value.",
            "OK, OK, yeah, basically yeah yeah, OK here I have one one state, so there is no.",
            "Yeah, actually there's no bootstrapping, because this is actually this is the stern that games people use it to analyze mutation reinforcement, learning algorithms.",
            "So yeah, I don't.",
            "I have only one state so I don't have bootstrapping.",
            "I don't have values of different states taking being carried over to new states, but they are one of the future works is to extend this to the multi state case.",
            "Which won't be easy anyway.",
            "And also the multiplication staircase.",
            "I stop casting games and game theory and there's there's a large body of literature on how to solve those things.",
            "Just looked at comparing your work with the work that people don't change that with theory.",
            "Things like what was it?",
            "Quinn Boston lose quickly with this pulled up the Wolf and.",
            "OK, yeah.",
            "Yeah, basically that those are all rhythms are specifically created for the multi agent learning material enforcement learning case.",
            "In this talk here and just cause people want to.",
            "Usually people do is they get this Q learning storyline algorithm and they use it in multi agent case without actually caring.",
            "What are the dynamics of the system will be and they usually find good results by doing it.",
            "Just putting up a standard colonialism inside one of the agent.",
            "And all of the agents, and.",
            "Yeah, that's a good.",
            "That's good.",
            "Maybe some future directions for exploration, and I know for example, I think they will forward and there's some proof of convergence which use the differential equation so people are maybe actually already using differential equations to analyze the dynamics.",
            "Almost.",
            "Crazy assumption.",
            "Yeah.",
            "Players going at each other at the same time.",
            "It's about.",
            "I'm going to go with.",
            "OK, I think about what they assume it mostly is that they know what actions they adversaries take.",
            "So this is one of the assumptions they use.",
            "But yeah, if you're talking about motivation reinforcement, learning cannot assume that you have driver side will be a stationary.",
            "Otherwise it's not motivation learning.",
            "Because.",
            "Yeah, sometimes what they do is that they actually they actually assume that are periods of adaptation, so they agree on this for them they will be learning for a specific amount of time and then there is a peer to to adopt this strategy.",
            "Learning that specific amount of time.",
            "We'll take that offline and then OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically motivation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning in the last year has become one of the most active researchers in artificial intelligence, and as this mutation based but agent learning basic system find applications in a wide variety of domains, the development of tools to understand and model the expected dynamics of the agents are becoming increasingly important.",
                    "label": 0
                },
                {
                    "sent": "And in this paper we tackled the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Specific case of multi agent learning with grid exploration and there is at least two good reasons why to study.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is algorithm first?",
                    "label": 0
                },
                {
                    "sent": "It's one of the classic algorithms of motivation, replacement, learning.",
                    "label": 0
                },
                {
                    "sent": "And also it has been applied with success in several domains and the model of this algorithm for the dynamics operations hasn't been developed for.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So welcome learning is perhaps the most disorder enforcement learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It has a strong theoretical support and convergence guarantees, but.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This only holds when I have a single agent single learner in the prod.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done.",
                    "label": 0
                },
                {
                    "sent": "When it goes to the mutation case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then there is actually a lack of theoretical Sparta.",
                    "label": 1
                },
                {
                    "sent": "No convergence guarantees at all, and this is basically be cause of the very dynamic environment generated by mutation learners.",
                    "label": 0
                },
                {
                    "sent": "And which option is generated?",
                    "label": 1
                },
                {
                    "sent": "This so called cooperation affecting which one Asian adapted its status to the others?",
                    "label": 0
                },
                {
                    "sent": "And also in the mutation case, the rewards in the state transitions.",
                    "label": 0
                },
                {
                    "sent": "They depend on the joint action of other agents, which make it very hard to obtain the expected dynamic.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in order to study.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem some researchers have explored links between reinforcement learning algorithms and evolutionary game theory, in particular, on the replicator dynamics of evolutionary game theory and basically what happens is that most mutation, refreshment, learning algorithms and they share the same principles of the replicator dynamics in which the growth of the probability of playing one of the action is directly proportional to the performance of this action.",
                    "label": 1
                },
                {
                    "sent": "I guess the others, so this link has been explored to model them.",
                    "label": 0
                },
                {
                    "sent": "Mutation, Killarney with bottom exploration which perhaps the most closest work of ours so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, the link cannot be applied in your case where you have the epsilon greedy exploration mechanism cause in this mechanism helps my uniform.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution in which the best action was chosen with probability 1 minus epsilon and around with strategies chosen probability epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before going to show your mother or talk a little bit more about motivation to learning so it's the natural extension of Q learning to motivation to problems in which each of the agents are placed under Q learning algorithm, they just learn independently so they are not aware of other agents in the environment.",
                    "label": 0
                },
                {
                    "sent": "And the as I said before, the rewards, they state transitions depend on the joint election of other agents.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a.",
                    "label": 0
                },
                {
                    "sent": "Baseball killer.",
                    "label": 0
                },
                {
                    "sent": "Now we know each agent at table for Q virus, and each value represented estimation of how good it is for the agent to take a particular action in each particular states.",
                    "label": 1
                },
                {
                    "sent": "So to learn needs to update this table of values and it's updated agent gather experience in the environment.",
                    "label": 0
                },
                {
                    "sent": "This is the standard Q learning operating room for learning, where R is the reward for taking a particular action in this state, which the agent receives after taking actions in the environment.",
                    "label": 1
                },
                {
                    "sent": "Alpha is the learning rate and gamma is the discount rate for eating it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Call it on problems.",
                    "label": 0
                },
                {
                    "sent": "So as some of the speakers have readmission.",
                    "label": 0
                },
                {
                    "sent": "There is this problem of exploration exploitation in a multi agent in learning refinement, learning in general and then here is where the epsilon greedy mechanism comes in place.",
                    "label": 1
                },
                {
                    "sent": "This mechanism is one of the strategy of the techniques used to balance to achieve this balance between exploration and exploitation and also said before this mechanism chooses the actions that currently the best with probability 1 minus epsilon and Brando action with probability.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Epson.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What does mechanism actually does it?",
                    "label": 0
                },
                {
                    "sent": "Assign probabilities to each of the actions for each state, and we can define us X to be this probability of playing action Wyatt status.",
                    "label": 0
                },
                {
                    "sent": "And this is how this is a conditional function which assigns this probability to the to the actions in the states.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now how we get?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To model for the algorithm, first build a continuous time version of the Q Learning update below.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After that we analyze the limits of this equation for the single learner case.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And show how this limits change dynamically when you have the mootry learner case.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After that, we study how the grid epsilon greedy mechanism affected the shape of the function.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, based on these observations, we develop a system of difference equation to obtain the expected behavior of the agents.",
                    "label": 0
                },
                {
                    "sent": "Ashal talk.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit more on the note.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation in this paper.",
                    "label": 0
                },
                {
                    "sent": "Consider all the single state scenarios composed of two agents which you actions each.",
                    "label": 1
                },
                {
                    "sent": "So far the model is able to.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the more agents with more actions.",
                    "label": 0
                },
                {
                    "sent": "But we haven't extended it yet to most scenarios, and this is perhaps the most important future work through this research when I have only two agents, it's not inconvenient to represent the rule.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Functions as payoff tables for each member of this table.",
                    "label": 0
                },
                {
                    "sent": "Here I have one table for the first agent an under the table for the 2nd and each element represented the reward that the agent will be receiving for playing that action, given the actual chosen by the other agent, and also as I have only one state, I can simplify the Q learning updated role so I can avoid the state dependent information.",
                    "label": 0
                },
                {
                    "sent": "So now I changed the notation as well.",
                    "label": 0
                },
                {
                    "sent": "From what I have shown before, Q AI is the Q value for officia NTE for action Y&R AI is the major award that the agent A receives for playing Action Y.",
                    "label": 1
                },
                {
                    "sent": "So now I'll talk about the continuous.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time is version of the Killarney Updatable.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Actually, what we do, we insert this time step independent notation in this formula.",
                    "label": 0
                },
                {
                    "sent": "And what if we move this component to the other side?",
                    "label": 0
                },
                {
                    "sent": "We have actually.",
                    "label": 0
                },
                {
                    "sent": "Open equation which states the absolute growth in the value of the Q in between two time steps.",
                    "label": 0
                },
                {
                    "sent": "You concept steps.",
                    "label": 0
                },
                {
                    "sent": "Now I need to obtain account.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No time version for this equation, so I can actually analyze the system based on the limits of this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for this I consider Delta T to be.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A small amount of time that elapses between two consecutive time step and then the growth in the Q value will be direct proportional to the amount of time.",
                    "label": 0
                },
                {
                    "sent": "We could buy both.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sides of the equation for this by this Delta T and take the limits of this equation.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lt goes to 0 actually have this.",
                    "label": 0
                },
                {
                    "sent": "Differential like equation which is actually an approximation of the continuous time version of this equation here.",
                    "label": 0
                },
                {
                    "sent": "With this differential equation, then we can use.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To solve it, use.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In standard integration and find out a family of equations that actually can solve the differential equation.",
                    "label": 0
                },
                {
                    "sent": "We can take the limit.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for this equation, when time goes to infinite and find out that actually the limit of the Q learning updated role is the reward, which is not surprising.",
                    "label": 1
                },
                {
                    "sent": "That's what we want of the Q learning to talk to the rewards.",
                    "label": 0
                },
                {
                    "sent": "But what is important?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is that actually you can show that the cue file is.",
                    "label": 0
                },
                {
                    "sent": "Then if the person is not learning and using a pure strategy, so he always plays the same action, the Q values will monotonically increase or decrease.",
                    "label": 1
                },
                {
                    "sent": "It offers the reward and this is because of this term here, which is actually a monotonic function.",
                    "label": 0
                },
                {
                    "sent": "Here I have a constant, so actually.",
                    "label": 0
                },
                {
                    "sent": "I can show that this the Q factor will increase or decrease in monotonically and here I have an example.",
                    "label": 0
                },
                {
                    "sent": "I thought the direction fields.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The differential equation Alpha is equal to 0.2 and the rewards.",
                    "label": 0
                },
                {
                    "sent": "This case is equal to five and then I have some specific solutions when the initial values Q values are 028 and 10 and as you can see, all the curves they converge towards the Cleveland point of the direction field, which is basically the limits of the equations I've shown fan.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what if the diversity is applying mixes?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, I can replace the reward.",
                    "label": 0
                },
                {
                    "sent": "The absolute reward by the expected rewards here have an example from the Prisoner's dilemma, again.",
                    "label": 0
                },
                {
                    "sent": "Then I have placed this in the equation and we solved.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is my integration again take the limits?",
                    "label": 0
                },
                {
                    "sent": "20 goes to it and.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, any can show that the Q value then move in expectation towards the expected reward and in the money.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sonic version.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now is the perhaps the most important aspect of this work here.",
                    "label": 0
                },
                {
                    "sent": "What if you did your service learning?",
                    "label": 0
                },
                {
                    "sent": "What happens with the dynamics of the agents?",
                    "label": 0
                },
                {
                    "sent": "If you UN playing against learning adversary?",
                    "label": 1
                },
                {
                    "sent": "In this case the other side can change its strategy in the process.",
                    "label": 1
                },
                {
                    "sent": "This is the result of the learning process, of course, and by changing it, is this true?",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Traditional changes, expected rewards.",
                    "label": 0
                },
                {
                    "sent": "So basically I have an example here where they.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Agency is playing this strategy profile.",
                    "label": 0
                },
                {
                    "sent": "We change it to this one, then expect a reward for playing action.",
                    "label": 0
                },
                {
                    "sent": "One of my agent is equal goes from one point 24.2.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So actually each time the expected will change, they change the limits of the equation and also the direction fields associated with the differential equation.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it becomes very important to identify who ended changes in the adversary structure will occur, and this is defined by the epsilon greedy mechanism which updates the strategy whenever a new.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Action becomes the one with the highest value.",
                    "label": 0
                },
                {
                    "sent": "So to find out who ended, this change will occur we need to find out.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The intersection points in the functions of the, the curves.",
                    "label": 1
                },
                {
                    "sent": "Occupiers of the diversity.",
                    "label": 0
                },
                {
                    "sent": "Here I have an example for the prisoners dilemma you can see here.",
                    "label": 0
                },
                {
                    "sent": "There's intersection points in the curves of the agent one and the results of this intersection point here is the change in direction of the curves of Asian two which were converted to these values at the beginning that time training.",
                    "label": 0
                },
                {
                    "sent": "When there is this intersection, the direction of the curves change.",
                    "label": 0
                },
                {
                    "sent": "So basically, uh, overall behavior of the agents are dependent on this intersection points, 'cause they will define it to which value is the Q value conversion tool.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now what are the effects of that?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hungry mechanism apart from this, determining the intersection points basically by the gate mechanism, the actions they have different probabilities of being played, so they are the actions are actually the kouvelis update at different speeds and in the MoD.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We simulate this behavior by making the growth in the Q value as direct proportional to the probability that nation will play that strategy.",
                    "label": 0
                },
                {
                    "sent": "Once again, we.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have integrated this function and what you find out here is that this probability it goes at the power term of this exponential term.",
                    "label": 0
                },
                {
                    "sent": "And what happens is that it doesn't change the limits of the equation, as you can see here.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The limit of the question is do they expect everyone?",
                    "label": 0
                },
                {
                    "sent": "Under but.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It does change the shape of the function.",
                    "label": 1
                },
                {
                    "sent": "Here I have an example.",
                    "label": 0
                },
                {
                    "sent": "Here is when the X is equal to 0.1 and point is equal to 0.9.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the larger the probability that action will be played, the quicker the Q values will evolve towards the clip on point.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So something different.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is basically the.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Star Wars.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are the values to which the Q value converter they speeds, which is basically the probability of playing they slash reader among the path that the values you follow to get there to get the expected rewards and intersection points they define whether the Q values will get there.",
                    "label": 1
                },
                {
                    "sent": "Because if there are intersection points and maybe they are changing the expected reward, so the Q values we start converging to the new expected words.",
                    "label": 0
                },
                {
                    "sent": "So based on this observation.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Define a system of a difference equation.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which model the expected evolution of the cube.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The status of the agents.",
                    "label": 0
                },
                {
                    "sent": "Well, the system they have.",
                    "label": 0
                },
                {
                    "sent": "It's only dependency between the equations here, for example, to define the Q values of the first agent decision, he uses the it's only strategy to define the speed of the convergence.",
                    "label": 0
                },
                {
                    "sent": "But it also uses the strategy of the opponent, which is Y to define what it is the expected reward.",
                    "label": 0
                },
                {
                    "sent": "So there are strong dependence between them.",
                    "label": 0
                },
                {
                    "sent": "And I applied this MoD.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing some.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Games some.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This game is found in the literature.",
                    "label": 0
                },
                {
                    "sent": "Here is an example using the prisoner dilemma.",
                    "label": 0
                },
                {
                    "sent": "Of course, the devolution of the values depends also on the initial setup of the values, but with different initial values.",
                    "label": 0
                },
                {
                    "sent": "The evolution will be different, so here is for this specific specific configuration, well based on the left side, I have the theoretical the comparison between the theoretical values, which are the key values obtained by the model and the Q values obtained.",
                    "label": 0
                },
                {
                    "sent": "He will experimentation with you learning.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, the model is able to capture the major trends found in the experiments in the center.",
                    "label": 0
                },
                {
                    "sent": "I have the strategy of the agent following by following model and on the right side I have this strategy of the agents obtaining observant in the experiments.",
                    "label": 0
                },
                {
                    "sent": "So here is an example.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One individual runs in the graph before they have shown they forgot to mention this.",
                    "label": 0
                },
                {
                    "sent": "I have shown here is the statistical median of the experiments over 100 experiments.",
                    "label": 0
                },
                {
                    "sent": "Well here I have an example of.",
                    "label": 0
                },
                {
                    "sent": "Sorry for them.",
                    "label": 0
                },
                {
                    "sent": "For one of the documents, as you can see the.",
                    "label": 0
                },
                {
                    "sent": "The behavior is similar to what the model predicts.",
                    "label": 0
                },
                {
                    "sent": "However in this case, as it is one more experiment one I have this random component which will basically.",
                    "label": 0
                },
                {
                    "sent": "Define accommodation choose this strategy during the learning process.",
                    "label": 0
                },
                {
                    "sent": "OK, now here I have another example with the battle of.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sexist.",
                    "label": 0
                },
                {
                    "sent": "So once again, the model is able to capture the major trends found in the experiment.",
                    "label": 0
                },
                {
                    "sent": "Here is the.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the individual runs of these experiments.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "El.",
                    "label": 0
                },
                {
                    "sent": "Here have another example.",
                    "label": 0
                },
                {
                    "sent": "This is a game taking.",
                    "label": 1
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Phone.",
                    "label": 0
                },
                {
                    "sent": "The work of call tools which has worked with the model of a Baltimore nutrition: of Baltimore.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exploration and I included here just people that are aware with that worker could actually compare the two and this is interesting because actually I have in this example have among first part where the model is able to capture the behavior found in the experiments.",
                    "label": 0
                },
                {
                    "sent": "But then in this second part the experiment seems to converge to a specific regulation while the model actually predicts that there will be cyclic adaptation.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "This cycle of the adaptations at the end.",
                    "label": 0
                },
                {
                    "sent": "Oh how.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when I am going to, when I look at the individual runs, I actually see that the experiment is actually they have this cyclic behavior and the conversations are shown before it's just.",
                    "label": 0
                },
                {
                    "sent": "The result of the application of the update status comedian.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So conclusions of this paper I have.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the model of the dynamics of motorcycle learning.",
                    "label": 0
                },
                {
                    "sent": "Which basically are based on system of differential equation.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the population in typical games will show its feasibility.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So future works we need to extend the model to multistate scenarios and we also need to develop techniques to help individualization operations behavior.",
                    "label": 1
                },
                {
                    "sent": "'cause it's not so.",
                    "label": 0
                },
                {
                    "sent": "Intuitive to look at, these graphs like this so we need to workout how to.",
                    "label": 0
                },
                {
                    "sent": "To help this visualization.",
                    "label": 0
                },
                {
                    "sent": "So that's all I had to talk to.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Determine if the.",
                    "label": 0
                },
                {
                    "sent": "Just looking at the expected value.",
                    "label": 0
                },
                {
                    "sent": "Yes, basically you can actually do it.",
                    "label": 0
                },
                {
                    "sent": "The two ways we can actually track.",
                    "label": 0
                },
                {
                    "sent": "Show one of the graphs here.",
                    "label": 0
                },
                {
                    "sent": "For example, we can track the action with a higher skill value and then update this strategy based on this one.",
                    "label": 0
                },
                {
                    "sent": "But actually they X equation.",
                    "label": 0
                },
                {
                    "sent": "This equation is here.",
                    "label": 0
                },
                {
                    "sent": "They actually track down the the status of the agents already, so so if you mean I'm not sure about all these things are presented, but all of them are relatively small, being so like if you had a much bigger game coming up with this kind of formulation would be quite difficult.",
                    "label": 0
                },
                {
                    "sent": "Oh well, basically if I have more agents, for example, I would need to include a new equation for that agent for the evolution of the values and a new equation for the epsilon greedy algorithm for that agents.",
                    "label": 0
                },
                {
                    "sent": "If I have more actions then then I wouldn't need to include more equations.",
                    "label": 0
                },
                {
                    "sent": "But in this case yeah, the why here is equation.",
                    "label": 0
                },
                {
                    "sent": "So if I have two actions, I have actually two equations like this.",
                    "label": 0
                },
                {
                    "sent": "In the model.",
                    "label": 0
                },
                {
                    "sent": "See maybe another show here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, with more actions I will have more courage here basically.",
                    "label": 0
                },
                {
                    "sent": "For each of these actions of these graphs.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what I'm saying is that it's your game is not trivial.",
                    "label": 0
                },
                {
                    "sent": "Then you may have a huge number of options.",
                    "label": 0
                },
                {
                    "sent": "Coffee.",
                    "label": 0
                },
                {
                    "sent": "Continually modeling all of these values and checking routine?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but yeah, anyway, it's much faster than running different experiments with Kalani and trying to combine all these experiments.",
                    "label": 0
                },
                {
                    "sent": "Then, in the hope to find out a good configuration for the company for the for the for the learning parameters, for example.",
                    "label": 0
                },
                {
                    "sent": "So this is where this model could be used with some success.",
                    "label": 0
                },
                {
                    "sent": "Bridge.",
                    "label": 0
                },
                {
                    "sent": "So you use the language of humor.",
                    "label": 0
                },
                {
                    "sent": "But as far as I can tell, if you're not actually getting temporary fix.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Are you?",
                    "label": 0
                },
                {
                    "sent": "With the word Q learning used means that you're asking state action values.",
                    "label": 0
                },
                {
                    "sent": "It usually means that you're estimating state action values and then you're using temporal difference learning to do that kind of bootstrap.",
                    "label": 0
                },
                {
                    "sent": "I was hearing you talk.",
                    "label": 0
                },
                {
                    "sent": "It doesn't seem like you're doing bootstrapping.",
                    "label": 0
                },
                {
                    "sent": "So with that being natural direction extent, yeah, actually, maybe there's misunderstanding.",
                    "label": 0
                },
                {
                    "sent": "I'm just presenting a system of differential equations that can actually model the dynamics of real Killarney in multi agent scenarios.",
                    "label": 0
                },
                {
                    "sent": "Given that decisions using the epsilon grid expiration mechanism.",
                    "label": 0
                },
                {
                    "sent": "So this is basically mathematical equations.",
                    "label": 0
                },
                {
                    "sent": "So I'm not using the Q learning actually.",
                    "label": 0
                },
                {
                    "sent": "I just compared the behavior of which I found in Killer Lee using real experimentation.",
                    "label": 0
                },
                {
                    "sent": "Colony with the mathematical equations I have derived from the Q learning formulas.",
                    "label": 0
                },
                {
                    "sent": "Thanks for anything.",
                    "label": 0
                },
                {
                    "sent": "Learning.",
                    "label": 0
                },
                {
                    "sent": "It's not, it's not.",
                    "label": 0
                },
                {
                    "sent": "The key thing that you're calling Q learning is that really Q learning.",
                    "label": 0
                },
                {
                    "sent": "And yes, yes, so you learn.",
                    "label": 0
                },
                {
                    "sent": "It usually has some kind of a temporal bootstrapping aspect to it.",
                    "label": 0
                },
                {
                    "sent": "Take the values from the next state and use them to back up the value.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, yeah, basically yeah yeah, OK here I have one one state, so there is no.",
                    "label": 0
                },
                {
                    "sent": "Yeah, actually there's no bootstrapping, because this is actually this is the stern that games people use it to analyze mutation reinforcement, learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I have only one state so I don't have bootstrapping.",
                    "label": 0
                },
                {
                    "sent": "I don't have values of different states taking being carried over to new states, but they are one of the future works is to extend this to the multi state case.",
                    "label": 0
                },
                {
                    "sent": "Which won't be easy anyway.",
                    "label": 0
                },
                {
                    "sent": "And also the multiplication staircase.",
                    "label": 0
                },
                {
                    "sent": "I stop casting games and game theory and there's there's a large body of literature on how to solve those things.",
                    "label": 0
                },
                {
                    "sent": "Just looked at comparing your work with the work that people don't change that with theory.",
                    "label": 0
                },
                {
                    "sent": "Things like what was it?",
                    "label": 0
                },
                {
                    "sent": "Quinn Boston lose quickly with this pulled up the Wolf and.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, basically that those are all rhythms are specifically created for the multi agent learning material enforcement learning case.",
                    "label": 0
                },
                {
                    "sent": "In this talk here and just cause people want to.",
                    "label": 0
                },
                {
                    "sent": "Usually people do is they get this Q learning storyline algorithm and they use it in multi agent case without actually caring.",
                    "label": 0
                },
                {
                    "sent": "What are the dynamics of the system will be and they usually find good results by doing it.",
                    "label": 0
                },
                {
                    "sent": "Just putting up a standard colonialism inside one of the agent.",
                    "label": 0
                },
                {
                    "sent": "And all of the agents, and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good.",
                    "label": 0
                },
                {
                    "sent": "That's good.",
                    "label": 0
                },
                {
                    "sent": "Maybe some future directions for exploration, and I know for example, I think they will forward and there's some proof of convergence which use the differential equation so people are maybe actually already using differential equations to analyze the dynamics.",
                    "label": 0
                },
                {
                    "sent": "Almost.",
                    "label": 0
                },
                {
                    "sent": "Crazy assumption.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Players going at each other at the same time.",
                    "label": 0
                },
                {
                    "sent": "It's about.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go with.",
                    "label": 0
                },
                {
                    "sent": "OK, I think about what they assume it mostly is that they know what actions they adversaries take.",
                    "label": 0
                },
                {
                    "sent": "So this is one of the assumptions they use.",
                    "label": 0
                },
                {
                    "sent": "But yeah, if you're talking about motivation reinforcement, learning cannot assume that you have driver side will be a stationary.",
                    "label": 0
                },
                {
                    "sent": "Otherwise it's not motivation learning.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sometimes what they do is that they actually they actually assume that are periods of adaptation, so they agree on this for them they will be learning for a specific amount of time and then there is a peer to to adopt this strategy.",
                    "label": 0
                },
                {
                    "sent": "Learning that specific amount of time.",
                    "label": 0
                },
                {
                    "sent": "We'll take that offline and then OK.",
                    "label": 0
                }
            ]
        }
    }
}