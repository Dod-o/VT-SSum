{
    "id": "cefurqgw5brku2d4thzyeieuq4h72tmy",
    "title": "Detecting Bids for Eye Contact Using A Wearable Camera",
    "info": {
        "author": [
            "Zhefan Ye, College of Computing, Georgia Institute of Technology"
        ],
        "published": "July 2, 2015",
        "recorded": "May 2015",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/fgconference2015_ye_wearable_camera/",
    "segmentation": [
        [
            "Yes, also my name is Ifeanyi.",
            "I am a graduate student from Georgia Institute of Technology, so that act."
        ],
        [
            "Eye contact is a very important topic in many aspects, so eye contact is a very important social social signal and they play a very crucial role in regulating social interaction from the first month of the life.",
            "So many current case measurement technologies has many limitations.",
            "For instance, for a table mounted eye tracker, it requires the person for this instance the child in the image to sit in front of a computer screen and look.",
            "At the computer, the Monitor for several minutes.",
            "So this is obviously not a naturalistic interaction.",
            "Using a wearable eye tracker, on the other hand, that can be more portable.",
            "However, it is still challenging things that the calibration has to be done individually and the child may not feel comfortable wearing those wearable device, so we."
        ],
        [
            "Took a first person view approach to try to detect the eye contact.",
            "So in this setting we asked adult Examiner to where the Eagles this kind of a first person view glasses.",
            "So we use the people had glasses and then the camera on the glasses.",
            "Captured the first person view of the child and in this case the child and the other can engage in a naturalistic interaction.",
            "So here we define bits for eye contact.",
            "As as at the moment when the child looking towards adults, wearable camera as an approximation of the child looking towards adult size.",
            "So.",
            "Of.",
            "So yes, so this experimental setup is for AR is designed for our multi modal dyadic behavioral behavior data set or MDB.",
            "So over the past several years we have been collecting this data set.",
            "It's over 160 sessions now and each of this is an adult child semi constructed place in interaction and each session lasts from three to five minutes and the age of the child is from 15.",
            "Muscle to 30 months old.",
            "So this data set is a unique collection of multimodal recordings, such as video, audio and physiological.",
            "So the purpose of this study is to elicit the social attention back, kind of back and forth that interaction and the nonverbal communication from the child.",
            "So, for instance, the nonverbal communication can be eye contact."
        ],
        [
            "OK, so for the contributions of this work, we demonstrate that the feasibility of the eye contact detection based on first person vision, first person video and we also collected the first data set of the first person video from adult child social interactions.",
            "We further evaluate the accuracy of first person approach at both frame and event level."
        ],
        [
            "OK, so in this talk I am going to introduce our frame level detection model followed by the event level detection and then the expense and the results and finally conclusion."
        ],
        [
            "OK, so to detect the face to find an eye contact the first thing we need to do is to to find the face.",
            "So we use the Omen or Calvin Library, which is a commercial software that can detect human face and we basically find the face in the image and once find image we further use the interface from CMU to great work.",
            "By the way to localize the facial landmarks and estimate hypothesis.",
            "Yes, so you can see the yellow dots is the facial Marks and the hippos is illustrated by the green, blue, and red lines.",
            "So once."
        ],
        [
            "We find that the official marks we can further extract using like the icon are to extract I region images.",
            "So just by looking at those images it can be very confusing like it's very hard even for human to determine which frame needs eye contact or not.",
            "So can anyone guess like which one is the eye contact list?",
            "On the right, and that is actually.",
            "So yeah, let's just put the image back to into the context so it's become more clear that the child on the right is actually looking at the camera, but left is not.",
            "So that's why we use this post dependent egocentric eye contact approach, or we call it peak two as our frame level model.",
            "So."
        ],
        [
            "So basically to go through our pipeline so the first thing we do is to estimate the hippos of all the images that has a face.",
            "So basically we find all the apples and then we cluster those hippos and within each cluster we further extract the eye region.",
            "We quite like crop the eye region images and then we."
        ],
        [
            "Further extract appearance feature from those regions.",
            "So in our case we use hot feature and the last step is to train a random forest classifier to basically determine if this frame is eye contact or not.",
            "So to summarize, our frame level model so we have we use this factor graph representation to to express our model.",
            "So we have the hippos.",
            "We estimate apples in each frame and then we using Gaussian.",
            "But we use Gaussian mixture model to cluster those face to Castle hypnosis and within each cluster we extract appearance feature from the original images and eventually we use the random forest classifier to predict the label of."
        ],
        [
            "It's pretty clear label on this frame.",
            "Also, we simplified our model to the YT and this white EXT.",
            "This factor graph, so YT is the label at the frame, T&XT is the feature at frame T, so we use that as our unit potential in our model later on."
        ],
        [
            "So let's move on to the available today."
        ],
        [
            "So a BI event we define that is event is a set of consecutive one set label on the time series.",
            "So in this case we have like 2 events like over the time series.",
            "So we have like you know potential for each frame from our previous frame level model and then we further add the pairwise potential between adjacent frames based on their unique potential based on their labels and they are.",
            "Features so by solving this so this constructs our linear chain conditional random field model and by solving this CRF we can get the final label for our event detection model.",
            "OK."
        ],
        [
            "So in this part I'm going to show to show the our performance, our performance of our method and also the experiment setup."
        ],
        [
            "So we have 12 sessions of adult child interaction and audio session has been an entity annotated by 5 individual annotators, and to get the ground truth we take the majority vote and also for the other both frame level and event level evaluation.",
            "We use the leave one subject out cross validation."
        ],
        [
            "So here is a video showing that there is the performance of our method.",
            "So when the facial bounding box turn red, that basically indicates that our detector find a true eye contact frame and when the bundles turned say blue or sign, that basically means that the our detector misfiled or Mr Eye contact frame.",
            "And you can see we have a lot of camera motion since it's mounted on the dogs head and also there are some like this.",
            "The fish the cruising happened quite often in our data set."
        ],
        [
            "OK, so two for the quantitative evaluation.",
            "We first showed the frame level result, so this is a precision recall plot of the precision recall curves, so you can see that we so the first thing is the red line.",
            "Here is the our baseline, so this is from the gaze locking method, which is a previous approach to find eye contact from a given image and so and then the yellow dot at the top right corner is.",
            "The human level performance and the colored clutter in the middle is our method with different features.",
            "So for instance, we try the hog feature, the LBP feature an even the convolutional network network feature, and so the figure shows that our method actually outperformed the baseline in a very large margin, but still cannot match with the human level performance.",
            "So we dig a little bit deeper into the field cases, so that's what we found.",
            "So most of the field cases are actually stemmed from the field failed face detections.",
            "So for instance, when the face is a partial included and when the faces like rotated in a very extreme angle, so in both those cases are our face detector failed, they can just cannot find like a face in the image that we do not have the eye contact result.",
            "However, human is still be there still.",
            "Able to pick up like the face and determined that there is there is an eye contact within those frames so but actually up after we apply our event level model, those feel some of the failed case.",
            "Failed cases can be elevated.",
            "OK, so.",
            "So."
        ],
        [
            "Next is the event level results.",
            "So to assess the level of the event level performance, we use the average precision as the metric to evaluate the number of matched events between ground truth and prediction so.",
            "The average precision is actually a is often used in object detection model community.",
            "So in our case we consider an event as a 1 dimensional object on a time series instead of 2D image.",
            "So we basically trying to find the like the matching number between the ground truth and the prediction.",
            "So as the bar graph below shows that we have the frame level result model and the event level model like produced every position and you can see the.",
            "Frame level model at the event level, model obviously achieves better performance, so to just to visualize the difference between the frame level and event level and ground truth.",
            "So we have.",
            "So we use a little bar.",
            "The small bar on the time Series 2 like Visual to represent unset frame and you can see the highlighted area by the red bounding box, so some of the like frames in the frame level model are missing within an event.",
            "But after we applied the available model.",
            "It can like smooth the result, pick up those missing frames and eventually, like you know, make one single event again so you can match with the ground truth quite nicely.",
            "So.",
            "Just to, uh"
        ],
        [
            "Conclude so, so we to our method.",
            "Our results showed that the noninvasive detection of eye contact during the face to face interaction can be achieved through a first person vision, and we also developed a prototype system for automatically measuring the frequency and duration of eye contact events.",
            "We also showed that the temporal smoothing, such as the CRF, our linear linear chain, CRF model, can improve the accuracy of event level detection and last but not least.",
            "Uh, we collected data set of the first person social interaction, which is which is available to the research community via MDB.",
            "So thank you very."
        ],
        [
            "Which."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, also my name is Ifeanyi.",
                    "label": 0
                },
                {
                    "sent": "I am a graduate student from Georgia Institute of Technology, so that act.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eye contact is a very important topic in many aspects, so eye contact is a very important social social signal and they play a very crucial role in regulating social interaction from the first month of the life.",
                    "label": 0
                },
                {
                    "sent": "So many current case measurement technologies has many limitations.",
                    "label": 1
                },
                {
                    "sent": "For instance, for a table mounted eye tracker, it requires the person for this instance the child in the image to sit in front of a computer screen and look.",
                    "label": 0
                },
                {
                    "sent": "At the computer, the Monitor for several minutes.",
                    "label": 0
                },
                {
                    "sent": "So this is obviously not a naturalistic interaction.",
                    "label": 0
                },
                {
                    "sent": "Using a wearable eye tracker, on the other hand, that can be more portable.",
                    "label": 1
                },
                {
                    "sent": "However, it is still challenging things that the calibration has to be done individually and the child may not feel comfortable wearing those wearable device, so we.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Took a first person view approach to try to detect the eye contact.",
                    "label": 0
                },
                {
                    "sent": "So in this setting we asked adult Examiner to where the Eagles this kind of a first person view glasses.",
                    "label": 0
                },
                {
                    "sent": "So we use the people had glasses and then the camera on the glasses.",
                    "label": 0
                },
                {
                    "sent": "Captured the first person view of the child and in this case the child and the other can engage in a naturalistic interaction.",
                    "label": 1
                },
                {
                    "sent": "So here we define bits for eye contact.",
                    "label": 0
                },
                {
                    "sent": "As as at the moment when the child looking towards adults, wearable camera as an approximation of the child looking towards adult size.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Of.",
                    "label": 0
                },
                {
                    "sent": "So yes, so this experimental setup is for AR is designed for our multi modal dyadic behavioral behavior data set or MDB.",
                    "label": 0
                },
                {
                    "sent": "So over the past several years we have been collecting this data set.",
                    "label": 0
                },
                {
                    "sent": "It's over 160 sessions now and each of this is an adult child semi constructed place in interaction and each session lasts from three to five minutes and the age of the child is from 15.",
                    "label": 0
                },
                {
                    "sent": "Muscle to 30 months old.",
                    "label": 0
                },
                {
                    "sent": "So this data set is a unique collection of multimodal recordings, such as video, audio and physiological.",
                    "label": 0
                },
                {
                    "sent": "So the purpose of this study is to elicit the social attention back, kind of back and forth that interaction and the nonverbal communication from the child.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, the nonverbal communication can be eye contact.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so for the contributions of this work, we demonstrate that the feasibility of the eye contact detection based on first person vision, first person video and we also collected the first data set of the first person video from adult child social interactions.",
                    "label": 0
                },
                {
                    "sent": "We further evaluate the accuracy of first person approach at both frame and event level.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so in this talk I am going to introduce our frame level detection model followed by the event level detection and then the expense and the results and finally conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to detect the face to find an eye contact the first thing we need to do is to to find the face.",
                    "label": 0
                },
                {
                    "sent": "So we use the Omen or Calvin Library, which is a commercial software that can detect human face and we basically find the face in the image and once find image we further use the interface from CMU to great work.",
                    "label": 0
                },
                {
                    "sent": "By the way to localize the facial landmarks and estimate hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Yes, so you can see the yellow dots is the facial Marks and the hippos is illustrated by the green, blue, and red lines.",
                    "label": 0
                },
                {
                    "sent": "So once.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find that the official marks we can further extract using like the icon are to extract I region images.",
                    "label": 0
                },
                {
                    "sent": "So just by looking at those images it can be very confusing like it's very hard even for human to determine which frame needs eye contact or not.",
                    "label": 0
                },
                {
                    "sent": "So can anyone guess like which one is the eye contact list?",
                    "label": 0
                },
                {
                    "sent": "On the right, and that is actually.",
                    "label": 0
                },
                {
                    "sent": "So yeah, let's just put the image back to into the context so it's become more clear that the child on the right is actually looking at the camera, but left is not.",
                    "label": 0
                },
                {
                    "sent": "So that's why we use this post dependent egocentric eye contact approach, or we call it peak two as our frame level model.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically to go through our pipeline so the first thing we do is to estimate the hippos of all the images that has a face.",
                    "label": 0
                },
                {
                    "sent": "So basically we find all the apples and then we cluster those hippos and within each cluster we further extract the eye region.",
                    "label": 0
                },
                {
                    "sent": "We quite like crop the eye region images and then we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further extract appearance feature from those regions.",
                    "label": 0
                },
                {
                    "sent": "So in our case we use hot feature and the last step is to train a random forest classifier to basically determine if this frame is eye contact or not.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, our frame level model so we have we use this factor graph representation to to express our model.",
                    "label": 0
                },
                {
                    "sent": "So we have the hippos.",
                    "label": 0
                },
                {
                    "sent": "We estimate apples in each frame and then we using Gaussian.",
                    "label": 0
                },
                {
                    "sent": "But we use Gaussian mixture model to cluster those face to Castle hypnosis and within each cluster we extract appearance feature from the original images and eventually we use the random forest classifier to predict the label of.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's pretty clear label on this frame.",
                    "label": 0
                },
                {
                    "sent": "Also, we simplified our model to the YT and this white EXT.",
                    "label": 0
                },
                {
                    "sent": "This factor graph, so YT is the label at the frame, T&XT is the feature at frame T, so we use that as our unit potential in our model later on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's move on to the available today.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So a BI event we define that is event is a set of consecutive one set label on the time series.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have like 2 events like over the time series.",
                    "label": 0
                },
                {
                    "sent": "So we have like you know potential for each frame from our previous frame level model and then we further add the pairwise potential between adjacent frames based on their unique potential based on their labels and they are.",
                    "label": 0
                },
                {
                    "sent": "Features so by solving this so this constructs our linear chain conditional random field model and by solving this CRF we can get the final label for our event detection model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this part I'm going to show to show the our performance, our performance of our method and also the experiment setup.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have 12 sessions of adult child interaction and audio session has been an entity annotated by 5 individual annotators, and to get the ground truth we take the majority vote and also for the other both frame level and event level evaluation.",
                    "label": 0
                },
                {
                    "sent": "We use the leave one subject out cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is a video showing that there is the performance of our method.",
                    "label": 0
                },
                {
                    "sent": "So when the facial bounding box turn red, that basically indicates that our detector find a true eye contact frame and when the bundles turned say blue or sign, that basically means that the our detector misfiled or Mr Eye contact frame.",
                    "label": 0
                },
                {
                    "sent": "And you can see we have a lot of camera motion since it's mounted on the dogs head and also there are some like this.",
                    "label": 0
                },
                {
                    "sent": "The fish the cruising happened quite often in our data set.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so two for the quantitative evaluation.",
                    "label": 0
                },
                {
                    "sent": "We first showed the frame level result, so this is a precision recall plot of the precision recall curves, so you can see that we so the first thing is the red line.",
                    "label": 0
                },
                {
                    "sent": "Here is the our baseline, so this is from the gaze locking method, which is a previous approach to find eye contact from a given image and so and then the yellow dot at the top right corner is.",
                    "label": 0
                },
                {
                    "sent": "The human level performance and the colored clutter in the middle is our method with different features.",
                    "label": 0
                },
                {
                    "sent": "So for instance, we try the hog feature, the LBP feature an even the convolutional network network feature, and so the figure shows that our method actually outperformed the baseline in a very large margin, but still cannot match with the human level performance.",
                    "label": 0
                },
                {
                    "sent": "So we dig a little bit deeper into the field cases, so that's what we found.",
                    "label": 0
                },
                {
                    "sent": "So most of the field cases are actually stemmed from the field failed face detections.",
                    "label": 0
                },
                {
                    "sent": "So for instance, when the face is a partial included and when the faces like rotated in a very extreme angle, so in both those cases are our face detector failed, they can just cannot find like a face in the image that we do not have the eye contact result.",
                    "label": 0
                },
                {
                    "sent": "However, human is still be there still.",
                    "label": 0
                },
                {
                    "sent": "Able to pick up like the face and determined that there is there is an eye contact within those frames so but actually up after we apply our event level model, those feel some of the failed case.",
                    "label": 0
                },
                {
                    "sent": "Failed cases can be elevated.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next is the event level results.",
                    "label": 0
                },
                {
                    "sent": "So to assess the level of the event level performance, we use the average precision as the metric to evaluate the number of matched events between ground truth and prediction so.",
                    "label": 0
                },
                {
                    "sent": "The average precision is actually a is often used in object detection model community.",
                    "label": 0
                },
                {
                    "sent": "So in our case we consider an event as a 1 dimensional object on a time series instead of 2D image.",
                    "label": 0
                },
                {
                    "sent": "So we basically trying to find the like the matching number between the ground truth and the prediction.",
                    "label": 0
                },
                {
                    "sent": "So as the bar graph below shows that we have the frame level result model and the event level model like produced every position and you can see the.",
                    "label": 0
                },
                {
                    "sent": "Frame level model at the event level, model obviously achieves better performance, so to just to visualize the difference between the frame level and event level and ground truth.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "So we use a little bar.",
                    "label": 0
                },
                {
                    "sent": "The small bar on the time Series 2 like Visual to represent unset frame and you can see the highlighted area by the red bounding box, so some of the like frames in the frame level model are missing within an event.",
                    "label": 0
                },
                {
                    "sent": "But after we applied the available model.",
                    "label": 0
                },
                {
                    "sent": "It can like smooth the result, pick up those missing frames and eventually, like you know, make one single event again so you can match with the ground truth quite nicely.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just to, uh",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclude so, so we to our method.",
                    "label": 0
                },
                {
                    "sent": "Our results showed that the noninvasive detection of eye contact during the face to face interaction can be achieved through a first person vision, and we also developed a prototype system for automatically measuring the frequency and duration of eye contact events.",
                    "label": 0
                },
                {
                    "sent": "We also showed that the temporal smoothing, such as the CRF, our linear linear chain, CRF model, can improve the accuracy of event level detection and last but not least.",
                    "label": 0
                },
                {
                    "sent": "Uh, we collected data set of the first person social interaction, which is which is available to the research community via MDB.",
                    "label": 0
                },
                {
                    "sent": "So thank you very.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which.",
                    "label": 0
                }
            ]
        }
    }
}