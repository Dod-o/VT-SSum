{
    "id": "3tmhlmehyfbq6ueg7lu2rcgqrwdilfw7",
    "title": "Multi-task Regularization of Generative Similarity Models",
    "info": {
        "author": [
            "Luca Cazzanti, Department of Electrical Engineering, University of Washington"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Multi-Task Learning"
        ]
    },
    "url": "http://videolectures.net/simbad2011_cazzanti_generative/",
    "segmentation": [
        [
            "Alright, well thank you very much.",
            "This is joint work with Professor My Gupta and her student Sergian.",
            "My colleague Mike Gabey."
        ],
        [
            "So what I'm going to present is going to be an improvement on a method that we developed a few years ago called local similarity discriminant analysis.",
            "So I'm going to spend a little bit of time describing that so that you know what the problem is.",
            "And in fact the problem is that we need to regularize the parameter estimates in some cases, and I'll tell you when there's a problem in that the solutions are.",
            "You know there are no solutions to our estimates, so this is what the topic the topic of the stock is going to be about, and the proposed solution is going to be this multitasker realization that I will explain and I'll show you how this approach works on some experimental results."
        ],
        [
            "So you all know this and we heard about it this morning too.",
            "In the standard approach, when you have feature based learning, you have objects that are characterized by features and you train your standard classifier any any classifier you want and you get a classification.",
            "And in the genitive case, the model quantity is the feature vector.",
            "OK, so you build a probability model of the future vector conditioned on a class."
        ],
        [
            "What we want to do is do something analogous to that, but with similarities so you do not have the features available, but you do have a notion of how related the objects are to each other.",
            "I know and that's dependent on the problem domain and by the way, I will be using this toy example of classifying an image as being a fruit or iOS logo.",
            "Just to illustrate the main points."
        ],
        [
            "So all you have available is a matrix of pairwise similarities and what we want to do in our approaches.",
            "Have a generative approach where we model the probability distribution of the signal of the pairwise similarities conditioned on a class OK."
        ],
        [
            "Now in in local similarity discriminant analysis, we build these.",
            "Probability distributions of the similarities in this way.",
            "Let's say you have to classify an unknown object, the Apple that's X. OK, an you know it's similarities to a set of training points.",
            "That belong to class one.",
            "So you look at all those similarities, OK?"
        ],
        [
            "And then you build your model.",
            "You assume that the Apple belongs to Class 1.",
            "And you build the probability model of all the similarities.",
            "All of its similarities to Class 1."
        ],
        [
            "Then you do the same.",
            "You assume that the Apple belongs to the other class.",
            "And you also build a class conditional probability model of all the similarities of that object to that 2 two Class 1."
        ],
        [
            "And you do the same for the other.",
            "For the similarities of that object to the other class.",
            "So there are actually G squared class conditional models that you build.",
            "G is the number of classes OK?"
        ],
        [
            "And then you classify in your standard ah, posterior way."
        ],
        [
            "So I haven't told you what the shape of that probability function is on the similarities and in local similarity discriminant analysis it's an exponential.",
            "So.",
            "So basically what it's actually.",
            "It's an average of exponentials.",
            "So what you do you look at the similarity of X to Z and it's an exponential.",
            "You read it off the chart, that's probability, then you add it to, you know the similarity of the Apple to another fruit, and you added on.",
            "You get pH of TH.",
            "OK."
        ],
        [
            "So of course you have to estimate the the exponent OK and the way that we do that in local SDA is to impose a mean constraint.",
            "So we say that the expected value of that exponential PDF must be equal to the observed mean similarity from the data, so that expression says expected value.",
            "The similarity of an object to class one must be equal to the mean similarity DI observed from the training data.",
            "Alright."
        ],
        [
            "And there it is.",
            "So you look at the look at the similarity from training data."
        ],
        [
            "You solve for Lambda and outcomes an exponential all right."
        ],
        [
            "You do the same thing for the similarities of the objects from Class 1 to Class 2.",
            "You get the mean similarity.",
            "You solve for Lambda and you get the exponential.",
            "OK."
        ],
        [
            "Now here's the problem arises when.",
            "The observed similarity locally is a maximum or minimum.",
            "Now in that.",
            "In that case, there's no solution.",
            "The Lambda is an infinite OK, so we need to do something about that, and So what we're going to do is we're going to let the mean similarities, which are estimated individually.",
            "Right now, we're going to actually let them influence each other, so we're going to let the mean similarity from Class 1 to class one or Class 1 to Class 2.",
            "Once each other and get updated values."
        ],
        [
            "And then hopefully we're going to be able to estimate the exponential.",
            "So this is what happens in pictures.",
            "Mean similarity of all the objects from class one within the same class.",
            "Let's say happens to fall on a maximum OK, and then you have the main similarity value objects from Class 1 to Class 2.",
            "It's right there now.",
            "In this case you can actually solve for the exponential, but in the first case you cannot OK."
        ],
        [
            "So if you let them influence each other and I'll tell you in a couple of slides, I would do that.",
            "What happens is that that observe similarity moves.",
            "Into an invisible region and you can actually solve for an exponential at the same time.",
            "The companion means similarity also moves a little bit OK. And so the exponential for the other similarity changes also look."
        ],
        [
            "So what we say in this in this.",
            "In this framework, we say that estimating each individual means similarity from Class G to class to class.",
            "H is the single task.",
            "But in a multi task framework we're going to let them influence each other.",
            "This is the way we set up the problem.",
            "It's an optimization problem.",
            "It's actually simpler than it looks.",
            "If you look at the first term, so let's pretend the second term is not there.",
            "If you solve that, you get the original similarities back.",
            "OK. What minimizes the sum of squares?",
            "It's the original meaning.",
            "OK, now if you look at the second term."
        ],
        [
            "Actually, if you look at the second term.",
            "You have the similarities, which are the V in the mean similarities V JK between class J&K and class airline.",
            "We're going to kind of trying to push him together a little bit.",
            "We're going to let them influence each other, but how much are they going to influence each other?",
            "They're going to influence each other by.",
            "Hey, by that matrix A, which is a task relatedness matrix.",
            "OK, that weighs the contribution of the mean similarity from another class conditional probability distribution on the one that I'm trying to regularize OK. And then finally that parameter ADA controls how much regularization I'm going to put on my pro."
        ],
        [
            "So if a is symmetric and invertible, the nice thing about this approach is that there's a closed form solution, and you don't really have to memorize this this expression, But the key point is that you can compute this in a closed form, which is really, really nice, really nice.",
            "So what you do is that first you compute.",
            "The mean similarities from your data.",
            "Then you apply this regularization, which is a close form regularization and you get new values.",
            "The mean star values of the similarities OK, and then you impose those on your original mean constraint and you get knew lambdas which are now feasible.",
            "So this is the approach that we took."
        ],
        [
            "So how do you?",
            "How do you choose the task relatedness matrix?",
            "Symmetric and invertible gives you a guaranteed closed form solution, so that's nice.",
            "We experimented with this kernel, just a Gaussian kernel, and the reason is that we want to have.",
            "Mean similarities that are values that are close to each other.",
            "We want to have them influence each other more than mean similarities that are far away and the reason is that if they are close to each other, their corresponding exponential distributions are going to be kind of close to each other, so more relevant to each other than if they are far away OK.",
            "But the other nice thing is that you can use any problem dependent task related this.",
            "So I will show you an example of how you can incorporate a problem specific task relatedness into this approach."
        ],
        [
            "So we we tried this on a set of benchmark datasets.",
            "And this is just to illustrate the breadth of applicability of this method.",
            "One is an Amazon data set where the similarities are the probability of buying a book, given that you had visited a webpage for another book, and if you think about a week's top this morning in this data set, self similarity could be less than similarity to something else, because it's possible that you visit a page for a book an you end up buying another book more frequently.",
            "Then you buy that book, so it's kind of an interesting case.",
            "The sonar data set is data set where people listen to sonar, echoes, pairs of sonar, echoes and judged.",
            "The perceptual similarity of the two echoes patrol data set.",
            "People were asked to name.",
            "Other people in their own military patrol, and so this is just a 01 similarity and people make mistakes, OK?",
            "Voting as you look at voting records of people in the US Congress and try to classify them as.",
            "You know what political party they belong to and then a face recognition data set.",
            "So many different types of similarities, some cognitive, some you know, image processing and whatnot."
        ],
        [
            "OK, so there are here the results.",
            "So this is the error.",
            "This is the error rate.",
            "It's a percent error rate.",
            "There are two types of things.",
            "Two things you can look at.",
            "One is this multi task help you.",
            "Compared to just a single task, local SD and the answer is yes.",
            "So Luckily our efforts were not wasted.",
            "Every time the multitask formulation helps over local SDA.",
            "So if you just look at the top 2 lines there, so that's great.",
            "You know this is what motivated our approach was to.",
            "Help in those cases when you really need it to regularize the parameter estimates OK."
        ],
        [
            "Now the other thing that you can say is that multi task is competitive with other methods.",
            "For example you can use K nearest neighbors.",
            "Using the similarity you can use an SVM KNN using the similarities of features and then using a. I think it's a inner product for the kernel, so you know it's going up there.",
            "It's something that you can put in your bag of tools if you're doing similarity based classification.",
            "And we think it's competitive, OK?"
        ],
        [
            "Now we had another problem in that we had a data set of press releases if you will, by insurgent groups in Iraq, they put out communicators that put out declarations and we have the problem of establishing the authorship of those documents.",
            "There are eight groups and we want to say.",
            "Who actually wrote that document?",
            "In this case we used the KL divergent over a set of keywords for document similarity, and then we used a problem specific task relatedness.",
            "So in the previous benchmark datasets we use that exponential kernel and here we use an exponential kernel.",
            "But instead of being between the mean similarities here, we take the number of documents that are published jointly.",
            "By two insurgent groups, sometimes groups put out a communicate in the cosigner OK?",
            "And that's a different data set.",
            "So we incorporate this side information as the task relatedness.",
            "Here, in order to regularize the estimates and the reason for doing excuse me, the reason for doing this is that.",
            "Say let's say two groups say group J&K.",
            "They put out.",
            "I don't know 1010 communicates together and then two other groups put out nine communicates together as opposed to maybe some other group to another pair of groups that only puts out two communicates together.",
            "Then the level of relatedness of these pairs of the first 2 pair of groups is more than compared to the third pair groups.",
            "OK, so the results here.",
            "We don't have an SVN.",
            "We don't get around to get the SVM running, but here we have the multi task formulation using the joint statements.",
            "Results which does a little better than using just the Gaussian kernel on the mean similarities and then compared to KNN and then just using the classifier.",
            "Just purely guessing based on the prior.",
            "OK."
        ],
        [
            "So.",
            "What I talked about today was introduced local SDA and reviewed it and I illustrated.",
            "The case when you need to regularize the parameters OK and the proposed solution for that was this multitask regularization approach that has a closed form solution.",
            "So that was nice, and then I illustrated how this performs on the set of benchmark datasets and on a specific practical problem that we were dealing with.",
            "We have a more detailed article submitted to J. Miller in which we discuss what an optimal task relatedness matrix would look like, and so it's available in archive.",
            "So please go and look it up if you if you're interested and the software for local SDA and the datasets are available, please.",
            "If you're interested, download them, try it out on your own problems and I'll be happy to work with you.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, well thank you very much.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Professor My Gupta and her student Sergian.",
                    "label": 0
                },
                {
                    "sent": "My colleague Mike Gabey.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to present is going to be an improvement on a method that we developed a few years ago called local similarity discriminant analysis.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to spend a little bit of time describing that so that you know what the problem is.",
                    "label": 0
                },
                {
                    "sent": "And in fact the problem is that we need to regularize the parameter estimates in some cases, and I'll tell you when there's a problem in that the solutions are.",
                    "label": 0
                },
                {
                    "sent": "You know there are no solutions to our estimates, so this is what the topic the topic of the stock is going to be about, and the proposed solution is going to be this multitasker realization that I will explain and I'll show you how this approach works on some experimental results.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you all know this and we heard about it this morning too.",
                    "label": 0
                },
                {
                    "sent": "In the standard approach, when you have feature based learning, you have objects that are characterized by features and you train your standard classifier any any classifier you want and you get a classification.",
                    "label": 0
                },
                {
                    "sent": "And in the genitive case, the model quantity is the feature vector.",
                    "label": 1
                },
                {
                    "sent": "OK, so you build a probability model of the future vector conditioned on a class.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want to do is do something analogous to that, but with similarities so you do not have the features available, but you do have a notion of how related the objects are to each other.",
                    "label": 0
                },
                {
                    "sent": "I know and that's dependent on the problem domain and by the way, I will be using this toy example of classifying an image as being a fruit or iOS logo.",
                    "label": 0
                },
                {
                    "sent": "Just to illustrate the main points.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all you have available is a matrix of pairwise similarities and what we want to do in our approaches.",
                    "label": 0
                },
                {
                    "sent": "Have a generative approach where we model the probability distribution of the signal of the pairwise similarities conditioned on a class OK.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now in in local similarity discriminant analysis, we build these.",
                    "label": 1
                },
                {
                    "sent": "Probability distributions of the similarities in this way.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have to classify an unknown object, the Apple that's X. OK, an you know it's similarities to a set of training points.",
                    "label": 0
                },
                {
                    "sent": "That belong to class one.",
                    "label": 0
                },
                {
                    "sent": "So you look at all those similarities, OK?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you build your model.",
                    "label": 0
                },
                {
                    "sent": "You assume that the Apple belongs to Class 1.",
                    "label": 0
                },
                {
                    "sent": "And you build the probability model of all the similarities.",
                    "label": 0
                },
                {
                    "sent": "All of its similarities to Class 1.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you do the same.",
                    "label": 0
                },
                {
                    "sent": "You assume that the Apple belongs to the other class.",
                    "label": 0
                },
                {
                    "sent": "And you also build a class conditional probability model of all the similarities of that object to that 2 two Class 1.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you do the same for the other.",
                    "label": 0
                },
                {
                    "sent": "For the similarities of that object to the other class.",
                    "label": 0
                },
                {
                    "sent": "So there are actually G squared class conditional models that you build.",
                    "label": 0
                },
                {
                    "sent": "G is the number of classes OK?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you classify in your standard ah, posterior way.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I haven't told you what the shape of that probability function is on the similarities and in local similarity discriminant analysis it's an exponential.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So basically what it's actually.",
                    "label": 0
                },
                {
                    "sent": "It's an average of exponentials.",
                    "label": 0
                },
                {
                    "sent": "So what you do you look at the similarity of X to Z and it's an exponential.",
                    "label": 0
                },
                {
                    "sent": "You read it off the chart, that's probability, then you add it to, you know the similarity of the Apple to another fruit, and you added on.",
                    "label": 0
                },
                {
                    "sent": "You get pH of TH.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course you have to estimate the the exponent OK and the way that we do that in local SDA is to impose a mean constraint.",
                    "label": 0
                },
                {
                    "sent": "So we say that the expected value of that exponential PDF must be equal to the observed mean similarity from the data, so that expression says expected value.",
                    "label": 0
                },
                {
                    "sent": "The similarity of an object to class one must be equal to the mean similarity DI observed from the training data.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And there it is.",
                    "label": 0
                },
                {
                    "sent": "So you look at the look at the similarity from training data.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You solve for Lambda and outcomes an exponential all right.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You do the same thing for the similarities of the objects from Class 1 to Class 2.",
                    "label": 1
                },
                {
                    "sent": "You get the mean similarity.",
                    "label": 0
                },
                {
                    "sent": "You solve for Lambda and you get the exponential.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now here's the problem arises when.",
                    "label": 0
                },
                {
                    "sent": "The observed similarity locally is a maximum or minimum.",
                    "label": 1
                },
                {
                    "sent": "Now in that.",
                    "label": 0
                },
                {
                    "sent": "In that case, there's no solution.",
                    "label": 0
                },
                {
                    "sent": "The Lambda is an infinite OK, so we need to do something about that, and So what we're going to do is we're going to let the mean similarities, which are estimated individually.",
                    "label": 0
                },
                {
                    "sent": "Right now, we're going to actually let them influence each other, so we're going to let the mean similarity from Class 1 to class one or Class 1 to Class 2.",
                    "label": 1
                },
                {
                    "sent": "Once each other and get updated values.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then hopefully we're going to be able to estimate the exponential.",
                    "label": 0
                },
                {
                    "sent": "So this is what happens in pictures.",
                    "label": 0
                },
                {
                    "sent": "Mean similarity of all the objects from class one within the same class.",
                    "label": 0
                },
                {
                    "sent": "Let's say happens to fall on a maximum OK, and then you have the main similarity value objects from Class 1 to Class 2.",
                    "label": 0
                },
                {
                    "sent": "It's right there now.",
                    "label": 0
                },
                {
                    "sent": "In this case you can actually solve for the exponential, but in the first case you cannot OK.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you let them influence each other and I'll tell you in a couple of slides, I would do that.",
                    "label": 0
                },
                {
                    "sent": "What happens is that that observe similarity moves.",
                    "label": 0
                },
                {
                    "sent": "Into an invisible region and you can actually solve for an exponential at the same time.",
                    "label": 0
                },
                {
                    "sent": "The companion means similarity also moves a little bit OK. And so the exponential for the other similarity changes also look.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we say in this in this.",
                    "label": 0
                },
                {
                    "sent": "In this framework, we say that estimating each individual means similarity from Class G to class to class.",
                    "label": 0
                },
                {
                    "sent": "H is the single task.",
                    "label": 0
                },
                {
                    "sent": "But in a multi task framework we're going to let them influence each other.",
                    "label": 0
                },
                {
                    "sent": "This is the way we set up the problem.",
                    "label": 0
                },
                {
                    "sent": "It's an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's actually simpler than it looks.",
                    "label": 0
                },
                {
                    "sent": "If you look at the first term, so let's pretend the second term is not there.",
                    "label": 0
                },
                {
                    "sent": "If you solve that, you get the original similarities back.",
                    "label": 0
                },
                {
                    "sent": "OK. What minimizes the sum of squares?",
                    "label": 0
                },
                {
                    "sent": "It's the original meaning.",
                    "label": 0
                },
                {
                    "sent": "OK, now if you look at the second term.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually, if you look at the second term.",
                    "label": 0
                },
                {
                    "sent": "You have the similarities, which are the V in the mean similarities V JK between class J&K and class airline.",
                    "label": 0
                },
                {
                    "sent": "We're going to kind of trying to push him together a little bit.",
                    "label": 0
                },
                {
                    "sent": "We're going to let them influence each other, but how much are they going to influence each other?",
                    "label": 0
                },
                {
                    "sent": "They're going to influence each other by.",
                    "label": 0
                },
                {
                    "sent": "Hey, by that matrix A, which is a task relatedness matrix.",
                    "label": 0
                },
                {
                    "sent": "OK, that weighs the contribution of the mean similarity from another class conditional probability distribution on the one that I'm trying to regularize OK. And then finally that parameter ADA controls how much regularization I'm going to put on my pro.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if a is symmetric and invertible, the nice thing about this approach is that there's a closed form solution, and you don't really have to memorize this this expression, But the key point is that you can compute this in a closed form, which is really, really nice, really nice.",
                    "label": 1
                },
                {
                    "sent": "So what you do is that first you compute.",
                    "label": 0
                },
                {
                    "sent": "The mean similarities from your data.",
                    "label": 0
                },
                {
                    "sent": "Then you apply this regularization, which is a close form regularization and you get new values.",
                    "label": 0
                },
                {
                    "sent": "The mean star values of the similarities OK, and then you impose those on your original mean constraint and you get knew lambdas which are now feasible.",
                    "label": 0
                },
                {
                    "sent": "So this is the approach that we took.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you choose the task relatedness matrix?",
                    "label": 1
                },
                {
                    "sent": "Symmetric and invertible gives you a guaranteed closed form solution, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "We experimented with this kernel, just a Gaussian kernel, and the reason is that we want to have.",
                    "label": 0
                },
                {
                    "sent": "Mean similarities that are values that are close to each other.",
                    "label": 1
                },
                {
                    "sent": "We want to have them influence each other more than mean similarities that are far away and the reason is that if they are close to each other, their corresponding exponential distributions are going to be kind of close to each other, so more relevant to each other than if they are far away OK.",
                    "label": 0
                },
                {
                    "sent": "But the other nice thing is that you can use any problem dependent task related this.",
                    "label": 0
                },
                {
                    "sent": "So I will show you an example of how you can incorporate a problem specific task relatedness into this approach.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we we tried this on a set of benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "And this is just to illustrate the breadth of applicability of this method.",
                    "label": 0
                },
                {
                    "sent": "One is an Amazon data set where the similarities are the probability of buying a book, given that you had visited a webpage for another book, and if you think about a week's top this morning in this data set, self similarity could be less than similarity to something else, because it's possible that you visit a page for a book an you end up buying another book more frequently.",
                    "label": 0
                },
                {
                    "sent": "Then you buy that book, so it's kind of an interesting case.",
                    "label": 0
                },
                {
                    "sent": "The sonar data set is data set where people listen to sonar, echoes, pairs of sonar, echoes and judged.",
                    "label": 0
                },
                {
                    "sent": "The perceptual similarity of the two echoes patrol data set.",
                    "label": 0
                },
                {
                    "sent": "People were asked to name.",
                    "label": 0
                },
                {
                    "sent": "Other people in their own military patrol, and so this is just a 01 similarity and people make mistakes, OK?",
                    "label": 0
                },
                {
                    "sent": "Voting as you look at voting records of people in the US Congress and try to classify them as.",
                    "label": 0
                },
                {
                    "sent": "You know what political party they belong to and then a face recognition data set.",
                    "label": 0
                },
                {
                    "sent": "So many different types of similarities, some cognitive, some you know, image processing and whatnot.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so there are here the results.",
                    "label": 0
                },
                {
                    "sent": "So this is the error.",
                    "label": 0
                },
                {
                    "sent": "This is the error rate.",
                    "label": 0
                },
                {
                    "sent": "It's a percent error rate.",
                    "label": 0
                },
                {
                    "sent": "There are two types of things.",
                    "label": 0
                },
                {
                    "sent": "Two things you can look at.",
                    "label": 0
                },
                {
                    "sent": "One is this multi task help you.",
                    "label": 0
                },
                {
                    "sent": "Compared to just a single task, local SD and the answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So Luckily our efforts were not wasted.",
                    "label": 0
                },
                {
                    "sent": "Every time the multitask formulation helps over local SDA.",
                    "label": 0
                },
                {
                    "sent": "So if you just look at the top 2 lines there, so that's great.",
                    "label": 0
                },
                {
                    "sent": "You know this is what motivated our approach was to.",
                    "label": 0
                },
                {
                    "sent": "Help in those cases when you really need it to regularize the parameter estimates OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the other thing that you can say is that multi task is competitive with other methods.",
                    "label": 0
                },
                {
                    "sent": "For example you can use K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Using the similarity you can use an SVM KNN using the similarities of features and then using a. I think it's a inner product for the kernel, so you know it's going up there.",
                    "label": 0
                },
                {
                    "sent": "It's something that you can put in your bag of tools if you're doing similarity based classification.",
                    "label": 0
                },
                {
                    "sent": "And we think it's competitive, OK?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we had another problem in that we had a data set of press releases if you will, by insurgent groups in Iraq, they put out communicators that put out declarations and we have the problem of establishing the authorship of those documents.",
                    "label": 1
                },
                {
                    "sent": "There are eight groups and we want to say.",
                    "label": 0
                },
                {
                    "sent": "Who actually wrote that document?",
                    "label": 0
                },
                {
                    "sent": "In this case we used the KL divergent over a set of keywords for document similarity, and then we used a problem specific task relatedness.",
                    "label": 1
                },
                {
                    "sent": "So in the previous benchmark datasets we use that exponential kernel and here we use an exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "But instead of being between the mean similarities here, we take the number of documents that are published jointly.",
                    "label": 0
                },
                {
                    "sent": "By two insurgent groups, sometimes groups put out a communicate in the cosigner OK?",
                    "label": 1
                },
                {
                    "sent": "And that's a different data set.",
                    "label": 0
                },
                {
                    "sent": "So we incorporate this side information as the task relatedness.",
                    "label": 0
                },
                {
                    "sent": "Here, in order to regularize the estimates and the reason for doing excuse me, the reason for doing this is that.",
                    "label": 0
                },
                {
                    "sent": "Say let's say two groups say group J&K.",
                    "label": 0
                },
                {
                    "sent": "They put out.",
                    "label": 0
                },
                {
                    "sent": "I don't know 1010 communicates together and then two other groups put out nine communicates together as opposed to maybe some other group to another pair of groups that only puts out two communicates together.",
                    "label": 0
                },
                {
                    "sent": "Then the level of relatedness of these pairs of the first 2 pair of groups is more than compared to the third pair groups.",
                    "label": 1
                },
                {
                    "sent": "OK, so the results here.",
                    "label": 0
                },
                {
                    "sent": "We don't have an SVN.",
                    "label": 0
                },
                {
                    "sent": "We don't get around to get the SVM running, but here we have the multi task formulation using the joint statements.",
                    "label": 0
                },
                {
                    "sent": "Results which does a little better than using just the Gaussian kernel on the mean similarities and then compared to KNN and then just using the classifier.",
                    "label": 0
                },
                {
                    "sent": "Just purely guessing based on the prior.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I talked about today was introduced local SDA and reviewed it and I illustrated.",
                    "label": 0
                },
                {
                    "sent": "The case when you need to regularize the parameters OK and the proposed solution for that was this multitask regularization approach that has a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "So that was nice, and then I illustrated how this performs on the set of benchmark datasets and on a specific practical problem that we were dealing with.",
                    "label": 0
                },
                {
                    "sent": "We have a more detailed article submitted to J. Miller in which we discuss what an optimal task relatedness matrix would look like, and so it's available in archive.",
                    "label": 0
                },
                {
                    "sent": "So please go and look it up if you if you're interested and the software for local SDA and the datasets are available, please.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, download them, try it out on your own problems and I'll be happy to work with you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}