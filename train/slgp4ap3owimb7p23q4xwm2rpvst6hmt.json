{
    "id": "slgp4ap3owimb7p23q4xwm2rpvst6hmt",
    "title": "Learning Structural Correspondences Across Different Linguistic Domains with Synchronous Neural Language Models",
    "info": {
        "author": [
            "Stephan Gouws, MIH Media Lab, Stellenbosch University"
        ],
        "published": "Jan. 11, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Computational Linguistics"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_gouws_neural_language/",
    "segmentation": [
        [
            "Thank you so my name is Stephen Coast but also respond to Stephen Gauss.",
            "And today I'd like to talk to you about some of our ongoing work on learning structural correspondences between different domains, and this is joint work with her, again from the Rain and Josh."
        ],
        [
            "Avenger.",
            "So after a lot of the talks today, this would be a very familiar setting.",
            "A lot of times in NLP, as with many other machine learning applications, we said with a problem where we have a lot of labeled data in one domain.",
            "For instance, in this case, we might want to do part of speech tagging on Wall Street Journal text.",
            "But what we really want to do is we want to apply this to a different domain.",
            "An if you do sort of this naively for part of speech tagging and you transfer and just apply directly.",
            "We see error increases of."
        ],
        [
            "Roughly run on the order of 10%, and it's easy to see why, because in the two different domains, the features that we training on the words behave differently."
        ],
        [
            "So domain adaptation is basically the quest of trying to train a classifier, be that in our case part of speech tagging or named entity recognition on a source domain where we have lots of labels and then do something with this classifier so that we can actually apply it to a target domain where we don't have lots of labeled information."
        ],
        [
            "One way that you can actually do that is by taking the representations of your sourcing you target domain and mapping them to a common common space where the similarities between the two domains are amplified."
        ],
        [
            "In that case, you can train your classifier on this common representation of the source domain and just directly apply it again to the target domain in the common representation an you would see improved results."
        ],
        [
            "So in the case of the part of speech tagging as an extreme example, one might imagine that we have again the English text.",
            "The clash is a sign of a new toughness and divisiveness in Japan's once cozy financial circles and you might want to apply this to a different language.",
            "For instance, my mother tongue Afrikans and I'll read this to you just because I can devote thing is attacking funny, but I ate in fragile.",
            "Tighten your pants in sickness financial occurring.",
            "And one way that one can approach doing this is if you can identify words between the two domains which behave similarly from a linguistic point of view as well as from a statistical point of view.",
            "Then these words, which we call pivot."
        ],
        [
            "It's the."
        ],
        [
            "Then the labeled information that you have for those perfect words should be able to tell you something for the same pivot words in the target domain."
        ],
        [
            "But Moreover."
        ],
        [
            "If we have access to lots of."
        ],
        [
            "Data one could imagine that the.",
            "The race of the vocabulary, besides the perfect words, if you can learn the relationship, statistical relationships of those words with respect to the permits, we can do the same thing.",
            "We can take the labeled information that we have in the source domain and transferred to the target domain.",
            "This is the goal."
        ],
        [
            "Structural correspondence learning, which is was approach or proposed by Blitz Red on actually Ryan McDonald.",
            "Takes this idea and the way that they do it is they take the feature representations in the source and the target domain, in this case represented by the vectors X&XT.",
            "And then they map them to a common low dimensional feature space via this linear operator Fi and the goal is that if I should make these domains look as similar as possible from a statistical point of view whilst allowing us to classify as well as possible.",
            "So this was the inspiration for this work, but in this work we would like to propose a."
        ],
        [
            "New idea called deep structural correspondence learning, and it's based on the ability of neural language models to learn unsupervised features which are meaningful representations of language in its domain.",
            "So this is my 32nd overview of neural language models.",
            "Basically, the neural language model takes a few words in its input domain.",
            "For instance, the cat sits on the IT then Maps it to an embedding space where we have low dimensional feature vectors which capture meaningful.",
            "Aspects of the language and jointly with these features, it learns a function which will just go F which can combine the feature vectors for previous words to predict the most likely next word mat.",
            "In this work, we don't just make use of one single neural language model."
        ],
        [
            "You actually make use of two neural language models, which we train jointly, but specifically we don't want the models just to model each domain world.",
            "We actually want them to extract the similarities between that."
        ],
        [
            "Domains, So what we do is we constrain these learned representations for a specific set of private words to be as similar as possible, and in the cases where it makes sense where we have linguistic domains which we know are very similar, the same kinds of languages we can actually constrain these learned functions to be similar as well."
        ],
        [
            "Specifically, the hypothesis that we that we test in this work is that.",
            "Aligning a well chosen subset of the words between the two vocabularies, which we call the pivot words.",
            "With respect to each other in this land, embedded space will iteratively align the rest of the words in your vocabulary is in a meaningful way, which I'll define in a moment with respect to each other in the shared feature space.",
            "The important thing here is we're working in a shared feature space in a meaningful way means that the rest of the vocabulary will arrange itself with respect to the pivots in order for the models, just the model, it's language, and it's the main well."
        ],
        [
            "So the language model that we use is called the log bilinear neural language model an.",
            "It's the simplest form of neural language model was proposed by me, an intern in 2007, and in a nutshell the way that it works is it takes the high dimensional one hot encoding's of the words on its input layer, Maps it to a low dimensional embedding vector layer, and then it learns a function which can combine these words to predict our head, which is the vector for the most likely next word.",
            "And in the NLM incase LBL case, it's just a simple linear combination of the feature vectors and finally the outputs of distribution multinomial over the vocabulary which represents the most likely next word in the training set.",
            "Because this model outputs a probability distribution, we can train this joint.",
            "Joint models."
        ],
        [
            "By defining a cost function in terms of the joint negative log likelihood of the two models and this allows us to actually model the data well in each domain.",
            "But because we want to learn these structural similarities between the vocabularies, we add two extra terms to the cost."
        ],
        [
            "Unction.",
            "Jr, which models the similarities between the learned features for the pivot words and also JF which expresses our prior belief about how similar the languages are in the two domains."
        ],
        [
            "So.",
            "How do we constrain the embeddings?",
            "Well, it's actually really simple, because these embeddings are vectors.",
            "And we have a list of the pivot words between the two domains which behave similarly.",
            "We simply minimize the sum, the weighted sum of the squared distances between these vectors for the perfect words.",
            "How do we?"
        ],
        [
            "String the functions to be similar.",
            "Well, in the cases where we know that the two languages are actually similar and this would be the case.",
            "For instance, if you have English to English, like the midline text to Wall Street Journal, we know that the language is very similar."
        ],
        [
            "We implement this in a way which I'm not going to too much detail, but you're welcome to speak to me afterwards, and it's also discussed in error."
        ],
        [
            "All.",
            "So how does this thing?",
            "Do we evaluate it in two different settings?",
            "The first one was on a synthetic data set which models the case where the three languages are very similar, and what we did is we sample two data, set each without replacements.",
            "In other words to disjoint data sets from the latimes, and then we encourage each data set in a different vocabulary by simply adding a one to the words in the one data set and two to the other words.",
            "We provided the networks with a certain percentage of overlaps of private words between the two domains.",
            "And then we evaluate how similar these representations of similar words by measuring the similarity of the learned vectors for the translation pair."
        ],
        [
            "So here I'm showing the results.",
            "On the Y axis I am showing the average distance between these learned representations for the known translation pairs.",
            "As we progress with training.",
            "The first thing to notice is that.",
            "These are the poses.",
            "Yes, these are the no.",
            "This is this is the distances between all the translation pairs given no pivot words, 1% perfect words and 10% perfect words.",
            "So the important thing to note is if we give the networks know private words to constrain these embeddings, the learned representations simply diverge because there's absolutely no reason for the models to learn any kind of similarity between the two domains.",
            "The interesting thing is if we if we give the models about 1% perfect words, we see a bit of convergence, but around 5 to 10% we see a clear convergence starting for the learned vector representations of these words to start moving towards each other.",
            "The important thing to note here is that this does not impede the models to model its individual domains, so the negative log likelihood on the valid."
        ],
        [
            "Patient states stay the same.",
            "What happens in monkeys because?",
            "Also there is a percentage of periods increasing, so it might be just learning on on this specific."
        ],
        [
            "So in this case we have 5% pivots.",
            "This shows the normalized distance between them and you would expect 5% pivots, probably to bring the average distance between all learn vectors about 5% closer to one another.",
            "If they'd only worked on their bullets, so it clearly works for."
        ],
        [
            "Other words as well.",
            "Frequency in the themselves.",
            "So they might represent yes, this is an unwished, just the distance between those are not included in your penalty.",
            "Yes, probably.",
            "So those are good points and and that was an interesting result, but it wasn't completely unexpected because the two datasets come from exactly the same distribution, so we also evaluated this on English, French, where again we sampled two non parallel datasets and again did the same thing, gave models of certain percentage of known translation pairs as perfect words and in this case we see again the same kind of of."
        ],
        [
            "Behavior if we have no bread, no private words, it just simply diverges because there's no reason to learn similar representations.",
            "If we add 1 to around 10% pivot words, but we increase the regularization weight of that function constraint, we see a slow decline in learning similar representations.",
            "But English and French really are not the same thing.",
            "So if we remove the function distance constraint, we see a very sharp decline in how the distance between all the translation pairs in the vocabulary."
        ],
        [
            "Again, the important thing to note is despite the fact that we constrain these models to learn similar representations for the similar words, we do not impede its ability to actually generalize to the data in its own domain.",
            "In other words, the negative log likelihood on the validation set stay the same."
        ],
        [
            "So in conclusion, we investigated the hypothesis that neural language models can learn similar features and meaningful features for given a well chosen subset of prior pivot words in the two domains.",
            "Our results so far indicate that even around 5% of perfect words is enough to start the convergence process, and even with 10% of forcing, 10% of the vocabulary to be exactly the same still allows the models to model data in its own domain well.",
            "The obvious next step is previous work, as shows that shown these distributed feature representations to be useful for part of speech tagging and named entity recognition in the single domain setting.",
            "So future work we will apply these learn features to see how well we can transfer label data from one domain to another in the multi domain setting."
        ],
        [
            "Thank you.",
            "That's my."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you so my name is Stephen Coast but also respond to Stephen Gauss.",
                    "label": 0
                },
                {
                    "sent": "And today I'd like to talk to you about some of our ongoing work on learning structural correspondences between different domains, and this is joint work with her, again from the Rain and Josh.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Avenger.",
                    "label": 0
                },
                {
                    "sent": "So after a lot of the talks today, this would be a very familiar setting.",
                    "label": 0
                },
                {
                    "sent": "A lot of times in NLP, as with many other machine learning applications, we said with a problem where we have a lot of labeled data in one domain.",
                    "label": 0
                },
                {
                    "sent": "For instance, in this case, we might want to do part of speech tagging on Wall Street Journal text.",
                    "label": 0
                },
                {
                    "sent": "But what we really want to do is we want to apply this to a different domain.",
                    "label": 0
                },
                {
                    "sent": "An if you do sort of this naively for part of speech tagging and you transfer and just apply directly.",
                    "label": 0
                },
                {
                    "sent": "We see error increases of.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roughly run on the order of 10%, and it's easy to see why, because in the two different domains, the features that we training on the words behave differently.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So domain adaptation is basically the quest of trying to train a classifier, be that in our case part of speech tagging or named entity recognition on a source domain where we have lots of labels and then do something with this classifier so that we can actually apply it to a target domain where we don't have lots of labeled information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way that you can actually do that is by taking the representations of your sourcing you target domain and mapping them to a common common space where the similarities between the two domains are amplified.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that case, you can train your classifier on this common representation of the source domain and just directly apply it again to the target domain in the common representation an you would see improved results.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the case of the part of speech tagging as an extreme example, one might imagine that we have again the English text.",
                    "label": 0
                },
                {
                    "sent": "The clash is a sign of a new toughness and divisiveness in Japan's once cozy financial circles and you might want to apply this to a different language.",
                    "label": 1
                },
                {
                    "sent": "For instance, my mother tongue Afrikans and I'll read this to you just because I can devote thing is attacking funny, but I ate in fragile.",
                    "label": 0
                },
                {
                    "sent": "Tighten your pants in sickness financial occurring.",
                    "label": 0
                },
                {
                    "sent": "And one way that one can approach doing this is if you can identify words between the two domains which behave similarly from a linguistic point of view as well as from a statistical point of view.",
                    "label": 0
                },
                {
                    "sent": "Then these words, which we call pivot.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then the labeled information that you have for those perfect words should be able to tell you something for the same pivot words in the target domain.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But Moreover.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we have access to lots of.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Data one could imagine that the.",
                    "label": 0
                },
                {
                    "sent": "The race of the vocabulary, besides the perfect words, if you can learn the relationship, statistical relationships of those words with respect to the permits, we can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We can take the labeled information that we have in the source domain and transferred to the target domain.",
                    "label": 0
                },
                {
                    "sent": "This is the goal.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structural correspondence learning, which is was approach or proposed by Blitz Red on actually Ryan McDonald.",
                    "label": 1
                },
                {
                    "sent": "Takes this idea and the way that they do it is they take the feature representations in the source and the target domain, in this case represented by the vectors X&XT.",
                    "label": 0
                },
                {
                    "sent": "And then they map them to a common low dimensional feature space via this linear operator Fi and the goal is that if I should make these domains look as similar as possible from a statistical point of view whilst allowing us to classify as well as possible.",
                    "label": 1
                },
                {
                    "sent": "So this was the inspiration for this work, but in this work we would like to propose a.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "New idea called deep structural correspondence learning, and it's based on the ability of neural language models to learn unsupervised features which are meaningful representations of language in its domain.",
                    "label": 1
                },
                {
                    "sent": "So this is my 32nd overview of neural language models.",
                    "label": 0
                },
                {
                    "sent": "Basically, the neural language model takes a few words in its input domain.",
                    "label": 1
                },
                {
                    "sent": "For instance, the cat sits on the IT then Maps it to an embedding space where we have low dimensional feature vectors which capture meaningful.",
                    "label": 0
                },
                {
                    "sent": "Aspects of the language and jointly with these features, it learns a function which will just go F which can combine the feature vectors for previous words to predict the most likely next word mat.",
                    "label": 0
                },
                {
                    "sent": "In this work, we don't just make use of one single neural language model.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You actually make use of two neural language models, which we train jointly, but specifically we don't want the models just to model each domain world.",
                    "label": 0
                },
                {
                    "sent": "We actually want them to extract the similarities between that.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Domains, So what we do is we constrain these learned representations for a specific set of private words to be as similar as possible, and in the cases where it makes sense where we have linguistic domains which we know are very similar, the same kinds of languages we can actually constrain these learned functions to be similar as well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Specifically, the hypothesis that we that we test in this work is that.",
                    "label": 0
                },
                {
                    "sent": "Aligning a well chosen subset of the words between the two vocabularies, which we call the pivot words.",
                    "label": 1
                },
                {
                    "sent": "With respect to each other in this land, embedded space will iteratively align the rest of the words in your vocabulary is in a meaningful way, which I'll define in a moment with respect to each other in the shared feature space.",
                    "label": 1
                },
                {
                    "sent": "The important thing here is we're working in a shared feature space in a meaningful way means that the rest of the vocabulary will arrange itself with respect to the pivots in order for the models, just the model, it's language, and it's the main well.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the language model that we use is called the log bilinear neural language model an.",
                    "label": 1
                },
                {
                    "sent": "It's the simplest form of neural language model was proposed by me, an intern in 2007, and in a nutshell the way that it works is it takes the high dimensional one hot encoding's of the words on its input layer, Maps it to a low dimensional embedding vector layer, and then it learns a function which can combine these words to predict our head, which is the vector for the most likely next word.",
                    "label": 0
                },
                {
                    "sent": "And in the NLM incase LBL case, it's just a simple linear combination of the feature vectors and finally the outputs of distribution multinomial over the vocabulary which represents the most likely next word in the training set.",
                    "label": 0
                },
                {
                    "sent": "Because this model outputs a probability distribution, we can train this joint.",
                    "label": 0
                },
                {
                    "sent": "Joint models.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By defining a cost function in terms of the joint negative log likelihood of the two models and this allows us to actually model the data well in each domain.",
                    "label": 0
                },
                {
                    "sent": "But because we want to learn these structural similarities between the vocabularies, we add two extra terms to the cost.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unction.",
                    "label": 0
                },
                {
                    "sent": "Jr, which models the similarities between the learned features for the pivot words and also JF which expresses our prior belief about how similar the languages are in the two domains.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How do we constrain the embeddings?",
                    "label": 1
                },
                {
                    "sent": "Well, it's actually really simple, because these embeddings are vectors.",
                    "label": 0
                },
                {
                    "sent": "And we have a list of the pivot words between the two domains which behave similarly.",
                    "label": 0
                },
                {
                    "sent": "We simply minimize the sum, the weighted sum of the squared distances between these vectors for the perfect words.",
                    "label": 1
                },
                {
                    "sent": "How do we?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "String the functions to be similar.",
                    "label": 0
                },
                {
                    "sent": "Well, in the cases where we know that the two languages are actually similar and this would be the case.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have English to English, like the midline text to Wall Street Journal, we know that the language is very similar.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We implement this in a way which I'm not going to too much detail, but you're welcome to speak to me afterwards, and it's also discussed in error.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "So how does this thing?",
                    "label": 0
                },
                {
                    "sent": "Do we evaluate it in two different settings?",
                    "label": 0
                },
                {
                    "sent": "The first one was on a synthetic data set which models the case where the three languages are very similar, and what we did is we sample two data, set each without replacements.",
                    "label": 1
                },
                {
                    "sent": "In other words to disjoint data sets from the latimes, and then we encourage each data set in a different vocabulary by simply adding a one to the words in the one data set and two to the other words.",
                    "label": 1
                },
                {
                    "sent": "We provided the networks with a certain percentage of overlaps of private words between the two domains.",
                    "label": 0
                },
                {
                    "sent": "And then we evaluate how similar these representations of similar words by measuring the similarity of the learned vectors for the translation pair.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I'm showing the results.",
                    "label": 0
                },
                {
                    "sent": "On the Y axis I am showing the average distance between these learned representations for the known translation pairs.",
                    "label": 0
                },
                {
                    "sent": "As we progress with training.",
                    "label": 0
                },
                {
                    "sent": "The first thing to notice is that.",
                    "label": 0
                },
                {
                    "sent": "These are the poses.",
                    "label": 0
                },
                {
                    "sent": "Yes, these are the no.",
                    "label": 0
                },
                {
                    "sent": "This is this is the distances between all the translation pairs given no pivot words, 1% perfect words and 10% perfect words.",
                    "label": 0
                },
                {
                    "sent": "So the important thing to note is if we give the networks know private words to constrain these embeddings, the learned representations simply diverge because there's absolutely no reason for the models to learn any kind of similarity between the two domains.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing is if we if we give the models about 1% perfect words, we see a bit of convergence, but around 5 to 10% we see a clear convergence starting for the learned vector representations of these words to start moving towards each other.",
                    "label": 0
                },
                {
                    "sent": "The important thing to note here is that this does not impede the models to model its individual domains, so the negative log likelihood on the valid.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patient states stay the same.",
                    "label": 0
                },
                {
                    "sent": "What happens in monkeys because?",
                    "label": 0
                },
                {
                    "sent": "Also there is a percentage of periods increasing, so it might be just learning on on this specific.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this case we have 5% pivots.",
                    "label": 0
                },
                {
                    "sent": "This shows the normalized distance between them and you would expect 5% pivots, probably to bring the average distance between all learn vectors about 5% closer to one another.",
                    "label": 1
                },
                {
                    "sent": "If they'd only worked on their bullets, so it clearly works for.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other words as well.",
                    "label": 0
                },
                {
                    "sent": "Frequency in the themselves.",
                    "label": 0
                },
                {
                    "sent": "So they might represent yes, this is an unwished, just the distance between those are not included in your penalty.",
                    "label": 0
                },
                {
                    "sent": "Yes, probably.",
                    "label": 0
                },
                {
                    "sent": "So those are good points and and that was an interesting result, but it wasn't completely unexpected because the two datasets come from exactly the same distribution, so we also evaluated this on English, French, where again we sampled two non parallel datasets and again did the same thing, gave models of certain percentage of known translation pairs as perfect words and in this case we see again the same kind of of.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Behavior if we have no bread, no private words, it just simply diverges because there's no reason to learn similar representations.",
                    "label": 0
                },
                {
                    "sent": "If we add 1 to around 10% pivot words, but we increase the regularization weight of that function constraint, we see a slow decline in learning similar representations.",
                    "label": 1
                },
                {
                    "sent": "But English and French really are not the same thing.",
                    "label": 0
                },
                {
                    "sent": "So if we remove the function distance constraint, we see a very sharp decline in how the distance between all the translation pairs in the vocabulary.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, the important thing to note is despite the fact that we constrain these models to learn similar representations for the similar words, we do not impede its ability to actually generalize to the data in its own domain.",
                    "label": 0
                },
                {
                    "sent": "In other words, the negative log likelihood on the validation set stay the same.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, we investigated the hypothesis that neural language models can learn similar features and meaningful features for given a well chosen subset of prior pivot words in the two domains.",
                    "label": 1
                },
                {
                    "sent": "Our results so far indicate that even around 5% of perfect words is enough to start the convergence process, and even with 10% of forcing, 10% of the vocabulary to be exactly the same still allows the models to model data in its own domain well.",
                    "label": 1
                },
                {
                    "sent": "The obvious next step is previous work, as shows that shown these distributed feature representations to be useful for part of speech tagging and named entity recognition in the single domain setting.",
                    "label": 0
                },
                {
                    "sent": "So future work we will apply these learn features to see how well we can transfer label data from one domain to another in the multi domain setting.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "That's my.",
                    "label": 0
                }
            ]
        }
    }
}