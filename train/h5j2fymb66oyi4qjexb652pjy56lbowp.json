{
    "id": "h5j2fymb66oyi4qjexb652pjy56lbowp",
    "title": "Study of Classification Algorithms using Moment Analysis",
    "info": {
        "author": [
            "Amit Dhurandha, University of Florida"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Classification"
        ]
    },
    "url": "http://videolectures.net/wehys08_dhurandhar_scm/",
    "segmentation": [
        [
            "This is joint work with my advisor, Doctor Arlene Dobra, and what I'm going to talk about today is.",
            "Methodology which we have recently introduced to study sort of classification algorithms and certain model selection measures an it's still in its infancy, but we have had some promising results with respect to certain algorithms in this, and so we would try.",
            "I would like to discuss this with you all."
        ],
        [
            "So as we know, the problem of classification is only present in today's world.",
            "There are a wide array or array of applications ranging from medicine to finance where classification methods are used."
        ],
        [
            "However, not only are the applications where it, but so are the apples or the actual algorithms, and there are.",
            "The problem is basically that for different applications different algorithms work better."
        ],
        [
            "So what I'm going to suggest today is an approach to study classification algorithms in the non asymptotic regime.",
            "That is, for finite sample sizes.",
            "So how it would behave, say on the real life sample size?"
        ],
        [
            "So consider this problem for the time being.",
            "That is, say you have a distribution say given by Q.",
            "And what you want to do is you want to answer the question how an algorithm behaves with respect to that distribution, say when trained over a sample size of some value in.",
            "So I guess the natural solution one would come up with is you take a sample of size N, train your algorithm, produce a classifier, maybe sample some test sets, find the test error, do this multiple types, and basically report the average average error invariants and say that this is an estimate as to how my algorithm would behave."
        ],
        [
            "Now, ideally, rather than the test error, we would probably want to know the generalization errors and rather than just sampling a few datasets and training your classifier and reporting the error, you would probably want to find the expected value over all possible datasets that you can get from their disk."
        ],
        [
            "Solution, so this is the kind of thing you would want to probably compute.",
            "So here small Caesar classifier G is the generalization error of that classifier D of N is the space of all possible datasets that you can sample from this distribution Q an.",
            "As you can see the generalization here.",
            "Here is a random variable.",
            "It's not a constant, since every classifier is random as it is produced by training a classification algorithm on the sample that you get, which is obviously random and so similarly the second moment is this moment."
        ],
        [
            "So now basically the problem of trying to study a classification algorithm with respect to our distribution can be reduced to this problem of say for example studying the first 2 moments of this accurately and efficiently."
        ],
        [
            "Now the users of this, our first of all, you can study the behavior of.",
            "So if you are able to find these moments and say you are able to plot these so for different values of North, you can study the behavior of the algorithm for finite sample size is another use of this is so.",
            "For example, right now what people do is.",
            "So say you have a data set and a person wants to say the algorithm designer wants to justify that his algorithm is good.",
            "So what he does is it rains.",
            "You're his algorithm on that particular data set and reports the error.",
            "However, if you were, for example, to sort of build a distribution on this data set and then evaluate these moments, it would basically in a way instill more confidence in the practitioner since the results are overall possible datasets from that distribution.",
            "So if the practitioners data set is safe from a similar source or a similar structure to the algorithm designers data set, you would feel more confident in using the person's algorithm then this.",
            "Then the moment also can be used in verification of certain PAC Bayes bounds, because as I'll show you later on, we have also sort of derived relationships between the moments of the generalization error and moments of certain error metrics such as cross validation and rolled out error.",
            "So you can even sort of see the behavior of those metrics.",
            "So I mean using standard PAC Bayes bounds.",
            "You can see how close they are with respect to the actual plots of these moments that you have.",
            "Then you can, since if you have these moments you can study trends.",
            "As I said, Nana symbiotic trends of the behavior of classification algorithms.",
            "This can help you probably to gain insights into why and when and exactly how a classification algorithm behaves when you change certain para meters in the algorithm and the last application.",
            "Basically, as I go on, you'll see what I mean by that."
        ],
        [
            "So now the bottom line is.",
            "Studying the movements can help us basically study algorithms accurately and efficiently so."
        ],
        [
            "The rest of the talk is basically going to be focused on how.",
            "Are you going to compute these moments with?",
            "I mean, as I said accurately and efficiently.",
            "So what I'll first discuss is there's some general strategies now.",
            "These strategies are absolutely general.",
            "The Naive Bayes classifier example just to motivate certain types of optimizations which we might need.",
            "Then I will talk about certain relationships that I've done with the moments of the cross validation error and holdout error, and so on, and so as an example, I will show you I mean certain behaviors that we were able to see through our theoretical formulas.",
            "And then I'll conclude and talk about future work and challenges that exist."
        ],
        [
            "This kind of work, so now note that I've basically written all the formulas with sums, but they're applicable continuous domain as well, and mostly there is more machinery to sort of compute integrals more efficiently than, say, finite sums and so."
        ],
        [
            "The thing is more sort of intuitive in that setting, so now this is our formal definition of generalization errors.",
            "As I said, small Zita is the classifier Ellis, the loss function, and we assume the data is IID and this is the standard."
        ],
        [
            "Definition so now if you were to compute moments directly, that is, say just using so analytically if you wanted to compute this is how you would compute, so you would see what is the probability of a particular data set, and then these data is basically the classifier which is obtained by training the algorithm on that particular data set.",
            "And this is the form you would have so."
        ],
        [
            "Help say consider you have this data set.",
            "OK, so since I said some so discrete data set so where you have same input vectors and K classes, here K is 2.",
            "So you have two classes and say you have a sample size of North.",
            "Now the counts in each of these cells is basically the number of data points that line X1Y1 and X1 white one so on so forth.",
            "So now as you can see if I just fix in there, basically each of these cells can take N possible values.",
            "So the number of possible datasets you confirm if I just fix end with this kind of a setting is.",
            "Andres, two MK minus one since the last time would I mean Umm K -- 1 free paramaters since they are in this table there are two M cells.",
            "If you had classes you would have NK cells."
        ],
        [
            "So if you were to just directly compute it, you would have essentially these many terms and the size of each probability would be MK if you go back to the table, you figure that.",
            "Each of the probabilities choose basically to instantiate a data set.",
            "You need these many terms, so this is definitely too many.",
            "I mean, this is absolutely impractical."
        ],
        [
            "So OK, so the first optimization I'll discuss is basically number of terms."
        ],
        [
            "Optimization, so now to get an idea to get an intuition about this.",
            "So what we first do is instead of going on the space of all possible datasets, we go over the space of all possible classifiers.",
            "Now, how does this intuitively help?",
            "Well, for example, consider deterministic classification algorithm or results are applicable to even randomized algorithms.",
            "But just for understanding, say so.",
            "If you have a deterministic classification algorithm.",
            "If you have a data set, you get only a single classifier.",
            "However, it is possible that multiple datasets give you the same classifier.",
            "So for example, if you have a majority classifier where you have a sample size of.",
            "And say two classes, there are N + 1 possible datasets.",
            "Since you can have all points belonging to one class to the other spectrum where all clear points belong to the other class.",
            "However, they are just two classifiers, because depending on whichever is more, you would basically classify in that particular class so."
        ],
        [
            "If you go with the space of classifiers, so now for this two by two table you basically get four terms, which is independent of N and as you have seen in the previous formulation.",
            "If you went over the space of datasets independent on the sample size, I mean the sample size.",
            "The data set size and the size of each probability is 2."
        ],
        [
            "So in in a general sense, if you had a general data set, you would have essentially carries 2M terms and the size of each probability would be of EM.",
            "Now this is a huge reduction, but still the number of terms is exponential in the input space."
        ],
        [
            "So now this takes us to our one of the central results in our paper now.",
            "The so the first probability and this is last probability depends on the underlying distribution.",
            "Only the middle term depends on the classification algorithm as well as the underlying distribution.",
            "Now these are conditionals given expert, just for readability.",
            "I'm not written those but and so as you can see you have two sums.",
            "So this is the summation overall possible in all inputs, which is M basically in our setting and this is over all classes which is K. So essentially you just have MK terms.",
            "Besides you can see that.",
            "This center probability has only one tab which is independent of the size of the input output space and the way we get this result is basically using linearity of expectation and going on the space of all classifiers to the space of all outputs, and then you get certain marginalization switch.",
            "Basically due to this kind of result.",
            "Now it's also important to notice that if you're given any arbitrary classification algorithm in the previous cases, you would have to sort of characterize its behavior or all possible inputs, while here you just have to look at the local behavior of the classifier essentially.",
            "See how it behaves on individual inputs and this can help in basically characterizing certain even complicated algorithms."
        ],
        [
            "So essentially.",
            "By so so, essentially, if you use the naive approach where you just go over the space of all possible datasets, you have so many, and if you use if you go through this procedure, you reduces to sort of linear in the input outside space, and it's important to note that we have not done any approximations here, so it's all the formulas are equivalent."
        ],
        [
            "And now this is, yeah, so this is basically the size of each term which is there in the probability.",
            "So that reduces to just to."
        ],
        [
            "So now essentially what we have is just these two probabilities in the moments which depend on the classification algorithm.",
            "So what about these probabilities?",
            "I mean, how efficiently can we compute each of them given any algorithm?"
        ],
        [
            "So now this is basically an example.",
            "As I said, I'll motivated through my base classifier.",
            "It's not import."
        ],
        [
            "And to one of the details of nitty gritty details of each of the terms.",
            "But basically what you have is.",
            "This is a two by two cell and two is just the number of data points in Class 2, and one is the number of data points in class 1C is C1 is the class.",
            "This is just input.",
            "This whole thing is an input space, so it's a 2 dimensional input space.",
            "Annex 1 one is just basically the marginal, so the number of points in X1C1.",
            "So this these two blocks.",
            "And so on so forth.",
            "And these terms are essentially the actual blocks S 1 one is just NX1Y1C1 and so on so forth.",
            "So if you have to compute even the probability of how it behaves on a single input, you have this kind of a formula which you have to compute, and so it's of course not feasible to directly compute such probabilities.",
            "So then how do we come?"
        ],
        [
            "Do them fast.",
            "So here.",
            "Is the 1st place where we start approximating OK?",
            "So we want some good approximations.",
            "So now as it turns out for that term which I showed in the probability, you can basically expand it in terms of the random variables of each of the cells, which is basically as it turns out, it's a polynomial.",
            "Now how does this help us and these such conditions arise even for decision trees and many other algorithms.",
            "So as I said, it's not just."
        ],
        [
            "So how does this help us?",
            "So the way it helps us is that say if I write the term which was there in the probability, that is basically what I've done is I had something greater than something.",
            "I've just brought the greater than Tom to one side, so you have a subtraction.",
            "So you basically want to find such a probability.",
            "What is the probability of greater than zero now?",
            "The use of this is that you have that discrete data set where N is fixed an you have some counts in each of those cells now, so basically lends to a multinomial distribution but doesn't have to be a multinomial irrespective of whichever is the underlying distribution.",
            "If you know the moment generating function."
        ],
        [
            "And use it to basically what you can do is.",
            "We know that if we find different different order partial partial derivatives of the moment generating function, you get essentially expressions for the moments of polynomials in the random variables, and so all of this can be basically precomputed and kept and stored.",
            "Whenever you."
        ],
        [
            "And So what our problem reduces to is essentially you want to find a CDF and you know the moments."
        ],
        [
            "So how can you do this?",
            "So there are multiple solutions, but this is I'll just discuss some of the preferred solutions, so you can basically frame it as a linear optimization problem.",
            "However, this is decent, but the number of variables that is which you have.",
            "Depends on the size of the domain, so which can be pretty huge.",
            "So the way you can basically approximate this is."
        ],
        [
            "Being the domain continuous and then finding the dual of."
        ],
        [
            "This relaxes the problem, but if you have as we've found, I mean reasonable number of moments, it's pretty.",
            "The bounds are pretty."
        ],
        [
            "And so essentially you have a convex kind of optimization problem, But you don't know the equation of the boundary, so you can't use like standard gradient descent to.",
            "Find the optimal point so."
        ],
        [
            "So these are some of the methods we tried and some of the methods which were developed.",
            "So as it turns out, the last three methods are the most promising, and so these are these two optimization methods and this will just speak about in a moment and you basically get really tight upper and lower bounds on the CDF.",
            "So what you're optimizing over is basically all possible CDF's.",
            "Where the CDs have those moments.",
            "Since you're just basically going to give a finite set of moments.",
            "And so the last thing is random sampling.",
            "So what we thought is I mean OK, you have all of this machinery, but why not just compute the moments directly, right?",
            "Using Monte Carlo.",
            "That's the simplest way of doing it.",
            "So what's bad about it?",
            "So what we did is."
        ],
        [
            "Be compared?",
            "The the fact where we computed the entire moments using Monte Carlo and then we compared it with, we derive the closed form formulas for computing the moments and we estimated only each of those single probabilities which I showed using Monte Carlo.",
            "So how does it compare in such a setting?",
            "Because it's an obvious comparison and as it turns out, when you increase N and remember here that N is not the sample size of the sample size of the space of all datasets.",
            "So if you increase in the space of datasets actually blows up.",
            "And so we found that.",
            "Actually, basically decomposing the moments into basically using the closed form formula and then just estimating the individual problem probability, say using Monte Carlo, turns out to be much more accurate for the same amount of computation then directly computing the moments.",
            "And this is basically because the parameter space is much much larger when you're computing the movements or the whole space.",
            "Then as compared to just over local points.",
            "And this we found.",
            "Of course this is not.",
            "Necessarily true always, but this is generally we have found this to be true for at least many of the algorithms which we characterize."
        ],
        [
            "So for example, for random decision trees, which we also we gave closed form formulas for using four different stopping criteria.",
            "So based on priority scarcity and also you have we have given closed form formulas for computing those moments and we estimated each of those probabilities using Monte Carlo.",
            "And then we compared say using the trying to estimate the entire moments using Monte Carlo and as it turns out, even for MC 10 is basically when we even did 10 times more computation from Monte Carlo, it was still nowhere close to.",
            "Actually, computing using these closed form formulas then we also compared to bramans bounds based on sender correlation and.",
            "It was much more accurate."
        ],
        [
            "Now, so that was for estimating the probability for the first moment.",
            "What about joint probabilities?",
            "So what basically this formula is is say so.",
            "As I said, for the discrete setting it's more difficult right?",
            "So you have a lattice of points and what you want to find is.",
            "You you cannot use the optimization techniques if you have a joint right?",
            "Because you just want one sort of polynomial which you would compute.",
            "So essentially what this does is it collapses joint probabilities to single probabilities.",
            "If you have, say, a lattice of points and you want to find over X&Y, and you want to find essentially said which is a function of X&Y and is a polynomial in X&Y since the partial derivatives of the moment generating function are polynomials, so this kind of a polynomial is actually.",
            "One to one correspondence, so if you find the probability of this random variable being greater than zero, it's equivalent to finding the original, and this is how the polynomial actually looks."
        ],
        [
            "So as I said, so using the optimization techniques is actually the best alternative since it's I mean there is no sort of approximation in terms, so there's no in accuracies in terms of computing the values and it's really fast.",
            "However, for like convenience one might estimate each of these individual probabilities using Monte Carlo directly to, I mean each of both of these methods work equally well."
        ],
        [
            "So essentially what we have done is we have reduced the number of terms and we have reduced the size of each term and we've turned from something which seemed like totally intractable to at least, which is sort of practical for medium scale problems.",
            "I wouldn't say LA."
        ],
        [
            "So now this is an example of how you would characterize that probability.",
            "For said decision trees.",
            "So the way I mean, how do you classify in a decision tree any individual input the way you classify is you basically go down a particular branch.",
            "Can you see if, say, for example here majority you could use some other more sophisticated classifier, but you basically see which is the most prevalent class and you classify in that particular class.",
            "So this is basically the kind of probability you would need to find an.",
            "As I've said we have characterized."
        ],
        [
            "Then for K nearest neighbor approach you would basically want to see what they came here.",
            "What is the probability of having certain Kate nearest neighbors with particular set of class labels an should confine."
        ],
        [
            "And then we can basically use."
        ],
        [
            "Then we have also derive certain relationship between the moments of the generalization error and moments of these error metrics.",
            "So if you."
        ],
        [
            "Can find the moments of the generalization error you can find in moments of these metrics, so this is just an example plot from our theoretical formulas.",
            "Of course this is just for one of the classifiers, but we have.",
            "For the classifiers that you have characterized, and as we can see that it is able to recreate some of the observed behavior generally seen in practice where you have sort of a dip at 10 to 24 hours and then performance goes."
        ],
        [
            "And so this is basically due to the covariance between the.",
            "There is often runs of cross validation, that is why the variance behaves in that way."
        ],
        [
            "And we were observing this for the Carol Gothams, which we characterize.",
            "We were able to give certain interesting sort of explanations for the behavior with.",
            "Wearing certain paramaters as correlation between the input output."
        ],
        [
            "Other things.",
            "So then you can also study so you can study serial.",
            "Data size convergence behavior of the various error metrics with respect to the generalization error, and so it's essentially like an exploratory tool.",
            "You can use.",
            "Use it to study."
        ],
        [
            "Different algorithms, different.",
            "So though we have taken some critical steps in reducing the complexity and given some and basically characterize certain algorithms and stuff, But as it still remains so, we need more.",
            "Scalable Solutions an it may be tedious to sort of figure out even though you have to figure out how classifier behaves on individual inputs, it still can be tedious to figure out for certain algorithms.",
            "And so.",
            "But, however, we feel that it's.",
            "Used as an exploratory tool as I said, and so it's other as an alternative line of study for classification algorithms in certain models, election measures, and so we feel that from this initial work, at least that it has some merit."
        ],
        [
            "Thank you baby."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is joint work with my advisor, Doctor Arlene Dobra, and what I'm going to talk about today is.",
                    "label": 0
                },
                {
                    "sent": "Methodology which we have recently introduced to study sort of classification algorithms and certain model selection measures an it's still in its infancy, but we have had some promising results with respect to certain algorithms in this, and so we would try.",
                    "label": 1
                },
                {
                    "sent": "I would like to discuss this with you all.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as we know, the problem of classification is only present in today's world.",
                    "label": 0
                },
                {
                    "sent": "There are a wide array or array of applications ranging from medicine to finance where classification methods are used.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, not only are the applications where it, but so are the apples or the actual algorithms, and there are.",
                    "label": 0
                },
                {
                    "sent": "The problem is basically that for different applications different algorithms work better.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I'm going to suggest today is an approach to study classification algorithms in the non asymptotic regime.",
                    "label": 1
                },
                {
                    "sent": "That is, for finite sample sizes.",
                    "label": 0
                },
                {
                    "sent": "So how it would behave, say on the real life sample size?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So consider this problem for the time being.",
                    "label": 0
                },
                {
                    "sent": "That is, say you have a distribution say given by Q.",
                    "label": 0
                },
                {
                    "sent": "And what you want to do is you want to answer the question how an algorithm behaves with respect to that distribution, say when trained over a sample size of some value in.",
                    "label": 1
                },
                {
                    "sent": "So I guess the natural solution one would come up with is you take a sample of size N, train your algorithm, produce a classifier, maybe sample some test sets, find the test error, do this multiple types, and basically report the average average error invariants and say that this is an estimate as to how my algorithm would behave.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, ideally, rather than the test error, we would probably want to know the generalization errors and rather than just sampling a few datasets and training your classifier and reporting the error, you would probably want to find the expected value over all possible datasets that you can get from their disk.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solution, so this is the kind of thing you would want to probably compute.",
                    "label": 0
                },
                {
                    "sent": "So here small Caesar classifier G is the generalization error of that classifier D of N is the space of all possible datasets that you can sample from this distribution Q an.",
                    "label": 1
                },
                {
                    "sent": "As you can see the generalization here.",
                    "label": 1
                },
                {
                    "sent": "Here is a random variable.",
                    "label": 0
                },
                {
                    "sent": "It's not a constant, since every classifier is random as it is produced by training a classification algorithm on the sample that you get, which is obviously random and so similarly the second moment is this moment.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now basically the problem of trying to study a classification algorithm with respect to our distribution can be reduced to this problem of say for example studying the first 2 moments of this accurately and efficiently.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the users of this, our first of all, you can study the behavior of.",
                    "label": 0
                },
                {
                    "sent": "So if you are able to find these moments and say you are able to plot these so for different values of North, you can study the behavior of the algorithm for finite sample size is another use of this is so.",
                    "label": 0
                },
                {
                    "sent": "For example, right now what people do is.",
                    "label": 0
                },
                {
                    "sent": "So say you have a data set and a person wants to say the algorithm designer wants to justify that his algorithm is good.",
                    "label": 0
                },
                {
                    "sent": "So what he does is it rains.",
                    "label": 0
                },
                {
                    "sent": "You're his algorithm on that particular data set and reports the error.",
                    "label": 0
                },
                {
                    "sent": "However, if you were, for example, to sort of build a distribution on this data set and then evaluate these moments, it would basically in a way instill more confidence in the practitioner since the results are overall possible datasets from that distribution.",
                    "label": 0
                },
                {
                    "sent": "So if the practitioners data set is safe from a similar source or a similar structure to the algorithm designers data set, you would feel more confident in using the person's algorithm then this.",
                    "label": 0
                },
                {
                    "sent": "Then the moment also can be used in verification of certain PAC Bayes bounds, because as I'll show you later on, we have also sort of derived relationships between the moments of the generalization error and moments of certain error metrics such as cross validation and rolled out error.",
                    "label": 0
                },
                {
                    "sent": "So you can even sort of see the behavior of those metrics.",
                    "label": 0
                },
                {
                    "sent": "So I mean using standard PAC Bayes bounds.",
                    "label": 1
                },
                {
                    "sent": "You can see how close they are with respect to the actual plots of these moments that you have.",
                    "label": 0
                },
                {
                    "sent": "Then you can, since if you have these moments you can study trends.",
                    "label": 0
                },
                {
                    "sent": "As I said, Nana symbiotic trends of the behavior of classification algorithms.",
                    "label": 1
                },
                {
                    "sent": "This can help you probably to gain insights into why and when and exactly how a classification algorithm behaves when you change certain para meters in the algorithm and the last application.",
                    "label": 0
                },
                {
                    "sent": "Basically, as I go on, you'll see what I mean by that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the bottom line is.",
                    "label": 0
                },
                {
                    "sent": "Studying the movements can help us basically study algorithms accurately and efficiently so.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rest of the talk is basically going to be focused on how.",
                    "label": 0
                },
                {
                    "sent": "Are you going to compute these moments with?",
                    "label": 0
                },
                {
                    "sent": "I mean, as I said accurately and efficiently.",
                    "label": 0
                },
                {
                    "sent": "So what I'll first discuss is there's some general strategies now.",
                    "label": 0
                },
                {
                    "sent": "These strategies are absolutely general.",
                    "label": 0
                },
                {
                    "sent": "The Naive Bayes classifier example just to motivate certain types of optimizations which we might need.",
                    "label": 1
                },
                {
                    "sent": "Then I will talk about certain relationships that I've done with the moments of the cross validation error and holdout error, and so on, and so as an example, I will show you I mean certain behaviors that we were able to see through our theoretical formulas.",
                    "label": 0
                },
                {
                    "sent": "And then I'll conclude and talk about future work and challenges that exist.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of work, so now note that I've basically written all the formulas with sums, but they're applicable continuous domain as well, and mostly there is more machinery to sort of compute integrals more efficiently than, say, finite sums and so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The thing is more sort of intuitive in that setting, so now this is our formal definition of generalization errors.",
                    "label": 0
                },
                {
                    "sent": "As I said, small Zita is the classifier Ellis, the loss function, and we assume the data is IID and this is the standard.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Definition so now if you were to compute moments directly, that is, say just using so analytically if you wanted to compute this is how you would compute, so you would see what is the probability of a particular data set, and then these data is basically the classifier which is obtained by training the algorithm on that particular data set.",
                    "label": 0
                },
                {
                    "sent": "And this is the form you would have so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Help say consider you have this data set.",
                    "label": 0
                },
                {
                    "sent": "OK, so since I said some so discrete data set so where you have same input vectors and K classes, here K is 2.",
                    "label": 1
                },
                {
                    "sent": "So you have two classes and say you have a sample size of North.",
                    "label": 0
                },
                {
                    "sent": "Now the counts in each of these cells is basically the number of data points that line X1Y1 and X1 white one so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So now as you can see if I just fix in there, basically each of these cells can take N possible values.",
                    "label": 0
                },
                {
                    "sent": "So the number of possible datasets you confirm if I just fix end with this kind of a setting is.",
                    "label": 0
                },
                {
                    "sent": "Andres, two MK minus one since the last time would I mean Umm K -- 1 free paramaters since they are in this table there are two M cells.",
                    "label": 0
                },
                {
                    "sent": "If you had classes you would have NK cells.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you were to just directly compute it, you would have essentially these many terms and the size of each probability would be MK if you go back to the table, you figure that.",
                    "label": 0
                },
                {
                    "sent": "Each of the probabilities choose basically to instantiate a data set.",
                    "label": 1
                },
                {
                    "sent": "You need these many terms, so this is definitely too many.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is absolutely impractical.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, so the first optimization I'll discuss is basically number of terms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization, so now to get an idea to get an intuition about this.",
                    "label": 0
                },
                {
                    "sent": "So what we first do is instead of going on the space of all possible datasets, we go over the space of all possible classifiers.",
                    "label": 1
                },
                {
                    "sent": "Now, how does this intuitively help?",
                    "label": 0
                },
                {
                    "sent": "Well, for example, consider deterministic classification algorithm or results are applicable to even randomized algorithms.",
                    "label": 0
                },
                {
                    "sent": "But just for understanding, say so.",
                    "label": 0
                },
                {
                    "sent": "If you have a deterministic classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "If you have a data set, you get only a single classifier.",
                    "label": 0
                },
                {
                    "sent": "However, it is possible that multiple datasets give you the same classifier.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have a majority classifier where you have a sample size of.",
                    "label": 0
                },
                {
                    "sent": "And say two classes, there are N + 1 possible datasets.",
                    "label": 0
                },
                {
                    "sent": "Since you can have all points belonging to one class to the other spectrum where all clear points belong to the other class.",
                    "label": 0
                },
                {
                    "sent": "However, they are just two classifiers, because depending on whichever is more, you would basically classify in that particular class so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you go with the space of classifiers, so now for this two by two table you basically get four terms, which is independent of N and as you have seen in the previous formulation.",
                    "label": 1
                },
                {
                    "sent": "If you went over the space of datasets independent on the sample size, I mean the sample size.",
                    "label": 0
                },
                {
                    "sent": "The data set size and the size of each probability is 2.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in in a general sense, if you had a general data set, you would have essentially carries 2M terms and the size of each probability would be of EM.",
                    "label": 0
                },
                {
                    "sent": "Now this is a huge reduction, but still the number of terms is exponential in the input space.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now this takes us to our one of the central results in our paper now.",
                    "label": 1
                },
                {
                    "sent": "The so the first probability and this is last probability depends on the underlying distribution.",
                    "label": 1
                },
                {
                    "sent": "Only the middle term depends on the classification algorithm as well as the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "Now these are conditionals given expert, just for readability.",
                    "label": 1
                },
                {
                    "sent": "I'm not written those but and so as you can see you have two sums.",
                    "label": 0
                },
                {
                    "sent": "So this is the summation overall possible in all inputs, which is M basically in our setting and this is over all classes which is K. So essentially you just have MK terms.",
                    "label": 0
                },
                {
                    "sent": "Besides you can see that.",
                    "label": 0
                },
                {
                    "sent": "This center probability has only one tab which is independent of the size of the input output space and the way we get this result is basically using linearity of expectation and going on the space of all classifiers to the space of all outputs, and then you get certain marginalization switch.",
                    "label": 0
                },
                {
                    "sent": "Basically due to this kind of result.",
                    "label": 0
                },
                {
                    "sent": "Now it's also important to notice that if you're given any arbitrary classification algorithm in the previous cases, you would have to sort of characterize its behavior or all possible inputs, while here you just have to look at the local behavior of the classifier essentially.",
                    "label": 0
                },
                {
                    "sent": "See how it behaves on individual inputs and this can help in basically characterizing certain even complicated algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially.",
                    "label": 0
                },
                {
                    "sent": "By so so, essentially, if you use the naive approach where you just go over the space of all possible datasets, you have so many, and if you use if you go through this procedure, you reduces to sort of linear in the input outside space, and it's important to note that we have not done any approximations here, so it's all the formulas are equivalent.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now this is, yeah, so this is basically the size of each term which is there in the probability.",
                    "label": 0
                },
                {
                    "sent": "So that reduces to just to.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now essentially what we have is just these two probabilities in the moments which depend on the classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what about these probabilities?",
                    "label": 0
                },
                {
                    "sent": "I mean, how efficiently can we compute each of them given any algorithm?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now this is basically an example.",
                    "label": 0
                },
                {
                    "sent": "As I said, I'll motivated through my base classifier.",
                    "label": 0
                },
                {
                    "sent": "It's not import.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And to one of the details of nitty gritty details of each of the terms.",
                    "label": 0
                },
                {
                    "sent": "But basically what you have is.",
                    "label": 0
                },
                {
                    "sent": "This is a two by two cell and two is just the number of data points in Class 2, and one is the number of data points in class 1C is C1 is the class.",
                    "label": 0
                },
                {
                    "sent": "This is just input.",
                    "label": 0
                },
                {
                    "sent": "This whole thing is an input space, so it's a 2 dimensional input space.",
                    "label": 0
                },
                {
                    "sent": "Annex 1 one is just basically the marginal, so the number of points in X1C1.",
                    "label": 0
                },
                {
                    "sent": "So this these two blocks.",
                    "label": 0
                },
                {
                    "sent": "And so on so forth.",
                    "label": 0
                },
                {
                    "sent": "And these terms are essentially the actual blocks S 1 one is just NX1Y1C1 and so on so forth.",
                    "label": 0
                },
                {
                    "sent": "So if you have to compute even the probability of how it behaves on a single input, you have this kind of a formula which you have to compute, and so it's of course not feasible to directly compute such probabilities.",
                    "label": 0
                },
                {
                    "sent": "So then how do we come?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do them fast.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "Is the 1st place where we start approximating OK?",
                    "label": 0
                },
                {
                    "sent": "So we want some good approximations.",
                    "label": 0
                },
                {
                    "sent": "So now as it turns out for that term which I showed in the probability, you can basically expand it in terms of the random variables of each of the cells, which is basically as it turns out, it's a polynomial.",
                    "label": 1
                },
                {
                    "sent": "Now how does this help us and these such conditions arise even for decision trees and many other algorithms.",
                    "label": 0
                },
                {
                    "sent": "So as I said, it's not just.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does this help us?",
                    "label": 1
                },
                {
                    "sent": "So the way it helps us is that say if I write the term which was there in the probability, that is basically what I've done is I had something greater than something.",
                    "label": 0
                },
                {
                    "sent": "I've just brought the greater than Tom to one side, so you have a subtraction.",
                    "label": 0
                },
                {
                    "sent": "So you basically want to find such a probability.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of greater than zero now?",
                    "label": 0
                },
                {
                    "sent": "The use of this is that you have that discrete data set where N is fixed an you have some counts in each of those cells now, so basically lends to a multinomial distribution but doesn't have to be a multinomial irrespective of whichever is the underlying distribution.",
                    "label": 1
                },
                {
                    "sent": "If you know the moment generating function.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And use it to basically what you can do is.",
                    "label": 0
                },
                {
                    "sent": "We know that if we find different different order partial partial derivatives of the moment generating function, you get essentially expressions for the moments of polynomials in the random variables, and so all of this can be basically precomputed and kept and stored.",
                    "label": 1
                },
                {
                    "sent": "Whenever you.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And So what our problem reduces to is essentially you want to find a CDF and you know the moments.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can you do this?",
                    "label": 0
                },
                {
                    "sent": "So there are multiple solutions, but this is I'll just discuss some of the preferred solutions, so you can basically frame it as a linear optimization problem.",
                    "label": 0
                },
                {
                    "sent": "However, this is decent, but the number of variables that is which you have.",
                    "label": 1
                },
                {
                    "sent": "Depends on the size of the domain, so which can be pretty huge.",
                    "label": 0
                },
                {
                    "sent": "So the way you can basically approximate this is.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Being the domain continuous and then finding the dual of.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This relaxes the problem, but if you have as we've found, I mean reasonable number of moments, it's pretty.",
                    "label": 0
                },
                {
                    "sent": "The bounds are pretty.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so essentially you have a convex kind of optimization problem, But you don't know the equation of the boundary, so you can't use like standard gradient descent to.",
                    "label": 0
                },
                {
                    "sent": "Find the optimal point so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are some of the methods we tried and some of the methods which were developed.",
                    "label": 0
                },
                {
                    "sent": "So as it turns out, the last three methods are the most promising, and so these are these two optimization methods and this will just speak about in a moment and you basically get really tight upper and lower bounds on the CDF.",
                    "label": 0
                },
                {
                    "sent": "So what you're optimizing over is basically all possible CDF's.",
                    "label": 0
                },
                {
                    "sent": "Where the CDs have those moments.",
                    "label": 0
                },
                {
                    "sent": "Since you're just basically going to give a finite set of moments.",
                    "label": 0
                },
                {
                    "sent": "And so the last thing is random sampling.",
                    "label": 0
                },
                {
                    "sent": "So what we thought is I mean OK, you have all of this machinery, but why not just compute the moments directly, right?",
                    "label": 0
                },
                {
                    "sent": "Using Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "That's the simplest way of doing it.",
                    "label": 0
                },
                {
                    "sent": "So what's bad about it?",
                    "label": 0
                },
                {
                    "sent": "So what we did is.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be compared?",
                    "label": 0
                },
                {
                    "sent": "The the fact where we computed the entire moments using Monte Carlo and then we compared it with, we derive the closed form formulas for computing the moments and we estimated only each of those single probabilities which I showed using Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "So how does it compare in such a setting?",
                    "label": 0
                },
                {
                    "sent": "Because it's an obvious comparison and as it turns out, when you increase N and remember here that N is not the sample size of the sample size of the space of all datasets.",
                    "label": 0
                },
                {
                    "sent": "So if you increase in the space of datasets actually blows up.",
                    "label": 0
                },
                {
                    "sent": "And so we found that.",
                    "label": 0
                },
                {
                    "sent": "Actually, basically decomposing the moments into basically using the closed form formula and then just estimating the individual problem probability, say using Monte Carlo, turns out to be much more accurate for the same amount of computation then directly computing the moments.",
                    "label": 0
                },
                {
                    "sent": "And this is basically because the parameter space is much much larger when you're computing the movements or the whole space.",
                    "label": 1
                },
                {
                    "sent": "Then as compared to just over local points.",
                    "label": 0
                },
                {
                    "sent": "And this we found.",
                    "label": 0
                },
                {
                    "sent": "Of course this is not.",
                    "label": 0
                },
                {
                    "sent": "Necessarily true always, but this is generally we have found this to be true for at least many of the algorithms which we characterize.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example, for random decision trees, which we also we gave closed form formulas for using four different stopping criteria.",
                    "label": 1
                },
                {
                    "sent": "So based on priority scarcity and also you have we have given closed form formulas for computing those moments and we estimated each of those probabilities using Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "And then we compared say using the trying to estimate the entire moments using Monte Carlo and as it turns out, even for MC 10 is basically when we even did 10 times more computation from Monte Carlo, it was still nowhere close to.",
                    "label": 1
                },
                {
                    "sent": "Actually, computing using these closed form formulas then we also compared to bramans bounds based on sender correlation and.",
                    "label": 0
                },
                {
                    "sent": "It was much more accurate.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, so that was for estimating the probability for the first moment.",
                    "label": 0
                },
                {
                    "sent": "What about joint probabilities?",
                    "label": 0
                },
                {
                    "sent": "So what basically this formula is is say so.",
                    "label": 0
                },
                {
                    "sent": "As I said, for the discrete setting it's more difficult right?",
                    "label": 0
                },
                {
                    "sent": "So you have a lattice of points and what you want to find is.",
                    "label": 0
                },
                {
                    "sent": "You you cannot use the optimization techniques if you have a joint right?",
                    "label": 0
                },
                {
                    "sent": "Because you just want one sort of polynomial which you would compute.",
                    "label": 0
                },
                {
                    "sent": "So essentially what this does is it collapses joint probabilities to single probabilities.",
                    "label": 0
                },
                {
                    "sent": "If you have, say, a lattice of points and you want to find over X&Y, and you want to find essentially said which is a function of X&Y and is a polynomial in X&Y since the partial derivatives of the moment generating function are polynomials, so this kind of a polynomial is actually.",
                    "label": 0
                },
                {
                    "sent": "One to one correspondence, so if you find the probability of this random variable being greater than zero, it's equivalent to finding the original, and this is how the polynomial actually looks.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I said, so using the optimization techniques is actually the best alternative since it's I mean there is no sort of approximation in terms, so there's no in accuracies in terms of computing the values and it's really fast.",
                    "label": 0
                },
                {
                    "sent": "However, for like convenience one might estimate each of these individual probabilities using Monte Carlo directly to, I mean each of both of these methods work equally well.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So essentially what we have done is we have reduced the number of terms and we have reduced the size of each term and we've turned from something which seemed like totally intractable to at least, which is sort of practical for medium scale problems.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't say LA.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now this is an example of how you would characterize that probability.",
                    "label": 0
                },
                {
                    "sent": "For said decision trees.",
                    "label": 0
                },
                {
                    "sent": "So the way I mean, how do you classify in a decision tree any individual input the way you classify is you basically go down a particular branch.",
                    "label": 0
                },
                {
                    "sent": "Can you see if, say, for example here majority you could use some other more sophisticated classifier, but you basically see which is the most prevalent class and you classify in that particular class.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the kind of probability you would need to find an.",
                    "label": 0
                },
                {
                    "sent": "As I've said we have characterized.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then for K nearest neighbor approach you would basically want to see what they came here.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of having certain Kate nearest neighbors with particular set of class labels an should confine.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we can basically use.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we have also derive certain relationship between the moments of the generalization error and moments of these error metrics.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can find the moments of the generalization error you can find in moments of these metrics, so this is just an example plot from our theoretical formulas.",
                    "label": 0
                },
                {
                    "sent": "Of course this is just for one of the classifiers, but we have.",
                    "label": 0
                },
                {
                    "sent": "For the classifiers that you have characterized, and as we can see that it is able to recreate some of the observed behavior generally seen in practice where you have sort of a dip at 10 to 24 hours and then performance goes.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so this is basically due to the covariance between the.",
                    "label": 0
                },
                {
                    "sent": "There is often runs of cross validation, that is why the variance behaves in that way.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we were observing this for the Carol Gothams, which we characterize.",
                    "label": 0
                },
                {
                    "sent": "We were able to give certain interesting sort of explanations for the behavior with.",
                    "label": 1
                },
                {
                    "sent": "Wearing certain paramaters as correlation between the input output.",
                    "label": 1
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other things.",
                    "label": 0
                },
                {
                    "sent": "So then you can also study so you can study serial.",
                    "label": 0
                },
                {
                    "sent": "Data size convergence behavior of the various error metrics with respect to the generalization error, and so it's essentially like an exploratory tool.",
                    "label": 0
                },
                {
                    "sent": "You can use.",
                    "label": 0
                },
                {
                    "sent": "Use it to study.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different algorithms, different.",
                    "label": 0
                },
                {
                    "sent": "So though we have taken some critical steps in reducing the complexity and given some and basically characterize certain algorithms and stuff, But as it still remains so, we need more.",
                    "label": 0
                },
                {
                    "sent": "Scalable Solutions an it may be tedious to sort of figure out even though you have to figure out how classifier behaves on individual inputs, it still can be tedious to figure out for certain algorithms.",
                    "label": 1
                },
                {
                    "sent": "And so.",
                    "label": 1
                },
                {
                    "sent": "But, however, we feel that it's.",
                    "label": 0
                },
                {
                    "sent": "Used as an exploratory tool as I said, and so it's other as an alternative line of study for classification algorithms in certain models, election measures, and so we feel that from this initial work, at least that it has some merit.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you baby.",
                    "label": 0
                }
            ]
        }
    }
}