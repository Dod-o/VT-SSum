{
    "id": "bhh7du5vffdma5etwid67rhpl3rubcx6",
    "title": "LWI-SVD: Low-rank, Windowed, Incremental Singular Value Decompositions on Time-Evolving Data Sets",
    "info": {
        "author": [
            "Xilun Chen, School of Computing, Informatics and Decision Systems Engineering, Arizona State University"
        ],
        "published": "Oct. 7, 2014",
        "recorded": "August 2014",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Knowledge Extraction"
        ]
    },
    "url": "http://videolectures.net/kdd2014_chen_lwi_svd/",
    "segmentation": [
        [
            "My name is Sheila and I'm very glad to share our research work.",
            "LWS video recommend, oh, incremental singular Value decomposition's on time involving data set.",
            "This is a joint work with my advisor case.",
            "There should Canon.",
            "We're from Arizona State University.",
            "This work is funded by NSF grants."
        ],
        [
            "So here today, this is what I'm going to talk about.",
            "First, give some background and then our briefly introduce our proposed algorithm.",
            "Then I will share some experiment results and then finally I will conclude the paper."
        ],
        [
            "So now introduction."
        ],
        [
            "So I think most of you are familiar with the SVD, so the singular decomposition takes matrix X, so usually it is an object feature matrix and decompose into three factors USB.",
            "So EU is the left singular value vectors.",
            "So usually describe the objects and we is the right singular.",
            "Vectors usually describe the features and as is the diagonal matrix having the singular values sorted.",
            "In an increasing order describing the strength of the latent semantics, so SVD is widely used in the latent semantic analysis, and then naturally it gives a code clustering of the objects and features.",
            "So."
        ],
        [
            "Our problem we are considering the situation where the data is time evolving, so the interest of the object and the features could change simultaneously.",
            "So assuming we have a set up tables forming this object feature matrix at MTI and then there could be 2 possibilities at a time step next time stamps so there could be.",
            "Knew objects and new features arriving or.",
            "Some of the objects and the features could expire, so these two situations leave.",
            "Lead to our."
        ],
        [
            "Incremental SVD problem.",
            "So where we have our latest medics at the timestamp, RTI and then we want to maintain this latent semantics at the following time stamps.",
            "So from this figure we can see the yellow one was first decompose into three vectors and then there are following updates.",
            "So the green circles and naturally well we have two ways of doing it so we can take the updated matrix and directly decompose.",
            "To get the new factors, obviously this is very time consuming or we can get the update and reuse the previous vectors to incrementally maintain the new vectors.",
            "So there are."
        ],
        [
            "Already some existing works on the subspace maintenance and such as the brand in 2006, but this existing work suffer from three main drawbacks.",
            "So the first is they often operate on only one single dimension.",
            "So which means either the objects are evolving or D features are evolving and the second drawback is the updates are usually considered as a vector stream, so the algorithm picks.",
            "One updated documents or feature in each update cycles.",
            "So if there are thousands of new documents or new features, the algorithm needs to do a loop for that many times.",
            "And these two drawbacks relate to the third one which is high online computation cost."
        ],
        [
            "So our challenges are how to maintain the SVD factors efficiently, because obviously repeat doing it will be time consuming.",
            "And what if both dimensions evolve also?"
        ],
        [
            "How can we maintain the factors accurately?",
            "Because if we want to speed up the online process, we are.",
            "We should use some approximations and the nature of operation approximation will lead to errors."
        ],
        [
            "So in our work we will build up the incremental maintenance of the SVD factors by leveraging a pivoted QR decomposition to obtain a second layer approximation, and we can also aggregate several incoming updates, including the either object or both of the object and features insertion or deletions."
        ],
        [
            "Also, we have an intelligent error control scheme which we build which we proposed matrix reservoir sampling method to prevent the accumulation of the errors."
        ],
        [
            "Uh, so now here is what we proposed."
        ],
        [
            "So before that, let me give a brief introduction to a very basic incremental SVD algorithm.",
            "So it is proposed by brand in 2006 and so given an initial feature object feature matrix X, which is already decomposed into USV and the AB transpose form, the updating matrix Delta so we can rewrite the updated matrix, the SVD of the updated matrix X prime.",
            "In the following form, but notice that in this process there could be 2 very time consuming steps.",
            "So which is there are two QR decompositions and there is another activity inside of it.",
            "And this matrix could potentially have larger size than the original matrix X.",
            "So in order to speed this up."
        ],
        [
            "So we can.",
            "First we speed up the curity compositions, recall the nuclear decompositions are in that form, and we can set the A.",
            "So maybe is the updating matrix and we can set A to the identity so we can avoid the first QR because the car off a identity with the robot is still that and the second one because we and we transport have large set of columns that are.",
            "But not to each other.",
            "So this will I minus that will result in the large zero blocks, so we can use the block matrix decomposition to save some online time cost."
        ],
        [
            "And with respect to that, as we did the inside of the process because that K has a very special structure.",
            "So if you can recall that K is the S paddings with some zero blocks an adding with the second term.",
            "So which in fact if we visualize it, it will be like this.",
            "So it is a error light.",
            "So the first upper left block will be the singular values from the previous vectors.",
            "And there the last rows and columns were corresponding to the newly arrived objects and features so."
        ],
        [
            "In order to build the SVD of this, we use the.",
            "Puberty QR decomposition by sampling the largest R rows and columns of this K so naturally it is obvious that the largest rows and columns will come from the first few columns or rows which containing the largest singular values or the last rows or columns which contain a lot of non zero entries.",
            "Note that in this process there will be still as we did, but it is applied to a very small size matrix.",
            "So which is R by R?",
            "So it is very efficient."
        ],
        [
            "And since we are doing a two layer approximation, so it will result in reconstruction errors.",
            "So there could be 2 possibilities.",
            "So why is the accumulated error?",
            "Because each update cycle we are doing approximation or there could be a sudden burst of error considering that the new arrived objects or features can possibly be highly different from what we have before, so.",
            "It."
        ],
        [
            "Could be the basis vectors could be different from what we have before, so from this figure the blue line shows the error accumulates without restarting the SVD process.",
            "So restarting means we do a full decomposition before we continue to the next incremental steps and the green line shows with restarting the errors could be pushing down.",
            "So how do we refresh it?",
            "So there could be.",
            "Obviously we can do that periodically, selecting iterations we refresh, but we could do that more intelligently to prevent the sudden burst of the arrows."
        ],
        [
            "So here is what we proposed LWSD with restores.",
            "So this is the process we we have seen in the previous slides.",
            "This is without resource, so the factors are incrementally maintained.",
            "Then we propose the Matrix reservoir sampling method, which the first matrix was reshaped into vector, and we built the initial reservoirs.",
            "So we keep the index of the entries in the original matrix and.",
            "Then whenever there is an update, we update the rest of us so each entries will get a an equal probability of getting into the reservoir or if entry was removed then the next added entry will get into the reservoir as if the removed one has never been into the reservoir.",
            "So I will omit the details here.",
            "You can refer to the paper so we only get the we only check the arrows of.",
            "Of the entries in the reservoir, so we don't have to reconstruct the original matrix too.",
            "Find out if the errors are too high, so in this way we can speed up the error check process because if the original matrix is large enough, reconstructing this refactors could be time consuming.",
            "So we check the sum of the arrows of the entries in the reservoir, so if it reaches a certain threshold then it means we should restart the SVD process in order to keep the arrows down.",
            "Then there is another update.",
            "Then we update the rest of us and then we check if we should restart or not.",
            "And the updates will keep going."
        ],
        [
            "As in, I will share some experiment results."
        ],
        [
            "So we implemented the algorithm in MATLAB and we compare some state of our algorithm such as the brands and the spirit and we tried different running parameters of our algorithms, such as the window size or target rank and the rest of our size.",
            "And we also tried with different restarting."
        ],
        [
            "Games so this figure shows the accuracy and the running time.",
            "So as we can see, the spirit is extremely fast, but the error overhead relative to the optimal top bar SPD is extremely high, but our algorithm still has a 50%.",
            "I'm going with negligible error overhead.",
            "And we also tried our algorithms on real data traces we obtained from thedick.com.",
            "So we use the First Avenger articles and first hand are most frequent terms to make up the initial matrix, and we consider 50 iterations of insertion and deletions."
        ],
        [
            "So in this figure we can see the without restarting, the errors could be as high as 4% relative to the optimal top or SVD.",
            "But if we are using restarting the errors could be kept below 1%.",
            "Anne."
        ],
        [
            "And with respect to the running time.",
            "So as we can see, we have the Lwi family algorithm has like 50% time gain.",
            "If we're running this on a real data set and the without restarting has a slightly faster because.",
            "Reconstruct the reservoir so it is a slightly faster."
        ],
        [
            "So in summary, the elderly SVD can speed up the incremental maintenance of the SVD factors by leveraging a two layer of exclamation, and we can also aggregate the incoming updates in the batch mode, and we also have the Matrix reservoir sampling model to keep checking the arrows each iterations so we can intelligently restart SVD in order to keep the accuracy high."
        ],
        [
            "Thank you any questions."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My name is Sheila and I'm very glad to share our research work.",
                    "label": 0
                },
                {
                    "sent": "LWS video recommend, oh, incremental singular Value decomposition's on time involving data set.",
                    "label": 0
                },
                {
                    "sent": "This is a joint work with my advisor case.",
                    "label": 0
                },
                {
                    "sent": "There should Canon.",
                    "label": 0
                },
                {
                    "sent": "We're from Arizona State University.",
                    "label": 0
                },
                {
                    "sent": "This work is funded by NSF grants.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here today, this is what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "First, give some background and then our briefly introduce our proposed algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then I will share some experiment results and then finally I will conclude the paper.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now introduction.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think most of you are familiar with the SVD, so the singular decomposition takes matrix X, so usually it is an object feature matrix and decompose into three factors USB.",
                    "label": 0
                },
                {
                    "sent": "So EU is the left singular value vectors.",
                    "label": 1
                },
                {
                    "sent": "So usually describe the objects and we is the right singular.",
                    "label": 1
                },
                {
                    "sent": "Vectors usually describe the features and as is the diagonal matrix having the singular values sorted.",
                    "label": 0
                },
                {
                    "sent": "In an increasing order describing the strength of the latent semantics, so SVD is widely used in the latent semantic analysis, and then naturally it gives a code clustering of the objects and features.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our problem we are considering the situation where the data is time evolving, so the interest of the object and the features could change simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So assuming we have a set up tables forming this object feature matrix at MTI and then there could be 2 possibilities at a time step next time stamps so there could be.",
                    "label": 0
                },
                {
                    "sent": "Knew objects and new features arriving or.",
                    "label": 0
                },
                {
                    "sent": "Some of the objects and the features could expire, so these two situations leave.",
                    "label": 0
                },
                {
                    "sent": "Lead to our.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Incremental SVD problem.",
                    "label": 0
                },
                {
                    "sent": "So where we have our latest medics at the timestamp, RTI and then we want to maintain this latent semantics at the following time stamps.",
                    "label": 1
                },
                {
                    "sent": "So from this figure we can see the yellow one was first decompose into three vectors and then there are following updates.",
                    "label": 0
                },
                {
                    "sent": "So the green circles and naturally well we have two ways of doing it so we can take the updated matrix and directly decompose.",
                    "label": 0
                },
                {
                    "sent": "To get the new factors, obviously this is very time consuming or we can get the update and reuse the previous vectors to incrementally maintain the new vectors.",
                    "label": 0
                },
                {
                    "sent": "So there are.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Already some existing works on the subspace maintenance and such as the brand in 2006, but this existing work suffer from three main drawbacks.",
                    "label": 0
                },
                {
                    "sent": "So the first is they often operate on only one single dimension.",
                    "label": 1
                },
                {
                    "sent": "So which means either the objects are evolving or D features are evolving and the second drawback is the updates are usually considered as a vector stream, so the algorithm picks.",
                    "label": 0
                },
                {
                    "sent": "One updated documents or feature in each update cycles.",
                    "label": 0
                },
                {
                    "sent": "So if there are thousands of new documents or new features, the algorithm needs to do a loop for that many times.",
                    "label": 0
                },
                {
                    "sent": "And these two drawbacks relate to the third one which is high online computation cost.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our challenges are how to maintain the SVD factors efficiently, because obviously repeat doing it will be time consuming.",
                    "label": 0
                },
                {
                    "sent": "And what if both dimensions evolve also?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How can we maintain the factors accurately?",
                    "label": 0
                },
                {
                    "sent": "Because if we want to speed up the online process, we are.",
                    "label": 0
                },
                {
                    "sent": "We should use some approximations and the nature of operation approximation will lead to errors.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in our work we will build up the incremental maintenance of the SVD factors by leveraging a pivoted QR decomposition to obtain a second layer approximation, and we can also aggregate several incoming updates, including the either object or both of the object and features insertion or deletions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we have an intelligent error control scheme which we build which we proposed matrix reservoir sampling method to prevent the accumulation of the errors.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Uh, so now here is what we proposed.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before that, let me give a brief introduction to a very basic incremental SVD algorithm.",
                    "label": 0
                },
                {
                    "sent": "So it is proposed by brand in 2006 and so given an initial feature object feature matrix X, which is already decomposed into USV and the AB transpose form, the updating matrix Delta so we can rewrite the updated matrix, the SVD of the updated matrix X prime.",
                    "label": 1
                },
                {
                    "sent": "In the following form, but notice that in this process there could be 2 very time consuming steps.",
                    "label": 0
                },
                {
                    "sent": "So which is there are two QR decompositions and there is another activity inside of it.",
                    "label": 0
                },
                {
                    "sent": "And this matrix could potentially have larger size than the original matrix X.",
                    "label": 0
                },
                {
                    "sent": "So in order to speed this up.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "First we speed up the curity compositions, recall the nuclear decompositions are in that form, and we can set the A.",
                    "label": 0
                },
                {
                    "sent": "So maybe is the updating matrix and we can set A to the identity so we can avoid the first QR because the car off a identity with the robot is still that and the second one because we and we transport have large set of columns that are.",
                    "label": 1
                },
                {
                    "sent": "But not to each other.",
                    "label": 0
                },
                {
                    "sent": "So this will I minus that will result in the large zero blocks, so we can use the block matrix decomposition to save some online time cost.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And with respect to that, as we did the inside of the process because that K has a very special structure.",
                    "label": 0
                },
                {
                    "sent": "So if you can recall that K is the S paddings with some zero blocks an adding with the second term.",
                    "label": 0
                },
                {
                    "sent": "So which in fact if we visualize it, it will be like this.",
                    "label": 0
                },
                {
                    "sent": "So it is a error light.",
                    "label": 0
                },
                {
                    "sent": "So the first upper left block will be the singular values from the previous vectors.",
                    "label": 0
                },
                {
                    "sent": "And there the last rows and columns were corresponding to the newly arrived objects and features so.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to build the SVD of this, we use the.",
                    "label": 0
                },
                {
                    "sent": "Puberty QR decomposition by sampling the largest R rows and columns of this K so naturally it is obvious that the largest rows and columns will come from the first few columns or rows which containing the largest singular values or the last rows or columns which contain a lot of non zero entries.",
                    "label": 1
                },
                {
                    "sent": "Note that in this process there will be still as we did, but it is applied to a very small size matrix.",
                    "label": 0
                },
                {
                    "sent": "So which is R by R?",
                    "label": 0
                },
                {
                    "sent": "So it is very efficient.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And since we are doing a two layer approximation, so it will result in reconstruction errors.",
                    "label": 1
                },
                {
                    "sent": "So there could be 2 possibilities.",
                    "label": 0
                },
                {
                    "sent": "So why is the accumulated error?",
                    "label": 1
                },
                {
                    "sent": "Because each update cycle we are doing approximation or there could be a sudden burst of error considering that the new arrived objects or features can possibly be highly different from what we have before, so.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Could be the basis vectors could be different from what we have before, so from this figure the blue line shows the error accumulates without restarting the SVD process.",
                    "label": 0
                },
                {
                    "sent": "So restarting means we do a full decomposition before we continue to the next incremental steps and the green line shows with restarting the errors could be pushing down.",
                    "label": 0
                },
                {
                    "sent": "So how do we refresh it?",
                    "label": 0
                },
                {
                    "sent": "So there could be.",
                    "label": 0
                },
                {
                    "sent": "Obviously we can do that periodically, selecting iterations we refresh, but we could do that more intelligently to prevent the sudden burst of the arrows.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here is what we proposed LWSD with restores.",
                    "label": 0
                },
                {
                    "sent": "So this is the process we we have seen in the previous slides.",
                    "label": 0
                },
                {
                    "sent": "This is without resource, so the factors are incrementally maintained.",
                    "label": 0
                },
                {
                    "sent": "Then we propose the Matrix reservoir sampling method, which the first matrix was reshaped into vector, and we built the initial reservoirs.",
                    "label": 0
                },
                {
                    "sent": "So we keep the index of the entries in the original matrix and.",
                    "label": 0
                },
                {
                    "sent": "Then whenever there is an update, we update the rest of us so each entries will get a an equal probability of getting into the reservoir or if entry was removed then the next added entry will get into the reservoir as if the removed one has never been into the reservoir.",
                    "label": 0
                },
                {
                    "sent": "So I will omit the details here.",
                    "label": 0
                },
                {
                    "sent": "You can refer to the paper so we only get the we only check the arrows of.",
                    "label": 0
                },
                {
                    "sent": "Of the entries in the reservoir, so we don't have to reconstruct the original matrix too.",
                    "label": 0
                },
                {
                    "sent": "Find out if the errors are too high, so in this way we can speed up the error check process because if the original matrix is large enough, reconstructing this refactors could be time consuming.",
                    "label": 0
                },
                {
                    "sent": "So we check the sum of the arrows of the entries in the reservoir, so if it reaches a certain threshold then it means we should restart the SVD process in order to keep the arrows down.",
                    "label": 0
                },
                {
                    "sent": "Then there is another update.",
                    "label": 0
                },
                {
                    "sent": "Then we update the rest of us and then we check if we should restart or not.",
                    "label": 0
                },
                {
                    "sent": "And the updates will keep going.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As in, I will share some experiment results.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we implemented the algorithm in MATLAB and we compare some state of our algorithm such as the brands and the spirit and we tried different running parameters of our algorithms, such as the window size or target rank and the rest of our size.",
                    "label": 0
                },
                {
                    "sent": "And we also tried with different restarting.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Games so this figure shows the accuracy and the running time.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, the spirit is extremely fast, but the error overhead relative to the optimal top bar SPD is extremely high, but our algorithm still has a 50%.",
                    "label": 0
                },
                {
                    "sent": "I'm going with negligible error overhead.",
                    "label": 0
                },
                {
                    "sent": "And we also tried our algorithms on real data traces we obtained from thedick.com.",
                    "label": 0
                },
                {
                    "sent": "So we use the First Avenger articles and first hand are most frequent terms to make up the initial matrix, and we consider 50 iterations of insertion and deletions.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this figure we can see the without restarting, the errors could be as high as 4% relative to the optimal top or SVD.",
                    "label": 0
                },
                {
                    "sent": "But if we are using restarting the errors could be kept below 1%.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And with respect to the running time.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, we have the Lwi family algorithm has like 50% time gain.",
                    "label": 0
                },
                {
                    "sent": "If we're running this on a real data set and the without restarting has a slightly faster because.",
                    "label": 0
                },
                {
                    "sent": "Reconstruct the reservoir so it is a slightly faster.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in summary, the elderly SVD can speed up the incremental maintenance of the SVD factors by leveraging a two layer of exclamation, and we can also aggregate the incoming updates in the batch mode, and we also have the Matrix reservoir sampling model to keep checking the arrows each iterations so we can intelligently restart SVD in order to keep the accuracy high.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you any questions.",
                    "label": 0
                }
            ]
        }
    }
}