{
    "id": "crmzcc3xgkfmp5k5ddfe6utq2y5y5wb5",
    "title": "Consensus of Ambiguity: Theory of Active Learning for Biomedical Image Analysis Applications",
    "info": {
        "author": [
            "Scott Doyle, Laboratory for Computational Imaging and Bioinformatics, Rutgers, The State University of New Jersey"
        ],
        "published": "Oct. 14, 2010",
        "recorded": "September 2010",
        "category": [
            "Top->Computer Science->Bioinformatics",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Computer Vision->Computational Photography"
        ]
    },
    "url": "http://videolectures.net/prib2010_doyle_cata/",
    "segmentation": [
        [
            "So I'm going to be giving the last talk and my work is on a consensus extension to active learning training, which is a supervised classification training technique and my name is Scott Doyle.",
            "By the way, I'm from Rutgers University in New Jersey, so yes."
        ],
        [
            "So here's an outline of my talk, and there's a lot to get through, but I'd like to focus on classification, specifically supervised class."
        ],
        [
            "Training training paradigms.",
            "So in your supervised classification setup, you typically have the following problem.",
            "You have a data set of samples, each of which belongs to some semantic label, and they're defined by a set of feature values, and you want to build a supervised classifier, which is an algorithm that can predict the class labels of each of these points.",
            "So you start off by randomly selecting some of these points, and these become your randomly sampled training set, and you can pass these points off to an expert who denotes whether the samples belong to Class A or Class B in the two class case, and then you can produce an algorithm.",
            "Which can predict the class label of a sample."
        ],
        [
            "Now typically each sample is considered an observation of that samples class, and although this isn't true 100% of the time because you can overtrain, there's a problem where with random sampling or learning, more samples are typically better.",
            "So in this case if we have two training sets and we produce two different classifiers, we can predict that classifier two is probably going to do better, and the reason is that there's a more complete class model represented by this training set versus this training set.",
            "Now, the problem with this is that training samples are often very difficult to obtain, and I can illustrate this with an example from our lab."
        ],
        [
            "I work typically in image analysis, so this is a prostate tissue biopsy sample which was taken from a cancer patient and this is a digital image of what would normally be looked at under a slide by a microscope and apologist.",
            "Would be looking for our regions of interest that contain cancer growth, so we've been able to do in the lab is actually generate a supervised classifier that can detect in a pixel wise fashion regions that are highly likely to contain cancer.",
            "But because this is a supervised classifier, we need to use labeled training data in order to do this.",
            "Now, if we want to get an accurate classifier, we need training that is correctly labeled.",
            "That represents the class information that we're looking for and can discriminate between the target class and the non target class."
        ],
        [
            "And the reason that this is a problem is that expert medical knowledge is required in order to generate this training.",
            "Typically you need to sit a pathologist down and actually trace out regions that contain cancer growth in these very large images that can be up to two 3 gigabytes in size.",
            "Sometimes it's very tedious and time consuming to generate these sort of contours that we need for training.",
            "What this means is that each image represents a great deal of effort and investment, so Luckily there's an."
        ],
        [
            "Give to random learning which is active learning and active learning.",
            "You can selectively choose only informative samples for training and in this case informative mean samples that are most likely to increase classifier performance.",
            "So in other words, in this case we have our data set as before and we can run an active learning algorithm to identify informative points versus uninformative points.",
            "And then we can do our sampling within that only the informative region.",
            "We can then go ahead and build our training set as before, but now each one of these points is guaranteed to be informative so our classifier can be accurate with a lower number of."
        ],
        [
            "Apples.",
            "So the question is, how do we find these informative annotation samples?",
            "Well, there's something called the concept of sample ambiguity, and this is just an example where we have a 2 dimensional feature field and we have some training data that we can plot out.",
            "We can identify clusters that these class labels refer to, and then we can plot our unlabeled data on top of that and the idea here is that we want to selectively annotate samples that do not fall within either of these two clusters, because those are going to be the most."
        ],
        [
            "Informative, so the idea here is that the more ambiguous or the more difficult to classify a sample is, the more likely it is to be informative and therefore should be."
        ],
        [
            "Elected for annotation.",
            "So now if you have very fast overview, I'll talk about extending active learning, which is what we're bringing to."
        ],
        [
            "The table, so there's a number of different ways that you can measure sample ambiguity.",
            "The way that I just illustrated is based on support vector machines, where you can calculate the distance to a decision hyperplane and selectively annotate only samples that fall within some margin."
        ],
        [
            "That hyperplane under the ways using Bayesian likelihoods, which is based on the likelihood of class membership.",
            "So you can build your probability distribution functions and then selectively annotate samples that have feature values corresponding to the maximum bezar and these are considered to be."
        ],
        [
            "Or ambiguous another way is using a query by committee approach which is based on the disagreement among weak bagged classifiers.",
            "So if you have your unlabeled data here you can run them through a set of weak classifiers and the idea is that you're looking for classifier for samples whose classification has a confidence around .5.",
            "Because these are going to be the samples for which most of the."
        ],
        [
            "The classifiers disagree.",
            "So what we're bringing to the table is actually combining these methods, so each of these methods that I just described and there are others.",
            "But these are some of the most popular.",
            "Each of these methods uses one description of ambiguity apiece, So what we want to do is exploit the variance among these methods to generate a consensus algorithm that's going to perform better, and this is a common concept and classification where you're looking for the consensus among the samples class.",
            "But what we're looking for is consensus among ambiguity of the samples, so we call this the consensus of ambiguity."
        ],
        [
            "Approach the advantage of using this versus individual active learning algorithms is as follows.",
            "In random learning, all your unlabeled samples are considered eligible for training because you don't know which one."
        ],
        [
            "You're going to be informed."
        ],
        [
            "In the active learning case, a subset of these samples is available for training.",
            "So now you can only annotate within the regions that are colored and white here."
        ],
        [
            "With consensus of ambiguity, you're actually combining different actively."
        ],
        [
            "Any algorithms so that you can generate a set of eligible samples that's actually lower?",
            "So now your field that you can sample from is smaller versus individual actively."
        ],
        [
            "Algorithms?",
            "So let me get into some of the theory, just to set up how we're going."
        ],
        [
            "Go about doing this so there's two specific properties that we want to examine with respect to consensus of ambiguity.",
            "The first is that by using multiple active learning algorithms, we can reduce the number of ambiguous samples that you have to annotate versus a single active learning algorithm, and additional algorithms are going to increase the benefits of using this system, and in order to do this, we need 3 necessary components.",
            "We need a general definition of what an ambiguous sample is.",
            "We need to quantitatively describe the consensus among different algorithms, which is something we call a consensus ratio.",
            "And we want to identify strongly ambiguous samples which are the samples that the consensus algorithm are going to ID."
        ],
        [
            "Before us, so again we have our data set as before.",
            "Each sample within the data set has a class label associated with it called Y, and we want to build our supervised classifier which can predict one of these class labels and we build our supervised classifier using a labeled training set and the goal of the training algorithm is to build our training set from the unlabeled samples contained in X and samples in this case are chosen according to a training function which we're going to call fee.",
            "So using these tools we can come up."
        ],
        [
            "With our first definition, a sample is considered to be ambiguous.",
            "If the value of Phi falls between these two thresholds.",
            "Amb so visually this is an example of what the output of fee would look like for a single sample.",
            "In this case you have values going from zero to 1 where values closer to one are considered highly confident.",
            "In Class A values close to 0 or highly confident in Class B and the values in the middle are intermediate confidence and samples with these values constitute are eligible samples and this is defined by these two thresholds and obviously this is for the two classes."
        ],
        [
            "Consists of ambiguity, employs multiple training algorithms, so now you don't just have fee, you have free 1, two and solve so forth and each one of those is going to yield."
        ],
        [
            "An eligible sample set.",
            "So now using this we can define our consensus ratio where given us a number of nonempty sets of ambiguous samples, the consensus ratio is defined as such where U is equal to the to the cardinality of the intersection between those sample sets and V is the cardinality of the Union.",
            "So again, we can describe this visually.",
            "We have our data set as we have before."
        ],
        [
            "Or an we operate on it using two different active learning algorithms or training algorithms?"
        ],
        [
            "So this gives us two eligible sample sets S."
        ],
        [
            "Wanting us to the Union, but we."
        ],
        [
            "M is represented in yellow and the cardinality."
        ],
        [
            "That is, you and or sorry that was the intersection and the Union of the two is in blue and the cardinality of that is V. So our consensus ratio is you divided by V. So if we wanted to describe this intuitively, if the two algorithms are overlapping exactly, then use."
        ],
        [
            "Equal to V&R is equal to 1 in this case."
        ],
        [
            "If they are completely independent then U is equal to 0 and R is equal to 0.",
            "So what this means is that low ratios represent a greater benefit from using the consensus scheme because each algorithm has greater variance and if they have a high ratio, the algorithms are performing the same, so there's less of a benefit."
        ],
        [
            "So we actually calculated this ratio for one of the datasets that I'll describe in a minute, and we did this for a number of iterations of active learning, where the increasing iterations means that our training set is actually growing in size, and what we find is that the consensus ratio actually plateaus at about 0.2.",
            "So that means that there's relatively little consensus among the three active learning algorithms that were trying, and that motivates the use of the consensus."
        ],
        [
            "Search for our data.",
            "So the last definition is that a sample will be considered strongly ambiguous if it's considered ambiguous by all of the training functions fee.",
            "So in this case again we have our data set that's operate."
        ],
        [
            "Did on by three different training functions."
        ],
        [
            "To give us three different eligible sample sets, the overlay."
        ],
        [
            "Between these represent the samples that are considered ambiguous by all three of them."
        ],
        [
            "Any algorithms and this represents the eligible sample set that we can annotate from before."
        ],
        [
            "So our proposition is that as the number of algorithms being combined increases, the consensus ratio that we defined will either remain the same or will decrease, and there's a proof of this in the paper, but basically the analogy is to receive or a filtering system as you add more layers of filtering, fewer samples are actually going to get through and be considered ambiguous.",
            "And keep in mind that smaller consensus ratio means that there's a greater motivation for using this algorithm.",
            "So the idea is that as we're adding more algorithms, the benefit that we get from using this technology is."
        ],
        [
            "Is increasing so now I'd like to go into some of the experimental results that we."
        ],
        [
            "Have for this algorithm in order to evaluate the quality of the training set, we build probabilistic boosting tree classifier and we test it on an independent testing set.",
            "We use two medical image analysis databases for this study.",
            "The first is the prostate cancer detection from histopathology that I showed earlier and the 2nd is a breast cancer grading data set which I'll go into in a minute and we combine the three different training algorithms that I described earlier.",
            "The query by committee approach.",
            "The Bayesian likelihood approach and the support vector machine."
        ],
        [
            "Approach so our first experiment is for prostate histopathology.",
            "We have a number of biopsy samples that I showed previously.",
            "We put a 30 by 30 pixel grid on top of the biopsy samples and we extract a number of texture features from each of those are lies.",
            "These texture features we've found in previous work to actually be very good at discriminating between cancer and non cancer and we are trying to discriminate between non cancer samples which are red and cancer samples which are black and we have over 12,000 ROI.",
            "In our data set that we're trying to classify."
        ],
        [
            "So here are the actual classification results.",
            "It's a little messy, but that's because we're putting a lot of things.",
            "The two things to take away from this are that random learning typically performs less, not as well as the active learning algorithms for both accuracy and area under the receiver operating characteristic curve, the black line represents the accuracy of the performance when we're using the entire unlabeled training set.",
            "So what we see is that in both cases, active learning is performing better than random learning, and that the active learning algorithms are performing about the same."
        ],
        [
            "But what we do see is that when we compare the eligible sample set size, this consensus of ambiguity system has fewer eligible samples versus the other three.",
            "So the idea here is that we're sampling from a much smaller so."
        ],
        [
            "Of samples.",
            "The second experiment is with a breast cancer histopathology grading set.",
            "Here we have regions of interest of homogeneous tissue, and we're trying to discriminate between low grades and high grades of cancer, and this is based on the Bloom Richardson grading scheme.",
            "We use a graph based set of features to describe the nuclear arrangement of that issue.",
            "Because we've shown that this can do well in discriminating between these two sample set."
        ],
        [
            "And we find the same thing as before, where the random learning does not perform as well in any in any case, for the versus the active learning, and that the active learning is performing about the same."
        ],
        [
            "But we do see again that consists of ambiguity, can greatly reduce the number of samples that you have to look at for annotation, so this is the benefit of using."
        ],
        [
            "This system so with."
        ],
        [
            "So I can conclude consensus of ambiguity uses multiple active learning algorithms to reduce the set of informative samples, which makes annotation easier.",
            "They're using additional methods, can increase the benefit of consensus of ambiguity if the consensus ratio decreases, and this training set is generalizable to any supervised classification problem where your data are costly or difficult to annotate, your target class is very complex, and random learning is infeasible, and multiple active learning algorithms can be leveraged."
        ],
        [
            "Final tanias Lee so.",
            "Question have you tried different cabinets for this?",
            "Different.",
            "We usually try a number of different kernels because we've gotten good results with RBF kernels and different polynomial kernels, but generally we will go with the with the RBF because for our datasets we don't see too much of a difference in the classification accuracy, so we basically stick with that.",
            "It will be different this place.",
            "If they have a result.",
            "If the sequence does know because we compute the eligible sample sets independently, so and then when we combine them where basically saying that as long as all three methods agree, then that sample is picked.",
            "There's another way of doing it where you want to say maybe if two out of the three agree, then the sample is considered ambiguous.",
            "Then the sequence might matter because the first 2 that pick it will will end up.",
            "But so yes.",
            "Iterations effectively.",
            "How many examples do ads?",
            "So that's a good question.",
            "We actually played around with a number of different step sizes because what happened originally was we in fact the results that I've shown are using step sizes of.",
            "I think 5 samples per iteration for each class.",
            "So the training set is actually growing pretty quickly, and what happens is at that point the different methods are performing about the same because the training set size gets big enough for all the methods to perform pretty well.",
            "We've done between 2:00 and 10:00, and.",
            "Depending on what we use, the graphs basically look a little bit different, but the accuracies are about the same.",
            "If you were to normalize the number of training sites.",
            "OK, so I guess with that that's the end of the session.",
            "So thank you for hanging out and I guess we'll take it away."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to be giving the last talk and my work is on a consensus extension to active learning training, which is a supervised classification training technique and my name is Scott Doyle.",
                    "label": 0
                },
                {
                    "sent": "By the way, I'm from Rutgers University in New Jersey, so yes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an outline of my talk, and there's a lot to get through, but I'd like to focus on classification, specifically supervised class.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Training training paradigms.",
                    "label": 0
                },
                {
                    "sent": "So in your supervised classification setup, you typically have the following problem.",
                    "label": 1
                },
                {
                    "sent": "You have a data set of samples, each of which belongs to some semantic label, and they're defined by a set of feature values, and you want to build a supervised classifier, which is an algorithm that can predict the class labels of each of these points.",
                    "label": 0
                },
                {
                    "sent": "So you start off by randomly selecting some of these points, and these become your randomly sampled training set, and you can pass these points off to an expert who denotes whether the samples belong to Class A or Class B in the two class case, and then you can produce an algorithm.",
                    "label": 0
                },
                {
                    "sent": "Which can predict the class label of a sample.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now typically each sample is considered an observation of that samples class, and although this isn't true 100% of the time because you can overtrain, there's a problem where with random sampling or learning, more samples are typically better.",
                    "label": 1
                },
                {
                    "sent": "So in this case if we have two training sets and we produce two different classifiers, we can predict that classifier two is probably going to do better, and the reason is that there's a more complete class model represented by this training set versus this training set.",
                    "label": 0
                },
                {
                    "sent": "Now, the problem with this is that training samples are often very difficult to obtain, and I can illustrate this with an example from our lab.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I work typically in image analysis, so this is a prostate tissue biopsy sample which was taken from a cancer patient and this is a digital image of what would normally be looked at under a slide by a microscope and apologist.",
                    "label": 0
                },
                {
                    "sent": "Would be looking for our regions of interest that contain cancer growth, so we've been able to do in the lab is actually generate a supervised classifier that can detect in a pixel wise fashion regions that are highly likely to contain cancer.",
                    "label": 0
                },
                {
                    "sent": "But because this is a supervised classifier, we need to use labeled training data in order to do this.",
                    "label": 1
                },
                {
                    "sent": "Now, if we want to get an accurate classifier, we need training that is correctly labeled.",
                    "label": 1
                },
                {
                    "sent": "That represents the class information that we're looking for and can discriminate between the target class and the non target class.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the reason that this is a problem is that expert medical knowledge is required in order to generate this training.",
                    "label": 1
                },
                {
                    "sent": "Typically you need to sit a pathologist down and actually trace out regions that contain cancer growth in these very large images that can be up to two 3 gigabytes in size.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's very tedious and time consuming to generate these sort of contours that we need for training.",
                    "label": 0
                },
                {
                    "sent": "What this means is that each image represents a great deal of effort and investment, so Luckily there's an.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Give to random learning which is active learning and active learning.",
                    "label": 0
                },
                {
                    "sent": "You can selectively choose only informative samples for training and in this case informative mean samples that are most likely to increase classifier performance.",
                    "label": 1
                },
                {
                    "sent": "So in other words, in this case we have our data set as before and we can run an active learning algorithm to identify informative points versus uninformative points.",
                    "label": 0
                },
                {
                    "sent": "And then we can do our sampling within that only the informative region.",
                    "label": 0
                },
                {
                    "sent": "We can then go ahead and build our training set as before, but now each one of these points is guaranteed to be informative so our classifier can be accurate with a lower number of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apples.",
                    "label": 0
                },
                {
                    "sent": "So the question is, how do we find these informative annotation samples?",
                    "label": 1
                },
                {
                    "sent": "Well, there's something called the concept of sample ambiguity, and this is just an example where we have a 2 dimensional feature field and we have some training data that we can plot out.",
                    "label": 0
                },
                {
                    "sent": "We can identify clusters that these class labels refer to, and then we can plot our unlabeled data on top of that and the idea here is that we want to selectively annotate samples that do not fall within either of these two clusters, because those are going to be the most.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Informative, so the idea here is that the more ambiguous or the more difficult to classify a sample is, the more likely it is to be informative and therefore should be.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elected for annotation.",
                    "label": 0
                },
                {
                    "sent": "So now if you have very fast overview, I'll talk about extending active learning, which is what we're bringing to.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The table, so there's a number of different ways that you can measure sample ambiguity.",
                    "label": 0
                },
                {
                    "sent": "The way that I just illustrated is based on support vector machines, where you can calculate the distance to a decision hyperplane and selectively annotate only samples that fall within some margin.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That hyperplane under the ways using Bayesian likelihoods, which is based on the likelihood of class membership.",
                    "label": 0
                },
                {
                    "sent": "So you can build your probability distribution functions and then selectively annotate samples that have feature values corresponding to the maximum bezar and these are considered to be.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or ambiguous another way is using a query by committee approach which is based on the disagreement among weak bagged classifiers.",
                    "label": 1
                },
                {
                    "sent": "So if you have your unlabeled data here you can run them through a set of weak classifiers and the idea is that you're looking for classifier for samples whose classification has a confidence around .5.",
                    "label": 0
                },
                {
                    "sent": "Because these are going to be the samples for which most of the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The classifiers disagree.",
                    "label": 0
                },
                {
                    "sent": "So what we're bringing to the table is actually combining these methods, so each of these methods that I just described and there are others.",
                    "label": 0
                },
                {
                    "sent": "But these are some of the most popular.",
                    "label": 0
                },
                {
                    "sent": "Each of these methods uses one description of ambiguity apiece, So what we want to do is exploit the variance among these methods to generate a consensus algorithm that's going to perform better, and this is a common concept and classification where you're looking for the consensus among the samples class.",
                    "label": 1
                },
                {
                    "sent": "But what we're looking for is consensus among ambiguity of the samples, so we call this the consensus of ambiguity.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approach the advantage of using this versus individual active learning algorithms is as follows.",
                    "label": 0
                },
                {
                    "sent": "In random learning, all your unlabeled samples are considered eligible for training because you don't know which one.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're going to be informed.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the active learning case, a subset of these samples is available for training.",
                    "label": 0
                },
                {
                    "sent": "So now you can only annotate within the regions that are colored and white here.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With consensus of ambiguity, you're actually combining different actively.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any algorithms so that you can generate a set of eligible samples that's actually lower?",
                    "label": 0
                },
                {
                    "sent": "So now your field that you can sample from is smaller versus individual actively.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Algorithms?",
                    "label": 0
                },
                {
                    "sent": "So let me get into some of the theory, just to set up how we're going.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go about doing this so there's two specific properties that we want to examine with respect to consensus of ambiguity.",
                    "label": 0
                },
                {
                    "sent": "The first is that by using multiple active learning algorithms, we can reduce the number of ambiguous samples that you have to annotate versus a single active learning algorithm, and additional algorithms are going to increase the benefits of using this system, and in order to do this, we need 3 necessary components.",
                    "label": 0
                },
                {
                    "sent": "We need a general definition of what an ambiguous sample is.",
                    "label": 1
                },
                {
                    "sent": "We need to quantitatively describe the consensus among different algorithms, which is something we call a consensus ratio.",
                    "label": 0
                },
                {
                    "sent": "And we want to identify strongly ambiguous samples which are the samples that the consensus algorithm are going to ID.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before us, so again we have our data set as before.",
                    "label": 0
                },
                {
                    "sent": "Each sample within the data set has a class label associated with it called Y, and we want to build our supervised classifier which can predict one of these class labels and we build our supervised classifier using a labeled training set and the goal of the training algorithm is to build our training set from the unlabeled samples contained in X and samples in this case are chosen according to a training function which we're going to call fee.",
                    "label": 1
                },
                {
                    "sent": "So using these tools we can come up.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With our first definition, a sample is considered to be ambiguous.",
                    "label": 1
                },
                {
                    "sent": "If the value of Phi falls between these two thresholds.",
                    "label": 0
                },
                {
                    "sent": "Amb so visually this is an example of what the output of fee would look like for a single sample.",
                    "label": 0
                },
                {
                    "sent": "In this case you have values going from zero to 1 where values closer to one are considered highly confident.",
                    "label": 0
                },
                {
                    "sent": "In Class A values close to 0 or highly confident in Class B and the values in the middle are intermediate confidence and samples with these values constitute are eligible samples and this is defined by these two thresholds and obviously this is for the two classes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Consists of ambiguity, employs multiple training algorithms, so now you don't just have fee, you have free 1, two and solve so forth and each one of those is going to yield.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An eligible sample set.",
                    "label": 0
                },
                {
                    "sent": "So now using this we can define our consensus ratio where given us a number of nonempty sets of ambiguous samples, the consensus ratio is defined as such where U is equal to the to the cardinality of the intersection between those sample sets and V is the cardinality of the Union.",
                    "label": 1
                },
                {
                    "sent": "So again, we can describe this visually.",
                    "label": 0
                },
                {
                    "sent": "We have our data set as we have before.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or an we operate on it using two different active learning algorithms or training algorithms?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this gives us two eligible sample sets S.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wanting us to the Union, but we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "M is represented in yellow and the cardinality.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is, you and or sorry that was the intersection and the Union of the two is in blue and the cardinality of that is V. So our consensus ratio is you divided by V. So if we wanted to describe this intuitively, if the two algorithms are overlapping exactly, then use.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equal to V&R is equal to 1 in this case.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If they are completely independent then U is equal to 0 and R is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "So what this means is that low ratios represent a greater benefit from using the consensus scheme because each algorithm has greater variance and if they have a high ratio, the algorithms are performing the same, so there's less of a benefit.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we actually calculated this ratio for one of the datasets that I'll describe in a minute, and we did this for a number of iterations of active learning, where the increasing iterations means that our training set is actually growing in size, and what we find is that the consensus ratio actually plateaus at about 0.2.",
                    "label": 0
                },
                {
                    "sent": "So that means that there's relatively little consensus among the three active learning algorithms that were trying, and that motivates the use of the consensus.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Search for our data.",
                    "label": 0
                },
                {
                    "sent": "So the last definition is that a sample will be considered strongly ambiguous if it's considered ambiguous by all of the training functions fee.",
                    "label": 1
                },
                {
                    "sent": "So in this case again we have our data set that's operate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did on by three different training functions.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give us three different eligible sample sets, the overlay.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between these represent the samples that are considered ambiguous by all three of them.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any algorithms and this represents the eligible sample set that we can annotate from before.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our proposition is that as the number of algorithms being combined increases, the consensus ratio that we defined will either remain the same or will decrease, and there's a proof of this in the paper, but basically the analogy is to receive or a filtering system as you add more layers of filtering, fewer samples are actually going to get through and be considered ambiguous.",
                    "label": 1
                },
                {
                    "sent": "And keep in mind that smaller consensus ratio means that there's a greater motivation for using this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that as we're adding more algorithms, the benefit that we get from using this technology is.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is increasing so now I'd like to go into some of the experimental results that we.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have for this algorithm in order to evaluate the quality of the training set, we build probabilistic boosting tree classifier and we test it on an independent testing set.",
                    "label": 1
                },
                {
                    "sent": "We use two medical image analysis databases for this study.",
                    "label": 1
                },
                {
                    "sent": "The first is the prostate cancer detection from histopathology that I showed earlier and the 2nd is a breast cancer grading data set which I'll go into in a minute and we combine the three different training algorithms that I described earlier.",
                    "label": 1
                },
                {
                    "sent": "The query by committee approach.",
                    "label": 0
                },
                {
                    "sent": "The Bayesian likelihood approach and the support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach so our first experiment is for prostate histopathology.",
                    "label": 1
                },
                {
                    "sent": "We have a number of biopsy samples that I showed previously.",
                    "label": 0
                },
                {
                    "sent": "We put a 30 by 30 pixel grid on top of the biopsy samples and we extract a number of texture features from each of those are lies.",
                    "label": 1
                },
                {
                    "sent": "These texture features we've found in previous work to actually be very good at discriminating between cancer and non cancer and we are trying to discriminate between non cancer samples which are red and cancer samples which are black and we have over 12,000 ROI.",
                    "label": 0
                },
                {
                    "sent": "In our data set that we're trying to classify.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the actual classification results.",
                    "label": 0
                },
                {
                    "sent": "It's a little messy, but that's because we're putting a lot of things.",
                    "label": 0
                },
                {
                    "sent": "The two things to take away from this are that random learning typically performs less, not as well as the active learning algorithms for both accuracy and area under the receiver operating characteristic curve, the black line represents the accuracy of the performance when we're using the entire unlabeled training set.",
                    "label": 0
                },
                {
                    "sent": "So what we see is that in both cases, active learning is performing better than random learning, and that the active learning algorithms are performing about the same.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what we do see is that when we compare the eligible sample set size, this consensus of ambiguity system has fewer eligible samples versus the other three.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we're sampling from a much smaller so.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of samples.",
                    "label": 0
                },
                {
                    "sent": "The second experiment is with a breast cancer histopathology grading set.",
                    "label": 1
                },
                {
                    "sent": "Here we have regions of interest of homogeneous tissue, and we're trying to discriminate between low grades and high grades of cancer, and this is based on the Bloom Richardson grading scheme.",
                    "label": 0
                },
                {
                    "sent": "We use a graph based set of features to describe the nuclear arrangement of that issue.",
                    "label": 1
                },
                {
                    "sent": "Because we've shown that this can do well in discriminating between these two sample set.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we find the same thing as before, where the random learning does not perform as well in any in any case, for the versus the active learning, and that the active learning is performing about the same.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we do see again that consists of ambiguity, can greatly reduce the number of samples that you have to look at for annotation, so this is the benefit of using.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This system so with.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I can conclude consensus of ambiguity uses multiple active learning algorithms to reduce the set of informative samples, which makes annotation easier.",
                    "label": 0
                },
                {
                    "sent": "They're using additional methods, can increase the benefit of consensus of ambiguity if the consensus ratio decreases, and this training set is generalizable to any supervised classification problem where your data are costly or difficult to annotate, your target class is very complex, and random learning is infeasible, and multiple active learning algorithms can be leveraged.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Final tanias Lee so.",
                    "label": 0
                },
                {
                    "sent": "Question have you tried different cabinets for this?",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "We usually try a number of different kernels because we've gotten good results with RBF kernels and different polynomial kernels, but generally we will go with the with the RBF because for our datasets we don't see too much of a difference in the classification accuracy, so we basically stick with that.",
                    "label": 0
                },
                {
                    "sent": "It will be different this place.",
                    "label": 0
                },
                {
                    "sent": "If they have a result.",
                    "label": 0
                },
                {
                    "sent": "If the sequence does know because we compute the eligible sample sets independently, so and then when we combine them where basically saying that as long as all three methods agree, then that sample is picked.",
                    "label": 0
                },
                {
                    "sent": "There's another way of doing it where you want to say maybe if two out of the three agree, then the sample is considered ambiguous.",
                    "label": 0
                },
                {
                    "sent": "Then the sequence might matter because the first 2 that pick it will will end up.",
                    "label": 0
                },
                {
                    "sent": "But so yes.",
                    "label": 0
                },
                {
                    "sent": "Iterations effectively.",
                    "label": 0
                },
                {
                    "sent": "How many examples do ads?",
                    "label": 0
                },
                {
                    "sent": "So that's a good question.",
                    "label": 0
                },
                {
                    "sent": "We actually played around with a number of different step sizes because what happened originally was we in fact the results that I've shown are using step sizes of.",
                    "label": 0
                },
                {
                    "sent": "I think 5 samples per iteration for each class.",
                    "label": 0
                },
                {
                    "sent": "So the training set is actually growing pretty quickly, and what happens is at that point the different methods are performing about the same because the training set size gets big enough for all the methods to perform pretty well.",
                    "label": 0
                },
                {
                    "sent": "We've done between 2:00 and 10:00, and.",
                    "label": 0
                },
                {
                    "sent": "Depending on what we use, the graphs basically look a little bit different, but the accuracies are about the same.",
                    "label": 0
                },
                {
                    "sent": "If you were to normalize the number of training sites.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess with that that's the end of the session.",
                    "label": 0
                },
                {
                    "sent": "So thank you for hanging out and I guess we'll take it away.",
                    "label": 0
                }
            ]
        }
    }
}