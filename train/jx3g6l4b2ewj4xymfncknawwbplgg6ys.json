{
    "id": "jx3g6l4b2ewj4xymfncknawwbplgg6ys",
    "title": "Optimal Computational Trade-Off of Inexact Proximal Methods",
    "info": {
        "author": [
            "Pierre Machart, INRIA Rennes"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_machart_methods/",
    "segmentation": [
        [
            "So before I get started I have alot of things to say in that presentation, so please take a deep breath and then I'll get started.",
            "OK so I will be talking about optimal computational tradeoff of inexact proximal methods, and this is a yeah, so NPM, I'm coming from the leaf Lobato, Edna formatic fundamental of masses in France and this is joint work with someone and look up at the Sky with here."
        ],
        [
            "OK, so first a bit of an introduction which has been actually partly covered by Charlotte's Web."
        ],
        [
            "It's this morning about the tradeoffs of learning."
        ],
        [
            "So very general notation and stuff about our setting, so we'll be considering supervised statistical learning and where the data are considered as realizations of pairs of random variables X&Y with the underlying but unknown distribution D."
        ],
        [
            "The goal usually is to learn the predictor.",
            "A good predictor, which is a mapping from the input SpaceX to the output space of target space Y."
        ],
        [
            "And usually the goodness of a prediction is measured through a loss function that basically measures the discrepancy between the predictions H of X and access associated target."
        ],
        [
            "Why and the goodness of a predictor is measured through risk function, which simply is the expectation over the unknown distribution D of this loss?"
        ],
        [
            "An yeah, what we would like to do is to retrieve these absolute best predictor which would be the function, the mapping H star that minimizes that that true risk.",
            "Of course, as you may know, it's not possible and."
        ],
        [
            "Because actually what will happen in practice is that the learning algorithms will give you functions that are denoted by HN~ which have an access error that can be decomposed in this way.",
            "So this decomposition is due to Limbo two and Lydia Bousquet and shrouds talked about it this morning and it can be so.",
            "Basically this access error is the difference between the risk of the true best predictor and the one that your algorithm will give you, and it can be decomposed in three terms, the 1st, that is the approximation term.",
            "That is due to the fact that you are not actually finding the best.",
            "The best mapping from X to Y, but only the best among a subset which is called the hypothesis class that you're exploring the estimation error.",
            "The second term, which is due to the fact that you are actually you can't directly minimize the expectation of the loss function, but usually what you can do is only minimize an estimator of this risk.",
            "So this induces a another type of access error, and finally the third.",
            "The first term that has been.",
            "Long overlooked, which is the optimization error which is due to the fact that usually the problem that you want to solve don't have a closed form solution.",
            "So what you have to do is find a numerical solution with some some numerical error.",
            "So this will induce another error which is called the optimization error."
        ],
        [
            "OK, so now you have two types of problems we could say so for a smoke small scale problems, which basically means that the limiting resource would be the number of data and well, you can do pretty much what you want and you can expect that the you will be able to solve your problems with such such a high precision that you can actually neglect the optimization error.",
            "So in that case what you can do for instance, is."
        ],
        [
            "Pay attention to what this guy has to say so the guy on the left is Vladimir Vapnik and he has addressed some years ago.",
            "Now the tradeoff that exists between the approximation and estimation, of course, is not the only one to have done that, but if you do, the thing that is advising, well, you know that you will be dealing with this with this tradeoff pretty well, and then so basically it will tell you what kind of problem you had to solve.",
            "If you want to do things properly."
        ],
        [
            "And the guy on the on the right is Yuri Nesterov.",
            "I could have put other pictures of course, but isn't more the optimization guy.",
            "And yeah, given a problem, this guy I've done a lot of algorithms that are very good theoretical properties and that will ensure you that you will efficiently solves, solves all sorts of, well, a lot of different problems.",
            "So but"
        ],
        [
            "The situation is a little more complicated when you have large large scale problems.",
            "Sorry, because now the limiting resource is not the number of data but the amount of time that you have to actually run your algorithm.",
            "So what what happened is that you have so much data that you actually can't afford to address the.",
            "Tradeoff between approximation and estimation.",
            "In such a principled way and still expect to be able to solve your problem with the arbitrarily high precision.",
            "So that's pretty much what this guy."
        ],
        [
            "Number two as said and what he's saying in his paper.",
            "That which is called the tradeoffs of large scale learning.",
            "And yeah, OK, so I won't be directly addressing this multi trade off, but there are some interesting questions that very easily arise from this."
        ],
        [
            "This for instance, yeah, as the runtime is the limiting resource.",
            "Well, computer computational efficiency matters when you're when you're when you're making algorithms learning algorithms, so we would like to find ways to assess this computational efficiency then.",
            "11 conclusion is that when you have this multi trade off what you may want to do is to balance the free terms, which means that you really want to optimize your to solve your problem with limited precision.",
            "So in that situation are the rates of convergence still relevant while they are very relevant when you want to solve problem with arbitrarily high precision, but when you want to deal with limited precision, it's not so clear that rates of convergence are still so so important and finally so.",
            "Can we directly take into account the runtime of our algorithm optimization algorithm?",
            "And that's the thing we have tried to do in our work, so we have done that in a very specific setting which is called."
        ],
        [
            "The inexact proximal methods, so I will give a few few notion."
        ],
        [
            "About that, so the context is the nonsmooth convex optimization.",
            "When we have problems where you want to minimize the function F of X, which is called composite, which is a sum of two terms which are both convex but only the first one G is smooth and H is not necessarily."
        ],
        [
            "News.",
            "And a general framework around type of algorithm that you can use to solve this problem is called proximal gradient methods and not share their generalization of.",
            "The well known gradient methods.",
            "Typically what you will do with given some iterates SRX of X of S, K -- 1.",
            "Sorry you will apply gradient step with respect to the smooth parts and then you will apply the proximity operator which is defined as this and roughly well what you want to do is solve this optimization problem.",
            "The proximity point will be the X that will.",
            "That will be the minimizer of this dysfunction, which this part corresponds to the quadratic approximation of this part.",
            "Plus your non smooth spot."
        ],
        [
            "OK, so now there are two types of situations depending on the on the function H that you have first, the function that are called usually refer to as simple in approximal sense.",
            "For instance, at one regularization term or indicators on the convex set an induced into scenarios.",
            "You can compute the proximity point in closed form, so that's not really a problem, but in many situations, for instance when you have TV regularization, terms or norms inducing structures.",
            "City like Mix mix normal things like that you have no clothes forms or for the computer proximity points.",
            "So what you have to."
        ],
        [
            "Do is to compute the numerical solution that will induce some approximation.",
            "So for now, let's assume that you have a black box that at iteration you will have to evaluate approximately point and suppose that you have a black box that gives you.",
            "That gives you a point.",
            "That is, that had some approximation that is defined in this way.",
            "Basically, it tells you that the proximity map the function that's trying to minimize in the inner problems.",
            "Evaluated at the point that you will get will be no further than epsilon K from the real minimizer.",
            "OK, so people have been using these methods a lot and I've tried to to study."
        ],
        [
            "And for a long time, sorry for long time we have had necessary condition, for example, necessary and sufficient conditions that ensure that your algorithm will converge to the minimizer of your problem.",
            "Things like that.",
            "And one thing I want to add is that here the epsilon, epsilon case, or actually nice can be seen as optimization hyperparameters, because when you're dealing with your problem, you will have a way to set the level of approximation that you want to have."
        ],
        [
            "OK, so just to make things clear here, that's how that's what inexact proximal algorithm look like.",
            "So we have what we call 2 nested loop.",
            "So the outer loop correspond to the basic proxamol method, and at each step you will have to use your black box to find an approximation of your proximity point, and you have to you have to have it run.",
            "Enough time so that you can get sufficient precision epsilon."
        ],
        [
            "I OK.",
            "So more recently, actually last year at Nips Mack Smith and his colleagues came with a very nice bound for this inexact proximal method.",
            "So it looks like that.",
            "So you see that it will depend, of course, on the difference F of X K -- F of X star.",
            "So excited, the minimizer of the problem will be as follows, and what you can see that of course depends on K, and it will depend on the all the epsilon eyes, although the inner precisions of your of your problems and.",
            "So what they showed you that you can actually well first, if you set epsilon to zero, you find the usual bound that you have with.",
            "Exact approximate method.",
            "And actually you can retrieve within the exact case.",
            "You can retrieve the optimal rates so 1 / K as long as the epsilon case converge at least as fast as 1 / K to 2 plus some positive Delta.",
            "So basically it means that you can have the best rates you can expect can be obtained if you impose."
        ],
        [
            "Very strict conditions of the epsilon K. This is very strict because in the first steps of your iteration it's not so much of a not a big deal, but as you go on in the optimization procedure you will have to solve the problems more and more precisely.",
            "So in a way, OK, this will let you have optimal rates, but each iteration each outer iteration will have a start having a huge cost because you will have to put a lot of energy for."
        ],
        [
            "Having no problems, so OK, Now let's remember our first motivations.",
            "We wanted to assess the computational efficiency of our algorithms.",
            "We wanted to deal with limited solving our problems with limited precision.",
            "So maybe the convergence rate are not so relevant.",
            "And finally we wanted to try to directly take into account that the runtime of our algorithms, and that's what we have."
        ],
        [
            "In that context, so."
        ],
        [
            "OK, so the global cost of those methods can be assessed as follow, so let's let us denote by ELISA number of inner iteration that you are using.",
            "So let's suppose that you're using an iterative procedure to compute approximately point Li will be the number of inner iteration that you will have at the outer iteration index by I and let us denote by C in the cost of an inner iteration and see out the cost of an outer iteration.",
            "The overall cost of your procedure.",
            "Will be as follows, so seeing times the sum of all the allies for one for I going from 1 to K + K times out.",
            "So this is the overall cost of."
        ],
        [
            "Procedure.",
            "No, the fastest strategy to solve a problem with the precision of throw can be achieved if you can solve this problem.",
            "So if you minimize over K an ALDI Elisa number of inner and outer iterations, you minimize the cost of your overall procedure under the constraint that your solution will have a precision that is at least row given precision row, then that's pretty cool, because you show that you have had you have done the the fastest strategy possible.",
            "So this problem is a little difficult because we don't exactly know how F of XK minus average store behaves.",
            "But what we do have our bounds on that, such as the one I introduced earlier."
        ],
        [
            "So now there is just one thing missing because the as you may remember the bounds depending on the epsilon eyes and now our cost depends on the allies, the number of iterations, but that of course related.",
            "For instance, if the black box you're using is an approach is an iterative algorithm with sublinear convergence rate.",
            "Then you can say that the epsilon I can be written as follows, so it will be some constant A over Eli to the Alpha, so Alpha will be the convergence rate of your order.",
            "Algorithm that you use in your inner iterations in your inner loops.",
            "Typically it will be 1/2 one."
        ],
        [
            "2.",
            "And now you can plug that back in the bound of Mark Schmitz and you will get this result, which now is still abound on the error, but which now depends on the number of inner and outer iterations, and you know that the the precision of your solution will be smaller than this bound."
        ],
        [
            "OK.",
            "So now here is our main result.",
            "OK, now what we have done is trying to we have solved the problems of minimizing the cost of the globe of the overall procedure under the constraint that the boundary installed I just introduced will be smaller than the given precision that you wanted row.",
            "So, and this can be so, this problem can be solved and here is the solution.",
            "The optimal number of inner and outer iterations.",
            "It looks a little ugly, but not that much.",
            "Actually what you have to note is that the Allies star actually do not depend on I.",
            "This means that you will have to use constant number of inner iterations along your optimization procedure.",
            "So that means that you will actually solve your problems within constant precision, which is extremely different from the strategy I told you earlier that.",
            "Allows to achieve the optimal rates of convergence because actually.",
            "OK, so you have to do that using a constant number of this constant number of iteration and to run the algorithm for this number of auto iteration.",
            "OK, there is no closed form solution for that, but this is a 1 dimensional problem and this function is monotonically decreasing then increasing so you can find exactly the integral that minimizes this problem."
        ],
        [
            "OK, so as I said, the number of iterations is constant and the president."
        ],
        [
            "The constant too, and actually the dis optimal numbers of iteration will be such that the bound we are optimizing the will exactly value row.",
            "After K start, number of auto iterations and that is the only guarantee that we have.",
            "Actually, if you have a look at the at the bound, if you optimize further and you keep this number of constant iteration and you optimize further and you do more than case documentation, actually the bound bound we start diverging.",
            "So the only guarantee that we have is that if you do that number of iterations, your solution.",
            "We have a precision that will be smaller than row and that's why I actually.",
            "It is optimal because the only guarantee that we have, whereas the the strategies that.",
            "Achieve the optimal rates will be such that you can obtain this.",
            "This kind of guarantees you can compute number of iterations that will ensure that your president will be smaller than row, but then it will also ensure that if you go on optimizing, your algorithm will converge to the optimal of your problem and that it will do so with an optimal rate.",
            "And actually, you don't really need that because you you just want a limited precision, so that's why we think that this strategy is optimal and actually is faster than the one achieving optimal rates."
        ],
        [
            "OK, so some numerical simulations to highlight those results."
        ],
        [
            "So the setting is pretty classical.",
            "We have used the the classical image of Lena that we have blurred artificially and we're using TV regularization methods to Deborah rates and all you have to know is actually actually it's a.",
            "It's a nonsmooth convex optimization problem that can be solved by using inexact proximal methods and what we are showing here.",
            "Is the F of X K -- F of X star so the the precision of the solution versus the computational cost of the optimization procedure and the different curves correspond to different strategies?",
            "The same algorithm but with different strategies.",
            "Different way of shooting the number of iterations.",
            "So for instance, the blue curve which is covered by the red one.",
            "Here is what happens when you're using one inner iteration all the time, and what you showed it at 4.",
            "Very low precision.",
            "It will be the fastest strategy as you can see because yeah, if you want to solve for 9:50 to the minus three precision, this will be the fastest possible strategy.",
            "But the drawback of that is that you won't be able to reach very high precision in the end as you see it seems to converge to some plateau with a very limited precision is now.",
            "If you use 2 two inner iterations where you can see that it will be slightly sore at first, but you will be able.",
            "To find a solution with a better precision in the end and so on, and the last curve that are displayed here, the black one.",
            "The Black one is actually what happens when you're using the strategy giving optimal rates or the one that was advised by Mark Smith and his colleagues and you can see that it's very much much slower than our strategies.",
            "Their orders of magnitudes in the computational efficiency.",
            "But of course the advantage of this method is that you can go on forever and the optimization error will go to zero because we will actually converge with the optimal of your problem.",
            "OK, so just about the red curve.",
            "I don't have time to talk about it, but it's another strategy that we're derived from that and that actually works amazingly well in practice, so feel free to come and see me at the poster too if you want more information about that so."
        ],
        [
            "To conclude.",
            "Uh."
        ],
        [
            "OK, what we have done here.",
            "Is to provide a new finite time analysis that in a way, is opposed to the absolute asymptotical ones in the sense that usually what people care about in optimization is to have nice asymptotical behavior.",
            "And what we have done is take a different direction and say, OK, we don't care so much about the antibiotics.",
            "What we want is a finite finite time guarantees.",
            "And we have seen that in that case at least it leads to very different.",
            "Strategies and algorithms that have very different efficient computational efficiency.",
            "So yeah, we have retrieved computationally optimal strategy that allows you to probably get row accurate solutions, which can be good in practice.",
            "And yeah, this is the part that I didn't have time to talk about.",
            "We have also derived new practical strategical sip for speedy, inexact proxamol method that seems to perform extremely well in practice, so feel free to ask about that."
        ],
        [
            "Several open questions and future directions with that work, but the main question, I think is we have studied this.",
            "We have applied this methodology in a very specific context, which is the context of inexact proximal methods.",
            "But of course we're wondering if this same methodology, the same kind of tools could be used to optimize the computational efficiency of order.",
            "Optimization procedure in very different settings, so that will be a.",
            "Can we apply this and can we find the same kind of sort of results on another?",
            "In other situations is a question we are really interesting interested into so thank."
        ],
        [
            "So your attention, so yeah, sorry, one last thing you can, you can actually we have a technical report that is available on archive which has the same name as the presentation, so feel free to have a look at it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I get started I have alot of things to say in that presentation, so please take a deep breath and then I'll get started.",
                    "label": 0
                },
                {
                    "sent": "OK so I will be talking about optimal computational tradeoff of inexact proximal methods, and this is a yeah, so NPM, I'm coming from the leaf Lobato, Edna formatic fundamental of masses in France and this is joint work with someone and look up at the Sky with here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first a bit of an introduction which has been actually partly covered by Charlotte's Web.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's this morning about the tradeoffs of learning.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So very general notation and stuff about our setting, so we'll be considering supervised statistical learning and where the data are considered as realizations of pairs of random variables X&Y with the underlying but unknown distribution D.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The goal usually is to learn the predictor.",
                    "label": 0
                },
                {
                    "sent": "A good predictor, which is a mapping from the input SpaceX to the output space of target space Y.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And usually the goodness of a prediction is measured through a loss function that basically measures the discrepancy between the predictions H of X and access associated target.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why and the goodness of a predictor is measured through risk function, which simply is the expectation over the unknown distribution D of this loss?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An yeah, what we would like to do is to retrieve these absolute best predictor which would be the function, the mapping H star that minimizes that that true risk.",
                    "label": 0
                },
                {
                    "sent": "Of course, as you may know, it's not possible and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because actually what will happen in practice is that the learning algorithms will give you functions that are denoted by HN~ which have an access error that can be decomposed in this way.",
                    "label": 0
                },
                {
                    "sent": "So this decomposition is due to Limbo two and Lydia Bousquet and shrouds talked about it this morning and it can be so.",
                    "label": 0
                },
                {
                    "sent": "Basically this access error is the difference between the risk of the true best predictor and the one that your algorithm will give you, and it can be decomposed in three terms, the 1st, that is the approximation term.",
                    "label": 0
                },
                {
                    "sent": "That is due to the fact that you are not actually finding the best.",
                    "label": 0
                },
                {
                    "sent": "The best mapping from X to Y, but only the best among a subset which is called the hypothesis class that you're exploring the estimation error.",
                    "label": 0
                },
                {
                    "sent": "The second term, which is due to the fact that you are actually you can't directly minimize the expectation of the loss function, but usually what you can do is only minimize an estimator of this risk.",
                    "label": 0
                },
                {
                    "sent": "So this induces a another type of access error, and finally the third.",
                    "label": 0
                },
                {
                    "sent": "The first term that has been.",
                    "label": 0
                },
                {
                    "sent": "Long overlooked, which is the optimization error which is due to the fact that usually the problem that you want to solve don't have a closed form solution.",
                    "label": 0
                },
                {
                    "sent": "So what you have to do is find a numerical solution with some some numerical error.",
                    "label": 0
                },
                {
                    "sent": "So this will induce another error which is called the optimization error.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now you have two types of problems we could say so for a smoke small scale problems, which basically means that the limiting resource would be the number of data and well, you can do pretty much what you want and you can expect that the you will be able to solve your problems with such such a high precision that you can actually neglect the optimization error.",
                    "label": 0
                },
                {
                    "sent": "So in that case what you can do for instance, is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pay attention to what this guy has to say so the guy on the left is Vladimir Vapnik and he has addressed some years ago.",
                    "label": 0
                },
                {
                    "sent": "Now the tradeoff that exists between the approximation and estimation, of course, is not the only one to have done that, but if you do, the thing that is advising, well, you know that you will be dealing with this with this tradeoff pretty well, and then so basically it will tell you what kind of problem you had to solve.",
                    "label": 0
                },
                {
                    "sent": "If you want to do things properly.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the guy on the on the right is Yuri Nesterov.",
                    "label": 0
                },
                {
                    "sent": "I could have put other pictures of course, but isn't more the optimization guy.",
                    "label": 0
                },
                {
                    "sent": "And yeah, given a problem, this guy I've done a lot of algorithms that are very good theoretical properties and that will ensure you that you will efficiently solves, solves all sorts of, well, a lot of different problems.",
                    "label": 0
                },
                {
                    "sent": "So but",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The situation is a little more complicated when you have large large scale problems.",
                    "label": 0
                },
                {
                    "sent": "Sorry, because now the limiting resource is not the number of data but the amount of time that you have to actually run your algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what what happened is that you have so much data that you actually can't afford to address the.",
                    "label": 0
                },
                {
                    "sent": "Tradeoff between approximation and estimation.",
                    "label": 0
                },
                {
                    "sent": "In such a principled way and still expect to be able to solve your problem with the arbitrarily high precision.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty much what this guy.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Number two as said and what he's saying in his paper.",
                    "label": 0
                },
                {
                    "sent": "That which is called the tradeoffs of large scale learning.",
                    "label": 1
                },
                {
                    "sent": "And yeah, OK, so I won't be directly addressing this multi trade off, but there are some interesting questions that very easily arise from this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This for instance, yeah, as the runtime is the limiting resource.",
                    "label": 1
                },
                {
                    "sent": "Well, computer computational efficiency matters when you're when you're when you're making algorithms learning algorithms, so we would like to find ways to assess this computational efficiency then.",
                    "label": 1
                },
                {
                    "sent": "11 conclusion is that when you have this multi trade off what you may want to do is to balance the free terms, which means that you really want to optimize your to solve your problem with limited precision.",
                    "label": 0
                },
                {
                    "sent": "So in that situation are the rates of convergence still relevant while they are very relevant when you want to solve problem with arbitrarily high precision, but when you want to deal with limited precision, it's not so clear that rates of convergence are still so so important and finally so.",
                    "label": 1
                },
                {
                    "sent": "Can we directly take into account the runtime of our algorithm optimization algorithm?",
                    "label": 0
                },
                {
                    "sent": "And that's the thing we have tried to do in our work, so we have done that in a very specific setting which is called.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The inexact proximal methods, so I will give a few few notion.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About that, so the context is the nonsmooth convex optimization.",
                    "label": 0
                },
                {
                    "sent": "When we have problems where you want to minimize the function F of X, which is called composite, which is a sum of two terms which are both convex but only the first one G is smooth and H is not necessarily.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "News.",
                    "label": 0
                },
                {
                    "sent": "And a general framework around type of algorithm that you can use to solve this problem is called proximal gradient methods and not share their generalization of.",
                    "label": 0
                },
                {
                    "sent": "The well known gradient methods.",
                    "label": 0
                },
                {
                    "sent": "Typically what you will do with given some iterates SRX of X of S, K -- 1.",
                    "label": 0
                },
                {
                    "sent": "Sorry you will apply gradient step with respect to the smooth parts and then you will apply the proximity operator which is defined as this and roughly well what you want to do is solve this optimization problem.",
                    "label": 0
                },
                {
                    "sent": "The proximity point will be the X that will.",
                    "label": 0
                },
                {
                    "sent": "That will be the minimizer of this dysfunction, which this part corresponds to the quadratic approximation of this part.",
                    "label": 0
                },
                {
                    "sent": "Plus your non smooth spot.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now there are two types of situations depending on the on the function H that you have first, the function that are called usually refer to as simple in approximal sense.",
                    "label": 0
                },
                {
                    "sent": "For instance, at one regularization term or indicators on the convex set an induced into scenarios.",
                    "label": 0
                },
                {
                    "sent": "You can compute the proximity point in closed form, so that's not really a problem, but in many situations, for instance when you have TV regularization, terms or norms inducing structures.",
                    "label": 0
                },
                {
                    "sent": "City like Mix mix normal things like that you have no clothes forms or for the computer proximity points.",
                    "label": 0
                },
                {
                    "sent": "So what you have to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is to compute the numerical solution that will induce some approximation.",
                    "label": 0
                },
                {
                    "sent": "So for now, let's assume that you have a black box that at iteration you will have to evaluate approximately point and suppose that you have a black box that gives you.",
                    "label": 0
                },
                {
                    "sent": "That gives you a point.",
                    "label": 0
                },
                {
                    "sent": "That is, that had some approximation that is defined in this way.",
                    "label": 0
                },
                {
                    "sent": "Basically, it tells you that the proximity map the function that's trying to minimize in the inner problems.",
                    "label": 0
                },
                {
                    "sent": "Evaluated at the point that you will get will be no further than epsilon K from the real minimizer.",
                    "label": 0
                },
                {
                    "sent": "OK, so people have been using these methods a lot and I've tried to to study.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for a long time, sorry for long time we have had necessary condition, for example, necessary and sufficient conditions that ensure that your algorithm will converge to the minimizer of your problem.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "And one thing I want to add is that here the epsilon, epsilon case, or actually nice can be seen as optimization hyperparameters, because when you're dealing with your problem, you will have a way to set the level of approximation that you want to have.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to make things clear here, that's how that's what inexact proximal algorithm look like.",
                    "label": 0
                },
                {
                    "sent": "So we have what we call 2 nested loop.",
                    "label": 0
                },
                {
                    "sent": "So the outer loop correspond to the basic proxamol method, and at each step you will have to use your black box to find an approximation of your proximity point, and you have to you have to have it run.",
                    "label": 0
                },
                {
                    "sent": "Enough time so that you can get sufficient precision epsilon.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I OK.",
                    "label": 0
                },
                {
                    "sent": "So more recently, actually last year at Nips Mack Smith and his colleagues came with a very nice bound for this inexact proximal method.",
                    "label": 0
                },
                {
                    "sent": "So it looks like that.",
                    "label": 0
                },
                {
                    "sent": "So you see that it will depend, of course, on the difference F of X K -- F of X star.",
                    "label": 0
                },
                {
                    "sent": "So excited, the minimizer of the problem will be as follows, and what you can see that of course depends on K, and it will depend on the all the epsilon eyes, although the inner precisions of your of your problems and.",
                    "label": 0
                },
                {
                    "sent": "So what they showed you that you can actually well first, if you set epsilon to zero, you find the usual bound that you have with.",
                    "label": 0
                },
                {
                    "sent": "Exact approximate method.",
                    "label": 0
                },
                {
                    "sent": "And actually you can retrieve within the exact case.",
                    "label": 0
                },
                {
                    "sent": "You can retrieve the optimal rates so 1 / K as long as the epsilon case converge at least as fast as 1 / K to 2 plus some positive Delta.",
                    "label": 0
                },
                {
                    "sent": "So basically it means that you can have the best rates you can expect can be obtained if you impose.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very strict conditions of the epsilon K. This is very strict because in the first steps of your iteration it's not so much of a not a big deal, but as you go on in the optimization procedure you will have to solve the problems more and more precisely.",
                    "label": 0
                },
                {
                    "sent": "So in a way, OK, this will let you have optimal rates, but each iteration each outer iteration will have a start having a huge cost because you will have to put a lot of energy for.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Having no problems, so OK, Now let's remember our first motivations.",
                    "label": 0
                },
                {
                    "sent": "We wanted to assess the computational efficiency of our algorithms.",
                    "label": 1
                },
                {
                    "sent": "We wanted to deal with limited solving our problems with limited precision.",
                    "label": 1
                },
                {
                    "sent": "So maybe the convergence rate are not so relevant.",
                    "label": 0
                },
                {
                    "sent": "And finally we wanted to try to directly take into account that the runtime of our algorithms, and that's what we have.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that context, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the global cost of those methods can be assessed as follow, so let's let us denote by ELISA number of inner iteration that you are using.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that you're using an iterative procedure to compute approximately point Li will be the number of inner iteration that you will have at the outer iteration index by I and let us denote by C in the cost of an inner iteration and see out the cost of an outer iteration.",
                    "label": 0
                },
                {
                    "sent": "The overall cost of your procedure.",
                    "label": 0
                },
                {
                    "sent": "Will be as follows, so seeing times the sum of all the allies for one for I going from 1 to K + K times out.",
                    "label": 0
                },
                {
                    "sent": "So this is the overall cost of.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Procedure.",
                    "label": 0
                },
                {
                    "sent": "No, the fastest strategy to solve a problem with the precision of throw can be achieved if you can solve this problem.",
                    "label": 1
                },
                {
                    "sent": "So if you minimize over K an ALDI Elisa number of inner and outer iterations, you minimize the cost of your overall procedure under the constraint that your solution will have a precision that is at least row given precision row, then that's pretty cool, because you show that you have had you have done the the fastest strategy possible.",
                    "label": 0
                },
                {
                    "sent": "So this problem is a little difficult because we don't exactly know how F of XK minus average store behaves.",
                    "label": 0
                },
                {
                    "sent": "But what we do have our bounds on that, such as the one I introduced earlier.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now there is just one thing missing because the as you may remember the bounds depending on the epsilon eyes and now our cost depends on the allies, the number of iterations, but that of course related.",
                    "label": 1
                },
                {
                    "sent": "For instance, if the black box you're using is an approach is an iterative algorithm with sublinear convergence rate.",
                    "label": 1
                },
                {
                    "sent": "Then you can say that the epsilon I can be written as follows, so it will be some constant A over Eli to the Alpha, so Alpha will be the convergence rate of your order.",
                    "label": 0
                },
                {
                    "sent": "Algorithm that you use in your inner iterations in your inner loops.",
                    "label": 0
                },
                {
                    "sent": "Typically it will be 1/2 one.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "And now you can plug that back in the bound of Mark Schmitz and you will get this result, which now is still abound on the error, but which now depends on the number of inner and outer iterations, and you know that the the precision of your solution will be smaller than this bound.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now here is our main result.",
                    "label": 1
                },
                {
                    "sent": "OK, now what we have done is trying to we have solved the problems of minimizing the cost of the globe of the overall procedure under the constraint that the boundary installed I just introduced will be smaller than the given precision that you wanted row.",
                    "label": 1
                },
                {
                    "sent": "So, and this can be so, this problem can be solved and here is the solution.",
                    "label": 0
                },
                {
                    "sent": "The optimal number of inner and outer iterations.",
                    "label": 0
                },
                {
                    "sent": "It looks a little ugly, but not that much.",
                    "label": 0
                },
                {
                    "sent": "Actually what you have to note is that the Allies star actually do not depend on I.",
                    "label": 0
                },
                {
                    "sent": "This means that you will have to use constant number of inner iterations along your optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "So that means that you will actually solve your problems within constant precision, which is extremely different from the strategy I told you earlier that.",
                    "label": 0
                },
                {
                    "sent": "Allows to achieve the optimal rates of convergence because actually.",
                    "label": 1
                },
                {
                    "sent": "OK, so you have to do that using a constant number of this constant number of iteration and to run the algorithm for this number of auto iteration.",
                    "label": 0
                },
                {
                    "sent": "OK, there is no closed form solution for that, but this is a 1 dimensional problem and this function is monotonically decreasing then increasing so you can find exactly the integral that minimizes this problem.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as I said, the number of iterations is constant and the president.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The constant too, and actually the dis optimal numbers of iteration will be such that the bound we are optimizing the will exactly value row.",
                    "label": 1
                },
                {
                    "sent": "After K start, number of auto iterations and that is the only guarantee that we have.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you have a look at the at the bound, if you optimize further and you keep this number of constant iteration and you optimize further and you do more than case documentation, actually the bound bound we start diverging.",
                    "label": 0
                },
                {
                    "sent": "So the only guarantee that we have is that if you do that number of iterations, your solution.",
                    "label": 0
                },
                {
                    "sent": "We have a precision that will be smaller than row and that's why I actually.",
                    "label": 0
                },
                {
                    "sent": "It is optimal because the only guarantee that we have, whereas the the strategies that.",
                    "label": 0
                },
                {
                    "sent": "Achieve the optimal rates will be such that you can obtain this.",
                    "label": 0
                },
                {
                    "sent": "This kind of guarantees you can compute number of iterations that will ensure that your president will be smaller than row, but then it will also ensure that if you go on optimizing, your algorithm will converge to the optimal of your problem and that it will do so with an optimal rate.",
                    "label": 0
                },
                {
                    "sent": "And actually, you don't really need that because you you just want a limited precision, so that's why we think that this strategy is optimal and actually is faster than the one achieving optimal rates.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so some numerical simulations to highlight those results.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the setting is pretty classical.",
                    "label": 0
                },
                {
                    "sent": "We have used the the classical image of Lena that we have blurred artificially and we're using TV regularization methods to Deborah rates and all you have to know is actually actually it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a nonsmooth convex optimization problem that can be solved by using inexact proximal methods and what we are showing here.",
                    "label": 1
                },
                {
                    "sent": "Is the F of X K -- F of X star so the the precision of the solution versus the computational cost of the optimization procedure and the different curves correspond to different strategies?",
                    "label": 0
                },
                {
                    "sent": "The same algorithm but with different strategies.",
                    "label": 0
                },
                {
                    "sent": "Different way of shooting the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the blue curve which is covered by the red one.",
                    "label": 0
                },
                {
                    "sent": "Here is what happens when you're using one inner iteration all the time, and what you showed it at 4.",
                    "label": 0
                },
                {
                    "sent": "Very low precision.",
                    "label": 0
                },
                {
                    "sent": "It will be the fastest strategy as you can see because yeah, if you want to solve for 9:50 to the minus three precision, this will be the fastest possible strategy.",
                    "label": 0
                },
                {
                    "sent": "But the drawback of that is that you won't be able to reach very high precision in the end as you see it seems to converge to some plateau with a very limited precision is now.",
                    "label": 0
                },
                {
                    "sent": "If you use 2 two inner iterations where you can see that it will be slightly sore at first, but you will be able.",
                    "label": 0
                },
                {
                    "sent": "To find a solution with a better precision in the end and so on, and the last curve that are displayed here, the black one.",
                    "label": 0
                },
                {
                    "sent": "The Black one is actually what happens when you're using the strategy giving optimal rates or the one that was advised by Mark Smith and his colleagues and you can see that it's very much much slower than our strategies.",
                    "label": 0
                },
                {
                    "sent": "Their orders of magnitudes in the computational efficiency.",
                    "label": 0
                },
                {
                    "sent": "But of course the advantage of this method is that you can go on forever and the optimization error will go to zero because we will actually converge with the optimal of your problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so just about the red curve.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to talk about it, but it's another strategy that we're derived from that and that actually works amazingly well in practice, so feel free to come and see me at the poster too if you want more information about that so.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what we have done here.",
                    "label": 0
                },
                {
                    "sent": "Is to provide a new finite time analysis that in a way, is opposed to the absolute asymptotical ones in the sense that usually what people care about in optimization is to have nice asymptotical behavior.",
                    "label": 1
                },
                {
                    "sent": "And what we have done is take a different direction and say, OK, we don't care so much about the antibiotics.",
                    "label": 0
                },
                {
                    "sent": "What we want is a finite finite time guarantees.",
                    "label": 0
                },
                {
                    "sent": "And we have seen that in that case at least it leads to very different.",
                    "label": 0
                },
                {
                    "sent": "Strategies and algorithms that have very different efficient computational efficiency.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we have retrieved computationally optimal strategy that allows you to probably get row accurate solutions, which can be good in practice.",
                    "label": 0
                },
                {
                    "sent": "And yeah, this is the part that I didn't have time to talk about.",
                    "label": 0
                },
                {
                    "sent": "We have also derived new practical strategical sip for speedy, inexact proxamol method that seems to perform extremely well in practice, so feel free to ask about that.",
                    "label": 1
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Several open questions and future directions with that work, but the main question, I think is we have studied this.",
                    "label": 0
                },
                {
                    "sent": "We have applied this methodology in a very specific context, which is the context of inexact proximal methods.",
                    "label": 1
                },
                {
                    "sent": "But of course we're wondering if this same methodology, the same kind of tools could be used to optimize the computational efficiency of order.",
                    "label": 1
                },
                {
                    "sent": "Optimization procedure in very different settings, so that will be a.",
                    "label": 1
                },
                {
                    "sent": "Can we apply this and can we find the same kind of sort of results on another?",
                    "label": 0
                },
                {
                    "sent": "In other situations is a question we are really interesting interested into so thank.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So your attention, so yeah, sorry, one last thing you can, you can actually we have a technical report that is available on archive which has the same name as the presentation, so feel free to have a look at it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}