{
    "id": "jcazu4saoidqj6xdzqsmlxjgmdgszbmw",
    "title": "A quasi-Newton proximal splitting method",
    "info": {
        "author": [
            "Jalal Fadili, Laboratoire GREYC, University of Caen Basse-Normandie"
        ],
        "published": "Jan. 14, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning",
            "Top->Computer Science->Optimization Methods->Convex Optimization"
        ]
    },
    "url": "http://videolectures.net/machine_fadili_splitting_method/",
    "segmentation": [
        [
            "The problem."
        ],
        [
            "We were interested in is for solving these kind of problems, which quite an abstract problems minimizing the sum of two functions.",
            "So this is called convex composite problems where F is F&G are both functions from RN to R&R proper lower some continuous on complex functions, proper meaning that their domain is non trivial and F is a function which is also convex Ann.",
            "It is smooth function which has gradient which is lipstick Lipschitz and she is a simple function meaning that its proximity operator can be computed in closed form.",
            "For instance think as G as the indicator function of some closed convex set.",
            "In which case we know how to project onto the corresponding closed convex set.",
            "We have some technical conditions also and we assume that certain minimizers is nonempty.",
            "Typical examples are law.",
            "So for instance, an also examples from machine learning.",
            "Such as the sparse support vector machine which is written here with the other norm and some data Fidelity, which is typically a loss, which could be the squared hinge or logic loss or any other loss which satisfies the above conditions.",
            "Many other potential applications of this problem could be in signal, image processing, machine learning, classification, statistical estimation, etc.",
            "So the goals that we had in mind are the following.",
            "Most of the algorithms, Archers nowadays are more more.",
            "First, are there algorithms so we are interested in higher order approximate solution algorithms, meaning that we want to use this second order information.",
            "We want to explore the simplicity of the function G but also the smoothness of function F. We want to deal with large scale data and avoid also to sub iterate using some nested algorithms and also to have some theoretical guarantees."
        ],
        [
            "OK, so here are the contributions we propose, a class of variable metric forward backward splitting, meaning that the inner product instead of being all all the time constant, which is the traditional Acadian scholar product, we're going to change this color product through a metric which is denoted here VN and in both the implicit and explicit step we're going to wait with this metric VN.",
            "And so we can ensure global local conversions.",
            "We can.",
            "All we were also able to design specific class of metrics VN with what we call here.",
            "The low memory symmetric rank one metrics and this guy kind of quasi Newton metric will allow us not only to be able to compute the matrix VN in a very efficient way and all just or typically vectors instead of matrices.",
            "We don't have any matrix to invert and the implicit.",
            "TAP, which is the computation, for instance, of the proximity operator, can be done in closed form and here.",
            "Oh, I'm just summarizing some results.",
            "Given that we have a new convex analysis results where we have exact calculation of the proximity operating in many many cases an which makes this algorithm very efficient.",
            "So for instance here we have the computation of this proximity operator for different classical norms used in regularization.",
            "OK, so."
        ],
        [
            "Here are the results.",
            "Sure, I'm almost done.",
            "Here are the results, typically for the lasso problem with two types of operators for a random operators on the left and on the right, we have these discrete differential operator where we can see we have compared to a lot of different algorithms.",
            "So basically what we can summarize here is that depending on the operator, none of the algorithms known so far was able to beat the others, but basically the one that we've proposed is.",
            "The most stable, which is among the fastest, not always the fastest, but among the fastest an if you want to know more, just come to the poster.",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We were interested in is for solving these kind of problems, which quite an abstract problems minimizing the sum of two functions.",
                    "label": 0
                },
                {
                    "sent": "So this is called convex composite problems where F is F&G are both functions from RN to R&R proper lower some continuous on complex functions, proper meaning that their domain is non trivial and F is a function which is also convex Ann.",
                    "label": 0
                },
                {
                    "sent": "It is smooth function which has gradient which is lipstick Lipschitz and she is a simple function meaning that its proximity operator can be computed in closed form.",
                    "label": 0
                },
                {
                    "sent": "For instance think as G as the indicator function of some closed convex set.",
                    "label": 0
                },
                {
                    "sent": "In which case we know how to project onto the corresponding closed convex set.",
                    "label": 0
                },
                {
                    "sent": "We have some technical conditions also and we assume that certain minimizers is nonempty.",
                    "label": 0
                },
                {
                    "sent": "Typical examples are law.",
                    "label": 0
                },
                {
                    "sent": "So for instance, an also examples from machine learning.",
                    "label": 0
                },
                {
                    "sent": "Such as the sparse support vector machine which is written here with the other norm and some data Fidelity, which is typically a loss, which could be the squared hinge or logic loss or any other loss which satisfies the above conditions.",
                    "label": 1
                },
                {
                    "sent": "Many other potential applications of this problem could be in signal, image processing, machine learning, classification, statistical estimation, etc.",
                    "label": 1
                },
                {
                    "sent": "So the goals that we had in mind are the following.",
                    "label": 0
                },
                {
                    "sent": "Most of the algorithms, Archers nowadays are more more.",
                    "label": 1
                },
                {
                    "sent": "First, are there algorithms so we are interested in higher order approximate solution algorithms, meaning that we want to use this second order information.",
                    "label": 0
                },
                {
                    "sent": "We want to explore the simplicity of the function G but also the smoothness of function F. We want to deal with large scale data and avoid also to sub iterate using some nested algorithms and also to have some theoretical guarantees.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here are the contributions we propose, a class of variable metric forward backward splitting, meaning that the inner product instead of being all all the time constant, which is the traditional Acadian scholar product, we're going to change this color product through a metric which is denoted here VN and in both the implicit and explicit step we're going to wait with this metric VN.",
                    "label": 0
                },
                {
                    "sent": "And so we can ensure global local conversions.",
                    "label": 0
                },
                {
                    "sent": "We can.",
                    "label": 0
                },
                {
                    "sent": "All we were also able to design specific class of metrics VN with what we call here.",
                    "label": 0
                },
                {
                    "sent": "The low memory symmetric rank one metrics and this guy kind of quasi Newton metric will allow us not only to be able to compute the matrix VN in a very efficient way and all just or typically vectors instead of matrices.",
                    "label": 0
                },
                {
                    "sent": "We don't have any matrix to invert and the implicit.",
                    "label": 0
                },
                {
                    "sent": "TAP, which is the computation, for instance, of the proximity operator, can be done in closed form and here.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm just summarizing some results.",
                    "label": 0
                },
                {
                    "sent": "Given that we have a new convex analysis results where we have exact calculation of the proximity operating in many many cases an which makes this algorithm very efficient.",
                    "label": 1
                },
                {
                    "sent": "So for instance here we have the computation of this proximity operator for different classical norms used in regularization.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "Sure, I'm almost done.",
                    "label": 0
                },
                {
                    "sent": "Here are the results, typically for the lasso problem with two types of operators for a random operators on the left and on the right, we have these discrete differential operator where we can see we have compared to a lot of different algorithms.",
                    "label": 0
                },
                {
                    "sent": "So basically what we can summarize here is that depending on the operator, none of the algorithms known so far was able to beat the others, but basically the one that we've proposed is.",
                    "label": 0
                },
                {
                    "sent": "The most stable, which is among the fastest, not always the fastest, but among the fastest an if you want to know more, just come to the poster.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}