{
    "id": "ty6aaddhq3yoypwriseotu7nrd4hkrhr",
    "title": "Deep Belief Networks",
    "info": {
        "author": [
            "Geoffrey E. Hinton, Department of Computer Science, University of Toronto"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_hinton_dbn/",
    "segmentation": [
        [
            "Talk overall for like 3 hours about a particular view of learning and a particular way of doing it, which I called deep belief Nets.",
            "And what you're going to learn over these three hours is how to learn."
        ],
        [
            "Multi layer generative models one layer at a time, which is much easier than learning the whole model all at once.",
            "And how you can make them fancier by within each layer having lateral interactions and implementing Markov random field.",
            "How, once you've learned these multi legendary models you can use them for sort of typical machine learning things like discrimination or classification or regression and how you can actually use these multilayer generative models as kernels for Gaussian processes.",
            "But kernels that adapt so you can actually learn the kernel.",
            "How you can do nonlinear dimensionality reduction on very big datasets, particularly how you can reduce it down to a very small binary vector that can be used as an address so that you can convert objects into addresses so you can find other similar objects very quickly so you can use hash coding for things that are just similar instead of things that are identical.",
            "Um and how you can apply this just to control data."
        ],
        [
            "So I want to start off by making a contrast between statistics and AI, and I think of myself as doing AI.",
            "This isn't really true anymore, right?",
            "But ten years ago, maybe statistics was typically fairly low dimensional data with lots of noise and not much structure, and the problem was to avoid modeling the noise.",
            "Avoid thinking you've got a model of financial data when you haven't, and things like Gaussian process is extremely good for that.",
            "And then AI is typically dealing with very highly structured data, very high dimensional.",
            "So things like big images or video sequences.",
            "And it's not typically very noisy.",
            "That is, there's lots and lots of structure.",
            "Then if you could only represent it right, you could see all that structure.",
            "Learn all that structure.",
            "So the problem isn't to avoid getting confused by noise.",
            "So you don't have to worry too much about using distributions over parameters instead of just one set of parameters.",
            "You're not trying to average away all the noise, you're just trying to find some way of representing all this structure so that you can learn it.",
            "That is a representation in which learning will work.",
            "And I'm going to go through the history of Europe."
        ],
        [
            "It works in three slides.",
            "There were perceptrons in the 60s and you typically hand coded some features which you could have done within your own natural.",
            "They could just be hand coded so you can image you got some features.",
            "You learn the weights from the features to decision units.",
            "And for funding you would use decision units like this and they.",
            "It was a good algorithm for learning.",
            "The weights is still used by Google a lot, but.",
            "All the difficulties in coming up with the right features to make it possible to discriminate with one layer of weights."
        ],
        [
            "So the second generation of neural networks.",
            "We tried having multiline Nets with continuous input output functions, logistic functions, and then what you would do is you put in, put in.",
            "You can output.",
            "You take some measure of how different this was from.",
            "I didn't mean measure in the technical sense some some way of saying what the discrepancy was between this and the desired output, and then you would use the chain rule which we discovered in the 1980s for sending derivatives backwards so that you could figure out derivatives for all these weights and then you change your little bit.",
            "So it should be better getting right answer and the problem with this is it was I can now say it was a huge disappointment.",
            "Galico managed to get it to work quite nicely for convolutional neural Nets.",
            "But on the whole, when you train lots of hidden layers like this, it's very hard to get it to learn, and it never really did do that much compared with what we expected of it.",
            "And so support vector machines came along and sort of pretty much wait this out within the machine learning community, and I want to show you what's wrong with this and what's wrong with it is you're trying to get all the information from the labels.",
            "And a label.",
            "Typically, unless you've got a huge set of possible labels, doesn't contain much information."
        ],
        [
            "Oh well, I won't go into that.",
            "You know about support vector machines right there?",
            "Just to kind of perceptron with a clever way fitting it.",
            "So.",
            "Back propagation.",
            "Big datasets are unlabeled, so the fact that all the action comes from the labels means you can't use unlabeled data unless you do something else.",
            "The learning time doesn't scale very well with multiple layers.",
            "Typically, if you start the weights off small as you back propagate through the layers, you get a small times a small times are small so you get tiny derivatives.",
            "If you start the weights off big, you've already decided what part of the space you're going to be in, and you wanted the learning algorithm to do that so.",
            "To get it and they also don't get very good local Optima when they have deep Nets.",
            "As we'll see later, you can get much."
        ],
        [
            "Better Optima.",
            "So the idea is we want to keep what's good about backpropagation, which is stochastic gradient descent.",
            "And we want to get rid of what's bad, which is everything.",
            "Depending on the labels.",
            "And so instead of learning the probability of a label given an image.",
            "We're just going to learn the probability of image.",
            "We're going to learn a density model of images.",
            "And now we don't need labels.",
            "And what's more, each image has much more information in it than a typical label.",
            "Images can be big and there's lots of pixels, and so each image puts a lot of constraint on identity function.",
            "Whereas if I give you an image and a label and I try and get the right answer, I don't get much constraint on the mapping from image to label.",
            "The bits of constraint on that mapping imposed by training examples, just the number of bits it takes to say what the answer is."
        ],
        [
            "Isn't very many.",
            "So now the question is what kind of generative model should we learn?",
            "And.",
            "As you all know, I think there's been a big level."
        ],
        [
            "Mission in statistics and AI.",
            "And there's a kind of model which is a directed acyclic graph.",
            "And there's been lots of work on how you learn these and how you do inference in these.",
            "If their sparsely connected, if each node only has a few parents, there's clever exact inference algorithms.",
            "If each node has many parents, influence gets much trickier.",
            "So in these I'll be using directed Nets, in which the observations are at the leaves at the bottom of the iris.",
            "And in that kind of net.",
            "Then the inference problem is, if I you know all the parameters, that is, you know how the states of these nodes determine the distribution for this node.",
            "Inference problem is figuring out what the states of these nodes probably were.",
            "When you see the data and the learning problem is figuring out weights on these connections that give you some parameterized function for the probability distribution.",
            "Here, given the states of these guys.",
            "Typically use binary units, but."
        ],
        [
            "Can generalize that later.",
            "So the units I'll use adjust stochastic binary units where you get some total input.",
            "From other units.",
            "So when you're doing generation this will be top down input.",
            "And the probability of turning unit on is just the logistic function of this total input."
        ],
        [
            "And so now if all the units are like that.",
            "So Radford Neal introduced Nets like this in about 1992.",
            "Full units are like that.",
            "It turns out it's easy to learn the parameters if you can compute the posterior distribution over these hidden causes.",
            "So I show you a data vector that's an observation.",
            "If you can compute the full posterior here, or even if you can just get an unbiased sample from that posterior, learning is easy.",
            "So I show you data vector if you can give me back an unbiased sample that is some binary vector that's the sort of plausible way in which this stuff might have caused that stuff.",
            "Then learning is very easy.",
            "The difficult thing is getting this unbiased sample from the posterior."
        ],
        [
            "So here's the learning rule.",
            "Suppose we had an unbiased sample from the posterior, so I gave you some data down here.",
            "My unbiased sample from the posterior gives me binary states for all these units.",
            "And the learning rule is just.",
            "For the weight on this connection, what I need to do is compute the difference between the actual state of this guy and the unbiased sample from the posterior.",
            "That's SSI here.",
            "That's a one or a zero.",
            "And the probability with which this guy will be turned on by the generative model.",
            "Given the actual states of these guys in the posterior, so that's all these.",
            "SJS here.",
            "So the probability of turning this on P is just the logistics of the input it gets from all these actual states here.",
            "And it turns out the maximum likelihood learning rule is 2.",
            "To follow the direction in which you're increasing the log problems, the observed data down here is to change the weight in proportion to the learning rate times the state of this guy times the difference between the state of that guy and what you would have predicted.",
            "So it's a very simple learning rule.",
            "And all the problem is getting this sample from the posterior."
        ],
        [
            "And the reason there's a problem is probably you will know is explaining away, so here's a very simple sigmoid belief net.",
            "This actually happened to me in California.",
            "I was asleep one night in the house, jumped and I assume the trucker hit it.",
            "If I be in California and I probably have assumed it was an earthquake, which is what the real reason was, and this network says houses don't just jump, the probability that is about each of minus 20.",
            "Um?",
            "Trucks don't hit houses, but that probably is only to the minus 10 and you don't get earthquakes.",
            "That's only reason minus 10.",
            "So if you see the house jump.",
            "We're assuming these are independent events in the prior, but these events, which were independent, suddenly become highly correlated.",
            "When I see the house jump.",
            "'cause I don't want to sort of pay 20 Nats to suppose that the house just jump for no reason but much rather pay 10 Nats to suppose this and then only one bit to say the House job 'cause it's got an even chance of jumping there 'cause this plus 20 here.",
            "But it be very stupid.",
            "Assume a truck hit the house and there was an earthquake.",
            "'cause that's going to cost me 20 knots.",
            "That's just as bad as assuming those jump spontaneously, so.",
            "So even though these are very rare and uncorrelated events in my model.",
            "Given that there has jumped, they're very highly anti correlated.",
            "That's what Pearl called explaining away.",
            "So the posterior distribution looks something like this over the four possible combinations of binary states here.",
            "And that's what makes it difficult to figure out what all these hidden guys are doing.",
            "Given that you made an observation, you can't separately sort of say how plausible is this, given that the House jumped.",
            "It depends on what that's up to."
        ],
        [
            "So now it gets much worse if we want to learn one of these multilayer sigmoid belief Nets.",
            "We have a kind of likelihood term here that saying given the data, what's that telling me about the probable states of these hidden variables?",
            "But we also have all these extra hidden layers that are determining how likely very likely the model is to have generated various combinations of.",
            "Binary states here and obviously to get the posterior for this layer you need to take the product of the distribution.",
            "That's saying, given the data we need to take things that are likely to cause the data.",
            "On the part of the distribution that's saying, given all this price stuff we know up here we need to get a distribution over how likely we are to generate things here anyway.",
            "So once you have a deep net, you have a very complicated product here.",
            "Even when the prize independent life tricky.",
            "But when you have a complicated product, sounds awful.",
            "Because just to get your hands on this prior, you need to integrate all this.",
            "All these hidden layers here.",
            "So getting your hands on this price is going to be very tricky.",
            "And you have to multiply this prior by the likelihood term here to get the posterior there so it looks horrible, and in particular it looks like you've got no chance of learning what these weights should be without already having learned or guess what these weights should be, so you're gonna have to learn everything all at once.",
            "And what I'm going to convince you is.",
            "That this horrible looking picture that says it's going to be very hard to learn these weights.",
            "Actually, there's a little trick we can do that makes it very easy to learn all this stuff.",
            "That's the sort of main point of this tutorial.",
            "So the obvious way to learn it.",
            "This is what Radford Neal did originally when he proposed these kinds of Nets.",
            "You can use Monte Carlo methods, but that can be a bit slow.",
            "People develop variational methods where they said, well, we know that when you do inference in these Nets, the posterior is complicated.",
            "But let's suppose that it's not.",
            "And there's reasons why this was a much more sensible idea than we thought it was when we sort of it.",
            "So we're going to say that there's all this prior stuff here, and anyway, the likelihood term gives you explaining away and we're just going to all of that.",
            "And we're going to say, given the data, let's suppose that these are independent of one another, and let's do inference like that.",
            "And maybe if we're really lucky if we do inference like that and run our learning algorithm, it'll work anyway.",
            "And.",
            "It turns out that was a sort of beginning of variational methods, where, yes, it does work quite well, and the reason it works quite well is 'cause the learning tries to change all these weights up here and these weights on the links here so that our assumption is true.",
            "That is, it tries to make it true that these really are independent in the posterior.",
            "And by using just the right weight shit they can get.",
            "You can make that fairly true, and that's sort of why variational learning works much better than you would have thought.",
            "So.",
            "The other thing is, even if you your approximation to the posterior distribution over these hidden units is not very good.",
            "Variational learning is still guaranteed to optimize something, but is there some measure that's in some quantity that's improving?",
            "It's a variational bound on the log probability.",
            "Um?",
            "But we're going to go for something else in this tutorial, but it will turn out to be quite closely related.",
            "What we're going to go for is a way of trying to learn the weights.",
            "Just one layer at a time, apparently.",
            "But a way that takes into account the fact that we're going to learn more layers later.",
            "So the sort of obvious way to approach this nasty problem here if you want to learn the weights one layer at a time is to say, let's suppose that none of this stuff upstairs was here.",
            "Let's suppose these hidden variables had independent priors, so it's a nice simple two layer network like my earthquake network.",
            "Let's learn these weights under that supposition.",
            "And once we've learned, let's start trying to learn a better model of the posterior distributions we actually get here.",
            "But that doesn't work very well because.",
            "If you don't have all this stuff upstairs.",
            "Then it's a really bad assumption that these guys are independent.",
            "Using all this stuff upstairs, if you use it right, you can make it actually quite a good assumption.",
            "In fact, if you set this stuff just right, you can make it a perfect assumption what we're going to see is that by setting this right, you can make it truly be the case that these really are independent, even better than that, you can make it be the cases these are independent and the correct way to figure out the distribution is just take the data multiplied by the weight matrix and put it through the logistic function.",
            "That's really very surprising.",
            "When I first started explaining this to graphical models, people didn't believe me because they knew about explaining away.",
            "So how could these be independent?",
            "And as you'll see later, the answer is that all this stuff up here can exactly explain away explaining away, so explaining away disappears and these become independent if you do it right.",
            "But we'll come to that later.",
            "OK, So what we're going to do now is take Watson.",
            "Apparently a big detour, and we're going to give up on trying to learn these directed models, 'cause it seems too hard.",
            "So that's what I did I gave up on trying to learn directed models and went off to some other kind of model.",
            "Which I think are in the past where you have undirected connection, so it's an undirected graphical model now.",
            "And all the independence relationships go the other way around for those as well see.",
            "And along time ago, Terry Sonoski and I came up with an elegant learning algorithm for undirected graphs of these binary stochastic units.",
            "But it was very, very inefficient.",
            "You had to use Monte Carlo methods and they were slow.",
            "Your noisy gradient and you had to follow this noisy gradient for a long time and in deep Nets it was terribly, terribly slow.",
            "So it was sort of rightly regarded as sort of theoretically interesting, but no practical value.",
            "Um?"
        ],
        [
            "It turns out that if you restrict the connectivity and go for a much simpler device, but still undirected, then you can get an efficient learning algorithm.",
            "It's a shame because you restricted the connectivity so much that you haven't got a multi level system anymore, so it looks like you throwing the baby out with the bathwater.",
            "But we're going to get the baby back later."
        ],
        [
            "So throw the baby out with the bathwater and then pull on the hairs in the plug and gradually the baby will come up.",
            "Mum.",
            "You have visible units.",
            "You have hidden units and you don't have any connections between the hidden units and you don't have any connections to invisible units, at least to begin with.",
            "So now.",
            "We get one very nice property already in this, which is if I tell you the states of the visible units, give you an observation vector, then these guys are all now completely independent.",
            "They really are independent of each other.",
            "This is a factorial distribution here.",
            "One way of thinking of it is once I fix these, this guy contact that guy.",
            "Hum.",
            "So inference in this net is really very simple and you can do.",
            "You can get the full posterior trivially.",
            "You just take this state multiplied by these weights.",
            "That gives you a probability, put it through logistic.",
            "Yeah probability, that's the probability of this guy being on and the full post series just the product of all these probabilities.",
            "So you've got it and you can sample from it very easily.",
            "So we got rid of explaining away in this net.",
            "There's no explaining away, cousin undirected model inference is very simple, so we got one nice property now which is inference is simple.",
            "Remember, with the directed Nets, we had the nice property that learning with simple if you could do inference and generation was simple.",
            "Here we have the property.",
            "Inference is simple.",
            "But learning is a bit trickier and also generations trickier.",
            "If you have weights on these connections and you want the model to tell you what it believes in, you have to go backwards and forwards lots of times, so there's no trivial way to generate like there is with the directive model.",
            "But inference is nice and easy."
        ],
        [
            "And in these undirected models you can think about the state of the network is having an energy.",
            "So the energy of our binary visible vector binary hidden vector.",
            "He is just the sum I'm gonna leave out biased terms 'cause they just make the math more complicated.",
            "It's a sum over all pairs of a binary state of visible visible unit I.",
            "So that's a one or a zero binary state of hidden unit and the weight on the connection.",
            "So if two guys are on and there's a big weight between 'em, that gives you a big negative term, that's low energy.",
            "That's good.",
            "It's a happy network.",
            "So happy networks are ones where.",
            "The binary is there on how big positive weights between them.",
            "One nice property of this is if you differentiate this energy with all the negative energy respect to wait, you just get this product of states here so the it's very easy to manipulate the energies.",
            "Um?",
            "By changing the weights and those other derivatives.",
            "And that yeah."
        ],
        [
            "And what we want to do is.",
            "Change the weights to change the energies to change the probabilities so that the log probability of the data is high.",
            "So for a joint configuration.",
            "A visible and hidden vector.",
            "The probability of the joint configuration is proportional to each of minus the energy if.",
            "You run the net in the right way, so if you take one of these."
        ],
        [
            "Pets.",
            "And I just go backwards and forwards, updating all these units in parallel and then updating all these units in parallel using the logistic to pick binary states.",
            "Then if I do it long enough, this will reach what's called its equilibrium distribution or a stationary distribution, and in that stationary distribution it will be sampling."
        ],
        [
            "Joint vectors in proportion to each of the minus their energy.",
            "Solar energy ones will be sampled a lot.",
            "I'm.",
            "OK."
        ],
        [
            "So the probability of a joint vector will be each of the minus its energy normalized by the same quantity of all possible pairs of a visible and the hidden vector.",
            "And if you want to probably visible vector, you just marginalized at age.",
            "So some lateral H and you get that.",
            "So in order to do learning, we just need to take logs and take derivatives and now we can adjust all the weights to change the energy impropriate way."
        ],
        [
            "And when you do that.",
            "You get a very simple learning rule.",
            "This is what's nice about Boltzmann machines.",
            "This stuff was already discovered a few years later by people doing Maxent.",
            "You run this Markov chain where you start with some data.",
            "You activate the hidden units you.",
            "Reactivate the pixels from the hidden units and you go.",
            "Packaged foods until you reach equilibrium, which could take a long time.",
            "And you're getting fantasies from the model.",
            "That is the kind of thing the model believes in is being generated, and it's completely forgotten where it started.",
            "That's the definition of equilibrium.",
            "And when you're showing it data.",
            "You measure the correlation between.",
            "The binary states of pixels in the binary states are feature detectors or visible units in hidden units.",
            "And that zero here means when you're showing your data.",
            "So at times zero in this Markov chain.",
            "And the angle back is just physics for the expected value of.",
            "I don't like to use any 'cause I've got a E for energy.",
            "You do this.",
            "Measure the same statistic when the model is sampling from the things it believes in.",
            "And hey presto, the learning algorithm is if you want to maximize the log probability of visible vector, measure this statistic with the visible vector clamped here.",
            "And these angle brackets are kind of averaging over the noise in these guys.",
            "And the same statistics when the model several ebrium and that is the derivative of the log prop.",
            "So we got a very simple learning rule and its local.",
            "It only depends on the behavior of the two units that the weight connects, which is quite surprising because the derivative here depends on all the other weights in the network.",
            "But its dependence on all the other weights in the network shows up in this difference of correlations.",
            "So the process of settling to equilibrium is kind of propagating information around the net in order to produce these statistics.",
            "And those statistics are telling you they're telling this weight WJ everything it needs to know about all the other weights.",
            "So as in back propagation, you did explicitly by the chain rule going backwards.",
            "This just runs this alternating Gibbs sampling and that somehow gets the information into these statistics."
        ],
        [
            "Now."
        ],
        [
            "This rule we figured out the general case in the early 80s, and then I thought this kind of network wasn't very interesting 'cause it's only got 1 hidden layer with no connections between them and it's just sort of boring special case.",
            "Um?",
            "Many years later I made it go a million times as fast.",
            "So the way you make your million times as fast as this.",
            "You"
        ],
        [
            "Instead of running along change week Librium for hundreds of steps, you just go up and down and up again.",
            "Now it's not actually quite the right algorithm, but it works quite nicely.",
            "There's a poster I think maybe today telling you conditions under which it doesn't work.",
            "There's all sorts of conditions where you can make you screw up, but in general it works quite nicely.",
            "So you go up, you come down, you go up again and you measure the difference in these statistics and sort of what you're seeing is what the statistics are with data and you're seeing sort of the direction in which is heading as it goes towards equilibrium and how the statistics are beginning to change.",
            "The model gets to have its say about what should be going on.",
            "And that difference is enough to allow you to do learning.",
            "So and we'll see later are much better justification for using this.",
            "It gives you more insight into it.",
            "And this is very justified when the weights are quite small.",
            "But we're going to substitute the learning rule as an Infinity here for this learning rule, so you don't have to go up and down and up again.",
            "So now that goes a million times as fast and the reason it goes a million times as fast is 'cause it's 100 times less computation.",
            "And it took me 17 years to think of it.",
            "And in those 17 years computers got 10,000 times faster.",
            "It was very important to take a long time to think of it."
        ],
        [
            "OK, so I'm going to show you a little example where you take some little handwritten digits.",
            "You have only 50 feature detecting neurons.",
            "You start off with all random weights.",
            "You activate the feature detectors, you reconstruct the pixels you activate the feature vectors again and you can think of it as with the data there.",
            "Every time a pair is active, you're going to increment weight slightly.",
            "When you've got reconstructions here, every time a pair of a feature detector pixel are active, you're going to decrement the weight slightly, and one way of thinking of it is, I think this gives some insight into what the algorithm is up to.",
            "I'm.",
            "The reconstructions will typically be things that the network is happier with than reality.",
            "You showed some reality.",
            "It represents the reality.",
            "Then you ask it to reconstruct the reality and then reconstruct it with the sort of surprising corners knocked off, it will reconstruct something that's smoother and more regular.",
            "It's something that it prefers to believe.",
            "When the training algorithm really consists of saying this, take the stuff you'd like to believe and don't believe it.",
            "And take the stuff that's actually there and do believe it.",
            "George Bush used to run the other algorithm."
        ],
        [
            "OK. Hum.",
            "So what I'm showing you here is the weights that are little network like this learns when you show lots of images of twos.",
            "And so each of these big squares is a feature detector.",
            "And the show you the strengths of the 256 connections to the pixels where white is positive and black is negative.",
            "So you can see this guy here has learned a little feature like this.",
            "He likes to have a piece of stroke there and he really insists you don't have anything there.",
            "So if you turn this guy on, he'll try and generate an image that looks like that.",
            "And if you turn on some subset of about 20 of these guys, it turns out if you turn on a sensible subset of 20, the generated images looks just like a 2.",
            "So."
        ],
        [
            "Here's an example.",
            "Where after learning I show it some more TOS that is never seen before.",
            "And I ask you to reconstruct them.",
            "And you see, it does a pretty good job reconstructing.",
            "The reconstruction is a bit more regular than the real data.",
            "But it reconstructs them very well for a whole variety of twos.",
            "And that's with only 50 feature detectors, so you can think of it as aggravated a subset of them and then basically added together the weights of that subset, put it through a nonlinear function, and then showing you the image and it doesn't really good job of reconstructing twos.",
            "It's not quite perfect, and when it gets a very unusual feature, like a vertical tail, which very few tools have, it makes it sort of messes it up a bit.",
            "Um?",
            "But it does quite a good job within 50 feet protectors.",
            "Now what we're going to do is we're going to take this model, was trained on twos that believes the whole world consists of twos, and we're going to hit threes.",
            "Um?",
            "OK.",
            "It's a bit like showing George Bush some real data.",
            "Now it has opinions about what the world should be like.",
            "And this is what it sees, so to speak.",
            "Um?",
            "And if you think about the difference in twos and threes, the difference is that you know threes.",
            "Don't join up there and they don't have a tail, but we can fix that.",
            "We just rub out a little bit here and put it a little bit here and hey presto, we didn't change much and it's a 2.",
            "So it's easy if we turned on the learning, it will very quickly learn not to do that, and would learn to deal with twos and threes.",
            "OK, so that's a little example of this algorithm running to show you it works."
        ],
        [
            "This is sort of a bit of an aside, but.",
            "I want to distinguish a whole bunch of ways of combining probabilistic models.",
            "So one thing we're all very familiar with is if you have a kind of probabilistic model.",
            "I say gaussian.",
            "You can have a bunch of those and average the distributions together, take some weighted average of those distributions.",
            "That's a mixture gaussian's.",
            "The problem with the mixture is the combined distribution can never be sharper than the sharpest of the individual distributions.",
            "So I'm never going to get very sharp representations of things that way.",
            "I'm an alternative is to take a product of the distributions and renormalize so I could take a whole bunch of guys and multiply them together, and if I take, say, a Gaussian Karina Garcia here and I multiply them together, what I'll get is something a tiny little Gaussian in the middle.",
            "Little have half the variance of the Gaussians like here.",
            "And there's almost no probability mass there.",
            "But when I re normalize, it turns into a ghost in the middle.",
            "And the point is, the more I multiply together, the sharper things get.",
            "So by having very vague individual models and multiplying them together, I can get a very sharp model.",
            "I can't do that with the mixture.",
            "Um?",
            "But I have this normalization term that makes learning difficult and you can think of a restricted Boltzmann machine as being a product model.",
            "In fact, it's a product of mixtures.",
            "So each of the hidden units is saying I'm a mixture model if I'm off.",
            "Then I believe in a uniform distribution.",
            "If I'm on.",
            "I believe in on a factorial distribution across visible units where the probability of each visible unit is the sigmoid of the weight on my connection.",
            "So put that weight on the connection through logistic.",
            "That's the probability of each of these units, but they're all independent.",
            "And then when you turn on a bunch of these guys, you get the products of all those distributions.",
            "That's what you do when you add up all these weights.",
            "So restricted Boltzmann machine is a product model and each hidden unit is 1 little probabilistic model and they will be multiplied together.",
            "And what we're going to do now is we're going to take these product models, and we're going to compose them.",
            "That's another way of combining models where we take the hidden units of 1 model and we make them the data for the next model.",
            "So that the input to the next restricted Boltzmann machine is the.",
            "Hidden units of the previous one, so we're sort of stacking 'em up.",
            "And so you can think of 1 theme of this tutorial.",
            "Is these guys work better than those guys?",
            "And in particular if you take these guys and you compose them together, you get something really nice."
        ],
        [
            "So the main reason these restricted Boltzmann machines are interesting is 'cause you can compose them.",
            "That is, you can stack 'em up.",
            "And we're going to learn in the following way, which I would justify in a minute.",
            "We're going to train their features in just where I just showed you.",
            "Then we're going to take the activations of those features when the model is being driven by data, and we could take the binary activations.",
            "Or we could actually use the real values.",
            "The theory is much clean if you say, take the binary activations, you work better in practice if you take the real values, but I'll stick with the theory for now.",
            "So you take a.",
            "You take data.",
            "You, um?",
            "Activate your feature detectors.",
            "You take the binary states of the feature detectors and there the data for training the next restricted Boltzmann machine for training the next layer of weights.",
            "So it's kind of a simple as it could be.",
            "The actual algorithm.",
            "And what you can show is that what you'd like to show is that if I take some data and I build a restricted Boltzmann machine, model of this data.",
            "So if I were to generate from my model, it would generate stuff a bit like the data.",
            "If I now add another hidden layer.",
            "Of sufficient size and I had in the right way, I'd like to be able to guarantee in a sort of expected sense that I get a better model of the data, the badging, another layer can always improve the model.",
            "Unless the model is really perfect.",
            "And you can almost guarantee that you can't quite guarantee that the first time.",
            "You can guarantee that you'll get a better model, but in general, as you add more layers, what you can guarantee is that the more complicated model you get with more layers.",
            "Will be will have a better bound on the log probability of the data.",
            "So your you have a model, there's some variational bound that says the log probability data must be at least this much.",
            "When I add a new layer.",
            "And take the bank for this deeper model.",
            "It's a better bound.",
            "The log probability will be bigger unless I already got a perfect model.",
            "If you add the new layer in the right way, so that's good enough.",
            "You definitely sort of improving things as your address."
        ],
        [
            "So let's suppose we do this.",
            "43 letters so it goes like this.",
            "You take some data.",
            "You learn a restricted Boltzmann machine here, so you've learned these weights.",
            "And they're kind of the same weights in both directions.",
            "Then you take the states of the hidden units.",
            "That you got by taking the data, multiplying by these weights, putting it through the logistic and sampling.",
            "Take those vectors there, treat those as data and learn another model.",
            "And then you take these.",
            "You treat them as data and you learn another model.",
            "And the question is, when I've learned all three things like that, all three models, what do I have?",
            "And I initially thought.",
            "Well, you haven't got a model at all.",
            "You've got three models.",
            "You got one model of the data.",
            "Then you've got another model of what those feature detectors are doing, and you got another model of what those features are doing.",
            "But it's not all one model.",
            "And then if I pointed out that actually you can view it all as one model, but it's not the model you expect.",
            "So you might expect if you viewed as one model, it would be a big undirected model where you have W 3 here and W2 there with our.",
            "Arrows on both ends to me is undirected.",
            "On W on there with others on both ends, so it's a big Boltzmann machine.",
            "But that's not the model you've got.",
            "What you've got is a model where the top level is an unrestricted box machine.",
            "And then below that you have a directed belief net.",
            "And.",
            "One way of saying what I mean by saying that is if you wanted to generate from this model what you want to do is go backwards and forwards here, ignoring all this stuff, go back as a force here until you reach equilibrium.",
            "Then having got a sample of equilibrium here, go chunk chunk OK.",
            "So it's not an undirected model, this parts directed.",
            "In particular, when you're going backwards here, all these weights down here having no influence on what's happening.",
            "Which they would if it was a undirected model.",
            "It's got the kind of valves that are directed.",
            "Model has the things up the other end.",
            "If they can observe don't influence you.",
            "There's no influence comes back from here when you're generating.",
            "OK, now I'm just asking you to take that on faith.",
            "For now, I'll explain why that's true later.",
            "Before I explain why that's true, I want to show you nice example.",
            "Now one of these deep Nets doing something."
        ],
        [
            "Who?",
            "Maybe I don't want to do that, just saying.",
            "No, I want to explain why that's a good idea.",
            "OK, so we first need a little aside.",
            "If you take some factorial distributions and you average them, you don't get a factorial distribution.",
            "You might think you did, but you don't.",
            "Hum.",
            "So in an RBM, if I give you a visible vector, the factorial, the distribution of the hidden units is factorial.",
            "If I now have, say, two visible vectors in half the time, I give you 1 and half the time we give you the other.",
            "When I give you one, you get a factorial distribution.",
            "When I give you the other, you get a factorial conditional distribution, but when you add those two distributions together, what you get is not a factorial distribution when you aren't together yet also correlations.",
            "In general, OK, that's sort of surprising to some people, so I'll call that thing the aggregated posterior.",
            "In other words, you're training restricted Boltzmann machine.",
            "You now show each of the data vectors in your training set, and for each of those data vectors you have a posterior distribution that's factorial across the hidden units.",
            "You know, average all those distributions together.",
            "And you know you get a complicated distribution.",
            "Which I'll call the aggregated posterior.",
            "OK, so you can think of a restricted Boltzmann machine as something that takes the data distribution and converts it into this other distribution.",
            "This aggregated posterior across the hidden units.",
            "And it converts it in such a way that.",
            "From a hidden vector, you're pretty good at reconstructing a data vector.",
            "And then these hidden vectors have some complicated distribution.",
            "So you can think."
        ],
        [
            "It like this.",
            "Hum.",
            "Once I've defined the weights in my restricted Boltzmann machine.",
            "R. That defines some prior distribution over hidden vectors.",
            "That is, if I would take this model and run the Markov chain backwards and forwards and then sample from the hidden units.",
            "Now I get some distribution, that's the distribution the model believes it.",
            "And we'd like that distribution to be a good model of the aggregated posterior that you get when you show it data.",
            "So if in a sense converted the task of modeling data into two tasks, try and get this prior over hidden vectors defined by these weights.",
            "Um?",
            "To match the aggregated posterior and try to get it so that by using the weights again when you convert a hidden vector into visible vector, you get the data distribution and it's sort of split into two tasks, both of which use the same parameters.",
            "And so when we've learned a restricted Boltzmann machine, we've learned to do this, and we've learned to do that.",
            "But we're making sort of compromise, 'cause we're using the same parameters for both.",
            "Now what we can do is we can keep these weights for doing that.",
            "And use some different parameters for doing this to do a better job of it.",
            "So really what we're doing is we're taking the task of modeling the data and we're splitting it in a funny way into a sort of parametric bit here.",
            "And then this aggregated posterior, which is sort of nonparametric thing which we're also trying to model like this, but later on we're going to model with something better, which is actually going to be a high level restricted Boltzmann machine.",
            "I'm going to give you another way to think about doing later, so you get two chances to under."
        ],
        [
            "Send it.",
            "So here's a way of thinking about what I just said.",
            "The probability of the visible vector given a restricted Boltzmann machine.",
            "Is the summer rule hidden vectors of the probability that you would generate that hidden vector if you ran to the equilibrium distribution?",
            "Times the probability that given the hidden vector, you would generate the visible vector where both of these depend on the weights.",
            "OK, and to maximize the probability of a visible vector, what you'd like to do is increase these probabilities in here.",
            "And suppose I freeze this term.",
            "Then to maximize the probability you'd somehow like to change the prior over hidden units, so this gets bigger.",
            "And you probably know from fitting mixture models.",
            "So when you try and set the mixing proportion in the mixture of Gaussians, what you do is you make the mixing proportion be equal to the posterior probability that you use that case.",
            "So in general, when you're adjusting priors, you want to change them to be more like the posteriors that you got using your current values of those priors.",
            "And so that's what we're going to do here.",
            "We're going to try and adjust our model of P of H to be more like the aggregated posterior that we got.",
            "And if we do that, we'll get a better model of the.",
            "So that's what we're doing in this recursive learning.",
            "That's one way of thinking about it."
        ],
        [
            "OK, so.",
            "If you try and do that in a directed net.",
            "Um, you could think about.",
            "Assuming independence here.",
            "And then modeling this.",
            "Um?",
            "And then looking at the posterior you get when you do inference, which is going to be complicated and then trying to model this.",
            "The problem is that if you do it in a directed net, when you learn these weights, it tries very hard to make these.",
            "Independent.",
            "Because that's the assumption of the model.",
            "And so it'll be willing to get weights that don't actually allow you to reconstruct very well, because they're trying to get these to be independent in the posterior.",
            "And that's silly, because later on you're going to put in more stuff here, so these don't actually need to be independent.",
            "You're trying to achieve something.",
            "But you don't really want to achieve in the end, because in the end these are going to be able to make these be very non independent.",
            "So you wasted a lot of effort trying to achieve this independence as a result of which this doesn't work very well.",
            "This bit, and you've already made a big loss here.",
            "And also you make these two independent.",
            "So it doesn't work nearly as well to sort of learn these weights and then assume independence here and then learn a model of this stuff."
        ],
        [
            "So I want to give you a practical example and then go back a bit more to the theory.",
            "We're going to take bigger images of digits, MNIST digits, and we're going to take all 10 different classes.",
            "And we're going to model like this 500 binary features.",
            "Then we're going to model like this 500 features.",
            "Then we're going to put in the class labels with that EMUI multinomial, and we're going to learn a model of this feature vector concatenated with the label, and it's a joint density model, so this is just another restricted Boltzmann machine.",
            "That learns the joint density, but this was done greedily.",
            "And then we're going to do a bit of fine tuning.",
            "I won't talk about much that makes it work quite a bit better, but it's too complicated for now.",
            "So basically think we just did this.",
            "And then we're going to look at what we got."
        ],
        [
            "So I might come back to that."
        ],
        [
            "Engineer this time, but I'm not going to talk about it now.",
            "I want to show you the model that we get.",
            "OK, so this is that.",
            "Net 28 by 28505 hundred 2010 and we can give it an image like this one.",
            "And we can.",
            "That it recognize it.",
            "So all I'm doing now is running it this way.",
            "So having trained as a generative modeling and I run it as a sort of discriminative model.",
            "And it's stochastic, so these keep changing, but it has no doubt that it's a four.",
            "It's very sure it's a four.",
            "If I run it faster you can see the top level ones don't change that much.",
            "Or rather, there's some that are fairly stable like that one there.",
            "I can share some other digit, maybe that one.",
            "It's less sure about that, but it's most of the time it says an education.",
            "It thinks it's a three, which isn't so unreasonable.",
            "Or, you know, so it can recognize OK.",
            "In fact, it does quite a good job of recognition.",
            "There is very sure that survived only has one moment of doubt.",
            "I can give it a tricky one.",
            "At and it will express its doubt by keeping changing his mind.",
            "It happens to end on the right answer, which is an 8, but that's just coincidence.",
            "Actually, the fact that this isn't in the last column means that the most frequent answer gives is Nate, but only just.",
            "It can deal with various kinds of noise quite well.",
            "So it thinks this is a one or a second most of the time.",
            "Hum.",
            "But what's most interesting about this is when you run it as a generator.",
            "So I'm going to run the model as a generator.",
            "And what's going to be happening is this?",
            "I'm going to fix some label like 2.",
            "And I'm just going to go.",
            "I'm down between these units here with this fixed.",
            "So I'm just doing alternating Gibbs sampling in a restricted Boltzmann machine.",
            "That's all that's happening causally.",
            "That's where the action is.",
            "And what will happen is it will enter various brain states here represent things and it's impossible for you to tell what they represent because there's just activities of neurons.",
            "What you'd be interested in this mental state?",
            "And so I'm going to show you both the brain state here and the mental state which I'm going to show you here.",
            "You see, the way mental state the language of mental states works is this.",
            "If I want to tell you one of my brain states, I could try saying you're in 50.",
            "Three is on.",
            "Well, that didn't do you much good, did it?",
            "Or I could say there's a pattern of active neurons here, which are just like the pattern of active neurons you get.",
            "If I was looking at a pink elephant.",
            "Now that's a rather clumsy way of saying of this rather clumsy set of words.",
            "There's a pattern of their brain state, which is like the brain state I would get if I was looking at a pink elephant, OK?",
            "So we have a funny way of saying that we have shorthand for that which, which is something like saying I've got the person providing elephant.",
            "What that really means is that some brain stated by claim there's some brain state.",
            "That's like the one I would have if I was looking at a pink elephant.",
            "Now, as soon as you got a generative model, if I ask you what brain stages this then.",
            "Well, because I can generate from it, I can say the kinds of things assuming my inference works, I can say the kinds of things that would normally cause that, so I'm going to show you these brain states, but I can show you the mental states by showing the kinds of things that would normally cause.",
            "So as it runs, that's where all the action is.",
            "But just so you know what's really happening up here, I'm going to show you what it has in mind by going chunk chunk each time.",
            "OK, so that's where the action is.",
            "We're going to each time.",
            "And if you do it faster.",
            "You'll see that after a while it settles into generating twos and once he settled into generating tools it will just generate truth forever and then all sorts of different tools.",
            "Once we link from that loops, it'll generate even rather bad twos.",
            "Thank you.",
            "This isn't really running, it's a canned demo.",
            "But that's important that it can generate those 'cause it means you can recognize that their twos.",
            "So this is the best generative model or is 100 digits.",
            "I say that in every lecture given nobody's yet challenged me, and if I can say it long enough, it will become true.",
            "So I just changed the label here.",
            "And now it will generate eights, and once he settled into the ravine, it'll generate only eights.",
            "When we're thinking about this model.",
            "Is as a way of dealing with the fact that all real world data lies on low demand lies on or close to low dimensional manifolds.",
            "And the normal way to deal with that is to say well, if twos lie on a manifold, there's about 12 dimensional inside my machine.",
            "Let's have 12 numbers that represent each two and let's figure out how to get to these 12 numbers, maybe?",
            "Well, that's tricky.",
            "Maybe we'll do it.",
            "Nonparametrically will associate sort of numbers with each two somehow.",
            "Um?",
            "The problem with that is if it 2 has a very long tail, it's actually got more degrees of freedom 'cause there's also special 14 degrees of freedom is long tail.",
            "Then obviously somewhere in between.",
            "It's got 13 1/2 degrees of freedom.",
            "Also degrees of freedom at all, or none.",
            "There's things you can do with the two, like making a very wobbly too that make it not quite such a good too, but not as bad as turning random pixels.",
            "So that's sort of a degree of some freedom, but you know it's a bit bad to do that, but not too bad.",
            "What's more, the data might have many of these manifolds, and you don't know how many.",
            "In fact, it might be that there's 30 eleven of these manifolds, but if you go up in energy a bit, that is, if you allow things to be not quite so good to the merge like you might have sevens across sevens.",
            "But if you get a bit less discriminating, they might be sort of more or less in the same manifold.",
            "And so, rather than doing dimensionality dealing with low dimensional data by explicitly trying to represent where you are on the manifold and much better way to do it, I think is to take your data that lies on these low dimensional manifolds, blow it up into some high dimensional space.",
            "And in that high dimensional space have an energy function.",
            "Start off with everything, equal energy, so all the weights are roughly 0 and then gradually dig out ravines, low dimensional ravines.",
            "So if we say will work in this dimensionality, so 500 dimensions here, then in this 500 dimensional space we can integrate out these units so we can get what's called a free energy for these 500 dimension vectors and there's going to be 10 ravines, each of which is long and skinny.",
            "And has a few degrees of freedom along the floor of the ravine.",
            "And then lots of degrees of freedom up the side of the ravine.",
            "And in particular, the ravine for twos might in some places come quite close to the ravine for threes.",
            "But there might be quite an energy barrier there, 'cause we expect low density regions are behind you regions between digit classes.",
            "Actually, for some lines that's not true.",
            "The ravine force in the ravines for 9 sort of merge into each other.",
            "One point more or less.",
            "So what this model is learned by this sort of greedy learning and very fine tuning is to turn the pixels into these features.",
            "And the features have the property that by using these hidden units to express sort of energies of combinations of features, we can make ravines in this space that capture the manifold's if we tried to do it by interactions between pixels, we couldn't do it.",
            "You just can't express what are two is by a Markov random field on pixels.",
            "That is, you can write down the microphone field pairwise interactions in pixels such that the only happy states are twos.",
            "But as we've seen, we can do it if we use multiple layers.",
            "The only happy states of this system when you stick when you clamp that label 8's here.",
            "I'll do one more 'cause I like it so much.",
            "Yeah.",
            "Do.",
            "You can't generate from them.",
            "1.",
            "I'll repeat the question convolutional networks do the same thing.",
            "Well comedy show lyrics are good at recognizing digits, but convolutional neural networks won't allow you to generate from the model 'cause they don't have a generative model.",
            "If you think of the classic ones from the late 90s from the 90s.",
            "Now more recently has been doing more complicated things where he tries to combine this kind of learning with convolutional Nets.",
            "OK, so basically I've infected Youngs convolutional Nets and he's now doing stuff a bit like this as well.",
            "Yeah.",
            "OK um."
        ],
        [
            "So these are just some samples generated from the model where you run 4000 alternatives.",
            "Separate iterations between samples.",
            "So the roughly independent.",
            "And you can see that if you clamp zeros you get zeros, and if you clamp one, you get once and almost all of them look like the right class.",
            "There's a few sort of bad guys like this guy.",
            "But it's a very good generative model.",
            "And of course, what's happening is when I clamp when I climb one of those high level units to a 2.",
            "Then there's 2000 outgoing connections and the weights on those connections lower the energy of the two ravine and raise the energy of all the other ravines.",
            "So now if I wander around in that space, eventually settle into that ravine and stay there."
        ],
        [
            "And it's good to do recognition.",
            "So these are examples of cases that it wasn't sure about but actually got right, and so we can recognize all sorts of different tools.",
            "Tom."
        ],
        [
            "These are results from a few years ago.",
            "A support vector machine gets about 1.4%.",
            "This model, after a bit of fine tuning, but it's not discriminative functioning, gets 1.25 as we'll see later with discriminative fine tuning can do quite a bit better.",
            "I'm back, probably can't do better than 1.6.",
            "Well, you might be able to 1.59, but you basically can't do better than 1.6 K nearest neighbors about that.",
            "So you get some idea of how tough it is.",
            "And there's lots more results in the car, some which are much well below 1%.",
            "But those are ones that use prior knowledge about pixels about like convolutional neural Nets know about pixels being near one another about translation invariants at least locally.",
            "This is a pure machine learning approach where you don't tell anything about pixels, you just give it a big vector and say these vectors have these labels.",
            "How well can you do so?",
            "I call that the permutation invariant version of M list 'cause these will also be just the same if I took the data set.",
            "And permitted the order of all the pixels and then fed it to the program.",
            "Was anything with the prior about special locality will get screwed up by that?",
            "In a sense, you might think this is doing some of that sort of.",
            "This is doing that by learning these local features.",
            "But yes, you could.",
            "But if you take most pure machine learning approaches like support vector machines.",
            "You just feed him to this.",
            "You get results like this."
        ],
        [
            "So if you take Jahns Nets, convolutional neural Nets trained by Marco really owns Ranzato.",
            "Using backpropagation, the best results yet, so about .49%.",
            "If he does unsupervised layer by layer pretraining followed by back propagation, he can do better and this is a record for a single method.",
            "You can do better than that by averaging methods, but for a single method I think this is the best service."
        ],
        [
            "So.",
            "I'm now going to give you a different view of why this layer by layer won't learningworks.",
            "That depends on equivalence between different kinds of networks, and I think this is if you want to understand what's going on when you stack up these bolts for machines, I think this is by far the best way of thinking about it."
        ],
        [
            "So here is.",
            "A directed acyclic graph, a directed belief net or directed Sigma belief net.",
            "This got a funny property.",
            "Between every pair of layers, it learns the same weights, or rather it has given the same weights.",
            "So this weight matrices transpose of those.",
            "And these two layers can be different sizes, but this leads the same size.",
            "Is that now in this less the same size as that man?",
            "This looks the same size this size that there so.",
            "I call this V1 and I call this V2.",
            "That's sort of a joke.",
            "That's what the visual systems like.",
            "But really what I mean is these are hidden units that have the same size as these units.",
            "And now I'm going to convince you that this infinitely deep.",
            "Sigma belief network.",
            "Is actually the same thing as a restricted Boltzmann machine.",
            "OK, they are the same model.",
            "They're just different ways of writing the same model.",
            "So first of all, let's try generating data from this.",
            "To generate data from this, you start a long way up here.",
            "Um, and you compute PV given HPHMVPV given HP HP envy, PV, given action so on where the PV given H is just.",
            "Given the state of the hidden unit should input to the visibles, put him through the logistics sample that spear vegan message.",
            "So if I keep doing that, that's exactly the Markov chain I was running to let the restricted Boltzmann machines sample from his equilibrium distribution and therefore the distribution.",
            "I'll get here.",
            "If I generate that way is exactly the same as the distribution I get if I run this alternating Gibb sampling.",
            "This is also heating up something.",
            "So they define the same distribution.",
            "Now the next thing is, how do I do inference in this model?",
            "OK, so I've got this directive belief net and remember with belief Nets you get explaining away.",
            "So when I try and infer the states of these given these, I can get explaining away that are going T is going to correlate things and surely inference is going to be complicated.",
            "Solo Generation is easy, except you had to start a long way up.",
            "Inference is going to be complicated.",
            "You thought.",
            "Well actually no.",
            "This is a very special kind of.",
            "Directly belief net in which inference is just as simple as generation, in fact is actually the same processor generation."
        ],
        [
            "So to do inference, what we're going to do is we're going to take the vector here.",
            "We're going to multiply by the transpose of those weights, and we're going to put it through logistics function.",
            "We're going to sample assuming all these are independent.",
            "That's just what we did with the restricted Boltzmann machine, right?",
            "Um?",
            "And that's going to work in this net.",
            "And that's what's exciting that you can do exact inference in a deep directed belief net.",
            "Becausw, but if you think about it, the fact that these weights are the same as these weights means that if I have, say, unit here.",
            "With big positive connections coming from these two units, then the equivalent unit up here will have big positive connections going to those two units.",
            "And if you think about the earthquake in track example, the reason you get explaining ways 'cause if I observe that that's on.",
            "Then probably, let's suppose this had a big negative bias.",
            "If I observe it's on, then one of those needs to be on to make it on.",
            "And so they become anti correlated.",
            "'cause you're only one of these to be able to make that up.",
            "So they become anti correlated.",
            "So in the likelihood term coming from the data, these are anticorrelated.",
            "But now, because these weights are the same as those weights in the prior term coming from here, these are positively correlated, because if that turned on, it would make both of these come on.",
            "So there's sort of positive correlation coming from there.",
            "And there's an anti correlation coming from here.",
            "And if we were really really lucky they would exactly cancel to make these independent.",
            "And it turns out we are lucky whatever vector you put there, as long as these things are in the exponential family, then when you do this infinite director belief that all this stuff upstairs, all of these weights up here will have constructed a prior such that.",
            "If I were to generate down to there, I get some correlation here, and that exactly cancels the anti correlation I get from that new term.",
            "So these really are independent.",
            "So now we've seen a way of making inference simple.",
            "Inner directed belief net that people haven't really thought of before, which is.",
            "Then try and do the difficult inference using Monte Carlo.",
            "Don't just assume independence.",
            "Actually stick in weights up here such that these really are independent.",
            "And I can do that simply by having an infinite net with these weights.",
            "Tide to those weights.",
            "Now one thing gets a bit more complicated, which is you remember, I told you that if I can get it so I can get a sample from the posterior really easily now.",
            "I take this state.",
            "I multiply the transpose of those weights.",
            "I put it through the sigmoid.",
            "I sample boom, I got a sample from the posterior here.",
            "I do the same item from posterior here so I can just run the change in this direction, getting sample from."
        ],
        [
            "Posterior all these less, and to learn the weight from SJO to SIO.",
            "What I do is I take the state of SJ in the posterior.",
            "Times the state of SI in the posterior in the data.",
            "Minus the predicted state press I.",
            "So my learning rule is that the change in the way it is proportional to the state of SJ times the difference between the state of SI and the prediction for the state of SI will use a hat for that.",
            "And that should be a probability.",
            "But if we could sample a binary thing according to that probability, that would do, but it would have a learning rule that was right in the expected sense.",
            "Right on average.",
            "So can we get a sample from this?",
            "So what we want is, given the binary states of these units in the posterior, what do they predict?",
            "This guy ought to be doing?",
            "Well, here's an easy way to do it.",
            "Take the binary states of these guys in the posterior.",
            "Put him through the weights.",
            "You see that same as those weights.",
            "Put him through the sigmoid and sample.",
            "So this S I-1 is a sample from this.",
            "It's a sample of that probability.",
            "Therefore we can stick it in there instead, we just created a bit of noise and so that's the learning rule.",
            "For the wait on the connection rest JOSI one.",
            "Very simple learning.",
            "But that weight is the same as the weight on the connection with Messi 12S J 0.",
            "So we need to apply the learning rule there too.",
            "And again, to get the predicted value, here we use the sample value there, so we get the learning like that.",
            "And then for this way here we get along like that when you add 'em all up you get a telescoping sum where you'll notice the SJOSI one has a - here and the SJOSI one as a + here.",
            "So they all cancel.",
            "Then you end up with this term.",
            "Minus that term, which is the Boltzmann machine learning rule.",
            "OK, so that's how it all comes out consistent.",
            "Um?",
            "But now suppose the weights were small.",
            "We start off with small bites.",
            "By the time we've run a few steps of inference here, the Nets close to its equilibrium distribution.",
            "OK, so now suppose you take a natural is equilibrium distribution and you sample.",
            "And you ask it, given these sampled values, what would you like to do with your parameters to make those values more likely?",
            "Well, if you take enough sample values if there from the equilibrium distribution, the net will tell you.",
            "I don't want to change my parameters at all.",
            "My parameters are just perfect for producing those sample values.",
            "So you know that once this Nets got close to equilibrium, the sum of all the derivatives up here have to come to approximately 0.",
            "So you can save yourself a whole lot of time.",
            "You can actually do something bit more sensible than we actually do, which is sort of trying to assess whether it's got close to equilibrium.",
            "Maybe by looking at the magnitude of the derivatives.",
            "But basically, let's take a really crude approximation and say, let's assume that after 2 steps there's already quite close to equilibrium, and that's certainly true when the weights are small.",
            "Then we only need to take these derivatives and these derivatives.",
            "That is, these guys in these guys and if you add up that that what you see is you get SJOSI 0 -- S I 1S J one other time cancels so that's the learning rule I've been using.",
            "That's what we call contrast emergence and basically ignoring all the derivatives up here.",
            "Obviously, as you Start learning, it might be better to go a few more steps back.",
            "And if you want to do really accurate learning, you just do more steps as you go.",
            "That's the sort of sensible thing to do, or a sensible."
        ],
        [
            "So what we're doing then when we learn a deep belief net.",
            "Is we have this model is equivalent to this model?",
            "We learn these weights using our approximation.",
            "Having learned those weights."
        ],
        [
            "We then freeze them here.",
            "We take the aggregated posterior distribution we get there.",
            "We try to model about, which involves learning these weights.",
            "But with all these weights together, so we're really learning this infinite net.",
            "And we keep doing that until we get bored."
        ],
        [
            "Um?",
            "Yeah, it's very hard to know how many layers you should use.",
            "Zoom in will tell you that you shouldn't use too many layers according to his because he was only using a rather small net.",
            "Because he was doing it so cleverly."
        ],
        [
            "My belief is that actually these things are very robust and you got at least something for evolution to do, and what evolution is going to do is decide roughly how many lesson, how many units?",
            "OK. Now there's one little snag to this whole argument, which is that."
        ],
        [
            "But if you get back here.",
            "When these weights were the same as these weights.",
            "This stuff up here constructs a prior distribution there that's exactly complementary to the likelihood term, so I can do exact inference by taking this binary vector.",
            "Multiplying by these weights and putting it through the sigmoid.",
            "But once I start changing these weights.",
            "Then if I use the transpose of these weights, which are no different, it won't be doing exact inference.",
            "So I'm going to lose 'cause I'm not doing inference, right?",
            "However, when I change these weights, I'll be getting a better model of the aggregated posterior here, so I'm going to win 'cause I get a better model of the aggregated posterior.",
            "And the question is.",
            "How does what I lose by doing inference wrong compared with what I win by getting a better model?",
            "And the answer is you win more than you lose when you write down the variational bound.",
            "So when I change these weights.",
            "I get a better variational bound.",
            "And even though my inference isn't quite right anymore, is still pretty good, it started off being perfect and I'll change the way to it so it's not quite perfect, but it's still the case that these are almost independent.",
            "Given the weights above, that's what typically happens."
        ],
        [
            "That's all proved in this paper, which is the sort of basic paper."
        ],
        [
            "This stuff.",
            "No, I want to quickly.",
            "Let's see how much more I got to do.",
            "Yeah I have time.",
            "So this not running the Markov chains with Librium has a worry which is once the weights get big.",
            "You don't move very far from the data and the might be huge low energy regions that you never visit.",
            "And of course, those will mean that the probability of things you do is actually very low under the models distribution 'cause the models putting all this weight on these other things that you never see.",
            "Because you never see them, you don't realize they're there and you think you've got a good model, but you haven't.",
            "Um?",
            "It's amazing that, given that that could happen, it doesn't completely screw you up.",
            "In general, it doesn't completely screw up and you can learn just fine with this sort of one step CD.",
            "But there's another thing you can do, which is what Mark Radford Neal did to begin with.",
            "Which is to say, well, instead of running my chain by starting at the data each time, why don't I do the following?",
            "Um?",
            "I'm going to have a number of Markov chains.",
            "And these Markov chains, I don't keep reinitializing at the data, so this is really CD at all anymore.",
            "We just keep running these Markov chains and as I change the weights I sort of change the weights in the Markov chain I'm running so.",
            "So each time I update the weights, I run each of these Markov chains a little bit further.",
            "Now, if the Markov chain was already close to equilibrium and I didn't change the weights much, it will stay close to equilibrium.",
            "So as long as I change the weight slowly enough, all these Markov chains are running would be close to the close to equilibrium, so they'll do just fine for estimating this sort of VHJ.",
            "According to the model, that is, when the models at equilibrium.",
            "And so I have these persistent chains for estimating that.",
            "And then for estimating the VHA with the data, I just take the day to activate the hidden units and just look at the correlation.",
            "That's a nice easy thing to do.",
            "That was the original idea of both machines that you would you would be awake and you look at things and estimate these correlations, and then you go to sleep and you'd imagine things by running this Markov chain for a long time, and you measure the correlations and then you take the difference in those two statistics.",
            "And that's how you learn.",
            "So actually the idea was good.",
            "As you're asleep, you measure these model statistics and then the next day you change your weights in proportion to the difference between the statistics you measure when you're awake and the model stresses you measure when you're asleep.",
            "So that's how you actually work.",
            "According to that theory, which may not be right.",
            "We can try now.",
            "Implement that learning algorithm.",
            "And it turns out it works very well and it works very well.",
            "Not at all, for the reason that you might think.",
            "OK.",
            "So one thing you can think about this chain is doing something very nice.",
            "Simulated annealing 'cause you start off with very small weights where it's easy to reach equilibrium.",
            "And as you learn the weights get bigger.",
            "So if you look at this chain it looks like very like a simulated annealing chain where the temperature is getting lower, and in fact if you run a persistent chain for a long time during learning.",
            "It can end up closer to equilibrium than if you started with the final weights and ran for the entire length of time you learn for.",
            "To try and get to equilibrium.",
            "Because it was doing this sort of annealing thing started with small weights and gradually using big ones.",
            "But it turns out you can learn much faster than that analysis will predict.",
            "And in fact, you only need about 100 of these fantasies.",
            "You can actually do with just one of these chains, which is very surprising.",
            "And there's a reason why the learning works much better than you'd expect.",
            "I'm not gonna, yeah.",
            "So what's happening when you do learning is you're changing the weights by you raise them by the statistics you measured with the data.",
            "And you lower them according to the correlations you measured in these Markov chains that you're running to estimate what the model believes in.",
            "What that means is.",
            "If you think of these Markov chains like fantasy particles moving around an energy surface.",
            "Wherever a fancy particle is, the learning will say raise the energy there.",
            "And that makes the fantasy particles go somewhere else.",
            "And so you can't get trapped in local Optima.",
            "OK, if you kept the weights the same, you might have some deep local Optima and the particle might just sort of sit here trying to jump out, but it's very high energy barriers that stays there.",
            "But if you're learning and you're using this particle for learning, if it's stuck in a minimum."
        ],
        [
            "Roger that minimum is going to keep going up until it falls out of the minimum.",
            "So there's a funny interaction between the learning algorithm and these Markov chains.",
            "That has the effect of making the Markov chains mix extremely fast.",
            "And so all the analysis statisticians do that say if you go slowly enough, blah blah blah.",
            "That's not why it's working, it's working because the learning is making very fast mixing for these Markov chains by changing the weights.",
            "So think of it like this.",
            "Suppose the green particles are what you measured with data.",
            "And the red particles where your Markov chains are.",
            "So what you're doing is you're going to lower the energy.",
            "For the data and you're going to raise the energy for their fantasies from the model, and since there's more particles from the model here than there is data, this minimum go up.",
            "And it will go up until these particles spill over into you.",
            "So actually you don't have to solve the problem of how can I jump this energy barrier?",
            "You're going to change the energy landscape so things don't stay trapped.",
            "And that means this works really nicely, yeah?",
            "It's not really mixing.",
            "Exactly very good.",
            "Yes, exactly.",
            "It's not mixing, it just looks like fast mixing.",
            "Yes, that's exactly right.",
            "Yeah, will you that referee know OK?",
            "OK.",
            "So."
        ],
        [
            "What I've said so far.",
            "Is that these restricted Boltzmann machines?",
            "Provide your simple way for learning features one layer at a time.",
            "If you run the Markov chain properties, get fantasies from the model.",
            "And you did that.",
            "Starting from scratch all the time, we're very computationally expensive, but actually you can if you start at the just running short time, it works pretty well.",
            "You can build many layers of representation that way.",
            "And you can think of what you're doing is taking these little models and composing them.",
            "And that creates very good generative models.",
            "The generative models can be fine tuned, which I haven't told you much about.",
            "And.",
            "Once you've done all that, you can actually also do discriminative fine tuning, which I'll tell you about after the break, and that makes them work even better.",
            "And.",
            "The break is going to last till my next lecture which is going to be tomorrow.",
            "In sorry it's going to be on Wednesday and it's going to be afternoon.",
            "I think if I get the time right, I might actually turn up.",
            "OK so I finished."
        ],
        [
            "This time for a few questions.",
            "Yeah.",
            "Earlier.",
            "Warning against doing maximum likelihood right?",
            "Especially if we have a huge number of parameters that seems to be exactly what we're doing.",
            "Well, I'm trying to.",
            "I'm trying to we don't actually achieve my ideal would be to do maximum likelihood, yes?",
            "Why would you want to do?",
            "Should be concerned about.",
            "OK.",
            "So you thought, given that I'm doing maximum light and not even doing that.",
            "I mean that's my idea.",
            "I would overfit horribly, but actually the kinds of tasks I'm working on.",
            "First of all, I'm modeling the data rather than the labels.",
            "There's much more information and image than in the labels.",
            "Secondly, I do actually overfit, but not very much.",
            "And Thirdly, I think there's two sets of concerns.",
            "One concern is can you sort of fit a reasonable model at all in any way.",
            "Fitting it will do for me, as long as you can fit it.",
            "And then on small datasets on big computers.",
            "Can you do something better than maximum likelihood so that first slide I had about two kinds of problems for AI problems?",
            "If you do maximum likelihood fitting of a really complicated model, and you could actually do it, that would be good enough for me.",
            "It's a two different regimes, right?",
            "And I think this regime where you have not much data, lots of noise, a big computer and you want to be just as Bayesian as possible and you want sort of infinities that you don't actually use and.",
            "All that stuff.",
            "But you don't.",
            "That's not the name of the game when you're trying to model complicated real data that doesn't have much noise in.",
            "It's important when you have come across some really neat idea to understand over what range that neatness can be applied and not to generalize it too far to two.",
            "For example, real problems.",
            "Zubin maybe comment, then you can respond.",
            "So in in the case of images where you have 28 by 28 pixels and every image has every pixel observed, and that's that's one kind of real world example where you get a lot of bits of information from your data set, and so maximum likelihood can work extremely well right in.",
            "And I know you have text examples as well, so you'll show some of that.",
            "But in examples like text which are also real world data, you could have vast amounts of real world data, but very few measurements.",
            "Our observations of particularly rare words and things like that, right, right?",
            "So in fact, you could have someone you've never seen a tool and you have to worry about those two, right?",
            "So so those are both situations where we have real data, but it doesn't mean that you could just do maximum likelihood ngram models, for example, protects it wouldn't work very well, right?",
            "Maybe not, although actually restricted Boltzmann machines if you apply them to bags of words, give you better density than if you apply all these fancy topic models.",
            "I'm not talking about the language language modeling, I'm talking about rusty stuff using our BMS to model bags of words.",
            "I agree, if you're doing language modeling you'd like to have an average between.",
            "I mean, of course.",
            "I believe averaging different kinds of models is always a good idea.",
            "But actually, for the stuff they use topic models for, which is I give you lots of bags of words and you try and learn stuff about what the topics are.",
            "It actually works quite a bit better in terms of.",
            "I'm giving high density 2 bags of words to learn restricted Boltzmann machines instead.",
            "That's coming out in the next nips.",
            "Yeah.",
            "Santa multivalued.",
            "And also, how do you choose a number of layers?",
            "OK, you can take binary units and you can turn them into sort of softmax or multinomial units.",
            "That's just fine.",
            "You can use Gaussian units if you connect getting used to binary units, that's OK, although you have to use low learning rate if getting used to getting units, things can blow up because you can get improper models.",
            "You can still learn there, but you have to be very careful and you have to sort of watch it beginning to blow up and learn not to do that, or you can put in some extra stuff to stop that happening.",
            "So basically the rule is you can use things in the exponential family.",
            "And it's all fine.",
            "With a few little visas.",
            "The question about how many layers and how big each layer?",
            "Yes, that's where this sort of doing the right Bayesian things to integrate over structures wins.",
            "It can decide how many layers and how many things in each layer.",
            "What we're doing is we're just trying a few things.",
            "Um, and you know you try 500, you try 1000, you try 2000.",
            "They work pretty well, so it's not like it's very sensitive to that.",
            "Modeling.",
            "You could do all that, the trouble is.",
            "It's hard enough to do an approximation to maximum likelihood learning.",
            "If I tell you how many layers and how many feet things perler.",
            "If you now try and do the search over all these structures as well.",
            "And if you try to do it properly, it's just a hopeless big computation.",
            "And all I want is something that works.",
            "It's not like we're trying to identify a real system, right?",
            "We you know these objects in the world, and I'd like to recognize them.",
            "And it's not like there is a correct net.",
            "Is not like the objects in the world really were generated from one of these Nets, so we're not doing exact identification where you'd like to see if it really is 10 hidden units or 11.",
            "You're in a situation where a few billion neurons will do it.",
            "If only you could shoot him right.",
            "Reading this and could take you in directions that you don't want it.",
            "I mean, does it always average out to the right gradient or?",
            "Is there reason to believe that?",
            "Well, sort of.",
            "If you got number assessment of the gradient, then on average it will do good things.",
            "So in the limit when you use very small learning rates, it'll average out and you're going the right direction.",
            "If you use big learning rates, of course, crazy things could happen.",
            "And typically what you want to do is push the learning rates to be about half as big as one crazy things happen.",
            "It's all very unsatisfactory for theorists.",
            "It just happens to work better than the other methods.",
            "Yep.",
            "So it's actually quite cool, but my question is thank you.",
            "Are these in this state?",
            "Is it really the right application?",
            "Because I think the customer so they knew that he was just a polynomial kernel this SVM and they get nearly as well.",
            "So are there domains of problems where I can really gain?",
            "A lot compared to classic in discriminative learning, right?",
            "So that's a very good question.",
            "I sort of completely agree with the sort of intent of it.",
            "Now, one reason for using Emnace to begin with is that there's lots of results on M list, and you can tell whether you've got a method that sort of works well or doesn't.",
            "So you can sort of like things that really don't work very well that way, but you're right, what you'd like is to get a really big win when you'd like to get it on big datasets with more realistic images, I'll talk about a little bit of that after the break.",
            "I mean, that is in two days time.",
            "And that's the direction in which we're trying to go, obviously, and that's when you should really believe it when we can get much better so already on 3D objects.",
            "So Jan has a database called the Norm database.",
            "That little plastic toys but seen from many different viewpoints with different lighting conditions.",
            "We can do much better than this films on those.",
            "So I'll talk about that.",
            "Yeah.",
            "Yes, yes, I will talk about that on Wednesday too.",
            "I'll show you some really cute models, right?",
            "Yeah.",
            "Chris continuous latent variables.",
            "No, I don't think so.",
            "I think if I mean one problem we have is if you have Gaussian and Gaussian you try to do this, it can become an improper model.",
            "So you have to do something about that.",
            "It would be a very sensible thing to do, but you might, for example, say just slightly inspired by how the brain works.",
            "You might say let's have positive only variables, but let them have approximate real values.",
            "And there's a neat way to hack those variables up.",
            "And which I think I will talk about on Wednesday.",
            "Where you can show that a whole collection of binary variables can be viewed as a rectified linear unit.",
            "And that works very nicely.",
            "I'm.",
            "But yeah, it's this sort of complete, which is.",
            "It's silly to be using binary hidden variables for representing continuous things.",
            "Unless of course, your learning algorithm works so much better with these binary variables that it works better that way.",
            "Then when you use continuous hidden things, and that's sort of roughly the state wherein.",
            "These binary hidden variables work very well.",
            "The easy to learn.",
            "And they seem to work just fine for modeling all sorts of continuous stuff when the observable variables are Gaussian or revenue.",
            "But we are working towards having things that aren't quite binary like question things.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk overall for like 3 hours about a particular view of learning and a particular way of doing it, which I called deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "And what you're going to learn over these three hours is how to learn.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multi layer generative models one layer at a time, which is much easier than learning the whole model all at once.",
                    "label": 1
                },
                {
                    "sent": "And how you can make them fancier by within each layer having lateral interactions and implementing Markov random field.",
                    "label": 0
                },
                {
                    "sent": "How, once you've learned these multi legendary models you can use them for sort of typical machine learning things like discrimination or classification or regression and how you can actually use these multilayer generative models as kernels for Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "But kernels that adapt so you can actually learn the kernel.",
                    "label": 0
                },
                {
                    "sent": "How you can do nonlinear dimensionality reduction on very big datasets, particularly how you can reduce it down to a very small binary vector that can be used as an address so that you can convert objects into addresses so you can find other similar objects very quickly so you can use hash coding for things that are just similar instead of things that are identical.",
                    "label": 0
                },
                {
                    "sent": "Um and how you can apply this just to control data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to start off by making a contrast between statistics and AI, and I think of myself as doing AI.",
                    "label": 0
                },
                {
                    "sent": "This isn't really true anymore, right?",
                    "label": 0
                },
                {
                    "sent": "But ten years ago, maybe statistics was typically fairly low dimensional data with lots of noise and not much structure, and the problem was to avoid modeling the noise.",
                    "label": 1
                },
                {
                    "sent": "Avoid thinking you've got a model of financial data when you haven't, and things like Gaussian process is extremely good for that.",
                    "label": 0
                },
                {
                    "sent": "And then AI is typically dealing with very highly structured data, very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "So things like big images or video sequences.",
                    "label": 0
                },
                {
                    "sent": "And it's not typically very noisy.",
                    "label": 1
                },
                {
                    "sent": "That is, there's lots and lots of structure.",
                    "label": 0
                },
                {
                    "sent": "Then if you could only represent it right, you could see all that structure.",
                    "label": 0
                },
                {
                    "sent": "Learn all that structure.",
                    "label": 0
                },
                {
                    "sent": "So the problem isn't to avoid getting confused by noise.",
                    "label": 1
                },
                {
                    "sent": "So you don't have to worry too much about using distributions over parameters instead of just one set of parameters.",
                    "label": 1
                },
                {
                    "sent": "You're not trying to average away all the noise, you're just trying to find some way of representing all this structure so that you can learn it.",
                    "label": 0
                },
                {
                    "sent": "That is a representation in which learning will work.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to go through the history of Europe.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works in three slides.",
                    "label": 0
                },
                {
                    "sent": "There were perceptrons in the 60s and you typically hand coded some features which you could have done within your own natural.",
                    "label": 0
                },
                {
                    "sent": "They could just be hand coded so you can image you got some features.",
                    "label": 1
                },
                {
                    "sent": "You learn the weights from the features to decision units.",
                    "label": 1
                },
                {
                    "sent": "And for funding you would use decision units like this and they.",
                    "label": 0
                },
                {
                    "sent": "It was a good algorithm for learning.",
                    "label": 1
                },
                {
                    "sent": "The weights is still used by Google a lot, but.",
                    "label": 0
                },
                {
                    "sent": "All the difficulties in coming up with the right features to make it possible to discriminate with one layer of weights.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second generation of neural networks.",
                    "label": 1
                },
                {
                    "sent": "We tried having multiline Nets with continuous input output functions, logistic functions, and then what you would do is you put in, put in.",
                    "label": 0
                },
                {
                    "sent": "You can output.",
                    "label": 0
                },
                {
                    "sent": "You take some measure of how different this was from.",
                    "label": 0
                },
                {
                    "sent": "I didn't mean measure in the technical sense some some way of saying what the discrepancy was between this and the desired output, and then you would use the chain rule which we discovered in the 1980s for sending derivatives backwards so that you could figure out derivatives for all these weights and then you change your little bit.",
                    "label": 0
                },
                {
                    "sent": "So it should be better getting right answer and the problem with this is it was I can now say it was a huge disappointment.",
                    "label": 1
                },
                {
                    "sent": "Galico managed to get it to work quite nicely for convolutional neural Nets.",
                    "label": 0
                },
                {
                    "sent": "But on the whole, when you train lots of hidden layers like this, it's very hard to get it to learn, and it never really did do that much compared with what we expected of it.",
                    "label": 0
                },
                {
                    "sent": "And so support vector machines came along and sort of pretty much wait this out within the machine learning community, and I want to show you what's wrong with this and what's wrong with it is you're trying to get all the information from the labels.",
                    "label": 0
                },
                {
                    "sent": "And a label.",
                    "label": 0
                },
                {
                    "sent": "Typically, unless you've got a huge set of possible labels, doesn't contain much information.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Oh well, I won't go into that.",
                    "label": 0
                },
                {
                    "sent": "You know about support vector machines right there?",
                    "label": 1
                },
                {
                    "sent": "Just to kind of perceptron with a clever way fitting it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "Back propagation.",
                    "label": 1
                },
                {
                    "sent": "Big datasets are unlabeled, so the fact that all the action comes from the labels means you can't use unlabeled data unless you do something else.",
                    "label": 0
                },
                {
                    "sent": "The learning time doesn't scale very well with multiple layers.",
                    "label": 0
                },
                {
                    "sent": "Typically, if you start the weights off small as you back propagate through the layers, you get a small times a small times are small so you get tiny derivatives.",
                    "label": 0
                },
                {
                    "sent": "If you start the weights off big, you've already decided what part of the space you're going to be in, and you wanted the learning algorithm to do that so.",
                    "label": 0
                },
                {
                    "sent": "To get it and they also don't get very good local Optima when they have deep Nets.",
                    "label": 0
                },
                {
                    "sent": "As we'll see later, you can get much.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Better Optima.",
                    "label": 0
                },
                {
                    "sent": "So the idea is we want to keep what's good about backpropagation, which is stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And we want to get rid of what's bad, which is everything.",
                    "label": 0
                },
                {
                    "sent": "Depending on the labels.",
                    "label": 0
                },
                {
                    "sent": "And so instead of learning the probability of a label given an image.",
                    "label": 0
                },
                {
                    "sent": "We're just going to learn the probability of image.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn a density model of images.",
                    "label": 0
                },
                {
                    "sent": "And now we don't need labels.",
                    "label": 0
                },
                {
                    "sent": "And what's more, each image has much more information in it than a typical label.",
                    "label": 0
                },
                {
                    "sent": "Images can be big and there's lots of pixels, and so each image puts a lot of constraint on identity function.",
                    "label": 0
                },
                {
                    "sent": "Whereas if I give you an image and a label and I try and get the right answer, I don't get much constraint on the mapping from image to label.",
                    "label": 0
                },
                {
                    "sent": "The bits of constraint on that mapping imposed by training examples, just the number of bits it takes to say what the answer is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Isn't very many.",
                    "label": 0
                },
                {
                    "sent": "So now the question is what kind of generative model should we learn?",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "As you all know, I think there's been a big level.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mission in statistics and AI.",
                    "label": 0
                },
                {
                    "sent": "And there's a kind of model which is a directed acyclic graph.",
                    "label": 1
                },
                {
                    "sent": "And there's been lots of work on how you learn these and how you do inference in these.",
                    "label": 0
                },
                {
                    "sent": "If their sparsely connected, if each node only has a few parents, there's clever exact inference algorithms.",
                    "label": 0
                },
                {
                    "sent": "If each node has many parents, influence gets much trickier.",
                    "label": 0
                },
                {
                    "sent": "So in these I'll be using directed Nets, in which the observations are at the leaves at the bottom of the iris.",
                    "label": 0
                },
                {
                    "sent": "And in that kind of net.",
                    "label": 1
                },
                {
                    "sent": "Then the inference problem is, if I you know all the parameters, that is, you know how the states of these nodes determine the distribution for this node.",
                    "label": 1
                },
                {
                    "sent": "Inference problem is figuring out what the states of these nodes probably were.",
                    "label": 0
                },
                {
                    "sent": "When you see the data and the learning problem is figuring out weights on these connections that give you some parameterized function for the probability distribution.",
                    "label": 0
                },
                {
                    "sent": "Here, given the states of these guys.",
                    "label": 0
                },
                {
                    "sent": "Typically use binary units, but.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can generalize that later.",
                    "label": 0
                },
                {
                    "sent": "So the units I'll use adjust stochastic binary units where you get some total input.",
                    "label": 1
                },
                {
                    "sent": "From other units.",
                    "label": 0
                },
                {
                    "sent": "So when you're doing generation this will be top down input.",
                    "label": 0
                },
                {
                    "sent": "And the probability of turning unit on is just the logistic function of this total input.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so now if all the units are like that.",
                    "label": 0
                },
                {
                    "sent": "So Radford Neal introduced Nets like this in about 1992.",
                    "label": 1
                },
                {
                    "sent": "Full units are like that.",
                    "label": 0
                },
                {
                    "sent": "It turns out it's easy to learn the parameters if you can compute the posterior distribution over these hidden causes.",
                    "label": 1
                },
                {
                    "sent": "So I show you a data vector that's an observation.",
                    "label": 0
                },
                {
                    "sent": "If you can compute the full posterior here, or even if you can just get an unbiased sample from that posterior, learning is easy.",
                    "label": 0
                },
                {
                    "sent": "So I show you data vector if you can give me back an unbiased sample that is some binary vector that's the sort of plausible way in which this stuff might have caused that stuff.",
                    "label": 0
                },
                {
                    "sent": "Then learning is very easy.",
                    "label": 1
                },
                {
                    "sent": "The difficult thing is getting this unbiased sample from the posterior.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the learning rule.",
                    "label": 1
                },
                {
                    "sent": "Suppose we had an unbiased sample from the posterior, so I gave you some data down here.",
                    "label": 0
                },
                {
                    "sent": "My unbiased sample from the posterior gives me binary states for all these units.",
                    "label": 1
                },
                {
                    "sent": "And the learning rule is just.",
                    "label": 0
                },
                {
                    "sent": "For the weight on this connection, what I need to do is compute the difference between the actual state of this guy and the unbiased sample from the posterior.",
                    "label": 0
                },
                {
                    "sent": "That's SSI here.",
                    "label": 0
                },
                {
                    "sent": "That's a one or a zero.",
                    "label": 0
                },
                {
                    "sent": "And the probability with which this guy will be turned on by the generative model.",
                    "label": 1
                },
                {
                    "sent": "Given the actual states of these guys in the posterior, so that's all these.",
                    "label": 0
                },
                {
                    "sent": "SJS here.",
                    "label": 0
                },
                {
                    "sent": "So the probability of turning this on P is just the logistics of the input it gets from all these actual states here.",
                    "label": 0
                },
                {
                    "sent": "And it turns out the maximum likelihood learning rule is 2.",
                    "label": 0
                },
                {
                    "sent": "To follow the direction in which you're increasing the log problems, the observed data down here is to change the weight in proportion to the learning rate times the state of this guy times the difference between the state of that guy and what you would have predicted.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple learning rule.",
                    "label": 0
                },
                {
                    "sent": "And all the problem is getting this sample from the posterior.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the reason there's a problem is probably you will know is explaining away, so here's a very simple sigmoid belief net.",
                    "label": 0
                },
                {
                    "sent": "This actually happened to me in California.",
                    "label": 0
                },
                {
                    "sent": "I was asleep one night in the house, jumped and I assume the trucker hit it.",
                    "label": 0
                },
                {
                    "sent": "If I be in California and I probably have assumed it was an earthquake, which is what the real reason was, and this network says houses don't just jump, the probability that is about each of minus 20.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Trucks don't hit houses, but that probably is only to the minus 10 and you don't get earthquakes.",
                    "label": 0
                },
                {
                    "sent": "That's only reason minus 10.",
                    "label": 0
                },
                {
                    "sent": "So if you see the house jump.",
                    "label": 0
                },
                {
                    "sent": "We're assuming these are independent events in the prior, but these events, which were independent, suddenly become highly correlated.",
                    "label": 0
                },
                {
                    "sent": "When I see the house jump.",
                    "label": 0
                },
                {
                    "sent": "'cause I don't want to sort of pay 20 Nats to suppose that the house just jump for no reason but much rather pay 10 Nats to suppose this and then only one bit to say the House job 'cause it's got an even chance of jumping there 'cause this plus 20 here.",
                    "label": 0
                },
                {
                    "sent": "But it be very stupid.",
                    "label": 0
                },
                {
                    "sent": "Assume a truck hit the house and there was an earthquake.",
                    "label": 1
                },
                {
                    "sent": "'cause that's going to cost me 20 knots.",
                    "label": 0
                },
                {
                    "sent": "That's just as bad as assuming those jump spontaneously, so.",
                    "label": 0
                },
                {
                    "sent": "So even though these are very rare and uncorrelated events in my model.",
                    "label": 0
                },
                {
                    "sent": "Given that there has jumped, they're very highly anti correlated.",
                    "label": 1
                },
                {
                    "sent": "That's what Pearl called explaining away.",
                    "label": 0
                },
                {
                    "sent": "So the posterior distribution looks something like this over the four possible combinations of binary states here.",
                    "label": 1
                },
                {
                    "sent": "And that's what makes it difficult to figure out what all these hidden guys are doing.",
                    "label": 0
                },
                {
                    "sent": "Given that you made an observation, you can't separately sort of say how plausible is this, given that the House jumped.",
                    "label": 0
                },
                {
                    "sent": "It depends on what that's up to.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now it gets much worse if we want to learn one of these multilayer sigmoid belief Nets.",
                    "label": 1
                },
                {
                    "sent": "We have a kind of likelihood term here that saying given the data, what's that telling me about the probable states of these hidden variables?",
                    "label": 0
                },
                {
                    "sent": "But we also have all these extra hidden layers that are determining how likely very likely the model is to have generated various combinations of.",
                    "label": 1
                },
                {
                    "sent": "Binary states here and obviously to get the posterior for this layer you need to take the product of the distribution.",
                    "label": 0
                },
                {
                    "sent": "That's saying, given the data we need to take things that are likely to cause the data.",
                    "label": 0
                },
                {
                    "sent": "On the part of the distribution that's saying, given all this price stuff we know up here we need to get a distribution over how likely we are to generate things here anyway.",
                    "label": 1
                },
                {
                    "sent": "So once you have a deep net, you have a very complicated product here.",
                    "label": 0
                },
                {
                    "sent": "Even when the prize independent life tricky.",
                    "label": 1
                },
                {
                    "sent": "But when you have a complicated product, sounds awful.",
                    "label": 0
                },
                {
                    "sent": "Because just to get your hands on this prior, you need to integrate all this.",
                    "label": 0
                },
                {
                    "sent": "All these hidden layers here.",
                    "label": 0
                },
                {
                    "sent": "So getting your hands on this price is going to be very tricky.",
                    "label": 0
                },
                {
                    "sent": "And you have to multiply this prior by the likelihood term here to get the posterior there so it looks horrible, and in particular it looks like you've got no chance of learning what these weights should be without already having learned or guess what these weights should be, so you're gonna have to learn everything all at once.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to convince you is.",
                    "label": 0
                },
                {
                    "sent": "That this horrible looking picture that says it's going to be very hard to learn these weights.",
                    "label": 0
                },
                {
                    "sent": "Actually, there's a little trick we can do that makes it very easy to learn all this stuff.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of main point of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "So the obvious way to learn it.",
                    "label": 0
                },
                {
                    "sent": "This is what Radford Neal did originally when he proposed these kinds of Nets.",
                    "label": 0
                },
                {
                    "sent": "You can use Monte Carlo methods, but that can be a bit slow.",
                    "label": 0
                },
                {
                    "sent": "People develop variational methods where they said, well, we know that when you do inference in these Nets, the posterior is complicated.",
                    "label": 0
                },
                {
                    "sent": "But let's suppose that it's not.",
                    "label": 0
                },
                {
                    "sent": "And there's reasons why this was a much more sensible idea than we thought it was when we sort of it.",
                    "label": 0
                },
                {
                    "sent": "So we're going to say that there's all this prior stuff here, and anyway, the likelihood term gives you explaining away and we're just going to all of that.",
                    "label": 0
                },
                {
                    "sent": "And we're going to say, given the data, let's suppose that these are independent of one another, and let's do inference like that.",
                    "label": 1
                },
                {
                    "sent": "And maybe if we're really lucky if we do inference like that and run our learning algorithm, it'll work anyway.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It turns out that was a sort of beginning of variational methods, where, yes, it does work quite well, and the reason it works quite well is 'cause the learning tries to change all these weights up here and these weights on the links here so that our assumption is true.",
                    "label": 0
                },
                {
                    "sent": "That is, it tries to make it true that these really are independent in the posterior.",
                    "label": 1
                },
                {
                    "sent": "And by using just the right weight shit they can get.",
                    "label": 0
                },
                {
                    "sent": "You can make that fairly true, and that's sort of why variational learning works much better than you would have thought.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The other thing is, even if you your approximation to the posterior distribution over these hidden units is not very good.",
                    "label": 0
                },
                {
                    "sent": "Variational learning is still guaranteed to optimize something, but is there some measure that's in some quantity that's improving?",
                    "label": 0
                },
                {
                    "sent": "It's a variational bound on the log probability.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But we're going to go for something else in this tutorial, but it will turn out to be quite closely related.",
                    "label": 0
                },
                {
                    "sent": "What we're going to go for is a way of trying to learn the weights.",
                    "label": 0
                },
                {
                    "sent": "Just one layer at a time, apparently.",
                    "label": 0
                },
                {
                    "sent": "But a way that takes into account the fact that we're going to learn more layers later.",
                    "label": 1
                },
                {
                    "sent": "So the sort of obvious way to approach this nasty problem here if you want to learn the weights one layer at a time is to say, let's suppose that none of this stuff upstairs was here.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose these hidden variables had independent priors, so it's a nice simple two layer network like my earthquake network.",
                    "label": 0
                },
                {
                    "sent": "Let's learn these weights under that supposition.",
                    "label": 0
                },
                {
                    "sent": "And once we've learned, let's start trying to learn a better model of the posterior distributions we actually get here.",
                    "label": 0
                },
                {
                    "sent": "But that doesn't work very well because.",
                    "label": 0
                },
                {
                    "sent": "If you don't have all this stuff upstairs.",
                    "label": 0
                },
                {
                    "sent": "Then it's a really bad assumption that these guys are independent.",
                    "label": 0
                },
                {
                    "sent": "Using all this stuff upstairs, if you use it right, you can make it actually quite a good assumption.",
                    "label": 0
                },
                {
                    "sent": "In fact, if you set this stuff just right, you can make it a perfect assumption what we're going to see is that by setting this right, you can make it truly be the case that these really are independent, even better than that, you can make it be the cases these are independent and the correct way to figure out the distribution is just take the data multiplied by the weight matrix and put it through the logistic function.",
                    "label": 0
                },
                {
                    "sent": "That's really very surprising.",
                    "label": 0
                },
                {
                    "sent": "When I first started explaining this to graphical models, people didn't believe me because they knew about explaining away.",
                    "label": 0
                },
                {
                    "sent": "So how could these be independent?",
                    "label": 0
                },
                {
                    "sent": "And as you'll see later, the answer is that all this stuff up here can exactly explain away explaining away, so explaining away disappears and these become independent if you do it right.",
                    "label": 1
                },
                {
                    "sent": "But we'll come to that later.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we're going to do now is take Watson.",
                    "label": 0
                },
                {
                    "sent": "Apparently a big detour, and we're going to give up on trying to learn these directed models, 'cause it seems too hard.",
                    "label": 0
                },
                {
                    "sent": "So that's what I did I gave up on trying to learn directed models and went off to some other kind of model.",
                    "label": 0
                },
                {
                    "sent": "Which I think are in the past where you have undirected connection, so it's an undirected graphical model now.",
                    "label": 0
                },
                {
                    "sent": "And all the independence relationships go the other way around for those as well see.",
                    "label": 0
                },
                {
                    "sent": "And along time ago, Terry Sonoski and I came up with an elegant learning algorithm for undirected graphs of these binary stochastic units.",
                    "label": 0
                },
                {
                    "sent": "But it was very, very inefficient.",
                    "label": 0
                },
                {
                    "sent": "You had to use Monte Carlo methods and they were slow.",
                    "label": 0
                },
                {
                    "sent": "Your noisy gradient and you had to follow this noisy gradient for a long time and in deep Nets it was terribly, terribly slow.",
                    "label": 0
                },
                {
                    "sent": "So it was sort of rightly regarded as sort of theoretically interesting, but no practical value.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out that if you restrict the connectivity and go for a much simpler device, but still undirected, then you can get an efficient learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's a shame because you restricted the connectivity so much that you haven't got a multi level system anymore, so it looks like you throwing the baby out with the bathwater.",
                    "label": 0
                },
                {
                    "sent": "But we're going to get the baby back later.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So throw the baby out with the bathwater and then pull on the hairs in the plug and gradually the baby will come up.",
                    "label": 0
                },
                {
                    "sent": "Mum.",
                    "label": 0
                },
                {
                    "sent": "You have visible units.",
                    "label": 0
                },
                {
                    "sent": "You have hidden units and you don't have any connections between the hidden units and you don't have any connections to invisible units, at least to begin with.",
                    "label": 1
                },
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We get one very nice property already in this, which is if I tell you the states of the visible units, give you an observation vector, then these guys are all now completely independent.",
                    "label": 0
                },
                {
                    "sent": "They really are independent of each other.",
                    "label": 1
                },
                {
                    "sent": "This is a factorial distribution here.",
                    "label": 0
                },
                {
                    "sent": "One way of thinking of it is once I fix these, this guy contact that guy.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "So inference in this net is really very simple and you can do.",
                    "label": 0
                },
                {
                    "sent": "You can get the full posterior trivially.",
                    "label": 0
                },
                {
                    "sent": "You just take this state multiplied by these weights.",
                    "label": 0
                },
                {
                    "sent": "That gives you a probability, put it through logistic.",
                    "label": 0
                },
                {
                    "sent": "Yeah probability, that's the probability of this guy being on and the full post series just the product of all these probabilities.",
                    "label": 0
                },
                {
                    "sent": "So you've got it and you can sample from it very easily.",
                    "label": 0
                },
                {
                    "sent": "So we got rid of explaining away in this net.",
                    "label": 0
                },
                {
                    "sent": "There's no explaining away, cousin undirected model inference is very simple, so we got one nice property now which is inference is simple.",
                    "label": 0
                },
                {
                    "sent": "Remember, with the directed Nets, we had the nice property that learning with simple if you could do inference and generation was simple.",
                    "label": 0
                },
                {
                    "sent": "Here we have the property.",
                    "label": 0
                },
                {
                    "sent": "Inference is simple.",
                    "label": 0
                },
                {
                    "sent": "But learning is a bit trickier and also generations trickier.",
                    "label": 0
                },
                {
                    "sent": "If you have weights on these connections and you want the model to tell you what it believes in, you have to go backwards and forwards lots of times, so there's no trivial way to generate like there is with the directive model.",
                    "label": 0
                },
                {
                    "sent": "But inference is nice and easy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in these undirected models you can think about the state of the network is having an energy.",
                    "label": 0
                },
                {
                    "sent": "So the energy of our binary visible vector binary hidden vector.",
                    "label": 1
                },
                {
                    "sent": "He is just the sum I'm gonna leave out biased terms 'cause they just make the math more complicated.",
                    "label": 0
                },
                {
                    "sent": "It's a sum over all pairs of a binary state of visible visible unit I.",
                    "label": 1
                },
                {
                    "sent": "So that's a one or a zero binary state of hidden unit and the weight on the connection.",
                    "label": 0
                },
                {
                    "sent": "So if two guys are on and there's a big weight between 'em, that gives you a big negative term, that's low energy.",
                    "label": 0
                },
                {
                    "sent": "That's good.",
                    "label": 0
                },
                {
                    "sent": "It's a happy network.",
                    "label": 0
                },
                {
                    "sent": "So happy networks are ones where.",
                    "label": 0
                },
                {
                    "sent": "The binary is there on how big positive weights between them.",
                    "label": 0
                },
                {
                    "sent": "One nice property of this is if you differentiate this energy with all the negative energy respect to wait, you just get this product of states here so the it's very easy to manipulate the energies.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "By changing the weights and those other derivatives.",
                    "label": 0
                },
                {
                    "sent": "And that yeah.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we want to do is.",
                    "label": 0
                },
                {
                    "sent": "Change the weights to change the energies to change the probabilities so that the log probability of the data is high.",
                    "label": 1
                },
                {
                    "sent": "So for a joint configuration.",
                    "label": 1
                },
                {
                    "sent": "A visible and hidden vector.",
                    "label": 0
                },
                {
                    "sent": "The probability of the joint configuration is proportional to each of minus the energy if.",
                    "label": 1
                },
                {
                    "sent": "You run the net in the right way, so if you take one of these.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pets.",
                    "label": 0
                },
                {
                    "sent": "And I just go backwards and forwards, updating all these units in parallel and then updating all these units in parallel using the logistic to pick binary states.",
                    "label": 0
                },
                {
                    "sent": "Then if I do it long enough, this will reach what's called its equilibrium distribution or a stationary distribution, and in that stationary distribution it will be sampling.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Joint vectors in proportion to each of the minus their energy.",
                    "label": 0
                },
                {
                    "sent": "Solar energy ones will be sampled a lot.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the probability of a joint vector will be each of the minus its energy normalized by the same quantity of all possible pairs of a visible and the hidden vector.",
                    "label": 1
                },
                {
                    "sent": "And if you want to probably visible vector, you just marginalized at age.",
                    "label": 0
                },
                {
                    "sent": "So some lateral H and you get that.",
                    "label": 0
                },
                {
                    "sent": "So in order to do learning, we just need to take logs and take derivatives and now we can adjust all the weights to change the energy impropriate way.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when you do that.",
                    "label": 0
                },
                {
                    "sent": "You get a very simple learning rule.",
                    "label": 0
                },
                {
                    "sent": "This is what's nice about Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "This stuff was already discovered a few years later by people doing Maxent.",
                    "label": 0
                },
                {
                    "sent": "You run this Markov chain where you start with some data.",
                    "label": 1
                },
                {
                    "sent": "You activate the hidden units you.",
                    "label": 1
                },
                {
                    "sent": "Reactivate the pixels from the hidden units and you go.",
                    "label": 0
                },
                {
                    "sent": "Packaged foods until you reach equilibrium, which could take a long time.",
                    "label": 0
                },
                {
                    "sent": "And you're getting fantasies from the model.",
                    "label": 0
                },
                {
                    "sent": "That is the kind of thing the model believes in is being generated, and it's completely forgotten where it started.",
                    "label": 0
                },
                {
                    "sent": "That's the definition of equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And when you're showing it data.",
                    "label": 0
                },
                {
                    "sent": "You measure the correlation between.",
                    "label": 0
                },
                {
                    "sent": "The binary states of pixels in the binary states are feature detectors or visible units in hidden units.",
                    "label": 1
                },
                {
                    "sent": "And that zero here means when you're showing your data.",
                    "label": 0
                },
                {
                    "sent": "So at times zero in this Markov chain.",
                    "label": 0
                },
                {
                    "sent": "And the angle back is just physics for the expected value of.",
                    "label": 0
                },
                {
                    "sent": "I don't like to use any 'cause I've got a E for energy.",
                    "label": 0
                },
                {
                    "sent": "You do this.",
                    "label": 1
                },
                {
                    "sent": "Measure the same statistic when the model is sampling from the things it believes in.",
                    "label": 0
                },
                {
                    "sent": "And hey presto, the learning algorithm is if you want to maximize the log probability of visible vector, measure this statistic with the visible vector clamped here.",
                    "label": 0
                },
                {
                    "sent": "And these angle brackets are kind of averaging over the noise in these guys.",
                    "label": 0
                },
                {
                    "sent": "And the same statistics when the model several ebrium and that is the derivative of the log prop.",
                    "label": 0
                },
                {
                    "sent": "So we got a very simple learning rule and its local.",
                    "label": 1
                },
                {
                    "sent": "It only depends on the behavior of the two units that the weight connects, which is quite surprising because the derivative here depends on all the other weights in the network.",
                    "label": 0
                },
                {
                    "sent": "But its dependence on all the other weights in the network shows up in this difference of correlations.",
                    "label": 0
                },
                {
                    "sent": "So the process of settling to equilibrium is kind of propagating information around the net in order to produce these statistics.",
                    "label": 0
                },
                {
                    "sent": "And those statistics are telling you they're telling this weight WJ everything it needs to know about all the other weights.",
                    "label": 0
                },
                {
                    "sent": "So as in back propagation, you did explicitly by the chain rule going backwards.",
                    "label": 0
                },
                {
                    "sent": "This just runs this alternating Gibbs sampling and that somehow gets the information into these statistics.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This rule we figured out the general case in the early 80s, and then I thought this kind of network wasn't very interesting 'cause it's only got 1 hidden layer with no connections between them and it's just sort of boring special case.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Many years later I made it go a million times as fast.",
                    "label": 0
                },
                {
                    "sent": "So the way you make your million times as fast as this.",
                    "label": 0
                },
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Instead of running along change week Librium for hundreds of steps, you just go up and down and up again.",
                    "label": 0
                },
                {
                    "sent": "Now it's not actually quite the right algorithm, but it works quite nicely.",
                    "label": 1
                },
                {
                    "sent": "There's a poster I think maybe today telling you conditions under which it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of conditions where you can make you screw up, but in general it works quite nicely.",
                    "label": 0
                },
                {
                    "sent": "So you go up, you come down, you go up again and you measure the difference in these statistics and sort of what you're seeing is what the statistics are with data and you're seeing sort of the direction in which is heading as it goes towards equilibrium and how the statistics are beginning to change.",
                    "label": 0
                },
                {
                    "sent": "The model gets to have its say about what should be going on.",
                    "label": 0
                },
                {
                    "sent": "And that difference is enough to allow you to do learning.",
                    "label": 0
                },
                {
                    "sent": "So and we'll see later are much better justification for using this.",
                    "label": 0
                },
                {
                    "sent": "It gives you more insight into it.",
                    "label": 1
                },
                {
                    "sent": "And this is very justified when the weights are quite small.",
                    "label": 0
                },
                {
                    "sent": "But we're going to substitute the learning rule as an Infinity here for this learning rule, so you don't have to go up and down and up again.",
                    "label": 0
                },
                {
                    "sent": "So now that goes a million times as fast and the reason it goes a million times as fast is 'cause it's 100 times less computation.",
                    "label": 0
                },
                {
                    "sent": "And it took me 17 years to think of it.",
                    "label": 0
                },
                {
                    "sent": "And in those 17 years computers got 10,000 times faster.",
                    "label": 0
                },
                {
                    "sent": "It was very important to take a long time to think of it.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'm going to show you a little example where you take some little handwritten digits.",
                    "label": 0
                },
                {
                    "sent": "You have only 50 feature detecting neurons.",
                    "label": 0
                },
                {
                    "sent": "You start off with all random weights.",
                    "label": 0
                },
                {
                    "sent": "You activate the feature detectors, you reconstruct the pixels you activate the feature vectors again and you can think of it as with the data there.",
                    "label": 0
                },
                {
                    "sent": "Every time a pair is active, you're going to increment weight slightly.",
                    "label": 0
                },
                {
                    "sent": "When you've got reconstructions here, every time a pair of a feature detector pixel are active, you're going to decrement the weight slightly, and one way of thinking of it is, I think this gives some insight into what the algorithm is up to.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "The reconstructions will typically be things that the network is happier with than reality.",
                    "label": 0
                },
                {
                    "sent": "You showed some reality.",
                    "label": 0
                },
                {
                    "sent": "It represents the reality.",
                    "label": 0
                },
                {
                    "sent": "Then you ask it to reconstruct the reality and then reconstruct it with the sort of surprising corners knocked off, it will reconstruct something that's smoother and more regular.",
                    "label": 0
                },
                {
                    "sent": "It's something that it prefers to believe.",
                    "label": 0
                },
                {
                    "sent": "When the training algorithm really consists of saying this, take the stuff you'd like to believe and don't believe it.",
                    "label": 0
                },
                {
                    "sent": "And take the stuff that's actually there and do believe it.",
                    "label": 0
                },
                {
                    "sent": "George Bush used to run the other algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK. Hum.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing you here is the weights that are little network like this learns when you show lots of images of twos.",
                    "label": 0
                },
                {
                    "sent": "And so each of these big squares is a feature detector.",
                    "label": 0
                },
                {
                    "sent": "And the show you the strengths of the 256 connections to the pixels where white is positive and black is negative.",
                    "label": 0
                },
                {
                    "sent": "So you can see this guy here has learned a little feature like this.",
                    "label": 0
                },
                {
                    "sent": "He likes to have a piece of stroke there and he really insists you don't have anything there.",
                    "label": 0
                },
                {
                    "sent": "So if you turn this guy on, he'll try and generate an image that looks like that.",
                    "label": 0
                },
                {
                    "sent": "And if you turn on some subset of about 20 of these guys, it turns out if you turn on a sensible subset of 20, the generated images looks just like a 2.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "Where after learning I show it some more TOS that is never seen before.",
                    "label": 0
                },
                {
                    "sent": "And I ask you to reconstruct them.",
                    "label": 0
                },
                {
                    "sent": "And you see, it does a pretty good job reconstructing.",
                    "label": 0
                },
                {
                    "sent": "The reconstruction is a bit more regular than the real data.",
                    "label": 0
                },
                {
                    "sent": "But it reconstructs them very well for a whole variety of twos.",
                    "label": 0
                },
                {
                    "sent": "And that's with only 50 feature detectors, so you can think of it as aggravated a subset of them and then basically added together the weights of that subset, put it through a nonlinear function, and then showing you the image and it doesn't really good job of reconstructing twos.",
                    "label": 0
                },
                {
                    "sent": "It's not quite perfect, and when it gets a very unusual feature, like a vertical tail, which very few tools have, it makes it sort of messes it up a bit.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But it does quite a good job within 50 feet protectors.",
                    "label": 0
                },
                {
                    "sent": "Now what we're going to do is we're going to take this model, was trained on twos that believes the whole world consists of twos, and we're going to hit threes.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "It's a bit like showing George Bush some real data.",
                    "label": 0
                },
                {
                    "sent": "Now it has opinions about what the world should be like.",
                    "label": 0
                },
                {
                    "sent": "And this is what it sees, so to speak.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And if you think about the difference in twos and threes, the difference is that you know threes.",
                    "label": 0
                },
                {
                    "sent": "Don't join up there and they don't have a tail, but we can fix that.",
                    "label": 1
                },
                {
                    "sent": "We just rub out a little bit here and put it a little bit here and hey presto, we didn't change much and it's a 2.",
                    "label": 0
                },
                {
                    "sent": "So it's easy if we turned on the learning, it will very quickly learn not to do that, and would learn to deal with twos and threes.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a little example of this algorithm running to show you it works.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is sort of a bit of an aside, but.",
                    "label": 1
                },
                {
                    "sent": "I want to distinguish a whole bunch of ways of combining probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "So one thing we're all very familiar with is if you have a kind of probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "I say gaussian.",
                    "label": 0
                },
                {
                    "sent": "You can have a bunch of those and average the distributions together, take some weighted average of those distributions.",
                    "label": 1
                },
                {
                    "sent": "That's a mixture gaussian's.",
                    "label": 0
                },
                {
                    "sent": "The problem with the mixture is the combined distribution can never be sharper than the sharpest of the individual distributions.",
                    "label": 1
                },
                {
                    "sent": "So I'm never going to get very sharp representations of things that way.",
                    "label": 0
                },
                {
                    "sent": "I'm an alternative is to take a product of the distributions and renormalize so I could take a whole bunch of guys and multiply them together, and if I take, say, a Gaussian Karina Garcia here and I multiply them together, what I'll get is something a tiny little Gaussian in the middle.",
                    "label": 0
                },
                {
                    "sent": "Little have half the variance of the Gaussians like here.",
                    "label": 0
                },
                {
                    "sent": "And there's almost no probability mass there.",
                    "label": 0
                },
                {
                    "sent": "But when I re normalize, it turns into a ghost in the middle.",
                    "label": 1
                },
                {
                    "sent": "And the point is, the more I multiply together, the sharper things get.",
                    "label": 0
                },
                {
                    "sent": "So by having very vague individual models and multiplying them together, I can get a very sharp model.",
                    "label": 0
                },
                {
                    "sent": "I can't do that with the mixture.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "But I have this normalization term that makes learning difficult and you can think of a restricted Boltzmann machine as being a product model.",
                    "label": 1
                },
                {
                    "sent": "In fact, it's a product of mixtures.",
                    "label": 0
                },
                {
                    "sent": "So each of the hidden units is saying I'm a mixture model if I'm off.",
                    "label": 0
                },
                {
                    "sent": "Then I believe in a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "If I'm on.",
                    "label": 0
                },
                {
                    "sent": "I believe in on a factorial distribution across visible units where the probability of each visible unit is the sigmoid of the weight on my connection.",
                    "label": 0
                },
                {
                    "sent": "So put that weight on the connection through logistic.",
                    "label": 0
                },
                {
                    "sent": "That's the probability of each of these units, but they're all independent.",
                    "label": 0
                },
                {
                    "sent": "And then when you turn on a bunch of these guys, you get the products of all those distributions.",
                    "label": 0
                },
                {
                    "sent": "That's what you do when you add up all these weights.",
                    "label": 0
                },
                {
                    "sent": "So restricted Boltzmann machine is a product model and each hidden unit is 1 little probabilistic model and they will be multiplied together.",
                    "label": 0
                },
                {
                    "sent": "And what we're going to do now is we're going to take these product models, and we're going to compose them.",
                    "label": 1
                },
                {
                    "sent": "That's another way of combining models where we take the hidden units of 1 model and we make them the data for the next model.",
                    "label": 0
                },
                {
                    "sent": "So that the input to the next restricted Boltzmann machine is the.",
                    "label": 0
                },
                {
                    "sent": "Hidden units of the previous one, so we're sort of stacking 'em up.",
                    "label": 0
                },
                {
                    "sent": "And so you can think of 1 theme of this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Is these guys work better than those guys?",
                    "label": 0
                },
                {
                    "sent": "And in particular if you take these guys and you compose them together, you get something really nice.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main reason these restricted Boltzmann machines are interesting is 'cause you can compose them.",
                    "label": 1
                },
                {
                    "sent": "That is, you can stack 'em up.",
                    "label": 0
                },
                {
                    "sent": "And we're going to learn in the following way, which I would justify in a minute.",
                    "label": 0
                },
                {
                    "sent": "We're going to train their features in just where I just showed you.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to take the activations of those features when the model is being driven by data, and we could take the binary activations.",
                    "label": 0
                },
                {
                    "sent": "Or we could actually use the real values.",
                    "label": 0
                },
                {
                    "sent": "The theory is much clean if you say, take the binary activations, you work better in practice if you take the real values, but I'll stick with the theory for now.",
                    "label": 0
                },
                {
                    "sent": "So you take a.",
                    "label": 0
                },
                {
                    "sent": "You take data.",
                    "label": 0
                },
                {
                    "sent": "You, um?",
                    "label": 0
                },
                {
                    "sent": "Activate your feature detectors.",
                    "label": 0
                },
                {
                    "sent": "You take the binary states of the feature detectors and there the data for training the next restricted Boltzmann machine for training the next layer of weights.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a simple as it could be.",
                    "label": 0
                },
                {
                    "sent": "The actual algorithm.",
                    "label": 0
                },
                {
                    "sent": "And what you can show is that what you'd like to show is that if I take some data and I build a restricted Boltzmann machine, model of this data.",
                    "label": 0
                },
                {
                    "sent": "So if I were to generate from my model, it would generate stuff a bit like the data.",
                    "label": 1
                },
                {
                    "sent": "If I now add another hidden layer.",
                    "label": 0
                },
                {
                    "sent": "Of sufficient size and I had in the right way, I'd like to be able to guarantee in a sort of expected sense that I get a better model of the data, the badging, another layer can always improve the model.",
                    "label": 0
                },
                {
                    "sent": "Unless the model is really perfect.",
                    "label": 0
                },
                {
                    "sent": "And you can almost guarantee that you can't quite guarantee that the first time.",
                    "label": 0
                },
                {
                    "sent": "You can guarantee that you'll get a better model, but in general, as you add more layers, what you can guarantee is that the more complicated model you get with more layers.",
                    "label": 0
                },
                {
                    "sent": "Will be will have a better bound on the log probability of the data.",
                    "label": 1
                },
                {
                    "sent": "So your you have a model, there's some variational bound that says the log probability data must be at least this much.",
                    "label": 0
                },
                {
                    "sent": "When I add a new layer.",
                    "label": 0
                },
                {
                    "sent": "And take the bank for this deeper model.",
                    "label": 0
                },
                {
                    "sent": "It's a better bound.",
                    "label": 0
                },
                {
                    "sent": "The log probability will be bigger unless I already got a perfect model.",
                    "label": 0
                },
                {
                    "sent": "If you add the new layer in the right way, so that's good enough.",
                    "label": 0
                },
                {
                    "sent": "You definitely sort of improving things as your address.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's suppose we do this.",
                    "label": 0
                },
                {
                    "sent": "43 letters so it goes like this.",
                    "label": 0
                },
                {
                    "sent": "You take some data.",
                    "label": 0
                },
                {
                    "sent": "You learn a restricted Boltzmann machine here, so you've learned these weights.",
                    "label": 0
                },
                {
                    "sent": "And they're kind of the same weights in both directions.",
                    "label": 0
                },
                {
                    "sent": "Then you take the states of the hidden units.",
                    "label": 0
                },
                {
                    "sent": "That you got by taking the data, multiplying by these weights, putting it through the logistic and sampling.",
                    "label": 0
                },
                {
                    "sent": "Take those vectors there, treat those as data and learn another model.",
                    "label": 0
                },
                {
                    "sent": "And then you take these.",
                    "label": 0
                },
                {
                    "sent": "You treat them as data and you learn another model.",
                    "label": 0
                },
                {
                    "sent": "And the question is, when I've learned all three things like that, all three models, what do I have?",
                    "label": 0
                },
                {
                    "sent": "And I initially thought.",
                    "label": 0
                },
                {
                    "sent": "Well, you haven't got a model at all.",
                    "label": 0
                },
                {
                    "sent": "You've got three models.",
                    "label": 0
                },
                {
                    "sent": "You got one model of the data.",
                    "label": 1
                },
                {
                    "sent": "Then you've got another model of what those feature detectors are doing, and you got another model of what those features are doing.",
                    "label": 0
                },
                {
                    "sent": "But it's not all one model.",
                    "label": 0
                },
                {
                    "sent": "And then if I pointed out that actually you can view it all as one model, but it's not the model you expect.",
                    "label": 1
                },
                {
                    "sent": "So you might expect if you viewed as one model, it would be a big undirected model where you have W 3 here and W2 there with our.",
                    "label": 0
                },
                {
                    "sent": "Arrows on both ends to me is undirected.",
                    "label": 0
                },
                {
                    "sent": "On W on there with others on both ends, so it's a big Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "But that's not the model you've got.",
                    "label": 0
                },
                {
                    "sent": "What you've got is a model where the top level is an unrestricted box machine.",
                    "label": 0
                },
                {
                    "sent": "And then below that you have a directed belief net.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "One way of saying what I mean by saying that is if you wanted to generate from this model what you want to do is go backwards and forwards here, ignoring all this stuff, go back as a force here until you reach equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Then having got a sample of equilibrium here, go chunk chunk OK.",
                    "label": 0
                },
                {
                    "sent": "So it's not an undirected model, this parts directed.",
                    "label": 0
                },
                {
                    "sent": "In particular, when you're going backwards here, all these weights down here having no influence on what's happening.",
                    "label": 0
                },
                {
                    "sent": "Which they would if it was a undirected model.",
                    "label": 0
                },
                {
                    "sent": "It's got the kind of valves that are directed.",
                    "label": 1
                },
                {
                    "sent": "Model has the things up the other end.",
                    "label": 0
                },
                {
                    "sent": "If they can observe don't influence you.",
                    "label": 0
                },
                {
                    "sent": "There's no influence comes back from here when you're generating.",
                    "label": 0
                },
                {
                    "sent": "OK, now I'm just asking you to take that on faith.",
                    "label": 0
                },
                {
                    "sent": "For now, I'll explain why that's true later.",
                    "label": 0
                },
                {
                    "sent": "Before I explain why that's true, I want to show you nice example.",
                    "label": 0
                },
                {
                    "sent": "Now one of these deep Nets doing something.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't want to do that, just saying.",
                    "label": 0
                },
                {
                    "sent": "No, I want to explain why that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "OK, so we first need a little aside.",
                    "label": 0
                },
                {
                    "sent": "If you take some factorial distributions and you average them, you don't get a factorial distribution.",
                    "label": 1
                },
                {
                    "sent": "You might think you did, but you don't.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 1
                },
                {
                    "sent": "So in an RBM, if I give you a visible vector, the factorial, the distribution of the hidden units is factorial.",
                    "label": 0
                },
                {
                    "sent": "If I now have, say, two visible vectors in half the time, I give you 1 and half the time we give you the other.",
                    "label": 0
                },
                {
                    "sent": "When I give you one, you get a factorial distribution.",
                    "label": 0
                },
                {
                    "sent": "When I give you the other, you get a factorial conditional distribution, but when you add those two distributions together, what you get is not a factorial distribution when you aren't together yet also correlations.",
                    "label": 0
                },
                {
                    "sent": "In general, OK, that's sort of surprising to some people, so I'll call that thing the aggregated posterior.",
                    "label": 0
                },
                {
                    "sent": "In other words, you're training restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "You now show each of the data vectors in your training set, and for each of those data vectors you have a posterior distribution that's factorial across the hidden units.",
                    "label": 0
                },
                {
                    "sent": "You know, average all those distributions together.",
                    "label": 1
                },
                {
                    "sent": "And you know you get a complicated distribution.",
                    "label": 0
                },
                {
                    "sent": "Which I'll call the aggregated posterior.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can think of a restricted Boltzmann machine as something that takes the data distribution and converts it into this other distribution.",
                    "label": 0
                },
                {
                    "sent": "This aggregated posterior across the hidden units.",
                    "label": 0
                },
                {
                    "sent": "And it converts it in such a way that.",
                    "label": 0
                },
                {
                    "sent": "From a hidden vector, you're pretty good at reconstructing a data vector.",
                    "label": 0
                },
                {
                    "sent": "And then these hidden vectors have some complicated distribution.",
                    "label": 0
                },
                {
                    "sent": "So you can think.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It like this.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "Once I've defined the weights in my restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "R. That defines some prior distribution over hidden vectors.",
                    "label": 1
                },
                {
                    "sent": "That is, if I would take this model and run the Markov chain backwards and forwards and then sample from the hidden units.",
                    "label": 1
                },
                {
                    "sent": "Now I get some distribution, that's the distribution the model believes it.",
                    "label": 0
                },
                {
                    "sent": "And we'd like that distribution to be a good model of the aggregated posterior that you get when you show it data.",
                    "label": 1
                },
                {
                    "sent": "So if in a sense converted the task of modeling data into two tasks, try and get this prior over hidden vectors defined by these weights.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 1
                },
                {
                    "sent": "To match the aggregated posterior and try to get it so that by using the weights again when you convert a hidden vector into visible vector, you get the data distribution and it's sort of split into two tasks, both of which use the same parameters.",
                    "label": 0
                },
                {
                    "sent": "And so when we've learned a restricted Boltzmann machine, we've learned to do this, and we've learned to do that.",
                    "label": 0
                },
                {
                    "sent": "But we're making sort of compromise, 'cause we're using the same parameters for both.",
                    "label": 0
                },
                {
                    "sent": "Now what we can do is we can keep these weights for doing that.",
                    "label": 0
                },
                {
                    "sent": "And use some different parameters for doing this to do a better job of it.",
                    "label": 0
                },
                {
                    "sent": "So really what we're doing is we're taking the task of modeling the data and we're splitting it in a funny way into a sort of parametric bit here.",
                    "label": 0
                },
                {
                    "sent": "And then this aggregated posterior, which is sort of nonparametric thing which we're also trying to model like this, but later on we're going to model with something better, which is actually going to be a high level restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you another way to think about doing later, so you get two chances to under.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Send it.",
                    "label": 0
                },
                {
                    "sent": "So here's a way of thinking about what I just said.",
                    "label": 0
                },
                {
                    "sent": "The probability of the visible vector given a restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "Is the summer rule hidden vectors of the probability that you would generate that hidden vector if you ran to the equilibrium distribution?",
                    "label": 1
                },
                {
                    "sent": "Times the probability that given the hidden vector, you would generate the visible vector where both of these depend on the weights.",
                    "label": 0
                },
                {
                    "sent": "OK, and to maximize the probability of a visible vector, what you'd like to do is increase these probabilities in here.",
                    "label": 0
                },
                {
                    "sent": "And suppose I freeze this term.",
                    "label": 0
                },
                {
                    "sent": "Then to maximize the probability you'd somehow like to change the prior over hidden units, so this gets bigger.",
                    "label": 0
                },
                {
                    "sent": "And you probably know from fitting mixture models.",
                    "label": 0
                },
                {
                    "sent": "So when you try and set the mixing proportion in the mixture of Gaussians, what you do is you make the mixing proportion be equal to the posterior probability that you use that case.",
                    "label": 0
                },
                {
                    "sent": "So in general, when you're adjusting priors, you want to change them to be more like the posteriors that you got using your current values of those priors.",
                    "label": 0
                },
                {
                    "sent": "And so that's what we're going to do here.",
                    "label": 0
                },
                {
                    "sent": "We're going to try and adjust our model of P of H to be more like the aggregated posterior that we got.",
                    "label": 1
                },
                {
                    "sent": "And if we do that, we'll get a better model of the.",
                    "label": 1
                },
                {
                    "sent": "So that's what we're doing in this recursive learning.",
                    "label": 0
                },
                {
                    "sent": "That's one way of thinking about it.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If you try and do that in a directed net.",
                    "label": 1
                },
                {
                    "sent": "Um, you could think about.",
                    "label": 0
                },
                {
                    "sent": "Assuming independence here.",
                    "label": 0
                },
                {
                    "sent": "And then modeling this.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then looking at the posterior you get when you do inference, which is going to be complicated and then trying to model this.",
                    "label": 1
                },
                {
                    "sent": "The problem is that if you do it in a directed net, when you learn these weights, it tries very hard to make these.",
                    "label": 1
                },
                {
                    "sent": "Independent.",
                    "label": 0
                },
                {
                    "sent": "Because that's the assumption of the model.",
                    "label": 0
                },
                {
                    "sent": "And so it'll be willing to get weights that don't actually allow you to reconstruct very well, because they're trying to get these to be independent in the posterior.",
                    "label": 0
                },
                {
                    "sent": "And that's silly, because later on you're going to put in more stuff here, so these don't actually need to be independent.",
                    "label": 0
                },
                {
                    "sent": "You're trying to achieve something.",
                    "label": 0
                },
                {
                    "sent": "But you don't really want to achieve in the end, because in the end these are going to be able to make these be very non independent.",
                    "label": 0
                },
                {
                    "sent": "So you wasted a lot of effort trying to achieve this independence as a result of which this doesn't work very well.",
                    "label": 0
                },
                {
                    "sent": "This bit, and you've already made a big loss here.",
                    "label": 0
                },
                {
                    "sent": "And also you make these two independent.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't work nearly as well to sort of learn these weights and then assume independence here and then learn a model of this stuff.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to give you a practical example and then go back a bit more to the theory.",
                    "label": 0
                },
                {
                    "sent": "We're going to take bigger images of digits, MNIST digits, and we're going to take all 10 different classes.",
                    "label": 0
                },
                {
                    "sent": "And we're going to model like this 500 binary features.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to model like this 500 features.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to put in the class labels with that EMUI multinomial, and we're going to learn a model of this feature vector concatenated with the label, and it's a joint density model, so this is just another restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "That learns the joint density, but this was done greedily.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to do a bit of fine tuning.",
                    "label": 0
                },
                {
                    "sent": "I won't talk about much that makes it work quite a bit better, but it's too complicated for now.",
                    "label": 0
                },
                {
                    "sent": "So basically think we just did this.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to look at what we got.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I might come back to that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Engineer this time, but I'm not going to talk about it now.",
                    "label": 0
                },
                {
                    "sent": "I want to show you the model that we get.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is that.",
                    "label": 0
                },
                {
                    "sent": "Net 28 by 28505 hundred 2010 and we can give it an image like this one.",
                    "label": 0
                },
                {
                    "sent": "And we can.",
                    "label": 0
                },
                {
                    "sent": "That it recognize it.",
                    "label": 0
                },
                {
                    "sent": "So all I'm doing now is running it this way.",
                    "label": 0
                },
                {
                    "sent": "So having trained as a generative modeling and I run it as a sort of discriminative model.",
                    "label": 0
                },
                {
                    "sent": "And it's stochastic, so these keep changing, but it has no doubt that it's a four.",
                    "label": 0
                },
                {
                    "sent": "It's very sure it's a four.",
                    "label": 0
                },
                {
                    "sent": "If I run it faster you can see the top level ones don't change that much.",
                    "label": 0
                },
                {
                    "sent": "Or rather, there's some that are fairly stable like that one there.",
                    "label": 0
                },
                {
                    "sent": "I can share some other digit, maybe that one.",
                    "label": 0
                },
                {
                    "sent": "It's less sure about that, but it's most of the time it says an education.",
                    "label": 0
                },
                {
                    "sent": "It thinks it's a three, which isn't so unreasonable.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, so it can recognize OK.",
                    "label": 0
                },
                {
                    "sent": "In fact, it does quite a good job of recognition.",
                    "label": 0
                },
                {
                    "sent": "There is very sure that survived only has one moment of doubt.",
                    "label": 0
                },
                {
                    "sent": "I can give it a tricky one.",
                    "label": 0
                },
                {
                    "sent": "At and it will express its doubt by keeping changing his mind.",
                    "label": 0
                },
                {
                    "sent": "It happens to end on the right answer, which is an 8, but that's just coincidence.",
                    "label": 0
                },
                {
                    "sent": "Actually, the fact that this isn't in the last column means that the most frequent answer gives is Nate, but only just.",
                    "label": 0
                },
                {
                    "sent": "It can deal with various kinds of noise quite well.",
                    "label": 0
                },
                {
                    "sent": "So it thinks this is a one or a second most of the time.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "But what's most interesting about this is when you run it as a generator.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to run the model as a generator.",
                    "label": 0
                },
                {
                    "sent": "And what's going to be happening is this?",
                    "label": 0
                },
                {
                    "sent": "I'm going to fix some label like 2.",
                    "label": 0
                },
                {
                    "sent": "And I'm just going to go.",
                    "label": 0
                },
                {
                    "sent": "I'm down between these units here with this fixed.",
                    "label": 0
                },
                {
                    "sent": "So I'm just doing alternating Gibbs sampling in a restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "That's all that's happening causally.",
                    "label": 0
                },
                {
                    "sent": "That's where the action is.",
                    "label": 0
                },
                {
                    "sent": "And what will happen is it will enter various brain states here represent things and it's impossible for you to tell what they represent because there's just activities of neurons.",
                    "label": 0
                },
                {
                    "sent": "What you'd be interested in this mental state?",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to show you both the brain state here and the mental state which I'm going to show you here.",
                    "label": 0
                },
                {
                    "sent": "You see, the way mental state the language of mental states works is this.",
                    "label": 0
                },
                {
                    "sent": "If I want to tell you one of my brain states, I could try saying you're in 50.",
                    "label": 0
                },
                {
                    "sent": "Three is on.",
                    "label": 0
                },
                {
                    "sent": "Well, that didn't do you much good, did it?",
                    "label": 0
                },
                {
                    "sent": "Or I could say there's a pattern of active neurons here, which are just like the pattern of active neurons you get.",
                    "label": 0
                },
                {
                    "sent": "If I was looking at a pink elephant.",
                    "label": 0
                },
                {
                    "sent": "Now that's a rather clumsy way of saying of this rather clumsy set of words.",
                    "label": 0
                },
                {
                    "sent": "There's a pattern of their brain state, which is like the brain state I would get if I was looking at a pink elephant, OK?",
                    "label": 0
                },
                {
                    "sent": "So we have a funny way of saying that we have shorthand for that which, which is something like saying I've got the person providing elephant.",
                    "label": 0
                },
                {
                    "sent": "What that really means is that some brain stated by claim there's some brain state.",
                    "label": 0
                },
                {
                    "sent": "That's like the one I would have if I was looking at a pink elephant.",
                    "label": 0
                },
                {
                    "sent": "Now, as soon as you got a generative model, if I ask you what brain stages this then.",
                    "label": 0
                },
                {
                    "sent": "Well, because I can generate from it, I can say the kinds of things assuming my inference works, I can say the kinds of things that would normally cause that, so I'm going to show you these brain states, but I can show you the mental states by showing the kinds of things that would normally cause.",
                    "label": 0
                },
                {
                    "sent": "So as it runs, that's where all the action is.",
                    "label": 0
                },
                {
                    "sent": "But just so you know what's really happening up here, I'm going to show you what it has in mind by going chunk chunk each time.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's where the action is.",
                    "label": 0
                },
                {
                    "sent": "We're going to each time.",
                    "label": 0
                },
                {
                    "sent": "And if you do it faster.",
                    "label": 0
                },
                {
                    "sent": "You'll see that after a while it settles into generating twos and once he settled into generating tools it will just generate truth forever and then all sorts of different tools.",
                    "label": 0
                },
                {
                    "sent": "Once we link from that loops, it'll generate even rather bad twos.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "This isn't really running, it's a canned demo.",
                    "label": 0
                },
                {
                    "sent": "But that's important that it can generate those 'cause it means you can recognize that their twos.",
                    "label": 0
                },
                {
                    "sent": "So this is the best generative model or is 100 digits.",
                    "label": 0
                },
                {
                    "sent": "I say that in every lecture given nobody's yet challenged me, and if I can say it long enough, it will become true.",
                    "label": 0
                },
                {
                    "sent": "So I just changed the label here.",
                    "label": 0
                },
                {
                    "sent": "And now it will generate eights, and once he settled into the ravine, it'll generate only eights.",
                    "label": 0
                },
                {
                    "sent": "When we're thinking about this model.",
                    "label": 0
                },
                {
                    "sent": "Is as a way of dealing with the fact that all real world data lies on low demand lies on or close to low dimensional manifolds.",
                    "label": 0
                },
                {
                    "sent": "And the normal way to deal with that is to say well, if twos lie on a manifold, there's about 12 dimensional inside my machine.",
                    "label": 0
                },
                {
                    "sent": "Let's have 12 numbers that represent each two and let's figure out how to get to these 12 numbers, maybe?",
                    "label": 0
                },
                {
                    "sent": "Well, that's tricky.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll do it.",
                    "label": 0
                },
                {
                    "sent": "Nonparametrically will associate sort of numbers with each two somehow.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The problem with that is if it 2 has a very long tail, it's actually got more degrees of freedom 'cause there's also special 14 degrees of freedom is long tail.",
                    "label": 0
                },
                {
                    "sent": "Then obviously somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "It's got 13 1/2 degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Also degrees of freedom at all, or none.",
                    "label": 0
                },
                {
                    "sent": "There's things you can do with the two, like making a very wobbly too that make it not quite such a good too, but not as bad as turning random pixels.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of a degree of some freedom, but you know it's a bit bad to do that, but not too bad.",
                    "label": 0
                },
                {
                    "sent": "What's more, the data might have many of these manifolds, and you don't know how many.",
                    "label": 0
                },
                {
                    "sent": "In fact, it might be that there's 30 eleven of these manifolds, but if you go up in energy a bit, that is, if you allow things to be not quite so good to the merge like you might have sevens across sevens.",
                    "label": 0
                },
                {
                    "sent": "But if you get a bit less discriminating, they might be sort of more or less in the same manifold.",
                    "label": 0
                },
                {
                    "sent": "And so, rather than doing dimensionality dealing with low dimensional data by explicitly trying to represent where you are on the manifold and much better way to do it, I think is to take your data that lies on these low dimensional manifolds, blow it up into some high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And in that high dimensional space have an energy function.",
                    "label": 0
                },
                {
                    "sent": "Start off with everything, equal energy, so all the weights are roughly 0 and then gradually dig out ravines, low dimensional ravines.",
                    "label": 0
                },
                {
                    "sent": "So if we say will work in this dimensionality, so 500 dimensions here, then in this 500 dimensional space we can integrate out these units so we can get what's called a free energy for these 500 dimension vectors and there's going to be 10 ravines, each of which is long and skinny.",
                    "label": 0
                },
                {
                    "sent": "And has a few degrees of freedom along the floor of the ravine.",
                    "label": 1
                },
                {
                    "sent": "And then lots of degrees of freedom up the side of the ravine.",
                    "label": 0
                },
                {
                    "sent": "And in particular, the ravine for twos might in some places come quite close to the ravine for threes.",
                    "label": 0
                },
                {
                    "sent": "But there might be quite an energy barrier there, 'cause we expect low density regions are behind you regions between digit classes.",
                    "label": 0
                },
                {
                    "sent": "Actually, for some lines that's not true.",
                    "label": 0
                },
                {
                    "sent": "The ravine force in the ravines for 9 sort of merge into each other.",
                    "label": 0
                },
                {
                    "sent": "One point more or less.",
                    "label": 0
                },
                {
                    "sent": "So what this model is learned by this sort of greedy learning and very fine tuning is to turn the pixels into these features.",
                    "label": 0
                },
                {
                    "sent": "And the features have the property that by using these hidden units to express sort of energies of combinations of features, we can make ravines in this space that capture the manifold's if we tried to do it by interactions between pixels, we couldn't do it.",
                    "label": 0
                },
                {
                    "sent": "You just can't express what are two is by a Markov random field on pixels.",
                    "label": 0
                },
                {
                    "sent": "That is, you can write down the microphone field pairwise interactions in pixels such that the only happy states are twos.",
                    "label": 0
                },
                {
                    "sent": "But as we've seen, we can do it if we use multiple layers.",
                    "label": 0
                },
                {
                    "sent": "The only happy states of this system when you stick when you clamp that label 8's here.",
                    "label": 0
                },
                {
                    "sent": "I'll do one more 'cause I like it so much.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Do.",
                    "label": 0
                },
                {
                    "sent": "You can't generate from them.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "I'll repeat the question convolutional networks do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Well comedy show lyrics are good at recognizing digits, but convolutional neural networks won't allow you to generate from the model 'cause they don't have a generative model.",
                    "label": 0
                },
                {
                    "sent": "If you think of the classic ones from the late 90s from the 90s.",
                    "label": 0
                },
                {
                    "sent": "Now more recently has been doing more complicated things where he tries to combine this kind of learning with convolutional Nets.",
                    "label": 0
                },
                {
                    "sent": "OK, so basically I've infected Youngs convolutional Nets and he's now doing stuff a bit like this as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK um.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these are just some samples generated from the model where you run 4000 alternatives.",
                    "label": 0
                },
                {
                    "sent": "Separate iterations between samples.",
                    "label": 0
                },
                {
                    "sent": "So the roughly independent.",
                    "label": 0
                },
                {
                    "sent": "And you can see that if you clamp zeros you get zeros, and if you clamp one, you get once and almost all of them look like the right class.",
                    "label": 0
                },
                {
                    "sent": "There's a few sort of bad guys like this guy.",
                    "label": 0
                },
                {
                    "sent": "But it's a very good generative model.",
                    "label": 0
                },
                {
                    "sent": "And of course, what's happening is when I clamp when I climb one of those high level units to a 2.",
                    "label": 0
                },
                {
                    "sent": "Then there's 2000 outgoing connections and the weights on those connections lower the energy of the two ravine and raise the energy of all the other ravines.",
                    "label": 0
                },
                {
                    "sent": "So now if I wander around in that space, eventually settle into that ravine and stay there.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's good to do recognition.",
                    "label": 0
                },
                {
                    "sent": "So these are examples of cases that it wasn't sure about but actually got right, and so we can recognize all sorts of different tools.",
                    "label": 0
                },
                {
                    "sent": "Tom.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are results from a few years ago.",
                    "label": 0
                },
                {
                    "sent": "A support vector machine gets about 1.4%.",
                    "label": 1
                },
                {
                    "sent": "This model, after a bit of fine tuning, but it's not discriminative functioning, gets 1.25 as we'll see later with discriminative fine tuning can do quite a bit better.",
                    "label": 0
                },
                {
                    "sent": "I'm back, probably can't do better than 1.6.",
                    "label": 0
                },
                {
                    "sent": "Well, you might be able to 1.59, but you basically can't do better than 1.6 K nearest neighbors about that.",
                    "label": 0
                },
                {
                    "sent": "So you get some idea of how tough it is.",
                    "label": 0
                },
                {
                    "sent": "And there's lots more results in the car, some which are much well below 1%.",
                    "label": 0
                },
                {
                    "sent": "But those are ones that use prior knowledge about pixels about like convolutional neural Nets know about pixels being near one another about translation invariants at least locally.",
                    "label": 0
                },
                {
                    "sent": "This is a pure machine learning approach where you don't tell anything about pixels, you just give it a big vector and say these vectors have these labels.",
                    "label": 1
                },
                {
                    "sent": "How well can you do so?",
                    "label": 0
                },
                {
                    "sent": "I call that the permutation invariant version of M list 'cause these will also be just the same if I took the data set.",
                    "label": 0
                },
                {
                    "sent": "And permitted the order of all the pixels and then fed it to the program.",
                    "label": 0
                },
                {
                    "sent": "Was anything with the prior about special locality will get screwed up by that?",
                    "label": 0
                },
                {
                    "sent": "In a sense, you might think this is doing some of that sort of.",
                    "label": 0
                },
                {
                    "sent": "This is doing that by learning these local features.",
                    "label": 0
                },
                {
                    "sent": "But yes, you could.",
                    "label": 0
                },
                {
                    "sent": "But if you take most pure machine learning approaches like support vector machines.",
                    "label": 0
                },
                {
                    "sent": "You just feed him to this.",
                    "label": 0
                },
                {
                    "sent": "You get results like this.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you take Jahns Nets, convolutional neural Nets trained by Marco really owns Ranzato.",
                    "label": 0
                },
                {
                    "sent": "Using backpropagation, the best results yet, so about .49%.",
                    "label": 0
                },
                {
                    "sent": "If he does unsupervised layer by layer pretraining followed by back propagation, he can do better and this is a record for a single method.",
                    "label": 1
                },
                {
                    "sent": "You can do better than that by averaging methods, but for a single method I think this is the best service.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm now going to give you a different view of why this layer by layer won't learningworks.",
                    "label": 1
                },
                {
                    "sent": "That depends on equivalence between different kinds of networks, and I think this is if you want to understand what's going on when you stack up these bolts for machines, I think this is by far the best way of thinking about it.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is.",
                    "label": 0
                },
                {
                    "sent": "A directed acyclic graph, a directed belief net or directed Sigma belief net.",
                    "label": 0
                },
                {
                    "sent": "This got a funny property.",
                    "label": 0
                },
                {
                    "sent": "Between every pair of layers, it learns the same weights, or rather it has given the same weights.",
                    "label": 0
                },
                {
                    "sent": "So this weight matrices transpose of those.",
                    "label": 0
                },
                {
                    "sent": "And these two layers can be different sizes, but this leads the same size.",
                    "label": 0
                },
                {
                    "sent": "Is that now in this less the same size as that man?",
                    "label": 0
                },
                {
                    "sent": "This looks the same size this size that there so.",
                    "label": 0
                },
                {
                    "sent": "I call this V1 and I call this V2.",
                    "label": 0
                },
                {
                    "sent": "That's sort of a joke.",
                    "label": 0
                },
                {
                    "sent": "That's what the visual systems like.",
                    "label": 0
                },
                {
                    "sent": "But really what I mean is these are hidden units that have the same size as these units.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to convince you that this infinitely deep.",
                    "label": 0
                },
                {
                    "sent": "Sigma belief network.",
                    "label": 0
                },
                {
                    "sent": "Is actually the same thing as a restricted Boltzmann machine.",
                    "label": 1
                },
                {
                    "sent": "OK, they are the same model.",
                    "label": 0
                },
                {
                    "sent": "They're just different ways of writing the same model.",
                    "label": 0
                },
                {
                    "sent": "So first of all, let's try generating data from this.",
                    "label": 0
                },
                {
                    "sent": "To generate data from this, you start a long way up here.",
                    "label": 0
                },
                {
                    "sent": "Um, and you compute PV given HPHMVPV given HP HP envy, PV, given action so on where the PV given H is just.",
                    "label": 0
                },
                {
                    "sent": "Given the state of the hidden unit should input to the visibles, put him through the logistics sample that spear vegan message.",
                    "label": 0
                },
                {
                    "sent": "So if I keep doing that, that's exactly the Markov chain I was running to let the restricted Boltzmann machines sample from his equilibrium distribution and therefore the distribution.",
                    "label": 0
                },
                {
                    "sent": "I'll get here.",
                    "label": 0
                },
                {
                    "sent": "If I generate that way is exactly the same as the distribution I get if I run this alternating Gibb sampling.",
                    "label": 0
                },
                {
                    "sent": "This is also heating up something.",
                    "label": 0
                },
                {
                    "sent": "So they define the same distribution.",
                    "label": 0
                },
                {
                    "sent": "Now the next thing is, how do I do inference in this model?",
                    "label": 0
                },
                {
                    "sent": "OK, so I've got this directive belief net and remember with belief Nets you get explaining away.",
                    "label": 0
                },
                {
                    "sent": "So when I try and infer the states of these given these, I can get explaining away that are going T is going to correlate things and surely inference is going to be complicated.",
                    "label": 0
                },
                {
                    "sent": "Solo Generation is easy, except you had to start a long way up.",
                    "label": 0
                },
                {
                    "sent": "Inference is going to be complicated.",
                    "label": 0
                },
                {
                    "sent": "You thought.",
                    "label": 0
                },
                {
                    "sent": "Well actually no.",
                    "label": 0
                },
                {
                    "sent": "This is a very special kind of.",
                    "label": 0
                },
                {
                    "sent": "Directly belief net in which inference is just as simple as generation, in fact is actually the same processor generation.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do inference, what we're going to do is we're going to take the vector here.",
                    "label": 0
                },
                {
                    "sent": "We're going to multiply by the transpose of those weights, and we're going to put it through logistics function.",
                    "label": 0
                },
                {
                    "sent": "We're going to sample assuming all these are independent.",
                    "label": 0
                },
                {
                    "sent": "That's just what we did with the restricted Boltzmann machine, right?",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And that's going to work in this net.",
                    "label": 0
                },
                {
                    "sent": "And that's what's exciting that you can do exact inference in a deep directed belief net.",
                    "label": 0
                },
                {
                    "sent": "Becausw, but if you think about it, the fact that these weights are the same as these weights means that if I have, say, unit here.",
                    "label": 0
                },
                {
                    "sent": "With big positive connections coming from these two units, then the equivalent unit up here will have big positive connections going to those two units.",
                    "label": 0
                },
                {
                    "sent": "And if you think about the earthquake in track example, the reason you get explaining ways 'cause if I observe that that's on.",
                    "label": 0
                },
                {
                    "sent": "Then probably, let's suppose this had a big negative bias.",
                    "label": 0
                },
                {
                    "sent": "If I observe it's on, then one of those needs to be on to make it on.",
                    "label": 0
                },
                {
                    "sent": "And so they become anti correlated.",
                    "label": 0
                },
                {
                    "sent": "'cause you're only one of these to be able to make that up.",
                    "label": 0
                },
                {
                    "sent": "So they become anti correlated.",
                    "label": 0
                },
                {
                    "sent": "So in the likelihood term coming from the data, these are anticorrelated.",
                    "label": 1
                },
                {
                    "sent": "But now, because these weights are the same as those weights in the prior term coming from here, these are positively correlated, because if that turned on, it would make both of these come on.",
                    "label": 0
                },
                {
                    "sent": "So there's sort of positive correlation coming from there.",
                    "label": 0
                },
                {
                    "sent": "And there's an anti correlation coming from here.",
                    "label": 0
                },
                {
                    "sent": "And if we were really really lucky they would exactly cancel to make these independent.",
                    "label": 0
                },
                {
                    "sent": "And it turns out we are lucky whatever vector you put there, as long as these things are in the exponential family, then when you do this infinite director belief that all this stuff upstairs, all of these weights up here will have constructed a prior such that.",
                    "label": 0
                },
                {
                    "sent": "If I were to generate down to there, I get some correlation here, and that exactly cancels the anti correlation I get from that new term.",
                    "label": 0
                },
                {
                    "sent": "So these really are independent.",
                    "label": 0
                },
                {
                    "sent": "So now we've seen a way of making inference simple.",
                    "label": 0
                },
                {
                    "sent": "Inner directed belief net that people haven't really thought of before, which is.",
                    "label": 0
                },
                {
                    "sent": "Then try and do the difficult inference using Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "Don't just assume independence.",
                    "label": 0
                },
                {
                    "sent": "Actually stick in weights up here such that these really are independent.",
                    "label": 0
                },
                {
                    "sent": "And I can do that simply by having an infinite net with these weights.",
                    "label": 0
                },
                {
                    "sent": "Tide to those weights.",
                    "label": 0
                },
                {
                    "sent": "Now one thing gets a bit more complicated, which is you remember, I told you that if I can get it so I can get a sample from the posterior really easily now.",
                    "label": 0
                },
                {
                    "sent": "I take this state.",
                    "label": 0
                },
                {
                    "sent": "I multiply the transpose of those weights.",
                    "label": 0
                },
                {
                    "sent": "I put it through the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "I sample boom, I got a sample from the posterior here.",
                    "label": 0
                },
                {
                    "sent": "I do the same item from posterior here so I can just run the change in this direction, getting sample from.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Posterior all these less, and to learn the weight from SJO to SIO.",
                    "label": 0
                },
                {
                    "sent": "What I do is I take the state of SJ in the posterior.",
                    "label": 1
                },
                {
                    "sent": "Times the state of SI in the posterior in the data.",
                    "label": 0
                },
                {
                    "sent": "Minus the predicted state press I.",
                    "label": 0
                },
                {
                    "sent": "So my learning rule is that the change in the way it is proportional to the state of SJ times the difference between the state of SI and the prediction for the state of SI will use a hat for that.",
                    "label": 0
                },
                {
                    "sent": "And that should be a probability.",
                    "label": 0
                },
                {
                    "sent": "But if we could sample a binary thing according to that probability, that would do, but it would have a learning rule that was right in the expected sense.",
                    "label": 0
                },
                {
                    "sent": "Right on average.",
                    "label": 0
                },
                {
                    "sent": "So can we get a sample from this?",
                    "label": 0
                },
                {
                    "sent": "So what we want is, given the binary states of these units in the posterior, what do they predict?",
                    "label": 0
                },
                {
                    "sent": "This guy ought to be doing?",
                    "label": 0
                },
                {
                    "sent": "Well, here's an easy way to do it.",
                    "label": 0
                },
                {
                    "sent": "Take the binary states of these guys in the posterior.",
                    "label": 0
                },
                {
                    "sent": "Put him through the weights.",
                    "label": 0
                },
                {
                    "sent": "You see that same as those weights.",
                    "label": 0
                },
                {
                    "sent": "Put him through the sigmoid and sample.",
                    "label": 0
                },
                {
                    "sent": "So this S I-1 is a sample from this.",
                    "label": 0
                },
                {
                    "sent": "It's a sample of that probability.",
                    "label": 0
                },
                {
                    "sent": "Therefore we can stick it in there instead, we just created a bit of noise and so that's the learning rule.",
                    "label": 0
                },
                {
                    "sent": "For the wait on the connection rest JOSI one.",
                    "label": 0
                },
                {
                    "sent": "Very simple learning.",
                    "label": 0
                },
                {
                    "sent": "But that weight is the same as the weight on the connection with Messi 12S J 0.",
                    "label": 0
                },
                {
                    "sent": "So we need to apply the learning rule there too.",
                    "label": 1
                },
                {
                    "sent": "And again, to get the predicted value, here we use the sample value there, so we get the learning like that.",
                    "label": 0
                },
                {
                    "sent": "And then for this way here we get along like that when you add 'em all up you get a telescoping sum where you'll notice the SJOSI one has a - here and the SJOSI one as a + here.",
                    "label": 0
                },
                {
                    "sent": "So they all cancel.",
                    "label": 0
                },
                {
                    "sent": "Then you end up with this term.",
                    "label": 0
                },
                {
                    "sent": "Minus that term, which is the Boltzmann machine learning rule.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how it all comes out consistent.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But now suppose the weights were small.",
                    "label": 0
                },
                {
                    "sent": "We start off with small bites.",
                    "label": 0
                },
                {
                    "sent": "By the time we've run a few steps of inference here, the Nets close to its equilibrium distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so now suppose you take a natural is equilibrium distribution and you sample.",
                    "label": 0
                },
                {
                    "sent": "And you ask it, given these sampled values, what would you like to do with your parameters to make those values more likely?",
                    "label": 0
                },
                {
                    "sent": "Well, if you take enough sample values if there from the equilibrium distribution, the net will tell you.",
                    "label": 0
                },
                {
                    "sent": "I don't want to change my parameters at all.",
                    "label": 0
                },
                {
                    "sent": "My parameters are just perfect for producing those sample values.",
                    "label": 0
                },
                {
                    "sent": "So you know that once this Nets got close to equilibrium, the sum of all the derivatives up here have to come to approximately 0.",
                    "label": 0
                },
                {
                    "sent": "So you can save yourself a whole lot of time.",
                    "label": 0
                },
                {
                    "sent": "You can actually do something bit more sensible than we actually do, which is sort of trying to assess whether it's got close to equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Maybe by looking at the magnitude of the derivatives.",
                    "label": 0
                },
                {
                    "sent": "But basically, let's take a really crude approximation and say, let's assume that after 2 steps there's already quite close to equilibrium, and that's certainly true when the weights are small.",
                    "label": 0
                },
                {
                    "sent": "Then we only need to take these derivatives and these derivatives.",
                    "label": 0
                },
                {
                    "sent": "That is, these guys in these guys and if you add up that that what you see is you get SJOSI 0 -- S I 1S J one other time cancels so that's the learning rule I've been using.",
                    "label": 0
                },
                {
                    "sent": "That's what we call contrast emergence and basically ignoring all the derivatives up here.",
                    "label": 0
                },
                {
                    "sent": "Obviously, as you Start learning, it might be better to go a few more steps back.",
                    "label": 0
                },
                {
                    "sent": "And if you want to do really accurate learning, you just do more steps as you go.",
                    "label": 0
                },
                {
                    "sent": "That's the sort of sensible thing to do, or a sensible.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're doing then when we learn a deep belief net.",
                    "label": 1
                },
                {
                    "sent": "Is we have this model is equivalent to this model?",
                    "label": 1
                },
                {
                    "sent": "We learn these weights using our approximation.",
                    "label": 0
                },
                {
                    "sent": "Having learned those weights.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We then freeze them here.",
                    "label": 1
                },
                {
                    "sent": "We take the aggregated posterior distribution we get there.",
                    "label": 1
                },
                {
                    "sent": "We try to model about, which involves learning these weights.",
                    "label": 0
                },
                {
                    "sent": "But with all these weights together, so we're really learning this infinite net.",
                    "label": 0
                },
                {
                    "sent": "And we keep doing that until we get bored.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's very hard to know how many layers you should use.",
                    "label": 0
                },
                {
                    "sent": "Zoom in will tell you that you shouldn't use too many layers according to his because he was only using a rather small net.",
                    "label": 0
                },
                {
                    "sent": "Because he was doing it so cleverly.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My belief is that actually these things are very robust and you got at least something for evolution to do, and what evolution is going to do is decide roughly how many lesson, how many units?",
                    "label": 0
                },
                {
                    "sent": "OK. Now there's one little snag to this whole argument, which is that.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But if you get back here.",
                    "label": 0
                },
                {
                    "sent": "When these weights were the same as these weights.",
                    "label": 0
                },
                {
                    "sent": "This stuff up here constructs a prior distribution there that's exactly complementary to the likelihood term, so I can do exact inference by taking this binary vector.",
                    "label": 0
                },
                {
                    "sent": "Multiplying by these weights and putting it through the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "But once I start changing these weights.",
                    "label": 0
                },
                {
                    "sent": "Then if I use the transpose of these weights, which are no different, it won't be doing exact inference.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to lose 'cause I'm not doing inference, right?",
                    "label": 0
                },
                {
                    "sent": "However, when I change these weights, I'll be getting a better model of the aggregated posterior here, so I'm going to win 'cause I get a better model of the aggregated posterior.",
                    "label": 0
                },
                {
                    "sent": "And the question is.",
                    "label": 0
                },
                {
                    "sent": "How does what I lose by doing inference wrong compared with what I win by getting a better model?",
                    "label": 0
                },
                {
                    "sent": "And the answer is you win more than you lose when you write down the variational bound.",
                    "label": 0
                },
                {
                    "sent": "So when I change these weights.",
                    "label": 0
                },
                {
                    "sent": "I get a better variational bound.",
                    "label": 0
                },
                {
                    "sent": "And even though my inference isn't quite right anymore, is still pretty good, it started off being perfect and I'll change the way to it so it's not quite perfect, but it's still the case that these are almost independent.",
                    "label": 0
                },
                {
                    "sent": "Given the weights above, that's what typically happens.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's all proved in this paper, which is the sort of basic paper.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This stuff.",
                    "label": 0
                },
                {
                    "sent": "No, I want to quickly.",
                    "label": 0
                },
                {
                    "sent": "Let's see how much more I got to do.",
                    "label": 0
                },
                {
                    "sent": "Yeah I have time.",
                    "label": 0
                },
                {
                    "sent": "So this not running the Markov chains with Librium has a worry which is once the weights get big.",
                    "label": 0
                },
                {
                    "sent": "You don't move very far from the data and the might be huge low energy regions that you never visit.",
                    "label": 0
                },
                {
                    "sent": "And of course, those will mean that the probability of things you do is actually very low under the models distribution 'cause the models putting all this weight on these other things that you never see.",
                    "label": 0
                },
                {
                    "sent": "Because you never see them, you don't realize they're there and you think you've got a good model, but you haven't.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's amazing that, given that that could happen, it doesn't completely screw you up.",
                    "label": 0
                },
                {
                    "sent": "In general, it doesn't completely screw up and you can learn just fine with this sort of one step CD.",
                    "label": 0
                },
                {
                    "sent": "But there's another thing you can do, which is what Mark Radford Neal did to begin with.",
                    "label": 0
                },
                {
                    "sent": "Which is to say, well, instead of running my chain by starting at the data each time, why don't I do the following?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "I'm going to have a number of Markov chains.",
                    "label": 0
                },
                {
                    "sent": "And these Markov chains, I don't keep reinitializing at the data, so this is really CD at all anymore.",
                    "label": 0
                },
                {
                    "sent": "We just keep running these Markov chains and as I change the weights I sort of change the weights in the Markov chain I'm running so.",
                    "label": 0
                },
                {
                    "sent": "So each time I update the weights, I run each of these Markov chains a little bit further.",
                    "label": 0
                },
                {
                    "sent": "Now, if the Markov chain was already close to equilibrium and I didn't change the weights much, it will stay close to equilibrium.",
                    "label": 1
                },
                {
                    "sent": "So as long as I change the weight slowly enough, all these Markov chains are running would be close to the close to equilibrium, so they'll do just fine for estimating this sort of VHJ.",
                    "label": 0
                },
                {
                    "sent": "According to the model, that is, when the models at equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And so I have these persistent chains for estimating that.",
                    "label": 0
                },
                {
                    "sent": "And then for estimating the VHA with the data, I just take the day to activate the hidden units and just look at the correlation.",
                    "label": 0
                },
                {
                    "sent": "That's a nice easy thing to do.",
                    "label": 0
                },
                {
                    "sent": "That was the original idea of both machines that you would you would be awake and you look at things and estimate these correlations, and then you go to sleep and you'd imagine things by running this Markov chain for a long time, and you measure the correlations and then you take the difference in those two statistics.",
                    "label": 1
                },
                {
                    "sent": "And that's how you learn.",
                    "label": 0
                },
                {
                    "sent": "So actually the idea was good.",
                    "label": 0
                },
                {
                    "sent": "As you're asleep, you measure these model statistics and then the next day you change your weights in proportion to the difference between the statistics you measure when you're awake and the model stresses you measure when you're asleep.",
                    "label": 0
                },
                {
                    "sent": "So that's how you actually work.",
                    "label": 0
                },
                {
                    "sent": "According to that theory, which may not be right.",
                    "label": 0
                },
                {
                    "sent": "We can try now.",
                    "label": 0
                },
                {
                    "sent": "Implement that learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it turns out it works very well and it works very well.",
                    "label": 0
                },
                {
                    "sent": "Not at all, for the reason that you might think.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So one thing you can think about this chain is doing something very nice.",
                    "label": 0
                },
                {
                    "sent": "Simulated annealing 'cause you start off with very small weights where it's easy to reach equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And as you learn the weights get bigger.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this chain it looks like very like a simulated annealing chain where the temperature is getting lower, and in fact if you run a persistent chain for a long time during learning.",
                    "label": 0
                },
                {
                    "sent": "It can end up closer to equilibrium than if you started with the final weights and ran for the entire length of time you learn for.",
                    "label": 0
                },
                {
                    "sent": "To try and get to equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Because it was doing this sort of annealing thing started with small weights and gradually using big ones.",
                    "label": 0
                },
                {
                    "sent": "But it turns out you can learn much faster than that analysis will predict.",
                    "label": 0
                },
                {
                    "sent": "And in fact, you only need about 100 of these fantasies.",
                    "label": 0
                },
                {
                    "sent": "You can actually do with just one of these chains, which is very surprising.",
                    "label": 0
                },
                {
                    "sent": "And there's a reason why the learning works much better than you'd expect.",
                    "label": 0
                },
                {
                    "sent": "I'm not gonna, yeah.",
                    "label": 0
                },
                {
                    "sent": "So what's happening when you do learning is you're changing the weights by you raise them by the statistics you measured with the data.",
                    "label": 0
                },
                {
                    "sent": "And you lower them according to the correlations you measured in these Markov chains that you're running to estimate what the model believes in.",
                    "label": 0
                },
                {
                    "sent": "What that means is.",
                    "label": 0
                },
                {
                    "sent": "If you think of these Markov chains like fantasy particles moving around an energy surface.",
                    "label": 0
                },
                {
                    "sent": "Wherever a fancy particle is, the learning will say raise the energy there.",
                    "label": 0
                },
                {
                    "sent": "And that makes the fantasy particles go somewhere else.",
                    "label": 0
                },
                {
                    "sent": "And so you can't get trapped in local Optima.",
                    "label": 0
                },
                {
                    "sent": "OK, if you kept the weights the same, you might have some deep local Optima and the particle might just sort of sit here trying to jump out, but it's very high energy barriers that stays there.",
                    "label": 0
                },
                {
                    "sent": "But if you're learning and you're using this particle for learning, if it's stuck in a minimum.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Roger that minimum is going to keep going up until it falls out of the minimum.",
                    "label": 0
                },
                {
                    "sent": "So there's a funny interaction between the learning algorithm and these Markov chains.",
                    "label": 0
                },
                {
                    "sent": "That has the effect of making the Markov chains mix extremely fast.",
                    "label": 0
                },
                {
                    "sent": "And so all the analysis statisticians do that say if you go slowly enough, blah blah blah.",
                    "label": 0
                },
                {
                    "sent": "That's not why it's working, it's working because the learning is making very fast mixing for these Markov chains by changing the weights.",
                    "label": 0
                },
                {
                    "sent": "So think of it like this.",
                    "label": 0
                },
                {
                    "sent": "Suppose the green particles are what you measured with data.",
                    "label": 0
                },
                {
                    "sent": "And the red particles where your Markov chains are.",
                    "label": 0
                },
                {
                    "sent": "So what you're doing is you're going to lower the energy.",
                    "label": 0
                },
                {
                    "sent": "For the data and you're going to raise the energy for their fantasies from the model, and since there's more particles from the model here than there is data, this minimum go up.",
                    "label": 0
                },
                {
                    "sent": "And it will go up until these particles spill over into you.",
                    "label": 0
                },
                {
                    "sent": "So actually you don't have to solve the problem of how can I jump this energy barrier?",
                    "label": 0
                },
                {
                    "sent": "You're going to change the energy landscape so things don't stay trapped.",
                    "label": 0
                },
                {
                    "sent": "And that means this works really nicely, yeah?",
                    "label": 0
                },
                {
                    "sent": "It's not really mixing.",
                    "label": 0
                },
                {
                    "sent": "Exactly very good.",
                    "label": 0
                },
                {
                    "sent": "Yes, exactly.",
                    "label": 0
                },
                {
                    "sent": "It's not mixing, it just looks like fast mixing.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's exactly right.",
                    "label": 0
                },
                {
                    "sent": "Yeah, will you that referee know OK?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I've said so far.",
                    "label": 0
                },
                {
                    "sent": "Is that these restricted Boltzmann machines?",
                    "label": 1
                },
                {
                    "sent": "Provide your simple way for learning features one layer at a time.",
                    "label": 0
                },
                {
                    "sent": "If you run the Markov chain properties, get fantasies from the model.",
                    "label": 0
                },
                {
                    "sent": "And you did that.",
                    "label": 0
                },
                {
                    "sent": "Starting from scratch all the time, we're very computationally expensive, but actually you can if you start at the just running short time, it works pretty well.",
                    "label": 0
                },
                {
                    "sent": "You can build many layers of representation that way.",
                    "label": 1
                },
                {
                    "sent": "And you can think of what you're doing is taking these little models and composing them.",
                    "label": 0
                },
                {
                    "sent": "And that creates very good generative models.",
                    "label": 0
                },
                {
                    "sent": "The generative models can be fine tuned, which I haven't told you much about.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Once you've done all that, you can actually also do discriminative fine tuning, which I'll tell you about after the break, and that makes them work even better.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The break is going to last till my next lecture which is going to be tomorrow.",
                    "label": 0
                },
                {
                    "sent": "In sorry it's going to be on Wednesday and it's going to be afternoon.",
                    "label": 0
                },
                {
                    "sent": "I think if I get the time right, I might actually turn up.",
                    "label": 0
                },
                {
                    "sent": "OK so I finished.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Earlier.",
                    "label": 0
                },
                {
                    "sent": "Warning against doing maximum likelihood right?",
                    "label": 0
                },
                {
                    "sent": "Especially if we have a huge number of parameters that seems to be exactly what we're doing.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm trying to.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to we don't actually achieve my ideal would be to do maximum likelihood, yes?",
                    "label": 0
                },
                {
                    "sent": "Why would you want to do?",
                    "label": 0
                },
                {
                    "sent": "Should be concerned about.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you thought, given that I'm doing maximum light and not even doing that.",
                    "label": 0
                },
                {
                    "sent": "I mean that's my idea.",
                    "label": 0
                },
                {
                    "sent": "I would overfit horribly, but actually the kinds of tasks I'm working on.",
                    "label": 0
                },
                {
                    "sent": "First of all, I'm modeling the data rather than the labels.",
                    "label": 0
                },
                {
                    "sent": "There's much more information and image than in the labels.",
                    "label": 0
                },
                {
                    "sent": "Secondly, I do actually overfit, but not very much.",
                    "label": 0
                },
                {
                    "sent": "And Thirdly, I think there's two sets of concerns.",
                    "label": 0
                },
                {
                    "sent": "One concern is can you sort of fit a reasonable model at all in any way.",
                    "label": 0
                },
                {
                    "sent": "Fitting it will do for me, as long as you can fit it.",
                    "label": 0
                },
                {
                    "sent": "And then on small datasets on big computers.",
                    "label": 0
                },
                {
                    "sent": "Can you do something better than maximum likelihood so that first slide I had about two kinds of problems for AI problems?",
                    "label": 0
                },
                {
                    "sent": "If you do maximum likelihood fitting of a really complicated model, and you could actually do it, that would be good enough for me.",
                    "label": 0
                },
                {
                    "sent": "It's a two different regimes, right?",
                    "label": 0
                },
                {
                    "sent": "And I think this regime where you have not much data, lots of noise, a big computer and you want to be just as Bayesian as possible and you want sort of infinities that you don't actually use and.",
                    "label": 0
                },
                {
                    "sent": "All that stuff.",
                    "label": 0
                },
                {
                    "sent": "But you don't.",
                    "label": 0
                },
                {
                    "sent": "That's not the name of the game when you're trying to model complicated real data that doesn't have much noise in.",
                    "label": 0
                },
                {
                    "sent": "It's important when you have come across some really neat idea to understand over what range that neatness can be applied and not to generalize it too far to two.",
                    "label": 0
                },
                {
                    "sent": "For example, real problems.",
                    "label": 0
                },
                {
                    "sent": "Zubin maybe comment, then you can respond.",
                    "label": 0
                },
                {
                    "sent": "So in in the case of images where you have 28 by 28 pixels and every image has every pixel observed, and that's that's one kind of real world example where you get a lot of bits of information from your data set, and so maximum likelihood can work extremely well right in.",
                    "label": 0
                },
                {
                    "sent": "And I know you have text examples as well, so you'll show some of that.",
                    "label": 0
                },
                {
                    "sent": "But in examples like text which are also real world data, you could have vast amounts of real world data, but very few measurements.",
                    "label": 0
                },
                {
                    "sent": "Our observations of particularly rare words and things like that, right, right?",
                    "label": 0
                },
                {
                    "sent": "So in fact, you could have someone you've never seen a tool and you have to worry about those two, right?",
                    "label": 0
                },
                {
                    "sent": "So so those are both situations where we have real data, but it doesn't mean that you could just do maximum likelihood ngram models, for example, protects it wouldn't work very well, right?",
                    "label": 0
                },
                {
                    "sent": "Maybe not, although actually restricted Boltzmann machines if you apply them to bags of words, give you better density than if you apply all these fancy topic models.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about the language language modeling, I'm talking about rusty stuff using our BMS to model bags of words.",
                    "label": 0
                },
                {
                    "sent": "I agree, if you're doing language modeling you'd like to have an average between.",
                    "label": 0
                },
                {
                    "sent": "I mean, of course.",
                    "label": 0
                },
                {
                    "sent": "I believe averaging different kinds of models is always a good idea.",
                    "label": 0
                },
                {
                    "sent": "But actually, for the stuff they use topic models for, which is I give you lots of bags of words and you try and learn stuff about what the topics are.",
                    "label": 0
                },
                {
                    "sent": "It actually works quite a bit better in terms of.",
                    "label": 0
                },
                {
                    "sent": "I'm giving high density 2 bags of words to learn restricted Boltzmann machines instead.",
                    "label": 0
                },
                {
                    "sent": "That's coming out in the next nips.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Santa multivalued.",
                    "label": 0
                },
                {
                    "sent": "And also, how do you choose a number of layers?",
                    "label": 0
                },
                {
                    "sent": "OK, you can take binary units and you can turn them into sort of softmax or multinomial units.",
                    "label": 0
                },
                {
                    "sent": "That's just fine.",
                    "label": 0
                },
                {
                    "sent": "You can use Gaussian units if you connect getting used to binary units, that's OK, although you have to use low learning rate if getting used to getting units, things can blow up because you can get improper models.",
                    "label": 0
                },
                {
                    "sent": "You can still learn there, but you have to be very careful and you have to sort of watch it beginning to blow up and learn not to do that, or you can put in some extra stuff to stop that happening.",
                    "label": 0
                },
                {
                    "sent": "So basically the rule is you can use things in the exponential family.",
                    "label": 0
                },
                {
                    "sent": "And it's all fine.",
                    "label": 0
                },
                {
                    "sent": "With a few little visas.",
                    "label": 0
                },
                {
                    "sent": "The question about how many layers and how big each layer?",
                    "label": 0
                },
                {
                    "sent": "Yes, that's where this sort of doing the right Bayesian things to integrate over structures wins.",
                    "label": 0
                },
                {
                    "sent": "It can decide how many layers and how many things in each layer.",
                    "label": 0
                },
                {
                    "sent": "What we're doing is we're just trying a few things.",
                    "label": 0
                },
                {
                    "sent": "Um, and you know you try 500, you try 1000, you try 2000.",
                    "label": 0
                },
                {
                    "sent": "They work pretty well, so it's not like it's very sensitive to that.",
                    "label": 0
                },
                {
                    "sent": "Modeling.",
                    "label": 0
                },
                {
                    "sent": "You could do all that, the trouble is.",
                    "label": 0
                },
                {
                    "sent": "It's hard enough to do an approximation to maximum likelihood learning.",
                    "label": 0
                },
                {
                    "sent": "If I tell you how many layers and how many feet things perler.",
                    "label": 0
                },
                {
                    "sent": "If you now try and do the search over all these structures as well.",
                    "label": 0
                },
                {
                    "sent": "And if you try to do it properly, it's just a hopeless big computation.",
                    "label": 0
                },
                {
                    "sent": "And all I want is something that works.",
                    "label": 0
                },
                {
                    "sent": "It's not like we're trying to identify a real system, right?",
                    "label": 0
                },
                {
                    "sent": "We you know these objects in the world, and I'd like to recognize them.",
                    "label": 0
                },
                {
                    "sent": "And it's not like there is a correct net.",
                    "label": 0
                },
                {
                    "sent": "Is not like the objects in the world really were generated from one of these Nets, so we're not doing exact identification where you'd like to see if it really is 10 hidden units or 11.",
                    "label": 0
                },
                {
                    "sent": "You're in a situation where a few billion neurons will do it.",
                    "label": 0
                },
                {
                    "sent": "If only you could shoot him right.",
                    "label": 0
                },
                {
                    "sent": "Reading this and could take you in directions that you don't want it.",
                    "label": 0
                },
                {
                    "sent": "I mean, does it always average out to the right gradient or?",
                    "label": 0
                },
                {
                    "sent": "Is there reason to believe that?",
                    "label": 0
                },
                {
                    "sent": "Well, sort of.",
                    "label": 0
                },
                {
                    "sent": "If you got number assessment of the gradient, then on average it will do good things.",
                    "label": 0
                },
                {
                    "sent": "So in the limit when you use very small learning rates, it'll average out and you're going the right direction.",
                    "label": 0
                },
                {
                    "sent": "If you use big learning rates, of course, crazy things could happen.",
                    "label": 0
                },
                {
                    "sent": "And typically what you want to do is push the learning rates to be about half as big as one crazy things happen.",
                    "label": 0
                },
                {
                    "sent": "It's all very unsatisfactory for theorists.",
                    "label": 0
                },
                {
                    "sent": "It just happens to work better than the other methods.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "So it's actually quite cool, but my question is thank you.",
                    "label": 0
                },
                {
                    "sent": "Are these in this state?",
                    "label": 0
                },
                {
                    "sent": "Is it really the right application?",
                    "label": 0
                },
                {
                    "sent": "Because I think the customer so they knew that he was just a polynomial kernel this SVM and they get nearly as well.",
                    "label": 0
                },
                {
                    "sent": "So are there domains of problems where I can really gain?",
                    "label": 0
                },
                {
                    "sent": "A lot compared to classic in discriminative learning, right?",
                    "label": 0
                },
                {
                    "sent": "So that's a very good question.",
                    "label": 0
                },
                {
                    "sent": "I sort of completely agree with the sort of intent of it.",
                    "label": 0
                },
                {
                    "sent": "Now, one reason for using Emnace to begin with is that there's lots of results on M list, and you can tell whether you've got a method that sort of works well or doesn't.",
                    "label": 0
                },
                {
                    "sent": "So you can sort of like things that really don't work very well that way, but you're right, what you'd like is to get a really big win when you'd like to get it on big datasets with more realistic images, I'll talk about a little bit of that after the break.",
                    "label": 0
                },
                {
                    "sent": "I mean, that is in two days time.",
                    "label": 0
                },
                {
                    "sent": "And that's the direction in which we're trying to go, obviously, and that's when you should really believe it when we can get much better so already on 3D objects.",
                    "label": 0
                },
                {
                    "sent": "So Jan has a database called the Norm database.",
                    "label": 0
                },
                {
                    "sent": "That little plastic toys but seen from many different viewpoints with different lighting conditions.",
                    "label": 0
                },
                {
                    "sent": "We can do much better than this films on those.",
                    "label": 0
                },
                {
                    "sent": "So I'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes, I will talk about that on Wednesday too.",
                    "label": 0
                },
                {
                    "sent": "I'll show you some really cute models, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Chris continuous latent variables.",
                    "label": 0
                },
                {
                    "sent": "No, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "I think if I mean one problem we have is if you have Gaussian and Gaussian you try to do this, it can become an improper model.",
                    "label": 0
                },
                {
                    "sent": "So you have to do something about that.",
                    "label": 0
                },
                {
                    "sent": "It would be a very sensible thing to do, but you might, for example, say just slightly inspired by how the brain works.",
                    "label": 0
                },
                {
                    "sent": "You might say let's have positive only variables, but let them have approximate real values.",
                    "label": 0
                },
                {
                    "sent": "And there's a neat way to hack those variables up.",
                    "label": 0
                },
                {
                    "sent": "And which I think I will talk about on Wednesday.",
                    "label": 0
                },
                {
                    "sent": "Where you can show that a whole collection of binary variables can be viewed as a rectified linear unit.",
                    "label": 0
                },
                {
                    "sent": "And that works very nicely.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "But yeah, it's this sort of complete, which is.",
                    "label": 0
                },
                {
                    "sent": "It's silly to be using binary hidden variables for representing continuous things.",
                    "label": 0
                },
                {
                    "sent": "Unless of course, your learning algorithm works so much better with these binary variables that it works better that way.",
                    "label": 0
                },
                {
                    "sent": "Then when you use continuous hidden things, and that's sort of roughly the state wherein.",
                    "label": 0
                },
                {
                    "sent": "These binary hidden variables work very well.",
                    "label": 0
                },
                {
                    "sent": "The easy to learn.",
                    "label": 0
                },
                {
                    "sent": "And they seem to work just fine for modeling all sorts of continuous stuff when the observable variables are Gaussian or revenue.",
                    "label": 0
                },
                {
                    "sent": "But we are working towards having things that aren't quite binary like question things.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}